{"home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.translate.get_parser": [[30, 55], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Translate sentences\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\"./dumped/\"", ",", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Experiment name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_id\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Experiment ID\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "\"Number of sentences per batch\"", ")", "\n", "\n", "# model / output paths", "\n", "parser", ".", "add_argument", "(", "\"--model_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Model path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Output path\"", ")", "\n", "\n", "# parser.add_argument(\"--max_vocab\", type=int, default=-1, help=\"Maximum vocabulary size (-1 to disable)\")", "\n", "# parser.add_argument(\"--min_count\", type=int, default=0, help=\"Minimum vocabulary count\")", "\n", "\n", "# source language / target language", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Source language\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Target language\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.translate.main": [[57, 126], ["src.utils.initialize_exp", "translate.get_parser", "get_parser.parse_args", "torch.load", "src.utils.AttrDict", "src.utils.initialize_exp.info", "src.data.dictionary.Dictionary", "src.model.transformer.TransformerModel().cuda().eval", "src.model.transformer.TransformerModel().cuda().eval", "TransformerModel().cuda().eval.load_state_dict", "TransformerModel().cuda().eval.load_state_dict", "sys.stdin.readlines", "src.utils.initialize_exp.info", "io.open", "range", "io.open.close", "setattr", "src_sent.append", "len", "torch.LongTensor", "torch.LongTensor().fill_", "enumerate", "torch.LongTensor().fill_.clone().fill_", "TransformerModel().cuda().eval.", "encoded.transpose.transpose", "TransformerModel().cuda().eval.generate", "range", "getattr", "src.model.transformer.TransformerModel().cuda", "src.model.transformer.TransformerModel().cuda", "len", "len", "torch.LongTensor", "torch.LongTensor.cuda", "decoded.size", "src_sent[].strip", "sys.stderr.write", "io.open.write", "src.utils.AttrDict.lang2id.keys", "line.strip().split", "torch.LongTensor", "batch[].copy_", "torch.LongTensor().fill_.clone", "torch.LongTensor().fill_.cuda", "torch.LongTensor.cuda", "batch.clone().fill_.cuda", "int", "src.model.transformer.TransformerModel", "src.model.transformer.TransformerModel", "src.data.dictionary.Dictionary.index", "len", "torch.LongTensor.max().item", "torch.LongTensor.size", "len", "delimiters[].item", "len", "line.strip", "s.strip().split", "range", "len", "torch.LongTensor.max", "torch.LongTensor.max().item", "sent[].item", "len", "s.strip", "torch.LongTensor.max"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.initialize_exp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_generate_macd.get_parser", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["", "def", "main", "(", "params", ")", ":", "\n", "\n", "# initialize the experiment", "\n", "    ", "logger", "=", "initialize_exp", "(", "params", ")", "\n", "\n", "# generate parser / parse parameters", "\n", "parser", "=", "get_parser", "(", ")", "\n", "params", "=", "parser", ".", "parse_args", "(", ")", "\n", "reloaded", "=", "torch", ".", "load", "(", "params", ".", "model_path", ")", "\n", "model_params", "=", "AttrDict", "(", "reloaded", "[", "'params'", "]", ")", "\n", "logger", ".", "info", "(", "\"Supported languages: %s\"", "%", "\", \"", ".", "join", "(", "model_params", ".", "lang2id", ".", "keys", "(", ")", ")", ")", "\n", "\n", "# update dictionary parameters", "\n", "for", "name", "in", "[", "'n_words'", ",", "'bos_index'", ",", "'eos_index'", ",", "'pad_index'", ",", "'unk_index'", ",", "'mask_index'", "]", ":", "\n", "        ", "setattr", "(", "params", ",", "name", ",", "getattr", "(", "model_params", ",", "name", ")", ")", "\n", "\n", "# build dictionary / build encoder / build decoder / reload weights", "\n", "", "dico", "=", "Dictionary", "(", "reloaded", "[", "'dico_id2word'", "]", ",", "reloaded", "[", "'dico_word2id'", "]", ",", "reloaded", "[", "'dico_counts'", "]", ")", "\n", "encoder", "=", "TransformerModel", "(", "model_params", ",", "dico", ",", "is_encoder", "=", "True", ",", "with_output", "=", "True", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", "\n", "decoder", "=", "TransformerModel", "(", "model_params", ",", "dico", ",", "is_encoder", "=", "False", ",", "with_output", "=", "True", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", "\n", "encoder", ".", "load_state_dict", "(", "reloaded", "[", "'encoder'", "]", ")", "\n", "decoder", ".", "load_state_dict", "(", "reloaded", "[", "'decoder'", "]", ")", "\n", "params", ".", "src_id", "=", "model_params", ".", "lang2id", "[", "params", ".", "src_lang", "]", "\n", "params", ".", "tgt_id", "=", "model_params", ".", "lang2id", "[", "params", ".", "tgt_lang", "]", "\n", "\n", "# read sentences from stdin", "\n", "src_sent", "=", "[", "]", "\n", "for", "line", "in", "sys", ".", "stdin", ".", "readlines", "(", ")", ":", "\n", "        ", "assert", "len", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ">", "0", "\n", "src_sent", ".", "append", "(", "line", ")", "\n", "", "logger", ".", "info", "(", "\"Read %i sentences from stdin. Translating ...\"", "%", "len", "(", "src_sent", ")", ")", "\n", "\n", "f", "=", "io", ".", "open", "(", "params", ".", "output_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "src_sent", ")", ",", "params", ".", "batch_size", ")", ":", "\n", "\n", "# prepare batch", "\n", "        ", "word_ids", "=", "[", "torch", ".", "LongTensor", "(", "[", "dico", ".", "index", "(", "w", ")", "for", "w", "in", "s", ".", "strip", "(", ")", ".", "split", "(", ")", "]", ")", "\n", "for", "s", "in", "src_sent", "[", "i", ":", "i", "+", "params", ".", "batch_size", "]", "]", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "[", "len", "(", "s", ")", "+", "2", "for", "s", "in", "word_ids", "]", ")", "\n", "batch", "=", "torch", ".", "LongTensor", "(", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", ",", "lengths", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "params", ".", "pad_index", ")", "\n", "batch", "[", "0", "]", "=", "params", ".", "eos_index", "\n", "for", "j", ",", "s", "in", "enumerate", "(", "word_ids", ")", ":", "\n", "            ", "if", "lengths", "[", "j", "]", ">", "2", ":", "# if sentence not empty", "\n", "                ", "batch", "[", "1", ":", "lengths", "[", "j", "]", "-", "1", ",", "j", "]", ".", "copy_", "(", "s", ")", "\n", "", "batch", "[", "lengths", "[", "j", "]", "-", "1", ",", "j", "]", "=", "params", ".", "eos_index", "\n", "", "langs", "=", "batch", ".", "clone", "(", ")", ".", "fill_", "(", "params", ".", "src_id", ")", "\n", "\n", "# encode source batch and translate it", "\n", "encoded", "=", "encoder", "(", "'fwd'", ",", "x", "=", "batch", ".", "cuda", "(", ")", ",", "lengths", "=", "lengths", ".", "cuda", "(", ")", ",", "langs", "=", "langs", ".", "cuda", "(", ")", ",", "causal", "=", "False", ")", "\n", "encoded", "=", "encoded", ".", "transpose", "(", "0", ",", "1", ")", "\n", "decoded", ",", "dec_lengths", "=", "decoder", ".", "generate", "(", "encoded", ",", "lengths", ".", "cuda", "(", ")", ",", "params", ".", "tgt_id", ",", "max_len", "=", "int", "(", "1.5", "*", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", ")", "\n", "\n", "# convert sentences to words", "\n", "for", "j", "in", "range", "(", "decoded", ".", "size", "(", "1", ")", ")", ":", "\n", "\n", "# remove delimiters", "\n", "            ", "sent", "=", "decoded", "[", ":", ",", "j", "]", "\n", "delimiters", "=", "(", "sent", "==", "params", ".", "eos_index", ")", ".", "nonzero", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "assert", "len", "(", "delimiters", ")", ">=", "1", "and", "delimiters", "[", "0", "]", ".", "item", "(", ")", "==", "0", "\n", "sent", "=", "sent", "[", "1", ":", "]", "if", "len", "(", "delimiters", ")", "==", "1", "else", "sent", "[", "1", ":", "delimiters", "[", "1", "]", "]", "\n", "\n", "# output translation", "\n", "source", "=", "src_sent", "[", "i", "+", "j", "]", ".", "strip", "(", ")", "\n", "target", "=", "\" \"", ".", "join", "(", "[", "dico", "[", "sent", "[", "k", "]", ".", "item", "(", ")", "]", "for", "k", "in", "range", "(", "len", "(", "sent", ")", ")", "]", ")", "\n", "sys", ".", "stderr", ".", "write", "(", "\"%i / %i: %s -> %s\\n\"", "%", "(", "i", "+", "j", ",", "len", "(", "src_sent", ")", ",", "source", ",", "target", ")", ")", "\n", "f", ".", "write", "(", "target", "+", "\"\\n\"", ")", "\n", "\n", "", "", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train.get_parser": [[24, 249], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "src.model.memory.HashingMemory.register_args", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "argparse.ArgumentParser.parse_known_args"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.register_args"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Language transfer\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\"./dumped/\"", ",", "\n", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_periodic\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Save the model periodically (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_id\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment ID\"", ")", "\n", "\n", "# float16 / AMP API", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Run model with float16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--amp\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Use AMP wrapper for float16 / distributed / gradient accumulation. Level of optimization. -1 to disable.\"", ")", "\n", "\n", "# only use an encoder (use a specific decoder for machine translation)", "\n", "parser", ".", "add_argument", "(", "\"--encoder_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Only use an encoder\"", ")", "\n", "\n", "# model parameters", "\n", "parser", ".", "add_argument", "(", "\"--emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Embedding layer size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_enc\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_dec\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--n_heads\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"Number of Transformer heads\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout in the attention layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gelu_activation\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use a GELU activation instead of ReLU\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_inout_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Share input and output embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sinusoidal_embeddings\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use sinusoidal embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_lang_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Use language embedding\"", ")", "\n", "\n", "# memory parameters", "\n", "parser", ".", "add_argument", "(", "\"--use_memory\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use an external memory\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "use_memory", ":", "\n", "        ", "HashingMemory", ".", "register_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_enc_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the encoder ('4' for inside layer 4, '7,10+' for inside layer 7 and after layer 10)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_dec_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the decoder. Same syntax as `mem_enc_positions`.\"", ")", "\n", "\n", "# adaptive softmax", "\n", "", "parser", ".", "add_argument", "(", "\"--asm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use adaptive softmax\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "asm", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\"--asm_cutoffs\"", ",", "type", "=", "str", ",", "default", "=", "\"8000,20000\"", ",", "\n", "help", "=", "\"Adaptive softmax cutoffs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--asm_div_value\"", ",", "type", "=", "float", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Adaptive softmax cluster sizes ratio\"", ")", "\n", "\n", "# causal language modeling task parameters", "\n", "", "parser", ".", "add_argument", "(", "\"--context_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Context size (0 means that the first elements in sequences won't have any context)\"", ")", "\n", "\n", "# masked language modeling task parameters", "\n", "parser", ".", "add_argument", "(", "\"--word_pred\"", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "\n", "help", "=", "\"Fraction of words for which we need to make a prediction\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sample_alpha\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Exponent for transforming word counts to probabilities (~word2vec sampling)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_mask_keep_rand\"", ",", "type", "=", "str", ",", "default", "=", "\"0.8,0.1,0.1\"", ",", "\n", "help", "=", "\"Fraction of words to mask out / keep / randomize, among the words to predict\"", ")", "\n", "\n", "# input sentence noise", "\n", "parser", ".", "add_argument", "(", "\"--word_shuffle\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly shuffle input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly dropout input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_blank\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly blank input words (0 to disable)\"", ")", "\n", "\n", "# ED-MLM Parameters", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_full\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Full prediction at decoder\"", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--data_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lgs\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Languages (lg1-lg2-lg3 .. ex: en-fr-es-de)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_vocab\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Maximum vocabulary size (-1 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_count\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Minimum vocabulary count\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lg_sampling_factor\"", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Language sampling factor\"", ")", "\n", "\n", "# batch parameters", "\n", "parser", ".", "add_argument", "(", "\"--bptt\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Sequence length\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_len\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Maximum length of sentences (after BPE)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--group_by_size\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Sort sentences by size during the training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Number of sentences per batch\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_batch_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokens_per_batch\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of tokens per batch\"", ")", "\n", "\n", "# training parameters", "\n", "parser", ".", "add_argument", "(", "\"--split_data\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Split data across workers of a same node\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"adam,lr=0.0001\"", ",", "\n", "help", "=", "\"Optimizer (SGD / RMSprop / Adam, etc.)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--clip_grad_norm\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "\n", "help", "=", "\"Clip gradients norm (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--epoch_size\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Epoch size / evaluation frequency (-1 for parallel data size)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Maximum epoch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stopping_criterion\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Stopping criterion, and number of non-increase before stopping the experiment\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--validation_metrics\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Validation metrics\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulate_gradients\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Accumulate model gradients over N iterations (N times larger batch sizes)\"", ")", "\n", "\n", "# training coefficients", "\n", "parser", ".", "add_argument", "(", "\"--lambda_edmlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (EDMLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (MLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_clm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Causal coefficient (LM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_pc\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"PC coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_ae\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"AE coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"MT coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_bt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"BT coefficient\"", ")", "\n", "# parser.add_argument(\"--lambda_bt_otf\", type=str, default=\"0\",", "\n", "#                     help=\"BT coefficient on the fly separate\")", "\n", "parser", ".", "add_argument", "(", "\"--bt_sync\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"log_per_iter\"", ")", "\n", "\n", "\n", "# training steps", "\n", "parser", ".", "add_argument", "(", "\"--clm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Causal prediction steps (CLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (MLM / TLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (EDMLM / EDTLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Machine translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ae_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Denoising auto-encoder steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Back-translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pc_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Parallel classification steps\"", ")", "\n", "\n", "# logging", "\n", "parser", ".", "add_argument", "(", "\"--log_per_iter\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"log_per_iter\"", ")", "\n", "\n", "# reload pretrained embeddings / pretrained model / checkpoint", "\n", "parser", ".", "add_argument", "(", "\"--reload_emb\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload pretrained word embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a pretrained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_checkpoint\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a checkpoint\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--reload_2nd_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a secondary parallel model. X ->(m1) Y1 =>(m2) X2\"", ")", "\n", "\n", "# beam search (for MT only)", "\n", "parser", ".", "add_argument", "(", "\"--beam_size\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size, default = 1 (greedy decoding)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Length penalty, values < 1.0 favor shorter sentences, while values > 1.0 favor longer ones.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--early_stopping\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Early stopping, stop as soon as we have `beam_size` hypotheses, although longer ones may have better scores.\"", ")", "\n", "\n", "# evaluation", "\n", "parser", ".", "add_argument", "(", "\"--eval_bleu\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Evaluate BLEU score during MT training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Only run evaluations\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Infer training data\"", ")", "\n", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use valid sets for train sets (faster loading)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug multi-GPU / multi-node within a SLURM job\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "help", "=", "\"Enable all debug flags\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "\n", "# multi-gpu / multi-node", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--master_port\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Master port (for multi-node SLURM jobs)\"", ")", "\n", "\n", "# seed", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"If >= 0, set the seed\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train.main": [[250, 396], ["src.slurm.init_distributed_mode", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.loader.load_data", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.set_sampling_probs", "range", "src.model.build_model", "src.trainer.SingleTrainer", "src.evaluation.evaluator.SingleEvaluator", "src.trainer.EncDecTrainer", "src.evaluation.evaluator.EncDecEvaluator", "src.evaluation.evaluator.EncDecEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.utils.initialize_exp.info", "exit", "print", "src.utils.initialize_exp.info", "isinstance", "src.evaluation.evaluator.EncDecEvaluator.infer_train", "evaluator.run_all_evals.items", "src.utils.initialize_exp.info", "exit", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.evaluation.evaluator.EncDecEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.trainer.EncDecTrainer.save_best_model", "src.trainer.EncDecTrainer.save_periodic", "src.trainer.EncDecTrainer.end_epoch", "src.utils.initialize_exp.info", "build_func", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.trainer.EncDecTrainer.iter", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "json.dumps", "json.dumps", "src.trainer.EncDecTrainer.clm_step", "src.trainer.EncDecTrainer.mlm_step", "src.trainer.EncDecTrainer.pc_step", "src.trainer.EncDecTrainer.mt_step", "src.trainer.EncDecTrainer.mt_step", "src.utils.shuf_order", "src.utils.shuf_order", "src.trainer.EncDecTrainer.bt_step", "src.trainer.EncDecTrainer.update_syn_model", "src.trainer.EncDecTrainer.bt_sync_step", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_distributed_mode", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.initialize_exp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.set_sampling_probs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_best_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_periodic", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.end_epoch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.iter", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.clm_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.mlm_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.pc_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mt_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mt_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.bt_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.update_syn_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.bt_sync_step"], ["", "def", "main", "(", "params", ")", ":", "\n", "\n", "# initialize the multi-GPU / multi-node training", "\n", "    ", "init_distributed_mode", "(", "params", ")", "\n", "\n", "if", "params", ".", "infer_train", ":", "\n", "        ", "log_filename", "=", "'infer.train.log'", "\n", "params_filename", "=", "'infer.train.params.pkl'", "\n", "", "else", ":", "\n", "        ", "log_filename", ",", "params_filename", "=", "None", ",", "None", "\n", "\n", "# initialize the experiment", "\n", "", "logger", "=", "initialize_exp", "(", "params", ",", "log_filename", ",", "params_filename", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "# load data", "\n", "data", "=", "load_data", "(", "params", ")", "\n", "logger", ".", "info", "(", "'INIT MODEL HERE'", ")", "\n", "logger", ".", "info", "(", "data", ")", "\n", "\n", "# build model", "\n", "if", "params", ".", "encoder_only", ":", "\n", "        ", "model", "=", "build_model", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "build_func", "=", "build_model_multilang", "if", "(", "params", ".", "share_enc", ">", "-", "1", "or", "params", ".", "share_dec", ">", "-", "1", ")", "else", "build_model", "\n", "logger", ".", "info", "(", "'Build function: {}'", ".", "format", "(", "build_func", ".", "__name__", ")", ")", "\n", "# encoder, decoder = build_model(params, data['dico'])", "\n", "encoder", ",", "decoder", "=", "build_func", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# print(data)", "\n", "            ", "raise", "e", "\n", "\n", "# build trainer, reload potential checkpoints / build evaluator", "\n", "", "", "if", "params", ".", "encoder_only", ":", "\n", "        ", "trainer", "=", "SingleTrainer", "(", "model", ",", "data", ",", "params", ")", "\n", "evaluator", "=", "SingleEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "", "else", ":", "\n", "        ", "trainer", "=", "EncDecTrainer", "(", "encoder", ",", "decoder", ",", "data", ",", "params", ")", "\n", "evaluator", "=", "EncDecEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "\n", "# evaluation", "\n", "", "if", "params", ".", "eval_only", ":", "\n", "        ", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "if", "params", ".", "infer_train", ":", "\n", "        ", "print", "(", "'**** Infer Train'", ")", "\n", "logger", ".", "info", "(", "'======== Start Generating ==========='", ")", "\n", "assert", "isinstance", "(", "evaluator", ",", "EncDecEvaluator", ")", "\n", "scores", "=", "evaluator", ".", "infer_train", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "# set sampling probabilities for training", "\n", "", "set_sampling_probs", "(", "data", ",", "params", ")", "\n", "\n", "# language model training", "\n", "for", "_", "in", "range", "(", "params", ".", "max_epoch", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "trainer", ".", "n_sentences", "=", "0", "\n", "\n", "while", "trainer", ".", "n_sentences", "<", "trainer", ".", "epoch_size", ":", "\n", "\n", "# CLM steps", "\n", "            ", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "clm_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "clm_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_clm", ")", "\n", "\n", "# MLM steps (also includes TLM if lang2 is not None)", "\n", "", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "mlm_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "mlm_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_mlm", ")", "\n", "\n", "# parallel classification steps", "\n", "", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "pc_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "pc_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_pc", ")", "\n", "\n", "# denoising auto-encoder steps", "\n", "", "for", "lang", "in", "shuf_order", "(", "params", ".", "ae_steps", ")", ":", "\n", "                ", "trainer", ".", "mt_step", "(", "lang", ",", "lang", ",", "params", ".", "lambda_ae", ")", "\n", "\n", "# machine translation steps", "\n", "", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "mt_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "mt_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_mt", ")", "\n", "\n", "# back-translation steps", "\n", "", "if", "params", ".", "bt_sync", "==", "1", ":", "\n", "                ", "for", "lang1", ",", "lang2", ",", "lang3", "in", "shuf_order", "(", "params", ".", "bt_steps", ")", ":", "\n", "                    ", "trainer", ".", "bt_step", "(", "lang1", ",", "lang2", ",", "lang3", ",", "params", ".", "lambda_bt", ")", "\n", "", "", "else", ":", "\n", "                ", "assert", "params", ".", "bt_sync", ">", "1", "\n", "if", "trainer", ".", "n_iter", "%", "params", ".", "bt_sync", "==", "0", "and", "trainer", ".", "n_iter", ">", "0", ":", "\n", "                    ", "trainer", ".", "update_syn_model", "(", ")", "\n", "", "for", "lang1", ",", "lang2", ",", "lang3", "in", "shuf_order", "(", "params", ".", "bt_steps", ")", ":", "\n", "                    ", "trainer", ".", "bt_sync_step", "(", "lang1", ",", "lang2", ",", "lang3", ",", "params", ".", "lambda_bt", ")", "\n", "\n", "# # AE - MT training (on the fly back-translation)", "\n", "# # start on-the-fly batch generations", "\n", "# if not getattr(params, 'started_otf_batch_gen', False):", "\n", "#     otf_iterator = trainer.otf_bt_gen_async()", "\n", "#     params.started_otf_batch_gen = True", "\n", "# # update model parameters on subprocesses", "\n", "# if trainer.n_iter % params.otf_sync_params_every == 0:", "\n", "#     trainer.otf_sync_params()", "\n", "# # get training batch from CPU", "\n", "# before_gen = time.time()", "\n", "# batches = next(otf_iterator)", "\n", "# trainer.gen_time += time.time() - before_gen", "\n", "# # training", "\n", "# for batch in batches:", "\n", "#     lang1, lang2, lang3 = batch['lang1'], batch['lang2'], batch['lang3']", "\n", "#     # 2-lang back-translation - autoencoding", "\n", "#     if lang1 != lang2 == lang3:", "\n", "#         trainer.otf_bt(batch, params.lambda_xe_otfa, params.otf_backprop_temperature)", "\n", "#     # 2-lang back-translation - parallel data", "\n", "#     elif lang1 == lang3 != lang2:", "\n", "#         trainer.otf_bt(batch, params.lambda_xe_otfd, params.otf_backprop_temperature)", "\n", "#     # 3-lang back-translation - parallel data", "\n", "#     elif lang1 != lang2 and lang2 != lang3 and lang1 != lang3:", "\n", "#         trainer.otf_bt(batch, params.lambda_xe_otfd, params.otf_backprop_temperature)", "\n", "\n", "", "", "trainer", ".", "iter", "(", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"============ End of epoch %i ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "# evaluate perplexity", "\n", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "\n", "# print / JSON log", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "if", "params", ".", "is_master", ":", "\n", "            ", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "trainer", ".", "save_best_model", "(", "scores", ")", "\n", "trainer", ".", "save_periodic", "(", ")", "\n", "trainer", ".", "end_epoch", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_online_cbd.get_parser": [[26, 320], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "src.model.memory.HashingMemory.register_args", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "argparse.ArgumentParser.parse_known_args"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.register_args"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Language transfer\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\"./dumped/\"", ",", "\n", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_periodic\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Save the model periodically (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_id\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment ID\"", ")", "\n", "\n", "# float16 / AMP API", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Run model with float16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--amp\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Use AMP wrapper for float16 / distributed / gradient accumulation. Level of optimization. -1 to disable.\"", ")", "\n", "\n", "# only use an encoder (use a specific decoder for machine translation)", "\n", "parser", ".", "add_argument", "(", "\"--encoder_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Only use an encoder\"", ")", "\n", "\n", "# model parameters", "\n", "parser", ".", "add_argument", "(", "\"--emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Embedding layer size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_enc\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_dec\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_dec_layers\"", ",", "type", "=", "int", ",", "default", "=", "6", ",", "\n", "help", "=", "\"Number of Decoder Transformer layers\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--n_heads\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"Number of Transformer heads\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout in the attention layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gelu_activation\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use a GELU activation instead of ReLU\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_inout_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Share input and output embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sinusoidal_embeddings\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use sinusoidal embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_lang_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Use language embedding\"", ")", "\n", "\n", "# memory parameters", "\n", "parser", ".", "add_argument", "(", "\"--use_memory\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use an external memory\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "use_memory", ":", "\n", "        ", "HashingMemory", ".", "register_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_enc_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the encoder ('4' for inside layer 4, '7,10+' for inside layer 7 and after layer 10)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_dec_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the decoder. Same syntax as `mem_enc_positions`.\"", ")", "\n", "\n", "# adaptive softmax", "\n", "", "parser", ".", "add_argument", "(", "\"--asm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use adaptive softmax\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "asm", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\"--asm_cutoffs\"", ",", "type", "=", "str", ",", "default", "=", "\"8000,20000\"", ",", "\n", "help", "=", "\"Adaptive softmax cutoffs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--asm_div_value\"", ",", "type", "=", "float", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Adaptive softmax cluster sizes ratio\"", ")", "\n", "\n", "# causal language modeling task parameters", "\n", "", "parser", ".", "add_argument", "(", "\"--context_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Context size (0 means that the first elements in sequences won't have any context)\"", ")", "\n", "\n", "# masked language modeling task parameters", "\n", "parser", ".", "add_argument", "(", "\"--word_pred\"", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "\n", "help", "=", "\"Fraction of words for which we need to make a prediction\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sample_alpha\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Exponent for transforming word counts to probabilities (~word2vec sampling)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_mask_keep_rand\"", ",", "type", "=", "str", ",", "default", "=", "\"0.8,0.1,0.1\"", ",", "\n", "help", "=", "\"Fraction of words to mask out / keep / randomize, among the words to predict\"", ")", "\n", "\n", "\n", "# input sentence noise", "\n", "parser", ".", "add_argument", "(", "\"--word_shuffle\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly shuffle input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly dropout input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_blank\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly blank input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_mass\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly mask input words (0 to disable)\"", ")", "\n", "\n", "# ED-MLM Parameters", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_full\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Full prediction at decoder\"", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--data_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lgs\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Languages (lg1-lg2-lg3 .. ex: en-fr-es-de)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_vocab\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Maximum vocabulary size (-1 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_count\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Minimum vocabulary count\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lg_sampling_factor\"", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Language sampling factor\"", ")", "\n", "\n", "# batch parameters", "\n", "parser", ".", "add_argument", "(", "\"--bptt\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Sequence length\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_len\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Minimum length of sentences (after BPE)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_len\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Maximum length of sentences (after BPE)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--group_by_size\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Sort sentences by size during the training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Number of sentences per batch\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_batch_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokens_per_batch\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of tokens per batch\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--is_sentencepiece\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Sentencepiece\"", ")", "\n", "\n", "# training parameters", "\n", "parser", ".", "add_argument", "(", "\"--split_data\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Split data across workers of a same node\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"adam,lr=0.0001\"", ",", "\n", "help", "=", "\"Optimizer (SGD / RMSprop / Adam, etc.)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--clip_grad_norm\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "\n", "help", "=", "\"Clip gradients norm (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--epoch_size\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Epoch size / evaluation frequency (-1 for parallel data size)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Maximum epoch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stopping_criterion\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Stopping criterion, and number of non-increase before stopping the experiment\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--validation_metrics\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Validation metrics\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulate_gradients\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Accumulate model gradients over N iterations (N times larger batch sizes)\"", ")", "\n", "\n", "# training coefficients", "\n", "parser", ".", "add_argument", "(", "\"--lambda_edmlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (EDMLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (MLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_clm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Causal coefficient (LM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_pc\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"PC coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_ae\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"AE coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mbeam_ae\"", ",", "type", "=", "str", ",", "default", "=", "\"0\"", ",", "\n", "help", "=", "\"Mbeam AE coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"MT coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_bt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"BT coefficient\"", ")", "\n", "# parser.add_argument(\"--lambda_bt_otf\", type=str, default=\"0\",", "\n", "#                     help=\"BT coefficient on the fly separate\")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mass\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"MASS coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_span\"", ",", "type", "=", "str", ",", "default", "=", "\"10000\"", ",", "\n", "help", "=", "\"Span coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bt_sync\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"log_per_iter\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"label_smoothing\"", ")", "\n", "\n", "# training steps", "\n", "parser", ".", "add_argument", "(", "\"--clm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Causal prediction steps (CLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (MLM / TLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (EDMLM / EDTLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Machine translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ae_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Denoising auto-encoder steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Back-translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pc_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Parallel classification steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mass_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"MASS prediction steps\"", ")", "\n", "# parser.add_argument(\"--mbeam_ae\", type=bool_flag, default=False,", "\n", "#                     help=\"stochastic_beam .\")", "\n", "\n", "# logging", "\n", "parser", ".", "add_argument", "(", "\"--log_per_iter\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"log_per_iter\"", ")", "\n", "\n", "# reload pretrained embeddings / pretrained model / checkpoint", "\n", "parser", ".", "add_argument", "(", "\"--reload_emb\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload pretrained word embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a pretrained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_checkpoint\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a checkpoint\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--reload_2nd_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a secondary parallel model. X ->(m1) Y1 =>(m2) X2\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--reload_model_xlm\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a pretrained parallel model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_model_mass\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a  \"", ")", "\n", "\n", "\n", "# beam search (for MT only)", "\n", "parser", ".", "add_argument", "(", "\"--beam_size\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size, default = 1 (greedy decoding)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Length penalty, values < 1.0 favor shorter sentences, while values > 1.0 favor longer ones.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--early_stopping\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Early stopping, stop as soon as we have `beam_size` hypotheses, although longer ones may have better scores.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--nbest\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Beam size, default = 1 (greedy decoding)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sampling_topp\"", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"sampling_topp.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--sec_bt_epoch\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"sec_bt_epoch for secondary run of BT using different decoding method\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sec_bt_iter\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"sec_bt_iter for secondary run of BT using different decoding method\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--select_opt\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"select_opt\"", ")", "\n", "\n", "\n", "# evaluation", "\n", "parser", ".", "add_argument", "(", "\"--eval_bleu\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Evaluate BLEU score during MT training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Only run evaluations\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Infer training data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fast_beam\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"fast_beam\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fast_beam_epoch\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"fast_beam_epoch\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mbeam_size\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"mbeam_size\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_per_steps\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"eval_per_steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--filter_bleu\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"filter_bleu\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--report_filter_bleu\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"report_filter_bleu\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--src_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"src\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"src\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--xlmmass_reverse\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"xlmmass_reverse\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_single_model\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"infer_single_model ensemble\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_single_mass\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"infer_single_mass ensemble\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--macd_version\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"macd_version\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--arch\"", ",", "type", "=", "str", ",", "default", "=", "\"mlm\"", ",", "\n", "help", "=", "\"architecture, in (mlm,mass)\"", ")", "\n", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use valid sets for train sets (faster loading)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug multi-GPU / multi-node within a SLURM job\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "help", "=", "\"Enable all debug flags\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "\n", "# multi-gpu / multi-node", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--master_port\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Master port (for multi-node SLURM jobs)\"", ")", "\n", "\n", "# seed", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"If >= 0, set the seed\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_online_cbd.main": [[322, 442], ["src.slurm.init_distributed_mode", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.loader.load_data", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.set_sampling_probs", "range", "ValueError", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "build_func1", "build_func2", "build_func", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer", "src.evaluation.evaluator.MACDOnlineEvaluator", "src.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.utils.initialize_exp.info", "exit", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.save_best_model", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.save_periodic", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.end_epoch", "ValueError", "src.utils.initialize_exp.info", "src.utils.shuf_order", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.iter", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "json.dumps", "bool", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.bt_step_macd", "src.utils.initialize_exp.info", "src.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.save_best_model", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.save_periodic", "src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.end_epoch", "numpy.random.randint", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "json.dumps", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_distributed_mode", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.initialize_exp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.set_sampling_probs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_best_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_periodic", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.end_epoch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.iter", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.bt_step_macd", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_best_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_periodic", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.end_epoch"], ["", "def", "main", "(", "params", ")", ":", "\n", "\n", "    ", "if", "params", ".", "arch", "==", "\"mlm\"", ":", "\n", "        ", "build_model_fn", "=", "build_model", "\n", "build_model_multilang_fn", "=", "build_model_multilang", "\n", "", "elif", "params", ".", "arch", "==", "\"mass\"", ":", "\n", "# remove AE training", "\n", "        ", "build_model_fn", "=", "mass_build_model", "\n", "build_model_multilang_fn", "=", "None", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'arch parameters wrong: {}'", ".", "format", "(", "params", ".", "arch", ")", ")", "\n", "\n", "# initialize the multi-GPU / multi-node training", "\n", "", "init_distributed_mode", "(", "params", ")", "\n", "\n", "if", "params", ".", "infer_train", ":", "\n", "        ", "log_filename", "=", "'infer.train.log'", "\n", "params_filename", "=", "'infer.train.params.pkl'", "\n", "", "else", ":", "\n", "        ", "log_filename", ",", "params_filename", "=", "None", ",", "None", "\n", "\n", "# initialize the experiment", "\n", "", "logger", "=", "initialize_exp", "(", "params", ",", "log_filename", ",", "params_filename", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "# load data", "\n", "data", "=", "load_data", "(", "params", ")", "\n", "logger", ".", "info", "(", "'INIT MODEL HERE: {} -> {}'", ".", "format", "(", "params", ".", "src_lang", ",", "params", ".", "tgt_lang", ")", ")", "\n", "logger", ".", "info", "(", "data", ")", "\n", "\n", "assert", "params", ".", "src_lang", "in", "params", ".", "langs", "\n", "assert", "params", ".", "tgt_lang", "in", "params", ".", "langs", "\n", "\n", "# build model", "\n", "if", "params", ".", "encoder_only", ":", "\n", "# model = build_model(params, data['dico'])", "\n", "        ", "raise", "ValueError", "(", "'encoder_only invalid for cbd'", ")", "\n", "# else:", "\n", "", "try", ":", "\n", "        ", "build_func", "=", "build_model_multilang_fn", "if", "(", "params", ".", "share_enc", ">", "-", "1", "or", "params", ".", "share_dec", ">", "-", "1", ")", "else", "build_model_fn", "\n", "build_func1", "=", "build_model", "\n", "build_func2", "=", "mass_build_model", "\n", "logger", ".", "info", "(", "'Build function 1: {}'", ".", "format", "(", "build_func1", ".", "__name__", ")", ")", "\n", "logger", ".", "info", "(", "'Build function 2: {}'", ".", "format", "(", "build_func2", ".", "__name__", ")", ")", "\n", "encoder1", ",", "decoder1", "=", "build_func1", "(", "params", ",", "data", "[", "'dico'", "]", ",", "checkpoint", "=", "params", ".", "reload_model_xlm", ")", "\n", "encoder2", ",", "decoder2", "=", "build_func2", "(", "params", ",", "data", "[", "'dico'", "]", ",", "checkpoint", "=", "params", ".", "reload_model_mass", ")", "\n", "encoder", ",", "decoder", "=", "build_func", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "raise", "e", "\n", "\n", "# build trainer, reload potential checkpoints / build evaluator", "\n", "", "if", "params", ".", "encoder_only", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "        ", "trainer", "=", "EncDecMACDmbeamOnlineTrainer", "(", "encoder", ",", "decoder", ",", "encoder1", ",", "decoder1", ",", "encoder2", ",", "decoder2", ",", "data", ",", "params", ")", "\n", "evaluator", "=", "MACDOnlineEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "\n", "# evaluation", "\n", "", "if", "params", ".", "eval_only", ":", "\n", "        ", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "# set sampling probabilities for training", "\n", "", "set_sampling_probs", "(", "data", ",", "params", ")", "\n", "assert", "params", ".", "beam_size", ">", "1", "\n", "# nbest = params.nbest", "\n", "for", "_", "in", "range", "(", "params", ".", "max_epoch", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "trainer", ".", "n_sentences", "=", "0", "\n", "\n", "while", "trainer", ".", "n_sentences", "<", "trainer", ".", "epoch_size", ":", "\n", "\n", "# MACD back-translation steps", "\n", "            ", "for", "lang1", ",", "lang2", ",", "lang3", "in", "shuf_order", "(", "params", ".", "bt_steps", ")", ":", "\n", "                ", "direction1", "=", "(", "lang1", "==", "params", ".", "src_lang", ")", "\n", "# if nbest is None or nbest <= 1:", "\n", "first_sec", "=", "bool", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "2", ")", ")", "\n", "trainer", ".", "bt_step_macd", "(", "\n", "lang1", ",", "lang2", ",", "lang3", ",", "params", ".", "lambda_bt", ",", "direction1", ",", "first_sec", "=", "first_sec", ",", "and_rev", "=", "True", ")", "\n", "", "trainer", ".", "iter", "(", ")", "\n", "\n", "if", "params", ".", "eval_per_steps", ">", "0", "and", "trainer", ".", "n_total_iter", "%", "params", ".", "eval_per_steps", "==", "0", "and", "trainer", ".", "n_total_iter", ">", "0", ":", "\n", "# eval mid", "\n", "# evaluate perplexity", "\n", "                ", "logger", ".", "info", "(", "'Start evaluate at step {}'", ".", "format", "(", "trainer", ".", "n_total_iter", ")", ")", "\n", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "\n", "# print / JSON log", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "if", "params", ".", "is_master", ":", "\n", "                    ", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "trainer", ".", "save_best_model", "(", "scores", ")", "\n", "trainer", ".", "save_periodic", "(", ")", "\n", "trainer", ".", "end_epoch", "(", "scores", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"============ End of epoch %i ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "# evaluate perplexity", "\n", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "\n", "# print / JSON log", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "if", "params", ".", "is_master", ":", "\n", "            ", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "trainer", ".", "save_best_model", "(", "scores", ")", "\n", "trainer", ".", "save_periodic", "(", ")", "\n", "trainer", ".", "end_epoch", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.default_avg_params": [[11, 22], ["collections.OrderedDict", "params_dict.items", "len"], "function", ["None"], ["def", "default_avg_params", "(", "params_dict", ")", ":", "\n", "    ", "averaged_params", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "# v should be a list of torch Tensor.", "\n", "for", "k", ",", "v", "in", "params_dict", ".", "items", "(", ")", ":", "\n", "        ", "summed_v", "=", "None", "\n", "for", "x", "in", "v", ":", "\n", "            ", "summed_v", "=", "summed_v", "+", "x", "if", "summed_v", "is", "not", "None", "else", "x", "\n", "", "averaged_params", "[", "k", "]", "=", "summed_v", "/", "len", "(", "v", ")", "\n", "\n", "", "return", "averaged_params", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.ema_avg_params": [[24, 44], ["collections.OrderedDict", "all", "params_dict.items", "len", "reversed", "params_dict.items"], "function", ["None"], ["", "def", "ema_avg_params", "(", "params_dict", ",", "ema_decay", ")", ":", "\n", "    ", "averaged_params", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "lens", "=", "[", "len", "(", "v", ")", "for", "k", ",", "v", "in", "params_dict", ".", "items", "(", ")", "]", "\n", "assert", "all", "(", "x", "==", "lens", "[", "0", "]", "for", "x", "in", "lens", ")", ",", "f'lens params: {lens}'", "\n", "num_checkpoints", "=", "lens", "[", "0", "]", "\n", "# y = x", "\n", "\n", "for", "k", ",", "v", "in", "params_dict", ".", "items", "(", ")", ":", "\n", "# order: newest to oldest", "\n", "# reverse the order", "\n", "# y_t = x_t * decay + y_{t-1} * (1 - decay)", "\n", "        ", "total_v", "=", "None", "\n", "for", "x", "in", "reversed", "(", "v", ")", ":", "\n", "            ", "if", "total_v", "is", "None", ":", "\n", "                ", "total_v", "=", "x", "\n", "", "else", ":", "\n", "                ", "total_v", "=", "x", "*", "ema_decay", "+", "total_v", "*", "(", "1.0", "-", "ema_decay", ")", "\n", "\n", "", "", "averaged_params", "[", "k", "]", "=", "total_v", "\n", "", "return", "averaged_params", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.average_checkpoints": [[46, 98], ["collections.OrderedDict", "enumerate", "torch.load", "list", "print", "average_checkpoints.ema_avg_params", "print", "average_checkpoints.default_avg_params", "model_params.keys", "isinstance", "params_dict[].append", "KeyError", "p.float.float", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.ema_avg_params", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.default_avg_params"], ["", "def", "average_checkpoints", "(", "inputs", ",", "ema_decay", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"Loads checkpoints from inputs and returns a model with averaged weights.\n\n    Args:\n      inputs: An iterable of string paths of checkpoints to load from.\n\n    Returns:\n      A dict of string keys mapping to various values. The 'model' key\n      from the returned dict should correspond to an OrderedDict mapping\n      string parameter names to torch Tensors.\n    \"\"\"", "\n", "params_dict", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "params_keys", "=", "None", "\n", "new_state", "=", "None", "\n", "for", "i", ",", "f", "in", "enumerate", "(", "inputs", ")", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "\n", "f", ",", "\n", "map_location", "=", "(", "\n", "lambda", "s", ",", "_", ":", "torch", ".", "serialization", ".", "default_restore_location", "(", "s", ",", "'cpu'", ")", "\n", ")", ",", "\n", ")", "\n", "# Copies over the settings from the first checkpoint", "\n", "if", "new_state", "is", "None", ":", "\n", "            ", "new_state", "=", "state", "\n", "\n", "", "model_params", "=", "state", "[", "'model'", "]", "\n", "\n", "model_params_keys", "=", "list", "(", "model_params", ".", "keys", "(", ")", ")", "\n", "if", "params_keys", "is", "None", ":", "\n", "            ", "params_keys", "=", "model_params_keys", "\n", "", "elif", "params_keys", "!=", "model_params_keys", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "'For checkpoint {}, expected list of params: {}, '", "\n", "'but found: {}'", ".", "format", "(", "f", ",", "params_keys", ",", "model_params_keys", ")", "\n", ")", "\n", "\n", "", "for", "k", "in", "params_keys", ":", "\n", "            ", "if", "k", "not", "in", "params_dict", ":", "\n", "                ", "params_dict", "[", "k", "]", "=", "[", "]", "\n", "", "p", "=", "model_params", "[", "k", "]", "\n", "if", "isinstance", "(", "p", ",", "torch", ".", "HalfTensor", ")", ":", "\n", "                ", "p", "=", "p", ".", "float", "(", ")", "\n", "", "params_dict", "[", "k", "]", ".", "append", "(", "p", ")", "\n", "\n", "", "", "if", "ema_decay", "<", "1.0", ":", "\n", "        ", "print", "(", "f'Exponential moving averaging, decay={ema_decay}'", ")", "\n", "averaged_params", "=", "ema_avg_params", "(", "params_dict", ",", "ema_decay", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f'Default averaging'", ")", "\n", "averaged_params", "=", "default_avg_params", "(", "params_dict", ")", "\n", "", "new_state", "[", "'model'", "]", "=", "averaged_params", "\n", "return", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.last_n_checkpoints": [[100, 119], ["os.listdir", "len", "re.compile", "re.compile", "re.compile.fullmatch", "len", "Exception", "os.path.join", "int", "len", "pt_regexp.fullmatch.group", "entries.append", "sorted", "pt_regexp.fullmatch.group"], "function", ["None"], ["", "def", "last_n_checkpoints", "(", "paths", ",", "n", ",", "update_based", ",", "upper_bound", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "paths", ")", "==", "1", "\n", "path", "=", "paths", "[", "0", "]", "\n", "if", "update_based", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint_\\d+_(\\d+)\\.pt'", ")", "\n", "", "else", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint(\\d+)\\.pt'", ")", "\n", "", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "\n", "entries", "=", "[", "]", "\n", "for", "f", "in", "files", ":", "\n", "        ", "m", "=", "pt_regexp", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "sort_key", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "if", "upper_bound", "is", "None", "or", "sort_key", "<=", "upper_bound", ":", "\n", "                ", "entries", ".", "append", "(", "(", "sort_key", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "", "", "if", "len", "(", "entries", ")", "<", "n", ":", "\n", "        ", "raise", "Exception", "(", "'Found {} checkpoint files but need at least {}'", ",", "len", "(", "entries", ")", ",", "n", ")", "\n", "", "return", "[", "os", ".", "path", ".", "join", "(", "path", ",", "x", "[", "1", "]", ")", "for", "x", "in", "sorted", "(", "entries", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.main": [[121, 184], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "fairseq.utils.import_user_module", "print", "print", "average_checkpoints.average_checkpoints", "torch.save", "print", "average_checkpoints.last_n_checkpoints", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.average_checkpoints", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.average_checkpoints.last_n_checkpoints"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Tool to average the params of input checkpoints to '", "\n", "'produce a new checkpoint'", ",", "\n", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--inputs'", ",", "required", "=", "True", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'Input checkpoint file paths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "required", "=", "True", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'Write the new checkpoint containing the averaged weights to this path.'", ")", "\n", "num_group", "=", "parser", ".", "add_mutually_exclusive_group", "(", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-epoch-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-update-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint-upper-bound'", ",", "type", "=", "int", ",", "\n", "help", "=", "'when using --num-epoch-checkpoints, this will set an upper bound on which checkpoint to use, '", "\n", "'e.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.'", ")", "\n", "\n", "# parser.add_argument('--ema', type=float, default=1.0, help='exponential moving average decay')", "\n", "# parser.add_argument('--no-progress-bar', action='store_true', help='disable progress bar')", "\n", "# --checkpoint-upper-bound 10 --num-epoch-checkpoints 5", "\n", "parser", ".", "add_argument", "(", "'--ema'", ",", "default", "=", "'False'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "help", "=", "'ema'", ")", "\n", "parser", ".", "add_argument", "(", "'--ema_decay'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'exponential moving average decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--user-dir'", ",", "default", "=", "None", ")", "\n", "\n", "# fmt: on", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "import_user_module", "(", "args", ")", "\n", "print", "(", "args", ")", "\n", "\n", "num", "=", "None", "\n", "is_update_based", "=", "False", "\n", "if", "args", ".", "num_update_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_update_checkpoints", "\n", "is_update_based", "=", "True", "\n", "", "elif", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_epoch_checkpoints", "\n", "\n", "", "assert", "args", ".", "checkpoint_upper_bound", "is", "None", "or", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ",", "'--checkpoint-upper-bound requires --num-epoch-checkpoints'", "\n", "assert", "args", ".", "num_epoch_checkpoints", "is", "None", "or", "args", ".", "num_update_checkpoints", "is", "None", ",", "'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'", "\n", "\n", "if", "num", "is", "not", "None", ":", "\n", "        ", "args", ".", "inputs", "=", "last_n_checkpoints", "(", "\n", "args", ".", "inputs", ",", "num", ",", "is_update_based", ",", "upper_bound", "=", "args", ".", "checkpoint_upper_bound", ",", "\n", ")", "\n", "# print('averaging checkpoints: ', args.inputs)", "\n", "print", "(", "'averaging checkpoints: '", ")", "\n", "for", "checkpoint", "in", "args", ".", "inputs", ":", "\n", "            ", "print", "(", "checkpoint", ")", "\n", "", "print", "(", "'-'", "*", "40", ")", "\n", "\n", "# ema = args.ema", "\n", "# assert isinstance(args.ema, bool)", "\n", "", "print", "(", "f'Start averaing with ema={args.ema}, ema_decay={args.ema_decay}'", ")", "\n", "new_state", "=", "average_checkpoints", "(", "args", ".", "inputs", ",", "args", ".", "ema_decay", ")", "\n", "torch", ".", "save", "(", "new_state", ",", "args", ".", "output", ")", "\n", "print", "(", "'Finished writing averaged checkpoint to {}.'", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.get_parser": [[40, 66], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Remove noisy data\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The path of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--target\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The path of target file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--lang\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The language of input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lang_target\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The language of input file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The path of output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_target\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The path of output file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--discard\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The path of discard file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--discard_target\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"The path of discard file\"", ")", "\n", "\n", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_exist_url": [[68, 74], ["re.findall", "re.findall", "re.findall", "re.findall", "len", "len", "len", "len"], "function", ["None"], ["", "def", "detect_exist_url", "(", "text", ")", ":", "\n", "    ", "urls", "=", "re", ".", "findall", "(", "'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'", ",", "text", ")", "\n", "url1", "=", "re", ".", "findall", "(", "'http[s]?//(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'", ",", "text", ")", "\n", "url2", "=", "re", ".", "findall", "(", "'http[s]? : / / (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'", ",", "text", ")", "\n", "url3", "=", "re", ".", "findall", "(", "'http[s]? / / (?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'", ",", "text", ")", "\n", "return", "len", "(", "urls", ")", ">", "0", "or", "len", "(", "url1", ")", ">", "0", "or", "len", "(", "url2", ")", ">", "0", "or", "len", "(", "url3", ")", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_lang": [[76, 86], ["enumerate", "detect", "Detector"], "function", ["None"], ["", "def", "detect_lang", "(", "text", ",", "lang", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "for", "i", ",", "l", "in", "enumerate", "(", "Detector", "(", "text", ",", "quiet", "=", "True", ")", ".", "languages", ")", ":", "\n", "            ", "if", "l", ".", "code", "==", "lang", "and", "i", "==", "0", ":", "\n", "                ", "return", "True", "\n", "", "", "if", "detect", "(", "text", ")", "==", "lang", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "", "except", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.run_single": [[88, 118], ["print", "open", "open", "open", "line.strip.strip", "mass_filter_noisy_data.detect_exist_url", "print", "mass_filter_noisy_data.detect_lang", "open.write", "open.write", "open.write"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_exist_url", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_lang"], ["", "", "def", "run_single", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "output", "is", "not", "None", ":", "\n", "        ", "f", "=", "open", "(", "args", ".", "output", ",", "'w'", ")", "\n", "", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "        ", "dis", "=", "open", "(", "args", ".", "discard", ",", "'w'", ")", "\n", "\n", "", "count", "=", "0", "\n", "discard", "=", "0", "\n", "allcount", "=", "0", "\n", "with", "open", "(", "args", ".", "input", ",", "encoding", "=", "'utf-8'", ")", "as", "input_file", ":", "\n", "        ", "for", "line", "in", "input_file", ":", "\n", "            ", "allcount", "+=", "1", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "detect_exist_url", "(", "line", ")", "is", "False", ":", "\n", "                ", "if", "detect_lang", "(", "line", ",", "args", ".", "lang", ")", "is", "True", ":", "\n", "                    ", "count", "+=", "1", "\n", "if", "args", ".", "output", "is", "not", "None", ":", "\n", "                        ", "f", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "", "", "else", ":", "\n", "                    ", "discard", "+=", "1", "\n", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "                        ", "dis", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "", "", "", "else", ":", "\n", "                ", "discard", "+=", "1", "\n", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "                    ", "dis", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "# print(line)", "\n", "", "", "if", "allcount", "%", "100000", "==", "0", ":", "\n", "                ", "print", "(", "\"{} sentences processed, count: {}, discard: {}\"", ".", "format", "(", "allcount", ",", "count", ",", "discard", ")", ")", "\n", "", "", "", "print", "(", "count", ",", "allcount", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.run_parallel": [[120, 162], ["print", "open", "open", "open", "open", "open", "open", "zip", "srcline.strip.strip", "tgtline.strip.strip", "print", "mass_filter_noisy_data.detect_exist_url", "mass_filter_noisy_data.detect_exist_url", "mass_filter_noisy_data.detect_lang", "mass_filter_noisy_data.detect_lang", "open.write", "open.write", "open.write", "open.write", "open.write", "open.write"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_exist_url", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_exist_url", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_lang", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.detect_lang"], ["", "def", "run_parallel", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "output", "is", "not", "None", ":", "\n", "        ", "f", "=", "open", "(", "args", ".", "output", ",", "'w'", ")", "\n", "", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "        ", "dis", "=", "open", "(", "args", ".", "discard", ",", "'w'", ")", "\n", "", "if", "args", ".", "output_target", "is", "not", "None", ":", "\n", "        ", "f_target", "=", "open", "(", "args", ".", "output_target", ",", "'w'", ")", "\n", "", "if", "args", ".", "discard_target", "is", "not", "None", ":", "\n", "        ", "dis_target", "=", "open", "(", "args", ".", "discard_target", ",", "'w'", ")", "\n", "\n", "", "count", "=", "0", "\n", "discard", "=", "0", "\n", "allcount", "=", "0", "\n", "with", "open", "(", "args", ".", "input", ",", "encoding", "=", "'utf-8'", ")", "as", "input_file", ":", "\n", "        ", "with", "open", "(", "args", ".", "target", ",", "encoding", "=", "'utf-8'", ")", "as", "target_file", ":", "\n", "            ", "for", "srcline", ",", "tgtline", "in", "zip", "(", "input_file", ",", "target_file", ")", ":", "\n", "                ", "allcount", "+=", "1", "\n", "srcline", "=", "srcline", ".", "strip", "(", ")", "\n", "tgtline", "=", "tgtline", ".", "strip", "(", ")", "\n", "if", "not", "detect_exist_url", "(", "srcline", ")", "and", "not", "detect_exist_url", "(", "tgtline", ")", ":", "\n", "                    ", "if", "detect_lang", "(", "srcline", ",", "args", ".", "lang", ")", "and", "detect_lang", "(", "tgtline", ",", "args", ".", "lang_target", ")", ":", "\n", "                        ", "count", "+=", "1", "\n", "if", "args", ".", "output", "is", "not", "None", ":", "\n", "                            ", "f", ".", "write", "(", "srcline", "+", "'\\n'", ")", "\n", "", "if", "args", ".", "output_target", "is", "not", "None", ":", "\n", "                            ", "f_target", ".", "write", "(", "tgtline", "+", "'\\n'", ")", "\n", "", "", "else", ":", "\n", "                        ", "discard", "+=", "1", "\n", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "                            ", "dis", ".", "write", "(", "srcline", "+", "'\\n'", ")", "\n", "", "if", "args", ".", "discard_target", "is", "not", "None", ":", "\n", "                            ", "dis_target", ".", "write", "(", "tgtline", "+", "'\\n'", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "discard", "+=", "1", "\n", "if", "args", ".", "discard", "is", "not", "None", ":", "\n", "                        ", "dis", ".", "write", "(", "srcline", "+", "'\\n'", ")", "\n", "", "if", "args", ".", "discard_target", "is", "not", "None", ":", "\n", "                        ", "dis_target", ".", "write", "(", "tgtline", "+", "'\\n'", ")", "\n", "# print(line)", "\n", "", "", "if", "allcount", "%", "100000", "==", "0", ":", "\n", "                    ", "print", "(", "\"{} sentences processed, count: {}, discard: {}\"", ".", "format", "(", "allcount", ",", "count", ",", "discard", ")", ")", "\n", "", "", "", "", "print", "(", "count", ",", "allcount", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.main": [[165, 188], ["mass_filter_noisy_data.get_parser", "get_parser.parse_args", "print", "print", "print", "os.path.exists", "print", "print", "print", "mass_filter_noisy_data.run_parallel", "mass_filter_noisy_data.run_single"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_generate_macd.get_parser", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.run_parallel", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.mass_filter_noisy_data.run_single"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "f", "=", "None", "\n", "dis", "=", "None", "\n", "f_target", "=", "None", "\n", "dis_target", "=", "None", "\n", "parallel", "=", "False", "\n", "if", "args", ".", "target", "is", "not", "None", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "target", ")", ":", "\n", "        ", "print", "(", "'Require target....'", ")", "\n", "assert", "args", ".", "lang_target", "is", "not", "None", "\n", "assert", "args", ".", "output_target", "is", "not", "None", "\n", "parallel", "=", "True", "\n", "\n", "", "print", "(", "'Filtering data'", ")", "\n", "print", "(", "'Write to: {}'", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "print", "(", "'Discard to: {}'", ".", "format", "(", "args", ".", "discard", ")", ")", "\n", "if", "parallel", ":", "\n", "        ", "print", "(", "'Write to target: {}'", ".", "format", "(", "args", ".", "output_target", ")", ")", "\n", "print", "(", "'Discard to target: {}'", ".", "format", "(", "args", ".", "discard_target", ")", ")", "\n", "run_parallel", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "run_single", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_generate_macd.get_parser": [[26, 254], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "src.model.memory.HashingMemory.register_args", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "argparse.ArgumentParser.parse_known_args"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.register_args"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Language transfer\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\"./dumped/\"", ",", "\n", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_name\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment name\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_periodic\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Save the model periodically (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--exp_id\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Experiment ID\"", ")", "\n", "\n", "# float16 / AMP API", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Run model with float16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--amp\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Use AMP wrapper for float16 / distributed / gradient accumulation. Level of optimization. -1 to disable.\"", ")", "\n", "\n", "# only use an encoder (use a specific decoder for machine translation)", "\n", "parser", ".", "add_argument", "(", "\"--encoder_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Only use an encoder\"", ")", "\n", "\n", "# model parameters", "\n", "parser", ".", "add_argument", "(", "\"--emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Embedding layer size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_layers\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_heads\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"Number of Transformer heads\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Dropout in the attention layer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gelu_activation\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use a GELU activation instead of ReLU\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_inout_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Share input and output embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sinusoidal_embeddings\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use sinusoidal embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_lang_emb\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Use language embedding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_enc\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_dec\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of Transformer layers\"", ")", "\n", "\n", "# memory parameters", "\n", "parser", ".", "add_argument", "(", "\"--use_memory\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use an external memory\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "use_memory", ":", "\n", "        ", "HashingMemory", ".", "register_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_enc_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the encoder ('4' for inside layer 4, '7,10+' for inside layer 7 and after layer 10)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_dec_positions\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Memory positions in the decoder. Same syntax as `mem_enc_positions`.\"", ")", "\n", "\n", "# adaptive softmax", "\n", "", "parser", ".", "add_argument", "(", "\"--asm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use adaptive softmax\"", ")", "\n", "if", "parser", ".", "parse_known_args", "(", ")", "[", "0", "]", ".", "asm", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\"--asm_cutoffs\"", ",", "type", "=", "str", ",", "default", "=", "\"8000,20000\"", ",", "\n", "help", "=", "\"Adaptive softmax cutoffs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--asm_div_value\"", ",", "type", "=", "float", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Adaptive softmax cluster sizes ratio\"", ")", "\n", "\n", "# causal language modeling task parameters", "\n", "", "parser", ".", "add_argument", "(", "\"--context_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Context size (0 means that the first elements in sequences won't have any context)\"", ")", "\n", "\n", "# masked language modeling task parameters", "\n", "parser", ".", "add_argument", "(", "\"--word_pred\"", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "\n", "help", "=", "\"Fraction of words for which we need to make a prediction\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sample_alpha\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Exponent for transforming word counts to probabilities (~word2vec sampling)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_mask_keep_rand\"", ",", "type", "=", "str", ",", "default", "=", "\"0.8,0.1,0.1\"", ",", "\n", "help", "=", "\"Fraction of words to mask out / keep / randomize, among the words to predict\"", ")", "\n", "\n", "# input sentence noise", "\n", "parser", ".", "add_argument", "(", "\"--word_shuffle\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly shuffle input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly dropout input words (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_blank\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Randomly blank input words (0 to disable)\"", ")", "\n", "\n", "# ED-MLM Parameters", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_full\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Full prediction at decoder\"", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--data_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lgs\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Languages (lg1-lg2-lg3 .. ex: en-fr-es-de)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_vocab\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Maximum vocabulary size (-1 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_count\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Minimum vocabulary count\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lg_sampling_factor\"", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Language sampling factor\"", ")", "\n", "\n", "# batch parameters", "\n", "parser", ".", "add_argument", "(", "\"--bptt\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Sequence length\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_len\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "\"Maximum length of sentences (after BPE)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--group_by_size\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Sort sentences by size during the training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Number of sentences per batch\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_batch_size\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokens_per_batch\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Number of tokens per batch\"", ")", "\n", "\n", "# logging", "\n", "parser", ".", "add_argument", "(", "\"--log_per_iter\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"log_per_iter\"", ")", "\n", "\n", "# training parameters", "\n", "parser", ".", "add_argument", "(", "\"--split_data\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Split data across workers of a same node\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"adam,lr=0.0001\"", ",", "\n", "help", "=", "\"Optimizer (SGD / RMSprop / Adam, etc.)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--clip_grad_norm\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "\n", "help", "=", "\"Clip gradients norm (0 to disable)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--epoch_size\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Epoch size / evaluation frequency (-1 for parallel data size)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Maximum epoch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stopping_criterion\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Stopping criterion, and number of non-increase before stopping the experiment\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--validation_metrics\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Validation metrics\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--accumulate_gradients\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Accumulate model gradients over N iterations (N times larger batch sizes)\"", ")", "\n", "\n", "# training coefficients", "\n", "parser", ".", "add_argument", "(", "\"--lambda_edmlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (EDMLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mlm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Prediction coefficient (MLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_clm\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"Causal coefficient (LM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_pc\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"PC coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_ae\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"AE coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_mt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"MT coefficient\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lambda_bt\"", ",", "type", "=", "str", ",", "default", "=", "\"1\"", ",", "\n", "help", "=", "\"BT coefficient\"", ")", "\n", "\n", "# training steps", "\n", "parser", ".", "add_argument", "(", "\"--clm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Causal prediction steps (CLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (MLM / TLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--edmlm_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Masked prediction steps (EDMLM / EDTLM)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Machine translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ae_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Denoising auto-encoder steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bt_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Back-translation steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pc_steps\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Parallel classification steps\"", ")", "\n", "\n", "# reload pretrained embeddings / pretrained model / checkpoint", "\n", "parser", ".", "add_argument", "(", "\"--reload_emb\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload pretrained word embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a pretrained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_checkpoint\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a checkpoint\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--reload_para_model\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Reload a pretrained parallel model\"", ")", "\n", "\n", "# beam search (for MT only)", "\n", "parser", ".", "add_argument", "(", "\"--beam_size\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size, default = 1 (greedy decoding)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Length penalty, values < 1.0 favor shorter sentences, while values > 1.0 favor longer ones.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--early_stopping\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Early stopping, stop as soon as we have `beam_size` hypotheses, although longer ones may have better scores.\"", ")", "\n", "\n", "# evaluation", "\n", "parser", ".", "add_argument", "(", "\"--eval_bleu\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Evaluate BLEU score during MT training\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_only\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Only run evaluations\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Infer training data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_2stage_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Infer training data with 2 stage data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_ens\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Eval ensemble\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--infer_2stage_same_model\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Infer training data with 2 stage data\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_name\"", ",", "type", "=", "str", ",", "default", "=", "\"bt2infer\"", ",", "\n", "help", "=", "\"Infer out name, only for bt2infer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--order_descending\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Order descending\"", ")", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug_train\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use valid sets for train sets (faster loading)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug multi-GPU / multi-node within a SLURM job\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "help", "=", "\"Enable all debug flags\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "\n", "# multi-gpu / multi-node", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--master_port\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Master port (for multi-node SLURM jobs)\"", ")", "\n", "\n", "# seed", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"If >= 0, set the seed\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.None.train_generate_macd.main": [[255, 388], ["src.slurm.init_distributed_mode", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.loader.load_data", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.set_sampling_probs", "range", "NotImplementedError", "src.model.build_model", "src.trainer.SingleTrainer", "src.evaluation.evaluator.SingleEvaluator", "src.trainer.EncDecParaPretrainedTrainer", "src.evaluation.evaluator.EncDecEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.utils.initialize_exp.info", "exit", "print", "src.utils.initialize_exp.info", "isinstance", "src.evaluation.evaluator.EncDecEvaluator.infer_train", "evaluator.run_all_evals.items", "src.utils.initialize_exp.info", "exit", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.evaluation.evaluator.EncDecEvaluator.run_all_evals", "evaluator.run_all_evals.items", "src.trainer.EncDecParaPretrainedTrainer.save_best_model", "src.trainer.EncDecParaPretrainedTrainer.save_periodic", "src.trainer.EncDecParaPretrainedTrainer.end_epoch", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "build_func", "src.evaluation.evaluator.EncDecParaPretrainedEvaluator", "src.evaluation.evaluator.EncDecEvaluator", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.utils.shuf_order", "src.trainer.EncDecParaPretrainedTrainer.iter", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "src.utils.initialize_exp.info", "build_para_func", "json.dumps", "json.dumps", "src.trainer.EncDecParaPretrainedTrainer.clm_step", "src.trainer.EncDecParaPretrainedTrainer.mlm_step", "src.trainer.EncDecParaPretrainedTrainer.pc_step", "src.trainer.EncDecParaPretrainedTrainer.mt_step", "src.trainer.EncDecParaPretrainedTrainer.para_bt_step", "src.trainer.EncDecParaPretrainedTrainer.bt_step", "json.dumps"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_distributed_mode", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.initialize_exp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.set_sampling_probs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_best_model", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_periodic", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.end_epoch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.iter", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.clm_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.mlm_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.pc_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mt_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer.para_bt_step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.bt_step"], ["", "def", "main", "(", "params", ")", ":", "\n", "\n", "# initialize the multi-GPU / multi-node training", "\n", "    ", "init_distributed_mode", "(", "params", ")", "\n", "\n", "if", "params", ".", "infer_train", ":", "\n", "        ", "log_filename", "=", "'infer.train.log'", "\n", "params_filename", "=", "'infer.train.params.pkl'", "\n", "", "else", ":", "\n", "        ", "log_filename", ",", "params_filename", "=", "None", ",", "None", "\n", "\n", "# initialize the experiment", "\n", "", "logger", "=", "initialize_exp", "(", "params", ",", "log_filename", ",", "params_filename", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "# load data", "\n", "data", "=", "load_data", "(", "params", ")", "\n", "logger", ".", "info", "(", "'INIT MODEL HERE'", ")", "\n", "logger", ".", "info", "(", "data", ")", "\n", "\n", "# build model", "\n", "if", "params", ".", "encoder_only", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'encoder_only must be false'", ")", "\n", "model", "=", "build_model", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "if", "params", ".", "share_enc", ">", "-", "1", "or", "params", ".", "share_dec", ">", "-", "1", ":", "\n", "                ", "build_func", "=", "build_model_multilang", "\n", "build_para_func", "=", "build_model_multilang_parallel", "\n", "", "else", ":", "\n", "                ", "build_func", "=", "build_model", "\n", "build_para_func", "=", "build_model_pretrained_parallel", "\n", "", "logger", ".", "info", "(", "'Build function: {}'", ".", "format", "(", "build_func", ".", "__name__", ")", ")", "\n", "logger", ".", "info", "(", "'Build para function: {}'", ".", "format", "(", "build_para_func", ".", "__name__", ")", ")", "\n", "encoder", ",", "decoder", "=", "build_func", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "if", "params", ".", "infer_2stage_same_model", ":", "\n", "                ", "logger", ".", "info", "(", "'Infer with SAME models'", ")", "\n", "encoder_para", ",", "decoder_para", "=", "encoder", ",", "decoder", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "'Infer with DIFFERENT models'", ")", "\n", "encoder_para", ",", "decoder_para", "=", "build_para_func", "(", "params", ",", "data", "[", "'dico'", "]", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "# print(data)", "\n", "            ", "raise", "e", "\n", "\n", "# build trainer, reload potential checkpoints / build evaluator", "\n", "", "", "if", "params", ".", "encoder_only", ":", "\n", "        ", "trainer", "=", "SingleTrainer", "(", "model", ",", "data", ",", "params", ")", "\n", "evaluator", "=", "SingleEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "", "else", ":", "\n", "# trainer = EncDecTrainer(encoder, decoder, data, params)", "\n", "        ", "trainer", "=", "EncDecParaPretrainedTrainer", "(", "encoder", ",", "decoder", ",", "encoder_para", ",", "decoder_para", ",", "data", ",", "params", ")", "\n", "if", "params", ".", "infer_2stage_train", ":", "\n", "            ", "evaluator", "=", "EncDecParaPretrainedEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "", "else", ":", "\n", "            ", "evaluator", "=", "EncDecEvaluator", "(", "trainer", ",", "data", ",", "params", ")", "\n", "\n", "# evaluation", "\n", "", "", "if", "params", ".", "eval_only", ":", "\n", "        ", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "if", "params", ".", "infer_train", ":", "\n", "        ", "print", "(", "'**** Infer Train'", ")", "\n", "logger", ".", "info", "(", "'======== Start Generating ==========='", ")", "\n", "assert", "isinstance", "(", "evaluator", ",", "EncDecEvaluator", ")", "\n", "scores", "=", "evaluator", ".", "infer_train", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "# set sampling probabilities for training", "\n", "", "set_sampling_probs", "(", "data", ",", "params", ")", "\n", "\n", "# language model training", "\n", "for", "_", "in", "range", "(", "params", ".", "max_epoch", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "trainer", ".", "n_sentences", "=", "0", "\n", "\n", "while", "trainer", ".", "n_sentences", "<", "trainer", ".", "epoch_size", ":", "\n", "\n", "# CLM steps", "\n", "            ", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "clm_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "clm_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_clm", ")", "\n", "\n", "# MLM steps (also includes TLM if lang2 is not None)", "\n", "", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "mlm_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "mlm_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_mlm", ")", "\n", "\n", "# parallel classification steps", "\n", "", "for", "lang1", ",", "lang2", "in", "shuf_order", "(", "params", ".", "pc_steps", ",", "params", ")", ":", "\n", "                ", "trainer", ".", "pc_step", "(", "lang1", ",", "lang2", ",", "params", ".", "lambda_pc", ")", "\n", "\n", "# denoising auto-encoder steps", "\n", "", "for", "lang", "in", "shuf_order", "(", "params", ".", "ae_steps", ")", ":", "\n", "                ", "trainer", ".", "mt_step", "(", "lang", ",", "lang", ",", "params", ".", "lambda_ae", ")", "\n", "\n", "# machine translation steps", "\n", "# FIXME: MT step is converted into 2-stage BT with pretrain model, lambda_mt is for BT", "\n", "# for lang1, lang2 in shuf_order(params.mt_steps, params):", "\n", "", "for", "lang1", ",", "lang2", ",", "lang3", "in", "shuf_order", "(", "params", ".", "bt_steps", ")", ":", "\n", "# trainer.para_bt_step(lang1, lang2, params.lambda_mt)", "\n", "                ", "trainer", ".", "para_bt_step", "(", "lang1", ",", "lang2", ",", "lang3", ",", "params", ".", "lambda_mt", ")", "\n", "\n", "# back-translation steps", "\n", "", "for", "lang1", ",", "lang2", ",", "lang3", "in", "shuf_order", "(", "params", ".", "bt_steps", ")", ":", "\n", "                ", "trainer", ".", "bt_step", "(", "lang1", ",", "lang2", ",", "lang3", ",", "params", ".", "lambda_bt", ")", "\n", "\n", "", "trainer", ".", "iter", "(", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"============ End of epoch %i ============\"", "%", "trainer", ".", "epoch", ")", "\n", "\n", "# evaluate perplexity", "\n", "scores", "=", "evaluator", ".", "run_all_evals", "(", "trainer", ")", "\n", "\n", "# print / JSON log", "\n", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s -> %.6f\"", "%", "(", "k", ",", "v", ")", ")", "\n", "", "if", "params", ".", "is_master", ":", "\n", "            ", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "trainer", ".", "save_best_model", "(", "scores", ")", "\n", "trainer", ".", "save_periodic", "(", ")", "\n", "trainer", ".", "end_epoch", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.sig_handler": [[20, 30], ["logger.warning", "int", "logger.warning", "sys.exit", "logger.warning", "os.system", "logger.warning", "str", "socket.gethostname"], "function", ["None"], ["def", "sig_handler", "(", "signum", ",", "frame", ")", ":", "\n", "    ", "logger", ".", "warning", "(", "\"Signal handler called with signal \"", "+", "str", "(", "signum", ")", ")", "\n", "prod_id", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "logger", ".", "warning", "(", "\"Host: %s - Global rank: %i\"", "%", "(", "socket", ".", "gethostname", "(", ")", ",", "prod_id", ")", ")", "\n", "if", "prod_id", "==", "0", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Requeuing job \"", "+", "os", ".", "environ", "[", "'SLURM_JOB_ID'", "]", ")", "\n", "os", ".", "system", "(", "'scontrol requeue '", "+", "os", ".", "environ", "[", "'SLURM_JOB_ID'", "]", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Not the master process, no need to requeue.\"", ")", "\n", "", "sys", ".", "exit", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.term_handler": [[32, 35], ["logger.warning", "logger.warning", "str"], "function", ["None"], ["", "def", "term_handler", "(", "signum", ",", "frame", ")", ":", "\n", "    ", "logger", ".", "warning", "(", "\"Signal handler called with signal \"", "+", "str", "(", "signum", ")", ")", "\n", "logger", ".", "warning", "(", "\"Bypassing SIGTERM.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_signal_handler": [[37, 44], ["signal.signal", "signal.signal", "logger.warning"], "function", ["None"], ["", "def", "init_signal_handler", "(", ")", ":", "\n", "    ", "\"\"\"\n    Handle signals sent by SLURM for time limit / pre-emption.\n    \"\"\"", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGUSR1", ",", "sig_handler", ")", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGTERM", ",", "term_handler", ")", "\n", "logger", ".", "warning", "(", "\"Signal handler installed.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.slurm.init_distributed_mode": [[46, 170], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "torch.cuda.set_device", "int", "int", "int", "int", "int", "subprocess.check_output", "[].decode", "print", "print", "str", "str", "str", "print", "torch.distributed.init_process_group", "str", "int", "os.environ.get", "print", "int", "int", "int", "str", "str", "str", "socket.gethostname", "subprocess.check_output.split", "str"], "function", ["None"], ["", "def", "init_distributed_mode", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Handle single and multi-GPU / multi-node / SLURM jobs.\n    Initialize the following variables:\n        - n_nodes\n        - node_id\n        - local_rank\n        - global_rank\n        - world_size\n    \"\"\"", "\n", "params", ".", "is_slurm_job", "=", "'SLURM_JOB_ID'", "in", "os", ".", "environ", "and", "not", "params", ".", "debug_slurm", "\n", "print", "(", "\"SLURM job: %s\"", "%", "str", "(", "params", ".", "is_slurm_job", ")", ")", "\n", "\n", "# SLURM job", "\n", "if", "params", ".", "is_slurm_job", ":", "\n", "\n", "        ", "assert", "params", ".", "local_rank", "==", "-", "1", "# on the cluster, this is handled by SLURM", "\n", "\n", "SLURM_VARIABLES", "=", "[", "\n", "'SLURM_JOB_ID'", ",", "\n", "'SLURM_JOB_NODELIST'", ",", "'SLURM_JOB_NUM_NODES'", ",", "'SLURM_NTASKS'", ",", "'SLURM_TASKS_PER_NODE'", ",", "\n", "'SLURM_MEM_PER_NODE'", ",", "'SLURM_MEM_PER_CPU'", ",", "\n", "'SLURM_NODEID'", ",", "'SLURM_PROCID'", ",", "'SLURM_LOCALID'", ",", "'SLURM_TASK_PID'", "\n", "]", "\n", "\n", "PREFIX", "=", "\"%i - \"", "%", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "for", "name", "in", "SLURM_VARIABLES", ":", "\n", "            ", "value", "=", "os", ".", "environ", ".", "get", "(", "name", ",", "None", ")", "\n", "print", "(", "PREFIX", "+", "\"%s: %s\"", "%", "(", "name", ",", "str", "(", "value", ")", ")", ")", "\n", "\n", "# # job ID", "\n", "# params.job_id = os.environ['SLURM_JOB_ID']", "\n", "\n", "# number of nodes / node ID", "\n", "", "params", ".", "n_nodes", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_JOB_NUM_NODES'", "]", ")", "\n", "params", ".", "node_id", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_NODEID'", "]", ")", "\n", "\n", "# local rank on the current node / global rank", "\n", "params", ".", "local_rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_LOCALID'", "]", ")", "\n", "params", ".", "global_rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "\n", "# number of processes / GPUs per node", "\n", "params", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_NTASKS'", "]", ")", "\n", "params", ".", "n_gpu_per_node", "=", "params", ".", "world_size", "//", "params", ".", "n_nodes", "\n", "\n", "# define master address and master port", "\n", "hostnames", "=", "subprocess", ".", "check_output", "(", "[", "'scontrol'", ",", "'show'", ",", "'hostnames'", ",", "os", ".", "environ", "[", "'SLURM_JOB_NODELIST'", "]", "]", ")", "\n", "params", ".", "master_addr", "=", "hostnames", ".", "split", "(", ")", "[", "0", "]", ".", "decode", "(", "'utf-8'", ")", "\n", "assert", "10001", "<=", "params", ".", "master_port", "<=", "20000", "or", "params", ".", "world_size", "==", "1", "\n", "print", "(", "PREFIX", "+", "\"Master address: %s\"", "%", "params", ".", "master_addr", ")", "\n", "print", "(", "PREFIX", "+", "\"Master port   : %i\"", "%", "params", ".", "master_port", ")", "\n", "\n", "# set environment variables for 'env://'", "\n", "os", ".", "environ", "[", "'MASTER_ADDR'", "]", "=", "params", ".", "master_addr", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "str", "(", "params", ".", "master_port", ")", "\n", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", "=", "str", "(", "params", ".", "world_size", ")", "\n", "os", ".", "environ", "[", "'RANK'", "]", "=", "str", "(", "params", ".", "global_rank", ")", "\n", "\n", "# multi-GPU job (local or multi-node) - jobs started with torch.distributed.launch", "\n", "", "elif", "params", ".", "local_rank", "!=", "-", "1", ":", "\n", "\n", "        ", "assert", "params", ".", "master_port", "==", "-", "1", "\n", "\n", "# read environment variables", "\n", "params", ".", "global_rank", "=", "int", "(", "os", ".", "environ", "[", "'RANK'", "]", ")", "\n", "params", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "params", ".", "n_gpu_per_node", "=", "int", "(", "os", ".", "environ", "[", "'NGPU'", "]", ")", "\n", "\n", "# number of nodes / node ID", "\n", "params", ".", "n_nodes", "=", "params", ".", "world_size", "//", "params", ".", "n_gpu_per_node", "\n", "params", ".", "node_id", "=", "params", ".", "global_rank", "//", "params", ".", "n_gpu_per_node", "\n", "\n", "# local job (single GPU)", "\n", "", "else", ":", "\n", "        ", "assert", "params", ".", "local_rank", "==", "-", "1", "\n", "assert", "params", ".", "master_port", "==", "-", "1", "\n", "params", ".", "n_nodes", "=", "1", "\n", "params", ".", "node_id", "=", "0", "\n", "params", ".", "local_rank", "=", "0", "\n", "params", ".", "global_rank", "=", "0", "\n", "params", ".", "world_size", "=", "1", "\n", "params", ".", "n_gpu_per_node", "=", "1", "\n", "\n", "# sanity checks", "\n", "", "assert", "params", ".", "n_nodes", ">=", "1", "\n", "assert", "0", "<=", "params", ".", "node_id", "<", "params", ".", "n_nodes", "\n", "assert", "0", "<=", "params", ".", "local_rank", "<=", "params", ".", "global_rank", "<", "params", ".", "world_size", "\n", "assert", "params", ".", "world_size", "==", "params", ".", "n_nodes", "*", "params", ".", "n_gpu_per_node", "\n", "\n", "# define whether this is the master process / if we are in distributed mode", "\n", "params", ".", "is_master", "=", "params", ".", "node_id", "==", "0", "and", "params", ".", "local_rank", "==", "0", "\n", "params", ".", "multi_node", "=", "params", ".", "n_nodes", ">", "1", "\n", "params", ".", "multi_gpu", "=", "params", ".", "world_size", ">", "1", "\n", "\n", "# summary", "\n", "PREFIX", "=", "\"%i - \"", "%", "params", ".", "global_rank", "\n", "print", "(", "PREFIX", "+", "\"Number of nodes: %i\"", "%", "params", ".", "n_nodes", ")", "\n", "print", "(", "PREFIX", "+", "\"Node ID        : %i\"", "%", "params", ".", "node_id", ")", "\n", "print", "(", "PREFIX", "+", "\"Local rank     : %i\"", "%", "params", ".", "local_rank", ")", "\n", "print", "(", "PREFIX", "+", "\"Global rank    : %i\"", "%", "params", ".", "global_rank", ")", "\n", "print", "(", "PREFIX", "+", "\"World size     : %i\"", "%", "params", ".", "world_size", ")", "\n", "print", "(", "PREFIX", "+", "\"GPUs per node  : %i\"", "%", "params", ".", "n_gpu_per_node", ")", "\n", "print", "(", "PREFIX", "+", "\"Master         : %s\"", "%", "str", "(", "params", ".", "is_master", ")", ")", "\n", "print", "(", "PREFIX", "+", "\"Multi-node     : %s\"", "%", "str", "(", "params", ".", "multi_node", ")", ")", "\n", "print", "(", "PREFIX", "+", "\"Multi-GPU      : %s\"", "%", "str", "(", "params", ".", "multi_gpu", ")", ")", "\n", "print", "(", "PREFIX", "+", "\"Hostname       : %s\"", "%", "socket", ".", "gethostname", "(", ")", ")", "\n", "\n", "# set GPU device", "\n", "torch", ".", "cuda", ".", "set_device", "(", "params", ".", "local_rank", ")", "\n", "\n", "# initialize multi-GPU", "\n", "if", "params", ".", "multi_gpu", ":", "\n", "\n", "# http://pytorch.apachecn.org/en/0.3.0/distributed.html#environment-variable-initialization", "\n", "# 'env://' will read these environment variables:", "\n", "# MASTER_PORT - required; has to be a free port on machine with rank 0", "\n", "# MASTER_ADDR - required (except for rank 0); address of rank 0 node", "\n", "# WORLD_SIZE - required; can be set either here, or in a call to init function", "\n", "# RANK - required; can be set either here, or in a call to init function", "\n", "\n", "        ", "print", "(", "\"Initializing PyTorch distributed ...\"", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "\n", "init_method", "=", "'env://'", ",", "\n", "backend", "=", "'nccl'", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.__init__": [[39, 156], ["logger.info", "logger.info", "trainer.Trainer.set_parameters", "trainer.Trainer.set_optimizers", "torch.FloatTensor", "numpy.array", "collections.OrderedDict", "time.time", "trainer.Trainer.reload_checkpoint", "utils.parse_lambda_config", "utils.find_modules", "utils.find_modules", "logger.info", "trainer.Trainer.init_amp", "params.stopping_criterion.split", "int", "list", "numpy.maximum", "trainer.Trainer.metrics.append", "getattr", "getattr", "len", "len", "setattr", "logger.info", "split[].isdigit", "trainer.Trainer.data[].counts.values", "params.validation_metrics.split", "torch.nn.parallel.DistributedDataParallel", "setattr", "len", "getattr", "apex.parallel.DistributedDataParallel", "getattr", "data[].keys", "data[].keys", "data[].keys", "data[].keys", "data[].keys", "data[].keys", "data[].keys", "data[].keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.set_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.set_optimizers", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.reload_checkpoint", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.parse_lambda_config", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.find_modules", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.find_modules", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.init_amp"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize trainer.\n        \"\"\"", "\n", "# epoch / iteration size", "\n", "self", ".", "epoch_size", "=", "params", ".", "epoch_size", "\n", "if", "self", ".", "epoch_size", "==", "-", "1", ":", "\n", "            ", "self", ".", "epoch_size", "=", "self", ".", "data", "\n", "assert", "self", ".", "epoch_size", ">", "0", "\n", "\n", "# data iterators", "\n", "", "self", ".", "iterators", "=", "{", "}", "\n", "\n", "# list memory components", "\n", "self", ".", "memory_list", "=", "[", "]", "\n", "self", ".", "ffn_list", "=", "[", "]", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "find_modules", "(", "getattr", "(", "self", ",", "name", ")", ",", "'self.{}'", ".", "format", "(", "name", ")", ",", "HashingMemory", ",", "self", ".", "memory_list", ")", "\n", "find_modules", "(", "getattr", "(", "self", ",", "name", ")", ",", "'self.{}'", ".", "format", "(", "name", ")", ",", "TransformerFFN", ",", "self", ".", "ffn_list", ")", "\n", "", "logger", ".", "info", "(", "\"Found %i memories.\"", "%", "len", "(", "self", ".", "memory_list", ")", ")", "\n", "logger", ".", "info", "(", "\"Found %i FFN.\"", "%", "len", "(", "self", ".", "ffn_list", ")", ")", "\n", "\n", "# set parameters", "\n", "self", ".", "set_parameters", "(", ")", "\n", "\n", "# float16 / distributed (no AMP)", "\n", "assert", "params", ".", "amp", ">=", "1", "or", "not", "params", ".", "fp16", "\n", "assert", "params", ".", "amp", ">=", "0", "or", "params", ".", "accumulate_gradients", "==", "1", "\n", "if", "params", ".", "multi_gpu", "and", "params", ".", "amp", "==", "-", "1", ":", "\n", "            ", "logger", ".", "info", "(", "\"Using nn.parallel.DistributedDataParallel ...\"", ")", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "                ", "setattr", "(", "self", ",", "name", ",", "\n", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "getattr", "(", "self", ",", "name", ")", ",", "device_ids", "=", "[", "params", ".", "local_rank", "]", ",", "\n", "output_device", "=", "params", ".", "local_rank", ",", "broadcast_buffers", "=", "True", ")", ")", "\n", "\n", "# set optimizers", "\n", "", "", "self", ".", "set_optimizers", "(", ")", "\n", "\n", "# float16 / distributed (AMP)", "\n", "if", "params", ".", "amp", ">=", "0", ":", "\n", "            ", "self", ".", "init_amp", "(", ")", "\n", "if", "params", ".", "multi_gpu", ":", "\n", "                ", "logger", ".", "info", "(", "\"Using apex.parallel.DistributedDataParallel ...\"", ")", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "                    ", "setattr", "(", "self", ",", "name", ",", "\n", "apex", ".", "parallel", ".", "DistributedDataParallel", "(", "getattr", "(", "self", ",", "name", ")", ",", "delay_allreduce", "=", "True", ")", ")", "\n", "\n", "# stopping criterion used for early stopping", "\n", "", "", "", "if", "params", ".", "stopping_criterion", "!=", "''", ":", "\n", "            ", "split", "=", "params", ".", "stopping_criterion", ".", "split", "(", "','", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "and", "split", "[", "1", "]", ".", "isdigit", "(", ")", "\n", "self", ".", "decrease_counts_max", "=", "int", "(", "split", "[", "1", "]", ")", "\n", "self", ".", "decrease_counts", "=", "0", "\n", "if", "split", "[", "0", "]", "[", "0", "]", "==", "'_'", ":", "\n", "                ", "self", ".", "stopping_criterion", "=", "(", "split", "[", "0", "]", "[", "1", ":", "]", ",", "False", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "stopping_criterion", "=", "(", "split", "[", "0", "]", ",", "True", ")", "\n", "", "self", ".", "best_stopping_criterion", "=", "-", "1e12", "if", "self", ".", "stopping_criterion", "[", "1", "]", "else", "1e12", "\n", "", "else", ":", "\n", "            ", "self", ".", "stopping_criterion", "=", "None", "\n", "self", ".", "best_stopping_criterion", "=", "None", "\n", "\n", "# probability of masking out / randomize / not modify words to predict", "\n", "", "params", ".", "pred_probs", "=", "torch", ".", "FloatTensor", "(", "[", "params", ".", "word_mask", ",", "params", ".", "word_keep", ",", "params", ".", "word_rand", "]", ")", "\n", "\n", "# probabilty to predict a word", "\n", "counts", "=", "np", ".", "array", "(", "list", "(", "self", ".", "data", "[", "'dico'", "]", ".", "counts", ".", "values", "(", ")", ")", ")", "\n", "params", ".", "mask_scores", "=", "np", ".", "maximum", "(", "counts", ",", "1", ")", "**", "-", "params", ".", "sample_alpha", "\n", "params", ".", "mask_scores", "[", "params", ".", "pad_index", "]", "=", "0", "# do not predict <PAD> index", "\n", "params", ".", "mask_scores", "[", "counts", "==", "0", "]", "=", "0", "# do not predict special tokens", "\n", "\n", "# validation metrics", "\n", "self", ".", "metrics", "=", "[", "]", "\n", "metrics", "=", "[", "m", "for", "m", "in", "params", ".", "validation_metrics", ".", "split", "(", "','", ")", "if", "m", "!=", "''", "]", "\n", "for", "m", "in", "metrics", ":", "\n", "            ", "m", "=", "(", "m", "[", "1", ":", "]", ",", "False", ")", "if", "m", "[", "0", "]", "==", "'_'", "else", "(", "m", ",", "True", ")", "\n", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "", "self", ".", "best_metrics", "=", "{", "metric", ":", "(", "-", "1e12", "if", "biggest", "else", "1e12", ")", "for", "(", "metric", ",", "biggest", ")", "in", "self", ".", "metrics", "}", "\n", "\n", "# training statistics", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "n_iter", "=", "0", "\n", "self", ".", "n_total_iter", "=", "0", "\n", "self", ".", "n_sentences", "=", "0", "\n", "self", ".", "stats", "=", "OrderedDict", "(", "\n", "[", "(", "'processed_s'", ",", "0", ")", ",", "(", "'processed_w'", ",", "0", ")", "]", "+", "\n", "[", "(", "'CLM-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'CLM-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'CLM-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'MLM-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'MLM-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'MLM-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'EDMLM-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'EDMLM-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'EDMLM-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'AEmbeam-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'AEmbeam-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'AEmbeam-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'PC-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "params", ".", "pc_steps", "]", "+", "\n", "[", "(", "'mbeamAE-%s'", "%", "lang", ",", "[", "]", ")", "for", "lang", "in", "params", ".", "ae_steps", "]", "+", "\n", "[", "(", "'AE-%s'", "%", "lang", ",", "[", "]", ")", "for", "lang", "in", "params", ".", "ae_steps", "]", "+", "\n", "[", "(", "'MT-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "params", ".", "mt_steps", "]", "+", "\n", "[", "(", "'2TBT-%s-%s-%s'", "%", "(", "l1", ",", "l2", ",", "l3", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "+", "\n", "[", "(", "'BT-%s-%s-%s'", "%", "(", "l1", ",", "l2", ",", "l3", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "+", "\n", "[", "(", "'BTCT-%s-%s-%s'", "%", "(", "l1", ",", "l2", ",", "l3", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "+", "\n", "[", "(", "'BTMA-%s'", "%", "l1", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "+", "\n", "[", "(", "'BTAE-%s'", "%", "l1", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "+", "\n", "# [('BTMA-%s' % lang, []) for lang in params.mass_steps] +", "\n", "[", "(", "'BTSEC-%s-%s-%s'", "%", "(", "l1", ",", "l2", ",", "l3", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "\n", ")", "\n", "self", ".", "last_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "# reload potential checkpoints", "\n", "self", ".", "reload_checkpoint", "(", ")", "\n", "\n", "# initialize lambda coefficients and their configurations", "\n", "parse_lambda_config", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.set_parameters": [[157, 179], ["trainer.Trainer.parameters.items", "named_params.extend", "logger.info", "len", "len", "k.endswith", "k.endswith", "len", "len", "getattr().named_parameters", "len", "getattr"], "methods", ["None"], ["", "def", "set_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Set parameters.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "parameters", "=", "{", "}", "\n", "named_params", "=", "[", "]", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "named_params", ".", "extend", "(", "[", "(", "k", ",", "p", ")", "for", "k", ",", "p", "in", "getattr", "(", "self", ",", "name", ")", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ")", "\n", "\n", "# model (excluding memory values)", "\n", "", "self", ".", "parameters", "[", "'model'", "]", "=", "[", "p", "for", "k", ",", "p", "in", "named_params", "if", "not", "k", ".", "endswith", "(", "HashingMemory", ".", "MEM_VALUES_PARAMS", ")", "]", "\n", "\n", "# memory values", "\n", "if", "params", ".", "use_memory", ":", "\n", "            ", "self", ".", "parameters", "[", "'memory'", "]", "=", "[", "p", "for", "k", ",", "p", "in", "named_params", "if", "k", ".", "endswith", "(", "HashingMemory", ".", "MEM_VALUES_PARAMS", ")", "]", "\n", "assert", "len", "(", "self", ".", "parameters", "[", "'memory'", "]", ")", "==", "len", "(", "params", ".", "mem_enc_positions", ")", "+", "len", "(", "params", ".", "mem_dec_positions", ")", "\n", "\n", "# log", "\n", "", "for", "k", ",", "v", "in", "self", ".", "parameters", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Found %i parameters in %s.\"", "%", "(", "len", "(", "v", ")", ",", "k", ")", ")", "\n", "assert", "len", "(", "v", ")", ">=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.set_optimizers": [[180, 196], ["optim.get_optimizer", "logger.info", "optim.get_optimizer", "trainer.Trainer.optimizers.keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer"], ["", "", "def", "set_optimizers", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Set optimizers.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "optimizers", "=", "{", "}", "\n", "\n", "# model optimizer (excluding memory values)", "\n", "self", ".", "optimizers", "[", "'model'", "]", "=", "get_optimizer", "(", "self", ".", "parameters", "[", "'model'", "]", ",", "params", ".", "optimizer", ")", "\n", "\n", "# memory values optimizer", "\n", "if", "params", ".", "use_memory", ":", "\n", "            ", "self", ".", "optimizers", "[", "'memory'", "]", "=", "get_optimizer", "(", "self", ".", "parameters", "[", "'memory'", "]", ",", "params", ".", "mem_values_optimizer", ")", "\n", "\n", "# log", "\n", "", "logger", ".", "info", "(", "\"Optimizers: %s\"", "%", "\", \"", ".", "join", "(", "self", ".", "optimizers", ".", "keys", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.init_amp": [[197, 215], ["trainer.Trainer.optimizers.keys", "apex.amp.initialize", "zip", "getattr", "setattr", "zip"], "methods", ["None"], ["", "def", "init_amp", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialize AMP optimizer.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "params", ".", "amp", "==", "0", "and", "params", ".", "fp16", "is", "False", "or", "params", ".", "amp", "in", "[", "1", ",", "2", ",", "3", "]", "and", "params", ".", "fp16", "is", "True", "\n", "opt_names", "=", "self", ".", "optimizers", ".", "keys", "(", ")", "\n", "models", "=", "[", "getattr", "(", "self", ",", "name", ")", "for", "name", "in", "self", ".", "MODEL_NAMES", "]", "\n", "models", ",", "optimizers", "=", "apex", ".", "amp", ".", "initialize", "(", "\n", "models", ",", "\n", "[", "self", ".", "optimizers", "[", "k", "]", "for", "k", "in", "opt_names", "]", ",", "\n", "opt_level", "=", "(", "'O%i'", "%", "params", ".", "amp", ")", "\n", ")", "\n", "for", "name", ",", "model", "in", "zip", "(", "self", ".", "MODEL_NAMES", ",", "models", ")", ":", "\n", "            ", "setattr", "(", "self", ",", "name", ",", "model", ")", "\n", "", "self", ".", "optimizers", "=", "{", "\n", "opt_name", ":", "optimizer", "\n", "for", "opt_name", ",", "optimizer", "in", "zip", "(", "opt_names", ",", "optimizers", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.optimize": [[217, 268], ["trainer.Trainer.optimizers.keys", "logger.warning", "loss.backward", "optimizer.zero_grad", "optimizer.step", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "optimizer.zero_grad", "apex.amp.scale_loss", "scaled_loss.backward", "apex.amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "apex.amp.master_params", "apex.amp.scale_loss", "scaled_loss.backward"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step"], ["", "def", "optimize", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"\n        Optimize.\n        \"\"\"", "\n", "# check NaN", "\n", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\"NaN detected\"", ")", "\n", "# exit()", "\n", "\n", "", "params", "=", "self", ".", "params", "\n", "\n", "# optimizers", "\n", "names", "=", "self", ".", "optimizers", ".", "keys", "(", ")", "\n", "optimizers", "=", "[", "self", ".", "optimizers", "[", "k", "]", "for", "k", "in", "names", "]", "\n", "\n", "# regular optimization", "\n", "if", "params", ".", "amp", "==", "-", "1", ":", "\n", "            ", "for", "optimizer", "in", "optimizers", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "if", "params", ".", "clip_grad_norm", ">", "0", ":", "\n", "                ", "for", "name", "in", "names", ":", "\n", "# norm_check_a = (sum([p.grad.norm(p=2).item() ** 2 for p in self.parameters[name]])) ** 0.5", "\n", "                    ", "clip_grad_norm_", "(", "self", ".", "parameters", "[", "name", "]", ",", "params", ".", "clip_grad_norm", ")", "\n", "# norm_check_b = (sum([p.grad.norm(p=2).item() ** 2 for p in self.parameters[name]])) ** 0.5", "\n", "# print(name, norm_check_a, norm_check_b)", "\n", "", "", "for", "optimizer", "in", "optimizers", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "\n", "# AMP optimization", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "n_iter", "%", "params", ".", "accumulate_gradients", "==", "0", ":", "\n", "                ", "try", ":", "\n", "                    ", "with", "apex", ".", "amp", ".", "scale_loss", "(", "loss", ",", "optimizers", ")", "as", "scaled_loss", ":", "\n", "                        ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "for", "opt", "in", "optimizers", ":", "\n", "                        ", "with", "apex", ".", "amp", ".", "scale_loss", "(", "loss", ",", "opt", ")", "as", "scaled_loss", ":", "\n", "                            ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "", "if", "params", ".", "clip_grad_norm", ">", "0", ":", "\n", "                    ", "for", "name", "in", "names", ":", "\n", "# norm_check_a = (sum([p.grad.norm(p=2).item() ** 2 for p in apex.amp.master_params(self.optimizers[name])])) ** 0.5", "\n", "                        ", "clip_grad_norm_", "(", "apex", ".", "amp", ".", "master_params", "(", "self", ".", "optimizers", "[", "name", "]", ")", ",", "params", ".", "clip_grad_norm", ")", "\n", "# norm_check_b = (sum([p.grad.norm(p=2).item() ** 2 for p in apex.amp.master_params(self.optimizers[name])])) ** 0.5", "\n", "# print(name, norm_check_a, norm_check_b)", "\n", "", "", "for", "optimizer", "in", "optimizers", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "with", "apex", ".", "amp", ".", "scale_loss", "(", "loss", ",", "optimizers", ",", "delay_unscale", "=", "True", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.iter": [[269, 277], ["utils.update_lambdas", "trainer.Trainer.print_stats"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.update_lambdas", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.print_stats"], ["", "", "", "", "def", "iter", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        End of iteration.\n        \"\"\"", "\n", "self", ".", "n_iter", "+=", "1", "\n", "self", ".", "n_total_iter", "+=", "1", "\n", "update_lambdas", "(", "self", ".", "params", ",", "self", ".", "n_total_iter", ")", "\n", "self", ".", "print_stats", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.print_stats": [[278, 313], ["trainer.Trainer.stats.keys", "trainer.Trainer.optimizers.items", "time.time", "logger.info", "type", "numpy.mean", "trainer.Trainer.stats.items", "type", "len"], "methods", ["None"], ["", "def", "print_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Print statistics about the training.\n        \"\"\"", "\n", "if", "self", ".", "n_total_iter", "%", "5", "!=", "0", ":", "\n", "            ", "return", "\n", "\n", "", "s_iter", "=", "\"%7i - \"", "%", "self", ".", "n_total_iter", "\n", "s_stat", "=", "' || '", ".", "join", "(", "[", "\n", "'{}: {:7.4f}'", ".", "format", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ")", "for", "k", ",", "v", "in", "self", ".", "stats", ".", "items", "(", ")", "\n", "if", "type", "(", "v", ")", "is", "list", "and", "len", "(", "v", ")", ">", "0", "\n", "]", ")", "\n", "for", "k", "in", "self", ".", "stats", ".", "keys", "(", ")", ":", "\n", "            ", "if", "type", "(", "self", ".", "stats", "[", "k", "]", ")", "is", "list", ":", "\n", "                ", "del", "self", ".", "stats", "[", "k", "]", "[", ":", "]", "\n", "\n", "# learning rates", "\n", "", "", "s_lr", "=", "\" - \"", "\n", "for", "k", ",", "v", "in", "self", ".", "optimizers", ".", "items", "(", ")", ":", "\n", "            ", "s_lr", "=", "s_lr", "+", "(", "\" - %s LR: \"", "%", "k", ")", "+", "\" / \"", ".", "join", "(", "\"{:.4e}\"", ".", "format", "(", "group", "[", "'lr'", "]", ")", "for", "group", "in", "v", ".", "param_groups", ")", "\n", "\n", "# processing speed", "\n", "", "new_time", "=", "time", ".", "time", "(", ")", "\n", "diff", "=", "new_time", "-", "self", ".", "last_time", "\n", "s_speed", "=", "\"{:7.2f} sent/s - {:8.2f} words/s - \"", ".", "format", "(", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "*", "1.0", "/", "diff", ",", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "*", "1.0", "/", "diff", "\n", ")", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "=", "0", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "=", "0", "\n", "self", ".", "last_time", "=", "new_time", "\n", "\n", "# log speed + stats + learning rate", "\n", "if", "self", ".", "n_total_iter", "%", "self", ".", "params", ".", "log_per_iter", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "s_iter", "+", "s_speed", "+", "s_stat", "+", "s_lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.get_iterator": [[314, 343], ["logger.info", "[].get_iterator", "[].get_iterator", "[].get_iterator", "str"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "", "def", "get_iterator", "(", "self", ",", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ",", "descending", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Create a new iterator for a dataset.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Creating new training data iterator (%s) ...\"", "%", "','", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "[", "iter_name", ",", "lang1", ",", "lang2", "]", "if", "x", "is", "not", "None", "]", ")", ")", "\n", "assert", "stream", "or", "not", "self", ".", "params", ".", "use_memory", "or", "not", "self", ".", "params", ".", "mem_query_batchnorm", "\n", "if", "lang2", "is", "None", ":", "\n", "            ", "if", "stream", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono_stream'", "]", "[", "lang1", "]", "[", "'train'", "]", ".", "get_iterator", "(", "shuffle", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono'", "]", "[", "lang1", "]", "[", "'train'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "n_sentences", "=", "-", "1", ",", "\n", "descending", "=", "descending", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "stream", "is", "False", "\n", "_lang1", ",", "_lang2", "=", "(", "lang1", ",", "lang2", ")", "if", "lang1", "<", "lang2", "else", "(", "lang2", ",", "lang1", ")", "\n", "iterator", "=", "self", ".", "data", "[", "'para'", "]", "[", "(", "_lang1", ",", "_lang2", ")", "]", "[", "'train'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "n_sentences", "=", "-", "1", ",", "\n", "descending", "=", "descending", ",", "\n", ")", "\n", "\n", "", "self", ".", "iterators", "[", "(", "iter_name", ",", "lang1", ",", "lang2", ")", "]", "=", "iterator", "\n", "return", "iterator", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.get_batch": [[344, 360], ["trainer.Trainer.iterators.get", "trainer.Trainer.get_iterator", "next", "trainer.Trainer.get_iterator", "next"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_batch", "(", "self", ",", "iter_name", ",", "lang1", ",", "lang2", "=", "None", ",", "stream", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Return a batch of sentences from a dataset.\n        \"\"\"", "\n", "assert", "lang1", "in", "self", ".", "params", ".", "langs", "\n", "assert", "lang2", "is", "None", "or", "lang2", "in", "self", ".", "params", ".", "langs", "\n", "assert", "stream", "is", "False", "or", "lang2", "is", "None", "\n", "iterator", "=", "self", ".", "iterators", ".", "get", "(", "(", "iter_name", ",", "lang1", ",", "lang2", ")", ",", "None", ")", "\n", "if", "iterator", "is", "None", ":", "\n", "            ", "iterator", "=", "self", ".", "get_iterator", "(", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ")", "\n", "", "try", ":", "\n", "            ", "x", "=", "next", "(", "iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "iterator", "=", "self", ".", "get_iterator", "(", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ")", "\n", "x", "=", "next", "(", "iterator", ")", "\n", "", "return", "x", "if", "lang2", "is", "None", "or", "lang1", "<", "lang2", "else", "x", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.word_shuffle": [[361, 391], ["numpy.random.uniform", "x.clone", "range", "lengths.size", "scores.argsort", "x2[].copy_", "numpy.arange", "x.size", "x.size", "lengths[].item", "torch.from_numpy"], "methods", ["None"], ["", "def", "word_shuffle", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Randomly shuffle input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_shuffle", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n", "# define noise word scores", "\n", "", "noise", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "self", ".", "params", ".", "word_shuffle", ",", "size", "=", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ")", "\n", "noise", "[", "0", "]", "=", "-", "1", "# do not move start sentence symbol", "\n", "# l_cpu = l.cpu()", "\n", "assert", "self", ".", "params", ".", "word_shuffle", ">", "1", "\n", "x2", "=", "x", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "# generate a random permutation", "\n", "# try:", "\n", "# print(lengths[i] - 1)", "\n", "# arange = np.arange(lengths[i].item() - 1)", "\n", "# noise_ = noise[:lengths[i] - 1, i]", "\n", "# print(noise_)", "\n", "# print(arange)", "\n", "# scores = arange + noise_", "\n", "            ", "scores", "=", "np", ".", "arange", "(", "lengths", "[", "i", "]", ".", "item", "(", ")", "-", "1", ")", "+", "noise", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "\n", "# except Exception as e:", "\n", "#     print('ERROR: ')", "\n", "#     raise e", "\n", "permutation", "=", "scores", ".", "argsort", "(", ")", "\n", "# shuffle words", "\n", "x2", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", ".", "copy_", "(", "x2", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "[", "torch", ".", "from_numpy", "(", "permutation", ")", "]", ")", "\n", "", "return", "x2", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.word_dropout": [[392, 429], ["lengths.size", "range", "torch.LongTensor", "torch.LongTensor().fill_", "range", "lengths.size", "numpy.random.rand", "x[].tolist", "new_s.append", "sentences.append", "lengths_.append", "torch.LongTensor.size", "x2[].copy_", "x.size", "new_s.append", "len", "torch.LongTensor", "torch.LongTensor", "x.size", "enumerate", "len", "len", "torch.LongTensor.max", "torch.LongTensor.size", "len", "len", "numpy.random.randint", "len"], "methods", ["None"], ["", "def", "word_dropout", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Randomly drop input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_dropout", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "", "assert", "0", "<", "self", ".", "params", ".", "word_dropout", "<", "1", "\n", "\n", "# define words to drop", "\n", "eos", "=", "self", ".", "params", ".", "eos_index", "\n", "assert", "(", "x", "[", "0", "]", "==", "eos", ")", ".", "sum", "(", ")", "==", "lengths", ".", "size", "(", "0", ")", "\n", "keep", "=", "np", ".", "random", ".", "rand", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ">=", "self", ".", "params", ".", "word_dropout", "\n", "keep", "[", "0", "]", "=", "1", "# do not drop the start sentence symbol", "\n", "\n", "bs", "=", "lengths", ".", "size", "(", "0", ")", "\n", "sentences", "=", "[", "]", "\n", "lengths_", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "bs", ")", ":", "\n", "            ", "assert", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "eos", "\n", "words", "=", "x", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", ".", "tolist", "(", ")", "\n", "# randomly drop words from the input", "\n", "new_s", "=", "[", "w", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "if", "keep", "[", "j", ",", "i", "]", "]", "\n", "\n", "# we need to have at least one word in the sentence (more than the start / end sentence symbols)", "\n", "if", "len", "(", "new_s", ")", "==", "1", "and", "len", "(", "words", ")", ">", "1", ":", "\n", "                ", "new_s", ".", "append", "(", "words", "[", "np", ".", "random", ".", "randint", "(", "1", ",", "len", "(", "words", ")", ")", "]", ")", "\n", "", "new_s", ".", "append", "(", "eos", ")", "\n", "assert", "(", "len", "(", "new_s", ")", ">=", "3", "or", "len", "(", "words", ")", "==", "1", ")", "and", "new_s", "[", "0", "]", "==", "eos", "and", "new_s", "[", "-", "1", "]", "==", "eos", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "lengths_", ".", "append", "(", "len", "(", "new_s", ")", ")", "\n", "\n", "# re-construct input", "\n", "", "l2", "=", "torch", ".", "LongTensor", "(", "lengths_", ")", "\n", "x2", "=", "torch", ".", "LongTensor", "(", "l2", ".", "max", "(", ")", ",", "l2", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "for", "i", "in", "range", "(", "l2", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "x2", "[", ":", "l2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "return", "x2", ",", "l2", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.word_blank": [[430, 458], ["range", "torch.LongTensor().fill_", "range", "lengths.size", "numpy.random.rand", "lengths.size", "x[].tolist", "new_s.append", "sentences.append", "lengths.size", "x2[].copy_", "x.size", "torch.LongTensor", "torch.LongTensor", "x.size", "enumerate", "len", "lengths.max", "lengths.size"], "methods", ["None"], ["", "def", "word_blank", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Randomly blank input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_blank", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "", "assert", "0", "<", "self", ".", "params", ".", "word_blank", "<", "1", "\n", "\n", "# define words to blank", "\n", "eos", "=", "self", ".", "params", ".", "eos_index", "\n", "assert", "(", "x", "[", "0", "]", "==", "eos", ")", ".", "sum", "(", ")", "==", "lengths", ".", "size", "(", "0", ")", "\n", "keep", "=", "np", ".", "random", ".", "rand", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ">=", "self", ".", "params", ".", "word_blank", "\n", "keep", "[", "0", "]", "=", "1", "# do not blank the start sentence symbol", "\n", "\n", "sentences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "assert", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "eos", "\n", "words", "=", "x", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", ".", "tolist", "(", ")", "\n", "# randomly blank words from the input", "\n", "new_s", "=", "[", "w", "if", "keep", "[", "j", ",", "i", "]", "else", "self", ".", "params", ".", "mask_index", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "]", "\n", "new_s", ".", "append", "(", "eos", ")", "\n", "assert", "len", "(", "new_s", ")", "==", "lengths", "[", "i", "]", "and", "new_s", "[", "0", "]", "==", "eos", "and", "new_s", "[", "-", "1", "]", "==", "eos", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "# re-construct input", "\n", "", "x2", "=", "torch", ".", "LongTensor", "(", "lengths", ".", "max", "(", ")", ",", "lengths", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "x2", "[", ":", "lengths", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "return", "x2", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.word_shuffle_dropout_blank": [[459, 462], ["None"], "methods", ["None"], ["", "def", "word_shuffle_dropout_blank", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "if", "self", ".", "params", ".", "word_shuffle", "==", "0", "and", "self", ".", "params", ".", "word_dropout", "==", "0", "and", "self", ".", "params", ".", "word_blank", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.add_noise": [[463, 471], ["trainer.Trainer.word_shuffle", "trainer.Trainer.word_dropout", "trainer.Trainer.word_blank"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_shuffle", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_dropout", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_blank"], ["", "", "def", "add_noise", "(", "self", ",", "words", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Add noise to the encoder input.\n        \"\"\"", "\n", "words", ",", "lengths", "=", "self", ".", "word_shuffle", "(", "words", ",", "lengths", ")", "\n", "words", ",", "lengths", "=", "self", ".", "word_dropout", "(", "words", ",", "lengths", ")", "\n", "words", ",", "lengths", "=", "self", ".", "word_blank", "(", "words", ",", "lengths", ")", "\n", "return", "words", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.mask_out": [[472, 518], ["x.masked_scatter.masked_scatter.size", "_x_real.clone().random_", "_x_real.clone().fill_", "torch.multinomial", "x.masked_scatter.masked_scatter.masked_scatter", "torch.from_numpy().bool", "math.ceil", "numpy.random.choice", "torch.zeros().bool", "pred_mask.view.view.view", "pred_mask.view.view.view", "pred_mask.view.view.sum().item", "max", "pred_mask.view.view.view", "len", "x.masked_scatter.masked_scatter.min", "x.masked_scatter.masked_scatter.max", "x.masked_scatter.masked_scatter.size", "pred_mask.view.view.size", "numpy.random.rand", "len", "_x_real.clone", "_x_real.clone", "torch.from_numpy", "x.masked_scatter.masked_scatter.flatten", "torch.zeros", "pred_mask.view.view.sum", "pred_mask.view.view.sum().item", "pred_mask.view.view.astype", "x_prob.sum", "torch.nonzero().view", "pred_mask.view.view.sum", "torch.nonzero"], "methods", ["None"], ["", "def", "mask_out", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Decide of random words to mask out, and what target they get assigned.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "\n", "# define target words to predict", "\n", "if", "params", ".", "sample_alpha", "==", "0", ":", "\n", "            ", "pred_mask", "=", "np", ".", "random", ".", "rand", "(", "slen", ",", "bs", ")", "<=", "params", ".", "word_pred", "\n", "pred_mask", "=", "torch", ".", "from_numpy", "(", "pred_mask", ".", "astype", "(", "np", ".", "uint8", ")", ")", ".", "bool", "(", ")", "\n", "", "else", ":", "\n", "            ", "x_prob", "=", "params", ".", "mask_scores", "[", "x", ".", "flatten", "(", ")", "]", "\n", "n_tgt", "=", "math", ".", "ceil", "(", "params", ".", "word_pred", "*", "slen", "*", "bs", ")", "\n", "tgt_ids", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "x_prob", ")", ",", "n_tgt", ",", "replace", "=", "False", ",", "p", "=", "x_prob", "/", "x_prob", ".", "sum", "(", ")", ")", "\n", "pred_mask", "=", "torch", ".", "zeros", "(", "slen", "*", "bs", ",", "dtype", "=", "torch", ".", "uint8", ")", ".", "bool", "(", ")", "\n", "pred_mask", "[", "tgt_ids", "]", "=", "1", "\n", "pred_mask", "=", "pred_mask", ".", "view", "(", "slen", ",", "bs", ")", "\n", "\n", "# do not predict padding", "\n", "", "pred_mask", "[", "x", "==", "params", ".", "pad_index", "]", "=", "0", "\n", "pred_mask", "[", "0", "]", "=", "0", "# TODO: remove", "\n", "\n", "# mask a number of words == 0 [8] (faster with fp16)", "\n", "if", "params", ".", "fp16", ":", "\n", "            ", "pred_mask", "=", "pred_mask", ".", "view", "(", "-", "1", ")", "\n", "n1", "=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "n2", "=", "max", "(", "n1", "%", "8", ",", "8", "*", "(", "n1", "//", "8", ")", ")", "\n", "if", "n2", "!=", "n1", ":", "\n", "                ", "pred_mask", "[", "torch", ".", "nonzero", "(", "pred_mask", ")", ".", "view", "(", "-", "1", ")", "[", ":", "n1", "-", "n2", "]", "]", "=", "0", "\n", "", "pred_mask", "=", "pred_mask", ".", "view", "(", "slen", ",", "bs", ")", "\n", "assert", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "%", "8", "==", "0", "\n", "\n", "# generate possible targets / update x input", "\n", "", "_x_real", "=", "x", "[", "pred_mask", "]", "\n", "_x_rand", "=", "_x_real", ".", "clone", "(", ")", ".", "random_", "(", "params", ".", "n_words", ")", "\n", "_x_mask", "=", "_x_real", ".", "clone", "(", ")", ".", "fill_", "(", "params", ".", "mask_index", ")", "\n", "probs", "=", "torch", ".", "multinomial", "(", "params", ".", "pred_probs", ",", "len", "(", "_x_real", ")", ",", "replacement", "=", "True", ")", "\n", "_x", "=", "_x_mask", "*", "(", "probs", "==", "0", ")", ".", "long", "(", ")", "+", "_x_real", "*", "(", "probs", "==", "1", ")", ".", "long", "(", ")", "+", "_x_rand", "*", "(", "probs", "==", "2", ")", ".", "long", "(", ")", "\n", "x", "=", "x", ".", "masked_scatter", "(", "pred_mask", ",", "_x", ")", "\n", "\n", "assert", "0", "<=", "x", ".", "min", "(", ")", "<=", "x", ".", "max", "(", ")", "<", "params", ".", "n_words", "\n", "assert", "x", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "assert", "pred_mask", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "\n", "return", "x", ",", "_x_real", ",", "pred_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.generate_batch": [[519, 543], ["trainer.Trainer.get_batch", "x.clone().fill_", "trainer.Trainer.get_batch", "trainer.Trainer.add_noise", "utils.concat_batches", "trainer.Trainer.get_batch", "utils.concat_batches", "x.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.add_noise", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches"], ["", "def", "generate_batch", "(", "self", ",", "lang1", ",", "lang2", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Prepare a batch (for causal or non-causal mode).\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "if", "lang2", "is", "not", "None", "else", "None", "\n", "\n", "if", "lang2", "is", "None", ":", "\n", "            ", "x", ",", "lengths", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ",", "stream", "=", "True", ")", "\n", "positions", "=", "None", "\n", "langs", "=", "x", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "if", "params", ".", "n_langs", ">", "1", "else", "None", "\n", "", "elif", "lang1", "==", "lang2", ":", "\n", "            ", "(", "x1", ",", "len1", ")", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ")", "\n", "(", "x2", ",", "len2", ")", "=", "(", "x1", ",", "len1", ")", "\n", "(", "x1", ",", "len1", ")", "=", "self", ".", "add_noise", "(", "x1", ",", "len1", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ",", "lang2", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "True", ")", "\n", "\n", "", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "(", "None", ",", "None", ")", "if", "lang2", "is", "None", "else", "(", "len1", ",", "len2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.save_checkpoint": [[544, 576], ["os.path.join", "logger.info", "torch.save", "logger.warning", "getattr().state_dict", "trainer.Trainer.optimizers.keys", "logger.warning", "trainer.Trainer.optimizers[].state_dict", "trainer.Trainer.params.__dict__.items", "getattr"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "name", ",", "include_optimizers", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Save the model / checkpoints.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'%s.pth'", "%", "name", ")", "\n", "logger", ".", "info", "(", "\"Saving %s to %s ...\"", "%", "(", "name", ",", "path", ")", ")", "\n", "\n", "data", "=", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'n_total_iter'", ":", "self", ".", "n_total_iter", ",", "\n", "'best_metrics'", ":", "self", ".", "best_metrics", ",", "\n", "'best_stopping_criterion'", ":", "self", ".", "best_stopping_criterion", ",", "\n", "}", "\n", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Saving {} parameters ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "state_dict", "(", ")", "\n", "\n", "", "if", "include_optimizers", ":", "\n", "            ", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Saving {} optimizer ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "", "data", "[", "'dico_id2word'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "id2word", "\n", "data", "[", "'dico_word2id'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "word2id", "\n", "data", "[", "'dico_counts'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "counts", "\n", "data", "[", "'params'", "]", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "\n", "torch", ".", "save", "(", "data", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.reload_checkpoint": [[577, 636], ["os.path.join", "logger.warning", "torch.load", "trainer.Trainer.optimizers.keys", "logger.warning", "os.path.isfile", "os.path.isfile", "getattr().load_state_dict", "logger.warning", "trainer.Trainer.optimizers[].load_state_dict", "logger.warning", "enumerate", "print", "all", "logger.warning", "trainer.Trainer.optimizers[].get_lr_for_step", "getattr", "getattr().load_state_dict", "logger.warning", "k.startswith", "print", "getattr().load_state_dict", "reload.keys", "reload.items", "getattr", "any", "len", "getattr", "k.startswith", "reload.items", "reload.keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step"], ["", "def", "reload_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload a checkpoint if we find one.\n        \"\"\"", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'checkpoint.pth'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "reload_checkpoint", "==", "''", ":", "\n", "                ", "return", "\n", "", "else", ":", "\n", "                ", "checkpoint_path", "=", "self", ".", "params", ".", "reload_checkpoint", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", "\n", "", "", "logger", ".", "warning", "(", "\"Reloading checkpoint from {} ...\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "data", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "# reload model parameters", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "if", "\"_clone\"", "in", "name", ":", "\n", "# fixme: quick fix", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "data", "[", "name", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "'Reload error: name={}, try removing \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                    ", "reload", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "try", ":", "\n", "                    ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "", "except", "Exception", "as", "ee", ":", "\n", "                    ", "print", "(", "'Reload error again: name={}, try adding \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "not", "any", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                        ", "reload", "=", "{", "'module.{}'", ".", "format", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "# raise e", "\n", "\n", "# reload optimizers", "\n", "", "", "", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "            ", "if", "False", ":", "# AMP checkpoint reloading is buggy, we cannot do that - TODO: fix - https://github.com/NVIDIA/apex/issues/250", "\n", "                ", "logger", ".", "warning", "(", "\"Reloading checkpoint optimizer {} ...\"", ".", "format", "(", "name", ")", ")", "\n", "self", ".", "optimizers", "[", "name", "]", ".", "load_state_dict", "(", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", ")", "\n", "", "else", ":", "# instead, we only reload current iterations / learning rates", "\n", "                ", "logger", ".", "warning", "(", "\"Not reloading checkpoint optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "for", "group_id", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizers", "[", "name", "]", ".", "param_groups", ")", ":", "\n", "                    ", "if", "'num_updates'", "not", "in", "param_group", ":", "\n", "                        ", "logger", ".", "warning", "(", "\"No 'num_updates' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "logger", ".", "warning", "(", "\"Reloading 'num_updates' and 'lr' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "param_group", "[", "'num_updates'", "]", "=", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "[", "'param_groups'", "]", "[", "group_id", "]", "[", "\n", "'num_updates'", "]", "\n", "param_group", "[", "'lr'", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "get_lr_for_step", "(", "param_group", "[", "'num_updates'", "]", ")", "\n", "\n", "# reload main metrics", "\n", "", "", "", "self", ".", "epoch", "=", "data", "[", "'epoch'", "]", "+", "1", "\n", "self", ".", "n_total_iter", "=", "data", "[", "'n_total_iter'", "]", "\n", "self", ".", "best_metrics", "=", "data", "[", "'best_metrics'", "]", "\n", "self", ".", "best_stopping_criterion", "=", "data", "[", "'best_stopping_criterion'", "]", "\n", "logger", ".", "warning", "(", "\n", "\"Checkpoint reloaded. Resuming at epoch {} / iteration {} ...\"", ".", "format", "(", "self", ".", "epoch", ",", "self", ".", "n_total_iter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.save_periodic": [[637, 645], ["trainer.Trainer.save_checkpoint"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_checkpoint"], ["", "def", "save_periodic", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Save the models periodically.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "params", ".", "save_periodic", ">", "0", "and", "self", ".", "epoch", "%", "self", ".", "params", ".", "save_periodic", "==", "0", ":", "\n", "            ", "self", ".", "save_checkpoint", "(", "'periodic-%i'", "%", "self", ".", "epoch", ",", "include_optimizers", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.save_best_model": [[646, 661], ["logger.warning", "logger.info", "trainer.Trainer.save_checkpoint"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_checkpoint"], ["", "", "def", "save_best_model", "(", "self", ",", "scores", ")", ":", "\n", "        ", "\"\"\"\n        Save best models according to given validation metrics.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "", "for", "metric", ",", "biggest", "in", "self", ".", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "scores", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Metric \\\"%s\\\" not found in scores!\"", "%", "metric", ")", "\n", "continue", "\n", "", "factor", "=", "1", "if", "biggest", "else", "-", "1", "\n", "if", "factor", "*", "scores", "[", "metric", "]", ">", "factor", "*", "self", ".", "best_metrics", "[", "metric", "]", ":", "\n", "                ", "self", ".", "best_metrics", "[", "metric", "]", "=", "scores", "[", "metric", "]", "\n", "logger", ".", "info", "(", "'New best score for %s: %.6f'", "%", "(", "metric", ",", "scores", "[", "metric", "]", ")", ")", "\n", "self", ".", "save_checkpoint", "(", "'best-%s'", "%", "metric", ",", "include_optimizers", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.end_epoch": [[662, 688], ["trainer.Trainer.save_checkpoint", "logger.info", "logger.info", "logger.info", "exit", "trainer.Trainer.stopping_criterion[].endswith", "os.system"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_checkpoint"], ["", "", "", "def", "end_epoch", "(", "self", ",", "scores", ")", ":", "\n", "        ", "\"\"\"\n        End the epoch.\n        \"\"\"", "\n", "# stop if the stopping criterion has not improved after a certain number of epochs", "\n", "if", "self", ".", "stopping_criterion", "is", "not", "None", "and", "(", "\n", "self", ".", "params", ".", "is_master", "or", "not", "self", ".", "stopping_criterion", "[", "0", "]", ".", "endswith", "(", "'_mt_bleu'", ")", ")", ":", "\n", "            ", "metric", ",", "biggest", "=", "self", ".", "stopping_criterion", "\n", "assert", "metric", "in", "scores", ",", "metric", "\n", "factor", "=", "1", "if", "biggest", "else", "-", "1", "\n", "if", "factor", "*", "scores", "[", "metric", "]", ">", "factor", "*", "self", ".", "best_stopping_criterion", ":", "\n", "                ", "self", ".", "best_stopping_criterion", "=", "scores", "[", "metric", "]", "\n", "logger", ".", "info", "(", "\"New best validation score: %f\"", "%", "self", ".", "best_stopping_criterion", ")", "\n", "self", ".", "decrease_counts", "=", "0", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"Not a better validation score (%i / %i).\"", "\n", "%", "(", "self", ".", "decrease_counts", ",", "self", ".", "decrease_counts_max", ")", ")", "\n", "self", ".", "decrease_counts", "+=", "1", "\n", "", "if", "self", ".", "decrease_counts", ">", "self", ".", "decrease_counts_max", ":", "\n", "                ", "logger", ".", "info", "(", "\"Stopping criterion has been below its best value for more \"", "\n", "\"than %i epochs. Ending the experiment...\"", "%", "self", ".", "decrease_counts_max", ")", "\n", "if", "self", ".", "params", ".", "multi_gpu", "and", "'SLURM_JOB_ID'", "in", "os", ".", "environ", ":", "\n", "                    ", "os", ".", "system", "(", "'scancel '", "+", "os", ".", "environ", "[", "'SLURM_JOB_ID'", "]", ")", "\n", "", "exit", "(", ")", "\n", "", "", "self", ".", "save_checkpoint", "(", "'checkpoint'", ",", "include_optimizers", "=", "True", ")", "\n", "self", ".", "epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.round_batch": [[689, 728], ["len", "torch.cat.size", "lengths.max().item", "torch.cat", "len", "torch.randperm", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "torch.cat.size", "lengths.max", "torch.LongTensor().fill_", "[].expand", "torch.LongTensor", "torch.arange"], "methods", ["None"], ["", "def", "round_batch", "(", "self", ",", "x", ",", "lengths", ",", "positions", ",", "langs", ")", ":", "\n", "        ", "\"\"\"\n        For float16 only.\n        Sub-sample sentences in a batch, and add padding,\n        so that each dimension is a multiple of 8.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "if", "not", "params", ".", "fp16", "or", "len", "(", "lengths", ")", "<", "8", ":", "\n", "            ", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "None", "\n", "\n", "# number of sentences == 0 [8]", "\n", "", "bs1", "=", "len", "(", "lengths", ")", "\n", "bs2", "=", "8", "*", "(", "bs1", "//", "8", ")", "\n", "assert", "bs2", ">", "0", "and", "bs2", "%", "8", "==", "0", "\n", "if", "bs1", "!=", "bs2", ":", "\n", "            ", "idx", "=", "torch", ".", "randperm", "(", "bs1", ")", "[", ":", "bs2", "]", "\n", "lengths", "=", "lengths", "[", "idx", "]", "\n", "slen", "=", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "x", "=", "x", "[", ":", "slen", ",", "idx", "]", "\n", "positions", "=", "None", "if", "positions", "is", "None", "else", "positions", "[", ":", "slen", ",", "idx", "]", "\n", "langs", "=", "None", "if", "langs", "is", "None", "else", "langs", "[", ":", "slen", ",", "idx", "]", "\n", "", "else", ":", "\n", "            ", "idx", "=", "None", "\n", "\n", "# sequence length == 0 [8]", "\n", "", "ml1", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "ml1", "%", "8", "!=", "0", ":", "\n", "            ", "pad", "=", "8", "-", "(", "ml1", "%", "8", ")", "\n", "ml2", "=", "ml1", "+", "pad", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "torch", ".", "LongTensor", "(", "pad", ",", "bs2", ")", ".", "fill_", "(", "params", ".", "pad_index", ")", "]", ",", "0", ")", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "torch", ".", "cat", "(", "[", "positions", ",", "torch", ".", "arange", "(", "pad", ")", "[", ":", ",", "None", "]", "+", "positions", "[", "-", "1", "]", "[", "None", "]", "+", "1", "]", ",", "0", ")", "\n", "", "if", "langs", "is", "not", "None", ":", "\n", "                ", "langs", "=", "torch", ".", "cat", "(", "[", "langs", ",", "langs", "[", "-", "1", "]", "[", "None", "]", ".", "expand", "(", "pad", ",", "bs2", ")", "]", ",", "0", ")", "\n", "", "assert", "x", ".", "size", "(", ")", "==", "(", "ml2", ",", "bs2", ")", "\n", "\n", "", "assert", "x", ".", "size", "(", "0", ")", "%", "8", "==", "0", "\n", "assert", "x", ".", "size", "(", "1", ")", "%", "8", "==", "0", "\n", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.clm_step": [[729, 768], ["getattr", "getattr.train", "trainer.Trainer.generate_batch", "trainer.Trainer.round_batch", "torch.arange", "x[].masked_select", "utils.to_cuda", "getattr.", "getattr.", "trainer.Trainer.stats[].append", "trainer.Trainer.optimize", "lengths.size", "pred_mask.sum().item", "lengths.max", "pred_mask.sum().item", "x[].masked_select.size", "loss.item", "pred_mask.sum", "pred_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.generate_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "clm_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Next word prediction step (causal prediction).\n        CLM objective.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'decoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# generate batch / select words to predict", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "generate_batch", "(", "lang1", ",", "lang2", ",", "'causal'", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "alen", "=", "torch", ".", "arange", "(", "lengths", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "lengths", "[", "None", "]", "-", "1", "\n", "if", "params", ".", "context_size", ">", "0", ":", "# do not predict without context", "\n", "            ", "pred_mask", "[", ":", "params", ".", "context_size", "]", "=", "0", "\n", "", "y", "=", "x", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "y", ".", "size", "(", "0", ")", "\n", "\n", "# cuda", "\n", "x", ",", "lengths", ",", "langs", ",", "pred_mask", ",", "y", "=", "to_cuda", "(", "x", ",", "lengths", ",", "langs", ",", "pred_mask", ",", "y", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "langs", "=", "langs", ",", "causal", "=", "True", ")", "\n", "_", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'CLM-%s'", "%", "lang1", ")", "if", "lang2", "is", "None", "else", "(", "'CLM-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "lengths", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.mlm_step": [[769, 803], ["getattr", "getattr.train", "trainer.Trainer.generate_batch", "trainer.Trainer.round_batch", "trainer.Trainer.mask_out", "utils.to_cuda", "getattr.", "getattr.", "trainer.Trainer.stats[].append", "trainer.Trainer.optimize", "lengths.size", "pred_mask.sum().item", "loss.item", "pred_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.generate_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.mask_out", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "mlm_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Masked word prediction step.\n        MLM objective is lang2 is None, TLM objective otherwise.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'encoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# generate batch / select words to predict", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "generate_batch", "(", "lang1", ",", "lang2", ",", "'pred'", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "x", ",", "y", ",", "pred_mask", "=", "self", ".", "mask_out", "(", "x", ",", "lengths", ")", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "\n", "_", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'MLM-%s'", "%", "lang1", ")", "if", "lang2", "is", "None", "else", "(", "'MLM-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "lengths", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.Trainer.pc_step": [[804, 859], ["getattr", "getattr.train", "trainer.Trainer.get_batch", "len1.size", "torch.LongTensor().random_", "torch.arange", "utils.concat_batches", "trainer.Trainer.round_batch", "utils.to_cuda", "torch.nn.functional.linear", "torch.nn.functional.binary_cross_entropy_with_logits", "trainer.Trainer.stats[].append", "trainer.Trainer.optimize", "lengths.sum().item", "getattr.", "emb[].unsqueeze", "torch.nn.functional.linear.view", "torch.LongTensor().random_.to().type_as", "torch.nn.functional.binary_cross_entropy_with_logits.item", "torch.LongTensor", "torch.LongTensor().random_", "lengths.sum", "torch.LongTensor().random_.to", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "pc_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Parallel classification step. Predict if pairs of sentences are mutual translations of each other.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'encoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# sample parallel sentences", "\n", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "'align'", ",", "lang1", ",", "lang2", ")", "\n", "bs", "=", "len1", ".", "size", "(", "0", ")", "\n", "if", "bs", "==", "1", ":", "# can happen (although very rarely), which makes the negative loss fail", "\n", "            ", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "return", "\n", "\n", "# associate lang1 sentences with their translations, and random lang2 sentences", "\n", "", "y", "=", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "2", ")", "\n", "idx_pos", "=", "torch", ".", "arange", "(", "bs", ")", "\n", "idx_neg", "=", "(", "(", "idx_pos", "+", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "1", ",", "bs", ")", ")", "%", "bs", ")", "\n", "idx", "=", "(", "y", "==", "1", ")", ".", "long", "(", ")", "*", "idx_pos", "+", "(", "y", "==", "0", ")", ".", "long", "(", ")", "*", "idx_neg", "\n", "x2", ",", "len2", "=", "x2", "[", ":", ",", "idx", "]", ",", "len2", "[", "idx", "]", "\n", "\n", "# generate batch / cuda", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "new_idx", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "if", "new_idx", "is", "not", "None", ":", "\n", "            ", "y", "=", "y", "[", "new_idx", "]", "\n", "", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# get sentence embeddings", "\n", "h", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "[", "0", "]", "\n", "\n", "# parallel classification loss", "\n", "CLF_ID1", ",", "CLF_ID2", "=", "8", ",", "9", "# very hacky, use embeddings to make weights for the classifier", "\n", "emb", "=", "(", "model", ".", "module", "if", "params", ".", "multi_gpu", "else", "model", ")", ".", "embeddings", ".", "weight", "\n", "pred", "=", "F", ".", "linear", "(", "h", ",", "emb", "[", "CLF_ID1", "]", ".", "unsqueeze", "(", "0", ")", ",", "emb", "[", "CLF_ID2", ",", "0", "]", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "pred", ".", "view", "(", "-", "1", ")", ",", "y", ".", "to", "(", "pred", ".", "device", ")", ".", "type_as", "(", "pred", ")", ")", "\n", "self", ".", "stats", "[", "'PC-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "bs", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "lengths", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.SingleTrainer.__init__": [[863, 872], ["trainer.Trainer.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "data", ",", "params", ")", ":", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'model'", "]", "\n", "\n", "# model / data / params", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.__init__": [[876, 894], ["trainer.Trainer.__init__", "getattr", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "data", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", "]", "\n", "\n", "# model / data / params", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "\n", "if", "getattr", "(", "params", ",", "'bt_sync'", ",", "-", "1", ")", ">", "1", ":", "\n", "# self.encoder_clone, self.decoder_clone = build_model(params, data['dico'])", "\n", "            ", "self", ".", "encoder_clone", ",", "self", ".", "decoder_clone", "=", "copy", ".", "deepcopy", "(", "self", ".", "encoder", ")", ",", "copy", ".", "deepcopy", "(", "self", ".", "decoder", ")", "\n", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", ",", "'encoder_clone'", ",", "'decoder_clone'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_clone", ",", "self", ".", "decoder_clone", "=", "None", ",", "None", "\n", "", "super", "(", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "self", ".", "generate_mbeam_func", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.save_checkpoint": [[895, 928], ["os.path.join", "logger.info", "torch.save", "logger.warning", "getattr().state_dict", "apex.amp.state_dict", "trainer.EncDecTrainer.optimizers.keys", "logger.warning", "trainer.EncDecTrainer.optimizers[].state_dict", "trainer.EncDecTrainer.params.__dict__.items", "getattr"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "name", ",", "include_optimizers", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n                Save the model / checkpoints.\n                \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'%s.pth'", "%", "name", ")", "\n", "logger", ".", "info", "(", "\"Saving %s to %s ...\"", "%", "(", "name", ",", "path", ")", ")", "\n", "\n", "data", "=", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'n_total_iter'", ":", "self", ".", "n_total_iter", ",", "\n", "'best_metrics'", ":", "self", ".", "best_metrics", ",", "\n", "'best_stopping_criterion'", ":", "self", ".", "best_stopping_criterion", ",", "\n", "}", "\n", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Saving {} parameters ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "state_dict", "(", ")", "\n", "\n", "", "if", "include_optimizers", ":", "\n", "            ", "data", "[", "'amp'", "]", "=", "apex", ".", "amp", ".", "state_dict", "(", ")", "\n", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Saving {} optimizer ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "", "data", "[", "'dico_id2word'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "id2word", "\n", "data", "[", "'dico_word2id'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "word2id", "\n", "data", "[", "'dico_counts'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "counts", "\n", "data", "[", "'params'", "]", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "\n", "torch", ".", "save", "(", "data", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.reload_checkpoint": [[929, 993], ["os.path.join", "logger.warning", "torch.load", "trainer.EncDecTrainer.optimizers.keys", "logger.warning", "os.path.isfile", "logger.info", "apex.amp.load_state_dict", "logger.info", "os.path.isfile", "getattr().load_state_dict", "logger.warning", "trainer.EncDecTrainer.optimizers[].load_state_dict", "logger.warning", "enumerate", "print", "all", "logger.warning", "trainer.EncDecTrainer.optimizers[].get_lr_for_step", "getattr", "getattr().load_state_dict", "logger.warning", "k.startswith", "print", "getattr().load_state_dict", "reload.keys", "reload.items", "getattr", "any", "len", "getattr", "k.startswith", "reload.items", "reload.keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step"], ["", "def", "reload_checkpoint", "(", "self", ")", ":", "\n", "        ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'checkpoint.pth'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "reload_checkpoint", "==", "''", ":", "\n", "                ", "return", "\n", "", "else", ":", "\n", "                ", "checkpoint_path", "=", "self", ".", "params", ".", "reload_checkpoint", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", "\n", "", "", "logger", ".", "warning", "(", "\"Reloading checkpoint from {} ...\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "data", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "# reload model parameters", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "if", "\"_clone\"", "in", "name", "or", "\"bertscore_model\"", "in", "name", ":", "\n", "# fixme: quick fix", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "data", "[", "name", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "'Reload error: name={}, try removing \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                    ", "reload", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "try", ":", "\n", "                    ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "", "except", "Exception", "as", "ee", ":", "\n", "                    ", "print", "(", "'Reload error again: name={}, try adding \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "not", "any", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                        ", "reload", "=", "{", "'module.{}'", ".", "format", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "# raise e", "\n", "\n", "# reload optimizers", "\n", "", "", "", "if", "self", ".", "params", ".", "amp", ">", "0", "and", "\"amp\"", "in", "data", ":", "\n", "            ", "assert", "\"amp\"", "in", "data", ",", "'amp must be in data for amp={}'", ".", "format", "(", "self", ".", "params", ".", "amp", ")", "\n", "logger", ".", "info", "(", "'Load AMP parameters...'", ")", "\n", "apex", ".", "amp", ".", "load_state_dict", "(", "data", "[", "'amp'", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'!!! amp not in data, so optimizer parameters will not be loaded'", ")", "\n", "\n", "", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "\n", "            ", "if", "\"amp\"", "in", "data", ":", "# AMP checkpoint reloading is buggy, we cannot do that - TODO: fix - https://github.com/NVIDIA/apex/issues/250", "\n", "                ", "logger", ".", "warning", "(", "\"Reloading checkpoint optimizer {} ...\"", ".", "format", "(", "name", ")", ")", "\n", "self", ".", "optimizers", "[", "name", "]", ".", "load_state_dict", "(", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", ")", "\n", "", "else", ":", "# instead, we only reload current iterations / learning rates", "\n", "                ", "logger", ".", "warning", "(", "\"Not reloading checkpoint optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "for", "group_id", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizers", "[", "name", "]", ".", "param_groups", ")", ":", "\n", "                    ", "if", "'num_updates'", "not", "in", "param_group", ":", "\n", "                        ", "logger", ".", "warning", "(", "\"No 'num_updates' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "logger", ".", "warning", "(", "\"Reloading 'num_updates' and 'lr' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "param_group", "[", "'num_updates'", "]", "=", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "[", "'param_groups'", "]", "[", "group_id", "]", "[", "\n", "'num_updates'", "]", "\n", "param_group", "[", "'lr'", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "get_lr_for_step", "(", "param_group", "[", "'num_updates'", "]", ")", "\n", "\n", "# reload main metrics", "\n", "", "", "", "self", ".", "epoch", "=", "data", "[", "'epoch'", "]", "+", "1", "\n", "self", ".", "n_total_iter", "=", "data", "[", "'n_total_iter'", "]", "\n", "self", ".", "best_metrics", "=", "data", "[", "'best_metrics'", "]", "\n", "self", ".", "best_stopping_criterion", "=", "data", "[", "'best_stopping_criterion'", "]", "\n", "logger", ".", "warning", "(", "\n", "\"Checkpoint reloaded. Resuming at epoch {} / iteration {} ...\"", ".", "format", "(", "self", ".", "epoch", ",", "self", ".", "n_total_iter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.update_syn_model": [[994, 1004], ["logger.info", "trainer.EncDecTrainer.encoder_clone.load_state_dict", "trainer.EncDecTrainer.decoder_clone.load_state_dict", "copy.deepcopy", "trainer.EncDecTrainer.encoder_clone.module.load_state_dict", "copy.deepcopy", "trainer.EncDecTrainer.decoder_clone.module.load_state_dict", "trainer.EncDecTrainer.encoder.state_dict", "copy.deepcopy", "trainer.EncDecTrainer.decoder.state_dict", "copy.deepcopy", "trainer.EncDecTrainer.encoder.module.state_dict", "trainer.EncDecTrainer.decoder.module.state_dict"], "methods", ["None"], ["", "def", "update_syn_model", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Update Cloned Model (iter={})'", ".", "format", "(", "self", ".", "n_iter", ")", ")", "\n", "try", ":", "\n", "            ", "self", ".", "encoder_clone", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "self", ".", "encoder", ".", "state_dict", "(", ")", ")", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "encoder_clone", ".", "module", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "self", ".", "encoder", ".", "module", ".", "state_dict", "(", ")", ")", ")", "\n", "", "try", ":", "\n", "            ", "self", ".", "decoder_clone", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "self", ".", "decoder", ".", "state_dict", "(", ")", ")", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "decoder_clone", ".", "module", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "self", ".", "decoder", ".", "module", ".", "state_dict", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.mt_step": [[1006, 1059], ["trainer.EncDecTrainer.encoder.train", "trainer.EncDecTrainer.decoder.train", "x1.clone().fill_", "x2.clone().fill_", "torch.arange", "x2[].masked_select", "utils.to_cuda", "trainer.EncDecTrainer.encoder", "enc1.transpose.transpose.transpose", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.stats[].append", "trainer.EncDecTrainer.optimize", "len2.size", "trainer.EncDecTrainer.get_batch", "trainer.EncDecTrainer.add_noise", "trainer.EncDecTrainer.get_batch", "len2.max", "len", "loss.item", "x1.clone", "x2.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.add_noise", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch"], ["", "", "def", "mt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Machine translation step.\n        Can also be used for denoising auto-encoding.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate batch", "\n", "if", "lang1", "==", "lang2", ":", "\n", "            ", "(", "x1", ",", "len1", ")", "=", "self", ".", "get_batch", "(", "'ae'", ",", "lang1", ")", "\n", "(", "x2", ",", "len2", ")", "=", "(", "x1", ",", "len1", ")", "\n", "(", "x1", ",", "len1", ")", "=", "self", ".", "add_noise", "(", "x1", ",", "len1", ")", "\n", "", "else", ":", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "'mt'", ",", "lang1", ",", "lang2", ")", "\n", "", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# target words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# decode target sentence", "\n", "dec2", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'AE-%s'", "%", "lang1", ")", "if", "lang1", "==", "lang2", "else", "(", "'MT-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len2", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.bt_step": [[1060, 1138], ["getattr", "getattr", "getattr", "trainer.EncDecTrainer.get_batch", "x1.clone().fill_", "utils.to_cuda", "trainer.EncDecTrainer.encoder", "enc2.transpose.transpose.transpose", "torch.arange", "x1[].masked_select", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.stats[].append", "trainer.EncDecTrainer.optimize", "len1.size", "torch.no_grad", "trainer.EncDecTrainer.encoder.eval", "trainer.EncDecTrainer.decoder.eval", "_encoder", "enc1.transpose.transpose.transpose", "x2.clone().fill_", "trainer.EncDecTrainer.encoder.train", "trainer.EncDecTrainer.decoder.train", "len1.max", "loss.item", "x1.clone", "_decoder.generate_nucleus_sampling", "_decoder.generate_topk_sampling", "_decoder.generate", "x2.clone", "int", "int", "int", "len1.max().item", "len1.max().item", "len1.max().item", "len1.max", "len1.max", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_nucleus_sampling", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_topk_sampling", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate"], ["", "def", "bt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Back-translation step for machine translation.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "sampling_topp", "=", "getattr", "(", "params", ",", "'sampling_topp'", ",", "-", "1", ")", "\n", "sampling_topk", "=", "getattr", "(", "params", ",", "'sampling_topk'", ",", "-", "1", ")", "\n", "topk_ample_temperature", "=", "getattr", "(", "params", ",", "'topk_ample_temperature'", ",", "None", ")", "\n", "\n", "_encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "_decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate source batch", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# generate a translation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# evaluation mode", "\n", "            ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "\n", "# encode source sentence and translate it", "\n", "enc1", "=", "_encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "sampling_topp", ">", "0", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder", ".", "generate_nucleus_sampling", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ",", "sampling_topp", "=", "sampling_topp", ")", "\n", "", "elif", "sampling_topk", ">", "0", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder", ".", "generate_topk_sampling", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ",", "sampling_topk", "=", "sampling_topk", ",", "\n", "sample_temperature", "=", "topk_ample_temperature", "\n", ")", "\n", "", "else", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# free CUDA memory", "\n", "del", "enc1", "\n", "\n", "# training mode", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "# encode generate sentence", "\n", "", "enc2", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len1", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len1", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len1", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y1", "=", "x1", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "\n", "# decode original sentence", "\n", "dec3", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "True", ",", "src_enc", "=", "enc2", ",", "src_len", "=", "len2", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec3", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y1", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecTrainer.bt_sync_step": [[1139, 1223], ["getattr", "getattr", "getattr", "trainer.EncDecTrainer.get_batch", "x1.clone().fill_", "utils.to_cuda", "trainer.EncDecTrainer.encoder", "enc2.transpose.transpose.transpose", "torch.arange", "x1[].masked_select", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.decoder", "trainer.EncDecTrainer.stats[].append", "trainer.EncDecTrainer.optimize", "len1.size", "torch.no_grad", "trainer.EncDecTrainer.encoder_clone.eval", "trainer.EncDecTrainer.decoder_clone.eval", "_encoder_clone", "enc1.transpose.transpose.transpose", "x2.clone().fill_", "len1.max", "loss.item", "x1.clone", "_decoder_clone.generate_nucleus_sampling", "_decoder.generate_topk_sampling", "_decoder_clone.generate", "x2.clone", "int", "int", "int", "len1.max().item", "len1.max().item", "len1.max().item", "len1.max", "len1.max", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_nucleus_sampling", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_topk_sampling", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate"], ["", "def", "bt_sync_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Back-translation step for machine translation.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "sampling_topp", "=", "getattr", "(", "params", ",", "'sampling_topp'", ",", "-", "1", ")", "\n", "sampling_topk", "=", "getattr", "(", "params", ",", "'sampling_topk'", ",", "-", "1", ")", "\n", "topk_ample_temperature", "=", "getattr", "(", "params", ",", "'topk_ample_temperature'", ",", "None", ")", "\n", "_encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "_decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "assert", "self", ".", "encoder_clone", "is", "not", "None", "\n", "assert", "self", ".", "decoder_clone", "is", "not", "None", "\n", "try", ":", "\n", "            ", "_encoder_clone", "=", "self", ".", "encoder_clone", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_clone", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "            ", "_encoder_clone", "=", "self", ".", "encoder_clone", "\n", "", "try", ":", "\n", "            ", "_decoder_clone", "=", "self", ".", "decoder_clone", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_clone", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "            ", "_decoder_clone", "=", "self", ".", "decoder_clone", "\n", "\n", "", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate source batch", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# generate a translation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# evaluation mode", "\n", "            ", "self", ".", "encoder_clone", ".", "eval", "(", ")", "\n", "self", ".", "decoder_clone", ".", "eval", "(", ")", "\n", "\n", "# encode source sentence and translate it", "\n", "enc1", "=", "_encoder_clone", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "sampling_topp", ">", "0", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder_clone", ".", "generate_nucleus_sampling", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ",", "sampling_topp", "=", "sampling_topp", ")", "\n", "", "elif", "sampling_topk", ">", "0", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder", ".", "generate_topk_sampling", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ",", "sampling_topk", "=", "sampling_topk", ",", "\n", "sample_temperature", "=", "topk_ample_temperature", "\n", ")", "\n", "", "else", ":", "\n", "                ", "x2", ",", "len2", "=", "_decoder_clone", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# free CUDA memory", "\n", "del", "enc1", "\n", "\n", "# encode generate sentence", "\n", "", "enc2", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len1", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len1", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len1", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y1", "=", "x1", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "\n", "# decode original sentence", "\n", "dec3", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "True", ",", "src_enc", "=", "enc2", ",", "src_len", "=", "len2", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec3", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y1", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer.__init__": [[1232, 1251], ["trainer.EncDecTrainer.__init__", "getattr", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "encoder_para", ",", "decoder_para", ",", "data", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", ",", "'encoder_para'", ",", "'decoder_para'", "]", "\n", "\n", "# model / data / params", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "encoder_para", "=", "encoder_para", "\n", "self", ".", "decoder_para", "=", "decoder_para", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "\n", "if", "getattr", "(", "params", ",", "'bt_sync'", ",", "-", "1", ")", ">", "1", ":", "\n", "# self.encoder_clone, self.decoder_clone = build_model(params, data['dico'])", "\n", "            ", "self", ".", "encoder_clone", ",", "self", ".", "decoder_clone", "=", "copy", ".", "deepcopy", "(", "self", ".", "encoder", ")", ",", "copy", ".", "deepcopy", "(", "self", ".", "decoder", ")", "\n", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", ",", "'encoder_clone'", ",", "'decoder_clone'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_clone", ",", "self", ".", "decoder_clone", "=", "None", ",", "None", "\n", "", "super", "(", "EncDecTrainer", ",", "self", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer.para_bt_step": [[1252, 1326], ["trainer.EncDecParaPretrainedTrainer.encoder.train", "trainer.EncDecParaPretrainedTrainer.decoder.train", "trainer.EncDecParaPretrainedTrainer.encoder_para.eval", "trainer.EncDecParaPretrainedTrainer.decoder_para.eval", "trainer.EncDecParaPretrainedTrainer.get_batch", "x1.clone().fill_", "utils.to_cuda", "trainer.EncDecParaPretrainedTrainer.encoder", "enc2.transpose.transpose.transpose", "torch.arange", "x1[].masked_select", "trainer.EncDecParaPretrainedTrainer.decoder", "trainer.EncDecParaPretrainedTrainer.decoder", "trainer.EncDecParaPretrainedTrainer.stats[].append", "trainer.EncDecParaPretrainedTrainer.optimize", "len1.size", "torch.no_grad", "_encoder_para", "enc1.transpose.transpose.transpose", "_decoder_para.generate", "x2.clone().fill_", "len1.max", "loss.item", "x1.clone", "int", "x2.clone", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate"], ["", "def", "para_bt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n                Back-translation step for machine translation.\n                \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "_encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "_decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "_encoder_para", "=", "self", ".", "encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_para", "\n", "_decoder_para", "=", "self", ".", "decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_para", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate source batch", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# generate a translation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# evaluation mode", "\n", "# self.encoder.eval()", "\n", "# self.decoder.eval()", "\n", "# self.encoder_para.eval()", "\n", "# self.decoder_para.eval()", "\n", "\n", "# encode source sentence and translate it", "\n", "            ", "enc1", "=", "_encoder_para", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "x2", ",", "len2", "=", "_decoder_para", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# free CUDA memory", "\n", "del", "enc1", "\n", "\n", "# training mode", "\n", "# self.encoder.train()", "\n", "# self.decoder.train()", "\n", "\n", "# encode generate sentence", "\n", "", "enc2", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len1", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len1", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len1", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y1", "=", "x1", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "\n", "# decode original sentence", "\n", "dec3", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "True", ",", "src_enc", "=", "enc2", ",", "src_len", "=", "len2", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec3", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y1", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'2TBT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_prob": [[1327, 1356], ["torch.cat().mean", "enc", "enc.transpose", "dec", "dec", "sc.gather", "zip", "zip", "x2[].unsqueeze", "torch.cat"], "methods", ["None"], ["", "def", "_dual_prob", "(", "self", ",", "encoders", ",", "decoders", ",", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ")", ":", "\n", "# alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)", "\n", "# pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word", "\n", "\n", "# y = x2[1:].masked_select(pred_mask[:-1])", "\n", "# assert len(y) == (len2 - 1).sum().item()", "\n", "\n", "        ", "encs1", "=", "[", "enc", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "for", "enc", "in", "encoders", "]", "\n", "encs1", "=", "[", "enc", ".", "transpose", "(", "0", ",", "1", ")", "for", "enc", "in", "encs1", "]", "\n", "\n", "decs2", "=", "[", "dec", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "for", "enc1", ",", "dec", "in", "zip", "(", "encs1", ",", "decoders", ")", "]", "\n", "\n", "# desc [len, b, dim)", "\n", "# scores_ = [F.softmax(dec.pred_layer.proj(mt[:-1]), -1) for dec, mt in zip(decs2, decoders)]", "\n", "scores_", "=", "[", "dec", "(", "'predict'", ",", "tensor", "=", "mt", "[", ":", "-", "1", "]", ",", "pred_mask", "=", "None", ",", "y", "=", "None", ",", "get_scores", "=", "False", ",", "softmax", "=", "True", ")", "\n", "for", "mt", ",", "dec", "in", "zip", "(", "decs2", ",", "decoders", ")", "]", "\n", "probs", "=", "[", "sc", ".", "gather", "(", "-", "1", ",", "x2", "[", "1", ":", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "for", "sc", "in", "scores_", "]", "\n", "# probs [len, b, 1]", "\n", "prob", "=", "torch", ".", "cat", "(", "probs", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", "\n", "\n", "# masked_t = [tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim) for tensor in decs2]", "\n", "#", "\n", "# scores_ = [F.softmax(dec.proj(mt).view(-1, dec.n_words), -1) for dec, mt in zip(masked_t, decoders)]", "\n", "# probs = [sc.gather(-1, y.unsqueeze(-1)).unsqueeze(-1) for sc in scores_]", "\n", "# prob = torch.cat(probs, -1).mean(-1)", "\n", "\n", "\n", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_loss": [[1357, 1374], ["torch.arange", "x2[].masked_select", "trainer.EncDecParaPretrainedTrainer._dual_prob", "len2.max", "len", "trainer.EncDecParaPretrainedTrainer.log().masked_fill_", "trainer.EncDecParaPretrainedTrainer.log"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_prob"], ["", "def", "_dual_loss", "(", "self", ",", "encoders", ",", "decoders", ",", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ")", ":", "\n", "        ", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "prob", "=", "self", ".", "_dual_prob", "(", "encoders", ",", "decoders", ",", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ")", "\n", "loss", "=", "-", "prob", ".", "log", "(", ")", ".", "masked_fill_", "(", "pred_mask", "[", ":", "-", "1", "]", ",", "0", ")", "\n", "\n", "# masked_t = [tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim) for tensor in decs2]", "\n", "# scores_ = [F.softmax(dec.proj(mt).view(-1, dec.n_words), -1) for dec, mt in zip(masked_t, decoders)]", "\n", "# probs = [sc.gather(-1, y.unsqueeze(-1)).unsqueeze(-1) for sc in scores_]", "\n", "# prob = torch.cat(probs, -1).mean(-1)", "\n", "# projs = [dec.proj(mt).view(-1, dec.n_words) for dec, mt in zip(masked_t, decoders)]", "\n", "# log(1/2(p1 + p2))", "\n", "# log(1/2(exp(h1)/sumexp(h1) + exp(h2)/sumexp(h2)))", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_single_prob": [[1375, 1389], ["torch.arange", "encoder", "enc1.transpose.transpose.transpose", "decoder", "decoder", "decoder.gather().masked_fill_", "len2.max", "decoder.gather", "pred_mask[].unsqueeze", "x2[].unsqueeze"], "methods", ["None"], ["", "def", "_dual_single_prob", "(", "self", ",", "encoder", ",", "decoder", ",", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ")", ":", "\n", "        ", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "dec2", "=", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "\n", "# desc [len, b, dim)", "\n", "scores_", "=", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", "[", ":", "-", "1", "]", ",", "pred_mask", "=", "None", ",", "y", "=", "None", ",", "get_scores", "=", "False", ",", "softmax", "=", "True", ")", "\n", "probs", "=", "scores_", ".", "gather", "(", "-", "1", ",", "x2", "[", "1", ":", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "masked_fill_", "(", "~", "pred_mask", "[", ":", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", ",", "1", ")", "\n", "# probs = probs.prod(0, keepdim=True)", "\n", "# probs [1, b, 1]", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer.para_bt_dual_step": [[1390, 1483], ["trainer.EncDecParaPretrainedTrainer.encoder.train", "trainer.EncDecParaPretrainedTrainer.decoder.train", "trainer.EncDecParaPretrainedTrainer.encoder_para.eval", "trainer.EncDecParaPretrainedTrainer.decoder_para.eval", "trainer.EncDecParaPretrainedTrainer.get_batch", "x1.clone().fill_", "utils.to_cuda", "trainer.EncDecParaPretrainedTrainer.encoder.train", "trainer.EncDecParaPretrainedTrainer.decoder.train", "trainer.EncDecParaPretrainedTrainer._dual_single_prob", "trainer.EncDecParaPretrainedTrainer.stats[].append", "trainer.EncDecParaPretrainedTrainer.optimize", "len1.size", "torch.no_grad", "trainer.EncDecParaPretrainedTrainer.encoder.eval", "trainer.EncDecParaPretrainedTrainer.decoder.eval", "trainer.EncDecParaPretrainedTrainer.encoder_para.eval", "trainer.EncDecParaPretrainedTrainer.decoder_para.eval", "_encoder_para", "enc1.transpose.transpose.transpose", "_decoder_para.generate", "y1.clone().fill_", "trainer.EncDecParaPretrainedTrainer._dual_single_prob", "trainer.EncDecParaPretrainedTrainer._dual_single_prob", "torch.cat().mean().detach", "trainer.EncDecParaPretrainedTrainer.squeeze().detach", "trainer.EncDecParaPretrainedTrainer._dual_single_prob", "o_mp.detach.detach.detach", "torch.cat().mean().log", "loss.sum", "len1.sum", "loss.item", "x1.clone", "int", "y1.clone", "torch.cat().mean", "trainer.EncDecParaPretrainedTrainer.squeeze", "torch.cat().mean", "torch.cat", "torch.cat", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_single_prob", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_single_prob", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_single_prob", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.EncDecParaPretrainedTrainer._dual_single_prob"], ["", "def", "para_bt_dual_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ")", ":", "\n", "        ", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "_encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "_decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "_encoder_para", "=", "self", ".", "encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_para", "\n", "_decoder_para", "=", "self", ".", "decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_para", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "# x2, len2 = self.get_batch('bt', lang2)", "\n", "# langs2 = x2.clone().fill_(lang2_id)", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "# x2, len2, langs2 = to_cuda(x2, len2, langs2)", "\n", "\n", "# encoders = [_encoder, _encoder_para]", "\n", "# decoders = [_decoder, _decoder_para]", "\n", "encoders", "=", "[", "self", ".", "encoder", ",", "self", ".", "encoder_para", "]", "\n", "decoders", "=", "[", "self", ".", "decoder", ",", "self", ".", "decoder_para", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "\n", "enc1", "=", "_encoder_para", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "y1", ",", "y_len1", "=", "_decoder_para", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "y_langs1", "=", "y1", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# enc2 = _encoder_para('fwd', x=x2, lengths=len2, langs=langs2, causal=False)", "\n", "# enc2 = enc2.transpose(0, 1)", "\n", "# y2, y_len2 = _decoder_para.generate(enc2, len2, lang1_id, max_len=int(1.3 * len2.max().item() + 5))", "\n", "# y_langs2 = y2.clone().fill_(lang1_id)", "\n", "\n", "# free CUDA memory", "\n", "del", "enc1", "\n", "# del enc2", "\n", "\n", "# sigma_p = self._dual_prob(encoders, decoders, x1, len1, langs1, y1, y_len1, y_langs1)", "\n", "\n", "p_m", "=", "self", ".", "_dual_single_prob", "(", "self", ".", "encoder", ",", "self", ".", "decoder", ",", "x1", ",", "len1", ",", "langs1", ",", "y1", ",", "y_len1", ",", "y_langs1", ")", "\n", "p_mp", "=", "self", ".", "_dual_single_prob", "(", "self", ".", "encoder_para", ",", "self", ".", "decoder_para", ",", "x1", ",", "len1", ",", "langs1", ",", "y1", ",", "y_len1", ",", "y_langs1", ")", "\n", "# [len, b, 1]", "\n", "p_mmp", "=", "torch", ".", "cat", "(", "[", "p_m", ",", "p_mp", "]", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "p_mp_", "=", "p_mp", ".", "squeeze", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "sigma_p", "=", "(", "p_mmp", "/", "p_mp_", ")", ".", "prod", "(", "0", ",", "keepdim", "=", "True", ")", ".", "detach", "(", ")", "\n", "\n", "o_mp", "=", "self", ".", "_dual_single_prob", "(", "self", ".", "encoder_para", ",", "self", ".", "decoder_para", ",", "y1", ",", "y_len1", ",", "y_langs1", ",", "x1", ",", "len1", ",", "langs1", ")", "\n", "o_mp", "=", "o_mp", ".", "detach", "(", ")", "\n", "\n", "# sigma(x, y') = (p(y'|x; [m, mp]) / p(y'|x; [mp]) log p(x|y'; [m, mp])", "\n", "# phi  (x', y) = (p(x'|y; [m, mp]) / p(x'|y; [mp]) log p(y|x'; [m, mp])", "\n", "\n", "", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "# self.encoder_para.eval()", "\n", "# self.decoder_para.eval()", "\n", "\n", "o_m", "=", "self", ".", "_dual_single_prob", "(", "self", ".", "encoder", ",", "self", ".", "decoder", ",", "y1", ",", "y_len1", ",", "y_langs1", ",", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "loss", "=", "-", "torch", ".", "cat", "(", "[", "o_m", ",", "o_mp", "]", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", ".", "log", "(", ")", "\n", "\n", "# loss = self._dual_loss(encoders, decoders, y1, y_len1, y_langs1, x1, len1, langs1)", "\n", "\n", "# logger.info('loss sigma_p: {} {}'.format(loss.size(), sigma_p.size()))", "\n", "loss", "=", "loss", "*", "sigma_p", "\n", "loss", "=", "loss", ".", "sum", "(", ")", "/", "len1", ".", "sum", "(", ")", "\n", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.trainer.convert_to_text": [[1487, 1512], ["batch.cpu().numpy.cpu().numpy", "lengths.cpu().numpy.cpu().numpy", "range", "range", "sentences.append", "batch.cpu().numpy.cpu", "lengths.cpu().numpy.cpu", "lengths.cpu().numpy.max", "words.append", "logger.info"], "function", ["None"], ["", "", "def", "convert_to_text", "(", "batch", ",", "lengths", ",", "dico", ",", "params", ",", "nbest", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert a batch of sentences to a list of text sentences.\n    \"\"\"", "\n", "batch", "=", "batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "slen", ",", "bs", "=", "batch", ".", "shape", "\n", "assert", "nbest", "is", "not", "None", "or", "(", "lengths", ".", "max", "(", ")", "==", "slen", "and", "lengths", ".", "shape", "[", "0", "]", "==", "bs", ")", "\n", "assert", "(", "batch", "[", "0", "]", "==", "params", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "bs", "\n", "assert", "(", "batch", "==", "params", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "sentences", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "bs", ")", ":", "\n", "        ", "words", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "lengths", "[", "j", "]", ")", ":", "\n", "            ", "if", "batch", "[", "k", ",", "j", "]", "==", "params", ".", "eos_index", ":", "\n", "                ", "break", "\n", "", "try", ":", "\n", "                ", "words", ".", "append", "(", "dico", "[", "batch", "[", "k", ",", "j", "]", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "logger", ".", "info", "(", "'dico size: {}'", ".", "format", "(", "dico", ")", ")", "\n", "raise", "e", "\n", "", "", "sentences", ".", "append", "(", "\" \"", ".", "join", "(", "words", ")", ")", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.logger.LogFormatter.__init__": [[15, 17], ["time.time"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.logger.LogFormatter.format": [[18, 29], ["round", "record.getMessage", "message.replace.replace.replace", "time.strftime", "datetime.timedelta", "len"], "methods", ["None"], ["", "def", "format", "(", "self", ",", "record", ")", ":", "\n", "        ", "elapsed_seconds", "=", "round", "(", "record", ".", "created", "-", "self", ".", "start_time", ")", "\n", "\n", "prefix", "=", "\"%s - %s - %s\"", "%", "(", "\n", "record", ".", "levelname", ",", "\n", "time", ".", "strftime", "(", "'%x %X'", ")", ",", "\n", "timedelta", "(", "seconds", "=", "elapsed_seconds", ")", "\n", ")", "\n", "message", "=", "record", ".", "getMessage", "(", ")", "\n", "message", "=", "message", ".", "replace", "(", "'\\n'", ",", "'\\n'", "+", "' '", "*", "(", "len", "(", "prefix", ")", "+", "3", ")", ")", "\n", "return", "\"%s - %s\"", "%", "(", "prefix", ",", "message", ")", "if", "message", "else", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.logger.create_logger": [[31, 67], ["logger.LogFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.addHandler", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.getLogger.addHandler", "time.time"], "function", ["None"], ["", "", "def", "create_logger", "(", "filepath", ",", "rank", ")", ":", "\n", "    ", "\"\"\"\n    Create a logger.\n    Use a different log file for each process.\n    \"\"\"", "\n", "# create log formatter", "\n", "log_formatter", "=", "LogFormatter", "(", ")", "\n", "\n", "# create file handler and set level to debug", "\n", "if", "filepath", "is", "not", "None", ":", "\n", "        ", "if", "rank", ">", "0", ":", "\n", "            ", "filepath", "=", "'%s-%i'", "%", "(", "filepath", ",", "rank", ")", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "filepath", ",", "\"a\"", ")", "\n", "file_handler", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "file_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create console handler and set level to info", "\n", "", "console_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console_handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "console_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create logger and set level to debug", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "handlers", "=", "[", "]", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "if", "filepath", "is", "not", "None", ":", "\n", "        ", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "", "logger", ".", "addHandler", "(", "console_handler", ")", "\n", "\n", "# reset logger elapsed time", "\n", "def", "reset_time", "(", ")", ":", "\n", "        ", "log_formatter", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "logger", ".", "reset_time", "=", "reset_time", "\n", "\n", "return", "logger", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.__init__": [[33, 51], ["trainer.EncDecTrainer.__init__", "getattr"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "encoder1", ",", "decoder1", ",", "encoder2", ",", "decoder2", ",", "data", ",", "params", ")", ":", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", ",", "'encoder1'", ",", "'decoder1'", ",", "'encoder2'", ",", "'decoder2'", "]", "\n", "self", ".", "MODEL_NOUSE_NAMES", "=", "[", "'encoder1'", ",", "'decoder1'", ",", "'encoder2'", ",", "'decoder2'", "]", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "encoder1", "=", "encoder1", "\n", "self", ".", "decoder1", "=", "decoder1", "\n", "self", ".", "encoder2", "=", "encoder2", "\n", "self", ".", "decoder2", "=", "decoder2", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "# self.onl_bleu_scorer = SentBLEUV2(self.params.pad_index, self.params.eos_index, self.params.unk_index)", "\n", "if", "getattr", "(", "params", ",", "'bt_sync'", ",", "-", "1", ")", ">", "1", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_clone", ",", "self", ".", "decoder_clone", "=", "None", ",", "None", "\n", "\n", "", "super", "(", "EncDecTrainer", ",", "self", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "# probability of masking out / randomize / not modify words to predict", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.save_checkpoint": [[54, 90], ["os.path.join", "logger.info", "torch.save", "logger.warning", "getattr().state_dict", "apex.amp.state_dict", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimizers.keys", "logger.warning", "logger.warning", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimizers[].state_dict", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.params.__dict__.items", "getattr"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "name", ",", "include_optimizers", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n                Save the model / checkpoints.\n                \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'%s.pth'", "%", "name", ")", "\n", "logger", ".", "info", "(", "\"Saving %s to %s ...\"", "%", "(", "name", ",", "path", ")", ")", "\n", "\n", "data", "=", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'n_total_iter'", ":", "self", ".", "n_total_iter", ",", "\n", "'best_metrics'", ":", "self", ".", "best_metrics", ",", "\n", "'best_stopping_criterion'", ":", "self", ".", "best_stopping_criterion", ",", "\n", "}", "\n", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "if", "name", "in", "self", ".", "MODEL_NOUSE_NAMES", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Skip saving {} parameters ...\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "logger", ".", "warning", "(", "\"Saving {} parameters ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "state_dict", "(", ")", "\n", "\n", "", "if", "include_optimizers", ":", "\n", "            ", "data", "[", "'amp'", "]", "=", "apex", ".", "amp", ".", "state_dict", "(", ")", "\n", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Saving {} optimizer ...\"", ".", "format", "(", "name", ")", ")", "\n", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "", "data", "[", "'dico_id2word'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "id2word", "\n", "data", "[", "'dico_word2id'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "word2id", "\n", "data", "[", "'dico_counts'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "counts", "\n", "data", "[", "'params'", "]", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "\n", "torch", ".", "save", "(", "data", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.reload_checkpoint": [[91, 158], ["os.path.join", "logger.warning", "torch.load", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimizers.keys", "logger.warning", "os.path.isfile", "logger.info", "apex.amp.load_state_dict", "logger.info", "os.path.isfile", "logger.warning", "getattr().load_state_dict", "logger.warning", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimizers[].load_state_dict", "logger.warning", "enumerate", "print", "all", "logger.warning", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimizers[].get_lr_for_step", "getattr", "getattr().load_state_dict", "logger.warning", "k.startswith", "print", "getattr().load_state_dict", "reload.keys", "reload.items", "getattr", "any", "len", "getattr", "k.startswith", "reload.items", "reload.keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step"], ["", "def", "reload_checkpoint", "(", "self", ")", ":", "\n", "        ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'checkpoint.pth'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "reload_checkpoint", "==", "''", ":", "\n", "                ", "return", "\n", "", "else", ":", "\n", "                ", "checkpoint_path", "=", "self", ".", "params", ".", "reload_checkpoint", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", "\n", "", "", "logger", ".", "warning", "(", "\"Reloading checkpoint from {} ...\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "data", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "# reload model parameters", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "if", "name", "in", "self", ".", "MODEL_NOUSE_NAMES", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Skip loading {} parameters ...\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "if", "\"_clone\"", "in", "name", "or", "\"bertscore_model\"", "in", "name", ":", "\n", "# fixme: quick fix", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "data", "[", "name", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "'Reload error: name={}, try removing \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                    ", "reload", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "try", ":", "\n", "                    ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "", "except", "Exception", "as", "ee", ":", "\n", "                    ", "print", "(", "'Reload error again: name={}, try adding \"module.\"'", ".", "format", "(", "name", ")", ")", "\n", "reload", "=", "data", "[", "name", "]", "\n", "if", "not", "any", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                        ", "reload", "=", "{", "'module.{}'", ".", "format", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "reload", ".", "items", "(", ")", "}", "\n", "", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "reload", ")", "\n", "# raise e", "\n", "\n", "# reload optimizers", "\n", "", "", "", "if", "self", ".", "params", ".", "amp", ">", "0", "and", "\"amp\"", "in", "data", ":", "\n", "            ", "assert", "\"amp\"", "in", "data", ",", "'amp must be in data for amp={}'", ".", "format", "(", "self", ".", "params", ".", "amp", ")", "\n", "logger", ".", "info", "(", "'Load AMP parameters...'", ")", "\n", "apex", ".", "amp", ".", "load_state_dict", "(", "data", "[", "'amp'", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'!!! amp not in data, so optimizer parameters will not be loaded'", ")", "\n", "\n", "", "for", "name", "in", "self", ".", "optimizers", ".", "keys", "(", ")", ":", "\n", "\n", "            ", "if", "\"amp\"", "in", "data", ":", "# AMP checkpoint reloading is buggy, we cannot do that - TODO: fix - https://github.com/NVIDIA/apex/issues/250", "\n", "                ", "logger", ".", "warning", "(", "\"Reloading checkpoint optimizer {} ...\"", ".", "format", "(", "name", ")", ")", "\n", "self", ".", "optimizers", "[", "name", "]", ".", "load_state_dict", "(", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", ")", "\n", "", "else", ":", "# instead, we only reload current iterations / learning rates", "\n", "                ", "logger", ".", "warning", "(", "\"Not reloading checkpoint optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "for", "group_id", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizers", "[", "name", "]", ".", "param_groups", ")", ":", "\n", "                    ", "if", "'num_updates'", "not", "in", "param_group", ":", "\n", "                        ", "logger", ".", "warning", "(", "\"No 'num_updates' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "logger", ".", "warning", "(", "\"Reloading 'num_updates' and 'lr' for optimizer {}.\"", ".", "format", "(", "name", ")", ")", "\n", "param_group", "[", "'num_updates'", "]", "=", "data", "[", "'{}_optimizer'", ".", "format", "(", "name", ")", "]", "[", "'param_groups'", "]", "[", "group_id", "]", "[", "\n", "'num_updates'", "]", "\n", "param_group", "[", "'lr'", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "get_lr_for_step", "(", "param_group", "[", "'num_updates'", "]", ")", "\n", "\n", "# reload main metrics", "\n", "", "", "", "self", ".", "epoch", "=", "data", "[", "'epoch'", "]", "+", "1", "\n", "self", ".", "n_total_iter", "=", "data", "[", "'n_total_iter'", "]", "\n", "self", ".", "best_metrics", "=", "data", "[", "'best_metrics'", "]", "\n", "self", ".", "best_stopping_criterion", "=", "data", "[", "'best_stopping_criterion'", "]", "\n", "logger", ".", "warning", "(", "\n", "\"Checkpoint reloaded. Resuming at epoch {} / iteration {} ...\"", ".", "format", "(", "self", ".", "epoch", ",", "self", ".", "n_total_iter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs": [[159, 180], ["len1.size", "torch.arange", "fil_idx.unsqueeze().expand", "fil_idx.unsqueeze().expand", "_x1.clone().fill_", "_x2.clone().fill_", "_x1.size", "fil_idx.numel", "x1.size", "fil_idx.size", "x2.size", "fil_idx.size", "x1.gather", "x2.gather", "fil_idx.unsqueeze", "fil_idx.unsqueeze", "_len1.max", "_len2.max", "_x1.clone", "_x2.clone"], "methods", ["None"], ["", "def", "_filter_valid_pairs", "(", "self", ",", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "max_len", "=", "params", ".", "max_len", "\n", "# filter_bleu = getattr(params, 'filter_bleu', 0)", "\n", "bsz", "=", "len1", ".", "size", "(", "0", ")", "\n", "indices", "=", "torch", ".", "arange", "(", "bsz", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x1", ".", "device", ")", "\n", "fil_idx", "=", "indices", "[", "(", "len1", "<=", "max_len", ")", "&", "(", "len2", "<=", "max_len", ")", "&", "(", "len1", ">", "2", ")", "&", "(", "len2", ">", "2", ")", "]", "\n", "if", "fil_idx", ".", "numel", "(", ")", "==", "0", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "", "_len1", "=", "len1", "[", "fil_idx", "]", "\n", "_len2", "=", "len2", "[", "fil_idx", "]", "\n", "\n", "fil_idx1", "=", "fil_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "x1", ".", "size", "(", "0", ")", ",", "fil_idx", ".", "size", "(", "0", ")", ")", "\n", "fil_idx2", "=", "fil_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "x2", ".", "size", "(", "0", ")", ",", "fil_idx", ".", "size", "(", "0", ")", ")", "\n", "_x1", "=", "x1", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "fil_idx1", ")", "[", ":", "_len1", ".", "max", "(", ")", "]", "\n", "_x2", "=", "x2", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "fil_idx2", ")", "[", ":", "_len2", ".", "max", "(", ")", "]", "\n", "_langs1", "=", "_x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "_langs2", "=", "_x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "new_bsz", "=", "_x1", ".", "size", "(", "1", ")", "\n", "\n", "return", "_x1", ",", "_len1", ",", "_langs1", ",", "_x2", ",", "_len2", ",", "_langs2", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_long_sents": [[181, 194], ["len1.size", "torch.arange", "fil_idx.unsqueeze().expand", "fil_idx.numel", "x1.size", "fil_idx.size", "x1.gather", "fil_idx.unsqueeze", "_len1.max"], "methods", ["None"], ["", "def", "_filter_long_sents", "(", "self", ",", "x1", ",", "len1", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "max_len", "=", "params", ".", "max_len", "\n", "bsz", "=", "len1", ".", "size", "(", "0", ")", "\n", "indices", "=", "torch", ".", "arange", "(", "bsz", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x1", ".", "device", ")", "\n", "fil_idx", "=", "indices", "[", "len1", "<=", "max_len", "]", "\n", "if", "fil_idx", ".", "numel", "(", ")", "==", "0", ":", "\n", "            ", "return", "None", ",", "None", "\n", "", "_len1", "=", "len1", "[", "fil_idx", "]", "\n", "fil_idx1", "=", "fil_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "x1", ".", "size", "(", "0", ")", ",", "fil_idx", ".", "size", "(", "0", ")", ")", "\n", "_x1", "=", "x1", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "fil_idx1", ")", "[", ":", "_len1", ".", "max", "(", ")", "]", "\n", "# _langs1 = _x1.clone().fill_(lang1_id)", "\n", "return", "_x1", ",", "_len1", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.bt_step_macd": [[195, 320], ["float", "getattr", "getattr", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.get_batch", "x1.clone().fill_", "x1.size", "utils.to_cuda", "len1.size", "torch.no_grad", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.stats[].append", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize", "print", "torch.no_grad", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.stats[].append", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize", "print", "x1.clone", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.item", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.item", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "bt_step_macd", "(", "\n", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ",", "direction1", ",", "enforce_default", "=", "False", ",", "first_sec", "=", "False", ",", "wrote", "=", "False", ",", "\n", "and_rev", "=", "False", ",", "with_ae_xz", "=", "False", ",", "ae_xz_coeff", "=", "0", ",", "with_mass_xz", "=", "False", ",", "mass_xz_coeff", "=", "0", ",", "infer_drop", "=", "True", "\n", ")", ":", "\n", "        ", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "mass_xz_coeff", "=", "float", "(", "mass_xz_coeff", ")", "\n", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "nbest", "=", "params", ".", "nbest", "\n", "assert", "nbest", "is", "None", "or", "nbest", "<=", "1", "\n", "tokens_per_batch", "=", "params", ".", "tokens_per_batch", "\n", "beam_size", "=", "params", ".", "beam_size", "\n", "mbeam_size", "=", "getattr", "(", "params", ",", "'mbeam_size'", ",", "-", "1", ")", "\n", "select_opt", "=", "getattr", "(", "params", ",", "'select_opt'", ",", "0", ")", "\n", "# assert beam_size > 1", "\n", "# assert nbest is not None and 1 <= nbest <= beam_size", "\n", "\n", "if", "params", ".", "multi_gpu", ":", "\n", "            ", "infer_encoder1", "=", "self", ".", "encoder1", ".", "module", "\n", "infer_decoder1", "=", "self", ".", "decoder1", ".", "module", "\n", "infer_encoder2", "=", "self", ".", "encoder2", ".", "module", "\n", "infer_decoder2", "=", "self", ".", "decoder2", ".", "module", "\n", "", "else", ":", "\n", "            ", "infer_encoder1", "=", "self", ".", "encoder1", "\n", "infer_decoder1", "=", "self", ".", "decoder1", "\n", "infer_encoder2", "=", "self", ".", "encoder2", "\n", "infer_decoder2", "=", "self", ".", "decoder2", "\n", "\n", "", "if", "first_sec", ":", "\n", "            ", "first_encoder", "=", "infer_encoder2", "\n", "first_decoder", "=", "infer_decoder2", "\n", "sec_encoder", "=", "infer_encoder1", "\n", "sec_decoder", "=", "infer_decoder1", "\n", "", "else", ":", "\n", "            ", "first_encoder", "=", "infer_encoder1", "\n", "first_decoder", "=", "infer_decoder1", "\n", "sec_encoder", "=", "infer_encoder2", "\n", "sec_decoder", "=", "infer_decoder2", "\n", "\n", "", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "bsz", "=", "x1", ".", "size", "(", "1", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "if", "direction1", ":", "\n", "            ", "d1_lang1_id", "=", "lang1_id", "\n", "d1_lang2_id", "=", "lang2_id", "\n", "d2_lang1_id", "=", "lang1_id", "\n", "d2_lang2_id", "=", "lang2_id", "\n", "", "else", ":", "\n", "            ", "d1_lang1_id", "=", "lang2_id", "\n", "d1_lang2_id", "=", "lang1_id", "\n", "d2_lang1_id", "=", "lang2_id", "\n", "d2_lang2_id", "=", "lang1_id", "\n", "\n", "# todo: primary training", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x2", ",", "len2", ",", "langs2", "=", "self", ".", "_generate_pair", "(", "\n", "first_encoder", ",", "first_decoder", ",", "x1", ",", "len1", ",", "langs1", ",", "lang2_id", ",", "mbeam_size", "=", "mbeam_size", ",", "infer_drop", "=", "infer_drop", ")", "\n", "if", "direction1", ":", "\n", "# lang1->lang2", "\n", "                ", "d1_x1", ",", "d1_len1", ",", "d1_langs1", "=", "x1", ",", "len1", ",", "langs1", "\n", "d1_x2", ",", "d1_len2", ",", "d1_langs2", "=", "x2", ",", "len2", ",", "langs2", "\n", "", "else", ":", "\n", "# lang2->lang1", "\n", "                ", "d1_x1", ",", "d1_len1", ",", "d1_langs1", "=", "x2", ",", "len2", ",", "langs2", "\n", "d1_x2", ",", "d1_len2", ",", "d1_langs2", "=", "x1", ",", "len1", ",", "langs1", "\n", "", "d1_x1", ",", "d1_len1", ",", "d1_langs1", ",", "d1_x2", ",", "d1_len2", ",", "d1_langs2", "=", "self", ".", "_filter_valid_pairs", "(", "\n", "d1_x1", ",", "d1_len1", ",", "d1_lang1_id", ",", "d1_x2", ",", "d1_len2", ",", "d1_lang2_id", ")", "\n", "\n", "", "if", "d1_x1", "is", "not", "None", ":", "\n", "# try:", "\n", "            ", "loss1", "=", "self", ".", "_train_step_bt", "(", "d1_x1", ",", "d1_len1", ",", "d1_langs1", ",", "d1_x2", ",", "d1_len2", ",", "d1_langs2", ")", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss1", ".", "item", "(", ")", ")", "\n", "loss1", "=", "lambda_coeff", "*", "loss1", "\n", "self", ".", "optimize", "(", "loss1", ")", "\n", "del", "loss1", "\n", "if", "and_rev", ":", "\n", "                ", "loss1_rev", "=", "self", ".", "_train_step_bt", "(", "d1_x2", ",", "d1_len2", ",", "d1_langs2", ",", "d1_x1", ",", "d1_len1", ",", "d1_langs1", ")", "\n", "loss1_rev", "=", "lambda_coeff", "*", "loss1_rev", "\n", "self", ".", "optimize", "(", "loss1_rev", ")", "\n", "del", "loss1_rev", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "'d1_x1 is NOne'", ")", "\n", "\n", "# todo: secondary training", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x3", ",", "len3", ",", "langs3", "=", "self", ".", "_generate_pair", "(", "\n", "sec_encoder", ",", "sec_decoder", ",", "x2", ",", "len2", ",", "langs2", ",", "lang1_id", ",", "mbeam_size", "=", "mbeam_size", ",", "infer_drop", "=", "infer_drop", ")", "\n", "if", "direction1", ":", "\n", "# lang3->lang2", "\n", "                ", "d2_x1", ",", "d2_len1", ",", "d2_langs1", "=", "x3", ",", "len3", ",", "langs3", "\n", "d2_x2", ",", "d2_len2", ",", "d2_langs2", "=", "x2", ",", "len2", ",", "langs2", "\n", "", "else", ":", "\n", "# lang2->lang3", "\n", "                ", "d2_x1", ",", "d2_len1", ",", "d2_langs1", "=", "x2", ",", "len2", ",", "langs2", "\n", "d2_x2", ",", "d2_len2", ",", "d2_langs2", "=", "x3", ",", "len3", ",", "langs3", "\n", "", "d2_x1", ",", "d2_len1", ",", "d2_langs1", ",", "d2_x2", ",", "d2_len2", ",", "d2_langs2", "=", "self", ".", "_filter_valid_pairs", "(", "\n", "d2_x1", ",", "d2_len1", ",", "d2_lang1_id", ",", "d2_x2", ",", "d2_len2", ",", "d2_lang2_id", ")", "\n", "\n", "", "if", "d2_x1", "is", "not", "None", ":", "\n", "            ", "loss2", "=", "self", ".", "_train_step_bt", "(", "d2_x1", ",", "d2_len1", ",", "d2_langs1", ",", "d2_x2", ",", "d2_len2", ",", "d2_langs2", ")", "\n", "suffix", "=", "'CT'", "\n", "self", ".", "stats", "[", "(", "'BT%s-%s-%s-%s'", "%", "(", "suffix", ",", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss2", ".", "item", "(", ")", ")", "\n", "loss2", "=", "lambda_coeff", "*", "loss2", "\n", "self", ".", "optimize", "(", "loss2", ")", "\n", "del", "loss2", "\n", "if", "and_rev", ":", "\n", "                ", "loss2_rev", "=", "self", ".", "_train_step_bt", "(", "d2_x2", ",", "d2_len2", ",", "d2_langs2", ",", "d2_x1", ",", "d2_len1", ",", "d2_langs1", ")", "\n", "loss2_rev", "=", "lambda_coeff", "*", "loss2_rev", "\n", "self", ".", "optimize", "(", "loss2_rev", ")", "\n", "del", "loss2_rev", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "'d1_x2 is NOne'", ")", "\n", "\n", "", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair": [[321, 339], ["macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder2.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder2.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder2.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder2.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder.eval", "torch.no_grad", "enc().transpose", "dec.generate", "x2.clone().fill_", "enc", "int", "x2.clone", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate"], ["", "def", "_generate_pair", "(", "self", ",", "enc", ",", "dec", ",", "x1", ",", "len1", ",", "langs1", ",", "lang2_id", ",", "nbest", "=", "None", ",", "mbeam_size", "=", "1", ",", "infer_drop", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "if", "infer_drop", ":", "\n", "            ", "self", ".", "encoder2", ".", "train", "(", ")", "\n", "self", ".", "decoder2", ".", "train", "(", ")", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder2", ".", "eval", "(", ")", "\n", "self", ".", "decoder2", ".", "eval", "(", ")", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# primary", "\n", "            ", "enc1", "=", "enc", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "x2", ",", "len2", "=", "dec", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "", "return", "x2", ",", "len2", ",", "langs2", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer.bt_step_macd_v2": [[340, 443], ["getattr", "getattr", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder2.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder2.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder1.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder1.eval", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.get_batch", "x1.clone().fill_", "x1.size", "utils.to_cuda", "len1.size", "torch.no_grad", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.stats[].append", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize", "print", "torch.no_grad", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.stats[].append", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.optimize", "print", "x1.clone", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.item", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.item"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._generate_pair", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._filter_valid_pairs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "bt_step_macd_v2", "(", "\n", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ",", "enforce_default", "=", "False", ",", "first_sec", "=", "False", ",", "wrote", "=", "False", ")", ":", "\n", "        ", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "nbest", "=", "params", ".", "nbest", "\n", "assert", "nbest", "is", "None", "or", "nbest", "<=", "1", "\n", "tokens_per_batch", "=", "params", ".", "tokens_per_batch", "\n", "beam_size", "=", "params", ".", "beam_size", "\n", "mbeam_size", "=", "getattr", "(", "params", ",", "'mbeam_size'", ",", "-", "1", ")", "\n", "select_opt", "=", "getattr", "(", "params", ",", "'select_opt'", ",", "0", ")", "\n", "# assert beam_size > 1", "\n", "# assert nbest is not None and 1 <= nbest <= beam_size", "\n", "\n", "if", "params", ".", "multi_gpu", ":", "\n", "            ", "infer_encoder1", "=", "self", ".", "encoder1", ".", "module", "\n", "infer_decoder1", "=", "self", ".", "decoder1", ".", "module", "\n", "infer_encoder2", "=", "self", ".", "encoder2", ".", "module", "\n", "infer_decoder2", "=", "self", ".", "decoder2", ".", "module", "\n", "", "else", ":", "\n", "            ", "infer_encoder1", "=", "self", ".", "encoder1", "\n", "infer_decoder1", "=", "self", ".", "decoder1", "\n", "infer_encoder2", "=", "self", ".", "encoder2", "\n", "infer_decoder2", "=", "self", ".", "decoder2", "\n", "", "self", ".", "encoder2", ".", "eval", "(", ")", "\n", "self", ".", "decoder2", ".", "eval", "(", ")", "\n", "self", ".", "encoder1", ".", "eval", "(", ")", "\n", "self", ".", "decoder1", ".", "eval", "(", ")", "\n", "\n", "if", "first_sec", ":", "\n", "            ", "first_encoder", "=", "infer_encoder2", "\n", "first_decoder", "=", "infer_decoder2", "\n", "sec_encoder", "=", "infer_encoder1", "\n", "sec_decoder", "=", "infer_decoder1", "\n", "", "else", ":", "\n", "            ", "first_encoder", "=", "infer_encoder1", "\n", "first_decoder", "=", "infer_decoder1", "\n", "sec_encoder", "=", "infer_encoder2", "\n", "sec_decoder", "=", "infer_decoder2", "\n", "\n", "", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "bsz", "=", "x1", ".", "size", "(", "1", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "pri_srclang_id", "=", "lang2_id", "\n", "pri_tgtlang_id", "=", "lang1_id", "\n", "sec_srclang_id", "=", "lang2_id", "\n", "sec_tgtlang_id", "=", "lang1_id", "\n", "\n", "# todo: primary training", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x2", ",", "len2", ",", "langs2", "=", "self", ".", "_generate_pair", "(", "\n", "first_encoder", ",", "first_decoder", ",", "x1", ",", "len1", ",", "langs1", ",", "lang2_id", ",", "mbeam_size", "=", "mbeam_size", ")", "\n", "# todo: lang2->lang1", "\n", "pri_src", ",", "pri_srclen", ",", "pri_src_langs", "=", "x2", ",", "len2", ",", "langs2", "\n", "pri_tgt", ",", "pri_tgtlen", ",", "pri_tgt_langs", "=", "x1", ",", "len1", ",", "langs1", "\n", "pri_src", ",", "pri_srclen", ",", "pri_src_langs", ",", "pri_tgt", ",", "pri_tgtlen", ",", "pri_tgt_langs", "=", "self", ".", "_filter_valid_pairs", "(", "\n", "pri_src", ",", "pri_srclen", ",", "pri_srclang_id", ",", "pri_tgt", ",", "pri_tgtlen", ",", "pri_tgtlang_id", ")", "\n", "\n", "", "if", "pri_src", "is", "not", "None", ":", "\n", "# try:", "\n", "            ", "loss1", "=", "self", ".", "_train_step_bt", "(", "pri_src", ",", "pri_srclen", ",", "pri_src_langs", ",", "pri_tgt", ",", "pri_tgtlen", ",", "pri_tgt_langs", ")", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss1", ".", "item", "(", ")", ")", "\n", "loss1", "=", "lambda_coeff", "*", "loss1", "\n", "self", ".", "optimize", "(", "loss1", ")", "\n", "del", "loss1", "\n", "", "else", ":", "\n", "            ", "print", "(", "'pri_src is NOne'", ")", "\n", "return", "\n", "\n", "# todo: secondary training", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# todo: lang2-lang1", "\n", "            ", "x3", ",", "len3", ",", "langs3", "=", "self", ".", "_generate_pair", "(", "\n", "sec_encoder", ",", "sec_decoder", ",", "pri_src", ",", "pri_srclen", ",", "pri_src_langs", ",", "lang1_id", ",", "mbeam_size", "=", "mbeam_size", ")", "\n", "# lang2->lang3", "\n", "sec_src", ",", "sec_srclen", ",", "sec_src_langs", "=", "pri_src", ",", "pri_srclen", ",", "pri_src_langs", "\n", "sec_tgt", ",", "sec_tgtlen", ",", "sec_tgt_langs", "=", "x3", ",", "len3", ",", "langs3", "\n", "sec_src", ",", "sec_srclen", ",", "sec_src_langs", ",", "sec_tgt", ",", "sec_tgtlen", ",", "sec_tgt_langs", "=", "self", ".", "_filter_valid_pairs", "(", "\n", "sec_src", ",", "sec_srclen", ",", "sec_srclang_id", ",", "sec_tgt", ",", "sec_tgtlen", ",", "sec_tgtlang_id", ")", "\n", "\n", "", "if", "sec_src", "is", "not", "None", ":", "\n", "# try:", "\n", "            ", "loss2", "=", "self", ".", "_train_step_bt", "(", "sec_src", ",", "sec_srclen", ",", "sec_src_langs", ",", "sec_tgt", ",", "sec_tgtlen", ",", "sec_tgt_langs", ")", "\n", "suffix", "=", "'CT'", "\n", "self", ".", "stats", "[", "(", "'BT%s-%s-%s-%s'", "%", "(", "suffix", ",", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss2", ".", "item", "(", ")", ")", "\n", "loss2", "=", "lambda_coeff", "*", "loss2", "\n", "self", ".", "optimize", "(", "loss2", ")", "\n", "del", "loss2", "\n", "", "else", ":", "\n", "            ", "print", "(", "'sec_src is NOne'", ")", "\n", "\n", "", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.macd_online_trainer.EncDecMACDmbeamOnlineTrainer._train_step_bt": [[444, 457], ["macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder.train", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.encoder", "enc.transpose.transpose.transpose", "torch.arange", "x2[].masked_select", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder", "macd_online_trainer.EncDecMACDmbeamOnlineTrainer.decoder", "len2.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train"], ["", "def", "_train_step_bt", "(", "self", ",", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ")", ":", "\n", "        ", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "enc", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc", "=", "enc", ".", "transpose", "(", "0", ",", "1", ")", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "ppred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "ppred_mask", "[", ":", "-", "1", "]", ")", "\n", "dec3", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc", ",", "src_len", "=", "len1", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec3", ",", "pred_mask", "=", "ppred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.Adam.__init__": [[23, 41], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "state", "[", "'step'", "]", "=", "0", "# torch.zeros(1)", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.Adam.__setstate__": [[42, 44], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.Adam.__setstate__"], ["", "", "", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.Adam.step": [[45, 87], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "RuntimeError", "p.data.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Step.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# if group['weight_decay'] != 0:", "\n", "#     grad.add_(group['weight_decay'], p.data)", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "# denom = exp_avg_sq.sqrt().clamp_(min=group['eps'])", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "# .item()", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "# .item()", "\n", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamInverseSqrtWithWarmup.__init__": [[104, 128], ["torch.optim.Adam.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "warmup_updates", "=", "4000", ",", "warmup_init_lr", "=", "1e-7", ",", "\n", "exp_factor", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", ",", "\n", "lr", "=", "warmup_init_lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n", "\n", "# linearly warmup for the first warmup_updates", "\n", "self", ".", "warmup_updates", "=", "warmup_updates", "\n", "self", ".", "warmup_init_lr", "=", "warmup_init_lr", "\n", "warmup_end_lr", "=", "lr", "\n", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "warmup_init_lr", ")", "/", "warmup_updates", "\n", "\n", "# then, decay prop. to the inverse square root of the update number", "\n", "self", ".", "exp_factor", "=", "exp_factor", "\n", "self", ".", "decay_factor", "=", "warmup_end_lr", "*", "warmup_updates", "**", "self", ".", "exp_factor", "\n", "\n", "# total number of updates", "\n", "for", "param_group", "in", "self", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'num_updates'", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamInverseSqrtWithWarmup.get_lr_for_step": [[129, 134], ["None"], "methods", ["None"], ["", "", "def", "get_lr_for_step", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "if", "num_updates", "<", "self", ".", "warmup_updates", ":", "\n", "            ", "return", "self", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "decay_factor", "*", "(", "num_updates", "**", "-", "self", ".", "exp_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamInverseSqrtWithWarmup.step": [[135, 140], ["torch.optim.Adam.step", "torch.optim.AdamInverseSqrtWithWarmup.get_lr_for_step"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "step", "(", "closure", ")", "\n", "for", "param_group", "in", "self", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'num_updates'", "]", "+=", "1", "\n", "param_group", "[", "'lr'", "]", "=", "self", ".", "get_lr_for_step", "(", "param_group", "[", "'num_updates'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.__init__": [[158, 185], ["torch.optim.Adam.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "warmup_updates", "=", "4000", ",", "warmup_init_lr", "=", "1e-7", ",", "\n", "min_lr", "=", "1e-9", ",", "init_period", "=", "1000000", ",", "period_mult", "=", "1", ",", "lr_shrink", "=", "0.75", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", ",", "\n", "lr", "=", "warmup_init_lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n", "\n", "# linearly warmup for the first warmup_updates", "\n", "self", ".", "warmup_updates", "=", "warmup_updates", "\n", "self", ".", "warmup_init_lr", "=", "warmup_init_lr", "\n", "warmup_end_lr", "=", "lr", "\n", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "warmup_init_lr", ")", "/", "warmup_updates", "\n", "\n", "# then, apply cosine scheduler", "\n", "self", ".", "min_lr", "=", "min_lr", "\n", "self", ".", "max_lr", "=", "lr", "\n", "self", ".", "period", "=", "init_period", "\n", "self", ".", "period_mult", "=", "period_mult", "\n", "self", ".", "lr_shrink", "=", "lr_shrink", "\n", "\n", "# total number of updates", "\n", "for", "param_group", "in", "self", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'num_updates'", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step": [[186, 203], ["math.floor", "math.floor", "math.log", "math.cos"], "methods", ["None"], ["", "", "def", "get_lr_for_step", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "if", "num_updates", "<", "self", ".", "warmup_updates", ":", "\n", "            ", "return", "self", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "t", "=", "num_updates", "-", "self", ".", "warmup_updates", "\n", "if", "self", ".", "period_mult", "==", "1", ":", "\n", "                ", "pid", "=", "math", ".", "floor", "(", "t", "/", "self", ".", "period", ")", "\n", "t_i", "=", "self", ".", "period", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "period", "*", "pid", ")", "\n", "", "else", ":", "\n", "                ", "pid", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "period", "*", "(", "1", "-", "self", ".", "period_mult", ")", ",", "self", ".", "period_mult", ")", ")", "\n", "t_i", "=", "self", ".", "period", "*", "(", "self", ".", "period_mult", "**", "pid", ")", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "period_mult", "**", "pid", ")", "/", "(", "1", "-", "self", ".", "period_mult", ")", "*", "self", ".", "period", "\n", "", "lr_shrink", "=", "self", ".", "lr_shrink", "**", "pid", "\n", "min_lr", "=", "self", ".", "min_lr", "*", "lr_shrink", "\n", "max_lr", "=", "self", ".", "max_lr", "*", "lr_shrink", "\n", "return", "min_lr", "+", "0.5", "*", "(", "max_lr", "-", "min_lr", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "t_curr", "/", "t_i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step": [[204, 209], ["torch.optim.Adam.step", "torch.optim.AdamCosineWithWarmup.get_lr_for_step"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.get_lr_for_step"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "step", "(", "closure", ")", "\n", "for", "param_group", "in", "self", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'num_updates'", "]", "+=", "1", "\n", "param_group", "[", "'lr'", "]", "=", "self", ".", "get_lr_for_step", "(", "param_group", "[", "'num_updates'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer": [[211, 271], ["optim_fn", "s[].split", "inspect.getargspec", "all", "Exception", "x.split", "float", "s.find", "len", "re.match", "optim_params.pop", "optim_params.pop", "optim_params.get", "optim_params.get", "optim_params.pop", "optim_params.pop", "optim_params.keys", "str", "str", "optim_params.get", "optim_params.get", "optim_params.pop", "optim_params.pop", "optim_params.keys", "s.find", "optim_params.get", "optim_params.get", "Exception"], "function", ["None"], ["", "", "", "def", "get_optimizer", "(", "parameters", ",", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse optimizer parameters.\n    Input should be of the form:\n        - \"sgd,lr=0.01\"\n        - \"adagrad,lr=0.1,lr_decay=0.05\"\n    \"\"\"", "\n", "if", "\",\"", "in", "s", ":", "\n", "        ", "method", "=", "s", "[", ":", "s", ".", "find", "(", "','", ")", "]", "\n", "optim_params", "=", "{", "}", "\n", "for", "x", "in", "s", "[", "s", ".", "find", "(", "','", ")", "+", "1", ":", "]", ".", "split", "(", "','", ")", ":", "\n", "            ", "split", "=", "x", ".", "split", "(", "'='", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "\n", "assert", "re", ".", "match", "(", "\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\"", ",", "split", "[", "1", "]", ")", "is", "not", "None", "\n", "optim_params", "[", "split", "[", "0", "]", "]", "=", "float", "(", "split", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "method", "=", "s", "\n", "optim_params", "=", "{", "}", "\n", "\n", "", "if", "method", "==", "'adadelta'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adadelta", "\n", "", "elif", "method", "==", "'adagrad'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adagrad", "\n", "", "elif", "method", "==", "'adam'", ":", "\n", "        ", "optim_fn", "=", "Adam", "\n", "optim_params", "[", "'betas'", "]", "=", "(", "optim_params", ".", "get", "(", "'beta1'", ",", "0.9", ")", ",", "optim_params", ".", "get", "(", "'beta2'", ",", "0.999", ")", ")", "\n", "optim_params", ".", "pop", "(", "'beta1'", ",", "None", ")", "\n", "optim_params", ".", "pop", "(", "'beta2'", ",", "None", ")", "\n", "", "elif", "method", "==", "'adam_inverse_sqrt'", ":", "\n", "        ", "optim_fn", "=", "AdamInverseSqrtWithWarmup", "\n", "optim_params", "[", "'betas'", "]", "=", "(", "optim_params", ".", "get", "(", "'beta1'", ",", "0.9", ")", ",", "optim_params", ".", "get", "(", "'beta2'", ",", "0.999", ")", ")", "\n", "optim_params", ".", "pop", "(", "'beta1'", ",", "None", ")", "\n", "optim_params", ".", "pop", "(", "'beta2'", ",", "None", ")", "\n", "", "elif", "method", "==", "'adam_cosine'", ":", "\n", "        ", "optim_fn", "=", "AdamCosineWithWarmup", "\n", "optim_params", "[", "'betas'", "]", "=", "(", "optim_params", ".", "get", "(", "'beta1'", ",", "0.9", ")", ",", "optim_params", ".", "get", "(", "'beta2'", ",", "0.999", ")", ")", "\n", "optim_params", ".", "pop", "(", "'beta1'", ",", "None", ")", "\n", "optim_params", ".", "pop", "(", "'beta2'", ",", "None", ")", "\n", "", "elif", "method", "==", "'adamax'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adamax", "\n", "", "elif", "method", "==", "'asgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "ASGD", "\n", "", "elif", "method", "==", "'rmsprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "RMSprop", "\n", "", "elif", "method", "==", "'rprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Rprop", "\n", "", "elif", "method", "==", "'sgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "SGD", "\n", "assert", "'lr'", "in", "optim_params", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Unknown optimization method: \"%s\"'", "%", "method", ")", "\n", "\n", "# check that we give good parameters to the optimizer", "\n", "", "expected_args", "=", "inspect", ".", "getargspec", "(", "optim_fn", ".", "__init__", ")", "[", "0", "]", "\n", "assert", "expected_args", "[", ":", "2", "]", "==", "[", "'self'", ",", "'params'", "]", "\n", "if", "not", "all", "(", "k", "in", "expected_args", "[", "2", ":", "]", "for", "k", "in", "optim_params", ".", "keys", "(", ")", ")", ":", "\n", "        ", "raise", "Exception", "(", "'Unexpected parameters: expected \"%s\", got \"%s\"'", "%", "(", "\n", "str", "(", "expected_args", "[", "2", ":", "]", ")", ",", "str", "(", "optim_params", ".", "keys", "(", ")", ")", ")", ")", "\n", "\n", "", "return", "optim_fn", "(", "parameters", ",", "**", "optim_params", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.AttrDict.__init__": [[30, 33], ["dict.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "AttrDict", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "__dict__", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.bool_flag": [[35, 45], ["s.lower", "s.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "def", "bool_flag", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse boolean arguments from the command line.\n    \"\"\"", "\n", "if", "s", ".", "lower", "(", ")", "in", "FALSY_STRINGS", ":", "\n", "        ", "return", "False", "\n", "", "elif", "s", ".", "lower", "(", ")", "in", "TRUTHY_STRINGS", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "\"Invalid value for a boolean flag!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.initialize_exp": [[47, 88], ["utils.get_dump_path", "pickle.dump", "logger.create_logger", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "open", "x.startswith", "len", "os.path.join", "os.path.join", "command.append", "re.match", "params.exp_name.strip", "getattr", "command.append", "command.append", "sorted", "str", "dict().items", "dict", "vars"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.get_dump_path", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.logger.create_logger"], ["", "", "def", "initialize_exp", "(", "params", ",", "log_filename", "=", "None", ",", "params_filename", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Initialize the experience:\n    - dump parameters\n    - create a logger\n    \"\"\"", "\n", "# dump parameters", "\n", "get_dump_path", "(", "params", ")", "\n", "if", "params_filename", "is", "None", ":", "\n", "        ", "params_filename", "=", "'params.pkl'", "\n", "", "pickle", ".", "dump", "(", "params", ",", "open", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "params_filename", ")", ",", "'wb'", ")", ")", "\n", "\n", "# get running command", "\n", "command", "=", "[", "\"python\"", ",", "sys", ".", "argv", "[", "0", "]", "]", "\n", "for", "x", "in", "sys", ".", "argv", "[", "1", ":", "]", ":", "\n", "        ", "if", "x", ".", "startswith", "(", "'--'", ")", ":", "\n", "            ", "assert", "'\"'", "not", "in", "x", "and", "\"'\"", "not", "in", "x", "\n", "command", ".", "append", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "assert", "\"'\"", "not", "in", "x", "\n", "if", "re", ".", "match", "(", "'^[a-zA-Z0-9_]+$'", ",", "x", ")", ":", "\n", "                ", "command", ".", "append", "(", "\"%s\"", "%", "x", ")", "\n", "", "else", ":", "\n", "                ", "command", ".", "append", "(", "\"'%s'\"", "%", "x", ")", "\n", "", "", "", "command", "=", "' '", ".", "join", "(", "command", ")", "\n", "params", ".", "command", "=", "command", "+", "' --exp_id \"%s\"'", "%", "params", ".", "exp_id", "\n", "\n", "# check experiment name", "\n", "assert", "len", "(", "params", ".", "exp_name", ".", "strip", "(", ")", ")", ">", "0", "\n", "\n", "# create a logger", "\n", "if", "log_filename", "is", "None", ":", "\n", "        ", "log_filename", "=", "'train.log'", "\n", "", "logger", "=", "create_logger", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "log_filename", ")", ",", "rank", "=", "getattr", "(", "params", ",", "'global_rank'", ",", "0", ")", ")", "\n", "logger", ".", "info", "(", "\"============ Initialized logger ============\"", ")", "\n", "logger", ".", "info", "(", "\"\\n\"", ".", "join", "(", "\"%s: %s\"", "%", "(", "k", ",", "str", "(", "v", ")", ")", "\n", "for", "k", ",", "v", "in", "sorted", "(", "dict", "(", "vars", "(", "params", ")", ")", ".", "items", "(", ")", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"The experiment will be stored in %s\\n\"", "%", "params", ".", "dump_path", ")", "\n", "logger", ".", "info", "(", "\"Running command: %s\"", "%", "command", ")", "\n", "logger", ".", "info", "(", "\"\"", ")", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.get_dump_path": [[90, 124], ["os.path.join", "os.path.join", "len", "os.path.exists", "subprocess.Popen().wait", "os.environ.get", "os.environ.get", "os.path.isdir", "subprocess.Popen().wait", "exp_id.isdigit", "subprocess.Popen", "subprocess.Popen", "os.path.isdir", "random.choice", "os.path.join", "range"], "function", ["None"], ["", "def", "get_dump_path", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Create a directory to store the experiment.\n    \"\"\"", "\n", "dump_path", "=", "DUMP_PATH", "if", "params", ".", "dump_path", "==", "''", "else", "params", ".", "dump_path", "\n", "assert", "len", "(", "params", ".", "exp_name", ")", ">", "0", "\n", "\n", "# create the sweep path if it does not exist", "\n", "sweep_path", "=", "os", ".", "path", ".", "join", "(", "dump_path", ",", "params", ".", "exp_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "sweep_path", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir -p %s\"", "%", "sweep_path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "\n", "# create an ID for the job if it is not given in the parameters.", "\n", "# if we run on the cluster, the job ID is the one of Chronos.", "\n", "# otherwise, it is randomly generated", "\n", "", "if", "params", ".", "exp_id", "==", "''", ":", "\n", "        ", "chronos_job_id", "=", "os", ".", "environ", ".", "get", "(", "'CHRONOS_JOB_ID'", ")", "\n", "slurm_job_id", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_ID'", ")", "\n", "assert", "chronos_job_id", "is", "None", "or", "slurm_job_id", "is", "None", "\n", "exp_id", "=", "chronos_job_id", "if", "chronos_job_id", "is", "not", "None", "else", "slurm_job_id", "\n", "if", "exp_id", "is", "None", ":", "\n", "            ", "chars", "=", "'abcdefghijklmnopqrstuvwxyz0123456789'", "\n", "while", "True", ":", "\n", "                ", "exp_id", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "chars", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "sweep_path", ",", "exp_id", ")", ")", ":", "\n", "                    ", "break", "\n", "", "", "", "else", ":", "\n", "            ", "assert", "exp_id", ".", "isdigit", "(", ")", "\n", "", "params", ".", "exp_id", "=", "exp_id", "\n", "\n", "# create the dump folder / update parameters", "\n", "", "params", ".", "dump_path", "=", "os", ".", "path", ".", "join", "(", "sweep_path", ",", "params", ".", "exp_id", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "params", ".", "dump_path", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir -p %s\"", "%", "params", ".", "dump_path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda": [[126, 131], ["x.cuda"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], ["", "", "def", "to_cuda", "(", "*", "args", ")", ":", "\n", "    ", "\"\"\"\n    Move tensors to CUDA.\n    \"\"\"", "\n", "return", "[", "None", "if", "x", "is", "None", "else", "x", ".", "cuda", "(", ")", "for", "x", "in", "args", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation": [[133, 144], ["os.path.isfile", "path.split", "copyfile", "subprocess.Popen().wait", "subprocess.Popen"], "function", ["None"], ["", "def", "restore_segmentation", "(", "path", ",", "raw_prefix", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Take a file segmented with BPE and restore it to its original segmentation.\n    \"\"\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "from", "shutil", "import", "copyfile", "\n", "parts", "=", "path", ".", "split", "(", "\".\"", ")", "\n", "raw_file", "=", "'.'", ".", "join", "(", "parts", "[", ":", "-", "1", "]", "+", "[", "'raw'", "]", "+", "parts", "[", "-", "1", ":", "]", ")", "\n", "copyfile", "(", "path", ",", "raw_file", "if", "raw_prefix", "else", "'{}.raw'", ".", "format", "(", "path", ")", ")", "\n", "restore_cmd", "=", "\"sed -i -r 's/(@@ )|(@@ ?$)//g' %s\"", "\n", "subprocess", ".", "Popen", "(", "restore_cmd", "%", "path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.parse_lambda_config": [[146, 166], ["getattr", "getattr.split", "len", "setattr", "setattr", "all", "all", "all", "setattr", "setattr", "float", "s.split", "float", "k.isdigit", "len", "int", "int", "range", "int", "float", "len"], "function", ["None"], ["", "def", "parse_lambda_config", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Parse the configuration of lambda coefficient (for scheduling).\n    x = \"3\"                  # lambda will be a constant equal to x\n    x = \"0:1,1000:0\"         # lambda will start from 1 and linearly decrease to 0 during the first 1000 iterations\n    x = \"0:0,1000:0,2000:1\"  # lambda will be equal to 0 for the first 1000 iterations, then will linearly increase to 1 until iteration 2000\n    \"\"\"", "\n", "for", "name", "in", "DYNAMIC_COEFF", ":", "\n", "        ", "x", "=", "getattr", "(", "params", ",", "name", ")", "\n", "split", "=", "x", ".", "split", "(", "','", ")", "\n", "if", "len", "(", "split", ")", "==", "1", ":", "\n", "            ", "setattr", "(", "params", ",", "name", ",", "float", "(", "x", ")", ")", "\n", "setattr", "(", "params", ",", "name", "+", "'_config'", ",", "None", ")", "\n", "", "else", ":", "\n", "            ", "split", "=", "[", "s", ".", "split", "(", "':'", ")", "for", "s", "in", "split", "]", "\n", "assert", "all", "(", "len", "(", "s", ")", "==", "2", "for", "s", "in", "split", ")", "\n", "assert", "all", "(", "k", ".", "isdigit", "(", ")", "for", "k", ",", "_", "in", "split", ")", "\n", "assert", "all", "(", "int", "(", "split", "[", "i", "]", "[", "0", "]", ")", "<", "int", "(", "split", "[", "i", "+", "1", "]", "[", "0", "]", ")", "for", "i", "in", "range", "(", "len", "(", "split", ")", "-", "1", ")", ")", "\n", "setattr", "(", "params", ",", "name", ",", "float", "(", "split", "[", "0", "]", "[", "1", "]", ")", ")", "\n", "setattr", "(", "params", ",", "name", "+", "'_config'", ",", "[", "(", "int", "(", "k", ")", ",", "float", "(", "v", ")", ")", "for", "k", ",", "v", "in", "split", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils._test_parse": [[168, 184], ["x.split", "len", "setattr", "setattr", "all", "all", "all", "float", "float", "s.split", "k.isdigit", "int", "float", "len", "int", "int", "range", "len"], "function", ["None"], ["", "", "", "def", "_test_parse", "(", "x", ",", "name", "=", "'a'", ")", ":", "\n", "    ", "split", "=", "x", ".", "split", "(", "','", ")", "\n", "params", "=", "{", "}", "\n", "if", "len", "(", "split", ")", "==", "1", ":", "\n", "        ", "setattr", "(", "params", ",", "name", ",", "float", "(", "x", ")", ")", "\n", "setattr", "(", "params", ",", "name", "+", "'_config'", ",", "None", ")", "\n", "params", "[", "name", "]", "=", "float", "\n", "params", "[", "name", "+", "'_config'", "]", "=", "None", "\n", "", "else", ":", "\n", "        ", "split", "=", "[", "s", ".", "split", "(", "':'", ")", "for", "s", "in", "split", "]", "\n", "assert", "all", "(", "len", "(", "s", ")", "==", "2", "for", "s", "in", "split", ")", "\n", "assert", "all", "(", "k", ".", "isdigit", "(", ")", "for", "k", ",", "_", "in", "split", ")", "\n", "assert", "all", "(", "int", "(", "split", "[", "i", "]", "[", "0", "]", ")", "<", "int", "(", "split", "[", "i", "+", "1", "]", "[", "0", "]", ")", "for", "i", "in", "range", "(", "len", "(", "split", ")", "-", "1", ")", ")", "\n", "params", "[", "name", "]", "=", "float", "(", "split", "[", "0", "]", "[", "1", "]", ")", "\n", "params", "[", "name", "+", "'_config'", "]", "=", "[", "(", "int", "(", "k", ")", ",", "float", "(", "v", ")", ")", "for", "k", ",", "v", "in", "split", "]", "\n", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.get_lambda_value": [[185, 198], ["len", "len", "range", "float", "float", "len"], "function", ["None"], ["", "def", "get_lambda_value", "(", "config", ",", "n_iter", ")", ":", "\n", "    ", "\"\"\"\n    Compute a lambda value according to its schedule configuration.\n    \"\"\"", "\n", "ranges", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "config", ")", "-", "1", ")", "if", "config", "[", "i", "]", "[", "0", "]", "<=", "n_iter", "<", "config", "[", "i", "+", "1", "]", "[", "0", "]", "]", "\n", "if", "len", "(", "ranges", ")", "==", "0", ":", "\n", "        ", "assert", "n_iter", ">=", "config", "[", "-", "1", "]", "[", "0", "]", "\n", "return", "config", "[", "-", "1", "]", "[", "1", "]", "\n", "", "assert", "len", "(", "ranges", ")", "==", "1", "\n", "i", "=", "ranges", "[", "0", "]", "\n", "x_a", ",", "y_a", "=", "config", "[", "i", "]", "\n", "x_b", ",", "y_b", "=", "config", "[", "i", "+", "1", "]", "\n", "return", "y_a", "+", "(", "n_iter", "-", "x_a", ")", "*", "float", "(", "y_b", "-", "y_a", ")", "/", "float", "(", "x_b", "-", "x_a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.update_lambdas": [[200, 208], ["getattr", "setattr", "utils.get_lambda_value"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.get_lambda_value"], ["", "def", "update_lambdas", "(", "params", ",", "n_iter", ")", ":", "\n", "    ", "\"\"\"\n    Update all lambda coefficients.\n    \"\"\"", "\n", "for", "name", "in", "DYNAMIC_COEFF", ":", "\n", "        ", "config", "=", "getattr", "(", "params", ",", "name", "+", "'_config'", ")", "\n", "if", "config", "is", "not", "None", ":", "\n", "            ", "setattr", "(", "params", ",", "name", ",", "get_lambda_value", "(", "config", ",", "n_iter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.set_sampling_probs": [[210, 236], ["len", "numpy.array", "np.array.sum", "numpy.array", "np.array.sum", "len", "numpy.array", "np.array.sum", "numpy.array", "np.array.sum", "data[].items", "data[].items", "len", "len"], "function", ["None"], ["", "", "", "def", "set_sampling_probs", "(", "data", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Set the probability of sampling specific languages / language pairs during training.\n    \"\"\"", "\n", "coeff", "=", "params", ".", "lg_sampling_factor", "\n", "if", "coeff", "==", "-", "1", ":", "\n", "        ", "return", "\n", "", "assert", "coeff", ">", "0", "\n", "\n", "# monolingual data", "\n", "params", ".", "mono_list", "=", "[", "k", "for", "k", ",", "v", "in", "data", "[", "'mono_stream'", "]", ".", "items", "(", ")", "if", "'train'", "in", "v", "]", "\n", "if", "len", "(", "params", ".", "mono_list", ")", ">", "0", ":", "\n", "        ", "probs", "=", "np", ".", "array", "(", "[", "1.0", "*", "len", "(", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "[", "'train'", "]", ")", "for", "lang", "in", "params", ".", "mono_list", "]", ")", "\n", "probs", "/=", "probs", ".", "sum", "(", ")", "\n", "probs", "=", "np", ".", "array", "(", "[", "p", "**", "coeff", "for", "p", "in", "probs", "]", ")", "\n", "probs", "/=", "probs", ".", "sum", "(", ")", "\n", "params", ".", "mono_probs", "=", "probs", "\n", "\n", "# parallel data", "\n", "", "params", ".", "para_list", "=", "[", "k", "for", "k", ",", "v", "in", "data", "[", "'para'", "]", ".", "items", "(", ")", "if", "'train'", "in", "v", "]", "\n", "if", "len", "(", "params", ".", "para_list", ")", ">", "0", ":", "\n", "        ", "probs", "=", "np", ".", "array", "(", "[", "1.0", "*", "len", "(", "data", "[", "'para'", "]", "[", "(", "l1", ",", "l2", ")", "]", "[", "'train'", "]", ")", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "para_list", "]", ")", "\n", "probs", "/=", "probs", ".", "sum", "(", ")", "\n", "probs", "=", "np", ".", "array", "(", "[", "p", "**", "coeff", "for", "p", "in", "probs", "]", ")", "\n", "probs", "/=", "probs", ".", "sum", "(", ")", "\n", "params", ".", "para_probs", "=", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches": [[238, 263], ["x1.new().fill_", "x[].copy_", "[].repeat().to", "x1.new().fill_", "range", "lengths.max().item", "lengths.size", "x[].copy_", "x1.new", "[].repeat", "x1.new", "lengths.max", "len1.max().item", "torch.arange", "len1.max"], "function", ["None"], ["", "", "def", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "pad_idx", ",", "eos_idx", ",", "reset_positions", ")", ":", "\n", "    ", "\"\"\"\n    Concat batches with different languages.\n    \"\"\"", "\n", "assert", "reset_positions", "is", "False", "or", "lang1_id", "!=", "lang2_id", "\n", "lengths", "=", "len1", "+", "len2", "\n", "if", "not", "reset_positions", ":", "\n", "        ", "lengths", "-=", "1", "\n", "", "slen", ",", "bs", "=", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", ",", "lengths", ".", "size", "(", "0", ")", "\n", "\n", "x", "=", "x1", ".", "new", "(", "slen", ",", "bs", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "x", "[", ":", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "]", ".", "copy_", "(", "x1", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "slen", ")", "[", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "bs", ")", ".", "to", "(", "x1", ".", "device", ")", "\n", "langs", "=", "x1", ".", "new", "(", "slen", ",", "bs", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "for", "i", "in", "range", "(", "bs", ")", ":", "\n", "        ", "l1", "=", "len1", "[", "i", "]", "if", "reset_positions", "else", "len1", "[", "i", "]", "-", "1", "\n", "x", "[", "l1", ":", "l1", "+", "len2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "x2", "[", ":", "len2", "[", "i", "]", ",", "i", "]", ")", "\n", "if", "reset_positions", ":", "\n", "            ", "positions", "[", "l1", ":", ",", "i", "]", "-=", "len1", "[", "i", "]", "\n", "", "langs", "[", "l1", ":", ",", "i", "]", "=", "lang2_id", "\n", "\n", "", "assert", "(", "x", "==", "eos_idx", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "(", "4", "if", "reset_positions", "else", "3", ")", "*", "bs", "\n", "\n", "return", "x", ",", "lengths", ",", "positions", ",", "langs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate": [[265, 277], ["lengths.clone.max().item", "x[].clone", "lengths.clone.clone", "range", "len", "lengths.clone.max"], "function", ["None"], ["", "def", "truncate", "(", "x", ",", "lengths", ",", "max_len", ",", "eos_index", ")", ":", "\n", "    ", "\"\"\"\n    Truncate long sentences.\n    \"\"\"", "\n", "if", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", ">", "max_len", ":", "\n", "        ", "x", "=", "x", "[", ":", "max_len", "]", ".", "clone", "(", ")", "\n", "lengths", "=", "lengths", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "lengths", ")", ")", ":", "\n", "            ", "if", "lengths", "[", "i", "]", ">", "max_len", ":", "\n", "                ", "lengths", "[", "i", "]", "=", "max_len", "\n", "x", "[", "max_len", "-", "1", ",", "i", "]", "=", "eos_index", "\n", "", "", "", "return", "x", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.shuf_order": [[279, 308], ["len", "numpy.array", "numpy.array", "np.array.sum", "np.array.sum", "len", "len", "len", "len", "numpy.random.permutation", "numpy.random.choice", "numpy.random.choice", "len", "len", "len", "params.mono_list.index", "params.para_list.index", "min", "min", "tuple", "len", "len", "sorted"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["", "def", "shuf_order", "(", "langs", ",", "params", "=", "None", ",", "n", "=", "5", ")", ":", "\n", "    ", "\"\"\"\n    Randomize training order.\n    \"\"\"", "\n", "if", "len", "(", "langs", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "if", "params", "is", "None", ":", "\n", "        ", "return", "[", "langs", "[", "i", "]", "for", "i", "in", "np", ".", "random", ".", "permutation", "(", "len", "(", "langs", ")", ")", "]", "\n", "\n", "# sample monolingual and parallel languages separately", "\n", "", "mono", "=", "[", "l1", "for", "l1", ",", "l2", "in", "langs", "if", "l2", "is", "None", "]", "\n", "para", "=", "[", "(", "l1", ",", "l2", ")", "for", "l1", ",", "l2", "in", "langs", "if", "l2", "is", "not", "None", "]", "\n", "\n", "# uniform / weighted sampling", "\n", "if", "params", ".", "lg_sampling_factor", "==", "-", "1", ":", "\n", "        ", "p_mono", "=", "None", "\n", "p_para", "=", "None", "\n", "", "else", ":", "\n", "        ", "p_mono", "=", "np", ".", "array", "(", "[", "params", ".", "mono_probs", "[", "params", ".", "mono_list", ".", "index", "(", "k", ")", "]", "for", "k", "in", "mono", "]", ")", "\n", "p_para", "=", "np", ".", "array", "(", "[", "params", ".", "para_probs", "[", "params", ".", "para_list", ".", "index", "(", "tuple", "(", "sorted", "(", "k", ")", ")", ")", "]", "for", "k", "in", "para", "]", ")", "\n", "p_mono", "=", "p_mono", "/", "p_mono", ".", "sum", "(", ")", "\n", "p_para", "=", "p_para", "/", "p_para", ".", "sum", "(", ")", "\n", "\n", "", "s_mono", "=", "[", "mono", "[", "i", "]", "for", "i", "in", "np", ".", "random", ".", "choice", "(", "len", "(", "mono", ")", ",", "size", "=", "min", "(", "n", ",", "len", "(", "mono", ")", ")", ",", "p", "=", "p_mono", ",", "replace", "=", "True", ")", "]", "if", "len", "(", "mono", ")", ">", "0", "else", "[", "]", "\n", "s_para", "=", "[", "para", "[", "i", "]", "for", "i", "in", "np", ".", "random", ".", "choice", "(", "len", "(", "para", ")", ",", "size", "=", "min", "(", "n", ",", "len", "(", "para", ")", ")", ",", "p", "=", "p_para", ",", "replace", "=", "True", ")", "]", "if", "len", "(", "para", ")", ">", "0", "else", "[", "]", "\n", "\n", "assert", "len", "(", "s_mono", ")", "+", "len", "(", "s_para", ")", ">", "0", "\n", "return", "[", "(", "lang", ",", "None", ")", "for", "lang", "in", "s_mono", "]", "+", "s_para", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.find_modules": [[310, 320], ["isinstance", "found.append", "module.named_children", "utils.find_modules", "name.isdigit"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.find_modules"], ["", "def", "find_modules", "(", "module", ",", "module_name", ",", "module_instance", ",", "found", ")", ":", "\n", "    ", "\"\"\"\n    Recursively find all instances of a specific module inside a module.\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "module_instance", ")", ":", "\n", "        ", "found", ".", "append", "(", "(", "module_name", ",", "module", ")", ")", "\n", "", "else", ":", "\n", "        ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "            ", "name", "=", "(", "'%s[%s]'", "if", "name", ".", "isdigit", "(", ")", "else", "'%s.%s'", ")", "%", "(", "module_name", ",", "name", ")", "\n", "find_modules", "(", "child", ",", "name", ",", "module_instance", ",", "found", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded": [[20, 107], ["hyp_best.clone", "hyp_best.size", "output.transpose().contiguous.masked_fill_", "hyp_best.clone", "output.transpose().contiguous.size", "output.transpose().contiguous.masked_fill_", "output.transpose().contiguous.transpose().contiguous", "torch.arange().unsqueeze_().expand_as", "torch.arange().unsqueeze_().expand_as", "torch.arange().unsqueeze_().expand_as", "torch.arange().unsqueeze_().expand_as", "range", "ValueError", "isinstance", "torch.arange().unsqueeze_().unsqueeze_().expand_as", "torch.arange().unsqueeze_().unsqueeze_().expand_as", "torch.arange().unsqueeze_().unsqueeze_().expand_as", "torch.arange().unsqueeze_().unsqueeze_().expand_as", "range", "ValueError", "hyp_len.max", "hyp_len.max", "output.transpose().contiguous.transpose", "range", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "logger.info", "torch.arange().unsqueeze_().unsqueeze_", "torch.arange().unsqueeze_().unsqueeze_", "torch.arange().unsqueeze_().unsqueeze_", "torch.arange().unsqueeze_().unsqueeze_", "logger.info", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["def", "beam_efficient_final_decoded", "(", "\n", "hyps_t", ",", "bs", ",", "src_len", ",", "pad_index", ",", "eos_index", ",", "beam_size", ",", "nbest", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n    \"\"\"", "\n", "if", "nbest", "is", "None", "or", "nbest", "<=", "0", ":", "\n", "\n", "        ", "hyp_best", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", ",", ":", ",", "0", "]", "\n", "output", "=", "hyp_best", ".", "clone", "(", ")", "\n", "# output = hyp_best", "\n", "\n", "max_len", ",", "_bs", "=", "hyp_best", ".", "size", "(", ")", "\n", "device", "=", "hyp_best", ".", "device", "\n", "assert", "_bs", "==", "bs", "\n", "assert", "(", "hyp_best", "[", "-", "1", "]", "==", "pad_index", ")", ".", "all", "(", ")", "\n", "assert", "(", "hyp_best", "[", "0", "]", "==", "eos_index", ")", ".", "all", "(", ")", "\n", "hyp_len", "=", "(", "(", "hyp_best", "!=", "pad_index", ")", "*", "(", "hyp_best", "!=", "eos_index", ")", ")", ".", "int", "(", ")", ".", "sum", "(", "0", ")", "+", "2", "\n", "# hyp_len = ((hyp_best != pad_index) * (hyp_best != eos_index)).int().cumsum(0).max(0)[0] + 2", "\n", "\n", "# FIXME: fix this with out the loop!", "\n", "eos_mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", ".", "expand_as", "(", "output", ")", "==", "(", "hyp_len", "-", "1", ")", ".", "unsqueeze_", "(", "0", ")", "\n", "# for i in range(bs):", "\n", "#     output[hyp_len[i] - 1, i] = eos_index", "\n", "output", "=", "output", ".", "masked_fill_", "(", "eos_mask", ",", "eos_index", ")", "\n", "\n", "output", "=", "output", "[", ":", "hyp_len", ".", "max", "(", ")", "]", "\n", "# assert (output == eos_index).sum() == 2 * bs", "\n", "sanity_check", "=", "(", "output", "==", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "if", "not", "sanity_check", ":", "\n", "            ", "for", "i", "in", "range", "(", "bs", ")", ":", "\n", "                ", "if", "(", "output", "[", ":", ",", "i", "]", "==", "eos_index", ")", ".", "sum", "(", ")", "!=", "2", ":", "\n", "                    ", "out", "=", "output", "[", ":", ",", "i", "]", "\n", "hyp", "=", "hyp_best", "[", ":", ",", "i", "]", "\n", "out_l", "=", "(", "(", "out", "!=", "pad_index", ")", "*", "(", "out", "!=", "eos_index", ")", ")", ".", "int", "(", ")", ".", "sum", "(", ")", "+", "2", "\n", "hyp_l", "=", "hyp_len", "[", "i", "]", "\n", "logger", ".", "info", "(", "'[b={}][o={}/h={}] out=\\n{}\\nhyp\\n{}'", ".", "format", "(", "\n", "i", ",", "out_l", ",", "hyp_l", ",", "out", ",", "hyp", "\n", ")", ")", "\n", "", "", "raise", "ValueError", "(", "'sanity check fails'", ")", "\n", "# assert sanity_check", "\n", "", "return", "output", ",", "hyp_len", "\n", "", "else", ":", "\n", "        ", "assert", "isinstance", "(", "nbest", ",", "int", ")", "and", "1", "<=", "nbest", "<=", "beam_size", "\n", "# [max_len, nbest, bs]", "\n", "hyp_best", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", ",", ":", ",", ":", "nbest", "]", "\n", "device", "=", "hyp_best", ".", "device", "\n", "\n", "output", "=", "hyp_best", ".", "clone", "(", ")", "\n", "max_len", ",", "_bs", ",", "_nb", "=", "output", ".", "size", "(", ")", "\n", "assert", "_bs", "==", "bs", "\n", "assert", "_nb", "==", "nbest", "\n", "assert", "(", "output", "[", "-", "1", "]", "==", "pad_index", ")", ".", "all", "(", ")", "\n", "assert", "(", "output", "[", "0", "]", "==", "eos_index", ")", ".", "all", "(", ")", "\n", "hyp_len", "=", "(", "(", "output", "!=", "pad_index", ")", "*", "(", "output", "!=", "eos_index", ")", ")", ".", "int", "(", ")", ".", "sum", "(", "0", ")", "+", "2", "\n", "# hyp_len: [bs, nb]", "\n", "\n", "# for i in range(bs):", "\n", "#     for j in range(nbest):", "\n", "#         output[hyp_len[i, j] - 1, i, j] = eos_index", "\n", "#         output[hyp_len[i, j]:, i, j] = pad_index", "\n", "\n", "eos_mask", "=", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", ".", "unsqueeze_", "(", "\n", "-", "1", ")", ".", "expand_as", "(", "output", ")", "==", "(", "hyp_len", "-", "1", ")", ".", "unsqueeze_", "(", "0", ")", "\n", "output", "=", "output", ".", "masked_fill_", "(", "eos_mask", ",", "eos_index", ")", "\n", "\n", "output", "=", "output", "[", ":", "hyp_len", ".", "max", "(", ")", "]", "\n", "# sanity check", "\n", "output", "=", "output", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "# FIXME: change it to nbest: [max_len, nbest, bs]", "\n", "\n", "sanity_check", "=", "(", "output", "==", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "*", "nbest", "\n", "# FIXME: problem is in more nbest!", "\n", "if", "not", "sanity_check", ":", "\n", "            ", "for", "i", "in", "range", "(", "bs", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "nbest", ")", ":", "\n", "                    ", "if", "(", "output", "[", ":", ",", "i", ",", "j", "]", "==", "eos_index", ")", ".", "sum", "(", ")", "!=", "2", ":", "\n", "                        ", "out", "=", "output", "[", ":", ",", "i", ",", "j", "]", "\n", "hyp", "=", "hyp_best", "[", ":", ",", "i", ",", "j", "]", "\n", "out_l", "=", "(", "(", "out", "!=", "pad_index", ")", "*", "(", "out", "!=", "eos_index", ")", ")", ".", "int", "(", ")", ".", "sum", "(", ")", "+", "2", "\n", "hyp_l", "=", "hyp_len", "[", "i", ",", "j", "]", "\n", "logger", ".", "info", "(", "'[b={},n={}][o={}/h={}] out=\\n{}\\nhyp\\n{}'", ".", "format", "(", "\n", "i", ",", "j", ",", "out_l", ",", "hyp_l", ",", "out", ",", "hyp", "\n", ")", ")", "\n", "", "", "", "raise", "ValueError", "(", "'sanity check fails'", ")", "\n", "# assert sanity_check", "\n", "\n", "", "return", "output", ",", "hyp_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu": [[109, 343], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "efficient_beam_search.beam_efficient_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_len.new.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "model.forward", "model.pred_layer.get_scores", "torch.log_softmax", "_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "worst_better.masked_fill_().bool", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "n_sent_b_t_out_val.masked_fill_.masked_fill_", "generated.new().fill_.gather", "generated.new().fill_.gather", "beam_scores.new.new", "src_len.new.new", "torch.arange().unsqueeze().expand_as.new", "cache.keys", "torch.all", "torch.all", "torch.all", "torch.all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.forward.size", "F.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "word_ids_t.new().fill_().bool", "torch.any", "torch.any", "torch.any", "torch.any", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "n_bucket_full.all", "n_sent_b_t_out_val.masked_fill_.view().type_as", "n_sent_b_t_widx.gather.view", "n_sent_b_t_pidx.gather.view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.new.new", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_enc.unsqueeze().expand().contiguous().view.new().float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "[].detach", "worst_better.masked_fill_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_len.new.new", "src_len.new.new", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "word_ids_t.new().fill_", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "value.detach", "torch.arange().masked_select().long.numel", "[].clone", "hyp.squeeze().detach.gather", "hyp.squeeze().detach.squeeze().detach", "bscore[].detach", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "[].gather", "torch.min", "torch.min", "torch.min", "torch.min", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "[].clone", "torch.min", "torch.min", "torch.min", "torch.min", "n_sent_b_t_out_val.masked_fill_.view", "src_enc.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "int", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "beam_ids[].unsqueeze().unsqueeze().expand", "next_hyp_idx.unsqueeze().expand_as", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.numel", "torch.arange().masked_select().long.numel", "hyps_t[].new().expand_as", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "word_ids_t.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().masked_select().long.size", "hyp.squeeze().detach.squeeze", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "generated[].view", "beam_ids[].unsqueeze().unsqueeze", "next_hyp_idx.unsqueeze", "hyps_t[].new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "beam_ids[].unsqueeze", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "", "def", "generate_beam_gpu", "(", "\n", "model", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "\"\"\"\n        ** Any further upgrade to beam search will based on this default version\n    :param model:\n    :param src_enc:\n    :param src_len:\n    :param tgt_lang_id:\n    :param beam_size:\n    :param length_penalty:\n    :param early_stopping:\n    :param max_len:\n    :param nbest:\n    :param sample_temperature:\n    :param sample_replacement:\n    :param kwargs:\n    :return:\n    \"\"\"", "\n", "# check inputs", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "pad_index", "=", "model", ".", "pad_index", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "model", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "model", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "# generated hypotheses", "\n", "device", "=", "src_enc", ".", "device", "\n", "\n", "done_t", "=", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "# hyps_size = beam_size", "\n", "# hyps_size = hyp_size_multiple * beam_size", "\n", "hyps_size", "=", "beam_size", "\n", "hyps_t", "=", "{", "\n", "\"len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"hyps\"", ":", "generated", ".", "new", "(", "max_len", ",", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "pad_index", ")", ",", "\n", "\"worst\"", ":", "src_enc", ".", "new", "(", "bs", ")", ".", "float", "(", ")", ".", "fill_", "(", "1e9", ")", ",", "\n", "\"score\"", ":", "src_enc", ".", "new", "(", "bs", ",", "hyps_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", ",", "\n", "\"hyp_len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "hyps_size", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "}", "\n", "# testing comapare", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "model", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "model", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# todo: get next words loop tensors, replacing for sent_id in range(bs) loop", "\n", "worst_better", "=", "(", "hyps_t", "[", "'worst'", "]", ">=", "(", "next_scores", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "/", "(", "(", "max_len", "-", "1", ")", "**", "length_penalty", ")", ")", ")", "\n", "len_less_beam", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "new_is_done", "=", "worst_better", ".", "masked_fill_", "(", "len_less_beam", ",", "torch", ".", "tensor", "(", "0", ")", ".", "bool", "(", ")", ")", ".", "bool", "(", ")", "\n", "done_t", "=", "done_t", "|", "new_is_done", "\n", "not_done", "=", "~", "done_t", "\n", "# todo: next_words for sentences", "\n", "beam_ids_t", "=", "next_words", "//", "n_words", "\n", "word_ids_t", "=", "next_words", "%", "n_words", "\n", "sent_beam_idx_t", "=", "beam_ids_t", "+", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "*", "beam_size", "\n", "\n", "is_eos", "=", "(", "word_ids_t", "==", "model", ".", "eos_index", ")", "|", "word_ids_t", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "\n", "int", "(", "cur_len", "+", "1", "==", "max_len", ")", ")", ".", "bool", "(", ")", "\n", "# next_sent_beam = [[] for i in range(bs)]", "\n", "n_sent_b_t_val", "=", "src_enc", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", "\n", "n_sent_b_t_widx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "pad_index", ")", "\n", "n_sent_b_t_pidx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "# todo: future work: how to GPU this loop!", "\n", "for", "i", "in", "range", "(", "2", "*", "beam_size", ")", ":", "\n", "            ", "value", "=", "next_scores", "[", ":", ",", "i", "]", "\n", "is_step_eos", "=", "is_eos", "[", ":", ",", "i", "]", "\n", "beam_ids", "=", "beam_ids_t", "[", ":", ",", "i", "]", "\n", "word_ids", "=", "word_ids_t", "[", ":", ",", "i", "]", "\n", "sent_beam_idx", "=", "sent_beam_idx_t", "[", ":", ",", "i", "]", "\n", "any_step_eos", "=", "torch", ".", "any", "(", "is_step_eos", ")", "\n", "\n", "add_supposed_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "not_done", ")", ".", "long", "(", ")", "\n", "batch_bucket_not_full", "=", "(", "n_sent_b_t_widx", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "allow_to_add", "=", "not_done", "&", "batch_bucket_not_full", "\n", "\n", "n_bucket_full", "=", "(", "n_sent_b_t_widx", "[", "add_supposed_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", ">=", "beam_size", "\n", "\n", "if", "any_step_eos", ":", "\n", "                ", "bscore", "=", "value", ".", "detach", "(", ")", "/", "(", "cur_len", "**", "length_penalty", ")", "# FIXME: check cur_len=len(hyp) in Hypo.add()", "\n", "len_less_beamsize", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "bscore_more_worst", "=", "bscore", ">", "hyps_t", "[", "'worst'", "]", "\n", "\n", "lenlessbeam_bscoremoreworst", "=", "len_less_beamsize", "|", "bscore_more_worst", "\n", "# add_or_not = lenlessbeam_bscoremoreworst & is_step_eos & not_done", "\n", "add_or_not", "=", "lenlessbeam_bscoremoreworst", "&", "is_step_eos", "&", "allow_to_add", "\n", "add_batch_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_or_not", ")", ".", "long", "(", ")", "\n", "if", "add_batch_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "\n", "                    ", "hyp", "=", "generated", "[", ":", "cur_len", "]", ".", "view", "(", "cur_len", ",", "bs", ",", "beam_size", ")", "[", ":", ",", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "hyp", "=", "hyp", ".", "gather", "(", "2", ",", "beam_ids", "[", "add_batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "cur_len", ",", "add_batch_idx", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "hyp", "=", "hyp", ".", "squeeze", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "bscore_batch", "=", "bscore", "[", "add_batch_idx", "]", ".", "detach", "(", ")", "\n", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", ",", "-", "1", "]", "=", "hyp", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", ",", "-", "1", "]", "=", "bscore_batch", "\n", "\n", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", "+=", "1", "\n", "\n", "# resort hyps and score", "\n", "# next_hyp_score, next_hyp_idx = torch.topk(", "\n", "#     hyps_t['score'][add_batch_idx], hyps_size, dim=1, largest=True, sorted=True)", "\n", "next_hyp_score", ",", "next_hyp_idx", "=", "torch", ".", "sort", "(", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", "=", "next_hyp_score", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ".", "gather", "(", "\n", "2", ",", "next_hyp_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ")", ")", "\n", "\n", "# assigning worst", "\n", "min_worst_bscore", "=", "torch", ".", "min", "(", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ",", "bscore_batch", ")", "\n", "min_worst_sort", "=", "next_hyp_score", "[", ":", ",", "-", "1", "]", "\n", "\n", "more_than_beam", "=", "(", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", ">", "beam_size", ")", "\n", "add_batch_more_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "more_than_beam", ")", ".", "long", "(", ")", "\n", "add_batch_less_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "~", "more_than_beam", ")", ".", "long", "(", ")", "\n", "assert", "add_batch_more_idx", ".", "size", "(", "0", ")", "+", "add_batch_less_idx", ".", "size", "(", "0", ")", "==", "add_batch_idx", ".", "size", "(", "0", ")", "\n", "worst_batch_idx", "=", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "\n", "if", "add_batch_more_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_more_idx", "]", "=", "min_worst_sort", "[", "add_batch_more_idx", "]", "\n", "", "if", "add_batch_less_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_less_idx", "]", "=", "min_worst_bscore", "[", "add_batch_less_idx", "]", "\n", "\n", "", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", "=", "worst_batch_idx", "\n", "hyps_t", "[", "'len'", "]", "=", "torch", ".", "min", "(", "hyps_t", "[", "'len'", "]", ",", "hyps_t", "[", "'len'", "]", ".", "new", "(", "[", "beam_size", "]", ")", ".", "expand_as", "(", "hyps_t", "[", "'len'", "]", ")", ")", "\n", "\n", "assert", "(", "hyps_t", "[", "'worst'", "]", ">", "-", "5e8", ")", ".", "all", "(", ")", ",", "'worst -inf: {}'", ".", "format", "(", "hyps_t", "[", "'worst'", "]", ")", "\n", "\n", "# ----- step_end_of_sent == False", "\n", "", "add_next_sent", "=", "(", "(", "~", "is_step_eos", ")", "&", "allow_to_add", ")", "\n", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_next_sent", ")", ".", "long", "(", ")", "\n", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "", "else", ":", "\n", "# all add to next_sent_beam", "\n", "                ", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "allow_to_add", ")", ".", "long", "(", ")", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "\n", "# finish loop beam size", "\n", "", "if", "n_bucket_full", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# update next beam content get beam_size", "\n", "", "", "n_sent_b_t_out_val", ",", "n_sent_b_t_out_idx", "=", "torch", ".", "topk", "(", "n_sent_b_t_val", ",", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "n_sent_b_t_out_val", "=", "n_sent_b_t_out_val", ".", "masked_fill_", "(", "n_sent_b_t_out_val", "<", "-", "5e8", ",", "0", ")", "\n", "\n", "n_sent_b_t_out_widx", "=", "n_sent_b_t_widx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "n_sent_b_t_out_pidx", "=", "n_sent_b_t_pidx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "n_sent_b_t_out_val", ".", "view", "(", "-", "1", ")", ".", "type_as", "(", "beam_scores", ")", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "n_sent_b_t_out_widx", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_idx", "=", "positions", ".", "new", "(", "n_sent_b_t_out_pidx", ".", "view", "(", "-", "1", ")", ")", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "torch", ".", "all", "(", "done_t", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "output", ",", "hyp_len", "=", "beam_efficient_final_decoded", "(", "hyps_t", ",", "bs", ",", "src_len", ",", "pad_index", ",", "model", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "return", "output", ",", "hyp_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu_sample_topn": [[345, 607], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "efficient_beam_search.beam_efficient_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_len.new.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "model.forward", "model.pred_layer.get_scores", "torch.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "logits.gather", "torch.softmax", "leftover_lprobs.view.view", "leftover_words.view.view", "leftover_probs.view().contiguous.view", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "leftover_lprobs.view.gather", "leftover_words.view.gather", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "leftover_words.gather.gather", "worst_better.masked_fill_().bool", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "n_sent_b_t_out_val.masked_fill_.masked_fill_", "generated.new().fill_.gather", "generated.new().fill_.gather", "beam_scores.new.new", "src_len.new.new", "torch.arange().unsqueeze().expand_as.new", "cache.keys", "torch.all", "torch.all", "torch.all", "torch.all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.forward.size", "beam_scores[].expand_as", "leftover_probs.view().contiguous.view().contiguous", "next_scores.size", "choice_words.gather.size", "word_ids_t.new().fill_().bool", "torch.any", "torch.any", "torch.any", "torch.any", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "n_bucket_full.all", "n_sent_b_t_out_val.masked_fill_.view().type_as", "n_sent_b_t_widx.gather.view", "n_sent_b_t_pidx.gather.view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.new.new", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_enc.unsqueeze().expand().contiguous().view.new().float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "[].detach", "worst_better.masked_fill_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_len.new.new", "src_len.new.new", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "leftover_probs.view().contiguous.view", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "word_ids_t.new().fill_", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "value.detach", "torch.arange().masked_select().long.numel", "[].clone", "hyp.squeeze().detach.gather", "hyp.squeeze().detach.squeeze().detach", "bscore[].detach", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "[].gather", "torch.min", "torch.min", "torch.min", "torch.min", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "[].clone", "torch.min", "torch.min", "torch.min", "torch.min", "n_sent_b_t_out_val.masked_fill_.view", "src_enc.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "int", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "beam_ids[].unsqueeze().unsqueeze().expand", "next_hyp_idx.unsqueeze().expand_as", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.numel", "torch.arange().masked_select().long.numel", "hyps_t[].new().expand_as", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "word_ids_t.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().masked_select().long.size", "hyp.squeeze().detach.squeeze", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "generated[].view", "beam_ids[].unsqueeze().unsqueeze", "next_hyp_idx.unsqueeze", "hyps_t[].new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "beam_ids[].unsqueeze", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate_beam_gpu_sample_topn", "(", "\n", "model", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "replacement", "=", "False", ",", "\n", "sample_topn", "=", "100", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "\"\"\"\n        ** this version acquire the top_n words, and then sample them into 2 * beam_size\n    :param model:\n    :param src_enc:\n    :param src_len:\n    :param tgt_lang_id:\n    :param beam_size:\n    :param length_penalty:\n    :param early_stopping:\n    :param max_len:\n    :param nbest:\n    :param sample_temperature:\n    :param sample_replacement:\n    :param sample_topn:\n    :param kwargs:\n    :return:\n    \"\"\"", "\n", "# check inputs", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "assert", "sample_topn", ">", "2", "*", "beam_size", "\n", "if", "sample_temperature", "is", "None", ":", "\n", "        ", "sample_temperature", "=", "1", "\n", "", "pad_index", "=", "model", ".", "pad_index", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "model", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "model", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "# generated hypotheses", "\n", "device", "=", "src_enc", ".", "device", "\n", "\n", "done_t", "=", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "# hyps_size = beam_size", "\n", "# hyps_size = hyp_size_multiple * beam_size", "\n", "hyps_size", "=", "beam_size", "\n", "hyps_t", "=", "{", "\n", "\"len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"hyps\"", ":", "generated", ".", "new", "(", "max_len", ",", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "pad_index", ")", ",", "\n", "\"worst\"", ":", "src_enc", ".", "new", "(", "bs", ")", ".", "float", "(", ")", ".", "fill_", "(", "1e9", ")", ",", "\n", "\"score\"", ":", "src_enc", ".", "new", "(", "bs", ",", "hyps_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", ",", "\n", "\"hyp_len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "hyps_size", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "}", "\n", "# testing comapare", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "model", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "model", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "logits", "=", "scores", "/", "sample_temperature", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "\n", "_lprobs", "=", "lprobs", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "logits", ")", "# (bs * beam_size, n_words)", "\n", "# _lprobs = _lprobs.view(bs, beam_size * n_words)  # (bs, beam_size * n_words)", "\n", "leftover_lprobs", ",", "leftover_words", "=", "torch", ".", "topk", "(", "_lprobs", ",", "sample_topn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "# leftover_lprobs:  (bs * beam_size, sample_topn)", "\n", "# leftover_words:   (bs * beam_size, sample_topn)", "\n", "leftover_logits", "=", "logits", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "leftover_words", ")", "\n", "leftover_probs", "=", "F", ".", "softmax", "(", "leftover_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# convert to beam_size * sample_topn", "\n", "leftover_lprobs", "=", "leftover_lprobs", ".", "view", "(", "bs", ",", "beam_size", "*", "sample_topn", ")", "\n", "leftover_words", "=", "leftover_words", ".", "view", "(", "bs", ",", "beam_size", "*", "sample_topn", ")", "\n", "if", "cur_len", "==", "1", ":", "\n", "            ", "leftover_probs", "=", "leftover_probs", ".", "view", "(", "bs", ",", "beam_size", ",", "sample_topn", ")", ".", "contiguous", "(", ")", "\n", "leftover_probs", "[", ":", ",", "1", ":", ",", ":", "]", "=", "0", "\n", "", "leftover_probs", "=", "leftover_probs", ".", "view", "(", "bs", ",", "beam_size", "*", "sample_topn", ")", "\n", "\n", "# assert (leftover_probs > 0).sum(-1)", "\n", "assert", "(", "(", "leftover_probs", ">", "0", ")", ".", "sum", "(", "-", "1", ")", ">=", "2", "*", "beam_size", ")", ".", "all", "(", ")", ",", "'(leftover_probs > 0)<2beam, consider replacement'", "\n", "choice_sample_indices", "=", "torch", ".", "multinomial", "(", "leftover_probs", ",", "2", "*", "beam_size", ",", "replacement", "=", "replacement", ")", "\n", "choice_lprobs", "=", "leftover_lprobs", ".", "gather", "(", "1", ",", "choice_sample_indices", ")", "\n", "choice_words", "=", "leftover_words", ".", "gather", "(", "1", ",", "choice_sample_indices", ")", "\n", "\n", "# sort for scores and words", "\n", "next_scores", ",", "next_word_idx", "=", "torch", ".", "sort", "(", "choice_lprobs", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "next_words", "=", "choice_words", ".", "gather", "(", "1", ",", "next_word_idx", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "assert", "(", "next_scores", ">", "-", "1e9", ")", ".", "all", "(", ")", ",", "'cur_len={}, next_scores: {}'", ".", "format", "(", "cur_len", ",", "next_scores", ")", "\n", "\n", "# todo: get next words loop tensors, replacing for sent_id in range(bs) loop", "\n", "worst_better", "=", "(", "hyps_t", "[", "'worst'", "]", ">=", "(", "next_scores", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "/", "(", "(", "max_len", "-", "1", ")", "**", "length_penalty", ")", ")", ")", "\n", "len_less_beam", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "new_is_done", "=", "worst_better", ".", "masked_fill_", "(", "len_less_beam", ",", "torch", ".", "tensor", "(", "0", ")", ".", "bool", "(", ")", ")", ".", "bool", "(", ")", "\n", "done_t", "=", "done_t", "|", "new_is_done", "\n", "not_done", "=", "~", "done_t", "\n", "# todo: next_words for sentences", "\n", "beam_ids_t", "=", "next_words", "//", "n_words", "\n", "word_ids_t", "=", "next_words", "%", "n_words", "\n", "sent_beam_idx_t", "=", "beam_ids_t", "+", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "*", "beam_size", "\n", "\n", "is_eos", "=", "(", "word_ids_t", "==", "model", ".", "eos_index", ")", "|", "word_ids_t", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "\n", "int", "(", "cur_len", "+", "1", "==", "max_len", ")", ")", ".", "bool", "(", ")", "\n", "# next_sent_beam = [[] for i in range(bs)]", "\n", "n_sent_b_t_val", "=", "src_enc", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", "\n", "n_sent_b_t_widx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "pad_index", ")", "\n", "n_sent_b_t_pidx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "# todo: future work: how to GPU this loop!", "\n", "for", "i", "in", "range", "(", "2", "*", "beam_size", ")", ":", "\n", "            ", "value", "=", "next_scores", "[", ":", ",", "i", "]", "\n", "is_step_eos", "=", "is_eos", "[", ":", ",", "i", "]", "\n", "beam_ids", "=", "beam_ids_t", "[", ":", ",", "i", "]", "\n", "word_ids", "=", "word_ids_t", "[", ":", ",", "i", "]", "\n", "sent_beam_idx", "=", "sent_beam_idx_t", "[", ":", ",", "i", "]", "\n", "any_step_eos", "=", "torch", ".", "any", "(", "is_step_eos", ")", "\n", "\n", "add_supposed_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "not_done", ")", ".", "long", "(", ")", "\n", "batch_bucket_not_full", "=", "(", "n_sent_b_t_widx", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "allow_to_add", "=", "not_done", "&", "batch_bucket_not_full", "\n", "\n", "n_bucket_full", "=", "(", "n_sent_b_t_widx", "[", "add_supposed_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", ">=", "beam_size", "\n", "\n", "if", "any_step_eos", ":", "\n", "                ", "bscore", "=", "value", ".", "detach", "(", ")", "/", "(", "cur_len", "**", "length_penalty", ")", "# FIXME: check cur_len=len(hyp) in Hypo.add()", "\n", "len_less_beamsize", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "bscore_more_worst", "=", "bscore", ">", "hyps_t", "[", "'worst'", "]", "\n", "\n", "lenlessbeam_bscoremoreworst", "=", "len_less_beamsize", "|", "bscore_more_worst", "\n", "# add_or_not = lenlessbeam_bscoremoreworst & is_step_eos & not_done", "\n", "add_or_not", "=", "lenlessbeam_bscoremoreworst", "&", "is_step_eos", "&", "allow_to_add", "\n", "add_batch_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_or_not", ")", ".", "long", "(", ")", "\n", "if", "add_batch_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "\n", "                    ", "hyp", "=", "generated", "[", ":", "cur_len", "]", ".", "view", "(", "cur_len", ",", "bs", ",", "beam_size", ")", "[", ":", ",", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "hyp", "=", "hyp", ".", "gather", "(", "2", ",", "beam_ids", "[", "add_batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "cur_len", ",", "add_batch_idx", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "hyp", "=", "hyp", ".", "squeeze", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "bscore_batch", "=", "bscore", "[", "add_batch_idx", "]", ".", "detach", "(", ")", "\n", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", ",", "-", "1", "]", "=", "hyp", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", ",", "-", "1", "]", "=", "bscore_batch", "\n", "\n", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", "+=", "1", "\n", "\n", "# resort hyps and score", "\n", "# next_hyp_score, next_hyp_idx = torch.topk(", "\n", "#     hyps_t['score'][add_batch_idx], hyps_size, dim=1, largest=True, sorted=True)", "\n", "next_hyp_score", ",", "next_hyp_idx", "=", "torch", ".", "sort", "(", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", "=", "next_hyp_score", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ".", "gather", "(", "\n", "2", ",", "next_hyp_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ")", ")", "\n", "\n", "# assigning worst", "\n", "min_worst_bscore", "=", "torch", ".", "min", "(", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ",", "bscore_batch", ")", "\n", "min_worst_sort", "=", "next_hyp_score", "[", ":", ",", "-", "1", "]", "\n", "\n", "more_than_beam", "=", "(", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", ">", "beam_size", ")", "\n", "add_batch_more_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "more_than_beam", ")", ".", "long", "(", ")", "\n", "add_batch_less_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "~", "more_than_beam", ")", ".", "long", "(", ")", "\n", "assert", "add_batch_more_idx", ".", "size", "(", "0", ")", "+", "add_batch_less_idx", ".", "size", "(", "0", ")", "==", "add_batch_idx", ".", "size", "(", "0", ")", "\n", "worst_batch_idx", "=", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "\n", "if", "add_batch_more_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_more_idx", "]", "=", "min_worst_sort", "[", "add_batch_more_idx", "]", "\n", "", "if", "add_batch_less_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_less_idx", "]", "=", "min_worst_bscore", "[", "add_batch_less_idx", "]", "\n", "\n", "", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", "=", "worst_batch_idx", "\n", "hyps_t", "[", "'len'", "]", "=", "torch", ".", "min", "(", "hyps_t", "[", "'len'", "]", ",", "hyps_t", "[", "'len'", "]", ".", "new", "(", "[", "beam_size", "]", ")", ".", "expand_as", "(", "hyps_t", "[", "'len'", "]", ")", ")", "\n", "\n", "assert", "(", "hyps_t", "[", "'worst'", "]", ">", "-", "5e8", ")", ".", "all", "(", ")", ",", "'worst -inf: {}'", ".", "format", "(", "hyps_t", "[", "'worst'", "]", ")", "\n", "\n", "# ----- step_end_of_sent == False", "\n", "", "add_next_sent", "=", "(", "(", "~", "is_step_eos", ")", "&", "allow_to_add", ")", "\n", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_next_sent", ")", ".", "long", "(", ")", "\n", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "", "else", ":", "\n", "# all add to next_sent_beam", "\n", "                ", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "allow_to_add", ")", ".", "long", "(", ")", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "\n", "# finish loop beam size", "\n", "", "if", "n_bucket_full", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# update next beam content?", "\n", "", "", "n_sent_b_t_out_val", ",", "n_sent_b_t_out_idx", "=", "torch", ".", "topk", "(", "\n", "n_sent_b_t_val", ",", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "n_sent_b_t_out_val", "=", "n_sent_b_t_out_val", ".", "masked_fill_", "(", "n_sent_b_t_out_val", "<", "-", "5e8", ",", "0", ")", "\n", "\n", "n_sent_b_t_out_widx", "=", "n_sent_b_t_widx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "n_sent_b_t_out_pidx", "=", "n_sent_b_t_pidx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "n_sent_b_t_out_val", ".", "view", "(", "-", "1", ")", ".", "type_as", "(", "beam_scores", ")", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "n_sent_b_t_out_widx", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_idx", "=", "positions", ".", "new", "(", "n_sent_b_t_out_pidx", ".", "view", "(", "-", "1", ")", ")", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "torch", ".", "all", "(", "done_t", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "output", ",", "hyp_len", "=", "beam_efficient_final_decoded", "(", "hyps_t", ",", "bs", ",", "src_len", ",", "pad_index", ",", "model", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "return", "output", ",", "hyp_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_diverse_beam_search_gpu": [[609, 889], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "beam_scores.new.view", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "efficient_beam_search.beam_efficient_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_len.new.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "model.forward", "model.pred_layer.get_scores", "torch.log_softmax", "F.log_softmax.view", "beam_scores.new.view().contiguous().unsqueeze_().expand_as", "range", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "torch.stack().view", "worst_better.masked_fill_().bool", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "n_sent_b_t_out_val.masked_fill_.masked_fill_", "generated.new().fill_.gather", "generated.new().fill_.gather", "beam_scores.new.new", "src_len.new.new", "torch.arange().unsqueeze().expand_as.new", "cache.keys", "torch.all", "torch.all", "torch.all", "torch.all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_enc.unsqueeze().expand().contiguous().view.new().float", "model.forward.size", "F.log_softmax.size", "_sent_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "_sub_beam_ids.mul_().add_", "scores_G.append", "words_G.append", "beams_G.append", "diversity_buf.scatter_add_.scatter_add_", "torch.stack().view.new().fill_().bool", "torch.any", "torch.any", "torch.any", "torch.any", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "n_bucket_full.all", "n_sent_b_t_out_val.masked_fill_.view().type_as", "n_sent_b_t_widx.gather.view", "n_sent_b_t_pidx.gather.view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.new.new", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_enc.unsqueeze().expand().contiguous().view.new().float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "beam_scores.new.view().contiguous().unsqueeze_", "lprobs_g.contiguous.size", "lprobs_g.contiguous.size", "torch.add", "torch.add", "torch.add", "torch.add", "lprobs_g.contiguous.contiguous", "_next_scores.size", "_next_words.size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "[].detach", "worst_better.masked_fill_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_len.new.new", "src_len.new.new", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_enc.unsqueeze().expand().contiguous().view.new", "diversity_buf.scatter_add_.unsqueeze", "_sub_beam_ids.mul_", "diversity_buf.scatter_add_.new_ones", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.stack().view.new().fill_", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "value.detach", "torch.arange().masked_select().long.numel", "[].clone", "hyp.squeeze().detach.gather", "hyp.squeeze().detach.squeeze().detach", "bscore[].detach", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "[].gather", "torch.min", "torch.min", "torch.min", "torch.min", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "[].clone", "torch.min", "torch.min", "torch.min", "torch.min", "n_sent_b_t_out_val.masked_fill_.view", "src_enc.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new", "beam_scores.new.view().contiguous", "_word_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "int", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "beam_ids[].unsqueeze().unsqueeze().expand", "next_hyp_idx.unsqueeze().expand_as", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.numel", "torch.arange().masked_select().long.numel", "hyps_t[].new().expand_as", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "torch.stack().view.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.stack().view.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().masked_select().long.size", "hyp.squeeze().detach.squeeze", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size", "beam_scores.new.view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "generated[].view", "beam_ids[].unsqueeze().unsqueeze", "next_hyp_idx.unsqueeze", "hyps_t[].new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "beam_ids[].unsqueeze", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_diverse_beam_search_gpu", "(", "\n", "model", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "num_groups", ",", "diversity_strength", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "\n", ")", ":", "\n", "# check inputs", "\n", "    ", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "assert", "num_groups", "is", "not", "None", "and", "beam_size", "%", "num_groups", "==", "0", "\n", "assert", "diversity_strength", "is", "not", "None", "\n", "# diversity_strength : (0.2 - 0.8)", "\n", "diversity_strength", "=", "-", "diversity_strength", "\n", "sub_beam_size", "=", "beam_size", "//", "num_groups", "\n", "\n", "pad_index", "=", "model", ".", "pad_index", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "model", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "model", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "# beam_scores = src_enc.new(bs, beam_size).fill_(0)", "\n", "# beam_scores[:, 1:] = torch.tensor(-1e9).type_as(beam_scores)", "\n", "# beam_scores = beam_scores.view(-1)", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "src_enc", ")", ")", "\n", "beam_scores", "[", ":", ",", ":", "num_groups", "]", "=", "0", "\n", "# [x, y, x, y, x, y] -> [0, 0, x, y, x, y]", "\n", "# [x, y, z, x, y, z, x, y, z] -> [0, 0, x, y, x, y]", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "# generated hypotheses", "\n", "device", "=", "src_enc", ".", "device", "\n", "\n", "done_t", "=", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "# hyps_size = beam_size", "\n", "# hyps_size = hyp_size_multiple * beam_size", "\n", "hyps_size", "=", "beam_size", "\n", "hyps_t", "=", "{", "\n", "\"len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"hyps\"", ":", "generated", ".", "new", "(", "max_len", ",", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "pad_index", ")", ",", "\n", "\"worst\"", ":", "src_enc", ".", "new", "(", "bs", ")", ".", "float", "(", ")", ".", "fill_", "(", "1e9", ")", ",", "\n", "\"score\"", ":", "src_enc", ".", "new", "(", "bs", ",", "hyps_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", ",", "\n", "\"hyp_len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "hyps_size", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "}", "\n", "# testing comapare", "\n", "diversity_buf", "=", "src_enc", ".", "new", "(", "bs", ",", "n_words", ")", ".", "float", "(", ")", ".", "fill_", "(", "0", ")", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "model", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "model", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "lprobs", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "# select next words with scores", "\n", "# _scores = lprobs + beam_scores[:, None].expand_as(lprobs)  # (bs * beam_size, n_words)", "\n", "# _scores = _scores.view(bs, beam_size * n_words)  # (bs, beam_size * n_words)", "\n", "# next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)", "\n", "# assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)", "\n", "\n", "# ----- diverse beam search----", "\n", "\n", "_lprobs", "=", "lprobs", ".", "view", "(", "bs", ",", "beam_size", ",", "n_words", ")", "\n", "_beam_scores", "=", "beam_scores", ".", "view", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "unsqueeze_", "(", "-", "1", ")", ".", "expand_as", "(", "_lprobs", ")", "\n", "scores_G", ",", "words_G", ",", "beams_G", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "g", "in", "range", "(", "num_groups", ")", ":", "\n", "            ", "lprobs_g", "=", "_lprobs", "[", ":", ",", "g", ":", ":", "num_groups", "]", "\n", "# scores_g = _beam_scores[:, g::num_groups, :] if cur_len > 1 else None", "\n", "scores_g", "=", "_beam_scores", "[", ":", ",", "g", ":", ":", "num_groups", ",", ":", "]", "\n", "assert", "lprobs_g", ".", "size", "(", "1", ")", "==", "sub_beam_size", ",", "'{} != {}'", ".", "format", "(", "lprobs_g", ".", "size", "(", "1", ")", ",", "sub_beam_size", ")", "\n", "if", "cur_len", "==", "1", ":", "\n", "                ", "assert", "(", "scores_g", "[", ":", ",", "1", ":", "]", "<", "-", "1e3", ")", ".", "all", "(", ")", "and", "(", "scores_g", "[", ":", ",", "0", "]", "==", "0", ")", ".", "all", "(", ")", ",", "'scores_g: {}'", ".", "format", "(", "scores_g", ")", "\n", "\n", "# apply diversity penalty", "\n", "", "if", "g", ">", "0", ":", "\n", "                ", "lprobs_g", "=", "torch", ".", "add", "(", "lprobs_g", ",", "diversity_strength", ",", "diversity_buf", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "lprobs_g", "=", "lprobs_g", ".", "contiguous", "(", ")", "\n", "\n", "# beam step for each group", "\n", "", "_sent_scores", "=", "lprobs_g", "+", "scores_g", "\n", "_sent_scores", "=", "_sent_scores", ".", "view", "(", "bs", ",", "sub_beam_size", "*", "n_words", ")", "\n", "_next_scores", ",", "_next_words", "=", "torch", ".", "topk", "(", "_sent_scores", ",", "2", "*", "sub_beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "_next_scores", ".", "size", "(", ")", "==", "_next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "sub_beam_size", ")", "\n", "# num=2, subbeam=3, g=0, -> subbeam=0 => beam=0 ?", "\n", "# num=2, subbeam=3, g=0, -> subbeam=1 => beam=2 ?", "\n", "# num=2, subbeam=3, g=0, -> subbeam=2 => beam=4 ?", "\n", "\n", "# num=2, subbeam=3, g=1, -> subbeam=0 => beam=1 ?", "\n", "# num=2, subbeam=3, g=1, -> subbeam=1 => beam=3 ?", "\n", "# num=2, subbeam=3, g=1, -> subbeam=2 => beam=5 ?", "\n", "# it's interleaving", "\n", "_sub_beam_ids", "=", "_next_words", "//", "n_words", "\n", "_sub_beam_ids", ".", "mul_", "(", "num_groups", ")", ".", "add_", "(", "g", ")", "\n", "_word_ids", "=", "_next_words", "%", "n_words", "\n", "\n", "scores_G", ".", "append", "(", "_next_scores", ")", "\n", "words_G", ".", "append", "(", "_word_ids", ")", "\n", "beams_G", ".", "append", "(", "_sub_beam_ids", ")", "\n", "# update diversity penalty", "\n", "diversity_buf", "=", "diversity_buf", ".", "scatter_add_", "(", "\n", "dim", "=", "1", ",", "index", "=", "_word_ids", ",", "src", "=", "diversity_buf", ".", "new_ones", "(", "_word_ids", ".", "size", "(", ")", ")", "\n", ")", "\n", "", "next_scores", "=", "torch", ".", "stack", "(", "scores_G", ",", "dim", "=", "2", ")", ".", "view", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "word_ids_t", "=", "torch", ".", "stack", "(", "words_G", ",", "dim", "=", "2", ")", ".", "view", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "beam_ids_t", "=", "torch", ".", "stack", "(", "beams_G", ",", "dim", "=", "2", ")", ".", "view", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# ---------------------------", "\n", "\n", "# todo: get next words loop tensors, replacing for sent_id in range(bs) loop", "\n", "worst_better", "=", "(", "hyps_t", "[", "'worst'", "]", ">=", "(", "next_scores", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "/", "(", "(", "max_len", "-", "1", ")", "**", "length_penalty", ")", ")", ")", "\n", "len_less_beam", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "new_is_done", "=", "worst_better", ".", "masked_fill_", "(", "len_less_beam", ",", "torch", ".", "tensor", "(", "0", ")", ".", "bool", "(", ")", ")", ".", "bool", "(", ")", "\n", "done_t", "=", "done_t", "|", "new_is_done", "\n", "not_done", "=", "~", "done_t", "\n", "# todo: next_words for sentences", "\n", "# beam_ids_t = next_words // n_words", "\n", "# word_ids_t = next_words % n_words", "\n", "sent_beam_idx_t", "=", "beam_ids_t", "+", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "*", "beam_size", "\n", "\n", "is_eos", "=", "(", "word_ids_t", "==", "model", ".", "eos_index", ")", "|", "word_ids_t", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "\n", "int", "(", "cur_len", "+", "1", "==", "max_len", ")", ")", ".", "bool", "(", ")", "\n", "# next_sent_beam = [[] for i in range(bs)]", "\n", "n_sent_b_t_val", "=", "src_enc", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", "\n", "n_sent_b_t_widx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "pad_index", ")", "\n", "n_sent_b_t_pidx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "# todo: future work: how to GPU this loop!", "\n", "for", "i", "in", "range", "(", "2", "*", "beam_size", ")", ":", "\n", "            ", "value", "=", "next_scores", "[", ":", ",", "i", "]", "\n", "is_step_eos", "=", "is_eos", "[", ":", ",", "i", "]", "\n", "beam_ids", "=", "beam_ids_t", "[", ":", ",", "i", "]", "\n", "word_ids", "=", "word_ids_t", "[", ":", ",", "i", "]", "\n", "sent_beam_idx", "=", "sent_beam_idx_t", "[", ":", ",", "i", "]", "\n", "any_step_eos", "=", "torch", ".", "any", "(", "is_step_eos", ")", "\n", "\n", "add_supposed_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "not_done", ")", ".", "long", "(", ")", "\n", "batch_bucket_not_full", "=", "(", "n_sent_b_t_widx", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "allow_to_add", "=", "not_done", "&", "batch_bucket_not_full", "\n", "\n", "n_bucket_full", "=", "(", "n_sent_b_t_widx", "[", "add_supposed_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", ">=", "beam_size", "\n", "\n", "if", "any_step_eos", ":", "\n", "                ", "bscore", "=", "value", ".", "detach", "(", ")", "/", "(", "cur_len", "**", "length_penalty", ")", "# FIXME: check cur_len=len(hyp) in Hypo.add()", "\n", "len_less_beamsize", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "bscore_more_worst", "=", "bscore", ">", "hyps_t", "[", "'worst'", "]", "\n", "\n", "lenlessbeam_bscoremoreworst", "=", "len_less_beamsize", "|", "bscore_more_worst", "\n", "# add_or_not = lenlessbeam_bscoremoreworst & is_step_eos & not_done", "\n", "add_or_not", "=", "lenlessbeam_bscoremoreworst", "&", "is_step_eos", "&", "allow_to_add", "\n", "add_batch_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_or_not", ")", ".", "long", "(", ")", "\n", "if", "add_batch_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "\n", "                    ", "hyp", "=", "generated", "[", ":", "cur_len", "]", ".", "view", "(", "cur_len", ",", "bs", ",", "beam_size", ")", "[", ":", ",", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "hyp", "=", "hyp", ".", "gather", "(", "2", ",", "beam_ids", "[", "add_batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "cur_len", ",", "add_batch_idx", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "hyp", "=", "hyp", ".", "squeeze", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "bscore_batch", "=", "bscore", "[", "add_batch_idx", "]", ".", "detach", "(", ")", "\n", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", ",", "-", "1", "]", "=", "hyp", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", ",", "-", "1", "]", "=", "bscore_batch", "\n", "\n", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", "+=", "1", "\n", "\n", "# resort hyps and score", "\n", "# next_hyp_score, next_hyp_idx = torch.topk(", "\n", "#     hyps_t['score'][add_batch_idx], hyps_size, dim=1, largest=True, sorted=True)", "\n", "next_hyp_score", ",", "next_hyp_idx", "=", "torch", ".", "sort", "(", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", "=", "next_hyp_score", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ".", "gather", "(", "\n", "2", ",", "next_hyp_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ")", ")", "\n", "\n", "# assigning worst", "\n", "min_worst_bscore", "=", "torch", ".", "min", "(", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ",", "bscore_batch", ")", "\n", "min_worst_sort", "=", "next_hyp_score", "[", ":", ",", "-", "1", "]", "\n", "\n", "more_than_beam", "=", "(", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", ">", "beam_size", ")", "\n", "add_batch_more_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "more_than_beam", ")", ".", "long", "(", ")", "\n", "add_batch_less_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "~", "more_than_beam", ")", ".", "long", "(", ")", "\n", "assert", "add_batch_more_idx", ".", "size", "(", "0", ")", "+", "add_batch_less_idx", ".", "size", "(", "0", ")", "==", "add_batch_idx", ".", "size", "(", "0", ")", "\n", "worst_batch_idx", "=", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "\n", "if", "add_batch_more_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_more_idx", "]", "=", "min_worst_sort", "[", "add_batch_more_idx", "]", "\n", "", "if", "add_batch_less_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_less_idx", "]", "=", "min_worst_bscore", "[", "add_batch_less_idx", "]", "\n", "\n", "", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", "=", "worst_batch_idx", "\n", "hyps_t", "[", "'len'", "]", "=", "torch", ".", "min", "(", "hyps_t", "[", "'len'", "]", ",", "hyps_t", "[", "'len'", "]", ".", "new", "(", "[", "beam_size", "]", ")", ".", "expand_as", "(", "hyps_t", "[", "'len'", "]", ")", ")", "\n", "\n", "assert", "(", "hyps_t", "[", "'worst'", "]", ">", "-", "5e8", ")", ".", "all", "(", ")", ",", "'worst -inf: {}'", ".", "format", "(", "hyps_t", "[", "'worst'", "]", ")", "\n", "\n", "# ----- step_end_of_sent == False", "\n", "", "add_next_sent", "=", "(", "(", "~", "is_step_eos", ")", "&", "allow_to_add", ")", "\n", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_next_sent", ")", ".", "long", "(", ")", "\n", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "", "else", ":", "\n", "# all add to next_sent_beam", "\n", "                ", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "allow_to_add", ")", ".", "long", "(", ")", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "\n", "# finish loop beam size", "\n", "", "if", "n_bucket_full", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# update next beam content? get beam_size", "\n", "", "", "n_sent_b_t_out_val", ",", "n_sent_b_t_out_idx", "=", "torch", ".", "topk", "(", "\n", "n_sent_b_t_val", ",", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "n_sent_b_t_out_val", "=", "n_sent_b_t_out_val", ".", "masked_fill_", "(", "n_sent_b_t_out_val", "<", "-", "5e8", ",", "0", ")", "\n", "\n", "n_sent_b_t_out_widx", "=", "n_sent_b_t_widx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "n_sent_b_t_out_pidx", "=", "n_sent_b_t_pidx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "n_sent_b_t_out_val", ".", "view", "(", "-", "1", ")", ".", "type_as", "(", "beam_scores", ")", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "n_sent_b_t_out_widx", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_idx", "=", "positions", ".", "new", "(", "n_sent_b_t_out_pidx", ".", "view", "(", "-", "1", ")", ")", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "torch", ".", "all", "(", "done_t", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "output", ",", "hyp_len", "=", "beam_efficient_final_decoded", "(", "hyps_t", ",", "bs", ",", "src_len", ",", "pad_index", ",", "model", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "return", "output", ",", "hyp_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu_backup1": [[891, 1103], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "efficient_beam_search.beam_efficient_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_len.new.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "model.forward", "model.pred_layer.get_scores", "worst_better.masked_fill_().bool", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "n_sent_b_t_out_val.masked_fill_.masked_fill_", "generated.new().fill_.gather", "generated.new().fill_.gather", "beam_scores.new.new", "src_len.new.new", "torch.arange().unsqueeze().expand_as.new", "cache.keys", "torch.all", "torch.all", "torch.all", "torch.all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.forward.size", "torch.log_softmax", "_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.log_softmax", "_lprobs.view.view", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "_lprobs.view.gather", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.multinomial.gather", "word_ids_t.new().fill_().bool", "torch.any", "torch.any", "torch.any", "torch.any", "torch.zeros().bool.all", "n_sent_b_t_out_val.masked_fill_.view", "n_sent_b_t_widx.gather.view", "n_sent_b_t_pidx.gather.view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.new.new", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "F.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "choice_words.gather.size", "beam_scores[].expand_as", "_lprobs.view.exp", "next_scores.size", "choice_words.gather.size", "[].detach", "worst_better.masked_fill_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.new.new", "src_len.new.new", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "value[].type_as", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "value[].type_as", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "word_ids_t.new().fill_", "value.detach", "torch.arange().masked_select().long.numel", "[].clone", "hyp.squeeze().detach.gather", "hyp.squeeze().detach.squeeze().detach", "bscore[].detach().type_as", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.min", "torch.min", "torch.min", "torch.min", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "[].clone", "[].gather", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "int", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "beam_ids[].unsqueeze().unsqueeze().expand", "torch.arange().masked_select().long.numel", "torch.arange().masked_select().long.numel", "next_hyp_idx.unsqueeze().expand_as", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "word_ids_t.new", "torch.arange().masked_select().long.size", "hyp.squeeze().detach.squeeze", "bscore[].detach", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "generated[].view", "beam_ids[].unsqueeze().unsqueeze", "next_hyp_idx.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "beam_ids[].unsqueeze", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate_beam_gpu_backup1", "(", "\n", "model", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "hyps_size_multiple", "=", "1", "\n", ")", ":", "\n", "# check inputs", "\n", "    ", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "model", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "model", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "model", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "# generated hypotheses", "\n", "device", "=", "src_enc", ".", "device", "\n", "\n", "done_t", "=", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "# hyps_size = beam_size", "\n", "hyps_size", "=", "hyps_size_multiple", "*", "beam_size", "\n", "hyps_t", "=", "{", "\n", "\"len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"hyps\"", ":", "generated", ".", "new", "(", "max_len", ",", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "model", ".", "pad_index", ")", ",", "\n", "\"score\"", ":", "src_enc", ".", "new", "(", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "-", "1e4", ")", ",", "\n", "\"hyp_len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "hyps_size", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"worst\"", ":", "src_enc", ".", "new", "(", "bs", ")", ".", "fill_", "(", "1e4", ")", ",", "\n", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "model", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "model", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "\n", "if", "sample_temperature", "is", "None", "or", "sample_temperature", "==", "1.0", ":", "\n", "            ", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "", "else", ":", "\n", "            ", "lprobs", "=", "F", ".", "log_softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "\n", "_lprobs", "=", "lprobs", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_lprobs", "=", "_lprobs", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "choice_words", "=", "torch", ".", "multinomial", "(", "_lprobs", ".", "exp", "(", ")", ",", "2", "*", "beam_size", ",", "replacement", "=", "False", ")", "\n", "choice_lprobs", "=", "_lprobs", ".", "gather", "(", "1", ",", "choice_words", ")", "\n", "\n", "next_scores", ",", "next_word_idx", "=", "torch", ".", "topk", "(", "choice_lprobs", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "next_words", "=", "choice_words", ".", "gather", "(", "1", ",", "next_word_idx", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# todo: get next words loop tensors, replacing for sent_id in range(bs) loop", "\n", "", "worst_better", "=", "(", "hyps_t", "[", "'worst'", "]", ">=", "(", "next_scores", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "/", "(", "(", "max_len", "-", "1", ")", "**", "length_penalty", ")", ")", ")", "\n", "is_done", "=", "worst_better", ".", "masked_fill_", "(", "hyps_t", "[", "'len'", "]", "<", "beam_size", ",", "torch", ".", "tensor", "(", "0", ")", ".", "bool", "(", ")", ")", ".", "bool", "(", ")", "\n", "# old_done_t = done_t", "\n", "done_t", "=", "done_t", "|", "is_done", "\n", "not_done", "=", "~", "done_t", "\n", "# todo: next_words for sentences", "\n", "beam_ids_t", "=", "next_words", "//", "n_words", "\n", "word_ids_t", "=", "next_words", "%", "n_words", "\n", "sent_beam_idx_t", "=", "beam_ids_t", "+", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "*", "beam_size", "\n", "\n", "is_eos", "=", "(", "word_ids_t", "==", "model", ".", "eos_index", ")", "|", "word_ids_t", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "\n", "int", "(", "cur_len", "+", "1", "==", "max_len", ")", ")", ".", "bool", "(", ")", "\n", "# next_sent_beam = [[] for i in range(bs)]", "\n", "n_sent_b_t_val", "=", "src_enc", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "-", "1e4", ")", "\n", "n_sent_b_t_widx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "model", ".", "pad_index", ")", "\n", "n_sent_b_t_pidx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "# todo: future work: how to GPU this loop!", "\n", "for", "i", "in", "range", "(", "2", "*", "beam_size", ")", ":", "\n", "# idx = next_words[:, i]", "\n", "            ", "value", "=", "next_scores", "[", ":", ",", "i", "]", "\n", "is_step_eos", "=", "is_eos", "[", ":", ",", "i", "]", "\n", "beam_ids", "=", "beam_ids_t", "[", ":", ",", "i", "]", "\n", "word_ids", "=", "word_ids_t", "[", ":", ",", "i", "]", "\n", "sent_beam_idx", "=", "sent_beam_idx_t", "[", ":", ",", "i", "]", "\n", "any_step_eos", "=", "torch", ".", "any", "(", "is_step_eos", ")", "\n", "\n", "if", "any_step_eos", ":", "\n", "                ", "bscore", "=", "value", ".", "detach", "(", ")", "/", "(", "cur_len", "**", "length_penalty", ")", "# FIXME: check cur_len=len(hyp) in Hypo.add()", "\n", "len_less_beamsize", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "bscore_more_worst", "=", "bscore", ">", "hyps_t", "[", "'worst'", "]", "\n", "\n", "# lenlessbeam_bscoremoreworst = len_less_beamsize | bscore_more_worst", "\n", "# add_or_not = lenlessbeam_bscoremoreworst & is_step_eos & not_done", "\n", "add_or_not", "=", "(", "len_less_beamsize", "|", "bscore_more_worst", ")", "&", "is_step_eos", "&", "not_done", "\n", "add_batch_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_or_not", ")", ".", "long", "(", ")", "\n", "if", "add_batch_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "\n", "                    ", "hyp", "=", "generated", "[", ":", "cur_len", "]", ".", "view", "(", "cur_len", ",", "bs", ",", "beam_size", ")", "[", ":", ",", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "hyp", "=", "hyp", ".", "gather", "(", "2", ",", "beam_ids", "[", "add_batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "cur_len", ",", "add_batch_idx", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "hyp", "=", "hyp", ".", "squeeze", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "bscore_batch", "=", "bscore", "[", "add_batch_idx", "]", ".", "detach", "(", ")", ".", "type_as", "(", "hyps_t", "[", "'score'", "]", ")", "\n", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", ",", "-", "1", "]", "=", "hyp", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", ",", "-", "1", "]", "=", "bscore_batch", "\n", "\n", "# resort hyps and score", "\n", "next_hyp_score", ",", "next_hyp_idx", "=", "torch", ".", "topk", "(", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", ",", "hyps_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "min_worst_bscore", "=", "torch", ".", "min", "(", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ",", "bscore_batch", ")", "\n", "min_worst_sort", "=", "next_hyp_score", "[", ":", ",", "-", "1", "]", "\n", "\n", "more_than_beam", "=", "(", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", ">=", "beam_size", ")", "\n", "add_batch_more_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "more_than_beam", ")", ".", "long", "(", ")", "\n", "add_batch_less_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "~", "more_than_beam", ")", ".", "long", "(", ")", "\n", "# assert add_batch_more_idx.size(0) + add_batch_less_idx.size(0) == add_batch_idx.size(0)", "\n", "worst_batch_idx", "=", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "\n", "if", "add_batch_more_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_more_idx", "]", "=", "min_worst_sort", "[", "add_batch_more_idx", "]", "\n", "", "if", "add_batch_less_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_less_idx", "]", "=", "min_worst_bscore", "[", "add_batch_less_idx", "]", "\n", "\n", "", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", "=", "worst_batch_idx", "\n", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", "=", "next_hyp_score", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ".", "gather", "(", "\n", "2", ",", "next_hyp_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ")", ")", "\n", "\n", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", "+=", "1", "\n", "\n", "# ----- step_end_of_sent == False", "\n", "", "add_next_sent", "=", "(", "(", "~", "is_step_eos", ")", "&", "not_done", ")", "\n", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_next_sent", ")", ".", "long", "(", ")", "\n", "n_sent_b_t_val", "[", "add_next_sent_idx", ",", "i", "]", "=", "value", "[", "add_next_sent_idx", "]", ".", "type_as", "(", "n_sent_b_t_val", ")", "\n", "n_sent_b_t_widx", "[", "add_next_sent_idx", ",", "i", "]", "=", "word_ids", "[", "add_next_sent_idx", "]", "\n", "n_sent_b_t_pidx", "[", "add_next_sent_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "add_next_sent_idx", "]", "\n", "", "else", ":", "\n", "# all add to next_sent_beam", "\n", "                ", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "not_done", ")", ".", "long", "(", ")", "\n", "n_sent_b_t_val", "[", "add_next_sent_idx", ",", "i", "]", "=", "value", "[", "add_next_sent_idx", "]", ".", "type_as", "(", "n_sent_b_t_val", ")", "\n", "n_sent_b_t_widx", "[", "add_next_sent_idx", ",", "i", "]", "=", "word_ids", "[", "add_next_sent_idx", "]", "\n", "n_sent_b_t_pidx", "[", "add_next_sent_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "add_next_sent_idx", "]", "\n", "\n", "# finish loop beam size", "\n", "", "if", "done_t", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# update next beam content?", "\n", "", "", "n_sent_b_t_out_val", ",", "n_sent_b_t_out_idx", "=", "torch", ".", "topk", "(", "\n", "n_sent_b_t_val", ",", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "n_sent_b_t_out_val", "=", "n_sent_b_t_out_val", ".", "masked_fill_", "(", "n_sent_b_t_out_val", "<", "-", "1e3", ",", "0", ")", "\n", "# assert (n_sent_b_t_out_val > -1e3).all() or cur_len + 1 == max_len or torch.all(", "\n", "#     done_t), 'len={}/{} n_sent_b_t_out_val: \\n{}\\n{}'.format(", "\n", "#     cur_len, max_len, n_sent_b_t_out_val, done_t)", "\n", "\n", "n_sent_b_t_out_widx", "=", "n_sent_b_t_widx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "n_sent_b_t_out_pidx", "=", "n_sent_b_t_pidx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "n_sent_b_t_out_val", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "n_sent_b_t_out_widx", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_idx", "=", "positions", ".", "new", "(", "n_sent_b_t_out_pidx", ".", "view", "(", "-", "1", ")", ")", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "torch", ".", "all", "(", "done_t", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "beam_efficient_final_decoded", "(", "hyps_t", ",", "bs", ",", "src_len", ",", "model", ".", "pad_index", ",", "model", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_efficient_validate_cpu": [[1105, 1587], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated_cpu[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores_cpu.new.view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "torch.zeros().bool", "efficient_beam_search.beam_efficient_final_decoded", "beam_search.compute_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "beam_search.BeamHypotheses", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "src_len.new.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "efficient_beam_search..forward", "efficient_beam_search..pred_layer.get_scores", "torch.log_softmax", "_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "list", "range", "all", "range", "beam_scores_cpu.new.new", "src_len.new.new", "src_len.unsqueeze().expand().contiguous().view.new", "cache_cpu.keys", "worst_better.masked_fill_().bool", "src_enc.unsqueeze().expand().contiguous().view.new().float().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_", "src_len.new.new().fill_().bool", "src_len.new.new().fill_", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "n_sent_b_t_out_val.masked_fill_.masked_fill_", "generated.new().fill_.gather", "generated.new().fill_.gather", "beam_scores.new.new", "src_len.new.new", "torch.arange().unsqueeze().expand_as.new", "cache.keys", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "generated.new().fill_.new", "hyps_t[].new", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.any", "torch.zeros().bool.new", "output.size", "output_cpu.size", "logger.info", "logger.info", "output.view().masked_select", "output_cpu.view().masked_select", "logger.info", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "efficient_beam_search..size", "F.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "generated_hyps[].is_done", "zip", "next_batch_beam.extend", "len", "ValueError", "word_ids_t.new().fill_().bool", "torch.any", "torch.any", "torch.any", "torch.any", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "n_bucket_full.all", "torch.all", "torch.all", "torch.all", "torch.all", "n_sent_b_t_out_val.masked_fill_.view().type_as", "n_sent_b_t_widx.gather.view", "n_sent_b_t_pidx.gather.view", "positions.new.size", "src_len.new.size", "positions.new.size", "src_len.new.size", "logger.info", "torch.allclose", "torch.allclose", "torch.allclose", "torch.allclose", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "range", "logger.info", "ValueError", "torch.all", "torch.all", "torch.all", "torch.all", "all", "output.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.new.new", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_enc.unsqueeze().expand().contiguous().view.new().float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "next_scores[].max().item", "range", "len", "range", "range", "range", "next_batch_beam.extend", "add_or_not_cpu[].append", "len", "len", "[].detach", "worst_better.masked_fill_", "logger.info", "logger.info", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "src_enc.unsqueeze().expand().contiguous().view.new().float", "src_len.new.new", "src_len.new.new", "src_len.new.new().fill_", "src_len.new.new", "n_bucket_not_full.type_as", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long.masked_select", "n_sent_b_t_pidx.gather.size", "src_len.new.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "output.view", "output_cpu.view", "diff.int().sum().float().mean", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "zip", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.tensor().bool", "torch.zeros().bool.new", "word_ids_t.new().fill_", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "value.detach", "torch.arange().masked_select().long.numel", "[].clone", "hyp.squeeze.gather", "hyp.squeeze.squeeze", "bscore[].clone().detach", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "[].gather", "torch.min", "torch.min", "torch.min", "torch.min", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "torch.arange().masked_select().long", "[].clone", "torch.min", "torch.min", "torch.min", "torch.min", "n_sent_b_t_out_val.masked_fill_.view", "torch.zeros().bool.tolist", "done_t.new.tolist", "logger.info", "torch.zeros().bool.tolist", "output.size", "output.size", "src_enc.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new", "next_scores[].max", "generated_cpu[].clone", "value.item", "len", "worst_better.new", "worst_better.tolist", "hyps_t[].tolist", "len_less_beam.new", "len_less_beam.tolist", "hyps_t[].tolist", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "int", "src_enc.unsqueeze().expand().contiguous().view.new", "src_len.new.new", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.all", "torch.all", "torch.all", "torch.all", "beam_ids[].unsqueeze().unsqueeze().expand", "next_hyp_idx.unsqueeze().expand_as", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.numel", "torch.arange().masked_select().long.numel", "hyps_t[].new().expand_as", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "len", "len", "diff.int().sum().float", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max", "next_scores.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "worst_better.new", "len_less_beam.new", "len", "word_ids_t.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange().masked_select().long.size", "bscore[].clone", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size", "generated_hyps[].is_done", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "generated[].view", "beam_ids[].unsqueeze().unsqueeze", "next_hyp_idx.unsqueeze", "hyps_t[].new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "diff.int().sum", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "beam_ids[].unsqueeze", "torch.arange().masked_select().long.size", "torch.arange().masked_select().long.size", "next_scores[].max", "diff.int"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.beam_efficient_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done"], ["", "def", "generate_beam_efficient_validate_cpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "\n", "sample_temperature", "=", "None", ",", "hyps_size_multiple", "=", "1", "\n", ")", ":", "\n", "# check inputs", "\n", "    ", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# TODO: CPU variables", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated_cpu", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated_cpu", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated_cpu", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", "=", "False", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "# positions", "\n", "positions_cpu", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions_cpu", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions_cpu", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated_cpu", ")", "\n", "# language IDs", "\n", "langs_cpu", "=", "positions_cpu", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "# scores for each sentence in the beam", "\n", "beam_scores_cpu", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores_cpu", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores_cpu", ")", "\n", "beam_scores_cpu", "=", "beam_scores_cpu", ".", "view", "(", "-", "1", ")", "\n", "# cache compute states", "\n", "cache_cpu", "=", "{", "'slen'", ":", "0", "}", "\n", "# done sentences", "\n", "done_cpu", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# TODO: GPU variables", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "device", "=", "src_enc", ".", "device", "\n", "done_t", "=", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "bool", "(", ")", "\n", "# hyps_size = hyps_size_multiple * beam_size", "\n", "\n", "hyps_size", "=", "beam_size", "\n", "hyps_t", "=", "{", "\n", "\"len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "device", "=", "device", ")", ".", "long", "(", ")", ",", "\n", "\"hyps\"", ":", "generated", ".", "new", "(", "max_len", ",", "bs", ",", "hyps_size", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", ",", "\n", "\"worst\"", ":", "src_enc", ".", "new", "(", "bs", ")", ".", "float", "(", ")", ".", "fill_", "(", "1e9", ")", ",", "\n", "\"score\"", ":", "src_enc", ".", "new", "(", "bs", ",", "hyps_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", ",", "\n", "\"hyp_len\"", ":", "torch", ".", "zeros", "(", "bs", ",", "hyps_size", ",", "device", "=", "device", ")", ".", "long", "(", ")", "\n", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "# FIXME: understand beam? why tensor len=1 ? it should be cur_len ?", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "# assert next_scores.dtype == src_enc.dtype, '{} != {}'.format(next_scores.dtype, src_enc.dtype)", "\n", "\n", "# FIXME: CPU computations ======================================", "\n", "next_batch_beam", "=", "[", "]", "\n", "gen_hyp_is_done", "=", "[", "generated_hyps", "[", "j", "]", ".", "is_done", "(", "next_scores", "[", "j", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "for", "j", "in", "range", "(", "bs", ")", "]", "\n", "# ---- test is done logic", "\n", "gen_hyp_len_less", "=", "[", "len", "(", "generated_hyps", "[", "j", "]", ")", "<", "generated_hyps", "[", "j", "]", ".", "n_hyp", "for", "j", "in", "range", "(", "bs", ")", "]", "\n", "gen_hyp_worst_better", "=", "[", "\n", "generated_hyps", "[", "j", "]", ".", "worst_score", ">=", "next_scores", "[", "j", "]", ".", "max", "(", ")", ".", "item", "(", ")", "/", "(", "max_len", "-", "1", ")", "**", "length_penalty", "for", "j", "in", "\n", "range", "(", "bs", ")", "]", "\n", "gen_hyp_is_done_manual", "=", "list", "(", "gen_hyp_worst_better", ")", "\n", "for", "j", "in", "range", "(", "bs", ")", ":", "\n", "            ", "if", "gen_hyp_len_less", "[", "j", "]", ":", "\n", "                ", "gen_hyp_is_done_manual", "[", "j", "]", "=", "False", "\n", "\n", "", "", "assert", "all", "(", "x", "==", "y", "for", "x", ",", "y", "in", "zip", "(", "gen_hyp_is_done", ",", "gen_hyp_is_done_manual", ")", ")", ",", "'logic wrong: \\n{}\\n{}'", ".", "format", "(", "\n", "gen_hyp_is_done", ",", "gen_hyp_is_done_manual", "\n", ")", "\n", "\n", "# ---------------------------", "\n", "add_or_not_cpu", "=", "[", "[", "]", "for", "i", "in", "range", "(", "bs", ")", "]", "\n", "loop_next_words_cpu", "=", "[", "0", "]", "*", "bs", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "# if we are done with this sentence", "\n", "            ", "done_cpu", "[", "sent_id", "]", "=", "done_cpu", "[", "sent_id", "]", "or", "gen_hyp_is_done", "[", "sent_id", "]", "\n", "if", "done_cpu", "[", "sent_id", "]", ":", "\n", "                ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "# add_or_not_cpu.append([])", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "# get beam and word IDs", "\n", "                ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                    ", "add_or_not_", "=", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "\n", "generated_cpu", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "add_or_not_", "=", "False", "\n", "\n", "", "add_or_not_cpu", "[", "sent_id", "]", ".", "append", "(", "add_or_not_", ")", "\n", "loop_next_words_cpu", "[", "sent_id", "]", "=", "loop_next_words_cpu", "[", "sent_id", "]", "+", "1", "\n", "# the beam for next step is full", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                    ", "break", "\n", "", "", "add_or_not_cpu", "[", "sent_id", "]", "=", "add_or_not_cpu", "[", "sent_id", "]", "+", "[", "False", "]", "*", "(", "2", "*", "beam_size", "-", "len", "(", "add_or_not_cpu", "[", "sent_id", "]", ")", ")", "\n", "\n", "# update next beam content", "\n", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores_cpu", "=", "beam_scores_cpu", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words_cpu", "=", "generated_cpu", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx_cpu", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "# re-order batch and internal states", "\n", "generated_cpu", "=", "generated_cpu", "[", ":", ",", "beam_idx_cpu", "]", "\n", "generated_cpu", "[", "cur_len", "]", "=", "beam_words_cpu", "\n", "for", "k", "in", "cache_cpu", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache_cpu", "[", "k", "]", "=", "(", "cache_cpu", "[", "k", "]", "[", "0", "]", "[", "beam_idx_cpu", "]", ",", "cache_cpu", "[", "k", "]", "[", "1", "]", "[", "beam_idx_cpu", "]", ")", "\n", "\n", "# FIXME: GPU computations ======================================", "\n", "", "", "worst_better", "=", "(", "hyps_t", "[", "'worst'", "]", ">=", "(", "next_scores", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", ".", "detach", "(", ")", "/", "(", "(", "max_len", "-", "1", ")", "**", "length_penalty", ")", ")", ")", "\n", "len_less_beam", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "new_is_done", "=", "worst_better", ".", "masked_fill_", "(", "len_less_beam", ",", "torch", ".", "tensor", "(", "0", ")", ".", "bool", "(", ")", ")", ".", "bool", "(", ")", "\n", "old_done_t", "=", "done_t", "\n", "done_t", "=", "done_t", "|", "new_is_done", "\n", "# assert (done_t.new(done_cpu) == done_t).all(), 'done not right: gpu\\n{}\\ndone_cpu\\n'.format(", "\n", "#     done_t.tolist(), done_cpu", "\n", "# )", "\n", "if", "(", "done_t", ".", "new", "(", "done_cpu", ")", "!=", "done_t", ")", ".", "any", "(", ")", ":", "\n", "# invest each element", "\n", "            ", "worst_better_cpu", "=", "gen_hyp_worst_better", "\n", "len_less_beam_cpu", "=", "gen_hyp_len_less", "\n", "if", "(", "worst_better", "!=", "worst_better", ".", "new", "(", "worst_better_cpu", ")", ")", ".", "any", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'Worst_better_wrong: gpu{}\\nworst_bteer_cpu\\n{}\\n{}\\nworst_gpu\\n{}\\nworst_cpu{}'", ".", "format", "(", "\n", "worst_better", ".", "tolist", "(", ")", ",", "worst_better_cpu", ",", "worst_better", "==", "worst_better", ".", "new", "(", "worst_better_cpu", ")", ",", "\n", "hyps_t", "[", "'worst'", "]", ".", "tolist", "(", ")", ",", "[", "gen", ".", "worst_score", "for", "gen", "in", "generated_hyps", "]", "\n", ")", ")", "\n", "", "if", "(", "len_less_beam", "!=", "len_less_beam", ".", "new", "(", "len_less_beam_cpu", ")", ")", ".", "any", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'len_less_beam_wrong: gpu{}\\nlen_less_beam_cpu\\n{}\\n{}\\nlen_gpu\\n{}\\nlen_cpu{}'", ".", "format", "(", "\n", "len_less_beam", ".", "tolist", "(", ")", ",", "len_less_beam_cpu", ",", "len_less_beam", "!=", "len_less_beam", ".", "new", "(", "len_less_beam_cpu", ")", ",", "\n", "hyps_t", "[", "'len'", "]", ".", "tolist", "(", ")", ",", "[", "len", "(", "gen", ")", "for", "gen", "in", "generated_hyps", "]", "\n", ")", ")", "\n", "# assert (len_less_beam == len_less_beam.new(len_less_beam_cpu)).all(), 'len_lessbeam:\\n{}\\n{}'.format(", "\n", "#     len_less_beam.tolist(), len_less_beam_cpu", "\n", "# )", "\n", "", "raise", "ValueError", "(", "'done wrong'", ")", "\n", "\n", "", "not_done", "=", "~", "done_t", "\n", "# todo: next_words for sentences", "\n", "beam_ids_t", "=", "next_words", "//", "n_words", "\n", "word_ids_t", "=", "next_words", "%", "n_words", "\n", "sent_beam_idx_t", "=", "beam_ids_t", "+", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "*", "beam_size", "\n", "\n", "is_eos", "=", "(", "word_ids_t", "==", "self", ".", "eos_index", ")", "|", "word_ids_t", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "\n", "int", "(", "cur_len", "+", "1", "==", "max_len", ")", ")", ".", "bool", "(", ")", "\n", "# next_sent_beam = [[] for i in range(bs)]", "\n", "n_sent_b_t_val", "=", "src_enc", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "float", "(", ")", ".", "fill_", "(", "-", "1e9", ")", "\n", "n_sent_b_t_widx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", "\n", "n_sent_b_t_pidx", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "add_or_not_gpu", "=", "generated", ".", "new", "(", "bs", ",", "2", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", ".", "bool", "(", ")", "\n", "loop_next_words_gpu", "=", "generated", ".", "new", "(", "bs", ")", ".", "fill_", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "2", "*", "beam_size", ")", ":", "\n", "# idx = next_words[:, i]", "\n", "            ", "value", "=", "next_scores", "[", ":", ",", "i", "]", "\n", "is_step_eos", "=", "is_eos", "[", ":", ",", "i", "]", "\n", "beam_ids", "=", "beam_ids_t", "[", ":", ",", "i", "]", "\n", "word_ids", "=", "word_ids_t", "[", ":", ",", "i", "]", "\n", "sent_beam_idx", "=", "sent_beam_idx_t", "[", ":", ",", "i", "]", "\n", "any_step_eos", "=", "torch", ".", "any", "(", "is_step_eos", ")", "\n", "\n", "add_supposed_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "not_done", ")", ".", "long", "(", ")", "\n", "batch_bucket_not_full", "=", "(", "n_sent_b_t_widx", "!=", "self", ".", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", "<", "beam_size", "\n", "allow_to_add", "=", "not_done", "&", "batch_bucket_not_full", "\n", "\n", "n_bucket_full", "=", "(", "n_sent_b_t_widx", "[", "add_supposed_sent_idx", "]", "!=", "self", ".", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "-", "1", ")", ">=", "beam_size", "\n", "n_bucket_not_full", "=", "~", "n_bucket_full", "\n", "loop_next_words_gpu", "[", "add_supposed_sent_idx", "]", "=", "loop_next_words_gpu", "[", "\n", "add_supposed_sent_idx", "]", "+", "n_bucket_not_full", ".", "type_as", "(", "\n", "loop_next_words_gpu", ")", "\n", "\n", "if", "any_step_eos", ":", "\n", "                ", "bscore", "=", "value", ".", "detach", "(", ")", "/", "(", "cur_len", "**", "length_penalty", ")", "# FIXME: check if cur_len=len(hyp) in Hypo.add()", "\n", "len_less_beamsize", "=", "hyps_t", "[", "'len'", "]", "<", "beam_size", "\n", "bscore_more_worst", "=", "bscore", ">", "hyps_t", "[", "'worst'", "]", "\n", "\n", "lenlessbeam_bscoremoreworst", "=", "len_less_beamsize", "|", "bscore_more_worst", "\n", "# add_or_not = lenlessbeam_bscoremoreworst & is_step_eos & not_done", "\n", "add_or_not", "=", "lenlessbeam_bscoremoreworst", "&", "is_step_eos", "&", "allow_to_add", "\n", "add_or_not_gpu", "[", ":", ",", "i", "]", "=", "add_or_not", "\n", "# add_or_not_back = ((hyps_t['len'] < beam_size) | (bscore > hyps_t['worst']) & is_step_eos) & not_done", "\n", "# assert torch.all(add_or_not == add_or_not_back), \"add_or_not: \\n{} \\n{}\".format(add_or_not, add_or_not_back)", "\n", "add_batch_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_or_not", ")", ".", "long", "(", ")", "\n", "if", "add_batch_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "\n", "                    ", "if", "cur_len", "+", "1", "<", "max_len", ":", "\n", "                        ", "assert", "torch", ".", "all", "(", "word_ids", "[", "\n", "add_batch_idx", "]", "==", "self", ".", "eos_index", ")", ",", "'curlen={}, words_ids:\\n{}\\nlen_less\\n{}\\nbscore_more_worst\\n{}\\nadd_ornot\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "word_ids", ",", "len_less_beamsize", ",", "bscore_more_worst", ",", "add_or_not", ")", "\n", "\n", "", "hyp", "=", "generated", "[", ":", "cur_len", "]", ".", "view", "(", "cur_len", ",", "bs", ",", "beam_size", ")", "[", ":", ",", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "hyp", "=", "hyp", ".", "gather", "(", "2", ",", "beam_ids", "[", "add_batch_idx", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "cur_len", ",", "add_batch_idx", ".", "size", "(", "0", ")", ",", "1", ")", ")", "\n", "hyp", "=", "hyp", ".", "squeeze", "(", "-", "1", ")", "\n", "# bvalue_batch = value[add_batch_idx].clone().type_as(hyps_t['score']).detach()", "\n", "bscore_batch", "=", "bscore", "[", "add_batch_idx", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", ",", "-", "1", "]", "=", "hyp", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", ",", "-", "1", "]", "=", "bscore_batch", "\n", "\n", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", "+=", "1", "\n", "\n", "# resort hyps and score", "\n", "next_hyp_score", ",", "next_hyp_idx", "=", "torch", ".", "topk", "(", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", ",", "hyps_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "# logger.info('Add with eos: (curlen={},i={}) batch:\\n{}\\nworst\\n{}\\n{}\\n{}\\nhyp_len\\n{}'.format(", "\n", "#     cur_len, i, add_batch_idx, min_worst_bscore, min_worst_sort, hyps_t['worst'][add_batch_idx],", "\n", "#     hyps_t['len'][add_batch_idx]", "\n", "# ))", "\n", "# what happen here:", "\n", "#   len=0: worst=1e4  => len=1, worst=-0.5      score=[-0.5, -1e4, -1e4]", "\n", "#   len=1: worst=-0.5 => len=2, worst=-0.6      score=[-0.5, -0.6, -1e4]", "\n", "#   len=2: worst=-0.6 => len=3, worst=-0.7      score=[-", "\n", "#   len=3: worst=-0.7 => len=1, worst=", "\n", "\n", "hyps_t", "[", "'score'", "]", "[", "add_batch_idx", "]", "=", "next_hyp_score", "\n", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", "=", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ".", "gather", "(", "\n", "# 2, next_hyp_idx.unsqueeze(0).expand_as(cur_len, add_batch_idx.size(0), beam_size))", "\n", "2", ",", "next_hyp_idx", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "hyps_t", "[", "'hyps'", "]", "[", ":", "cur_len", ",", "add_batch_idx", "]", ")", ")", "\n", "\n", "# assigning worst", "\n", "min_worst_bscore", "=", "torch", ".", "min", "(", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ",", "bscore_batch", ")", "\n", "min_worst_sort", "=", "next_hyp_score", "[", ":", ",", "-", "1", "]", "\n", "\n", "more_than_beam", "=", "(", "hyps_t", "[", "'len'", "]", "[", "add_batch_idx", "]", ">", "beam_size", ")", "\n", "add_batch_more_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "more_than_beam", ")", ".", "long", "(", ")", "\n", "add_batch_less_idx", "=", "torch", ".", "arange", "(", "add_batch_idx", ".", "size", "(", "0", ")", ",", "device", "=", "device", ")", ".", "masked_select", "(", "\n", "~", "more_than_beam", ")", ".", "long", "(", ")", "\n", "assert", "add_batch_more_idx", ".", "size", "(", "0", ")", "+", "add_batch_less_idx", ".", "size", "(", "0", ")", "==", "add_batch_idx", ".", "size", "(", "0", ")", "\n", "worst_batch_idx", "=", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", ".", "clone", "(", ")", "\n", "\n", "if", "add_batch_more_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_more_idx", "]", "=", "min_worst_sort", "[", "add_batch_more_idx", "]", "\n", "", "if", "add_batch_less_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                        ", "worst_batch_idx", "[", "add_batch_less_idx", "]", "=", "min_worst_bscore", "[", "add_batch_less_idx", "]", "\n", "\n", "", "hyps_t", "[", "'worst'", "]", "[", "add_batch_idx", "]", "=", "worst_batch_idx", "\n", "hyps_t", "[", "'len'", "]", "=", "torch", ".", "min", "(", "hyps_t", "[", "'len'", "]", ",", "hyps_t", "[", "'len'", "]", ".", "new", "(", "[", "beam_size", "]", ")", ".", "expand_as", "(", "hyps_t", "[", "'len'", "]", ")", ")", "\n", "\n", "assert", "(", "hyps_t", "[", "'worst'", "]", ">", "-", "5e8", ")", ".", "all", "(", ")", ",", "'worst -inf: {}'", ".", "format", "(", "hyps_t", "[", "'worst'", "]", ")", "\n", "\n", "# ----- step_end_of_sent == False", "\n", "# FIXME: make sure that n_sent_b_t_val/n_sent_b_t_widx .. only contains beam ones", "\n", "", "add_next_sent", "=", "(", "(", "~", "is_step_eos", ")", "&", "allow_to_add", ")", "\n", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "add_next_sent", ")", ".", "long", "(", ")", "\n", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "self", ".", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "\n", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "# n_sent_b_t_val[add_next_sent_idx, i] = value[add_next_sent_idx].type_as(n_sent_b_t_val)", "\n", "# n_sent_b_t_widx[add_next_sent_idx, i] = word_ids[add_next_sent_idx]", "\n", "# n_sent_b_t_pidx[add_next_sent_idx, i] = sent_beam_idx[add_next_sent_idx]", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "\n", "", "else", ":", "\n", "# all add to next_sent_beam", "\n", "# add_next_sent_idx = torch.arange(bs, device=device).masked_select(not_done).long()", "\n", "# n_sent_b_t_val[add_next_sent_idx, i] = value[add_next_sent_idx].type_as(n_sent_b_t_val)", "\n", "# n_sent_b_t_widx[add_next_sent_idx, i] = word_ids[add_next_sent_idx]", "\n", "# n_sent_b_t_pidx[add_next_sent_idx, i] = sent_beam_idx[add_next_sent_idx]", "\n", "\n", "                ", "add_next_sent_idx", "=", "torch", ".", "arange", "(", "bs", ",", "device", "=", "device", ")", ".", "masked_select", "(", "allow_to_add", ")", ".", "long", "(", ")", "\n", "n_sent_bucket_not_full", "=", "(", "n_sent_b_t_widx", "[", "add_next_sent_idx", "]", "!=", "self", ".", "pad_index", ")", ".", "int", "(", ")", ".", "sum", "(", "\n", "-", "1", ")", "<", "beam_size", "\n", "n_sent_add_idx", "=", "add_next_sent_idx", ".", "masked_select", "(", "n_sent_bucket_not_full", ")", "\n", "\n", "n_sent_b_t_val", "[", "n_sent_add_idx", ",", "i", "]", "=", "value", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_widx", "[", "n_sent_add_idx", ",", "i", "]", "=", "word_ids", "[", "n_sent_add_idx", "]", "\n", "n_sent_b_t_pidx", "[", "n_sent_add_idx", ",", "i", "]", "=", "sent_beam_idx", "[", "n_sent_add_idx", "]", "\n", "\n", "", "if", "n_bucket_full", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# in what case  n_sent_b_t_val not filled up?", "\n", "#       all(done_t) or add_or_not    -> what cpu looks like?", "\n", "#       cpu: not done!, none are add to hyps_cpu? Or is it?", "\n", "# update next beam content?", "\n", "# assert ((n_sent_b_t_widx != self.pad_index).int().sum(-1) == beam_size).all(), 'not pad_index: {}'.format(n_sent_b_t_widx)", "\n", "\n", "", "", "n_sent_b_t_out_val", ",", "n_sent_b_t_out_idx", "=", "torch", ".", "topk", "(", "\n", "n_sent_b_t_val", ",", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "n_sent_b_t_out_val", "=", "n_sent_b_t_out_val", ".", "masked_fill_", "(", "n_sent_b_t_out_val", "<", "-", "1e8", ",", "0", ")", "\n", "assert", "(", "n_sent_b_t_out_val", ">", "-", "1e8", ")", ".", "all", "(", ")", "or", "cur_len", "+", "1", "==", "max_len", "or", "torch", ".", "all", "(", "\n", "done_t", ")", ",", "'len={}/{} n_sent_b_t_out_val: \\n{}\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "n_sent_b_t_out_val", ",", "done_t", ")", "\n", "\n", "n_sent_b_t_out_widx", "=", "n_sent_b_t_widx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "n_sent_b_t_out_pidx", "=", "n_sent_b_t_pidx", ".", "gather", "(", "1", ",", "n_sent_b_t_out_idx", ")", "\n", "assert", "n_sent_b_t_out_pidx", ".", "size", "(", ")", "==", "(", "bs", ",", "beam_size", ")", "and", "beam_idx_cpu", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", ")", "\n", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "n_sent_b_t_out_val", ".", "view", "(", "-", "1", ")", ".", "type_as", "(", "beam_scores", ")", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "n_sent_b_t_out_widx", ".", "view", "(", "-", "1", ")", ")", "\n", "beam_idx", "=", "positions", ".", "new", "(", "n_sent_b_t_out_pidx", ".", "view", "(", "-", "1", ")", ")", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# FIXME: sanatify check", "\n", "#   generated vs generated_cpu", "\n", "#   beam_scores vs beam_scores_cpu", "\n", "#   beam_idx vs beam_idx_cpu", "\n", "", "", "assert", "beam_idx", ".", "size", "(", ")", "==", "beam_idx_cpu", ".", "size", "(", ")", ",", "'size: {} != {}'", ".", "format", "(", "beam_idx", ".", "size", "(", ")", ",", "beam_idx_cpu", ".", "size", "(", ")", ")", "\n", "test_fit", "=", "True", "\n", "hyp_lens_gpu", "=", "hyps_t", "[", "'len'", "]", "\n", "hyp_lens_cpu", "=", "torch", ".", "tensor", "(", "[", "len", "(", "g", ")", "for", "g", "in", "generated_hyps", "]", ",", "device", "=", "device", ")", ".", "type_as", "(", "hyp_lens_gpu", ")", "\n", "# add_or_not_cpu = add_or_not_gpu.new(add_or_not_cpu)", "\n", "\n", "loop_next_words_cpu", "=", "loop_next_words_gpu", ".", "new", "(", "loop_next_words_cpu", ")", "\n", "worst_score_cpu", "=", "hyps_t", "[", "'worst'", "]", ".", "new", "(", "[", "gen", ".", "worst_score", "for", "gen", "in", "generated_hyps", "]", ")", "\n", "\n", "if", "torch", ".", "any", "(", "loop_next_words_gpu", "!=", "loop_next_words_cpu", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'loop_next_words_cpu: curlen={}/{}, loop_next_words_gpu\\n{}\\nloop_next_words_cpu\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "loop_next_words_gpu", ",", "loop_next_words_cpu", "\n", ")", ")", "\n", "test_fit", "=", "False", "\n", "\n", "# if torch.any(add_or_not_cpu != add_or_not_gpu):", "\n", "#     logger.info('add_or_not_cpu: curlen={}/{}, add_or_not_gpu\\n{}\\nadd_or_not_cpu\\n{}'.format(", "\n", "#         cur_len, max_len, add_or_not_gpu, add_or_not_cpu", "\n", "#     ))", "\n", "#     test_fit = False", "\n", "# if torch.any(worst_score_cpu != hyps_t['worst']):", "\n", "", "if", "not", "torch", ".", "allclose", "(", "worst_score_cpu", ",", "hyps_t", "[", "'worst'", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'worst: curlen={}/{}, worst_gpu\\n{}\\nworst_cpu\\n{}\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "hyps_t", "[", "'worst'", "]", ",", "worst_score_cpu", ",", "hyps_t", "[", "'worst'", "]", "==", "worst_score_cpu", ",", "\n", ")", ")", "\n", "test_fit", "=", "False", "\n", "\n", "", "if", "torch", ".", "any", "(", "hyp_lens_gpu", "!=", "hyp_lens_cpu", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'hyp_lens_cpu: curlen={}/{}, hyp_lens_gpu\\n{}hyp_lens_cpu\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "hyp_lens_gpu", ",", "hyp_lens_cpu", "\n", ")", ")", "\n", "test_fit", "=", "False", "\n", "\n", "", "if", "torch", ".", "any", "(", "beam_scores", "!=", "beam_scores_cpu", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'beam_scores: curlen={}/{}, beam_scores\\n{}\\nbeam_scores_cpu\\n{}\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "beam_scores", ",", "beam_scores_cpu", ",", "beam_scores", "==", "beam_scores_cpu", "\n", ")", ")", "\n", "test_fit", "=", "False", "\n", "\n", "# if torch.any(beam_idx != beam_idx_cpu):", "\n", "#     # not triggering this because the 2 beam idx may have the same score they are order differenly", "\n", "#     logger.info('beam_idx: curlen={}, beam_idx\\n{}beam_idx_cpu\\n{}\\n{}'.format(", "\n", "#         cur_len, beam_idx, beam_idx_cpu, beam_idx == beam_idx_cpu", "\n", "#     ))", "\n", "#     test_fit = False", "\n", "# if torch.any(beam_words != beam_words_cpu):", "\n", "#     # not triggering this because the 2 words may have the same score they are order differenly", "\n", "#     # instead of Raise exception, report number of wrongs", "\n", "#     logger.info('beam_words: curlen={}, beam_words\\n{}beam_words_cpus\\n{}\\n{}'.format(", "\n", "#         cur_len, beam_words, beam_words_cpu, beam_words == beam_words_cpu", "\n", "#     ))", "\n", "#     test_fit = False", "\n", "#     num_wrongs = (beam_words != beam_words_cpu).int().sum()", "\n", "", "done_cpu_t", "=", "done_t", ".", "new", "(", "done_cpu", ")", "\n", "if", "(", "done_t", "!=", "done_cpu_t", ")", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'done_t: curlen={}/{}, done_t\\n{}\\ndone_cpu_t\\n{}\\n{}'", ".", "format", "(", "\n", "cur_len", ",", "max_len", ",", "done_t", ".", "tolist", "(", ")", ",", "done_cpu_t", ".", "tolist", "(", ")", ",", "(", "done_t", "==", "done_cpu_t", ")", ".", "tolist", "(", ")", "\n", ")", ")", "\n", "test_fit", "=", "False", "\n", "\n", "", "if", "not", "test_fit", ":", "\n", "            ", "logger", ".", "info", "(", "'---- Final Log of Failed Test! -------------'", ")", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "                ", "if", "done_t", "[", "sent_id", "]", "!=", "done_cpu", "[", "sent_id", "]", ":", "\n", "                    ", "logger", ".", "info", "(", "'test_done[curlen={}][b={}]: {}/{} -> is_done: {}'", ".", "format", "(", "\n", "cur_len", ",", "sent_id", ",", "done_t", "[", "sent_id", "]", ",", "done_cpu", "[", "sent_id", "]", ",", "\n", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", ")", ")", "\n", "", "", "logger", ".", "info", "(", "'Done test: old_done:\\n{}\\nworst_better:\\n{}\\nhyps_len:\\n{}\\nis_done:\\n{}\\nworst\\n{}'", ".", "format", "(", "\n", "old_done_t", ",", "worst_better", ",", "hyps_t", "[", "'len'", "]", ",", "new_is_done", ",", "hyps_t", "[", "'worst'", "]", "\n", ")", ")", "\n", "raise", "ValueError", "(", "'some problem occur: done_t:\\n{}\\ndone\\n{}\\nhyps_cpu\\n{}'", ".", "format", "(", "\n", "done_t", ".", "tolist", "(", ")", ",", "done_cpu", ",", "[", "len", "(", "x", ")", "for", "x", "in", "generated_hyps", "]", "\n", ")", ")", "\n", "\n", "", "cur_len", "+=", "1", "\n", "if", "torch", ".", "all", "(", "done_t", ")", "&", "all", "(", "done_cpu", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "output", ",", "hyp_len", "=", "beam_efficient_final_decoded", "(", "\n", "hyps_t", ",", "bs", ",", "src_len", ",", "self", ".", "pad_index", ",", "self", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "output_cpu", ",", "hyp_len_cpu", "=", "compute_final_decoded", "(", "\n", "generated_hyps", ",", "bs", ",", "src_len", ",", "self", ".", "pad_index", ",", "self", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "final_fit", "=", "True", "\n", "# if (hyp_len != hyp_len_cpu).any():", "\n", "#     logger.info('Final hyp_len wring: hyp_len:\\n{}\\nhyp_len_cpu\\n{}\\n{}'.format(", "\n", "#         hyp_len, hyp_len_cpu, hyp_len == hyp_len_cpu", "\n", "#     ))", "\n", "#     final_fit =False", "\n", "if", "output", ".", "size", "(", "0", ")", "!=", "output_cpu", ".", "size", "(", "0", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Final output not the same lengths'", ")", "\n", "return", "output", ",", "hyp_len", "\n", "\n", "", "if", "(", "output", "!=", "output_cpu", ")", ".", "any", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Final output_cpu wrong: output:\\n{}\\noutput_cpu\\n{}\\n{}'", ".", "format", "(", "\n", "output", ",", "output_cpu", ",", "output", "==", "output_cpu", "\n", ")", ")", "\n", "diff", "=", "(", "output", "!=", "output_cpu", ")", ".", "view", "(", "output", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "output_diff", "=", "output", ".", "view", "(", "output", ".", "size", "(", "0", ")", ",", "-", "1", ")", ".", "masked_select", "(", "diff", ")", "\n", "output_diff_cpu", "=", "output_cpu", ".", "view", "(", "output", ".", "size", "(", "0", ")", ",", "-", "1", ")", ".", "masked_select", "(", "diff", ")", "\n", "# ? multiple of 0.03125 = 1/32", "\n", "logger", ".", "info", "(", "'Final output_cpu wrong %: {} diff_gpu:\\n{}\\ndiff_cpu\\n{}'", ".", "format", "(", "\n", "diff", ".", "int", "(", ")", ".", "sum", "(", "0", ")", ".", "float", "(", ")", ".", "mean", "(", ")", ",", "output_diff", ",", "output_diff_cpu", "\n", ")", ")", "\n", "final_fit", "=", "False", "\n", "\n", "", "output", "=", "output", "[", ":", ",", ":", ",", "0", "]", "\n", "hyp_len", "=", "output", "[", ":", ",", "0", "]", "\n", "#", "\n", "# if not final_fit:", "\n", "#     raise ValueError('final output wrong!')", "\n", "\n", "return", "output", ",", "hyp_len", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.BeamHypotheses.__init__": [[6, 16], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_hyp", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", ":", "\n", "        ", "\"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"", "\n", "self", ".", "max_len", "=", "max_len", "-", "1", "# ignoring <BOS>", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "n_hyp", "=", "n_hyp", "\n", "self", ".", "hyp", "=", "[", "]", "\n", "self", ".", "worst_score", "=", "1e9", "\n", "# self.worst_score = 1e4", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.BeamHypotheses.__len__": [[18, 23], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of hypotheses in the list.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "hyp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.BeamHypotheses.add": [[24, 40], ["beam_search.BeamHypotheses.hyp.append", "len", "len", "len", "sorted", "min", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "hyp", ",", "sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"", "\n", "score", "=", "sum_logprobs", "/", "len", "(", "hyp", ")", "**", "self", ".", "length_penalty", "\n", "add_or_not", "=", "False", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", "or", "score", ">", "self", ".", "worst_score", ":", "\n", "            ", "self", ".", "hyp", ".", "append", "(", "(", "score", ",", "hyp", ")", ")", "\n", "add_or_not", "=", "True", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "n_hyp", ":", "\n", "                ", "sorted_scores", "=", "sorted", "(", "[", "(", "s", ",", "idx", ")", "for", "idx", ",", "(", "s", ",", "_", ")", "in", "enumerate", "(", "self", ".", "hyp", ")", "]", ")", "\n", "del", "self", ".", "hyp", "[", "sorted_scores", "[", "0", "]", "[", "1", "]", "]", "\n", "self", ".", "worst_score", "=", "sorted_scores", "[", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "worst_score", "=", "min", "(", "score", ",", "self", ".", "worst_score", ")", "\n", "", "", "return", "add_or_not", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.BeamHypotheses.is_done": [[41, 52], ["len"], "methods", ["None"], ["", "def", "is_done", "(", "self", ",", "best_sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", ":", "\n", "            ", "return", "False", "\n", "", "elif", "self", ".", "early_stopping", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "worst_score", ">=", "best_sum_logprobs", "/", "self", ".", "max_len", "**", "self", ".", "length_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded": [[54, 98], ["src_len.new", "enumerate", "src_len.new().fill_", "enumerate", "src_len.new", "enumerate", "src_len.new().fill_", "enumerate", "best.append", "isinstance", "enumerate", "bests.append", "enumerate", "max", "len", "src_len.new", "src_len.new", "src_len.new.max().item", "len", "src_len.new.max().item", "sorted", "src_len.new.max", "src_len.new.max"], "function", ["None"], ["", "", "", "def", "compute_final_decoded", "(", "generated_hyps", ",", "bs", ",", "src_len", ",", "pad_index", ",", "eos_index", ",", "beam_size", ",", "nbest", "=", "None", ")", ":", "\n", "    ", "if", "nbest", "is", "None", "or", "nbest", "<=", "0", ":", "\n", "# select the best hypotheses", "\n", "        ", "tgt_len", "=", "src_len", ".", "new", "(", "bs", ")", "\n", "best", "=", "[", "]", "\n", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyp", "=", "max", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", "1", "]", "\n", "tgt_len", "[", "i", "]", "=", "len", "(", "best_hyp", ")", "+", "1", "# +1 for the <EOS> symbol", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "src_len", ".", "new", "(", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ",", "bs", ")", ".", "fill_", "(", "pad_index", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "            ", "decoded", "[", ":", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "hypo", "\n", "decoded", "[", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "eos_index", "\n", "\n", "# sanity check, why 2 * bs", "\n", "", "assert", "(", "decoded", "==", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "decoded", ",", "tgt_len", "\n", "", "else", ":", "\n", "        ", "assert", "isinstance", "(", "nbest", ",", "int", ")", "and", "1", "<=", "nbest", "<=", "beam_size", "\n", "bests", "=", "[", "]", "\n", "tgt_len", "=", "src_len", ".", "new", "(", "bs", ",", "nbest", ")", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyps", "=", "[", "z", "[", "1", "]", "for", "z", "in", "sorted", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "nbest", "]", "]", "\n", "for", "j", ",", "x", "in", "enumerate", "(", "best_hyps", ")", ":", "\n", "                ", "tgt_len", "[", "i", ",", "j", "]", "=", "len", "(", "x", ")", "+", "1", "\n", "# tgt_len[i, :] = max([len(x) for x in best_hyps]) + 1  # +1 for the <EOS> symbol", "\n", "", "bests", ".", "append", "(", "best_hyps", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "src_len", ".", "new", "(", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ",", "nbest", ",", "bs", ")", ".", "fill_", "(", "pad_index", ")", "\n", "# [max_len, nbest, bs]", "\n", "for", "i", ",", "hypos", "in", "enumerate", "(", "bests", ")", ":", "\n", "            ", "for", "j", ",", "hyp", "in", "enumerate", "(", "hypos", ")", ":", "\n", "                ", "decoded", "[", ":", "tgt_len", "[", "i", ",", "j", "]", "-", "1", ",", "j", ",", "i", "]", "=", "hyp", "\n", "decoded", "[", "tgt_len", "[", "i", ",", "j", "]", "-", "1", ",", "j", ",", "i", "]", "=", "eos_index", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "decoded", "==", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "*", "nbest", "\n", "\n", "return", "decoded", ",", "tgt_len", "", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_fasttext_model": [[17, 27], ["fastText.load_model", "Exception"], "function", ["None"], ["def", "load_fasttext_model", "(", "path", ")", ":", "\n", "    ", "\"\"\"\n    Load a binarized fastText model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "fastText", "\n", "", "except", "ImportError", ":", "\n", "        ", "raise", "Exception", "(", "\"Unable to import fastText. Please install fastText for Python: \"", "\n", "\"https://github.com/facebookresearch/fastText\"", ")", "\n", "", "return", "fastText", ".", "load_model", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.read_txt_embeddings": [[29, 67], ["logger.info", "numpy.concatenate", "torch.from_numpy().float", "io.open", "enumerate", "len", "len", "torch.from_numpy().float.size", "line.rstrip().split", "numpy.fromstring", "len", "vectors.append", "torch.from_numpy", "len", "line.split", "logger.warning", "logger.warning", "len", "len", "int", "int", "line.rstrip"], "function", ["None"], ["", "def", "read_txt_embeddings", "(", "path", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings from a text file.\n    \"\"\"", "\n", "word2id", "=", "{", "}", "\n", "vectors", "=", "[", "]", "\n", "\n", "# load pretrained embeddings", "\n", "_emb_dim_file", "=", "params", ".", "emb_dim", "\n", "with", "io", ".", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "'\\n'", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "split", "=", "line", ".", "split", "(", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "\n", "assert", "_emb_dim_file", "==", "int", "(", "split", "[", "1", "]", ")", ",", "'{} // {}'", ".", "format", "(", "_emb_dim_file", ",", "int", "(", "split", "[", "1", "]", ")", ")", "\n", "continue", "\n", "", "word", ",", "vect", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ",", "1", ")", "\n", "vect", "=", "np", ".", "fromstring", "(", "vect", ",", "sep", "=", "' '", ")", "\n", "if", "word", "in", "word2id", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Word \\\"%s\\\" found twice!\"", "%", "word", ")", "\n", "continue", "\n", "", "if", "not", "vect", ".", "shape", "==", "(", "_emb_dim_file", ",", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Invalid dimension (%i) for word \\\"%s\\\" in line %i.\"", "\n", "%", "(", "vect", ".", "shape", "[", "0", "]", ",", "word", ",", "i", ")", ")", "\n", "continue", "\n", "", "assert", "vect", ".", "shape", "==", "(", "_emb_dim_file", ",", ")", "\n", "word2id", "[", "word", "]", "=", "len", "(", "word2id", ")", "\n", "vectors", ".", "append", "(", "vect", "[", "None", "]", ")", "\n", "\n", "", "", "assert", "len", "(", "word2id", ")", "==", "len", "(", "vectors", ")", "\n", "logger", ".", "info", "(", "\"Loaded %i pretrained word embeddings from %s\"", "%", "(", "len", "(", "vectors", ")", ",", "path", ")", ")", "\n", "\n", "# compute new vocabulary / embeddings", "\n", "embeddings", "=", "np", ".", "concatenate", "(", "vectors", ",", "0", ")", "\n", "embeddings", "=", "torch", ".", "from_numpy", "(", "embeddings", ")", ".", "float", "(", ")", "\n", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "word2id", ")", ",", "params", ".", "emb_dim", ")", "\n", "return", "word2id", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_bin_embeddings": [[69, 86], ["pretrain.load_fasttext_model", "load_fasttext_model.get_labels", "logger.info", "numpy.concatenate", "torch.from_numpy().float", "logger.info", "load_fasttext_model.get_dimension", "torch.from_numpy().float.size", "torch.from_numpy", "enumerate", "len", "len", "load_fasttext_model.get_word_vector"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_fasttext_model"], ["", "def", "load_bin_embeddings", "(", "path", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings from a fastText binary file.\n    \"\"\"", "\n", "model", "=", "load_fasttext_model", "(", "path", ")", "\n", "assert", "model", ".", "get_dimension", "(", ")", "==", "params", ".", "emb_dim", "\n", "words", "=", "model", ".", "get_labels", "(", ")", "\n", "logger", ".", "info", "(", "\"Loaded binary model from %s\"", "%", "path", ")", "\n", "\n", "# compute new vocabulary / embeddings", "\n", "embeddings", "=", "np", ".", "concatenate", "(", "[", "model", ".", "get_word_vector", "(", "w", ")", "[", "None", "]", "for", "w", "in", "words", "]", ",", "0", ")", "\n", "embeddings", "=", "torch", ".", "from_numpy", "(", "embeddings", ")", ".", "float", "(", ")", "\n", "word2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "words", ")", "}", "\n", "logger", ".", "info", "(", "\"Generated embeddings for %i words.\"", "%", "len", "(", "words", ")", ")", "\n", "\n", "assert", "embeddings", ".", "size", "(", ")", "==", "(", "len", "(", "word2id", ")", ",", "params", ".", "emb_dim", ")", "\n", "return", "word2id", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings": [[88, 96], ["path.endswith", "pretrain.load_bin_embeddings", "pretrain.read_txt_embeddings"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_bin_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.read_txt_embeddings"], ["", "def", "load_embeddings", "(", "path", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Reload pretrained embeddings.\n    \"\"\"", "\n", "if", "path", ".", "endswith", "(", "'.bin'", ")", ":", "\n", "        ", "return", "load_bin_embeddings", "(", "path", ",", "params", ")", "\n", "", "else", ":", "\n", "        ", "return", "read_txt_embeddings", "(", "path", ",", "params", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.reload": [[21, 53], ["torch.load", "data.dictionary.Dictionary", "utils.AttrDict", "len", "data.dictionary.Dictionary.index", "data.dictionary.Dictionary.index", "data.dictionary.Dictionary.index", "data.dictionary.Dictionary.index", "data.dictionary.Dictionary.index", "transformer.TransformerModel", "transformer.TransformerModel.load_state_dict", "transformer.TransformerModel.eval", "embedder.SentenceEmbedder", "k.startswith", "state_dict.items"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval"], ["    ", "@", "staticmethod", "\n", "def", "reload", "(", "path", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Create a sentence embedder from a pretrained model.\n        \"\"\"", "\n", "# reload model", "\n", "reloaded", "=", "torch", ".", "load", "(", "path", ")", "\n", "state_dict", "=", "reloaded", "[", "'model'", "]", "\n", "\n", "# handle models from multi-GPU checkpoints", "\n", "if", "'checkpoint'", "in", "path", ":", "\n", "            ", "state_dict", "=", "{", "(", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module.'", ")", "else", "k", ")", ":", "v", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", "}", "\n", "\n", "# reload dictionary and model parameters", "\n", "", "dico", "=", "Dictionary", "(", "reloaded", "[", "'dico_id2word'", "]", ",", "reloaded", "[", "'dico_word2id'", "]", ",", "reloaded", "[", "'dico_counts'", "]", ")", "\n", "pretrain_params", "=", "AttrDict", "(", "reloaded", "[", "'params'", "]", ")", "\n", "pretrain_params", ".", "n_words", "=", "len", "(", "dico", ")", "\n", "pretrain_params", ".", "bos_index", "=", "dico", ".", "index", "(", "BOS_WORD", ")", "\n", "pretrain_params", ".", "eos_index", "=", "dico", ".", "index", "(", "EOS_WORD", ")", "\n", "pretrain_params", ".", "pad_index", "=", "dico", ".", "index", "(", "PAD_WORD", ")", "\n", "pretrain_params", ".", "unk_index", "=", "dico", ".", "index", "(", "UNK_WORD", ")", "\n", "pretrain_params", ".", "mask_index", "=", "dico", ".", "index", "(", "MASK_WORD", ")", "\n", "\n", "# build model and reload weights", "\n", "model", "=", "TransformerModel", "(", "pretrain_params", ",", "dico", ",", "True", ",", "True", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# adding missing parameters", "\n", "params", ".", "max_batch_size", "=", "0", "\n", "\n", "return", "SentenceEmbedder", "(", "model", ",", "dico", ",", "pretrain_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.__init__": [[54, 65], ["pretrain_params.__dict__.items"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "model", ",", "dico", ",", "pretrain_params", ")", ":", "\n", "        ", "\"\"\"\n        Wrapper on top of the different sentence embedders.\n        Returns sequence-wise or single-vector sentence representations.\n        \"\"\"", "\n", "self", ".", "pretrain_params", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "pretrain_params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "dico", "=", "dico", "\n", "self", ".", "n_layers", "=", "model", ".", "n_layers", "\n", "self", ".", "out_dim", "=", "model", ".", "dim", "\n", "self", ".", "n_words", "=", "model", ".", "n_words", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.train": [[66, 68], ["embedder.SentenceEmbedder.model.train"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.eval": [[69, 71], ["embedder.SentenceEmbedder.model.eval"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval"], ["", "def", "eval", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda": [[72, 74], ["embedder.SentenceEmbedder.model.cuda"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_parameters": [[75, 119], ["layer_range.split", "range", "logger.info", "len", "int", "int", "embedder.SentenceEmbedder.model.embeddings.parameters", "logger.info", "hasattr", "embedder.SentenceEmbedder.model.layer_norm_emb.parameters", "max", "embedder.SentenceEmbedder.model.attentions[].parameters", "embedder.SentenceEmbedder.model.layer_norm1[].parameters", "embedder.SentenceEmbedder.model.ffns[].parameters", "embedder.SentenceEmbedder.model.layer_norm2[].parameters", "logger.info", "s[].replace", "s[].replace", "embedder.SentenceEmbedder.model.position_embeddings.parameters", "logger.info", "embedder.SentenceEmbedder.model.lang_embeddings.parameters", "logger.info", "sum", "p.nelement"], "methods", ["None"], ["", "def", "get_parameters", "(", "self", ",", "layer_range", ")", ":", "\n", "\n", "        ", "s", "=", "layer_range", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "s", ")", "==", "2", "\n", "i", ",", "j", "=", "int", "(", "s", "[", "0", "]", ".", "replace", "(", "'_'", ",", "'-'", ")", ")", ",", "int", "(", "s", "[", "1", "]", ".", "replace", "(", "'_'", ",", "'-'", ")", ")", "\n", "\n", "# negative indexing", "\n", "i", "=", "self", ".", "n_layers", "+", "i", "+", "1", "if", "i", "<", "0", "else", "i", "\n", "j", "=", "self", ".", "n_layers", "+", "j", "+", "1", "if", "j", "<", "0", "else", "j", "\n", "\n", "# sanity check", "\n", "assert", "0", "<=", "i", "<=", "self", ".", "n_layers", "\n", "assert", "0", "<=", "j", "<=", "self", ".", "n_layers", "\n", "\n", "if", "i", ">", "j", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "parameters", "=", "[", "]", "\n", "\n", "# embeddings", "\n", "if", "i", "==", "0", ":", "\n", "# embeddings", "\n", "            ", "parameters", "+=", "self", ".", "model", ".", "embeddings", ".", "parameters", "(", ")", "\n", "logger", ".", "info", "(", "\"Adding embedding parameters to optimizer\"", ")", "\n", "# positional embeddings", "\n", "if", "self", ".", "pretrain_params", "[", "'sinusoidal_embeddings'", "]", "is", "False", ":", "\n", "                ", "parameters", "+=", "self", ".", "model", ".", "position_embeddings", ".", "parameters", "(", ")", "\n", "logger", ".", "info", "(", "\"Adding positional embedding parameters to optimizer\"", ")", "\n", "# language embeddings", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'lang_embeddings'", ")", ":", "\n", "                ", "parameters", "+=", "self", ".", "model", ".", "lang_embeddings", ".", "parameters", "(", ")", "\n", "logger", ".", "info", "(", "\"Adding language embedding parameters to optimizer\"", ")", "\n", "", "parameters", "+=", "self", ".", "model", ".", "layer_norm_emb", ".", "parameters", "(", ")", "\n", "# layers", "\n", "", "for", "l", "in", "range", "(", "max", "(", "i", "-", "1", ",", "0", ")", ",", "j", ")", ":", "\n", "            ", "parameters", "+=", "self", ".", "model", ".", "attentions", "[", "l", "]", ".", "parameters", "(", ")", "\n", "parameters", "+=", "self", ".", "model", ".", "layer_norm1", "[", "l", "]", ".", "parameters", "(", ")", "\n", "parameters", "+=", "self", ".", "model", ".", "ffns", "[", "l", "]", ".", "parameters", "(", ")", "\n", "parameters", "+=", "self", ".", "model", ".", "layer_norm2", "[", "l", "]", ".", "parameters", "(", ")", "\n", "logger", ".", "info", "(", "\"Adding layer-%s parameters to optimizer\"", "%", "(", "l", "+", "1", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Optimizing on %i Transformer elements.\"", "%", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "parameters", "]", ")", ")", "\n", "\n", "return", "parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_embeddings": [[120, 138], ["x.size", "embedder.SentenceEmbedder.model", "embedder.SentenceEmbedder.size", "lengths.size", "lengths.max().item", "lengths.max"], "methods", ["None"], ["", "def", "get_embeddings", "(", "self", ",", "x", ",", "lengths", ",", "positions", "=", "None", ",", "langs", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            `x`        : LongTensor of shape (slen, bs)\n            `lengths`  : LongTensor of shape (bs,)\n        Outputs:\n            `sent_emb` : FloatTensor of shape (bs, out_dim)\n        With out_dim == emb_dim\n        \"\"\"", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "assert", "lengths", ".", "size", "(", "0", ")", "==", "bs", "and", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "==", "slen", "\n", "\n", "# get transformer last hidden layer", "\n", "tensor", "=", "self", ".", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ",", "self", ".", "out_dim", ")", "\n", "\n", "# single-vector sentence representation (first column of last layer)", "\n", "return", "tensor", "[", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.nucleus_sampling.sample_topp": [[7, 59], ["lprobs.exp_", "lprobs.exp_.sort", "sorted_probs.cumsum", "sorted_probs.cumsum.lt", "mask.scatter_.cumsum", "last_included.clamp_", "mask.scatter_.scatter_", "last_included.max", "truncated_probs.masked_fill_", "mask.scatter_.size"], "function", ["None"], ["def", "sample_topp", "(", "lprobs", ",", "sampling_topp", ",", "remove_indices", "=", "None", ")", ":", "\n", "    ", "\"\"\"Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n\n    See `\"The Curious Case of Neural Text Degeneration\"\n    (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n\n    Args:\n        lprobs: (bsz x input_beam_size x vocab_size)\n            the model's log-probabilities over the vocabulary at the current step\n\n    Return: A tuple of (trimed_probs, truncated_indices) where:\n        trimed_probs: (bsz x input_beam_size x ?)\n            the model's probabilities over the elements selected to sample from. The\n            width of the third dimension is determined by top-P.\n        truncated_indices: (bsz x input_beam_size x ?)\n            the indices of the chosen elements.\n\n    ** for XLM code:\n        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}\n        # so we have to remove bos and pad only, but not eos\n        but in the original, they truncated first 2 words, then the vocab indices will be shifted\n    \"\"\"", "\n", "if", "remove_indices", "is", "None", ":", "\n", "        ", "remove_indices", "=", "[", "0", ",", "2", "]", "\n", "", "probs", "=", "lprobs", ".", "exp_", "(", ")", "\n", "probs", "[", ":", ",", ":", ",", "remove_indices", "]", "=", "0", "\n", "\n", "# sort the last dimension (vocab dimension) in descending order", "\n", "sorted_probs", ",", "sorted_indices", "=", "probs", ".", "sort", "(", "descending", "=", "True", ")", "\n", "\n", "# compute a mask to indicate the words to be included in the top-P set.", "\n", "cumsum_probs", "=", "sorted_probs", ".", "cumsum", "(", "dim", "=", "2", ")", "\n", "mask", "=", "cumsum_probs", ".", "lt", "(", "sampling_topp", ")", "\n", "\n", "# note that mask was computed by 'lt'. One more word needs to be included", "\n", "# so that the cumulative probability mass can exceed p.", "\n", "cumsum_mask", "=", "mask", ".", "cumsum", "(", "dim", "=", "2", ")", "\n", "last_included", "=", "cumsum_mask", "[", ":", ",", ":", ",", "-", "1", ":", "]", "\n", "last_included", ".", "clamp_", "(", "0", ",", "mask", ".", "size", "(", ")", "[", "2", "]", "-", "1", ")", "\n", "mask", "=", "mask", ".", "scatter_", "(", "2", ",", "last_included", ",", "1", ")", "\n", "\n", "# truncate unnecessary dims.", "\n", "max_dim", "=", "last_included", ".", "max", "(", ")", "\n", "truncated_mask", "=", "mask", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "truncated_probs", "=", "sorted_probs", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "truncated_indices", "=", "sorted_indices", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "\n", "# trim the words that are not in top-P by setting their probabilities", "\n", "# to 0, so that they would not be sampled later.", "\n", "trim_mask", "=", "(", "~", "truncated_mask", ")", "\n", "trimed_probs", "=", "truncated_probs", ".", "masked_fill_", "(", "trim_mask", ",", "0", ")", "\n", "return", "trimed_probs", ",", "truncated_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.check_model_params": [[21, 88], ["params.word_mask_keep_rand.split", "len", "float", "all", "len", "params.asm_cutoffs.split", "all", "memory.HashingMemory.check_params", "all", "all", "os.path.isfile", "sum", "int", "len", "len", "len", "len", "os.path.isfile", "params.reload_model.split", "all", "x.isdigit", "params.mem_enc_positions.split", "params.mem_dec_positions.split", "set", "set", "len", "len", "len", "min", "max", "len", "min", "max", "len", "x.isdigit", "x.isdigit", "int", "int", "int", "int", "x[].isdigit", "x[].isdigit", "os.path.isfile"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.check_params"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.set_pretrain_emb": [[90, 105], ["logger.info", "torch.no_grad", "range", "len", "word2id.get", "embeddings[].cuda", "embeddings[].cuda", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model": [[107, 188], ["transformer.TransformerModel", "logger.info", "logger.info", "transformer.TransformerModel.cuda", "transformer.TransformerModel", "transformer.TransformerModel", "logger.debug", "logger.debug", "logger.info", "logger.info", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "logger.info", "all", "transformer.TransformerModel.load_state_dict", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "__init__.set_pretrain_emb", "params.reload_model.split", "transformer.TransformerModel.cuda", "transformer.TransformerModel.cuda", "torch.load", "sum", "logger.info", "torch.load", "all", "transformer.TransformerModel.load_state_dict", "logger.info", "torch.load", "all", "range", "transformer.TransformerModel.load_state_dict", "sum", "sum", "k.startswith", "reloaded.keys", "reloaded.items", "p.numel", "k.startswith", "k.startswith", "p.numel", "p.numel", "storage.cuda", "len", "transformer.TransformerModel.parameters", "storage.cuda", "torch.load.keys", "torch.load.items", "storage.cuda", "torch.load.keys", "torch.load.items", "logger.warning", "transformer.TransformerModel.parameters", "transformer.TransformerModel.parameters", "len", "len", "transformer.TransformerModel.state_dict"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model_multilang": [[190, 273], ["transformer.MultiLangTransformerModel", "logger.info", "logger.info", "transformer.MultiLangTransformerModel.cuda", "transformer.MultiLangTransformerModel", "transformer.MultiLangTransformerModel", "logger.debug", "logger.debug", "logger.info", "logger.info", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "logger.info", "all", "transformer.MultiLangTransformerModel.load_state_dict", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "__init__.set_pretrain_emb", "params.reload_model.split", "transformer.MultiLangTransformerModel.cuda", "transformer.MultiLangTransformerModel.cuda", "torch.load", "sum", "logger.info", "torch.load", "all", "transformer.MultiLangTransformerModel.load_state_dict", "logger.info", "torch.load", "all", "range", "transformer.MultiLangTransformerModel.load_state_dict", "sum", "sum", "k.startswith", "reloaded.keys", "reloaded.items", "p.numel", "k.startswith", "k.startswith", "p.numel", "p.numel", "storage.cuda", "len", "transformer.MultiLangTransformerModel.parameters", "storage.cuda", "torch.load.keys", "torch.load.items", "storage.cuda", "torch.load.keys", "torch.load.items", "logger.warning", "transformer.MultiLangTransformerModel.parameters", "transformer.MultiLangTransformerModel.parameters", "len", "len", "transformer.MultiLangTransformerModel.state_dict", "logger.warning"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model_pretrained_parallel": [[275, 355], ["transformer.TransformerModel", "logger.info", "logger.info", "transformer.TransformerModel.cuda", "transformer.TransformerModel", "transformer.TransformerModel", "logger.debug", "logger.debug", "logger.info", "logger.info", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "logger.info", "all", "transformer.TransformerModel.load_state_dict", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "__init__.set_pretrain_emb", "params.reload_para_model.split", "transformer.TransformerModel.cuda", "transformer.TransformerModel.cuda", "torch.load", "sum", "logger.info", "torch.load", "all", "transformer.TransformerModel.load_state_dict", "logger.info", "torch.load", "all", "range", "transformer.TransformerModel.load_state_dict", "sum", "sum", "k.startswith", "reloaded.keys", "reloaded.items", "p.numel", "k.startswith", "k.startswith", "p.numel", "p.numel", "storage.cuda", "len", "transformer.TransformerModel.parameters", "storage.cuda", "torch.load.keys", "torch.load.items", "storage.cuda", "torch.load.keys", "torch.load.items", "logger.warning", "transformer.TransformerModel.parameters", "transformer.TransformerModel.parameters", "len", "len", "transformer.TransformerModel.state_dict"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.__init__.build_model_multilang_parallel": [[357, 437], ["transformer.MultiLangTransformerModel", "logger.info", "logger.info", "transformer.MultiLangTransformerModel.cuda", "transformer.MultiLangTransformerModel", "transformer.MultiLangTransformerModel", "logger.debug", "logger.debug", "logger.info", "logger.info", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "logger.info", "all", "transformer.MultiLangTransformerModel.load_state_dict", "pretrain.load_embeddings", "__init__.set_pretrain_emb", "__init__.set_pretrain_emb", "params.reload_para_model.split", "transformer.MultiLangTransformerModel.cuda", "transformer.MultiLangTransformerModel.cuda", "torch.load", "sum", "logger.info", "torch.load", "all", "transformer.MultiLangTransformerModel.load_state_dict", "logger.info", "torch.load", "all", "range", "transformer.MultiLangTransformerModel.load_state_dict", "sum", "sum", "k.startswith", "reloaded.keys", "reloaded.items", "p.numel", "k.startswith", "k.startswith", "p.numel", "p.numel", "storage.cuda", "len", "transformer.MultiLangTransformerModel.parameters", "storage.cuda", "torch.load.keys", "torch.load.items", "storage.cuda", "torch.load.keys", "torch.load.items", "logger.warning", "transformer.MultiLangTransformerModel.parameters", "transformer.MultiLangTransformerModel.parameters", "len", "len", "transformer.MultiLangTransformerModel.state_dict", "logger.warning"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.pretrain.load_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], []], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.PredLayer.__init__": [[113, 130], ["torch.Module.__init__", "getattr", "transformer.Linear", "torch.AdaptiveLogSoftmaxWithLoss", "torch.AdaptiveLogSoftmaxWithLoss", "torch.AdaptiveLogSoftmaxWithLoss"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "asm", "=", "params", ".", "asm", "\n", "self", ".", "label_smoothing", "=", "getattr", "(", "params", ",", "'label_smoothing'", ",", "0", ")", "\n", "self", ".", "n_words", "=", "params", ".", "n_words", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "dim", "=", "params", ".", "emb_dim", "\n", "\n", "if", "params", ".", "asm", "is", "False", ":", "\n", "            ", "self", ".", "proj", "=", "Linear", "(", "dim", ",", "params", ".", "n_words", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "AdaptiveLogSoftmaxWithLoss", "(", "\n", "in_features", "=", "dim", ",", "\n", "n_classes", "=", "params", ".", "n_words", ",", "\n", "cutoffs", "=", "params", ".", "asm_cutoffs", ",", "\n", "div_value", "=", "params", ".", "asm_div_value", ",", "\n", "head_bias", "=", "True", ",", "# default is False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.PredLayer._label_smoothed_nll_loss": [[132, 151], ["targets.unsqueeze.unsqueeze.ne", "nll_loss.sum.sum.sum", "smooth_loss.sum.sum.sum", "targets.unsqueeze.unsqueeze.dim", "targets.unsqueeze.unsqueeze.unsqueeze", "lprobs.gather", "lprobs.sum", "lprobs.size", "lprobs.dim"], "methods", ["None"], ["", "", "def", "_label_smoothed_nll_loss", "(", "self", ",", "lprobs", ",", "targets", ")", ":", "\n", "        ", "if", "targets", ".", "dim", "(", ")", "==", "lprobs", ".", "dim", "(", ")", "-", "1", ":", "\n", "            ", "targets", "=", "targets", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# logger.info('lprobs: {} , targets: {}'.format(lprobs.size(), targets.size()))", "\n", "", "nll_loss", "=", "-", "lprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "targets", ")", "\n", "smooth_loss", "=", "-", "lprobs", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "# if ignore_index is not None:", "\n", "non_pad_mask", "=", "targets", ".", "ne", "(", "self", ".", "pad_index", ")", "\n", "nll_loss", "=", "nll_loss", "[", "non_pad_mask", "]", "\n", "smooth_loss", "=", "smooth_loss", "[", "non_pad_mask", "]", "\n", "# else:", "\n", "#     nll_loss = nll_loss.squeeze(-1)", "\n", "#     smooth_loss = smooth_loss.squeeze(-1)", "\n", "# if reduce:", "\n", "nll_loss", "=", "nll_loss", ".", "sum", "(", ")", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "eps_i", "=", "self", ".", "label_smoothing", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "self", ".", "label_smoothing", ")", "*", "nll_loss", "+", "eps_i", "*", "smooth_loss", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.PredLayer.forward": [[152, 169], ["transformer.PredLayer.proj().view", "transformer.PredLayer.proj", "transformer.PredLayer._label_smoothed_nll_loss", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "transformer.PredLayer.proj.log_prob", "transformer.PredLayer.proj", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.PredLayer._label_smoothed_nll_loss"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "get_scores", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Compute the loss, and optionally the scores.\n        \"\"\"", "\n", "assert", "(", "y", "==", "self", ".", "pad_index", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "0", "\n", "\n", "if", "self", ".", "asm", "is", "False", ":", "\n", "            ", "scores", "=", "self", ".", "proj", "(", "x", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_words", ")", "\n", "if", "self", ".", "label_smoothing", ">", "0", ":", "\n", "                ", "loss", "=", "self", ".", "_label_smoothed_nll_loss", "(", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", ",", "y", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "y", ",", "reduction", "=", "'mean'", ")", "\n", "", "", "else", ":", "\n", "            ", "_", ",", "loss", "=", "self", ".", "proj", "(", "x", ",", "y", ")", "\n", "scores", "=", "self", ".", "proj", ".", "log_prob", "(", "x", ")", "if", "get_scores", "else", "None", "\n", "\n", "", "return", "scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.PredLayer.get_scores": [[170, 176], ["x.dim", "transformer.PredLayer.proj.log_prob", "transformer.PredLayer.proj"], "methods", ["None"], ["", "def", "get_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Compute scores.\n        \"\"\"", "\n", "assert", "x", ".", "dim", "(", ")", "==", "2", "\n", "return", "self", ".", "proj", ".", "log_prob", "(", "x", ")", "if", "self", ".", "asm", "else", "self", ".", "proj", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.MultiHeadAttention.__init__": [[181, 193], ["torch.Module.__init__", "next", "transformer.Linear", "transformer.Linear", "transformer.Linear", "transformer.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["def", "__init__", "(", "self", ",", "n_heads", ",", "dim", ",", "dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layer_id", "=", "next", "(", "MultiHeadAttention", ".", "NEW_ID", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "n_heads", "=", "n_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "assert", "self", ".", "dim", "%", "self", ".", "n_heads", "==", "0", "\n", "\n", "self", ".", "q_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "k_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "v_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "out_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.MultiHeadAttention.forward": [[194, 254], ["input.size", "transformer.MultiHeadAttention.forward.shape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "mask", ",", "kv", "=", "None", ",", "cache", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n        \"\"\"", "\n", "# Input is (bs, qlen, dim)", "\n", "# Mask is (bs, klen) (non-causal) or (bs, klen, klen)", "\n", "bs", ",", "qlen", ",", "dim", "=", "input", ".", "size", "(", ")", "\n", "if", "kv", "is", "None", ":", "\n", "            ", "klen", "=", "qlen", "if", "cache", "is", "None", "else", "cache", "[", "'slen'", "]", "+", "qlen", "\n", "", "else", ":", "\n", "            ", "klen", "=", "kv", ".", "size", "(", "1", ")", "\n", "", "assert", "dim", "==", "self", ".", "dim", ",", "'Dimensions do not match: %s input vs %s configured'", "%", "(", "dim", ",", "self", ".", "dim", ")", "\n", "n_heads", "=", "self", ".", "n_heads", "\n", "dim_per_head", "=", "dim", "//", "n_heads", "\n", "mask_reshape", "=", "(", "bs", ",", "1", ",", "qlen", ",", "klen", ")", "if", "mask", ".", "dim", "(", ")", "==", "3", "else", "(", "bs", ",", "1", ",", "1", ",", "klen", ")", "\n", "\n", "def", "shape", "(", "x", ")", ":", "\n", "            ", "\"\"\"  projection \"\"\"", "\n", "return", "x", ".", "view", "(", "bs", ",", "-", "1", ",", "self", ".", "n_heads", ",", "dim_per_head", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "", "def", "unshape", "(", "x", ")", ":", "\n", "            ", "\"\"\"  compute context \"\"\"", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "bs", ",", "-", "1", ",", "self", ".", "n_heads", "*", "dim_per_head", ")", "\n", "\n", "", "q", "=", "shape", "(", "self", ".", "q_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "if", "kv", "is", "None", ":", "\n", "            ", "k", "=", "shape", "(", "self", ".", "k_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "v", "=", "shape", "(", "self", ".", "v_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "", "elif", "cache", "is", "None", "or", "self", ".", "layer_id", "not", "in", "cache", ":", "\n", "            ", "k", "=", "v", "=", "kv", "\n", "k", "=", "shape", "(", "self", ".", "k_lin", "(", "k", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "v", "=", "shape", "(", "self", ".", "v_lin", "(", "v", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "layer_id", "in", "cache", ":", "\n", "                ", "if", "kv", "is", "None", ":", "\n", "                    ", "k_", ",", "v_", "=", "cache", "[", "self", ".", "layer_id", "]", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k_", ",", "k", "]", ",", "dim", "=", "2", ")", "# (bs, n_heads, klen, dim_per_head)", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v_", ",", "v", "]", ",", "dim", "=", "2", ")", "# (bs, n_heads, klen, dim_per_head)", "\n", "", "else", ":", "\n", "                    ", "k", ",", "v", "=", "cache", "[", "self", ".", "layer_id", "]", "\n", "", "", "cache", "[", "self", ".", "layer_id", "]", "=", "(", "k", ",", "v", ")", "\n", "\n", "", "q", "=", "q", "/", "math", ".", "sqrt", "(", "dim_per_head", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "scores", "=", "torch", ".", "matmul", "(", "q", ",", "k", ".", "transpose", "(", "2", ",", "3", ")", ")", "# (bs, n_heads, qlen, klen)", "\n", "mask", "=", "(", "mask", "==", "0", ")", ".", "view", "(", "mask_reshape", ")", ".", "expand_as", "(", "scores", ")", "# (bs, n_heads, qlen, klen)", "\n", "# scores.masked_fill_(mask, -float('inf'))                              # (bs, n_heads, qlen, klen)", "\n", "try", ":", "\n", "# scores.masked_fill_(mask.byte(), -float('inf'))  # (bs, n_heads, qlen, klen)", "\n", "            ", "scores", ".", "masked_fill_", "(", "mask", ".", "bool", "(", ")", ",", "-", "float", "(", "'inf'", ")", ")", "# (bs, n_heads, qlen, klen)", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "# scores.masked_fill_(mask.bool(), -float('inf'))  # (bs, n_heads, qlen, klen)", "\n", "            ", "scores", ".", "masked_fill_", "(", "mask", ".", "byte", "(", ")", ",", "-", "float", "(", "'inf'", ")", ")", "# (bs, n_heads, qlen, klen)", "\n", "\n", "", "weights", "=", "F", ".", "softmax", "(", "scores", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ".", "type_as", "(", "scores", ")", "# (bs, n_heads, qlen, klen)", "\n", "weights", "=", "F", ".", "dropout", "(", "weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "# (bs, n_heads, qlen, klen)", "\n", "context", "=", "torch", ".", "matmul", "(", "weights", ",", "v", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "context", "=", "unshape", "(", "context", ")", "# (bs, qlen, dim)", "\n", "\n", "return", "self", ".", "out_lin", "(", "context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerFFN.__init__": [[258, 264], ["torch.Module.__init__", "transformer.Linear", "transformer.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "dim_hidden", ",", "out_dim", ",", "dropout", ",", "gelu_activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "lin1", "=", "Linear", "(", "in_dim", ",", "dim_hidden", ")", "\n", "self", ".", "lin2", "=", "Linear", "(", "dim_hidden", ",", "out_dim", ")", "\n", "self", ".", "act", "=", "gelu", "if", "gelu_activation", "else", "F", ".", "relu", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerFFN.forward": [[265, 271], ["transformer.TransformerFFN.lin1", "transformer.TransformerFFN.act", "transformer.TransformerFFN.lin2", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", "=", "self", ".", "lin1", "(", "input", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "lin2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.__init__": [[435, 511], ["torch.Module.__init__", "getattr", "transformer.Embedding", "transformer.Embedding", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "getattr", "range", "len", "len", "len", "transformer.create_sinusoidal_embeddings", "transformer.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.TransformerModel.attentions.append", "transformer.TransformerModel.layer_norm1.append", "transformer.TransformerModel.layer_norm2.append", "transformer.PredLayer", "memory.HashingMemory.build", "transformer.MultiHeadAttention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.TransformerModel.layer_norm15.append", "transformer.TransformerModel.encoder_attn.append", "transformer.TransformerModel.ffns.append", "transformer.TransformerModel.ffns.append", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.MultiHeadAttention", "transformer.TransformerFFN"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.create_sinusoidal_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.build"], ["def", "__init__", "(", "self", ",", "params", ",", "dico", ",", "is_encoder", ",", "with_output", ")", ":", "\n", "        ", "\"\"\"\n        Transformer model (encoder or decoder).\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# encoder / decoder, output layer", "\n", "self", ".", "is_encoder", "=", "is_encoder", "\n", "self", ".", "is_decoder", "=", "not", "is_encoder", "\n", "self", ".", "with_output", "=", "with_output", "\n", "\n", "# dictionary / languages", "\n", "self", ".", "n_langs", "=", "params", ".", "n_langs", "\n", "self", ".", "n_words", "=", "params", ".", "n_words", "\n", "self", ".", "eos_index", "=", "params", ".", "eos_index", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "self", ".", "dico", "=", "dico", "\n", "self", ".", "id2lang", "=", "params", ".", "id2lang", "\n", "self", ".", "lang2id", "=", "params", ".", "lang2id", "\n", "self", ".", "use_lang_emb", "=", "getattr", "(", "params", ",", "'use_lang_emb'", ",", "True", ")", "\n", "assert", "len", "(", "self", ".", "dico", ")", "==", "self", ".", "n_words", "\n", "assert", "len", "(", "self", ".", "id2lang", ")", "==", "len", "(", "self", ".", "lang2id", ")", "==", "self", ".", "n_langs", "\n", "\n", "# model parameters", "\n", "self", ".", "dim", "=", "params", ".", "emb_dim", "# 512 by default", "\n", "self", ".", "hidden_dim", "=", "self", ".", "dim", "*", "4", "# 2048 by default", "\n", "self", ".", "n_heads", "=", "params", ".", "n_heads", "# 8 by default", "\n", "self", ".", "n_layers", "=", "params", ".", "n_layers", "\n", "self", ".", "dropout", "=", "params", ".", "dropout", "\n", "self", ".", "attention_dropout", "=", "params", ".", "attention_dropout", "\n", "assert", "self", ".", "dim", "%", "self", ".", "n_heads", "==", "0", ",", "'transformer dim must be a multiple of n_heads'", "\n", "\n", "# embeddings", "\n", "self", ".", "position_embeddings", "=", "Embedding", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ")", "\n", "if", "params", ".", "sinusoidal_embeddings", ":", "\n", "            ", "create_sinusoidal_embeddings", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ",", "out", "=", "self", ".", "position_embeddings", ".", "weight", ")", "\n", "", "if", "params", ".", "n_langs", ">", "1", "and", "self", ".", "use_lang_emb", ":", "\n", "            ", "self", ".", "lang_embeddings", "=", "Embedding", "(", "self", ".", "n_langs", ",", "self", ".", "dim", ")", "\n", "", "self", ".", "embeddings", "=", "Embedding", "(", "self", ".", "n_words", ",", "self", ".", "dim", ",", "padding_idx", "=", "self", ".", "pad_index", ")", "\n", "self", ".", "layer_norm_emb", "=", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "\n", "\n", "# transformer layers", "\n", "self", ".", "attentions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm1", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "ffns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm2", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "self", ".", "layer_norm15", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "encoder_attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# memories", "\n", "", "self", ".", "memories", "=", "nn", ".", "ModuleDict", "(", ")", "\n", "if", "getattr", "(", "params", ",", "'use_memory'", ",", "False", ")", ":", "\n", "            ", "mem_positions", "=", "params", ".", "mem_enc_positions", "if", "is_encoder", "else", "params", ".", "mem_dec_positions", "\n", "for", "layer_id", ",", "pos", "in", "mem_positions", ":", "\n", "                ", "assert", "0", "<=", "layer_id", "<=", "params", ".", "n_layers", "-", "1", "\n", "assert", "pos", "in", "[", "'in'", ",", "'after'", "]", "\n", "self", ".", "memories", "[", "'%i_%s'", "%", "(", "layer_id", ",", "pos", ")", "]", "=", "HashingMemory", ".", "build", "(", "self", ".", "dim", ",", "self", ".", "dim", ",", "params", ")", "\n", "\n", "", "", "for", "layer_id", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "            ", "self", ".", "attentions", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", ")", "\n", "self", ".", "layer_norm1", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                ", "self", ".", "layer_norm15", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "self", ".", "encoder_attn", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", ")", "\n", "", "if", "(", "'%i_in'", "%", "layer_id", ")", "in", "self", ".", "memories", ":", "\n", "                ", "self", ".", "ffns", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "ffns", ".", "append", "(", "TransformerFFN", "(", "self", ".", "dim", ",", "self", ".", "hidden_dim", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "dropout", ",", "gelu_activation", "=", "params", ".", "gelu_activation", ")", ")", "\n", "", "self", ".", "layer_norm2", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "\n", "# output layer", "\n", "", "if", "self", ".", "with_output", ":", "\n", "            ", "self", ".", "pred_layer", "=", "PredLayer", "(", "params", ")", "\n", "if", "params", ".", "share_inout_emb", ":", "\n", "                ", "self", ".", "pred_layer", ".", "proj", ".", "weight", "=", "self", ".", "embeddings", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.forward": [[512, 523], ["transformer.TransformerModel.fwd", "transformer.TransformerModel.predict", "Exception"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.predict"], ["", "", "", "def", "forward", "(", "self", ",", "mode", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Forward function with different forward modes.\n        ### Small hack to handle PyTorch distributed.\n        \"\"\"", "\n", "if", "mode", "==", "'fwd'", ":", "\n", "            ", "return", "self", ".", "fwd", "(", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "'predict'", ":", "\n", "            ", "return", "self", ".", "predict", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown mode: %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.fwd": [[524, 623], ["x.transpose.transpose.size", "x.transpose.transpose.transpose", "transformer.get_masks", "transformer.TransformerModel.embeddings", "transformer.TransformerModel.layer_norm_emb", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "range", "tensor.transpose.transpose.transpose", "lengths.size", "lengths.max().item", "x.transpose.transpose.new().long", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "positions.transpose.transpose.transpose", "langs.transpose.transpose.transpose", "transformer.TransformerModel.position_embeddings().expand_as", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "tensor.transpose.transpose.size", "src_enc.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "positions.transpose.transpose.size", "langs.transpose.transpose.size", "transformer.TransformerModel.lang_embeddings", "mask.unsqueeze", "torch.dropout", "torch.dropout", "torch.dropout", "lengths.max", "src_len.max", "x.transpose.transpose.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "transformer.TransformerModel.position_embeddings", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.get_masks"], ["", "", "def", "fwd", "(", "self", ",", "x", ",", "lengths", ",", "causal", ",", "src_enc", "=", "None", ",", "src_len", "=", "None", ",", "positions", "=", "None", ",", "langs", "=", "None", ",", "cache", "=", "None", ",", "enc_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            `x` LongTensor(slen, bs), containing word indices\n            `lengths` LongTensor(bs), containing the length of each sentence\n            `causal` Boolean, if True, the attention is only done over previous hidden states\n            `positions` LongTensor(slen, bs), containing word positions\n            `langs` LongTensor(slen, bs), containing language IDs\n        \"\"\"", "\n", "# lengths = (x != self.pad_index).float().sum(dim=1)", "\n", "# mask = x != self.pad_index", "\n", "\n", "# check inputs", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "assert", "lengths", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "<=", "slen", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# batch size as dimension 0", "\n", "assert", "(", "src_enc", "is", "None", ")", "==", "(", "src_len", "is", "None", ")", "\n", "if", "src_enc", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "is_decoder", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generate masks", "\n", "", "mask", ",", "attn_mask", "=", "get_masks", "(", "slen", ",", "lengths", ",", "causal", ")", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "            ", "src_mask", "=", "torch", ".", "arange", "(", "src_len", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "<", "src_len", "[", ":", ",", "None", "]", "\n", "if", "enc_mask", "is", "not", "None", ":", "\n", "                ", "src_mask", "&=", "enc_mask", "\n", "\n", "# positions", "\n", "", "", "if", "positions", "is", "None", ":", "\n", "            ", "positions", "=", "x", ".", "new", "(", "slen", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "slen", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "assert", "positions", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "positions", "=", "positions", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# langs", "\n", "", "if", "langs", "is", "not", "None", ":", "\n", "            ", "assert", "langs", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "langs", "=", "langs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# do not recompute cached elements", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "_slen", "=", "slen", "-", "cache", "[", "'slen'", "]", "\n", "x", "=", "x", "[", ":", ",", "-", "_slen", ":", "]", "\n", "positions", "=", "positions", "[", ":", ",", "-", "_slen", ":", "]", "\n", "if", "langs", "is", "not", "None", ":", "\n", "                ", "langs", "=", "langs", "[", ":", ",", "-", "_slen", ":", "]", "\n", "", "mask", "=", "mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "attn_mask", "=", "attn_mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "\n", "# embeddings", "\n", "", "tensor", "=", "self", ".", "embeddings", "(", "x", ")", "\n", "tensor", "=", "tensor", "+", "self", ".", "position_embeddings", "(", "positions", ")", ".", "expand_as", "(", "tensor", ")", "\n", "if", "langs", "is", "not", "None", "and", "self", ".", "use_lang_emb", ":", "\n", "            ", "tensor", "=", "tensor", "+", "self", ".", "lang_embeddings", "(", "langs", ")", "\n", "", "tensor", "=", "self", ".", "layer_norm_emb", "(", "tensor", ")", "\n", "tensor", "=", "F", ".", "dropout", "(", "tensor", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "# transformer layers", "\n", "for", "i", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "\n", "# self attention", "\n", "            ", "attn", "=", "self", ".", "attentions", "[", "i", "]", "(", "tensor", ",", "attn_mask", ",", "cache", "=", "cache", ")", "\n", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm1", "[", "i", "]", "(", "tensor", ")", "\n", "\n", "# encoder attention (for decoder only)", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "                ", "attn", "=", "self", ".", "encoder_attn", "[", "i", "]", "(", "tensor", ",", "src_mask", ",", "kv", "=", "src_enc", ",", "cache", "=", "cache", ")", "\n", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm15", "[", "i", "]", "(", "tensor", ")", "\n", "\n", "# FFN", "\n", "", "if", "(", "'%i_in'", "%", "i", ")", "in", "self", ".", "memories", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "memories", "[", "'%i_in'", "%", "i", "]", "(", "tensor", ")", "\n", "", "else", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "ffns", "[", "i", "]", "(", "tensor", ")", "\n", "", "tensor", "=", "self", ".", "layer_norm2", "[", "i", "]", "(", "tensor", ")", "\n", "\n", "# memory", "\n", "if", "(", "'%i_after'", "%", "i", ")", "in", "self", ".", "memories", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "memories", "[", "'%i_after'", "%", "i", "]", "(", "tensor", ")", "\n", "# TODO: add extra layer norm here?", "\n", "\n", "", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "# update cache length", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "cache", "[", "'slen'", "]", "+=", "tensor", ".", "size", "(", "1", ")", "\n", "\n", "# move back sequence length to dimension 0", "\n", "", "tensor", "=", "tensor", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.predict": [[624, 638], ["tensor[].view", "transformer.TransformerModel.pred_layer", "torch.softmax", "torch.softmax", "torch.softmax", "transformer.TransformerModel.pred_layer.proj", "pred_mask.unsqueeze().expand_as", "pred_mask.unsqueeze"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "tensor", ",", "pred_mask", ",", "y", ",", "get_scores", ",", "softmax", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Given the last hidden state, compute word scores and/or the loss.\n            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n                we need to predict a word\n            `y` is a LongTensor of shape (pred_mask.sum(),)\n            `get_scores` is a boolean specifying whether we need to return scores\n        \"\"\"", "\n", "if", "softmax", ":", "\n", "            ", "out_softmax", "=", "F", ".", "softmax", "(", "self", ".", "pred_layer", ".", "proj", "(", "tensor", ")", ",", "-", "1", ")", "\n", "return", "out_softmax", "\n", "", "masked_tensor", "=", "tensor", "[", "pred_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "tensor", ")", "]", ".", "view", "(", "-", "1", ",", "self", ".", "dim", ")", "\n", "scores", ",", "loss", "=", "self", ".", "pred_layer", "(", "masked_tensor", ",", "y", ",", "get_scores", ")", "\n", "return", "scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate": [[639, 730], ["len", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "src_enc.size", "transformer.TransformerModel.forward", "tensor.data[].type_as.data[].type_as.data[].type_as", "transformer.TransformerModel.pred_layer.get_scores", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "tensor.data[].type_as.data[].type_as.size", "src_enc.size", "tensor.data[].type_as.data[].type_as.size", "[].squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "src_len.clone().fill_.max", "generated[].masked_fill_", "src_len.clone().fill_.bool", "generated[].masked_fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "src_len.clone().fill_.byte", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate", "(", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# input batch", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ",", "(", "cur_len", ",", "max_len", ",", "src_enc", ".", "size", "(", ")", ",", "tensor", ".", "size", "(", ")", ",", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "src_enc", ")", "# (bs, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs, n_words)", "\n", "\n", "# select next words: sample or greedy", "\n", "if", "sample_temperature", "is", "None", ":", "\n", "                ", "next_words", "=", "torch", ".", "topk", "(", "scores", ",", "1", ")", "[", "1", "]", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "1", ")", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "try", ":", "\n", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "bool", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# generated[-1].masked_fill_(unfinished_sents.bool(), self.eos_index)", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_nucleus_sampling": [[731, 829], ["len", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "src_enc.size", "transformer.TransformerModel.forward", "tensor.data[].type_as.data[].type_as.data[].type_as", "transformer.TransformerModel.pred_layer.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "nucleus_sampling.sample_topp", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "top_indices.gather().squeeze", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "tensor.data[].type_as.data[].type_as.size", "src_enc.size", "tensor.data[].type_as.data[].type_as.size", "torch.log_softmax.unsqueeze_", "probs.squeeze_", "top_indices.squeeze_", "top_indices.gather().squeeze.size", "top_indices.gather().squeeze.size", "top_indices.gather().squeeze.ne().long", "src_len.clone().fill_.max", "generated[].masked_fill_", "top_indices.gather", "src_len.clone().fill_.bool", "generated[].masked_fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "top_indices.gather().squeeze.ne", "src_len.clone().fill_.byte"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.nucleus_sampling.sample_topp"], ["", "def", "generate_nucleus_sampling", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ",", "sampling_topp", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start. using nucleus sampling\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# input batch", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "sampling_topp", ">", "0", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ",", "(", "cur_len", ",", "max_len", ",", "src_enc", ".", "size", "(", ")", ",", "tensor", ".", "size", "(", ")", ",", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "src_enc", ")", "# (bs, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs, n_words)", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "# lprobs_nopad = lprobs[:, ]", "\n", "# # select next words: sample or greedy", "\n", "# if sample_temperature is None:", "\n", "#     next_words = torch.topk(scores, 1)[1].squeeze(1)", "\n", "# else:", "\n", "#     next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)", "\n", "probs", ",", "top_indices", "=", "sample_topp", "(", "lprobs", ".", "unsqueeze_", "(", "1", ")", ",", "sampling_topp", ")", "\n", "probs", ",", "top_indices", "=", "probs", ".", "squeeze_", "(", "1", ")", ",", "top_indices", ".", "squeeze_", "(", "1", ")", "\n", "sample_indices", "=", "torch", ".", "multinomial", "(", "probs", ",", "1", ",", "replacement", "=", "True", ")", "\n", "next_words", "=", "top_indices", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "sample_indices", ")", ".", "squeeze", "(", "1", ")", "\n", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", ",", "'{} != {}'", ".", "format", "(", "next_words", ".", "size", "(", ")", ",", "bs", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "try", ":", "\n", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "bool", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# generated[-1].masked_fill_(unfinished_sents.bool(), self.eos_index)", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_topk_sampling": [[830, 935], ["len", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "src_enc.size", "transformer.TransformerModel.forward", "tensor.data[].type_as.data[].type_as.data[].type_as", "transformer.TransformerModel.pred_layer.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "lprobs_topk.exp", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "topk_indices.gather().squeeze", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "tensor.data[].type_as.data[].type_as.size", "src_enc.size", "tensor.data[].type_as.data[].type_as.size", "topk_indices.gather().squeeze.size", "topk_indices.gather().squeeze.size", "topk_indices.gather().squeeze.ne().long", "src_len.clone().fill_.max", "generated[].masked_fill_", "topk_indices.gather", "src_len.clone().fill_.bool", "generated[].masked_fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "topk_indices.gather().squeeze.ne", "src_len.clone().fill_.byte"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate_topk_sampling", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ",", "sampling_topk", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start. using nucleus sampling\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# input batch", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "sampling_topk", ">", "0", "\n", "sample_temperature", "=", "1", "if", "sample_temperature", "is", "None", "else", "sample_temperature", "\n", "assert", "0", "<", "sample_temperature", "<", "1", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ",", "(", "cur_len", ",", "max_len", ",", "src_enc", ".", "size", "(", ")", ",", "tensor", ".", "size", "(", ")", ",", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "src_enc", ")", "# (bs, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs, n_words)", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "1", ")", "\n", "\n", "# lprobs_nopad = lprobs[:, ]", "\n", "# # select next words: sample or greedy", "\n", "# if sample_temperature is None:", "\n", "#     next_words = torch.topk(scores, 1)[1].squeeze(1)", "\n", "# else:", "\n", "#     next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)", "\n", "lprobs_topk", ",", "topk_indices", "=", "torch", ".", "topk", "(", "lprobs", ",", "sampling_topk", ",", "dim", "=", "-", "1", ")", "\n", "probs_topk", "=", "lprobs_topk", ".", "exp", "(", ")", "\n", "\n", "# probs, top_indices = sample_topp(lprobs.unsqueeze_(1), sampling_topp)", "\n", "# probs, top_indices = probs.squeeze_(1), top_indices.squeeze_(1)", "\n", "sample_indices", "=", "torch", ".", "multinomial", "(", "probs_topk", ",", "1", ",", "replacement", "=", "True", ")", "\n", "next_words", "=", "topk_indices", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "sample_indices", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", ",", "'{} != {}'", ".", "format", "(", "next_words", ".", "size", "(", ")", ",", "bs", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "try", ":", "\n", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "bool", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# generated[-1].masked_fill_(unfinished_sents.bool(), self.eos_index)", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam": [[936, 1120], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.new.view", "beam_search.compute_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "beam_search.BeamHypotheses", "transformer.TransformerModel.forward", "transformer.TransformerModel.pred_layer.get_scores", "range", "beam_scores.new.new.new", "src_len.unsqueeze().expand().contiguous().view.new.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "cache.keys", "all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "transformer.TransformerModel.size", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "_lprobs.view.view.view", "_lprobs.view.view.gather", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.multinomial.gather", "torch.multinomial.gather", "torch.multinomial.gather", "zip", "next_batch_beam.extend", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "torch.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "torch.multinomial.gather.size", "beam_scores[].expand_as", "logger.info", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "next_scores.size", "torch.multinomial.gather.size", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "_lprobs.view.view.exp", "logger.info", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "_lprobs.view.view.size", "_lprobs.view.view.exp", "generated[].clone", "value.item", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "_lprobs.view.view.size", "_lprobs.view.view.exp", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_beam", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# check inputs", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "\n", "# ---old code --------------", "\n", "# scores = F.log_softmax(scores, dim=-1)       # (bs * beam_size, n_words)", "\n", "# assert scores.size() == (bs * beam_size, n_words)", "\n", "#", "\n", "# assert sample_temperature is None or sample_temperature == 1.0, 'sample_temperature={} not support'.format(", "\n", "#     sample_temperature)", "\n", "#", "\n", "# # select next words with scores", "\n", "# _scores = scores + beam_scores[:, None].expand_as(scores)  # (bs * beam_size, n_words)", "\n", "# _scores = _scores.view(bs, beam_size * n_words)            # (bs, beam_size * n_words)", "\n", "#", "\n", "# next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)", "\n", "# assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)", "\n", "# ----- old code -----------", "\n", "if", "sample_temperature", "is", "None", "or", "sample_temperature", "==", "1.0", ":", "\n", "                ", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "", "else", ":", "\n", "                ", "lprobs", "=", "F", ".", "log_softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "\n", "_lprobs", "=", "lprobs", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_lprobs", "=", "_lprobs", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "try", ":", "\n", "                    ", "logger", ".", "info", "(", "'choice_words: ({},{},{}) {} / \\n{}'", ".", "format", "(", "\n", "bs", ",", "beam_size", ",", "n_words", ",", "_lprobs", ".", "size", "(", ")", ",", "_lprobs", ".", "exp", "(", ")", ")", ")", "\n", "choice_words", "=", "torch", ".", "multinomial", "(", "_lprobs", ".", "exp", "(", ")", ",", "2", "*", "beam_size", ",", "replacement", "=", "False", ")", "\n", "# choice_words: (18,5,10158) torch.Size([18, 50790])", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "logger", ".", "info", "(", "'Failed choice_words: ({},{},{}) {} / \\n{}'", ".", "format", "(", "\n", "bs", ",", "beam_size", ",", "n_words", ",", "_lprobs", ".", "size", "(", ")", ",", "_lprobs", ".", "exp", "(", ")", ")", ")", "\n", "raise", "e", "\n", "", "choice_lprobs", "=", "_lprobs", ".", "gather", "(", "1", ",", "choice_words", ")", "\n", "\n", "next_scores", ",", "next_word_idx", "=", "torch", ".", "topk", "(", "choice_lprobs", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "next_words", "=", "choice_words", ".", "gather", "(", "1", ",", "next_word_idx", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "# ---- new code", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                        ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "                ", "if", "k", "!=", "'slen'", ":", "\n", "                    ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "compute_final_decoded", "(", "generated_hyps", ",", "bs", ",", "src_len", ",", "self", ".", "pad_index", ",", "self", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam_efficient": [[1121, 1146], ["efficient_beam_search.generate_beam_gpu"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu"], ["", "def", "generate_beam_efficient", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "hyps_size_multiple", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# hyps_size_multiple = getattr(self.)", "\n", "return", "generate_beam_gpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "nbest", "=", "nbest", ",", "sample_temperature", "=", "sample_temperature", ",", "\n", "hyps_size_multiple", "=", "hyps_size_multiple", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam_efficient_topn": [[1148, 1173], ["efficient_beam_search.generate_beam_gpu_sample_topn"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu_sample_topn"], ["", "def", "generate_beam_efficient_topn", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "sample_topn", "=", "100", ",", "replacement", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# hyps_size_multiple = getattr(self.)", "\n", "return", "generate_beam_gpu_sample_topn", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "nbest", "=", "nbest", ",", "sample_temperature", "=", "sample_temperature", ",", "\n", "sample_topn", "=", "sample_topn", ",", "replacement", "=", "replacement", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam_efficient_diverse": [[1175, 1199], ["efficient_beam_search.generate_diverse_beam_search_gpu"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_diverse_beam_search_gpu"], ["", "def", "generate_beam_efficient_diverse", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "num_groups", ",", "diversity_strength", ",", "max_len", "=", "200", ",", "nbest", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# hyps_size_multiple = getattr(self.)", "\n", "return", "generate_diverse_beam_search_gpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "nbest", "=", "nbest", ",", "num_groups", "=", "num_groups", ",", "diversity_strength", "=", "diversity_strength", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam_efficient_validate_cpu": [[1202, 1227], ["efficient_beam_search.generate_beam_efficient_validate_cpu"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_beam_efficient_validate_cpu"], ["", "def", "generate_beam_efficient_validate_cpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "hyps_size_multiple", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# hyps_size_multiple = getattr(self.)", "\n", "return", "generate_beam_efficient_validate_cpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "nbest", "=", "nbest", ",", "sample_temperature", "=", "sample_temperature", ",", "\n", "hyps_size_multiple", "=", "hyps_size_multiple", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_limit_genetic_variation_beam": [[1229, 1276], ["transformer.TransformerModel.generate", "transformer.TransformerModel.generate_beam_efficient", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.min", "int", "transformer.generate_beam_from_prefix", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.dim", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.dim", "min", "logger.info", "logger.info", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.dim", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.size", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous", "NotImplementedError", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.size", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.min", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.min", "fil_generated.size", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.size", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze().expand", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.min", "generated.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze", "lengths.unsqueeze().expand().contiguous.unsqueeze().expand().contiguous.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.generate_beam_efficient", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.generate_beam_from_prefix"], ["", "def", "generate_limit_genetic_variation_beam", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "limit_percent", ",", "limit_beam_size", "=", "1", ",", "max_len", "=", "200", ",", "nbest", "=", "None", ")", ":", "\n", "\n", "# max_len = int(1.5 * src_len.max().item() + 10)", "\n", "        ", "if", "limit_beam_size", "==", "1", ":", "\n", "            ", "generated", ",", "lengths", "=", "self", ".", "generate", "(", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "            ", "generated", ",", "lengths", "=", "self", ".", "generate_beam_efficient", "(", "\n", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", "=", "limit_beam_size", ",", "\n", "length_penalty", "=", "length_penalty", ",", "\n", "early_stopping", "=", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "\n", "# filter", "\n", "", "if", "lengths", ".", "min", "(", ")", ">", "4", ":", "\n", "            ", "filter_len_limit", "=", "int", "(", "generated", ".", "size", "(", "0", ")", "*", "limit_percent", ")", "\n", "filter_len", "=", "min", "(", "lengths", ".", "min", "(", ")", ",", "filter_len_limit", ")", "-", "1", "\n", "assert", "filter_len", ">", "1", ",", "'len {}'", ".", "format", "(", "filter_len", ")", "\n", "fil_generated", "=", "generated", "[", ":", "filter_len", "]", "\n", "if", "(", "fil_generated", "[", "1", ":", "]", "==", "self", ".", "eos_index", ")", ".", "any", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'Out: min={} , {} {} / {}'", ".", "format", "(", "lengths", ".", "min", "(", ")", ",", "filter_len", ",", "fil_generated", ".", "size", "(", ")", ",", "lengths", ".", "size", "(", ")", ")", ")", "\n", "logger", ".", "info", "(", "'Gen: {}'", ".", "format", "(", "fil_generated", ")", ")", "\n", "", "generated", ",", "lengths", "=", "generate_beam_from_prefix", "(", "\n", "self", ",", "fil_generated", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "\n", "early_stopping", ",", "max_len", ",", "nbest", "\n", ")", "\n", "assert", "generated", ".", "dim", "(", ")", "==", "3", "\n", "# logger.info('run from_prefix generated {}'.format(generated.size()))", "\n", "", "elif", "nbest", "is", "not", "None", "and", "nbest", ">=", "1", "and", "limit_beam_size", "==", "1", ":", "\n", "# _len2, _nbest, bsz = mbeam_x2.size()", "\n", "            ", "_len", ",", "_bsz", "=", "generated", ".", "size", "(", ")", "\n", "# logger.info('{}, {}, nbest {}'.format(_len, _bsz, nbest))", "\n", "generated", "=", "generated", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "_len", ",", "nbest", ",", "_bsz", ")", ".", "contiguous", "(", ")", "\n", "lengths", "=", "lengths", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "_bsz", ",", "nbest", ")", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"nbest={}, lenmin={}, limitbeam={}\"", ".", "format", "(", "\n", "nbest", ",", "lengths", ".", "min", "(", ")", ",", "limit_beam_size", "\n", ")", ")", "\n", "\n", "# _len2, _nbest, bsz = mbeam_x2.size()", "\n", "", "assert", "generated", ".", "dim", "(", ")", "==", "3", "\n", "assert", "lengths", ".", "dim", "(", ")", "==", "2", "\n", "\n", "# logger.info('generated {}'.format(generated.size()))", "\n", "return", "generated", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_stochastic_beam": [[1277, 1454], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new.fill_", "generated[].fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.new.view", "beam_search.compute_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "beam_search.BeamHypotheses", "transformer.TransformerModel.forward", "transformer.TransformerModel.pred_layer.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.clone", "_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.log_softmax.view().gather", "torch.log_softmax.view().gather", "range", "beam_scores.new.new.new", "src_len.unsqueeze().expand().contiguous().view.new.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.log_softmax.new", "torch.log_softmax.new", "cache.keys", "all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "range", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "transformer.TransformerModel.size", "torch.log_softmax.size", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.add_", "torch.log_softmax.add_", "gumbel.gumbel.gumbel_with_maximum", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "next_scores.size", "F.log_softmax.view().gather.size", "zip", "next_batch_beam.extend", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "gumbel.gumbel.gumbel_like", "log_ps_t[].unsqueeze", "log_ps[].unsqueeze", "torch.log_softmax.view", "torch.log_softmax.view", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "generated[].clone", "value.item", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_with_maximum", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_like", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_stochastic_beam", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temp", "=", "1.0", ",", "sample_topk", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        From stochastic beam search paper\n        http://proceedings.mlr.press/v97/kool19a/kool19a.pdf\n        https://github.com/wouterkool/stochastic-beam-search\n        \"\"\"", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# todo: stochastic beam search variables", "\n", "log_ps", "=", "src_enc", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "log_ps_t", "=", "src_enc", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "cand_scores_buf", "=", "src_enc", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "            ", "step", "=", "cur_len", "-", "1", "\n", "# compute word scores", "\n", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "logits", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "# scores is lprobs", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "lprobs", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# todo: stochastic beam", "\n", "lprobs_t", "=", "lprobs", ".", "clone", "(", ")", "\n", "if", "sample_temp", "!=", "1.0", ":", "\n", "                ", "lprobs_t", "=", "F", ".", "log_softmax", "(", "logits", "/", "sample_temp", ",", "-", "1", ")", "\n", "\n", "", "if", "cur_len", "==", "1", ":", "\n", "                ", "cand_scores", "=", "gumbel_like", "(", "lprobs_t", ")", "+", "lprobs_t", "\n", "", "else", ":", "\n", "                ", "lprobs_t", ".", "add_", "(", "log_ps_t", "[", "cur_len", "-", "2", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "lprobs", ".", "add_", "(", "log_ps", "[", "cur_len", "-", "2", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "# logger.info('gumbel_max: {} / {} / {}'.format(cur_len, lprobs_t.size(), cand_scores_buf.size()))", "\n", "cand_scores", ",", "_", "=", "gumbel_with_maximum", "(", "lprobs_t", ",", "cand_scores_buf", "[", "cur_len", "-", "2", "]", ",", "-", "1", ")", "\n", "\n", "# select next words with scores", "\n", "", "_scores", "=", "cand_scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "lprobs", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "next_lprobs", "=", "lprobs", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", ".", "gather", "(", "-", "1", ",", "next_words", ")", "\n", "next_lprobs_t", "=", "lprobs_t", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", ".", "gather", "(", "-", "1", ",", "next_words", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_lprobs", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "# next_batch_beam.extend([(0, self.pad_index, 0)] * beam_size)  # pad the batch", "\n", "# fixme: next_batch_beam change 3 places", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ",", "0", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", ",", "lprob", ",", "lprob_t", "in", "zip", "(", "\n", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ",", "next_lprobs", "[", "sent_id", "]", ",", "next_lprobs_t", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                        ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "\n", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "value", ".", "item", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "\n", "# (value, word_id, sent_id * beam_size + beam_id)", "\n", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ",", "lprob", ",", "lprob_t", ")", "\n", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "# next_sent_beam = [(0, self.pad_index, 0)] * beam_size  # pad the batch", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ",", "0", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_lprob", "=", "lprobs", ".", "new", "(", "[", "x", "[", "3", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_lprob_t", "=", "lprobs_t", ".", "new", "(", "[", "x", "[", "4", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "cand_scores_buf", "=", "cand_scores_buf", "[", ":", ",", "beam_idx", "]", "\n", "cand_scores_buf", "[", "cur_len", "-", "1", "]", "=", "beam_scores", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "                ", "if", "k", "!=", "'slen'", ":", "\n", "                    ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "", "", "log_ps", "=", "log_ps", "[", ":", ",", "beam_idx", "]", "\n", "log_ps", "[", "cur_len", "-", "1", "]", "=", "beam_lprob", "\n", "log_ps_t", "=", "log_ps_t", "[", ":", ",", "beam_idx", "]", "\n", "log_ps_t", "[", "cur_len", "-", "1", "]", "=", "beam_lprob_t", "\n", "\n", "# update current length", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "compute_final_decoded", "(", "generated_hyps", ",", "bs", ",", "src_len", ",", "self", ".", "pad_index", ",", "self", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.MultiLangTransformerModel.__init__": [[1461, 1585], ["transformer.TransformerModel.__init__", "getattr", "getattr", "getattr", "transformer.Embedding", "transformer.Embedding", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "getattr", "logger.info", "range", "len", "len", "len", "transformer.create_sinusoidal_embeddings", "transformer.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.MultiLangTransformerModel.attentions.append", "transformer.MultiLangTransformerModel.layer_norm1.append", "transformer.MultiLangTransformerModel.layer_norm2.append", "range", "transformer.PredLayer", "memory.HashingMemory.build", "logger.info", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.MultiLangTransformerModel.layer_norm15.append", "transformer.MultiLangTransformerModel.encoder_attn.append", "transformer.MultiLangTransformerModel.ffns.append", "transformer.MultiLangTransformerModel.ffns.append", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.MultiLangTransformerModel.attentions[].append", "transformer.MultiLangTransformerModel.layer_norm1[].append", "transformer.MultiLangTransformerModel.ffns[].append", "transformer.MultiLangTransformerModel.layer_norm2[].append", "transformer.MultiLangTransformerModel.attentions[].append", "transformer.MultiLangTransformerModel.layer_norm1[].append", "transformer.MultiLangTransformerModel.layer_norm2[].append", "transformer.MultiHeadAttention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.MultiLangTransformerModel.layer_norm15[].append", "transformer.MultiLangTransformerModel.encoder_attn[].append", "transformer.MultiHeadAttention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.MultiLangTransformerModel.layer_norm15[].append", "transformer.MultiLangTransformerModel.encoder_attn[].append", "transformer.MultiLangTransformerModel.ffns[].append", "transformer.MultiLangTransformerModel.ffns[].append", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.MultiHeadAttention", "transformer.TransformerFFN", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer.MultiHeadAttention", "transformer.TransformerFFN"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.create_sinusoidal_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.build"], ["def", "__init__", "(", "self", ",", "params", ",", "dico", ",", "is_encoder", ",", "with_output", ")", ":", "\n", "        ", "\"\"\"\n        Transformer model (encoder or decoder).\n        \"\"\"", "\n", "# nn.Module.__init__(self)", "\n", "super", "(", "TransformerModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# encoder / decoder, output layer", "\n", "self", ".", "is_encoder", "=", "is_encoder", "\n", "self", ".", "is_decoder", "=", "not", "is_encoder", "\n", "self", ".", "with_output", "=", "with_output", "\n", "\n", "# dictionary / languages", "\n", "self", ".", "n_langs", "=", "params", ".", "n_langs", "\n", "self", ".", "n_words", "=", "params", ".", "n_words", "\n", "self", ".", "eos_index", "=", "params", ".", "eos_index", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "self", ".", "dico", "=", "dico", "\n", "self", ".", "id2lang", "=", "params", ".", "id2lang", "\n", "self", ".", "lang2id", "=", "params", ".", "lang2id", "\n", "self", ".", "use_lang_emb", "=", "getattr", "(", "params", ",", "'use_lang_emb'", ",", "True", ")", "\n", "\n", "self", ".", "share_enc", "=", "getattr", "(", "params", ",", "'share_enc'", ",", "-", "1", ")", "\n", "self", ".", "share_dec", "=", "getattr", "(", "params", ",", "'share_dec'", ",", "-", "1", ")", "\n", "self", ".", "share_enc", "=", "self", ".", "share_enc", "if", "self", ".", "share_enc", ">", "-", "1", "else", "self", ".", "n_layers", "\n", "self", ".", "share_dec", "=", "self", ".", "share_dec", "if", "self", ".", "share_dec", ">", "-", "1", "else", "self", ".", "n_layers", "\n", "\n", "assert", "len", "(", "self", ".", "dico", ")", "==", "self", ".", "n_words", "\n", "assert", "len", "(", "self", ".", "id2lang", ")", "==", "len", "(", "self", ".", "lang2id", ")", "==", "self", ".", "n_langs", "\n", "\n", "# model parameters", "\n", "self", ".", "dim", "=", "params", ".", "emb_dim", "# 512 by default", "\n", "self", ".", "hidden_dim", "=", "self", ".", "dim", "*", "4", "# 2048 by default", "\n", "self", ".", "n_heads", "=", "params", ".", "n_heads", "# 8 by default", "\n", "self", ".", "n_layers", "=", "params", ".", "n_layers", "\n", "self", ".", "dropout", "=", "params", ".", "dropout", "\n", "self", ".", "attention_dropout", "=", "params", ".", "attention_dropout", "\n", "assert", "self", ".", "dim", "%", "self", ".", "n_heads", "==", "0", ",", "'transformer dim must be a multiple of n_heads'", "\n", "\n", "# embeddings", "\n", "self", ".", "position_embeddings", "=", "Embedding", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ")", "\n", "if", "params", ".", "sinusoidal_embeddings", ":", "\n", "            ", "create_sinusoidal_embeddings", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ",", "out", "=", "self", ".", "position_embeddings", ".", "weight", ")", "\n", "", "if", "params", ".", "n_langs", ">", "1", "and", "self", ".", "use_lang_emb", ":", "\n", "            ", "self", ".", "lang_embeddings", "=", "Embedding", "(", "self", ".", "n_langs", ",", "self", ".", "dim", ")", "\n", "", "self", ".", "embeddings", "=", "Embedding", "(", "self", ".", "n_words", ",", "self", ".", "dim", ",", "padding_idx", "=", "self", ".", "pad_index", ")", "\n", "self", ".", "layer_norm_emb", "=", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "\n", "\n", "# transformer layers", "\n", "self", ".", "attentions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm1", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "ffns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm2", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "self", ".", "layer_norm15", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "encoder_attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# memories", "\n", "", "self", ".", "memories", "=", "nn", ".", "ModuleDict", "(", ")", "\n", "if", "getattr", "(", "params", ",", "'use_memory'", ",", "False", ")", ":", "\n", "            ", "mem_positions", "=", "params", ".", "mem_enc_positions", "if", "is_encoder", "else", "params", ".", "mem_dec_positions", "\n", "for", "layer_id", ",", "pos", "in", "mem_positions", ":", "\n", "                ", "assert", "0", "<=", "layer_id", "<=", "params", ".", "n_layers", "-", "1", "\n", "assert", "pos", "in", "[", "'in'", ",", "'after'", "]", "\n", "self", ".", "memories", "[", "'%i_%s'", "%", "(", "layer_id", ",", "pos", ")", "]", "=", "HashingMemory", ".", "build", "(", "self", ".", "dim", ",", "self", ".", "dim", ",", "params", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "'Build layers: {}  (enc={},{})'", ".", "format", "(", "self", ".", "n_layers", ",", "self", ".", "share_enc", ",", "self", ".", "share_dec", ")", ")", "\n", "for", "layer_id", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "            ", "layer_is_shared", "=", "(", "layer_id", ">=", "(", "self", ".", "n_layers", "-", "self", ".", "share_enc", ")", ")", "\n", "if", "layer_is_shared", ":", "\n", "                ", "logger", ".", "info", "(", "\"Sharing parameters for layer {}\"", ".", "format", "(", "layer_id", ")", ")", "\n", "\n", "", "self", ".", "attentions", ".", "append", "(", "\n", "nn", ".", "ModuleList", "(", "[", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", "]", ")", "\n", ")", "\n", "self", ".", "layer_norm1", ".", "append", "(", "nn", ".", "ModuleList", "(", "[", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "]", ")", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                ", "self", ".", "layer_norm15", ".", "append", "(", "\n", "nn", ".", "ModuleList", "(", "[", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "]", ")", ")", "\n", "self", ".", "encoder_attn", ".", "append", "(", "\n", "nn", ".", "ModuleList", "(", "[", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", "]", ")", ")", "\n", "", "if", "(", "'%i_in'", "%", "layer_id", ")", "in", "self", ".", "memories", ":", "\n", "                ", "self", ".", "ffns", ".", "append", "(", "[", "None", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "ffns", ".", "append", "(", "\n", "nn", ".", "ModuleList", "(", "[", "TransformerFFN", "(", "self", ".", "dim", ",", "self", ".", "hidden_dim", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "dropout", ",", "\n", "gelu_activation", "=", "params", ".", "gelu_activation", ")", "]", ")", ")", "\n", "", "self", ".", "layer_norm2", ".", "append", "(", "nn", ".", "ModuleList", "(", "[", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "]", ")", ")", "\n", "\n", "# add to other langs", "\n", "# self.n_langs", "\n", "for", "i", "in", "range", "(", "1", ",", "self", ".", "n_langs", ")", ":", "\n", "# layer for lang i", "\n", "                ", "if", "layer_is_shared", ":", "\n", "# share layer from lang 0", "\n", "                    ", "self", ".", "attentions", "[", "layer_id", "]", ".", "append", "(", "self", ".", "attentions", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "self", ".", "layer_norm1", "[", "layer_id", "]", ".", "append", "(", "self", ".", "layer_norm1", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                        ", "self", ".", "layer_norm15", "[", "layer_id", "]", ".", "append", "(", "self", ".", "layer_norm15", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "self", ".", "encoder_attn", "[", "layer_id", "]", ".", "append", "(", "self", ".", "encoder_attn", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "", "self", ".", "ffns", "[", "layer_id", "]", ".", "append", "(", "self", ".", "ffns", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "self", ".", "layer_norm2", "[", "layer_id", "]", ".", "append", "(", "self", ".", "layer_norm2", "[", "layer_id", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "# self.layers[layer_id].append(TransformerEncoderLayer(args))", "\n", "                    ", "self", ".", "attentions", "[", "layer_id", "]", ".", "append", "(", "\n", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", "\n", ")", "\n", "self", ".", "layer_norm1", "[", "layer_id", "]", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                        ", "self", ".", "layer_norm15", "[", "layer_id", "]", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "self", ".", "encoder_attn", "[", "layer_id", "]", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", ")", "\n", "", "if", "(", "'%i_in'", "%", "layer_id", ")", "in", "self", ".", "memories", ":", "\n", "                        ", "self", ".", "ffns", "[", "layer_id", "]", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "ffns", "[", "layer_id", "]", ".", "append", "(", "\n", "TransformerFFN", "(", "self", ".", "dim", ",", "self", ".", "hidden_dim", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "dropout", ",", "\n", "gelu_activation", "=", "params", ".", "gelu_activation", ")", ")", "\n", "", "self", ".", "layer_norm2", "[", "layer_id", "]", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "\n", "# output layer", "\n", "", "", "", "if", "self", ".", "with_output", ":", "\n", "            ", "self", ".", "pred_layer", "=", "PredLayer", "(", "params", ")", "\n", "if", "params", ".", "share_inout_emb", ":", "\n", "                ", "self", ".", "pred_layer", ".", "proj", ".", "weight", "=", "self", ".", "embeddings", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.MultiLangTransformerModel.forward": [[1586, 1597], ["transformer.MultiLangTransformerModel.fwd", "transformer.MultiLangTransformerModel.predict", "Exception"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.predict"], ["", "", "", "def", "forward", "(", "self", ",", "mode", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Forward function with different forward modes.\n        ### Small hack to handle PyTorch distributed.\n        \"\"\"", "\n", "if", "mode", "==", "'fwd'", ":", "\n", "            ", "return", "self", ".", "fwd", "(", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "'predict'", ":", "\n", "            ", "return", "self", ".", "predict", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown mode: %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.MultiLangTransformerModel.fwd": [[1598, 1701], ["x.transpose.transpose.size", "x.transpose.transpose.transpose", "transformer.get_masks", "transformer.MultiLangTransformerModel.embeddings", "transformer.MultiLangTransformerModel.layer_norm_emb", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "range", "tensor.transpose.transpose.transpose", "lengths.size", "lengths.max().item", "x.transpose.transpose.new().long", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "positions.transpose.transpose.transpose", "langs.transpose.transpose.transpose", "int", "transformer.MultiLangTransformerModel.position_embeddings().expand_as", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "tensor.transpose.transpose.size", "src_enc.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "positions.transpose.transpose.size", "langs.transpose.transpose.size", "langs[].item", "transformer.MultiLangTransformerModel.lang_embeddings", "mask.unsqueeze", "torch.dropout", "torch.dropout", "torch.dropout", "lengths.max", "src_len.max", "x.transpose.transpose.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "transformer.MultiLangTransformerModel.position_embeddings", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.get_masks"], ["", "", "def", "fwd", "(", "self", ",", "x", ",", "lengths", ",", "causal", ",", "src_enc", "=", "None", ",", "src_len", "=", "None", ",", "positions", "=", "None", ",", "langs", "=", "None", ",", "cache", "=", "None", ",", "enc_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            `x` LongTensor(slen, bs), containing word indices\n            `lengths` LongTensor(bs), containing the length of each sentence\n            `causal` Boolean, if True, the attention is only done over previous hidden states\n            `positions` LongTensor(slen, bs), containing word positions\n            `langs` LongTensor(slen, bs), containing language IDs\n        \"\"\"", "\n", "# lengths = (x != self.pad_index).float().sum(dim=1)", "\n", "# mask = x != self.pad_index", "\n", "\n", "# check inputs", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "assert", "lengths", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "<=", "slen", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# batch size as dimension 0", "\n", "assert", "(", "src_enc", "is", "None", ")", "==", "(", "src_len", "is", "None", ")", "\n", "if", "src_enc", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "is_decoder", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generate masks", "\n", "", "mask", ",", "attn_mask", "=", "get_masks", "(", "slen", ",", "lengths", ",", "causal", ")", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "            ", "src_mask", "=", "torch", ".", "arange", "(", "src_len", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "<", "src_len", "[", ":", ",", "None", "]", "\n", "if", "enc_mask", "is", "not", "None", ":", "\n", "                ", "src_mask", "&=", "enc_mask", "\n", "\n", "# positions", "\n", "", "", "if", "positions", "is", "None", ":", "\n", "            ", "positions", "=", "x", ".", "new", "(", "slen", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "slen", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "assert", "positions", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "positions", "=", "positions", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# langs", "\n", "", "if", "langs", "is", "not", "None", ":", "\n", "            ", "assert", "langs", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "langs", "=", "langs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "lang_id", "=", "int", "(", "langs", "[", "0", ",", "0", "]", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "lang_id", "=", "0", "\n", "\n", "# do not recompute cached elements", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "_slen", "=", "slen", "-", "cache", "[", "'slen'", "]", "\n", "x", "=", "x", "[", ":", ",", "-", "_slen", ":", "]", "\n", "positions", "=", "positions", "[", ":", ",", "-", "_slen", ":", "]", "\n", "if", "langs", "is", "not", "None", ":", "\n", "                ", "langs", "=", "langs", "[", ":", ",", "-", "_slen", ":", "]", "\n", "", "mask", "=", "mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "attn_mask", "=", "attn_mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "\n", "# embeddings", "\n", "", "tensor", "=", "self", ".", "embeddings", "(", "x", ")", "\n", "tensor", "=", "tensor", "+", "self", ".", "position_embeddings", "(", "positions", ")", ".", "expand_as", "(", "tensor", ")", "\n", "if", "langs", "is", "not", "None", "and", "self", ".", "use_lang_emb", ":", "\n", "            ", "tensor", "=", "tensor", "+", "self", ".", "lang_embeddings", "(", "langs", ")", "\n", "", "tensor", "=", "self", ".", "layer_norm_emb", "(", "tensor", ")", "\n", "tensor", "=", "F", ".", "dropout", "(", "tensor", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "# transformer layers", "\n", "for", "i", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "\n", "# self attention", "\n", "            ", "attn", "=", "self", ".", "attentions", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ",", "attn_mask", ",", "cache", "=", "cache", ")", "\n", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm1", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ")", "\n", "\n", "# encoder attention (for decoder only)", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "                ", "attn", "=", "self", ".", "encoder_attn", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ",", "src_mask", ",", "kv", "=", "src_enc", ",", "cache", "=", "cache", ")", "\n", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm15", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ")", "\n", "\n", "# FFN", "\n", "", "if", "(", "'%i_in'", "%", "i", ")", "in", "self", ".", "memories", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "memories", "[", "'%i_in'", "%", "i", "]", "(", "tensor", ")", "\n", "", "else", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "ffns", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ")", "\n", "", "tensor", "=", "self", ".", "layer_norm2", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ")", "\n", "\n", "# memory", "\n", "if", "(", "'%i_after'", "%", "i", ")", "in", "self", ".", "memories", ":", "\n", "                ", "tensor", "=", "tensor", "+", "self", ".", "memories", "[", "'%i_after'", "%", "i", "]", "(", "tensor", ")", "\n", "# TODO: add extra layer norm here?", "\n", "\n", "", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "# update cache length", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "cache", "[", "'slen'", "]", "+=", "tensor", ".", "size", "(", "1", ")", "\n", "\n", "# move back sequence length to dimension 0", "\n", "", "tensor", "=", "tensor", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.Embedding": [[49, 55], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding"], ["def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "None", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "if", "padding_idx", "is", "not", "None", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.Linear": [[57, 63], ["torch.Linear"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "# nn.init.normal_(m.weight, mean=0, std=1)", "\n", "# nn.init.xavier_uniform_(m.weight)", "\n", "# nn.init.constant_(m.bias, 0.)", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.create_sinusoidal_embeddings": [[65, 74], ["numpy.array", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "out.detach_", "numpy.sin", "numpy.cos", "range", "float", "numpy.power", "range"], "function", ["None"], ["", "def", "create_sinusoidal_embeddings", "(", "n_pos", ",", "dim", ",", "out", ")", ":", "\n", "    ", "position_enc", "=", "np", ".", "array", "(", "[", "\n", "[", "float", "(", "pos", ")", "/", "np", ".", "power", "(", "10000", ",", "2", "*", "(", "j", "//", "2", ")", "/", "dim", ")", "for", "j", "in", "range", "(", "dim", ")", "]", "\n", "for", "pos", "in", "range", "(", "n_pos", ")", "\n", "]", ")", "\n", "out", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "FloatTensor", "(", "np", ".", "sin", "(", "position_enc", "[", ":", ",", "0", ":", ":", "2", "]", ")", ")", "\n", "out", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "FloatTensor", "(", "np", ".", "cos", "(", "position_enc", "[", ":", ",", "1", ":", ":", "2", "]", ")", ")", "\n", "out", ".", "detach_", "(", ")", "\n", "out", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.gelu": [[76, 85], ["torch.erf", "torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    GELU activation\n    https://arxiv.org/abs/1606.08415\n    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n    https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/modeling.py\n    \"\"\"", "\n", "# return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))", "\n", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.get_masks": [[87, 107], ["lengths.size", "torch.arange", "torch.arange", "torch.arange", "lengths.max().item", "mask.size", "alen[].repeat", "attn_mask.size", "lengths.max"], "function", ["None"], ["", "def", "get_masks", "(", "slen", ",", "lengths", ",", "causal", ")", ":", "\n", "    ", "\"\"\"\n    Generate hidden states mask, and optionally an attention mask.\n    \"\"\"", "\n", "assert", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "<=", "slen", "\n", "bs", "=", "lengths", ".", "size", "(", "0", ")", "\n", "alen", "=", "torch", ".", "arange", "(", "slen", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "\n", "mask", "=", "alen", "<", "lengths", "[", ":", ",", "None", "]", "\n", "\n", "# attention mask is the same as mask, or triangular inferior attention (causal)", "\n", "if", "causal", ":", "\n", "        ", "attn_mask", "=", "alen", "[", "None", ",", "None", ",", ":", "]", ".", "repeat", "(", "bs", ",", "slen", ",", "1", ")", "<=", "alen", "[", "None", ",", ":", ",", "None", "]", "\n", "", "else", ":", "\n", "        ", "attn_mask", "=", "mask", "\n", "\n", "# sanity check", "\n", "", "assert", "mask", ".", "size", "(", ")", "==", "(", "bs", ",", "slen", ")", "\n", "assert", "causal", "is", "False", "or", "attn_mask", ".", "size", "(", ")", "==", "(", "bs", ",", "slen", ",", "slen", ")", "\n", "\n", "return", "mask", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.generate_beam_from_prefix": [[273, 429], ["prefix.transpose().contiguous().view.dim", "len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "beam_search.compute_final_decoded", "prefix.transpose().contiguous().view.size", "prefix.transpose().contiguous().view.unsqueeze().expand().contiguous().view", "prefix.transpose().contiguous().view.size", "prefix.transpose().contiguous().view.transpose().contiguous().view", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "prefix.transpose().contiguous().view.size", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "beam_search.BeamHypotheses", "model.forward", "model.pred_layer.get_scores", "torch.log_softmax", "_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "range", "beam_scores.new.new", "src_len.new.new", "src_len.unsqueeze().expand().contiguous().view.new", "cache.keys", "all", "prefix.transpose().contiguous().view.dim", "prefix.transpose().contiguous().view.size", "prefix.transpose().contiguous().view.size", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "range", "model.forward.size", "model.forward.size", "F.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "zip", "next_batch_beam.extend", "len", "prefix.transpose().contiguous().view.unsqueeze().expand().contiguous", "prefix.transpose().contiguous().view.transpose().contiguous", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "prefix.transpose().contiguous().view.unsqueeze().expand", "prefix.transpose().contiguous().view.transpose", "src_len.unsqueeze().expand().contiguous().view.new", "generated[].clone", "value.item", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max", "prefix.transpose().contiguous().view.unsqueeze"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "", "def", "generate_beam_from_prefix", "(", "\n", "model", ",", "prefix", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "max_len", "=", "200", ",", "nbest", "=", "None", ")", ":", "\n", "# k is nbest", "\n", "# prefix, prefix_len: either ([t, b], [b]) or ([t, k, b], [k, b])", "\n", "# expect prefix_len is exactly the same len as prefix", "\n", "# fixme: this may cause problem for sentence with small length (=1 or 2)", "\n", "#       Consider rejecting prefix that is too short", "\n", "\n", "    ", "assert", "not", "(", "prefix", "[", "1", ":", "]", "==", "model", ".", "eos_index", ")", ".", "any", "(", ")", "\n", "# assert prefix.dim() == 2", "\n", "# extend to beam size", "\n", "prefix_dim", "=", "prefix", ".", "dim", "(", ")", "\n", "if", "prefix_dim", "==", "2", ":", "\n", "        ", "pref_len", ",", "pref_bs", "=", "prefix", ".", "size", "(", ")", "\n", "prefix", "=", "prefix", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "pref_len", ",", "pref_bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "pref_len", ",", "pref_bs", "*", "beam_size", ")", "\n", "", "else", ":", "\n", "        ", "assert", "prefix", ".", "dim", "(", ")", "==", "3", ",", "'prefix: {}'", ".", "format", "(", "prefix", ".", "size", "(", ")", ")", "\n", "pref_len", ",", "_nb", ",", "pref_bs", "=", "prefix", ".", "size", "(", ")", "\n", "assert", "_nb", "==", "beam_size", ",", "'{} != {}, prefix {}'", ".", "format", "(", "_nb", ",", "beam_size", ",", "prefix", ".", "size", "(", ")", ")", "\n", "prefix", "=", "prefix", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "pref_len", ",", "pref_bs", "*", "beam_size", ")", "\n", "\n", "# check inputs", "\n", "", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "model", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "assert", "pref_bs", "==", "bs", ",", "'{} != {},prefdim {},prefix={}, src_enc={}, src_len={}'", ".", "format", "(", "\n", "pref_bs", ",", "bs", ",", "prefix_dim", ",", "prefix", ".", "size", "(", ")", ",", "src_enc", ".", "size", "(", ")", ",", "src_len", ".", "size", "(", ")", ")", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "model", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "model", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "generated", "[", ":", "pref_len", ",", ":", "]", "=", "prefix", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "pref_len", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "model", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "if", "tensor", ".", "size", "(", ")", "!=", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", ":", "\n", "            ", "tensor", "=", "tensor", "[", "-", "1", ":", "]", "\n", "", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "model", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "model", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "            ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "model", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "model", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                    ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                    ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                ", "next_sent_beam", "=", "[", "(", "0", ",", "model", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "compute_final_decoded", "(", "generated_hyps", ",", "bs", ",", "src_len", ",", "model", ".", "pad_index", ",", "model", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.diverse_beam_search.generate_diverse_beam": [[13, 170], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.new", "src_len.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.view", "beam_search.compute_final_decoded", "src_enc.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.size", "beam_search.BeamHypotheses", "diverse_beam_search..forward", "diverse_beam_search..pred_layer.get_scores", "torch.log_softmax", "_scores.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "range", "beam_scores.new.new", "src_len.new.new", "src_len.unsqueeze().expand().contiguous().view.new", "cache.keys", "all", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "diverse_beam_search..size", "F.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "zip", "next_batch_beam.extend", "len", "src_len.unsqueeze().expand().contiguous().view.new().fill_", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "src_len.unsqueeze().expand().contiguous().view.new", "generated[].clone", "value.item", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.beam_search.compute_final_decoded", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["def", "generate_diverse_beam", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "num_groups", ",", "diversity_strength", ",", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Decode a sentence given initial start.\n    `x`:\n        - LongTensor(bs, slen)\n            <EOS> W1 W2 W3 <EOS> <PAD>\n            <EOS> W1 W2 W3   W4  <EOS>\n    `lengths`:\n        - LongTensor(bs) [5, 6]\n    `positions`:\n        - False, for regular \"arange\" positions (LM)\n        - True, to reset positions from the new generation (MT)\n    `langs`:\n        - must be None if the model only supports one language\n        - lang_id if only one language is involved (LM)\n        - (lang_id1, lang_id2) if two languages are involved (MT)\n    \"\"\"", "\n", "\n", "# check inputs", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "assert", "beam_size", "%", "num_groups", "==", "0", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "        ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "assert", "sample_temperature", "is", "None", "or", "sample_temperature", "==", "1.0", ",", "'sample_temperature={} not support'", ".", "format", "(", "\n", "sample_temperature", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "            ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                    ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "\n", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                    ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", "!=", "'slen'", ":", "\n", "                ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "compute_final_decoded", "(", "generated_hyps", ",", "bs", ",", "src_len", ",", "self", ".", "pad_index", ",", "self", ".", "eos_index", ",", "beam_size", ",", "nbest", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_like": [[11, 13], ["gumbel._gumbel", "torch.rand_like", "torch.rand_like"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._gumbel"], ["def", "gumbel_like", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_gumbel", "(", "torch", ".", "rand_like", "(", "*", "args", ",", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel": [[15, 17], ["gumbel._gumbel", "torch.rand", "torch.rand"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._gumbel"], ["", "def", "gumbel", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "_gumbel", "(", "torch", ".", "rand", "(", "*", "args", ",", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._gumbel": [[19, 21], ["torch.log", "torch.log", "torch.log", "torch.log"], "function", ["None"], ["", "def", "_gumbel", "(", "u", ")", ":", "\n", "    ", "return", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "u", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_log_survival": [[23, 30], ["torch.exp", "torch.exp", "torch.where", "torch.where", "torch.log", "torch.log", "torch.expm1", "torch.expm1", "torch.exp", "torch.exp"], "function", ["None"], ["", "def", "gumbel_log_survival", "(", "x", ")", ":", "\n", "    ", "\"\"\"Computes log P(g > x) = log(1 - P(g < x)) = log(1 - exp(-exp(-x))) for a standard Gumbel\"\"\"", "\n", "y", "=", "torch", ".", "exp", "(", "-", "x", ")", "\n", "return", "torch", ".", "where", "(", "\n", "x", ">=", "10", ",", "# means that y < 1e-4 so O(y^6) <= 1e-24 so we can use series expansion", "\n", "-", "x", "-", "y", "/", "2", "+", "y", "**", "2", "/", "24", "-", "y", "**", "4", "/", "2880", ",", "# + O(y^6), https://www.wolframalpha.com/input/?i=log(1+-+exp(-y))", "\n", "torch", ".", "log", "(", "-", "torch", ".", "expm1", "(", "-", "torch", ".", "exp", "(", "-", "x", ")", ")", ")", "# Hope for the best", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_with_maximum": [[33, 47], ["g_phi.max", "gumbel._shift_gumbel_maximum", "gumbel.gumbel_like", "gumbel._shift_gumbel_maximum"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._shift_gumbel_maximum", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel.gumbel_like", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._shift_gumbel_maximum"], ["", "def", "gumbel_with_maximum", "(", "phi", ",", "T", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n    Samples a set of gumbels which are conditioned on having a maximum along a dimension\n    phi.max(dim)[0] should be broadcastable with the desired maximum T\n    \"\"\"", "\n", "# Gumbel with location phi", "\n", "g_phi", "=", "phi", "+", "gumbel_like", "(", "phi", ")", "\n", "Z", ",", "argmax", "=", "g_phi", ".", "max", "(", "dim", ")", "\n", "g", "=", "_shift_gumbel_maximum", "(", "g_phi", ",", "T", ",", "dim", ",", "Z", "=", "Z", ")", "\n", "CHECK_VALIDITY", "=", "False", "\n", "if", "CHECK_VALIDITY", ":", "\n", "        ", "g_inv", "=", "_shift_gumbel_maximum", "(", "g", ",", "Z", ",", "dim", ")", "\n", "assert", "(", "(", "(", "g_phi", "-", "g_inv", ")", "<", "1e-3", ")", "|", "(", "g_phi", "==", "g_inv", ")", ")", ".", "all", "(", ")", "\n", "", "return", "g", ",", "argmax", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.gumbel._shift_gumbel_maximum": [[49, 54], ["g_phi.max", "torch.log1p", "torch.log1p", "torch.log1p", "torch.log1p", "T.unsqueeze", "T.unsqueeze", "torch.relu", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "u.abs", "Z.unsqueeze"], "function", ["None"], ["", "def", "_shift_gumbel_maximum", "(", "g_phi", ",", "T", ",", "dim", "=", "-", "1", ",", "Z", "=", "None", ")", ":", "\n", "    ", "if", "Z", "is", "None", ":", "\n", "        ", "Z", ",", "_", "=", "g_phi", ".", "max", "(", "dim", ")", "\n", "", "u", "=", "T", ".", "unsqueeze", "(", "dim", ")", "-", "g_phi", "+", "torch", ".", "log1p", "(", "-", "torch", ".", "exp", "(", "g_phi", "-", "Z", ".", "unsqueeze", "(", "dim", ")", ")", ")", "\n", "return", "T", ".", "unsqueeze", "(", "dim", ")", "-", "F", ".", "relu", "(", "u", ")", "-", "torch", ".", "log1p", "(", "torch", ".", "exp", "(", "-", "u", ".", "abs", "(", ")", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.GroupedLinear.__init__": [[61, 71], ["torch.nn.Module.__init__", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "groups", "=", "1", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "out_features", "=", "out_features", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "assert", "groups", ">", "1", "\n", "\n", "self", ".", "layer", "=", "nn", ".", "Conv1d", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ",", "kernel_size", "=", "1", ",", "groups", "=", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.GroupedLinear.forward": [[72, 75], ["query.GroupedLinear.layer().squeeze", "input.dim", "input.size", "query.GroupedLinear.layer", "input.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "assert", "input", ".", "dim", "(", ")", "==", "2", "and", "input", ".", "size", "(", "1", ")", "==", "self", ".", "in_features", "\n", "return", "self", ".", "layer", "(", "input", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.GroupedLinear.extra_repr": [[76, 79], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'in_features={}, out_features={}, groups={}, bias={}'", ".", "format", "(", "\n", "self", ".", "in_features", ",", "self", ".", "out_features", ",", "self", ".", "groups", ",", "self", ".", "bias", "is", "not", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.BottleneckResidualConv2d.__init__": [[84, 103], ["torch.nn.Module.__init__", "min", "all", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.Sequential", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "bias", "=", "True", ",", "batchnorm", "=", "True", ",", "groups", "=", "1", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "hidden_channels", "=", "min", "(", "input_channels", ",", "output_channels", ")", "\n", "assert", "all", "(", "k", "%", "2", "==", "1", "for", "k", "in", "kernel_size", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "input_channels", ",", "hidden_channels", ",", "kernel_size", ",", "padding", "=", "[", "k", "//", "2", "for", "k", "in", "kernel_size", "]", ",", "bias", "=", "bias", ",", "groups", "=", "groups", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "hidden_channels", ",", "output_channels", ",", "kernel_size", ",", "padding", "=", "[", "k", "//", "2", "for", "k", "in", "kernel_size", "]", ",", "bias", "=", "bias", ",", "groups", "=", "groups", ")", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "batchnorm", "=", "batchnorm", "\n", "if", "self", ".", "batchnorm", ":", "\n", "            ", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "hidden_channels", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "output_channels", ")", "\n", "\n", "", "if", "input_channels", "==", "output_channels", ":", "\n", "            ", "self", ".", "residual", "=", "nn", ".", "Sequential", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "residual", "=", "nn", ".", "Conv2d", "(", "input_channels", ",", "output_channels", ",", "(", "1", ",", "1", ")", ",", "bias", "=", "False", ",", "groups", "=", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.BottleneckResidualConv2d.forward": [[104, 112], ["query.BottleneckResidualConv2d.conv1", "query.BottleneckResidualConv2d.act", "query.BottleneckResidualConv2d.conv2", "query.BottleneckResidualConv2d.act", "query.BottleneckResidualConv2d.bn1", "query.BottleneckResidualConv2d.bn2", "query.BottleneckResidualConv2d.residual"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "input", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "if", "self", ".", "batchnorm", "else", "x", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "bn2", "(", "x", ")", "if", "self", ".", "batchnorm", "else", "x", "\n", "x", "=", "self", ".", "act", "(", "x", "+", "self", ".", "residual", "(", "input", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryIdentity.__init__": [[116, 125], ["torch.nn.Module.__init__", "utils.get_slices", "range"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_slices"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "heads", ",", "shuffle_hidden", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "shuffle_query", "=", "shuffle_hidden", "\n", "assert", "shuffle_hidden", "is", "False", "or", "heads", ">", "1", "\n", "assert", "shuffle_hidden", "is", "False", "or", "self", ".", "input_dim", "%", "(", "2", "**", "self", ".", "heads", ")", "==", "0", "\n", "if", "shuffle_hidden", ":", "\n", "            ", "self", ".", "slices", "=", "{", "head_id", ":", "get_slices", "(", "input_dim", ",", "head_id", ")", "for", "head_id", "in", "range", "(", "heads", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryIdentity.forward": [[126, 151], ["len", "input.contiguous().view", "input.dim", "input.unsqueeze().repeat", "torch.cat().view.view", "torch.cat().view", "input.contiguous", "input.unsqueeze", "torch.cat", "range"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        Generate queries from hidden states by either\n        repeating them or creating some shuffled version.\n        \"\"\"", "\n", "assert", "input", ".", "shape", "[", "-", "1", "]", "==", "self", ".", "input_dim", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "input_dim", ")", "if", "input", ".", "dim", "(", ")", ">", "2", "else", "input", "\n", "bs", "=", "len", "(", "input", ")", "\n", "\n", "if", "self", ".", "heads", "==", "1", ":", "\n", "            ", "query", "=", "input", "\n", "\n", "", "elif", "not", "self", ".", "shuffle_query", ":", "\n", "            ", "query", "=", "input", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "heads", ",", "1", ")", "\n", "query", "=", "query", ".", "view", "(", "bs", "*", "self", ".", "heads", ",", "self", ".", "input_dim", ")", "\n", "\n", "", "else", ":", "\n", "            ", "query", "=", "torch", ".", "cat", "(", "[", "\n", "input", "[", ":", ",", "a", ":", "b", "]", "\n", "for", "head_id", "in", "range", "(", "self", ".", "heads", ")", "\n", "for", "a", ",", "b", "in", "self", ".", "slices", "[", "head_id", "]", "\n", "]", ",", "1", ")", ".", "view", "(", "bs", "*", "self", ".", "heads", ",", "self", ".", "input_dim", ")", "\n", "\n", "", "assert", "query", ".", "shape", "==", "(", "bs", "*", "self", ".", "heads", ",", "self", ".", "input_dim", ")", "\n", "return", "query", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryMLP.__init__": [[155, 184], ["torch.nn.Module.__init__", "query.mlp", "len", "len", "list", "query.mlp", "torch.nn.ModuleList", "query.mlp", "range"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.mlp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.mlp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.mlp"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_dim", ",", "heads", ",", "k_dim", ",", "product_quantization", ",", "multi_query_net", ",", "\n", "sizes", ",", "bias", "=", "True", ",", "batchnorm", "=", "True", ",", "grouped_conv", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "k_dim", "=", "k_dim", "\n", "self", ".", "sizes", "=", "sizes", "\n", "self", ".", "grouped_conv", "=", "grouped_conv", "\n", "assert", "not", "multi_query_net", "or", "product_quantization", "or", "heads", ">=", "2", "\n", "assert", "sizes", "[", "0", "]", "==", "input_dim", "\n", "assert", "sizes", "[", "-", "1", "]", "==", "(", "k_dim", "//", "2", ")", "if", "multi_query_net", "else", "(", "heads", "*", "k_dim", ")", "\n", "assert", "self", ".", "grouped_conv", "is", "False", "or", "len", "(", "sizes", ")", ">", "2", "\n", "\n", "# number of required MLPs", "\n", "self", ".", "groups", "=", "(", "2", "*", "heads", ")", "if", "multi_query_net", "else", "1", "\n", "\n", "# MLPs", "\n", "if", "self", ".", "grouped_conv", ":", "\n", "            ", "self", ".", "query_mlps", "=", "mlp", "(", "sizes", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "groups", "=", "self", ".", "groups", ")", "\n", "", "elif", "len", "(", "self", ".", "sizes", ")", "==", "2", ":", "\n", "            ", "sizes_", "=", "list", "(", "sizes", ")", "\n", "sizes_", "[", "-", "1", "]", "=", "sizes_", "[", "-", "1", "]", "*", "self", ".", "groups", "\n", "self", ".", "query_mlps", "=", "mlp", "(", "sizes_", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "groups", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "query_mlps", "=", "nn", ".", "ModuleList", "(", "[", "\n", "mlp", "(", "sizes", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "groups", "=", "1", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "groups", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryMLP.forward": [[186, 202], ["len", "query.QueryMLP.QueryMLP.view", "input.contiguous().view", "query.QueryMLP.QueryMLP.query_mlps", "input.dim", "len", "m", "torch.cat", "input.contiguous", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        Compute queries using either grouped 1D convolutions or ModuleList + concat.\n        \"\"\"", "\n", "assert", "input", ".", "shape", "[", "-", "1", "]", "==", "self", ".", "input_dim", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "input_dim", ")", "if", "input", ".", "dim", "(", ")", ">", "2", "else", "input", "\n", "bs", "=", "len", "(", "input", ")", "\n", "\n", "if", "self", ".", "grouped_conv", "or", "len", "(", "self", ".", "sizes", ")", "==", "2", ":", "\n", "            ", "query", "=", "self", ".", "query_mlps", "(", "input", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "[", "m", "(", "input", ")", "for", "m", "in", "self", ".", "query_mlps", "]", "\n", "query", "=", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "if", "len", "(", "outputs", ")", ">", "1", "else", "outputs", "[", "0", "]", "\n", "\n", "", "assert", "query", ".", "shape", "==", "(", "bs", ",", "self", ".", "heads", "*", "self", ".", "k_dim", ")", "\n", "return", "query", ".", "view", "(", "bs", "*", "self", ".", "heads", ",", "self", ".", "k_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryConv.__init__": [[206, 237], ["torch.nn.Module.__init__", "all", "query.convs", "len", "len", "len", "list", "query.convs", "torch.nn.ModuleList", "len", "query.convs", "range"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.convs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.convs", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.convs"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_dim", ",", "heads", ",", "k_dim", ",", "product_quantization", ",", "multi_query_net", ",", "\n", "sizes", ",", "kernel_sizes", ",", "bias", "=", "True", ",", "batchnorm", "=", "True", ",", "\n", "residual", "=", "False", ",", "grouped_conv", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "k_dim", "=", "k_dim", "\n", "self", ".", "sizes", "=", "sizes", "\n", "self", ".", "grouped_conv", "=", "grouped_conv", "\n", "assert", "not", "multi_query_net", "or", "product_quantization", "or", "heads", ">=", "2", "\n", "assert", "sizes", "[", "0", "]", "==", "input_dim", "\n", "assert", "sizes", "[", "-", "1", "]", "==", "(", "k_dim", "//", "2", ")", "if", "multi_query_net", "else", "(", "heads", "*", "k_dim", ")", "\n", "assert", "self", ".", "grouped_conv", "is", "False", "or", "len", "(", "sizes", ")", ">", "2", "\n", "assert", "len", "(", "sizes", ")", "==", "len", "(", "kernel_sizes", ")", "+", "1", ">=", "2", "and", "all", "(", "ks", "%", "2", "==", "1", "for", "ks", "in", "kernel_sizes", ")", "\n", "\n", "# number of required CNNs", "\n", "self", ".", "groups", "=", "(", "2", "*", "heads", ")", "if", "multi_query_net", "else", "1", "\n", "\n", "# CNNs", "\n", "if", "self", ".", "grouped_conv", ":", "\n", "            ", "self", ".", "query_convs", "=", "convs", "(", "sizes", ",", "kernel_sizes", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "residual", "=", "residual", ",", "groups", "=", "self", ".", "groups", ")", "\n", "", "elif", "len", "(", "self", ".", "sizes", ")", "==", "2", ":", "\n", "            ", "sizes_", "=", "list", "(", "sizes", ")", "\n", "sizes_", "[", "-", "1", "]", "=", "sizes_", "[", "-", "1", "]", "*", "self", ".", "groups", "\n", "self", ".", "query_convs", "=", "convs", "(", "sizes_", ",", "kernel_sizes", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "residual", "=", "residual", ",", "groups", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "query_convs", "=", "nn", ".", "ModuleList", "(", "[", "\n", "convs", "(", "sizes", ",", "kernel_sizes", ",", "bias", "=", "bias", ",", "batchnorm", "=", "batchnorm", ",", "residual", "=", "residual", ",", "groups", "=", "1", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "groups", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.QueryConv.forward": [[239, 253], ["query.QueryConv.QueryConv.transpose().contiguous().view", "query.QueryConv.QueryConv.query_convs", "len", "m", "torch.cat", "query.QueryConv.QueryConv.transpose().contiguous", "len", "query.QueryConv.QueryConv.transpose"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "\n", "        ", "bs", ",", "nf", ",", "h", ",", "w", "=", "input", ".", "shape", "\n", "assert", "nf", "==", "self", ".", "input_dim", "\n", "\n", "if", "self", ".", "grouped_conv", "or", "len", "(", "self", ".", "sizes", ")", "==", "2", ":", "\n", "            ", "query", "=", "self", ".", "query_convs", "(", "input", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "[", "m", "(", "input", ")", "for", "m", "in", "self", ".", "query_convs", "]", "\n", "query", "=", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "if", "len", "(", "outputs", ")", ">", "1", "else", "outputs", "[", "0", "]", "\n", "\n", "", "assert", "query", ".", "shape", "==", "(", "bs", ",", "self", ".", "heads", "*", "self", ".", "k_dim", ",", "h", ",", "w", ")", "\n", "query", "=", "query", ".", "transpose", "(", "1", ",", "3", ")", ".", "contiguous", "(", ")", ".", "view", "(", "bs", "*", "w", "*", "h", "*", "self", ".", "heads", ",", "self", ".", "k_dim", ")", "\n", "return", "query", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.mlp": [[7, 26], ["enumerate", "torch.nn.Sequential", "len", "range", "layers.append", "layers.append", "layers.append", "layers.append", "torch.nn.Linear", "query.GroupedLinear", "torch.nn.BatchNorm1d", "len", "torch.nn.ReLU", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["def", "mlp", "(", "sizes", ",", "bias", "=", "True", ",", "batchnorm", "=", "True", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Generate a feedforward neural network.\n    \"\"\"", "\n", "assert", "len", "(", "sizes", ")", ">=", "2", "\n", "pairs", "=", "[", "(", "sizes", "[", "i", "]", ",", "sizes", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "len", "(", "sizes", ")", "-", "1", ")", "]", "\n", "layers", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "dim_in", ",", "dim_out", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "if", "groups", "==", "1", "or", "i", "==", "0", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "dim_in", ",", "groups", "*", "dim_out", ",", "bias", "=", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "layers", ".", "append", "(", "GroupedLinear", "(", "groups", "*", "dim_in", ",", "groups", "*", "dim_out", ",", "bias", "=", "bias", ",", "groups", "=", "groups", ")", ")", "\n", "", "if", "batchnorm", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "groups", "*", "dim_out", ")", ")", "\n", "", "if", "i", "<", "len", "(", "pairs", ")", "-", "1", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.query.convs": [[28, 57], ["enumerate", "torch.nn.Sequential", "len", "len", "len", "range", "layers.append", "layers.append", "torch.nn.Conv2d", "layers.append", "layers.append", "query.BottleneckResidualConv2d", "layers.append", "len", "torch.nn.BatchNorm2d", "len", "torch.nn.ReLU", "len", "torch.nn.Conv2d"], "function", ["None"], ["", "def", "convs", "(", "channel_sizes", ",", "kernel_sizes", ",", "bias", "=", "True", ",", "batchnorm", "=", "True", ",", "residual", "=", "False", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Generate a convolutional neural network.\n    \"\"\"", "\n", "assert", "len", "(", "channel_sizes", ")", ">=", "2", "\n", "assert", "len", "(", "channel_sizes", ")", "==", "len", "(", "kernel_sizes", ")", "+", "1", "\n", "pairs", "=", "[", "(", "channel_sizes", "[", "i", "]", ",", "channel_sizes", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "len", "(", "channel_sizes", ")", "-", "1", ")", "]", "\n", "layers", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "dim_in", ",", "dim_out", ")", "in", "enumerate", "(", "pairs", ")", ":", "\n", "        ", "ks", "=", "(", "kernel_sizes", "[", "i", "]", ",", "kernel_sizes", "[", "i", "]", ")", "\n", "in_group", "=", "1", "if", "i", "==", "0", "else", "groups", "\n", "_dim_in", "=", "dim_in", "*", "in_group", "\n", "_dim_out", "=", "dim_out", "*", "groups", "\n", "if", "not", "residual", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "Conv2d", "(", "_dim_in", ",", "_dim_out", ",", "ks", ",", "padding", "=", "[", "k", "//", "2", "for", "k", "in", "ks", "]", ",", "bias", "=", "bias", ",", "groups", "=", "in_group", ")", ")", "\n", "if", "batchnorm", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "BatchNorm2d", "(", "_dim_out", ")", ")", "\n", "", "if", "i", "<", "len", "(", "pairs", ")", "-", "1", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "layers", ".", "append", "(", "BottleneckResidualConv2d", "(", "\n", "_dim_in", ",", "_dim_out", ",", "ks", ",", "bias", "=", "bias", ",", "\n", "batchnorm", "=", "batchnorm", ",", "groups", "=", "in_group", "\n", ")", ")", "\n", "if", "i", "==", "len", "(", "pairs", ")", "-", "1", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Conv2d", "(", "_dim_out", ",", "_dim_out", ",", "(", "1", ",", "1", ")", ",", "bias", "=", "bias", ")", ")", "\n", "\n", "", "", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_gaussian_keys": [[16, 25], ["numpy.random.RandomState", "np.random.RandomState.randn", "rng.randn.astype", "numpy.linalg.norm"], "function", ["None"], ["import", "numpy", "as", "np", "\n", "import", "torch", "\n", "\n", "from", ".", "logger", "import", "create_logger", "\n", "\n", "\n", "FALSY_STRINGS", "=", "{", "'off'", ",", "'false'", ",", "'0'", "}", "\n", "TRUTHY_STRINGS", "=", "{", "'on'", ",", "'true'", ",", "'1'", "}", "\n", "\n", "DUMP_PATH", "=", "'/checkpoint/%s/dumped'", "%", "getpass", ".", "getuser", "(", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_uniform_keys": [[27, 37], ["numpy.random.RandomState", "np.random.RandomState.uniform", "rng.uniform.astype", "math.sqrt", "numpy.linalg.norm"], "function", ["None"], ["\n", "\n", "class", "AttrDict", "(", "dict", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "AttrDict", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "__dict__", "=", "self", "\n", "\n", "\n", "", "", "def", "bool_flag", "(", "s", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_slices": [[39, 52], ["numpy.arange", "enumerate", "enumerate"], "function", ["None"], ["if", "s", ".", "lower", "(", ")", "in", "FALSY_STRINGS", ":", "\n", "        ", "return", "False", "\n", "", "elif", "s", ".", "lower", "(", ")", "in", "TRUTHY_STRINGS", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "\"Invalid value for a boolean flag!\"", ")", "\n", "\n", "\n", "", "", "def", "initialize_exp", "(", "params", ",", "log_filename", "=", "None", ",", "params_filename", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Initialize the experience:\n    - dump parameters\n    - create a logger\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.cartesian_product": [[54, 70], ["torch.cat().view", "torch.cat", "a.unsqueeze().repeat().unsqueeze", "b.repeat().view().unsqueeze", "a.unsqueeze().repeat", "b.repeat().view", "a.unsqueeze", "b.repeat"], "function", ["None"], ["get_dump_path", "(", "params", ")", "\n", "if", "params_filename", "is", "None", ":", "\n", "        ", "params_filename", "=", "'params.pkl'", "\n", "", "pickle", ".", "dump", "(", "params", ",", "open", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "params_filename", ")", ",", "'wb'", ")", ")", "\n", "\n", "# get running command", "\n", "command", "=", "[", "\"python\"", ",", "sys", ".", "argv", "[", "0", "]", "]", "\n", "for", "x", "in", "sys", ".", "argv", "[", "1", ":", "]", ":", "\n", "        ", "if", "x", ".", "startswith", "(", "'--'", ")", ":", "\n", "            ", "assert", "'\"'", "not", "in", "x", "and", "\"'\"", "not", "in", "x", "\n", "command", ".", "append", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "assert", "\"'\"", "not", "in", "x", "\n", "if", "re", ".", "match", "(", "'^[a-zA-Z0-9_]+$'", ",", "x", ")", ":", "\n", "                ", "command", ".", "append", "(", "\"%s\"", "%", "x", ")", "\n", "", "else", ":", "\n", "                ", "command", ".", "append", "(", "\"'%s'\"", "%", "x", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_FloatTensor": [[72, 76], ["x.is_contiguous", "faiss.cast_integer_to_float_ptr", "x.storage().data_ptr", "x.storage_offset", "x.storage"], "function", ["None"], ["params", ".", "command", "=", "command", "+", "' --exp_id \"%s\"'", "%", "params", ".", "exp_id", "\n", "\n", "# check experiment name", "\n", "assert", "len", "(", "params", ".", "exp_name", ".", "strip", "(", ")", ")", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_LongTensor": [[78, 82], ["x.is_contiguous", "faiss.cast_integer_to_long_ptr", "x.storage().data_ptr", "x.storage_offset", "x.storage"], "function", ["None"], ["if", "log_filename", "is", "None", ":", "\n", "        ", "log_filename", "=", "'train.log'", "\n", "", "logger", "=", "create_logger", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "log_filename", ")", ",", "rank", "=", "getattr", "(", "params", ",", "'global_rank'", ",", "0", ")", ")", "\n", "logger", ".", "info", "(", "\"============ Initialized logger ============\"", ")", "\n", "logger", ".", "info", "(", "\"\\n\"", ".", "join", "(", "\"%s: %s\"", "%", "(", "k", ",", "str", "(", "v", ")", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_knn_pytorch": [[84, 122], ["a.size", "b.size", "b.size", "torch.no_grad", "a.mm.topk", "a.mm.t", "indices.t.t", "a.mm", "b.t", "a.mm", "b.t", "a.mm", "a.norm", "b.norm", "b.t"], "function", ["None"], ["logger", ".", "info", "(", "\"The experiment will be stored in %s\\n\"", "%", "params", ".", "dump_path", ")", "\n", "logger", ".", "info", "(", "\"Running command: %s\"", "%", "command", ")", "\n", "logger", ".", "info", "(", "\"\"", ")", "\n", "return", "logger", "\n", "\n", "\n", "", "def", "get_dump_path", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Create a directory to store the experiment.\n    \"\"\"", "\n", "dump_path", "=", "DUMP_PATH", "if", "params", ".", "dump_path", "==", "''", "else", "params", ".", "dump_path", "\n", "assert", "len", "(", "params", ".", "exp_name", ")", ">", "0", "\n", "\n", "# create the sweep path if it does not exist", "\n", "sweep_path", "=", "os", ".", "path", ".", "join", "(", "dump_path", ",", "params", ".", "exp_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "sweep_path", ")", ":", "\n", "        ", "subprocess", ".", "Popen", "(", "\"mkdir -p %s\"", "%", "sweep_path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "\n", "# create an ID for the job if it is not given in the parameters.", "\n", "# if we run on the cluster, the job ID is the one of Chronos.", "\n", "# otherwise, it is randomly generated", "\n", "", "if", "params", ".", "exp_id", "==", "''", ":", "\n", "        ", "chronos_job_id", "=", "os", ".", "environ", ".", "get", "(", "'CHRONOS_JOB_ID'", ")", "\n", "slurm_job_id", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_ID'", ")", "\n", "assert", "chronos_job_id", "is", "None", "or", "slurm_job_id", "is", "None", "\n", "exp_id", "=", "chronos_job_id", "if", "chronos_job_id", "is", "not", "None", "else", "slurm_job_id", "\n", "if", "exp_id", "is", "None", ":", "\n", "            ", "chars", "=", "'abcdefghijklmnopqrstuvwxyz0123456789'", "\n", "while", "True", ":", "\n", "                ", "exp_id", "=", "''", ".", "join", "(", "random", ".", "choice", "(", "chars", ")", "for", "_", "in", "range", "(", "10", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "sweep_path", ",", "exp_id", ")", ")", ":", "\n", "                    ", "break", "\n", "", "", "", "else", ":", "\n", "            ", "assert", "exp_id", ".", "isdigit", "(", ")", "\n", "", "params", ".", "exp_id", "=", "exp_id", "\n", "\n", "# create the dump folder / update parameters", "\n", "", "params", ".", "dump_path", "=", "os", ".", "path", ".", "join", "(", "sweep_path", ",", "params", ".", "exp_id", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "params", ".", "dump_path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_knn_faiss": [[124, 154], ["utils.swig_ptr_from_FloatTensor", "utils.swig_ptr_from_FloatTensor", "xq.size", "xb.size", "torch.empty", "torch.empty", "utils.swig_ptr_from_FloatTensor", "utils.swig_ptr_from_LongTensor", "faiss.bruteForceKnn"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_FloatTensor", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_FloatTensor", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_FloatTensor", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.swig_ptr_from_LongTensor"], ["\n", "\n", "", "", "def", "to_cuda", "(", "*", "args", ")", ":", "\n", "    ", "\"\"\"\n    Move tensors to CUDA.\n    \"\"\"", "\n", "return", "[", "None", "if", "x", "is", "None", "else", "x", ".", "cuda", "(", ")", "for", "x", "in", "args", "]", "\n", "\n", "\n", "", "def", "restore_segmentation", "(", "path", ",", "raw_prefix", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Take a file segmented with BPE and restore it to its original segmentation.\n    \"\"\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "from", "shutil", "import", "copyfile", "\n", "parts", "=", "path", ".", "split", "(", "\".\"", ")", "\n", "raw_file", "=", "'.'", ".", "join", "(", "parts", "[", ":", "-", "1", "]", "+", "[", "'raw'", "]", "+", "parts", "[", "-", "1", ":", "]", ")", "\n", "copyfile", "(", "path", ",", "raw_file", "if", "raw_prefix", "else", "'{}.raw'", ".", "format", "(", "path", ")", ")", "\n", "restore_cmd", "=", "\"sed -i -r 's/(@@ )|(@@ ?$)//g' %s\"", "\n", "subprocess", ".", "Popen", "(", "restore_cmd", "%", "path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "\n", "\n", "", "def", "parse_lambda_config", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Parse the configuration of lambda coefficient (for scheduling).\n    x = \"3\"                  # lambda will be a constant equal to x\n    x = \"0:1,1000:0\"         # lambda will start from 1 and linearly decrease to 0 during the first 1000 iterations\n    x = \"0:0,1000:0,2000:1\"  # lambda will be equal to 0 for the first 1000 iterations, then will linearly increase to 1 until iteration 2000\n    \"\"\"", "\n", "for", "name", "in", "DYNAMIC_COEFF", ":", "\n", "        ", "x", "=", "getattr", "(", "params", ",", "name", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.__init__": [[25, 137], ["torch.nn.Module.__init__", "next", "memory.HashingMemory.init_keys", "torch.nn.EmbeddingBag", "torch.nn.init.zeros_", "torch.nn.init.normal_", "len", "query.QueryIdentity", "len", "list", "memory.HashingMemory.register_buffer", "memory.HashingMemory.query_proj.parameters", "query.QueryConv", "query.QueryMLP", "torch.randperm().unsqueeze", "torch.cat", "len", "range", "torch.randperm"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.init_keys"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "params", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "id", "=", "next", "(", "self", ".", "_ids", ")", "\n", "\n", "# global parameters", "\n", "self", ".", "input2d", "=", "params", ".", "mem_input2d", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "size", "=", "params", ".", "mem_size", "\n", "self", ".", "modulo_size", "=", "params", ".", "mem_modulo_size", "\n", "self", ".", "n_indices", "=", "params", ".", "n_indices", "\n", "self", ".", "k_dim", "=", "params", ".", "mem_k_dim", "\n", "self", ".", "v_dim", "=", "params", ".", "mem_v_dim", "if", "params", ".", "mem_v_dim", ">", "0", "else", "output_dim", "\n", "self", ".", "heads", "=", "params", ".", "mem_heads", "\n", "self", ".", "knn", "=", "params", ".", "mem_knn", "\n", "self", ".", "shuffle_indices", "=", "params", ".", "mem_shuffle_indices", "\n", "self", ".", "keys_normalized_init", "=", "params", ".", "mem_keys_normalized_init", "\n", "self", ".", "product_quantization", "=", "params", ".", "mem_product_quantization", "\n", "assert", "self", ".", "modulo_size", "==", "-", "1", "and", "self", ".", "size", "==", "self", ".", "n_indices", "or", "self", ".", "n_indices", ">", "self", ".", "size", "==", "self", ".", "modulo_size", ">=", "1", "\n", "\n", "# keys / queries", "\n", "self", ".", "keys_type", "=", "params", ".", "mem_keys_type", "\n", "self", ".", "learn_keys", "=", "params", ".", "mem_keys_learn", "\n", "self", ".", "use_different_keys", "=", "params", ".", "mem_use_different_keys", "\n", "self", ".", "query_detach_input", "=", "params", ".", "mem_query_detach_input", "\n", "self", ".", "query_net_learn", "=", "params", ".", "mem_query_net_learn", "\n", "self", ".", "multi_query_net", "=", "params", ".", "mem_multi_query_net", "\n", "self", ".", "shuffle_query", "=", "params", ".", "mem_shuffle_query", "\n", "assert", "self", ".", "use_different_keys", "is", "False", "or", "self", ".", "keys_type", "in", "[", "'gaussian'", ",", "'uniform'", "]", "\n", "assert", "self", ".", "use_different_keys", "is", "False", "or", "self", ".", "heads", ">=", "2", "or", "self", ".", "product_quantization", "\n", "assert", "self", ".", "multi_query_net", "is", "False", "or", "self", ".", "heads", ">=", "2", "or", "self", ".", "product_quantization", "\n", "assert", "self", ".", "shuffle_query", "is", "False", "or", "self", ".", "heads", ">", "1", "and", "params", ".", "mem_query_layer_sizes", "==", "''", "\n", "assert", "self", ".", "shuffle_query", "is", "False", "or", "self", ".", "input_dim", "%", "(", "2", "**", "self", ".", "heads", ")", "==", "0", "\n", "\n", "# scoring / re-scoring", "\n", "self", ".", "normalize_query", "=", "params", ".", "mem_normalize_query", "\n", "self", ".", "temperature", "=", "params", ".", "mem_temperature", "\n", "self", ".", "score_softmax", "=", "params", ".", "mem_score_softmax", "\n", "self", ".", "score_subtract", "=", "params", ".", "mem_score_subtract", "\n", "self", ".", "score_normalize", "=", "params", ".", "mem_score_normalize", "\n", "assert", "self", ".", "score_subtract", "in", "[", "''", ",", "'min'", ",", "'mean'", ",", "'median'", "]", "\n", "assert", "self", ".", "score_subtract", "==", "''", "or", "self", ".", "knn", ">=", "2", "\n", "assert", "not", "(", "self", ".", "score_normalize", "and", "self", ".", "score_softmax", "and", "self", ".", "score_subtract", "==", "''", ")", "\n", "\n", "# dropout", "\n", "self", ".", "input_dropout", "=", "params", ".", "mem_input_dropout", "\n", "self", ".", "query_dropout", "=", "params", ".", "mem_query_dropout", "\n", "self", ".", "value_dropout", "=", "params", ".", "mem_value_dropout", "\n", "\n", "# initialize keys", "\n", "self", ".", "init_keys", "(", ")", "\n", "\n", "# self.values = nn.Embedding(self.size, self.v_dim, sparse=params.mem_sparse)", "\n", "self", ".", "values", "=", "nn", ".", "EmbeddingBag", "(", "self", ".", "size", ",", "self", ".", "v_dim", ",", "mode", "=", "'sum'", ",", "sparse", "=", "params", ".", "mem_sparse", ")", "\n", "\n", "# optionally use the same values for all memories", "\n", "if", "params", ".", "mem_share_values", ":", "\n", "            ", "if", "HashingMemory", ".", "VALUES", "is", "None", ":", "\n", "                ", "HashingMemory", ".", "VALUES", "=", "self", ".", "values", ".", "weight", "\n", "", "else", ":", "\n", "                ", "self", ".", "values", ".", "weight", "=", "HashingMemory", ".", "VALUES", "\n", "\n", "# values initialization", "\n", "", "", "if", "params", ".", "mem_value_zero_init", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "values", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "values", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "self", ".", "v_dim", "**", "-", "0.5", ")", "\n", "\n", "# no query network", "\n", "", "if", "len", "(", "params", ".", "mem_query_layer_sizes", ")", "==", "0", ":", "\n", "            ", "assert", "self", ".", "heads", "==", "1", "or", "self", ".", "use_different_keys", "or", "self", ".", "shuffle_query", "\n", "assert", "self", ".", "input_dim", "==", "self", ".", "k_dim", "\n", "self", ".", "query_proj", "=", "QueryIdentity", "(", "self", ".", "input_dim", ",", "self", ".", "heads", ",", "self", ".", "shuffle_query", ")", "\n", "\n", "# query network", "\n", "", "if", "len", "(", "params", ".", "mem_query_layer_sizes", ")", ">", "0", ":", "\n", "            ", "assert", "not", "self", ".", "shuffle_query", "\n", "\n", "# layer sizes / number of features", "\n", "l_sizes", "=", "list", "(", "params", ".", "mem_query_layer_sizes", ")", "\n", "assert", "len", "(", "l_sizes", ")", ">=", "2", "and", "l_sizes", "[", "0", "]", "==", "l_sizes", "[", "-", "1", "]", "==", "0", "\n", "l_sizes", "[", "0", "]", "=", "self", ".", "input_dim", "\n", "l_sizes", "[", "-", "1", "]", "=", "(", "self", ".", "k_dim", "//", "2", ")", "if", "self", ".", "multi_query_net", "else", "(", "self", ".", "heads", "*", "self", ".", "k_dim", ")", "\n", "\n", "# convolutional or feedforward", "\n", "if", "self", ".", "input2d", ":", "\n", "                ", "self", ".", "query_proj", "=", "QueryConv", "(", "\n", "self", ".", "input_dim", ",", "self", ".", "heads", ",", "self", ".", "k_dim", ",", "self", ".", "product_quantization", ",", "\n", "self", ".", "multi_query_net", ",", "l_sizes", ",", "params", ".", "mem_query_kernel_sizes", ",", "\n", "bias", "=", "params", ".", "mem_query_bias", ",", "batchnorm", "=", "params", ".", "mem_query_batchnorm", ",", "\n", "grouped_conv", "=", "params", ".", "mem_grouped_conv", "\n", ")", "\n", "", "else", ":", "\n", "                ", "assert", "params", ".", "mem_query_kernel_sizes", "==", "''", "\n", "assert", "not", "params", ".", "mem_query_residual", "\n", "self", ".", "query_proj", "=", "QueryMLP", "(", "\n", "self", ".", "input_dim", ",", "self", ".", "heads", ",", "self", ".", "k_dim", ",", "self", ".", "product_quantization", ",", "\n", "self", ".", "multi_query_net", ",", "l_sizes", ",", "\n", "bias", "=", "params", ".", "mem_query_bias", ",", "batchnorm", "=", "params", ".", "mem_query_batchnorm", ",", "\n", "grouped_conv", "=", "params", ".", "mem_grouped_conv", "\n", ")", "\n", "\n", "# shuffle indices for different heads", "\n", "", "", "if", "self", ".", "shuffle_indices", ":", "\n", "            ", "head_permutations", "=", "[", "torch", ".", "randperm", "(", "self", ".", "n_indices", ")", ".", "unsqueeze", "(", "0", ")", "for", "i", "in", "range", "(", "self", ".", "heads", ")", "]", "\n", "self", ".", "register_buffer", "(", "'head_permutations'", ",", "torch", ".", "cat", "(", "head_permutations", ",", "0", ")", ")", "\n", "\n", "# do not learn the query network", "\n", "", "if", "self", ".", "query_net_learn", "is", "False", ":", "\n", "            ", "for", "p", "in", "self", ".", "query_proj", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.forward": [[138, 218], ["numpy.prod", "torch.nn.functional.dropout", "memory.HashingMemory.query_proj", "torch.nn.functional.dropout", "memory.HashingMemory.get_indices", "torch.cat().view.view", "torch.nn.functional.softmax().type_as.view", "memory.HashingMemory.values().to", "torch.nn.functional.dropout", "input.detach.detach.detach", "torch.cat().view.view().chunk", "torch.cat().view", "torch.nn.functional.softmax().type_as", "output.view.view.view", "output.view.view.transpose", "torch.cat().view.view().detach().cpu", "torch.nn.functional.softmax().type_as.view().detach().cpu().float", "torch.nn.functional.softmax().type_as.mean", "torch.nn.functional.softmax().type_as.norm", "memory.HashingMemory.values", "len", "output.view.view.view", "torch.cat().view.view", "zip", "torch.cat", "torch.nn.functional.softmax", "torch.nn.functional.softmax().type_as.min", "torch.nn.functional.softmax().type_as.median", "torch.cat().view.view().detach", "torch.nn.functional.softmax().type_as.view().detach().cpu", "torch.nn.functional.softmax().type_as.float", "torch.nn.functional.softmax().type_as.to", "torch.cat().view.view", "torch.nn.functional.softmax().type_as.view().detach", "torch.nn.functional.softmax().type_as.view"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.get_indices"], ["", "", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        Read from the memory.\n        \"\"\"", "\n", "# detach input", "\n", "if", "self", ".", "query_detach_input", ":", "\n", "            ", "input", "=", "input", ".", "detach", "(", ")", "\n", "\n", "# input dimensions", "\n", "", "if", "self", ".", "input2d", ":", "\n", "            ", "assert", "input", ".", "shape", "[", "1", "]", "==", "self", ".", "input_dim", "\n", "n_images", ",", "_", ",", "height", ",", "width", "=", "input", ".", "shape", "\n", "prefix_shape", "=", "(", "n_images", ",", "width", ",", "height", ")", "\n", "", "else", ":", "\n", "            ", "assert", "input", ".", "shape", "[", "-", "1", "]", "==", "self", ".", "input_dim", "\n", "prefix_shape", "=", "input", ".", "shape", "[", ":", "-", "1", "]", "\n", "\n", "# compute query / store it", "\n", "", "bs", "=", "np", ".", "prod", "(", "prefix_shape", ")", "\n", "input", "=", "F", ".", "dropout", "(", "input", ",", "p", "=", "self", ".", "input_dropout", ",", "training", "=", "self", ".", "training", ")", "# input shape", "\n", "query", "=", "self", ".", "query_proj", "(", "input", ")", "# (bs * heads, k_dim)", "\n", "query", "=", "F", ".", "dropout", "(", "query", ",", "p", "=", "self", ".", "query_dropout", ",", "training", "=", "self", ".", "training", ")", "# (bs * heads, k_dim)", "\n", "assert", "query", ".", "shape", "==", "(", "bs", "*", "self", ".", "heads", ",", "self", ".", "k_dim", ")", "\n", "\n", "# get indices", "\n", "scores", ",", "indices", "=", "self", ".", "get_indices", "(", "query", ",", "self", ".", "knn", ")", "# (bs * heads, knn) ** 2", "\n", "\n", "# optionally shuffle indices for different heads", "\n", "if", "self", ".", "shuffle_indices", ":", "\n", "            ", "indices", "=", "indices", ".", "view", "(", "bs", ",", "self", ".", "heads", ",", "-", "1", ")", ".", "chunk", "(", "self", ".", "heads", ",", "1", ")", "\n", "indices", "=", "[", "p", "[", "idx", "]", "for", "p", ",", "idx", "in", "zip", "(", "self", ".", "head_permutations", ",", "indices", ")", "]", "\n", "indices", "=", "torch", ".", "cat", "(", "indices", ",", "1", ")", ".", "view", "(", "bs", "*", "self", ".", "heads", ",", "-", "1", ")", "\n", "\n", "# take indices modulo the memory size", "\n", "", "if", "self", ".", "modulo_size", "!=", "-", "1", ":", "\n", "            ", "indices", "=", "indices", "%", "self", ".", "modulo_size", "\n", "\n", "# re-scoring", "\n", "", "if", "self", ".", "temperature", "!=", "1", ":", "\n", "            ", "scores", "=", "scores", "/", "self", ".", "temperature", "# (bs * heads, knn)", "\n", "", "if", "self", ".", "score_softmax", ":", "\n", "            ", "scores", "=", "F", ".", "softmax", "(", "scores", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ".", "type_as", "(", "scores", ")", "# (bs * heads, knn)", "\n", "", "if", "self", ".", "score_subtract", "!=", "''", ":", "\n", "            ", "if", "self", ".", "score_subtract", "==", "'min'", ":", "\n", "                ", "to_sub", "=", "scores", ".", "min", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "# (bs * heads, 1)", "\n", "", "if", "self", ".", "score_subtract", "==", "'mean'", ":", "\n", "                ", "to_sub", "=", "scores", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", "# (bs * heads, 1)", "\n", "", "if", "self", ".", "score_subtract", "==", "'median'", ":", "\n", "                ", "to_sub", "=", "scores", ".", "median", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "# (bs * heads, 1)", "\n", "", "scores", "=", "scores", "-", "to_sub", "# (bs * heads, knn)", "\n", "", "if", "self", ".", "score_normalize", ":", "\n", "            ", "scores", "=", "scores", "/", "scores", ".", "norm", "(", "p", "=", "1", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# (bs * heads, knn)", "\n", "\n", "# merge heads / knn (since we sum heads)", "\n", "", "indices", "=", "indices", ".", "view", "(", "bs", ",", "self", ".", "heads", "*", "self", ".", "knn", ")", "# (bs, heads * knn)", "\n", "scores", "=", "scores", ".", "view", "(", "bs", ",", "self", ".", "heads", "*", "self", ".", "knn", ")", "# (bs, heads * knn)", "\n", "\n", "# weighted sum of values", "\n", "# output = self.values(indices) * scores.unsqueeze(-1)                    # (bs * heads, knn, v_dim)", "\n", "# output = output.sum(1)                                                  # (bs * heads, v_dim)", "\n", "output", "=", "self", ".", "values", "(", "\n", "indices", ",", "\n", "per_sample_weights", "=", "scores", ".", "to", "(", "self", ".", "values", ".", "weight", ".", "data", ")", "\n", ")", ".", "to", "(", "scores", ")", "# (bs, v_dim)", "\n", "output", "=", "F", ".", "dropout", "(", "output", ",", "p", "=", "self", ".", "value_dropout", ",", "training", "=", "self", ".", "training", ")", "# (bs, v_dim)", "\n", "\n", "# reshape output", "\n", "if", "self", ".", "input2d", ":", "\n", "            ", "output", "=", "output", ".", "view", "(", "n_images", ",", "width", ",", "height", ",", "self", ".", "v_dim", ")", "# (n_images, width, height, v_dim)", "\n", "output", "=", "output", ".", "transpose", "(", "1", ",", "3", ")", "# (n_images, v_dim, height, width)", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "prefix_shape", ")", ">=", "2", ":", "\n", "                ", "output", "=", "output", ".", "view", "(", "prefix_shape", "+", "(", "self", ".", "v_dim", ",", ")", ")", "# (..., v_dim)", "\n", "\n", "# store indices / scores (eval mode only - for usage statistics)", "\n", "", "", "if", "not", "self", ".", "training", "and", "HashingMemory", ".", "EVAL_MEMORY", ":", "\n", "            ", "self", ".", "last_indices", "=", "indices", ".", "view", "(", "bs", ",", "self", ".", "heads", ",", "self", ".", "knn", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "self", ".", "last_scores", "=", "scores", ".", "view", "(", "bs", ",", "self", ".", "heads", ",", "self", ".", "knn", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "float", "(", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.init_keys": [[219, 221], ["Exception"], "methods", ["None"], ["", "def", "init_keys", "(", "self", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"Not implemented!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory._get_indices": [[222, 224], ["Exception"], "methods", ["None"], ["", "def", "_get_indices", "(", "self", ",", "query", ",", "knn", ",", "keys", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"Not implemented!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.get_indices": [[225, 227], ["Exception"], "methods", ["None"], ["", "def", "get_indices", "(", "self", ",", "query", ",", "knn", ")", ":", "\n", "        ", "raise", "Exception", "(", "\"Not implemented!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.register_args": [[228, 318], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "register_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"\n        Register memory parameters\n        \"\"\"", "\n", "# memory implementation", "\n", "parser", ".", "add_argument", "(", "\"--mem_implementation\"", ",", "type", "=", "str", ",", "default", "=", "\"pq_fast\"", ",", "\n", "help", "=", "\"Memory implementation (flat, pq_default, pq_fast)\"", ")", "\n", "\n", "# optimization", "\n", "parser", ".", "add_argument", "(", "\"--mem_grouped_conv\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use grouped convolutions in the query network\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_values_optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"adam,lr=0.001\"", ",", "\n", "help", "=", "\"Memory values optimizer (\"", "\" for the same optimizer as the rest of the model)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_sparse\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Perform sparse updates for the values\"", ")", "\n", "\n", "# global parameters", "\n", "parser", ".", "add_argument", "(", "\"--mem_input2d\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Convolutional query network\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_k_dim\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Memory keys dimension\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_v_dim\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Memory values dimension (-1 for automatic output dimension)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_heads\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Number of memory reading heads\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_knn\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Number of memory slots to read / update - k-NN to the query\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_share_values\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Share values across memories\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_shuffle_indices\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Shuffle indices for different heads\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_shuffle_query\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Shuffle query dimensions (when the query network is the identity and there are multiple heads)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_modulo_size\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Effective memory size: indices are taken modulo this parameter. -1 to disable.\"", ")", "\n", "\n", "# keys", "\n", "parser", ".", "add_argument", "(", "\"--mem_keys_type\"", ",", "type", "=", "str", ",", "default", "=", "\"uniform\"", ",", "\n", "help", "=", "\"Memory keys type (binary,gaussian,uniform)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_n_keys\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "\"Number of keys\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_keys_normalized_init\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Normalize keys at initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_keys_learn\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Learn keys\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_use_different_keys\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Use different keys for each head / product quantization\"", ")", "\n", "\n", "# queries", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_detach_input\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Detach input\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_layer_sizes\"", ",", "type", "=", "str", ",", "default", "=", "\"0,0\"", ",", "\n", "help", "=", "\"Query MLP layer sizes ('', '0,0', '0,512,0')\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_kernel_sizes\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Query MLP kernel sizes (2D inputs only)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_bias\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Query MLP bias\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_batchnorm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Query MLP batch norm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_net_learn\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Query MLP learn\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_residual\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use a bottleneck with a residual layer in the query MLP\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_multi_query_net\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Use multiple query MLP (one for each head)\"", ")", "\n", "\n", "# values initialization", "\n", "parser", ".", "add_argument", "(", "\"--mem_value_zero_init\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Initialize values with zeros\"", ")", "\n", "\n", "# scoring", "\n", "parser", ".", "add_argument", "(", "\"--mem_normalize_query\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Normalize queries\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_temperature\"", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Divide scores by a temperature\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_score_softmax\"", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "\"Apply softmax on scores\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_score_subtract\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Subtract scores ('', min, mean, median)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_score_normalize\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"L1 normalization of the scores\"", ")", "\n", "\n", "# dropout", "\n", "parser", ".", "add_argument", "(", "\"--mem_input_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Input dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_query_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Query dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mem_value_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Value dropout\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.build": [[319, 330], ["M", "Exception"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build", "(", "input_dim", ",", "output_dim", ",", "params", ")", ":", "\n", "        ", "if", "params", ".", "mem_implementation", "==", "'flat'", ":", "\n", "            ", "M", "=", "HashingMemoryFlat", "\n", "", "elif", "params", ".", "mem_implementation", "==", "'pq_default'", ":", "\n", "            ", "M", "=", "HashingMemoryProduct", "\n", "", "elif", "params", ".", "mem_implementation", "==", "'pq_fast'", ":", "\n", "            ", "M", "=", "HashingMemoryProductFast", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown memory implementation!\"", ")", "\n", "", "return", "M", "(", "input_dim", ",", "output_dim", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemory.check_params": [[331, 407], ["params.mem_values_optimizer.replace", "all", "logger.warning", "int", "int", "len", "filter", "len", "filter", "len", "params.mem_query_layer_sizes.split", "params.mem_query_kernel_sizes.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_params", "(", "params", ")", ":", "\n", "        ", "\"\"\"\n        Check and initialize memory parameters.\n        \"\"\"", "\n", "# memory", "\n", "assert", "params", ".", "mem_implementation", "in", "[", "'flat'", ",", "'pq_default'", ",", "'pq_fast'", "]", "\n", "params", ".", "mem_product_quantization", "=", "params", ".", "mem_implementation", "!=", "'flat'", "\n", "\n", "# optimization", "\n", "assert", "params", ".", "mem_grouped_conv", "is", "False", "or", "params", ".", "mem_multi_query_net", "\n", "params", ".", "mem_values_optimizer", "=", "params", ".", "optimizer", "if", "params", ".", "mem_values_optimizer", "==", "''", "else", "params", ".", "mem_values_optimizer", "\n", "params", ".", "mem_values_optimizer", "=", "params", ".", "mem_values_optimizer", ".", "replace", "(", "'adam'", ",", "'sparseadam'", ")", "if", "params", ".", "mem_sparse", "else", "params", ".", "mem_values_optimizer", "\n", "\n", "# even number of key dimensions for product quantization", "\n", "assert", "params", ".", "mem_k_dim", ">=", "2", "\n", "assert", "params", ".", "mem_product_quantization", "is", "False", "or", "params", ".", "mem_k_dim", "%", "2", "==", "0", "\n", "\n", "# memory type", "\n", "assert", "params", ".", "mem_keys_type", "in", "[", "'binary'", ",", "'gaussian'", ",", "'uniform'", "]", "\n", "\n", "# number of indices", "\n", "if", "params", ".", "mem_keys_type", "==", "'binary'", ":", "\n", "            ", "assert", "params", ".", "mem_keys_normalized_init", "is", "False", "\n", "assert", "1", "<<", "params", ".", "mem_k_dim", "==", "params", ".", "mem_n_keys", "\n", "", "if", "params", ".", "mem_product_quantization", ":", "\n", "            ", "params", ".", "n_indices", "=", "params", ".", "mem_n_keys", "**", "2", "\n", "", "else", ":", "\n", "            ", "params", ".", "n_indices", "=", "params", ".", "mem_n_keys", "\n", "\n", "# actual memory size", "\n", "", "if", "params", ".", "mem_modulo_size", "==", "-", "1", ":", "\n", "            ", "params", ".", "mem_size", "=", "params", ".", "n_indices", "\n", "", "else", ":", "\n", "            ", "assert", "1", "<=", "params", ".", "mem_modulo_size", "<", "params", ".", "n_indices", "\n", "params", ".", "mem_size", "=", "params", ".", "mem_modulo_size", "\n", "\n", "# different keys / different query MLP / shuffle hidden dimensions when no query network", "\n", "", "assert", "not", "params", ".", "mem_use_different_keys", "or", "params", ".", "mem_keys_type", "in", "[", "'gaussian'", ",", "'uniform'", "]", "\n", "assert", "not", "params", ".", "mem_use_different_keys", "or", "params", ".", "mem_heads", ">=", "2", "or", "params", ".", "mem_product_quantization", "\n", "assert", "not", "params", ".", "mem_multi_query_net", "or", "params", ".", "mem_heads", ">=", "2", "or", "params", ".", "mem_product_quantization", "\n", "assert", "not", "params", ".", "mem_multi_query_net", "or", "params", ".", "mem_query_layer_sizes", "not", "in", "[", "''", ",", "'0,0'", "]", "\n", "assert", "not", "params", ".", "mem_shuffle_query", "or", "params", ".", "mem_heads", ">", "1", "and", "params", ".", "mem_query_layer_sizes", "==", "''", "\n", "\n", "# query network", "\n", "if", "params", ".", "mem_query_layer_sizes", "==", "''", ":", "\n", "            ", "assert", "params", ".", "mem_heads", "==", "1", "or", "params", ".", "mem_use_different_keys", "or", "params", ".", "mem_shuffle_query", "\n", "", "else", ":", "\n", "            ", "s", "=", "[", "int", "(", "x", ")", "for", "x", "in", "filter", "(", "None", ",", "params", ".", "mem_query_layer_sizes", ".", "split", "(", "','", ")", ")", "]", "\n", "assert", "len", "(", "s", ")", ">=", "2", "and", "s", "[", "0", "]", "==", "s", "[", "-", "1", "]", "==", "0", "\n", "params", ".", "mem_query_layer_sizes", "=", "s", "\n", "assert", "not", "params", ".", "mem_query_residual", "or", "params", ".", "mem_input2d", "\n", "\n", "# convolutional query network kernel sizes", "\n", "", "if", "params", ".", "mem_query_kernel_sizes", "==", "''", ":", "\n", "            ", "assert", "not", "params", ".", "mem_input2d", "or", "params", ".", "mem_query_layer_sizes", "==", "''", "\n", "", "else", ":", "\n", "            ", "assert", "params", ".", "mem_input2d", "\n", "s", "=", "[", "int", "(", "x", ")", "for", "x", "in", "filter", "(", "None", ",", "params", ".", "mem_query_kernel_sizes", ".", "split", "(", "','", ")", ")", "]", "\n", "params", ".", "mem_query_kernel_sizes", "=", "s", "\n", "assert", "all", "(", "ks", "%", "2", "==", "1", "for", "ks", "in", "s", ")", "\n", "assert", "len", "(", "params", ".", "mem_query_kernel_sizes", ")", "==", "len", "(", "params", ".", "mem_query_layer_sizes", ")", "-", "1", ">=", "1", "\n", "\n", "# scoring", "\n", "", "assert", "params", ".", "mem_score_subtract", "in", "[", "''", ",", "'min'", ",", "'mean'", ",", "'median'", "]", "\n", "assert", "params", ".", "mem_score_subtract", "==", "''", "or", "params", ".", "mem_knn", ">=", "2", "\n", "assert", "not", "(", "params", ".", "mem_score_normalize", "and", "params", ".", "mem_score_softmax", "and", "params", ".", "mem_score_subtract", "==", "''", ")", "\n", "\n", "# dropout", "\n", "assert", "0", "<=", "params", ".", "mem_input_dropout", "<", "1", "\n", "assert", "0", "<=", "params", ".", "mem_query_dropout", "<", "1", "\n", "assert", "0", "<=", "params", ".", "mem_value_dropout", "<", "1", "\n", "\n", "# query batchnorm", "\n", "if", "params", ".", "mem_query_batchnorm", ":", "\n", "            ", "logger", ".", "warning", "(", "\"WARNING: if you use batch normalization, be sure that you use batches of sentences with the same size at training time. Otherwise, the padding token will result in incorrect mean/variance estimations in the BatchNorm layer.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryFlat.__init__": [[411, 415], ["memory.HashingMemory.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_dim", ",", "output_dim", ",", "params", ")", "\n", "assert", "self", ".", "use_different_keys", "is", "False", "or", "self", ".", "heads", ">=", "2", "\n", "assert", "not", "self", ".", "product_quantization", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryFlat.init_keys": [[416, 448], ["torch.FloatTensor", "range", "math.sqrt", "torch.nn.Parameter", "memory.HashingMemoryFlat.register_buffer", "range", "torch.from_numpy().view", "torch.from_numpy", "int", "init", "torch.from_numpy", "numpy.array", "init", "range"], "methods", ["None"], ["", "def", "init_keys", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialize keys.\n        \"\"\"", "\n", "assert", "self", ".", "keys_type", "in", "[", "'binary'", ",", "'gaussian'", ",", "'uniform'", "]", "\n", "\n", "# binary keys", "\n", "if", "self", ".", "keys_type", "==", "'binary'", ":", "\n", "            ", "keys", "=", "torch", ".", "FloatTensor", "(", "2", "**", "self", ".", "k_dim", ",", "self", ".", "k_dim", ")", "\n", "for", "i", "in", "range", "(", "keys", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "keys", ".", "shape", "[", "1", "]", ")", ":", "\n", "                    ", "keys", "[", "i", ",", "j", "]", "=", "int", "(", "(", "1", "<<", "j", ")", "&", "i", ">", "0", ")", "\n", "", "", "keys", "*=", "2", "\n", "keys", "-=", "1", "\n", "keys", "/=", "math", ".", "sqrt", "(", "self", ".", "k_dim", ")", "\n", "\n", "# random keys from Gaussian or uniform distributions", "\n", "", "if", "self", ".", "keys_type", "in", "[", "'gaussian'", ",", "'uniform'", "]", ":", "\n", "            ", "init", "=", "get_gaussian_keys", "if", "self", ".", "keys_type", "==", "'gaussian'", "else", "get_uniform_keys", "\n", "if", "self", ".", "use_different_keys", ":", "\n", "                ", "keys", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "\n", "init", "(", "self", ".", "n_indices", ",", "self", ".", "k_dim", ",", "self", ".", "keys_normalized_init", ",", "seed", "=", "i", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "heads", ")", "\n", "]", ")", ")", ".", "view", "(", "self", ".", "heads", ",", "self", ".", "n_indices", ",", "self", ".", "k_dim", ")", "\n", "", "else", ":", "\n", "                ", "keys", "=", "torch", ".", "from_numpy", "(", "init", "(", "self", ".", "n_indices", ",", "self", ".", "k_dim", ",", "self", ".", "keys_normalized_init", ",", "seed", "=", "0", ")", ")", "\n", "\n", "# learned or fixed keys", "\n", "", "", "if", "self", ".", "learn_keys", ":", "\n", "            ", "self", ".", "keys", "=", "nn", ".", "Parameter", "(", "keys", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'keys'", ",", "keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryFlat._get_indices": [[480, 498], ["torch.nn.functional.linear", "torch.nn.functional.linear.topk", "query.dim", "query.size", "query.norm().expand_as", "query.norm"], "methods", ["None"], ["", "", "def", "_get_indices", "(", "self", ",", "query", ",", "knn", ",", "keys", ")", ":", "\n", "        ", "\"\"\"\n        Generate scores and indices given keys and unnormalized queries.\n        \"\"\"", "\n", "assert", "query", ".", "dim", "(", ")", "==", "2", "and", "query", ".", "size", "(", "1", ")", "==", "self", ".", "k_dim", "\n", "\n", "# optionally normalize queries", "\n", "if", "self", ".", "normalize_query", ":", "\n", "            ", "query", "=", "query", "/", "query", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "query", ")", "# (bs, kdim)", "\n", "\n", "# compute scores with indices", "\n", "", "scores", "=", "F", ".", "linear", "(", "query", ",", "keys", ",", "bias", "=", "None", ")", "# (bs, n_keys)", "\n", "scores", ",", "indices", "=", "scores", ".", "topk", "(", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "# (bs, knn) ** 2", "\n", "# scores, indices = get_knn_faiss(keys.float(), query.float().contiguous(), knn, distance='dot_product')   # (bs, knn) ** 2", "\n", "\n", "# return scores with indices", "\n", "assert", "scores", ".", "shape", "==", "indices", ".", "shape", "==", "(", "query", ".", "shape", "[", "0", "]", ",", "knn", ")", "\n", "return", "scores", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryFlat.get_indices": [[499, 516], ["memory.HashingMemoryFlat._get_indices", "len", "query.view.view.view", "torch.cat().view", "torch.cat().view", "query.view.view.dim", "query.view.view.size", "memory.HashingMemoryFlat._get_indices", "range", "torch.cat", "torch.cat", "s.unsqueeze", "idx.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast._get_indices", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast._get_indices"], ["", "def", "get_indices", "(", "self", ",", "query", ",", "knn", ")", ":", "\n", "        ", "\"\"\"\n        Generate scores and indices given unnormalized queries.\n        \"\"\"", "\n", "assert", "query", ".", "dim", "(", ")", "==", "2", "and", "query", ".", "size", "(", "1", ")", "==", "self", ".", "k_dim", "\n", "if", "self", ".", "use_different_keys", "is", "False", ":", "\n", "            ", "return", "self", ".", "_get_indices", "(", "query", ",", "knn", ",", "self", ".", "keys", ")", "\n", "", "else", ":", "\n", "            ", "bs", "=", "len", "(", "query", ")", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "self", ".", "heads", ",", "self", ".", "k_dim", ")", "\n", "outputs", "=", "[", "\n", "self", ".", "_get_indices", "(", "query", "[", ":", ",", "i", "]", ",", "knn", ",", "self", ".", "keys", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "heads", ")", "\n", "]", "\n", "scores", "=", "torch", ".", "cat", "(", "[", "s", ".", "unsqueeze", "(", "1", ")", "for", "s", ",", "_", "in", "outputs", "]", ",", "1", ")", ".", "view", "(", "bs", ",", "knn", ")", "\n", "indices", "=", "torch", ".", "cat", "(", "[", "idx", ".", "unsqueeze", "(", "1", ")", "for", "_", ",", "idx", "in", "outputs", "]", ",", "1", ")", ".", "view", "(", "bs", ",", "knn", ")", "\n", "return", "scores", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.__init__": [[520, 524], ["memory.HashingMemory.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_dim", ",", "output_dim", ",", "params", ")", "\n", "assert", "self", ".", "k_dim", "%", "2", "==", "0", "\n", "assert", "self", ".", "product_quantization", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.create_keys": [[525, 557], ["int", "torch.FloatTensor", "range", "math.sqrt", "range", "torch.from_numpy().view", "torch.from_numpy", "int", "init", "torch.from_numpy", "numpy.array", "init", "range", "range"], "methods", ["None"], ["", "def", "create_keys", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This function creates keys and returns them.\n        I guess you could see that from the name of the function and the fact that is has a return statement.\n        \"\"\"", "\n", "assert", "self", ".", "keys_type", "in", "[", "'binary'", ",", "'gaussian'", ",", "'uniform'", "]", "\n", "half", "=", "self", ".", "k_dim", "//", "2", "\n", "n_keys", "=", "int", "(", "self", ".", "n_indices", "**", "0.5", ")", "\n", "\n", "# binary keys", "\n", "if", "self", ".", "keys_type", "==", "'binary'", ":", "\n", "            ", "keys", "=", "torch", ".", "FloatTensor", "(", "2", "**", "half", ",", "half", ")", "\n", "for", "i", "in", "range", "(", "keys", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "keys", ".", "shape", "[", "1", "]", ")", ":", "\n", "                    ", "keys", "[", "i", ",", "j", "]", "=", "int", "(", "(", "1", "<<", "j", ")", "&", "i", ">", "0", ")", "\n", "", "", "keys", "*=", "2", "\n", "keys", "-=", "1", "\n", "keys", "/=", "math", ".", "sqrt", "(", "self", ".", "k_dim", ")", "\n", "\n", "# random keys from Gaussian or uniform distributions", "\n", "", "if", "self", ".", "keys_type", "in", "[", "'gaussian'", ",", "'uniform'", "]", ":", "\n", "            ", "init", "=", "get_gaussian_keys", "if", "self", ".", "keys_type", "==", "'gaussian'", "else", "get_uniform_keys", "\n", "if", "self", ".", "use_different_keys", ":", "\n", "                ", "keys", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "\n", "init", "(", "n_keys", ",", "half", ",", "self", ".", "keys_normalized_init", ",", "seed", "=", "(", "2", "*", "i", "+", "j", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "heads", ")", "\n", "for", "j", "in", "range", "(", "2", ")", "\n", "]", ")", ")", ".", "view", "(", "self", ".", "heads", ",", "2", ",", "n_keys", ",", "half", ")", "\n", "", "else", ":", "\n", "                ", "keys", "=", "torch", ".", "from_numpy", "(", "init", "(", "n_keys", ",", "half", ",", "self", ".", "keys_normalized_init", ",", "seed", "=", "0", ")", ")", "\n", "\n", "", "", "return", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.init_keys": [[558, 569], ["memory.HashingMemoryProduct.create_keys", "torch.nn.Parameter", "memory.HashingMemoryProduct.register_buffer"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.create_keys"], ["", "def", "init_keys", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialize keys.\n        \"\"\"", "\n", "keys", "=", "self", ".", "create_keys", "(", ")", "\n", "\n", "# learned or fixed keys", "\n", "if", "self", ".", "learn_keys", ":", "\n", "            ", "self", ".", "keys", "=", "nn", ".", "Parameter", "(", "keys", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'keys'", ",", "keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct._get_indices": [[570, 615], ["len", "len", "len", "torch.no_grad", "utils.get_knn_faiss", "utils.get_knn_faiss", "utils.cartesian_product", "utils.cartesian_product", "utils.cartesian_product.sum", "torch.topk", "all_indices.gather", "query.dim", "query.size", "q1.norm().expand_as", "q2.norm().expand_as", "keys1.float", "q1.float", "keys2.float", "q2.float", "q1.unsqueeze", "q2.unsqueeze", "q1.norm", "q2.norm"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_knn_faiss", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.get_knn_faiss", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.cartesian_product", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.utils.cartesian_product"], ["", "", "def", "_get_indices", "(", "self", ",", "query", ",", "knn", ",", "keys1", ",", "keys2", ")", ":", "\n", "        ", "\"\"\"\n        Generate scores and indices given keys and unnormalized queries.\n        \"\"\"", "\n", "assert", "query", ".", "dim", "(", ")", "==", "2", "and", "query", ".", "size", "(", "1", ")", "==", "self", ".", "k_dim", "\n", "assert", "len", "(", "keys1", ")", "==", "len", "(", "keys2", ")", "\n", "half", "=", "self", ".", "k_dim", "//", "2", "\n", "n_keys", "=", "len", "(", "keys1", ")", "\n", "\n", "# split query for product quantization", "\n", "q1", "=", "query", "[", ":", ",", ":", "half", "]", "# (bs, half)", "\n", "q2", "=", "query", "[", ":", ",", "half", ":", "]", "# (bs, half)", "\n", "\n", "# optionally normalize queries", "\n", "if", "self", ".", "normalize_query", ":", "\n", "            ", "q1", "=", "q1", "/", "q1", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "q1", ")", "# (bs, half)", "\n", "q2", "=", "q2", "/", "q2", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "q2", ")", "# (bs, half)", "\n", "\n", "# compute memory value indices", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# compute indices with associated scores", "\n", "            ", "scores1", ",", "indices1", "=", "get_knn_faiss", "(", "keys1", ".", "float", "(", ")", ",", "q1", ".", "float", "(", ")", ",", "knn", ",", "distance", "=", "'dot_product'", ")", "# (bs, knn) ** 2", "\n", "scores2", ",", "indices2", "=", "get_knn_faiss", "(", "keys2", ".", "float", "(", ")", ",", "q2", ".", "float", "(", ")", ",", "knn", ",", "distance", "=", "'dot_product'", ")", "# (bs, knn) ** 2", "\n", "\n", "# cartesian product on best candidate keys", "\n", "concat_scores", "=", "cartesian_product", "(", "scores1", ",", "scores2", ")", "# (bs, knn ** 2, 2)", "\n", "concat_indices", "=", "cartesian_product", "(", "indices1", ",", "indices2", ")", "# (bs, knn ** 2, 2)", "\n", "\n", "all_scores", "=", "concat_scores", ".", "sum", "(", "2", ")", "# (bs, knn ** 2)", "\n", "all_indices", "=", "concat_indices", "[", ":", ",", ":", ",", "0", "]", "*", "n_keys", "+", "concat_indices", "[", ":", ",", ":", ",", "1", "]", "# (bs, knn ** 2)", "\n", "\n", "_scores", ",", "best_indices", "=", "torch", ".", "topk", "(", "all_scores", ",", "k", "=", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "# (bs, knn)", "\n", "indices", "=", "all_indices", ".", "gather", "(", "1", ",", "best_indices", ")", "# (bs, knn)", "\n", "\n", "# compute value scores - for some reason, this part is extremely slow when the keys are learned", "\n", "", "indices1", "=", "indices", "/", "n_keys", "\n", "indices2", "=", "indices", "%", "n_keys", "\n", "scores1", "=", "(", "keys1", "[", "indices1", "]", "*", "q1", ".", "unsqueeze", "(", "1", ")", ")", ".", "sum", "(", "2", ")", "\n", "scores2", "=", "(", "keys2", "[", "indices2", "]", "*", "q2", ".", "unsqueeze", "(", "1", ")", ")", ".", "sum", "(", "2", ")", "\n", "scores", "=", "scores1", "+", "scores2", "\n", "\n", "# return scores with indices", "\n", "assert", "scores", ".", "shape", "==", "indices", ".", "shape", "==", "(", "query", ".", "shape", "[", "0", "]", ",", "knn", ")", "\n", "return", "scores", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProduct.get_indices": [[616, 633], ["memory.HashingMemoryProduct._get_indices", "len", "query.view.view.view", "torch.cat().view", "torch.cat().view", "query.view.view.dim", "query.view.view.size", "memory.HashingMemoryProduct._get_indices", "range", "torch.cat", "torch.cat", "s.unsqueeze", "idx.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast._get_indices", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast._get_indices"], ["", "def", "get_indices", "(", "self", ",", "query", ",", "knn", ")", ":", "\n", "        ", "\"\"\"\n        Generate scores and indices given unnormalized queries.\n        \"\"\"", "\n", "assert", "query", ".", "dim", "(", ")", "==", "2", "and", "query", ".", "size", "(", "1", ")", "==", "self", ".", "k_dim", "\n", "if", "self", ".", "use_different_keys", "is", "False", ":", "\n", "            ", "return", "self", ".", "_get_indices", "(", "query", ",", "knn", ",", "self", ".", "keys", ",", "self", ".", "keys", ")", "\n", "", "else", ":", "\n", "            ", "bs", "=", "len", "(", "query", ")", "\n", "query", "=", "query", ".", "view", "(", "-", "1", ",", "self", ".", "heads", ",", "self", ".", "k_dim", ")", "\n", "outputs", "=", "[", "\n", "self", ".", "_get_indices", "(", "query", "[", ":", ",", "i", "]", ",", "knn", ",", "self", ".", "keys", "[", "i", "]", "[", "0", "]", ",", "self", ".", "keys", "[", "i", "]", "[", "1", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "heads", ")", "\n", "]", "\n", "scores", "=", "torch", ".", "cat", "(", "[", "s", ".", "unsqueeze", "(", "1", ")", "for", "s", ",", "_", "in", "outputs", "]", ",", "1", ")", ".", "view", "(", "bs", ",", "knn", ")", "\n", "indices", "=", "torch", ".", "cat", "(", "[", "idx", ".", "unsqueeze", "(", "1", ")", "for", "_", ",", "idx", "in", "outputs", "]", ",", "1", ")", ".", "view", "(", "bs", ",", "knn", ")", "\n", "return", "scores", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast.__init__": [[637, 639], ["memory.HashingMemoryProduct.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_dim", ",", "output_dim", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.memory.memory.HashingMemoryProductFast._get_indices": [[640, 688], ["query.size", "len", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear.topk", "torch.nn.functional.linear.topk", "torch.topk", "all_indices.gather", "len", "len", "query.dim", "query.size", "q1.norm().expand_as", "q2.norm().expand_as", "torch.nn.functional.linear.view().expand", "torch.nn.functional.linear.view().expand", "indices2.view().expand", "q1.norm", "q2.norm", "indices1.view().expand", "torch.nn.functional.linear.view", "torch.nn.functional.linear.view", "indices2.view", "indices1.view"], "methods", ["None"], ["", "def", "_get_indices", "(", "self", ",", "query", ",", "knn", ",", "keys1", ",", "keys2", ")", ":", "\n", "        ", "\"\"\"\n        Generate scores and indices given keys and unnormalized queries.\n        \"\"\"", "\n", "assert", "query", ".", "dim", "(", ")", "==", "2", "and", "query", ".", "size", "(", "1", ")", "==", "self", ".", "k_dim", "\n", "assert", "len", "(", "keys1", ")", "==", "len", "(", "keys2", ")", "\n", "bs", "=", "query", ".", "size", "(", "0", ")", "\n", "half", "=", "self", ".", "k_dim", "//", "2", "\n", "n_keys", "=", "len", "(", "keys1", ")", "\n", "\n", "# split query for product quantization", "\n", "q1", "=", "query", "[", ":", ",", ":", "half", "]", "# (bs, half)", "\n", "q2", "=", "query", "[", ":", ",", "half", ":", "]", "# (bs, half)", "\n", "\n", "# optionally normalize queries", "\n", "if", "self", ".", "normalize_query", ":", "\n", "            ", "q1", "=", "q1", "/", "q1", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "q1", ")", "# (bs, half)", "\n", "q2", "=", "q2", "/", "q2", ".", "norm", "(", "2", ",", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "q2", ")", "# (bs, half)", "\n", "\n", "# compute indices with associated scores", "\n", "", "scores1", "=", "F", ".", "linear", "(", "q1", ",", "keys1", ",", "bias", "=", "None", ")", "# (bs, n_keys ** 0.5)", "\n", "scores2", "=", "F", ".", "linear", "(", "q2", ",", "keys2", ",", "bias", "=", "None", ")", "# (bs, n_keys ** 0.5)", "\n", "scores1", ",", "indices1", "=", "scores1", ".", "topk", "(", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "# (bs, knn) ** 2", "\n", "scores2", ",", "indices2", "=", "scores2", ".", "topk", "(", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "# (bs, knn) ** 2", "\n", "# scores1, indices1 = get_knn_faiss(keys1, q1.contiguous(), knn, distance='dot_product')                        # (bs, knn) ** 2", "\n", "# scores2, indices2 = get_knn_faiss(keys2, q2.contiguous(), knn, distance='dot_product')                        # (bs, knn) ** 2", "\n", "\n", "# cartesian product on best candidate keys", "\n", "all_scores", "=", "(", "\n", "scores1", ".", "view", "(", "bs", ",", "knn", ",", "1", ")", ".", "expand", "(", "bs", ",", "knn", ",", "knn", ")", "+", "\n", "scores2", ".", "view", "(", "bs", ",", "1", ",", "knn", ")", ".", "expand", "(", "bs", ",", "knn", ",", "knn", ")", "\n", ")", ".", "view", "(", "bs", ",", "-", "1", ")", "# (bs, knn ** 2)", "\n", "all_indices", "=", "(", "\n", "indices1", ".", "view", "(", "bs", ",", "knn", ",", "1", ")", ".", "expand", "(", "bs", ",", "knn", ",", "knn", ")", "*", "n_keys", "+", "\n", "indices2", ".", "view", "(", "bs", ",", "1", ",", "knn", ")", ".", "expand", "(", "bs", ",", "knn", ",", "knn", ")", "\n", ")", ".", "view", "(", "bs", ",", "-", "1", ")", "# (bs, knn ** 2)", "\n", "\n", "# select overall best scores and indices", "\n", "scores", ",", "best_indices", "=", "torch", ".", "topk", "(", "all_scores", ",", "k", "=", "knn", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "# (bs, knn)", "\n", "indices", "=", "all_indices", ".", "gather", "(", "1", ",", "best_indices", ")", "# (bs, knn)", "\n", "\n", "# code below: debug instant retrieval speed", "\n", "# scores = torch.zeros(bs, knn, dtype=query.dtype, device=query.device)", "\n", "# indices = torch.arange(knn, dtype=torch.int64, device=query.device).view(1, knn).expand(bs, knn)", "\n", "\n", "# return scores with indices", "\n", "assert", "scores", ".", "shape", "==", "indices", ".", "shape", "==", "(", "bs", ",", "knn", ")", "\n", "return", "scores", ",", "indices", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.__init__": [[87, 103], ["torch.Module.__init__", "mass_transformer.Linear", "torch.AdaptiveLogSoftmaxWithLoss", "torch.AdaptiveLogSoftmaxWithLoss", "torch.AdaptiveLogSoftmaxWithLoss"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "asm", "=", "params", ".", "asm", "\n", "self", ".", "n_words", "=", "params", ".", "n_words", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "dim", "=", "params", ".", "emb_dim", "\n", "\n", "if", "params", ".", "asm", "is", "False", ":", "\n", "            ", "self", ".", "proj", "=", "Linear", "(", "dim", ",", "params", ".", "n_words", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "AdaptiveLogSoftmaxWithLoss", "(", "\n", "in_features", "=", "dim", ",", "\n", "n_classes", "=", "params", ".", "n_words", ",", "\n", "cutoffs", "=", "params", ".", "asm_cutoffs", ",", "\n", "div_value", "=", "params", ".", "asm_div_value", ",", "\n", "head_bias", "=", "True", ",", "# default is False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.forward": [[105, 119], ["mass_transformer.PredLayer.proj().view", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "mass_transformer.PredLayer.proj", "mass_transformer.PredLayer.proj.log_prob", "mass_transformer.PredLayer.proj"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "get_scores", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Compute the loss, and optionally the scores.\n        \"\"\"", "\n", "assert", "(", "y", "==", "self", ".", "pad_index", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "0", "\n", "\n", "if", "self", ".", "asm", "is", "False", ":", "\n", "            ", "scores", "=", "self", ".", "proj", "(", "x", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_words", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "y", ",", "reduction", "=", "'elementwise_mean'", ")", "\n", "", "else", ":", "\n", "            ", "_", ",", "loss", "=", "self", ".", "proj", "(", "x", ",", "y", ")", "\n", "scores", "=", "self", ".", "proj", ".", "log_prob", "(", "x", ")", "if", "get_scores", "else", "None", "\n", "\n", "", "return", "scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores": [[120, 126], ["x.dim", "mass_transformer.PredLayer.proj.log_prob", "mass_transformer.PredLayer.proj"], "methods", ["None"], ["", "def", "get_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Compute scores.\n        \"\"\"", "\n", "assert", "x", ".", "dim", "(", ")", "==", "2", "\n", "return", "self", ".", "proj", ".", "log_prob", "(", "x", ")", "if", "self", ".", "asm", "else", "self", ".", "proj", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.MultiHeadAttention.__init__": [[131, 149], ["torch.Module.__init__", "next", "mass_transformer.Linear", "mass_transformer.Linear", "mass_transformer.Linear", "mass_transformer.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "mass_transformer.MultiHeadAttention.out_lin.append", "mass_transformer.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["def", "__init__", "(", "self", ",", "n_heads", ",", "dim", ",", "dropout", ",", "n_langs", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layer_id", "=", "next", "(", "MultiHeadAttention", ".", "NEW_ID", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "n_heads", "=", "n_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "assert", "self", ".", "dim", "%", "self", ".", "n_heads", "==", "0", "\n", "\n", "self", ".", "q_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "k_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "v_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "n_langs", "=", "n_langs", "\n", "if", "n_langs", "is", "None", ":", "\n", "            ", "self", ".", "out_lin", "=", "Linear", "(", "dim", ",", "dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out_lin", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "n_langs", ")", ":", "\n", "                ", "self", ".", "out_lin", ".", "append", "(", "Linear", "(", "dim", ",", "dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.MultiHeadAttention.forward": [[150, 207], ["input.size", "mass_transformer.MultiHeadAttention.forward.shape"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "input", ",", "mask", ",", "kv", "=", "None", ",", "cache", "=", "None", ",", "segment_label", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n        \"\"\"", "\n", "# Input is (bs, qlen, dim)", "\n", "# Mask is (bs, klen) (non-causal) or (bs, klen, klen)", "\n", "bs", ",", "qlen", ",", "dim", "=", "input", ".", "size", "(", ")", "\n", "if", "kv", "is", "None", ":", "\n", "            ", "klen", "=", "qlen", "if", "cache", "is", "None", "else", "cache", "[", "'slen'", "]", "+", "qlen", "\n", "", "else", ":", "\n", "            ", "klen", "=", "kv", ".", "size", "(", "1", ")", "\n", "", "assert", "dim", "==", "self", ".", "dim", ",", "'Dimensions do not match: %s input vs %s configured'", "%", "(", "dim", ",", "self", ".", "dim", ")", "\n", "n_heads", "=", "self", ".", "n_heads", "\n", "dim_per_head", "=", "dim", "//", "n_heads", "\n", "mask_reshape", "=", "(", "bs", ",", "1", ",", "qlen", ",", "klen", ")", "if", "mask", ".", "dim", "(", ")", "==", "3", "else", "(", "bs", ",", "1", ",", "1", ",", "klen", ")", "\n", "\n", "def", "shape", "(", "x", ")", ":", "\n", "            ", "\"\"\"  projection \"\"\"", "\n", "return", "x", ".", "view", "(", "bs", ",", "-", "1", ",", "self", ".", "n_heads", ",", "dim_per_head", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "", "def", "unshape", "(", "x", ")", ":", "\n", "            ", "\"\"\"  compute context \"\"\"", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "bs", ",", "-", "1", ",", "self", ".", "n_heads", "*", "dim_per_head", ")", "\n", "\n", "", "q", "=", "shape", "(", "self", ".", "q_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "if", "kv", "is", "None", ":", "\n", "            ", "k", "=", "shape", "(", "self", ".", "k_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "v", "=", "shape", "(", "self", ".", "v_lin", "(", "input", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "", "elif", "cache", "is", "None", "or", "self", ".", "layer_id", "not", "in", "cache", ":", "\n", "            ", "k", "=", "v", "=", "kv", "\n", "k", "=", "shape", "(", "self", ".", "k_lin", "(", "k", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "v", "=", "shape", "(", "self", ".", "v_lin", "(", "v", ")", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "layer_id", "in", "cache", ":", "\n", "                ", "if", "kv", "is", "None", ":", "\n", "                    ", "k_", ",", "v_", "=", "cache", "[", "self", ".", "layer_id", "]", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k_", ",", "k", "]", ",", "dim", "=", "2", ")", "# (bs, n_heads, klen, dim_per_head)", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v_", ",", "v", "]", ",", "dim", "=", "2", ")", "# (bs, n_heads, klen, dim_per_head)", "\n", "", "else", ":", "\n", "                    ", "k", ",", "v", "=", "cache", "[", "self", ".", "layer_id", "]", "\n", "", "", "cache", "[", "self", ".", "layer_id", "]", "=", "(", "k", ",", "v", ")", "\n", "\n", "", "q", "=", "q", "/", "math", ".", "sqrt", "(", "dim_per_head", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "scores", "=", "torch", ".", "matmul", "(", "q", ",", "k", ".", "transpose", "(", "2", ",", "3", ")", ")", "# (bs, n_heads, qlen, klen)", "\n", "mask", "=", "(", "mask", "==", "0", ")", ".", "view", "(", "mask_reshape", ")", ".", "expand_as", "(", "scores", ")", "# (bs, n_heads, qlen, klen)", "\n", "scores", ".", "masked_fill_", "(", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "# (bs, n_heads, qlen, klen)", "\n", "\n", "weights", "=", "F", ".", "softmax", "(", "scores", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ".", "type_as", "(", "scores", ")", "# (bs, n_heads, qlen, klen)", "\n", "weights", "=", "F", ".", "dropout", "(", "weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "# (bs, n_heads, qlen, klen)", "\n", "context", "=", "torch", ".", "matmul", "(", "weights", ",", "v", ")", "# (bs, n_heads, qlen, dim_per_head)", "\n", "context", "=", "unshape", "(", "context", ")", "# (bs, qlen, dim)", "\n", "\n", "if", "self", ".", "n_langs", "is", "None", ":", "\n", "            ", "return", "self", ".", "out_lin", "(", "context", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "out_lin", "[", "segment_label", "]", "(", "context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerFFN.__init__": [[211, 217], ["torch.Module.__init__", "mass_transformer.Linear", "mass_transformer.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "dim_hidden", ",", "out_dim", ",", "dropout", ",", "gelu_activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "lin1", "=", "Linear", "(", "in_dim", ",", "dim_hidden", ")", "\n", "self", ".", "lin2", "=", "Linear", "(", "dim_hidden", ",", "out_dim", ")", "\n", "self", ".", "act", "=", "gelu", "if", "gelu_activation", "else", "F", ".", "relu", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerFFN.forward": [[218, 224], ["mass_transformer.TransformerFFN.lin1", "mass_transformer.TransformerFFN.act", "mass_transformer.TransformerFFN.lin2", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "x", "=", "self", ".", "lin1", "(", "input", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "lin2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.__init__": [[230, 306], ["torch.Module.__init__", "getattr", "mass_transformer.Embedding", "mass_transformer.Embedding", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "getattr", "range", "len", "len", "len", "mass_transformer.create_sinusoidal_embeddings", "mass_transformer.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "mass_transformer.TransformerModel.attentions.append", "mass_transformer.TransformerModel.layer_norm1.append", "mass_transformer.TransformerModel.ffns.append", "mass_transformer.TransformerModel.layer_norm2.append", "mass_transformer.PredLayer", "mass_transformer.MultiHeadAttention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "mass_transformer.TransformerModel.layer_norm15.append", "mass_transformer.TransformerFFN", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "mass_transformer.TransformerModel.encoder_attn.append", "mass_transformer.MultiHeadAttention", "mass_transformer.TransformerModel.encoder_attn.append", "mass_transformer.TransformerModel.encoder_attn.append", "mass_transformer.MultiHeadAttention", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "mass_transformer.MultiHeadAttention", "range"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.create_sinusoidal_embeddings", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding"], ["def", "__init__", "(", "self", ",", "params", ",", "dico", ",", "is_encoder", ",", "with_output", ")", ":", "\n", "        ", "\"\"\"\n        Transformer model (encoder or decoder).\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# encoder / decoder, output layer", "\n", "self", ".", "is_encoder", "=", "is_encoder", "\n", "self", ".", "is_decoder", "=", "not", "is_encoder", "\n", "self", ".", "with_output", "=", "with_output", "\n", "\n", "# dictionary / languages", "\n", "self", ".", "n_langs", "=", "params", ".", "n_langs", "\n", "self", ".", "n_words", "=", "params", ".", "n_words", "\n", "self", ".", "eos_index", "=", "params", ".", "eos_index", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "self", ".", "dico", "=", "dico", "\n", "self", ".", "id2lang", "=", "params", ".", "id2lang", "\n", "self", ".", "lang2id", "=", "params", ".", "lang2id", "\n", "self", ".", "english_only", "=", "getattr", "(", "params", ",", "'english_only'", ",", "False", ")", "\n", "assert", "len", "(", "self", ".", "dico", ")", "==", "self", ".", "n_words", "\n", "assert", "len", "(", "self", ".", "id2lang", ")", "==", "len", "(", "self", ".", "lang2id", ")", "==", "self", ".", "n_langs", "\n", "\n", "# model parameters", "\n", "self", ".", "dim", "=", "params", ".", "emb_dim", "# 512 by default", "\n", "self", ".", "hidden_dim", "=", "self", ".", "dim", "*", "4", "# 2048 by default", "\n", "self", ".", "n_heads", "=", "params", ".", "n_heads", "# 8 by default", "\n", "self", ".", "n_layers", "=", "params", ".", "n_layers", "if", "is_encoder", "else", "params", ".", "n_dec_layers", "\n", "self", ".", "dropout", "=", "params", ".", "dropout", "\n", "self", ".", "attention_dropout", "=", "params", ".", "attention_dropout", "\n", "assert", "self", ".", "dim", "%", "self", ".", "n_heads", "==", "0", ",", "'transformer dim must be a multiple of n_heads'", "\n", "\n", "# embeddings", "\n", "self", ".", "position_embeddings", "=", "Embedding", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ")", "\n", "if", "params", ".", "sinusoidal_embeddings", ":", "\n", "            ", "create_sinusoidal_embeddings", "(", "N_MAX_POSITIONS", ",", "self", ".", "dim", ",", "out", "=", "self", ".", "position_embeddings", ".", "weight", ")", "\n", "", "if", "params", ".", "n_langs", ">", "1", "and", "self", ".", "english_only", "is", "False", ":", "\n", "            ", "self", ".", "lang_embeddings", "=", "Embedding", "(", "self", ".", "n_langs", ",", "self", ".", "dim", ")", "\n", "", "self", ".", "embeddings", "=", "Embedding", "(", "self", ".", "n_words", ",", "self", ".", "dim", ",", "padding_idx", "=", "self", ".", "pad_index", ")", "\n", "self", ".", "layer_norm_emb", "=", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", "\n", "\n", "# transformer layers", "\n", "self", ".", "attentions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm1", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "ffns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "layer_norm2", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "self", ".", "layer_norm15", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "encoder_attn", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "", "self", ".", "attention_setting", "=", "getattr", "(", "params", ",", "'attention_setting'", ",", "'v1'", ")", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "            ", "self", ".", "attentions", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", ")", "\n", "self", ".", "layer_norm1", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "if", "self", ".", "is_decoder", ":", "\n", "                ", "self", ".", "layer_norm15", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "if", "self", ".", "english_only", "is", "True", ":", "\n", "                    ", "self", ".", "encoder_attn", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", ")", "\n", "", "elif", "self", ".", "attention_setting", "==", "\"v1\"", ":", "\n", "                    ", "self", ".", "encoder_attn", ".", "append", "(", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ",", "\n", "n_langs", "=", "self", ".", "n_langs", ")", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "encoder_attn", ".", "append", "(", "nn", ".", "ModuleList", "(", "[", "\n", "MultiHeadAttention", "(", "self", ".", "n_heads", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "attention_dropout", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "n_langs", ")", "\n", "]", ")", ")", "\n", "", "", "self", ".", "ffns", ".", "append", "(", "TransformerFFN", "(", "self", ".", "dim", ",", "self", ".", "hidden_dim", ",", "self", ".", "dim", ",", "dropout", "=", "self", ".", "dropout", ",", "\n", "gelu_activation", "=", "params", ".", "gelu_activation", ")", ")", "\n", "self", ".", "layer_norm2", ".", "append", "(", "nn", ".", "LayerNorm", "(", "self", ".", "dim", ",", "eps", "=", "1e-12", ")", ")", "\n", "\n", "# output layer", "\n", "", "if", "self", ".", "with_output", ":", "\n", "            ", "self", ".", "pred_layer", "=", "PredLayer", "(", "params", ")", "\n", "if", "params", ".", "share_inout_emb", ":", "\n", "                ", "self", ".", "pred_layer", ".", "proj", ".", "weight", "=", "self", ".", "embeddings", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.forward": [[307, 318], ["mass_transformer.TransformerModel.fwd", "mass_transformer.TransformerModel.predict", "Exception"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.predict"], ["", "", "", "def", "forward", "(", "self", ",", "mode", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Forward function with different forward modes.\n        ### Small hack to handle PyTorch distributed.\n        \"\"\"", "\n", "if", "mode", "==", "'fwd'", ":", "\n", "            ", "return", "self", ".", "fwd", "(", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "'predict'", ":", "\n", "            ", "return", "self", ".", "predict", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown mode: %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.fwd": [[319, 416], ["x.transpose.transpose.size", "x.transpose.transpose.transpose", "mass_transformer.get_masks", "mass_transformer.TransformerModel.embeddings", "mass_transformer.TransformerModel.layer_norm_emb", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "range", "tensor.transpose.transpose.transpose", "lengths.size", "lengths.max().item", "x.transpose.transpose.new().long", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "positions.transpose.transpose.transpose", "langs.transpose.transpose.transpose", "mass_transformer.TransformerModel.position_embeddings().expand_as", "langs.transpose.transpose.max", "torch.dropout", "torch.dropout", "torch.dropout", "mask.unsqueeze().to", "tensor.transpose.transpose.size", "src_enc.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "positions.transpose.transpose.size", "langs.transpose.transpose.size", "mass_transformer.TransformerModel.lang_embeddings", "mask.unsqueeze", "torch.dropout", "torch.dropout", "torch.dropout", "lengths.max", "src_len.max", "x.transpose.transpose.new", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "mass_transformer.TransformerModel.position_embeddings", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.get_masks"], ["", "", "def", "fwd", "(", "self", ",", "x", ",", "lengths", ",", "causal", ",", "src_enc", "=", "None", ",", "src_len", "=", "None", ",", "positions", "=", "None", ",", "langs", "=", "None", ",", "cache", "=", "None", ",", "\n", "enc_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            `x` LongTensor(slen, bs), containing word indices\n            `lengths` LongTensor(bs), containing the length of each sentence\n            `causal` Boolean, if True, the attention is only done over previous hidden states\n            `positions` LongTensor(slen, bs), containing word positions\n            `langs` LongTensor(slen, bs), containing language IDs\n        \"\"\"", "\n", "# lengths = (x != self.pad_index).float().sum(dim=1)", "\n", "# mask = x != self.pad_index", "\n", "\n", "# check inputs", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "assert", "lengths", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "<=", "slen", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# batch size as dimension 0", "\n", "assert", "(", "src_enc", "is", "None", ")", "==", "(", "src_len", "is", "None", ")", "\n", "if", "src_enc", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "is_decoder", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generate masks", "\n", "", "mask", ",", "attn_mask", "=", "get_masks", "(", "slen", ",", "lengths", ",", "causal", ")", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "            ", "src_mask", "=", "torch", ".", "arange", "(", "src_len", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "<", "src_len", "[", ":", ",", "None", "]", "\n", "if", "enc_mask", "is", "not", "None", ":", "\n", "                ", "src_mask", "&=", "enc_mask", "\n", "\n", "# positions", "\n", "", "", "if", "positions", "is", "None", ":", "\n", "            ", "positions", "=", "x", ".", "new", "(", "slen", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "slen", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "assert", "positions", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "positions", "=", "positions", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# langs", "\n", "", "if", "langs", "is", "not", "None", ":", "\n", "            ", "assert", "langs", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "langs", "=", "langs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# do not recompute cached elements", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "_slen", "=", "slen", "-", "cache", "[", "'slen'", "]", "\n", "x", "=", "x", "[", ":", ",", "-", "_slen", ":", "]", "\n", "positions", "=", "positions", "[", ":", ",", "-", "_slen", ":", "]", "\n", "if", "langs", "is", "not", "None", ":", "\n", "                ", "langs", "=", "langs", "[", ":", ",", "-", "_slen", ":", "]", "\n", "", "mask", "=", "mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "attn_mask", "=", "attn_mask", "[", ":", ",", "-", "_slen", ":", "]", "\n", "\n", "# embeddings", "\n", "", "tensor", "=", "self", ".", "embeddings", "(", "x", ")", "\n", "tensor", "=", "tensor", "+", "self", ".", "position_embeddings", "(", "positions", ")", ".", "expand_as", "(", "tensor", ")", "\n", "if", "langs", "is", "not", "None", "and", "self", ".", "english_only", "is", "False", ":", "\n", "            ", "tensor", "=", "tensor", "+", "self", ".", "lang_embeddings", "(", "langs", ")", "\n", "", "tensor", "=", "self", ".", "layer_norm_emb", "(", "tensor", ")", "\n", "tensor", "=", "F", ".", "dropout", "(", "tensor", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "lang_id", "=", "langs", ".", "max", "(", ")", "if", "langs", "is", "not", "None", "else", "None", "\n", "\n", "# transformer layers", "\n", "for", "i", "in", "range", "(", "self", ".", "n_layers", ")", ":", "\n", "# self attention", "\n", "            ", "attn", "=", "self", ".", "attentions", "[", "i", "]", "(", "tensor", ",", "attn_mask", ",", "cache", "=", "cache", ")", "\n", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm1", "[", "i", "]", "(", "tensor", ")", "\n", "\n", "# encoder attention (for decoder only)", "\n", "if", "self", ".", "is_decoder", "and", "src_enc", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "english_only", "is", "True", ":", "\n", "                    ", "attn", "=", "self", ".", "encoder_attn", "[", "i", "]", "(", "tensor", ",", "src_mask", ",", "kv", "=", "src_enc", ",", "cache", "=", "cache", ")", "\n", "", "elif", "self", ".", "attention_setting", "==", "\"v1\"", ":", "\n", "                    ", "attn", "=", "self", ".", "encoder_attn", "[", "i", "]", "(", "tensor", ",", "src_mask", ",", "kv", "=", "src_enc", ",", "cache", "=", "cache", ",", "segment_label", "=", "lang_id", ")", "\n", "", "else", ":", "\n", "                    ", "attn", "=", "self", ".", "encoder_attn", "[", "i", "]", "[", "lang_id", "]", "(", "tensor", ",", "src_mask", ",", "kv", "=", "src_enc", ",", "cache", "=", "cache", ")", "\n", "", "attn", "=", "F", ".", "dropout", "(", "attn", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "tensor", "=", "tensor", "+", "attn", "\n", "tensor", "=", "self", ".", "layer_norm15", "[", "i", "]", "(", "tensor", ")", "\n", "\n", "# FFN", "\n", "", "tensor", "=", "tensor", "+", "self", ".", "ffns", "[", "i", "]", "(", "tensor", ")", "\n", "tensor", "=", "self", ".", "layer_norm2", "[", "i", "]", "(", "tensor", ")", "\n", "tensor", "*=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "to", "(", "tensor", ".", "dtype", ")", "\n", "\n", "# update cache length", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "cache", "[", "'slen'", "]", "+=", "tensor", ".", "size", "(", "1", ")", "\n", "\n", "# move back sequence length to dimension 0", "\n", "", "tensor", "=", "tensor", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.predict": [[417, 428], ["tensor[].view", "mass_transformer.TransformerModel.pred_layer", "pred_mask.unsqueeze().expand_as", "pred_mask.unsqueeze"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "tensor", ",", "pred_mask", ",", "y", ",", "get_scores", ")", ":", "\n", "        ", "\"\"\"\n        Given the last hidden state, compute word scores and/or the loss.\n            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n                we need to predict a word\n            `y` is a LongTensor of shape (pred_mask.sum(),)\n            `get_scores` is a boolean specifying whether we need to return scores\n        \"\"\"", "\n", "masked_tensor", "=", "tensor", "[", "pred_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "tensor", ")", "]", ".", "view", "(", "-", "1", ",", "self", ".", "dim", ")", "\n", "scores", ",", "loss", "=", "self", ".", "pred_layer", "(", "masked_tensor", ",", "y", ",", "get_scores", ")", "\n", "return", "scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.generate": [[429, 515], ["len", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "src_enc.size", "mass_transformer.TransformerModel.forward", "mass_transformer.TransformerModel.pred_layer.get_scores", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "generated[].masked_fill_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "mass_transformer.TransformerModel.size", "[].squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "src_len.clone().fill_.max", "src_len.clone().fill_.byte", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate", "(", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# input batch", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs, n_words)", "\n", "\n", "# select next words: sample or greedy", "\n", "if", "sample_temperature", "is", "None", ":", "\n", "                ", "next_words", "=", "torch", ".", "topk", "(", "scores", ",", "1", ")", "[", "1", "]", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "1", ")", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.generate_beam": [[516, 691], ["len", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "beam_scores.new.new.view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "enumerate", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "enumerate", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "mass_transformer.BeamHypotheses", "mass_transformer.TransformerModel.forward", "mass_transformer.TransformerModel.pred_layer.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "range", "beam_scores.new.new.new", "src_len.unsqueeze().expand().contiguous().view.new.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "cache.keys", "all", "best.append", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "range", "mass_transformer.TransformerModel.size", "torch.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "zip", "next_batch_beam.extend", "len", "max", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_len.unsqueeze().expand().contiguous().view.new.max().item", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "generated[].clone", "value.item", "src_len.unsqueeze().expand().contiguous().view.new.max", "src_enc.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_beam", "(", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "max_len", "=", "200", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# check inputs", "\n", "assert", "src_enc", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc", "=", "src_enc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc", "=", "src_enc", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache", "=", "cache", "\n", ")", "\n", "assert", "tensor", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor", "=", "tensor", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores", "=", "self", ".", "pred_layer", ".", "get_scores", "(", "tensor", ")", "# (bs * beam_size, n_words)", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                        ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "\n", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "                ", "if", "k", "!=", "'slen'", ":", "\n", "                    ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# visualize hypotheses", "\n", "# print([len(x) for x in generated_hyps], cur_len)", "\n", "# globals().update( locals() );", "\n", "# !import code; code.interact(local=vars())", "\n", "# for ii in range(bs):", "\n", "#     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):", "\n", "#         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))", "\n", "#     print(\"\")", "\n", "\n", "# select the best hypotheses", "\n", "", "", "tgt_len", "=", "src_len", ".", "new", "(", "bs", ")", "\n", "best", "=", "[", "]", "\n", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyp", "=", "max", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", "1", "]", "\n", "tgt_len", "[", "i", "]", "=", "len", "(", "best_hyp", ")", "+", "1", "# +1 for the <EOS> symbol", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "src_len", ".", "new", "(", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ",", "bs", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "            ", "decoded", "[", ":", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "hypo", "\n", "decoded", "[", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "self", ".", "eos_index", "\n", "\n", "# sanity check", "\n", "", "assert", "(", "decoded", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "decoded", ",", "tgt_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.generate_beam_efficient": [[692, 716], ["model.efficient_beam_search.generate_beam_gpu"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.efficient_beam_search.generate_beam_gpu"], ["", "def", "generate_beam_efficient", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ",", "nbest", "=", "None", ",", "sample_temperature", "=", "None", ",", "hyps_size_multiple", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        *** CREDIT TO PHI for this!: nxphi47@gmail.com / nxphi47@github.com ****\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "# hyps_size_multiple = getattr(self.)", "\n", "return", "generate_beam_gpu", "(", "\n", "self", ",", "src_enc", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "nbest", "=", "nbest", ",", "sample_temperature", "=", "sample_temperature", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.__init__": [[721, 731], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_hyp", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", ":", "\n", "        ", "\"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"", "\n", "self", ".", "max_len", "=", "max_len", "-", "1", "# ignoring <BOS>", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "n_hyp", "=", "n_hyp", "\n", "self", ".", "hyp", "=", "[", "]", "\n", "self", ".", "worst_score", "=", "1e9", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.__len__": [[732, 737], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of hypotheses in the list.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "hyp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add": [[738, 751], ["mass_transformer.BeamHypotheses.hyp.append", "len", "len", "len", "sorted", "min", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "hyp", ",", "sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"", "\n", "score", "=", "sum_logprobs", "/", "len", "(", "hyp", ")", "**", "self", ".", "length_penalty", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", "or", "score", ">", "self", ".", "worst_score", ":", "\n", "            ", "self", ".", "hyp", ".", "append", "(", "(", "score", ",", "hyp", ")", ")", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "n_hyp", ":", "\n", "                ", "sorted_scores", "=", "sorted", "(", "[", "(", "s", ",", "idx", ")", "for", "idx", ",", "(", "s", ",", "_", ")", "in", "enumerate", "(", "self", ".", "hyp", ")", "]", ")", "\n", "del", "self", ".", "hyp", "[", "sorted_scores", "[", "0", "]", "[", "1", "]", "]", "\n", "self", ".", "worst_score", "=", "sorted_scores", "[", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "worst_score", "=", "min", "(", "score", ",", "self", ".", "worst_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done": [[752, 763], ["len"], "methods", ["None"], ["", "", "", "def", "is_done", "(", "self", ",", "best_sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "n_hyp", ":", "\n", "            ", "return", "False", "\n", "", "elif", "self", ".", "early_stopping", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "worst_score", ">=", "best_sum_logprobs", "/", "self", ".", "max_len", "**", "self", ".", "length_penalty", "", "", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding": [[22, 28], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Embedding"], ["def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "None", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "if", "padding_idx", "is", "not", "None", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear": [[30, 36], ["torch.Linear"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "# nn.init.normal_(m.weight, mean=0, std=1)", "\n", "# nn.init.xavier_uniform_(m.weight)", "\n", "# nn.init.constant_(m.bias, 0.)", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.create_sinusoidal_embeddings": [[38, 47], ["numpy.array", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "out.detach_", "numpy.sin", "numpy.cos", "range", "numpy.power", "range"], "function", ["None"], ["", "def", "create_sinusoidal_embeddings", "(", "n_pos", ",", "dim", ",", "out", ")", ":", "\n", "    ", "position_enc", "=", "np", ".", "array", "(", "[", "\n", "[", "pos", "/", "np", ".", "power", "(", "10000", ",", "2", "*", "(", "j", "//", "2", ")", "/", "dim", ")", "for", "j", "in", "range", "(", "dim", ")", "]", "\n", "for", "pos", "in", "range", "(", "n_pos", ")", "\n", "]", ")", "\n", "out", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "FloatTensor", "(", "np", ".", "sin", "(", "position_enc", "[", ":", ",", "0", ":", ":", "2", "]", ")", ")", "\n", "out", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "FloatTensor", "(", "np", ".", "cos", "(", "position_enc", "[", ":", ",", "1", ":", ":", "2", "]", ")", ")", "\n", "out", ".", "detach_", "(", ")", "\n", "out", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.gelu": [[49, 58], ["torch.erf", "torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    GELU activation\n    https://arxiv.org/abs/1606.08415\n    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n    https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/modeling.py\n    \"\"\"", "\n", "# return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))", "\n", "return", "0.5", "*", "x", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.get_masks": [[60, 80], ["lengths.size", "torch.arange", "torch.arange", "torch.arange", "lengths.max().item", "mask.size", "alen[].repeat", "attn_mask.size", "lengths.max"], "function", ["None"], ["", "def", "get_masks", "(", "slen", ",", "lengths", ",", "causal", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Generate hidden states mask, and optionally an attention mask.\n    \"\"\"", "\n", "assert", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "<=", "slen", "\n", "bs", "=", "lengths", ".", "size", "(", "0", ")", "\n", "alen", "=", "torch", ".", "arange", "(", "slen", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "\n", "mask", "=", "alen", "<", "lengths", "[", ":", ",", "None", "]", "\n", "\n", "# attention mask is the same as mask, or triangular inferior attention (causal)", "\n", "if", "causal", ":", "\n", "        ", "attn_mask", "=", "alen", "[", "None", ",", "None", ",", ":", "]", ".", "repeat", "(", "bs", ",", "slen", ",", "1", ")", "<=", "alen", "[", "None", ",", ":", ",", "None", "]", "\n", "", "else", ":", "\n", "        ", "attn_mask", "=", "mask", "\n", "\n", "# sanity check", "\n", "", "assert", "mask", ".", "size", "(", ")", "==", "(", "bs", ",", "slen", ")", "\n", "assert", "causal", "is", "False", "or", "attn_mask", ".", "size", "(", ")", "==", "(", "bs", ",", "slen", ",", "slen", ")", "\n", "\n", "return", "mask", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.__init__": [[31, 103], ["torch.FloatTensor", "numpy.array", "collections.OrderedDict", "time.time", "mass_trainer.Trainer.reload_checkpoint", "utils.parse_lambda_config", "params.stopping_criterion.split", "int", "list", "numpy.maximum", "mass_trainer.Trainer.metrics.append", "split[].isdigit", "mass_trainer.Trainer.data[].counts.values", "params.validation_metrics.split", "len", "data[].keys", "data[].keys", "data[].keys", "data[].keys"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.reload_checkpoint", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.parse_lambda_config"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize trainer.\n        \"\"\"", "\n", "# epoch / iteration size", "\n", "self", ".", "epoch_size", "=", "params", ".", "epoch_size", "\n", "if", "self", ".", "epoch_size", "==", "-", "1", ":", "\n", "            ", "self", ".", "epoch_size", "=", "self", ".", "data", "\n", "assert", "self", ".", "epoch_size", ">", "0", "\n", "\n", "# stopping criterion used for early stopping", "\n", "", "if", "params", ".", "stopping_criterion", "!=", "''", ":", "\n", "            ", "split", "=", "params", ".", "stopping_criterion", ".", "split", "(", "','", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "and", "split", "[", "1", "]", ".", "isdigit", "(", ")", "\n", "self", ".", "decrease_counts_max", "=", "int", "(", "split", "[", "1", "]", ")", "\n", "self", ".", "decrease_counts", "=", "0", "\n", "if", "split", "[", "0", "]", "[", "0", "]", "==", "'_'", ":", "\n", "                ", "self", ".", "stopping_criterion", "=", "(", "split", "[", "0", "]", "[", "1", ":", "]", ",", "False", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "stopping_criterion", "=", "(", "split", "[", "0", "]", ",", "True", ")", "\n", "", "self", ".", "best_stopping_criterion", "=", "-", "1e12", "if", "self", ".", "stopping_criterion", "[", "1", "]", "else", "1e12", "\n", "", "else", ":", "\n", "            ", "self", ".", "stopping_criterion", "=", "None", "\n", "self", ".", "best_stopping_criterion", "=", "None", "\n", "\n", "# data iterators", "\n", "", "self", ".", "iterators", "=", "{", "}", "\n", "\n", "# probability of masking out / randomize / not modify words to predict", "\n", "params", ".", "pred_probs", "=", "torch", ".", "FloatTensor", "(", "[", "params", ".", "word_mask", ",", "params", ".", "word_keep", ",", "params", ".", "word_rand", "]", ")", "\n", "\n", "# probabilty to predict a word", "\n", "counts", "=", "np", ".", "array", "(", "list", "(", "self", ".", "data", "[", "'dico'", "]", ".", "counts", ".", "values", "(", ")", ")", ")", "\n", "params", ".", "mask_scores", "=", "np", ".", "maximum", "(", "counts", ",", "1", ")", "**", "-", "params", ".", "sample_alpha", "\n", "params", ".", "mask_scores", "[", "params", ".", "pad_index", "]", "=", "0", "# do not predict <PAD> index", "\n", "params", ".", "mask_scores", "[", "counts", "==", "0", "]", "=", "0", "# do not predict special tokens", "\n", "\n", "# validation metrics", "\n", "self", ".", "metrics", "=", "[", "]", "\n", "metrics", "=", "[", "m", "for", "m", "in", "params", ".", "validation_metrics", ".", "split", "(", "','", ")", "if", "m", "!=", "''", "]", "\n", "for", "m", "in", "metrics", ":", "\n", "            ", "m", "=", "(", "m", "[", "1", ":", "]", ",", "False", ")", "if", "m", "[", "0", "]", "==", "'_'", "else", "(", "m", ",", "True", ")", "\n", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "", "self", ".", "best_metrics", "=", "{", "metric", ":", "(", "-", "1e12", "if", "biggest", "else", "1e12", ")", "for", "(", "metric", ",", "biggest", ")", "in", "self", ".", "metrics", "}", "\n", "\n", "# training statistics", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "n_iter", "=", "0", "\n", "self", ".", "n_total_iter", "=", "0", "\n", "self", ".", "n_sentences", "=", "0", "\n", "self", ".", "stats", "=", "OrderedDict", "(", "\n", "[", "(", "'processed_s'", ",", "0", ")", ",", "(", "'processed_w'", ",", "0", ")", "]", "+", "\n", "[", "(", "'CLM-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'CLM-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'CLM-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'MLM-%s'", "%", "l", ",", "[", "]", ")", "for", "l", "in", "params", ".", "langs", "]", "+", "\n", "[", "(", "'MLM-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'MLM-%s-%s'", "%", "(", "l2", ",", "l1", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "data", "[", "'para'", "]", ".", "keys", "(", ")", "]", "+", "\n", "[", "(", "'PC-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "params", ".", "pc_steps", "]", "+", "\n", "[", "(", "'AE-%s'", "%", "lang", ",", "[", "]", ")", "for", "lang", "in", "params", ".", "ae_steps", "]", "+", "\n", "[", "(", "'MT-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "params", ".", "mt_steps", "]", "+", "\n", "[", "(", "'BMT-%s-%s'", "%", "(", "l1", ",", "l2", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", "in", "params", ".", "bmt_steps", "]", "+", "\n", "[", "(", "'MA-%s'", "%", "lang", ",", "[", "]", ")", "for", "lang", "in", "params", ".", "mass_steps", "]", "+", "\n", "[", "(", "'BT-%s-%s-%s'", "%", "(", "l1", ",", "l2", ",", "l3", ")", ",", "[", "]", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", "\n", ")", "\n", "self", ".", "last_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "# reload potential checkpoints", "\n", "self", ".", "reload_checkpoint", "(", ")", "\n", "\n", "# initialize lambda coefficients and their configurations", "\n", "parse_lambda_config", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_optimizer_fp": [[104, 113], ["utils.get_optimizer", "getattr().parameters", "apex.fp16_utils.FP16_Optimizer", "getattr"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer"], ["", "def", "get_optimizer_fp", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\"\n        Build optimizer.\n        \"\"\"", "\n", "assert", "module", "in", "[", "'model'", ",", "'encoder'", ",", "'decoder'", "]", "\n", "optimizer", "=", "get_optimizer", "(", "getattr", "(", "self", ",", "module", ")", ".", "parameters", "(", ")", ",", "self", ".", "params", ".", "optimizer", ")", "\n", "if", "self", ".", "params", ".", "fp16", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize": [[114, 148], ["type", "logger.error", "exit", "mass_trainer.Trainer.optimizers[].zero_grad", "mass_trainer.Trainer.optimizers[].backward", "loss.backward", "mass_trainer.Trainer.optimizers[].step", "len", "mass_trainer.Trainer.optimizers[].clip_master_grads", "torch.nn.utils.clip_grad_norm_", "getattr().parameters", "getattr"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step"], ["", "def", "optimize", "(", "self", ",", "loss", ",", "modules", ")", ":", "\n", "        ", "\"\"\"\n        Optimize.\n        \"\"\"", "\n", "if", "type", "(", "modules", ")", "is", "str", ":", "\n", "            ", "modules", "=", "[", "modules", "]", "\n", "\n", "# check NaN", "\n", "", "if", "(", "loss", "!=", "loss", ")", ".", "data", ".", "any", "(", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"NaN detected\"", ")", "\n", "exit", "(", ")", "\n", "\n", "# zero grad", "\n", "", "for", "module", "in", "modules", ":", "\n", "            ", "self", ".", "optimizers", "[", "module", "]", ".", "zero_grad", "(", ")", "\n", "\n", "# backward", "\n", "", "if", "self", ".", "params", ".", "fp16", ":", "\n", "            ", "assert", "len", "(", "modules", ")", "==", "1", ",", "\"fp16 not implemented for more than one module\"", "\n", "self", ".", "optimizers", "[", "module", "]", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", ")", "\n", "\n", "# clip gradients", "\n", "", "if", "self", ".", "params", ".", "clip_grad_norm", ">", "0", ":", "\n", "            ", "for", "module", "in", "modules", ":", "\n", "                ", "if", "self", ".", "params", ".", "fp16", ":", "\n", "                    ", "self", ".", "optimizers", "[", "module", "]", ".", "clip_master_grads", "(", "self", ".", "params", ".", "clip_grad_norm", ")", "\n", "", "else", ":", "\n", "                    ", "clip_grad_norm_", "(", "getattr", "(", "self", ",", "module", ")", ".", "parameters", "(", ")", ",", "self", ".", "params", ".", "clip_grad_norm", ")", "\n", "\n", "# optimization step", "\n", "", "", "", "for", "module", "in", "modules", ":", "\n", "            ", "self", ".", "optimizers", "[", "module", "]", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.iter": [[149, 157], ["utils.update_lambdas", "mass_trainer.Trainer.print_stats"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.update_lambdas", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.print_stats"], ["", "", "def", "iter", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        End of iteration.\n        \"\"\"", "\n", "self", ".", "n_iter", "+=", "1", "\n", "self", ".", "n_total_iter", "+=", "1", "\n", "update_lambdas", "(", "self", ".", "params", ",", "self", ".", "n_total_iter", ")", "\n", "self", ".", "print_stats", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.print_stats": [[158, 191], ["mass_trainer.Trainer.stats.keys", "time.time", "logger.info", "type", "numpy.mean", "mass_trainer.Trainer.stats.items", "type", "len"], "methods", ["None"], ["", "def", "print_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Print statistics about the training.\n        \"\"\"", "\n", "if", "self", ".", "n_iter", "%", "5", "!=", "0", ":", "\n", "            ", "return", "\n", "\n", "", "s_iter", "=", "\"%7i - \"", "%", "self", ".", "n_iter", "\n", "s_stat", "=", "' || '", ".", "join", "(", "[", "\n", "'{}: {:7.4f}'", ".", "format", "(", "k", ",", "np", ".", "mean", "(", "v", ")", ")", "for", "k", ",", "v", "in", "self", ".", "stats", ".", "items", "(", ")", "\n", "if", "type", "(", "v", ")", "is", "list", "and", "len", "(", "v", ")", ">", "0", "\n", "]", ")", "\n", "for", "k", "in", "self", ".", "stats", ".", "keys", "(", ")", ":", "\n", "            ", "if", "type", "(", "self", ".", "stats", "[", "k", "]", ")", "is", "list", ":", "\n", "                ", "del", "self", ".", "stats", "[", "k", "]", "[", ":", "]", "\n", "\n", "# transformer learning rate", "\n", "", "", "lr", "=", "self", ".", "optimizers", "[", "self", ".", "MODEL_NAMES", "[", "0", "]", "]", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "s_lr", "=", "\" - Transformer LR = {:.4e}\"", ".", "format", "(", "lr", ")", "\n", "\n", "# processing speed", "\n", "new_time", "=", "time", ".", "time", "(", ")", "\n", "diff", "=", "new_time", "-", "self", ".", "last_time", "\n", "s_speed", "=", "\"{:7.2f} sent/s - {:8.2f} words/s - \"", ".", "format", "(", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "*", "1.0", "/", "diff", ",", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "*", "1.0", "/", "diff", "\n", ")", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "=", "0", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "=", "0", "\n", "self", ".", "last_time", "=", "new_time", "\n", "\n", "# log speed + stats + learning rate", "\n", "logger", ".", "info", "(", "s_iter", "+", "s_speed", "+", "s_stat", "+", "s_lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_iterator": [[192, 225], ["logger.info", "logger.info", "[].get_iterator", "[].get_iterator", "[].get_iterator", "[].get_iterator", "str", "str"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_iterator", "(", "self", ",", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ",", "back", ")", ":", "\n", "        ", "\"\"\"\n        Create a new iterator for a dataset.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Creating new training data iterator (%s) ...\"", "%", "','", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "[", "iter_name", ",", "lang1", ",", "lang2", "]", "if", "x", "is", "not", "None", "]", ")", ")", "\n", "if", "lang2", "is", "None", ":", "\n", "            ", "if", "stream", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono_stream'", "]", "[", "lang1", "]", "[", "'train'", "]", ".", "get_iterator", "(", "shuffle", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono'", "]", "[", "lang1", "]", "[", "'train'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "n_sentences", "=", "-", "1", ",", "\n", ")", "\n", "", "", "elif", "back", "is", "True", ":", "\n", "            ", "iterator", "=", "self", ".", "data", "[", "'back'", "]", "[", "(", "lang1", ",", "lang2", ")", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "n_sentences", "=", "-", "1", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "stream", "is", "False", "\n", "_lang1", ",", "_lang2", "=", "(", "lang1", ",", "lang2", ")", "if", "lang1", "<", "lang2", "else", "(", "lang2", ",", "lang1", ")", "\n", "iterator", "=", "self", ".", "data", "[", "'para'", "]", "[", "(", "_lang1", ",", "_lang2", ")", "]", "[", "'train'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "n_sentences", "=", "-", "1", ",", "\n", ")", "\n", "", "logger", ".", "info", "(", "\"iterator (%s) done\"", "%", "','", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "[", "iter_name", ",", "lang1", ",", "lang2", "]", "if", "x", "is", "not", "None", "]", ")", ")", "\n", "\n", "self", ".", "iterators", "[", "(", "iter_name", ",", "lang1", ",", "lang2", ")", "]", "=", "iterator", "\n", "return", "iterator", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch": [[226, 242], ["mass_trainer.Trainer.iterators.get", "mass_trainer.Trainer.get_iterator", "next", "mass_trainer.Trainer.get_iterator", "next"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_batch", "(", "self", ",", "iter_name", ",", "lang1", ",", "lang2", "=", "None", ",", "stream", "=", "False", ",", "back", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Return a batch of sentences from a dataset.\n        \"\"\"", "\n", "assert", "lang1", "in", "self", ".", "params", ".", "langs", "\n", "assert", "lang2", "is", "None", "or", "lang2", "in", "self", ".", "params", ".", "langs", "\n", "assert", "stream", "is", "False", "or", "lang2", "is", "None", "\n", "iterator", "=", "self", ".", "iterators", ".", "get", "(", "(", "iter_name", ",", "lang1", ",", "lang2", ")", ",", "None", ")", "\n", "if", "iterator", "is", "None", ":", "\n", "            ", "iterator", "=", "self", ".", "get_iterator", "(", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ",", "back", ")", "\n", "", "try", ":", "\n", "            ", "x", "=", "next", "(", "iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "iterator", "=", "self", ".", "get_iterator", "(", "iter_name", ",", "lang1", ",", "lang2", ",", "stream", ",", "back", ")", "\n", "x", "=", "next", "(", "iterator", ")", "\n", "", "return", "x", "if", "lang2", "is", "None", "or", "lang1", "<", "lang2", "else", "x", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_shuffle": [[243, 263], ["numpy.random.uniform", "x.clone", "range", "l.size", "scores.argsort", "x2[].copy_", "numpy.arange", "x.size", "x.size", "torch.from_numpy"], "methods", ["None"], ["", "def", "word_shuffle", "(", "self", ",", "x", ",", "l", ")", ":", "\n", "        ", "\"\"\"\n        Randomly shuffle input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_shuffle", "==", "0", ":", "\n", "            ", "return", "x", ",", "l", "\n", "\n", "# define noise word scores", "\n", "", "noise", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "self", ".", "params", ".", "word_shuffle", ",", "size", "=", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ")", "\n", "noise", "[", "0", "]", "=", "-", "1", "# do not move start sentence symbol", "\n", "\n", "assert", "self", ".", "params", ".", "word_shuffle", ">", "1", "\n", "x2", "=", "x", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "# generate a random permutation", "\n", "            ", "scores", "=", "np", ".", "arange", "(", "l", "[", "i", "]", "-", "1", ")", "+", "noise", "[", ":", "l", "[", "i", "]", "-", "1", ",", "i", "]", "\n", "permutation", "=", "scores", ".", "argsort", "(", ")", "\n", "# shuffle words", "\n", "x2", "[", ":", "l", "[", "i", "]", "-", "1", ",", "i", "]", ".", "copy_", "(", "x2", "[", ":", "l", "[", "i", "]", "-", "1", ",", "i", "]", "[", "torch", ".", "from_numpy", "(", "permutation", ")", "]", ")", "\n", "", "return", "x2", ",", "l", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_dropout": [[264, 298], ["range", "torch.LongTensor", "torch.LongTensor().fill_", "range", "l.size", "numpy.random.rand", "l.size", "x[].tolist", "new_s.append", "sentences.append", "lengths.append", "torch.LongTensor.size", "x2[].copy_", "x.size", "len", "new_s.append", "len", "torch.LongTensor", "torch.LongTensor", "x.size", "enumerate", "len", "torch.LongTensor.max", "torch.LongTensor.size", "numpy.random.randint", "len"], "methods", ["None"], ["", "def", "word_dropout", "(", "self", ",", "x", ",", "l", ")", ":", "\n", "        ", "\"\"\"\n        Randomly drop input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_dropout", "==", "0", ":", "\n", "            ", "return", "x", ",", "l", "\n", "", "assert", "0", "<", "self", ".", "params", ".", "word_dropout", "<", "1", "\n", "\n", "# define words to drop", "\n", "eos", "=", "self", ".", "params", ".", "eos_index", "\n", "assert", "(", "x", "[", "0", "]", "==", "eos", ")", ".", "sum", "(", ")", "==", "l", ".", "size", "(", "0", ")", "\n", "keep", "=", "np", ".", "random", ".", "rand", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ">=", "self", ".", "params", ".", "word_dropout", "\n", "keep", "[", "0", "]", "=", "1", "# do not drop the start sentence symbol", "\n", "\n", "sentences", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "assert", "x", "[", "l", "[", "i", "]", "-", "1", ",", "i", "]", "==", "eos", "\n", "words", "=", "x", "[", ":", "l", "[", "i", "]", "-", "1", ",", "i", "]", ".", "tolist", "(", ")", "\n", "# randomly drop words from the input", "\n", "new_s", "=", "[", "w", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "if", "keep", "[", "j", ",", "i", "]", "]", "\n", "# we need to have at least one word in the sentence (more than the start / end sentence symbols)", "\n", "if", "len", "(", "new_s", ")", "==", "1", ":", "\n", "                ", "new_s", ".", "append", "(", "words", "[", "np", ".", "random", ".", "randint", "(", "1", ",", "len", "(", "words", ")", ")", "]", ")", "\n", "", "new_s", ".", "append", "(", "eos", ")", "\n", "assert", "len", "(", "new_s", ")", ">=", "3", "and", "new_s", "[", "0", "]", "==", "eos", "and", "new_s", "[", "-", "1", "]", "==", "eos", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "lengths", ".", "append", "(", "len", "(", "new_s", ")", ")", "\n", "# re-construct input", "\n", "", "l2", "=", "torch", ".", "LongTensor", "(", "lengths", ")", "\n", "x2", "=", "torch", ".", "LongTensor", "(", "l2", ".", "max", "(", ")", ",", "l2", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "for", "i", "in", "range", "(", "l2", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "x2", "[", ":", "l2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "return", "x2", ",", "l2", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_blank": [[299, 327], ["range", "torch.LongTensor().fill_", "range", "l.size", "numpy.random.rand", "l.size", "x[].tolist", "new_s.append", "sentences.append", "l.size", "x2[].copy_", "x.size", "torch.LongTensor", "torch.LongTensor", "x.size", "enumerate", "len", "l.max", "l.size"], "methods", ["None"], ["", "def", "word_blank", "(", "self", ",", "x", ",", "l", ")", ":", "\n", "        ", "\"\"\"\n        Randomly blank input words.\n        \"\"\"", "\n", "if", "self", ".", "params", ".", "word_blank", "==", "0", ":", "\n", "            ", "return", "x", ",", "l", "\n", "", "assert", "0", "<", "self", ".", "params", ".", "word_blank", "<", "1", "\n", "\n", "# define words to blank", "\n", "eos", "=", "self", ".", "params", ".", "eos_index", "\n", "assert", "(", "x", "[", "0", "]", "==", "eos", ")", ".", "sum", "(", ")", "==", "l", ".", "size", "(", "0", ")", "\n", "keep", "=", "np", ".", "random", ".", "rand", "(", "x", ".", "size", "(", "0", ")", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", ">=", "self", ".", "params", ".", "word_blank", "\n", "keep", "[", "0", "]", "=", "1", "# do not blank the start sentence symbol", "\n", "\n", "sentences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "assert", "x", "[", "l", "[", "i", "]", "-", "1", ",", "i", "]", "==", "eos", "\n", "words", "=", "x", "[", ":", "l", "[", "i", "]", "-", "1", ",", "i", "]", ".", "tolist", "(", ")", "\n", "# randomly blank words from the input", "\n", "new_s", "=", "[", "w", "if", "keep", "[", "j", ",", "i", "]", "else", "self", ".", "params", ".", "mask_index", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "]", "\n", "new_s", ".", "append", "(", "eos", ")", "\n", "assert", "len", "(", "new_s", ")", "==", "l", "[", "i", "]", "and", "new_s", "[", "0", "]", "==", "eos", "and", "new_s", "[", "-", "1", "]", "==", "eos", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "# re-construct input", "\n", "", "x2", "=", "torch", ".", "LongTensor", "(", "l", ".", "max", "(", ")", ",", "l", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "x2", "[", ":", "l", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "", "return", "x2", ",", "l", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.add_noise": [[328, 336], ["mass_trainer.Trainer.word_shuffle", "mass_trainer.Trainer.word_dropout", "mass_trainer.Trainer.word_blank"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_shuffle", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_dropout", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.word_blank"], ["", "def", "add_noise", "(", "self", ",", "words", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Add noise to the encoder input.\n        \"\"\"", "\n", "words", ",", "lengths", "=", "self", ".", "word_shuffle", "(", "words", ",", "lengths", ")", "\n", "words", ",", "lengths", "=", "self", ".", "word_dropout", "(", "words", ",", "lengths", ")", "\n", "words", ",", "lengths", "=", "self", ".", "word_blank", "(", "words", ",", "lengths", ")", "\n", "return", "words", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.mask_out": [[337, 383], ["x.masked_scatter.masked_scatter.size", "_x_real.clone().random_", "_x_real.clone().fill_", "torch.multinomial", "x.masked_scatter.masked_scatter.masked_scatter", "torch.from_numpy", "math.ceil", "numpy.random.choice", "torch.zeros", "pred_mask.view.view.view", "pred_mask.view.view.view", "pred_mask.view.view.sum().item", "max", "pred_mask.view.view.view", "len", "x.masked_scatter.masked_scatter.min", "x.masked_scatter.masked_scatter.max", "x.masked_scatter.masked_scatter.size", "pred_mask.view.view.size", "numpy.random.rand", "pred_mask.view.view.astype", "len", "_x_real.clone", "_x_real.clone", "x.masked_scatter.masked_scatter.flatten", "pred_mask.view.view.sum", "pred_mask.view.view.sum().item", "x_prob.sum", "torch.nonzero().view", "pred_mask.view.view.sum", "torch.nonzero"], "methods", ["None"], ["", "def", "mask_out", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "\"\"\"\n        Decide of random words to mask out, and what target they get assigned.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "\n", "# define target words to predict", "\n", "if", "params", ".", "sample_alpha", "==", "0", ":", "\n", "            ", "pred_mask", "=", "np", ".", "random", ".", "rand", "(", "slen", ",", "bs", ")", "<=", "params", ".", "word_pred", "\n", "pred_mask", "=", "torch", ".", "from_numpy", "(", "pred_mask", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "else", ":", "\n", "            ", "x_prob", "=", "params", ".", "mask_scores", "[", "x", ".", "flatten", "(", ")", "]", "\n", "n_tgt", "=", "math", ".", "ceil", "(", "params", ".", "word_pred", "*", "slen", "*", "bs", ")", "\n", "tgt_ids", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "x_prob", ")", ",", "n_tgt", ",", "replace", "=", "False", ",", "p", "=", "x_prob", "/", "x_prob", ".", "sum", "(", ")", ")", "\n", "pred_mask", "=", "torch", ".", "zeros", "(", "slen", "*", "bs", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "pred_mask", "[", "tgt_ids", "]", "=", "1", "\n", "pred_mask", "=", "pred_mask", ".", "view", "(", "slen", ",", "bs", ")", "\n", "\n", "# do not predict padding", "\n", "", "pred_mask", "[", "x", "==", "params", ".", "pad_index", "]", "=", "0", "\n", "pred_mask", "[", "0", "]", "=", "0", "# TODO: remove", "\n", "\n", "# mask a number of words == 0 [8] (faster with fp16)", "\n", "if", "params", ".", "fp16", ":", "\n", "            ", "pred_mask", "=", "pred_mask", ".", "view", "(", "-", "1", ")", "\n", "n1", "=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "n2", "=", "max", "(", "n1", "%", "8", ",", "8", "*", "(", "n1", "//", "8", ")", ")", "\n", "if", "n2", "!=", "n1", ":", "\n", "                ", "pred_mask", "[", "torch", ".", "nonzero", "(", "pred_mask", ")", ".", "view", "(", "-", "1", ")", "[", ":", "n1", "-", "n2", "]", "]", "=", "0", "\n", "", "pred_mask", "=", "pred_mask", ".", "view", "(", "slen", ",", "bs", ")", "\n", "assert", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "%", "8", "==", "0", "\n", "\n", "# generate possible targets / update x input", "\n", "", "_x_real", "=", "x", "[", "pred_mask", "]", "\n", "_x_rand", "=", "_x_real", ".", "clone", "(", ")", ".", "random_", "(", "params", ".", "n_words", ")", "\n", "_x_mask", "=", "_x_real", ".", "clone", "(", ")", ".", "fill_", "(", "params", ".", "mask_index", ")", "\n", "probs", "=", "torch", ".", "multinomial", "(", "params", ".", "pred_probs", ",", "len", "(", "_x_real", ")", ",", "replacement", "=", "True", ")", "\n", "_x", "=", "_x_mask", "*", "(", "probs", "==", "0", ")", ".", "long", "(", ")", "+", "_x_real", "*", "(", "probs", "==", "1", ")", ".", "long", "(", ")", "+", "_x_rand", "*", "(", "probs", "==", "2", ")", ".", "long", "(", ")", "\n", "x", "=", "x", ".", "masked_scatter", "(", "pred_mask", ",", "_x", ")", "\n", "\n", "assert", "0", "<=", "x", ".", "min", "(", ")", "<=", "x", ".", "max", "(", ")", "<", "params", ".", "n_words", "\n", "assert", "x", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "assert", "pred_mask", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "\n", "return", "x", ",", "_x_real", ",", "pred_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.generate_batch": [[384, 408], ["mass_trainer.Trainer.get_batch", "x.clone().fill_", "mass_trainer.Trainer.get_batch", "mass_trainer.Trainer.add_noise", "utils.concat_batches", "mass_trainer.Trainer.get_batch", "utils.concat_batches", "x.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.add_noise", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches"], ["", "def", "generate_batch", "(", "self", ",", "lang1", ",", "lang2", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Prepare a batch (for causal or non-causal mode).\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "if", "lang2", "is", "not", "None", "else", "None", "\n", "\n", "if", "lang2", "is", "None", ":", "\n", "            ", "x", ",", "lengths", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ",", "stream", "=", "True", ")", "\n", "positions", "=", "None", "\n", "langs", "=", "x", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "if", "params", ".", "n_langs", ">", "1", "else", "None", "\n", "", "elif", "lang1", "==", "lang2", ":", "\n", "            ", "(", "x1", ",", "len1", ")", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ")", "\n", "(", "x2", ",", "len2", ")", "=", "(", "x1", ",", "len1", ")", "\n", "(", "x1", ",", "len1", ")", "=", "self", ".", "add_noise", "(", "x1", ",", "len1", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "name", ",", "lang1", ",", "lang2", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "True", ")", "\n", "\n", "", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "(", "None", ",", "None", ")", "if", "lang2", "is", "None", "else", "(", "len1", ",", "len2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_model": [[409, 428], ["os.path.join", "logger.info", "torch.save", "getattr().module.state_dict", "getattr().state_dict", "mass_trainer.Trainer.params.__dict__.items", "getattr", "getattr"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Save the model.\n        \"\"\"", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'%s.pth'", "%", "name", ")", "\n", "logger", ".", "info", "(", "'Saving models to %s ...'", "%", "path", ")", "\n", "data", "=", "{", "}", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "if", "self", ".", "params", ".", "multi_gpu", ":", "\n", "                ", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "                ", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "state_dict", "(", ")", "\n", "\n", "", "", "data", "[", "'dico_id2word'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "id2word", "\n", "data", "[", "'dico_word2id'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "word2id", "\n", "data", "[", "'dico_counts'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "counts", "\n", "data", "[", "'params'", "]", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "\n", "torch", ".", "save", "(", "data", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_checkpoint": [[429, 455], ["os.path.join", "logger.info", "torch.save", "getattr().state_dict", "mass_trainer.Trainer.optimizers[].state_dict", "mass_trainer.Trainer.params.__dict__.items", "getattr"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Checkpoint the experiment.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "\n", "", "data", "=", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'n_total_iter'", ":", "self", ".", "n_total_iter", ",", "\n", "'best_metrics'", ":", "self", ".", "best_metrics", ",", "\n", "'best_stopping_criterion'", ":", "self", ".", "best_stopping_criterion", ",", "\n", "}", "\n", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "data", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", ".", "state_dict", "(", ")", "\n", "data", "[", "name", "+", "'_optimizer'", "]", "=", "self", ".", "optimizers", "[", "name", "]", ".", "state_dict", "(", ")", "\n", "\n", "", "data", "[", "'dico_id2word'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "id2word", "\n", "data", "[", "'dico_word2id'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "word2id", "\n", "data", "[", "'dico_counts'", "]", "=", "self", ".", "data", "[", "'dico'", "]", ".", "counts", "\n", "data", "[", "'params'", "]", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "params", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'checkpoint.pth'", ")", "\n", "logger", ".", "info", "(", "\"Saving checkpoint to %s ...\"", "%", "checkpoint_path", ")", "\n", "torch", ".", "save", "(", "data", ",", "checkpoint_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.reload_checkpoint": [[456, 477], ["os.path.join", "logger.warning", "torch.load", "logger.warning", "os.path.isfile", "getattr().load_state_dict", "mass_trainer.Trainer.optimizers[].load_state_dict", "storage.cuda", "getattr"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], ["", "def", "reload_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reload a checkpoint if we find one.\n        \"\"\"", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "dump_path", ",", "'checkpoint.pth'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "            ", "return", "\n", "", "logger", ".", "warning", "(", "'Reloading checkpoint from %s ...'", "%", "checkpoint_path", ")", "\n", "data", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "self", ".", "params", ".", "local_rank", ")", ")", "\n", "\n", "# reload model parameters and optimizers", "\n", "for", "name", "in", "self", ".", "MODEL_NAMES", ":", "\n", "            ", "getattr", "(", "self", ",", "name", ")", ".", "load_state_dict", "(", "data", "[", "name", "]", ")", "\n", "self", ".", "optimizers", "[", "name", "]", ".", "load_state_dict", "(", "data", "[", "name", "+", "'_optimizer'", "]", ")", "\n", "\n", "# reload main metrics", "\n", "", "self", ".", "epoch", "=", "data", "[", "'epoch'", "]", "+", "1", "\n", "self", ".", "n_total_iter", "=", "data", "[", "'n_total_iter'", "]", "\n", "self", ".", "best_metrics", "=", "data", "[", "'best_metrics'", "]", "\n", "self", ".", "best_stopping_criterion", "=", "data", "[", "'best_stopping_criterion'", "]", "\n", "logger", ".", "warning", "(", "'Checkpoint reloaded. Resuming at epoch %i ...'", "%", "self", ".", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_periodic": [[478, 486], ["mass_trainer.Trainer.save_model"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_model"], ["", "def", "save_periodic", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Save the models periodically.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "params", ".", "save_periodic", ">", "0", "and", "self", ".", "epoch", "%", "self", ".", "params", ".", "save_periodic", "==", "0", ":", "\n", "            ", "self", ".", "save_model", "(", "'periodic-%i'", "%", "self", ".", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_best_model": [[487, 502], ["logger.warning", "logger.info", "mass_trainer.Trainer.save_model"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_model"], ["", "", "def", "save_best_model", "(", "self", ",", "scores", ")", ":", "\n", "        ", "\"\"\"\n        Save best models according to given validation metrics.\n        \"\"\"", "\n", "if", "not", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "return", "\n", "", "for", "metric", ",", "biggest", "in", "self", ".", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "scores", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Metric \\\"%s\\\" not found in scores!\"", "%", "metric", ")", "\n", "continue", "\n", "", "factor", "=", "1", "if", "biggest", "else", "-", "1", "\n", "if", "factor", "*", "scores", "[", "metric", "]", ">", "factor", "*", "self", ".", "best_metrics", "[", "metric", "]", ":", "\n", "                ", "self", ".", "best_metrics", "[", "metric", "]", "=", "scores", "[", "metric", "]", "\n", "logger", ".", "info", "(", "'New best score for %s: %.6f'", "%", "(", "metric", ",", "scores", "[", "metric", "]", ")", ")", "\n", "self", ".", "save_model", "(", "'best-%s'", "%", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.end_epoch": [[503, 529], ["mass_trainer.Trainer.save_checkpoint", "logger.info", "logger.info", "logger.info", "exit", "mass_trainer.Trainer.stopping_criterion[].endswith", "os.system"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.save_checkpoint"], ["", "", "", "def", "end_epoch", "(", "self", ",", "scores", ")", ":", "\n", "        ", "\"\"\"\n        End the epoch.\n        \"\"\"", "\n", "# stop if the stopping criterion has not improved after a certain number of epochs", "\n", "if", "self", ".", "stopping_criterion", "is", "not", "None", "and", "(", "\n", "self", ".", "params", ".", "is_master", "or", "not", "self", ".", "stopping_criterion", "[", "0", "]", ".", "endswith", "(", "'_mt_bleu'", ")", ")", ":", "\n", "            ", "metric", ",", "biggest", "=", "self", ".", "stopping_criterion", "\n", "assert", "metric", "in", "scores", ",", "metric", "\n", "factor", "=", "1", "if", "biggest", "else", "-", "1", "\n", "if", "factor", "*", "scores", "[", "metric", "]", ">", "factor", "*", "self", ".", "best_stopping_criterion", ":", "\n", "                ", "self", ".", "best_stopping_criterion", "=", "scores", "[", "metric", "]", "\n", "logger", ".", "info", "(", "\"New best validation score: %f\"", "%", "self", ".", "best_stopping_criterion", ")", "\n", "self", ".", "decrease_counts", "=", "0", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"Not a better validation score (%i / %i).\"", "\n", "%", "(", "self", ".", "decrease_counts", ",", "self", ".", "decrease_counts_max", ")", ")", "\n", "self", ".", "decrease_counts", "+=", "1", "\n", "", "if", "self", ".", "decrease_counts", ">", "self", ".", "decrease_counts_max", ":", "\n", "                ", "logger", ".", "info", "(", "\"Stopping criterion has been below its best value for more \"", "\n", "\"than %i epochs. Ending the experiment...\"", "%", "self", ".", "decrease_counts_max", ")", "\n", "if", "self", ".", "params", ".", "multi_gpu", "and", "'SLURM_JOB_ID'", "in", "os", ".", "environ", ":", "\n", "                    ", "os", ".", "system", "(", "'scancel '", "+", "os", ".", "environ", "[", "'SLURM_JOB_ID'", "]", ")", "\n", "", "exit", "(", ")", "\n", "", "", "self", ".", "save_checkpoint", "(", ")", "\n", "self", ".", "epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch": [[530, 569], ["len", "torch.cat.size", "lengths.max().item", "torch.cat", "len", "torch.randperm", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "torch.cat.size", "lengths.max", "torch.LongTensor().fill_", "[].expand", "torch.LongTensor", "torch.arange"], "methods", ["None"], ["", "def", "round_batch", "(", "self", ",", "x", ",", "lengths", ",", "positions", ",", "langs", ")", ":", "\n", "        ", "\"\"\"\n        For float16 only.\n        Sub-sample sentences in a batch, and add padding,\n        so that each dimension is a multiple of 8.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "if", "not", "params", ".", "fp16", "or", "len", "(", "lengths", ")", "<", "8", ":", "\n", "            ", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "None", "\n", "\n", "# number of sentences == 0 [8]", "\n", "", "bs1", "=", "len", "(", "lengths", ")", "\n", "bs2", "=", "8", "*", "(", "bs1", "//", "8", ")", "\n", "assert", "bs2", ">", "0", "and", "bs2", "%", "8", "==", "0", "\n", "if", "bs1", "!=", "bs2", ":", "\n", "            ", "idx", "=", "torch", ".", "randperm", "(", "bs1", ")", "[", ":", "bs2", "]", "\n", "lengths", "=", "lengths", "[", "idx", "]", "\n", "slen", "=", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "x", "=", "x", "[", ":", "slen", ",", "idx", "]", "\n", "positions", "=", "None", "if", "positions", "is", "None", "else", "positions", "[", ":", "slen", ",", "idx", "]", "\n", "langs", "=", "None", "if", "langs", "is", "None", "else", "langs", "[", ":", "slen", ",", "idx", "]", "\n", "", "else", ":", "\n", "            ", "idx", "=", "None", "\n", "\n", "# sequence length == 0 [8]", "\n", "", "ml1", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "ml1", "%", "8", "!=", "0", ":", "\n", "            ", "pad", "=", "8", "-", "(", "ml1", "%", "8", ")", "\n", "ml2", "=", "ml1", "+", "pad", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "torch", ".", "LongTensor", "(", "pad", ",", "bs2", ")", ".", "fill_", "(", "params", ".", "pad_index", ")", "]", ",", "0", ")", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "torch", ".", "cat", "(", "[", "positions", ",", "torch", ".", "arange", "(", "pad", ")", "[", ":", ",", "None", "]", "+", "positions", "[", "-", "1", "]", "[", "None", "]", "+", "1", "]", ",", "0", ")", "\n", "", "if", "langs", "is", "not", "None", ":", "\n", "                ", "langs", "=", "torch", ".", "cat", "(", "[", "langs", ",", "langs", "[", "-", "1", "]", "[", "None", "]", ".", "expand", "(", "pad", ",", "bs2", ")", "]", ",", "0", ")", "\n", "", "assert", "x", ".", "size", "(", ")", "==", "(", "ml2", ",", "bs2", ")", "\n", "\n", "", "assert", "x", ".", "size", "(", "0", ")", "%", "8", "==", "0", "\n", "assert", "x", ".", "size", "(", "1", ")", "%", "8", "==", "0", "\n", "return", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.clm_step": [[570, 609], ["getattr", "getattr.train", "mass_trainer.Trainer.generate_batch", "mass_trainer.Trainer.round_batch", "torch.arange", "x[].masked_select", "utils.to_cuda", "getattr.", "getattr.", "mass_trainer.Trainer.stats[].append", "mass_trainer.Trainer.optimize", "lengths.size", "pred_mask.sum().item", "lengths.max", "pred_mask.sum().item", "x[].masked_select.size", "loss.item", "pred_mask.sum", "pred_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.generate_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "clm_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Next word prediction step (causal prediction).\n        CLM objective.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'decoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# generate batch / select words to predict", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "generate_batch", "(", "lang1", ",", "lang2", ",", "'causal'", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "alen", "=", "torch", ".", "arange", "(", "lengths", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "lengths", "[", "None", "]", "-", "1", "\n", "if", "params", ".", "context_size", ">", "0", ":", "# do not predict without context", "\n", "            ", "pred_mask", "[", ":", "params", ".", "context_size", "]", "=", "0", "\n", "", "y", "=", "x", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "y", ".", "size", "(", "0", ")", "\n", "\n", "# cuda", "\n", "x", ",", "lengths", ",", "langs", ",", "pred_mask", ",", "y", "=", "to_cuda", "(", "x", ",", "lengths", ",", "langs", ",", "pred_mask", ",", "y", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "langs", "=", "langs", ",", "causal", "=", "True", ")", "\n", "_", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'CLM-%s'", "%", "lang1", ")", "if", "lang2", "is", "None", "else", "(", "'CLM-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "name", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "lengths", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.mlm_step": [[610, 644], ["getattr", "getattr.train", "mass_trainer.Trainer.generate_batch", "mass_trainer.Trainer.round_batch", "mass_trainer.Trainer.mask_out", "utils.to_cuda", "getattr.", "getattr.", "mass_trainer.Trainer.stats[].append", "mass_trainer.Trainer.optimize", "lengths.size", "pred_mask.sum().item", "loss.item", "pred_mask.sum"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.generate_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.mask_out", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "mlm_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Masked word prediction step.\n        MLM objective is lang2 is None, TLM objective otherwise.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'encoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# generate batch / select words to predict", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "generate_batch", "(", "lang1", ",", "lang2", ",", "'pred'", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "_", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "x", ",", "y", ",", "pred_mask", "=", "self", ".", "mask_out", "(", "x", ",", "lengths", ")", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "\n", "_", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'MLM-%s'", "%", "lang1", ")", "if", "lang2", "is", "None", "else", "(", "'MLM-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "name", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "lengths", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.pc_step": [[645, 700], ["getattr", "getattr.train", "mass_trainer.Trainer.get_batch", "len1.size", "torch.LongTensor().random_", "torch.arange", "utils.concat_batches", "mass_trainer.Trainer.round_batch", "utils.to_cuda", "torch.nn.functional.linear", "torch.nn.functional.binary_cross_entropy_with_logits", "mass_trainer.Trainer.stats[].append", "mass_trainer.Trainer.optimize", "lengths.sum().item", "getattr.", "emb[].unsqueeze", "torch.nn.functional.linear.view", "torch.LongTensor().random_.to().type_as", "torch.nn.functional.binary_cross_entropy_with_logits.item", "torch.LongTensor", "torch.LongTensor().random_", "lengths.sum", "torch.LongTensor().random_.to", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.round_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "pc_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Parallel classification step. Predict if pairs of sentences are mutual translations of each other.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "name", "=", "'model'", "if", "params", ".", "encoder_only", "else", "'encoder'", "\n", "model", "=", "getattr", "(", "self", ",", "name", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# sample parallel sentences", "\n", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "'align'", ",", "lang1", ",", "lang2", ")", "\n", "bs", "=", "len1", ".", "size", "(", "0", ")", "\n", "if", "bs", "==", "1", ":", "# can happen (although very rarely), which makes the negative loss fail", "\n", "            ", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "return", "\n", "\n", "# associate lang1 sentences with their translations, and random lang2 sentences", "\n", "", "y", "=", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "2", ")", "\n", "idx_pos", "=", "torch", ".", "arange", "(", "bs", ")", "\n", "idx_neg", "=", "(", "(", "idx_pos", "+", "torch", ".", "LongTensor", "(", "bs", ")", ".", "random_", "(", "1", ",", "bs", ")", ")", "%", "bs", ")", "\n", "idx", "=", "(", "y", "==", "1", ")", ".", "long", "(", ")", "*", "idx_pos", "+", "(", "y", "==", "0", ")", ".", "long", "(", ")", "*", "idx_neg", "\n", "x2", ",", "len2", "=", "x2", "[", ":", ",", "idx", "]", ",", "len2", "[", "idx", "]", "\n", "\n", "# generate batch / cuda", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "x1", ",", "len1", ",", "lang1_id", ",", "x2", ",", "len2", ",", "lang2_id", ",", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "new_idx", "=", "self", ".", "round_batch", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "if", "new_idx", "is", "not", "None", ":", "\n", "            ", "y", "=", "y", "[", "new_idx", "]", "\n", "", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# get sentence embeddings", "\n", "h", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "[", "0", "]", "\n", "\n", "# parallel classification loss", "\n", "CLF_ID1", ",", "CLF_ID2", "=", "8", ",", "9", "# very hacky, use embeddings to make weights for the classifier", "\n", "emb", "=", "(", "model", ".", "module", "if", "params", ".", "multi_gpu", "else", "model", ")", ".", "embeddings", ".", "weight", "\n", "pred", "=", "F", ".", "linear", "(", "h", ",", "emb", "[", "CLF_ID1", "]", ".", "unsqueeze", "(", "0", ")", ",", "emb", "[", "CLF_ID2", ",", "0", "]", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "pred", ".", "view", "(", "-", "1", ")", ",", "y", ".", "to", "(", "pred", ".", "device", ")", ".", "type_as", "(", "pred", ")", ")", "\n", "self", ".", "stats", "[", "'PC-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "name", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "bs", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "lengths", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.SingleTrainer.__init__": [[704, 716], ["mass_trainer.Trainer.__init__", "mass_trainer.SingleTrainer.get_optimizer_fp"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_optimizer_fp"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "data", ",", "params", ")", ":", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'model'", "]", "\n", "\n", "# model / data / params", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "\n", "# optimizers", "\n", "self", ".", "optimizers", "=", "{", "'model'", ":", "self", ".", "get_optimizer_fp", "(", "'model'", ")", "}", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.__init__": [[720, 737], ["mass_trainer.Trainer.__init__", "mass_trainer.EncDecTrainer.get_optimizer_fp", "mass_trainer.EncDecTrainer.get_optimizer_fp"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_optimizer_fp", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_optimizer_fp"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "data", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "MODEL_NAMES", "=", "[", "'encoder'", ",", "'decoder'", "]", "\n", "\n", "# model / data / params", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "params", "=", "params", "\n", "\n", "# optimizers", "\n", "self", ".", "optimizers", "=", "{", "\n", "'encoder'", ":", "self", ".", "get_optimizer_fp", "(", "'encoder'", ")", ",", "\n", "'decoder'", ":", "self", ".", "get_optimizer_fp", "(", "'decoder'", ")", ",", "\n", "}", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mask_word": [[738, 747], ["numpy.random.randint", "numpy.full", "torch.multinomial", "len"], "methods", ["None"], ["", "def", "mask_word", "(", "self", ",", "w", ")", ":", "\n", "        ", "_w_real", "=", "w", "\n", "_w_rand", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "params", ".", "n_words", ",", "size", "=", "w", ".", "shape", ")", "\n", "_w_mask", "=", "np", ".", "full", "(", "w", ".", "shape", ",", "self", ".", "params", ".", "mask_index", ")", "\n", "\n", "probs", "=", "torch", ".", "multinomial", "(", "self", ".", "params", ".", "pred_probs", ",", "len", "(", "_w_real", ")", ",", "replacement", "=", "True", ")", "\n", "\n", "_w", "=", "_w_mask", "*", "(", "probs", "==", "0", ")", ".", "numpy", "(", ")", "+", "_w_real", "*", "(", "probs", "==", "1", ")", ".", "numpy", "(", ")", "+", "_w_rand", "*", "(", "probs", "==", "2", ")", ".", "numpy", "(", ")", "\n", "return", "_w", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.unfold_segments": [[748, 767], ["numpy.array", "pos.extend", "range"], "methods", ["None"], ["", "def", "unfold_segments", "(", "self", ",", "segs", ")", ":", "\n", "        ", "\"\"\"Unfold the random mask segments, for example:\n           The shuffle segment is [2, 0, 0, 2, 0],\n           so the masked segment is like:\n           [1, 1, 0, 0, 1, 1, 0]\n           [1, 2, 3, 4, 5, 6, 7] (positions)\n           (1 means this token will be masked, otherwise not)\n           We return the position of the masked tokens like:\n           [1, 2, 5, 6]\n        \"\"\"", "\n", "pos", "=", "[", "]", "\n", "curr", "=", "1", "# We do not mask the start token", "\n", "for", "l", "in", "segs", ":", "\n", "            ", "if", "l", ">=", "1", ":", "\n", "                ", "pos", ".", "extend", "(", "[", "curr", "+", "i", "for", "i", "in", "range", "(", "l", ")", "]", ")", "\n", "curr", "+=", "l", "\n", "", "else", ":", "\n", "                ", "curr", "+=", "1", "\n", "", "", "return", "np", ".", "array", "(", "pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.shuffle_segments": [[768, 790], ["numpy.random.random", "random.shuffle"], "methods", ["None"], ["", "def", "shuffle_segments", "(", "self", ",", "segs", ",", "unmasked_tokens", ")", ":", "\n", "        ", "\"\"\"\n        We control 20% mask segment is at the start of sentences\n                   20% mask segment is at the end   of sentences\n                   60% mask segment is at random positions,\n        \"\"\"", "\n", "\n", "p", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "p", ">=", "0.8", ":", "\n", "            ", "shuf_segs", "=", "segs", "[", "1", ":", "]", "+", "unmasked_tokens", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "            ", "shuf_segs", "=", "segs", "[", ":", "-", "1", "]", "+", "unmasked_tokens", "\n", "", "else", ":", "\n", "            ", "shuf_segs", "=", "segs", "+", "unmasked_tokens", "\n", "\n", "", "random", ".", "shuffle", "(", "shuf_segs", ")", "\n", "\n", "if", "p", ">=", "0.8", ":", "\n", "            ", "shuf_segs", "=", "segs", "[", "0", ":", "1", "]", "+", "shuf_segs", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "            ", "shuf_segs", "=", "shuf_segs", "+", "segs", "[", "-", "1", ":", "]", "\n", "", "return", "shuf_segs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.get_segments": [[791, 799], ["segs.append", "segs.append"], "methods", ["None"], ["", "def", "get_segments", "(", "self", ",", "mask_len", ",", "span_len", ")", ":", "\n", "        ", "segs", "=", "[", "]", "\n", "while", "mask_len", ">=", "span_len", ":", "\n", "            ", "segs", ".", "append", "(", "span_len", ")", "\n", "mask_len", "-=", "span_len", "\n", "", "if", "mask_len", "!=", "0", ":", "\n", "            ", "segs", ".", "append", "(", "mask_len", ")", "\n", "", "return", "segs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.restricted_mask_sent": [[800, 845], ["round", "mass_trainer.EncDecTrainer.get_segments", "range", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "torch.LongTensor().fill_", "l.clone", "torch.LongTensor", "range", "y.masked_select.masked_select.masked_select", "l.size", "numpy.array", "mass_trainer.EncDecTrainer.shuffle_segments", "mass_trainer.EncDecTrainer.unfold_segments", "words[].copy", "words[].copy", "mass_trainer.EncDecTrainer.mask_word", "inputs.append", "targets.append", "outputs.append", "positions.append", "l.size", "x1[].copy_", "x2[].copy_", "y[].copy_", "pos[].copy_", "len", "range", "range", "x[].tolist", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "l.size", "max", "l.size", "l.size", "l.size", "l.size"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.get_segments", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.shuffle_segments", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.unfold_segments", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.mask_word"], ["", "def", "restricted_mask_sent", "(", "self", ",", "x", ",", "l", ",", "span_len", "=", "100000", ")", ":", "\n", "        ", "\"\"\" Restricted mask sents\n            if span_len is equal to 1, it can be viewed as\n            discrete mask;\n            if span_len -> inf, it can be viewed as\n            pure sentence mask\n        \"\"\"", "\n", "if", "span_len", "<=", "0", ":", "\n", "            ", "span_len", "=", "1", "\n", "", "max_len", "=", "0", "\n", "positions", ",", "inputs", ",", "targets", ",", "outputs", ",", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "mask_len", "=", "round", "(", "len", "(", "x", "[", ":", ",", "0", "]", ")", "*", "self", ".", "params", ".", "word_mass", ")", "\n", "len2", "=", "[", "mask_len", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", "]", "\n", "\n", "unmasked_tokens", "=", "[", "0", "for", "i", "in", "range", "(", "l", "[", "0", "]", "-", "mask_len", "-", "1", ")", "]", "\n", "segs", "=", "self", ".", "get_segments", "(", "mask_len", ",", "span_len", ")", "\n", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "words", "=", "np", ".", "array", "(", "x", "[", ":", "l", "[", "i", "]", ",", "i", "]", ".", "tolist", "(", ")", ")", "\n", "shuf_segs", "=", "self", ".", "shuffle_segments", "(", "segs", ",", "unmasked_tokens", ")", "\n", "pos_i", "=", "self", ".", "unfold_segments", "(", "shuf_segs", ")", "\n", "output_i", "=", "words", "[", "pos_i", "]", ".", "copy", "(", ")", "\n", "target_i", "=", "words", "[", "pos_i", "-", "1", "]", ".", "copy", "(", ")", "\n", "words", "[", "pos_i", "]", "=", "self", ".", "mask_word", "(", "words", "[", "pos_i", "]", ")", "\n", "\n", "inputs", ".", "append", "(", "words", ")", "\n", "targets", ".", "append", "(", "target_i", ")", "\n", "outputs", ".", "append", "(", "output_i", ")", "\n", "positions", ".", "append", "(", "pos_i", "-", "1", ")", "\n", "\n", "", "x1", "=", "torch", ".", "LongTensor", "(", "max", "(", "l", ")", ",", "l", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "x2", "=", "torch", ".", "LongTensor", "(", "mask_len", ",", "l", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "y", "=", "torch", ".", "LongTensor", "(", "mask_len", ",", "l", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "pos", "=", "torch", ".", "LongTensor", "(", "mask_len", ",", "l", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "params", ".", "pad_index", ")", "\n", "l1", "=", "l", ".", "clone", "(", ")", "\n", "l2", "=", "torch", ".", "LongTensor", "(", "len2", ")", "\n", "for", "i", "in", "range", "(", "l", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "x1", "[", ":", "l1", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "inputs", "[", "i", "]", ")", ")", "\n", "x2", "[", ":", "l2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "targets", "[", "i", "]", ")", ")", "\n", "y", "[", ":", "l2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "outputs", "[", "i", "]", ")", ")", "\n", "pos", "[", ":", "l2", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "positions", "[", "i", "]", ")", ")", "\n", "\n", "", "pred_mask", "=", "y", "!=", "self", ".", "params", ".", "pad_index", "\n", "y", "=", "y", ".", "masked_select", "(", "pred_mask", ")", "\n", "return", "x1", ",", "l1", ",", "x2", ",", "l2", ",", "y", ",", "pred_mask", ",", "pos", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mt_step": [[846, 899], ["mass_trainer.EncDecTrainer.encoder.train", "mass_trainer.EncDecTrainer.decoder.train", "x1.clone().fill_", "x2.clone().fill_", "torch.arange", "x2[].masked_select", "utils.to_cuda", "mass_trainer.EncDecTrainer.encoder", "enc1.transpose.transpose.transpose", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.stats[].append", "mass_trainer.EncDecTrainer.optimize", "len2.size", "mass_trainer.EncDecTrainer.get_batch", "mass_trainer.EncDecTrainer.add_noise", "mass_trainer.EncDecTrainer.get_batch", "len2.max", "len", "loss.item", "x1.clone", "x2.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.add_noise", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch"], ["", "def", "mt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Machine translation step.\n        Can also be used for denoising auto-encoding.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate batch", "\n", "if", "lang1", "==", "lang2", ":", "\n", "            ", "(", "x1", ",", "len1", ")", "=", "self", ".", "get_batch", "(", "'ae'", ",", "lang1", ")", "\n", "(", "x2", ",", "len2", ")", "=", "(", "x1", ",", "len1", ")", "\n", "(", "x1", ",", "len1", ")", "=", "self", ".", "add_noise", "(", "x1", ",", "len1", ")", "\n", "", "else", ":", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "'mt'", ",", "lang1", ",", "lang2", ")", "\n", "", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# target words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# decode target sentence", "\n", "dec2", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'AE-%s'", "%", "lang1", ")", "if", "lang1", "==", "lang2", "else", "(", "'MT-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "[", "'encoder'", ",", "'decoder'", "]", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len2", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.bt_step": [[900, 964], ["mass_trainer.EncDecTrainer.get_batch", "x1.clone().fill_", "utils.to_cuda", "mass_trainer.EncDecTrainer.encoder", "enc2.transpose.transpose.transpose", "torch.arange", "x1[].masked_select", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.stats[].append", "mass_trainer.EncDecTrainer.optimize", "len1.size", "torch.no_grad", "mass_trainer.EncDecTrainer.encoder.eval", "mass_trainer.EncDecTrainer.decoder.eval", "_encoder", "enc1.transpose.transpose.transpose", "_decoder.generate", "x2.clone().fill_", "mass_trainer.EncDecTrainer.encoder.train", "mass_trainer.EncDecTrainer.decoder.train", "len1.max", "loss.item", "x1.clone", "int", "x2.clone", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train"], ["", "def", "bt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lang3", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Back-translation step for machine translation.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "assert", "lang1", "==", "lang3", "and", "lang1", "!=", "lang2", "and", "lang2", "is", "not", "None", "\n", "params", "=", "self", ".", "params", "\n", "_encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "_decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate source batch", "\n", "x1", ",", "len1", "=", "self", ".", "get_batch", "(", "'bt'", ",", "lang1", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# generate a translation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# evaluation mode", "\n", "            ", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "\n", "# encode source sentence and translate it", "\n", "enc1", "=", "_encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "x2", ",", "len2", "=", "_decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "int", "(", "1.3", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "5", ")", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# free CUDA memory", "\n", "del", "enc1", "\n", "\n", "# training mode", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "# encode generate sentence", "\n", "", "enc2", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len1", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len1", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len1", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y1", "=", "x1", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "\n", "# decode original sentence", "\n", "dec3", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "True", ",", "src_enc", "=", "enc2", ",", "src_len", "=", "len2", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec3", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y1", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'BT-%s-%s-%s'", "%", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "[", "'encoder'", ",", "'decoder'", "]", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len1", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len1", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.mass_step": [[965, 1003], ["mass_trainer.EncDecTrainer.encoder.train", "mass_trainer.EncDecTrainer.decoder.train", "mass_trainer.EncDecTrainer.get_batch", "mass_trainer.EncDecTrainer.restricted_mask_sent", "x1.clone().fill_", "x2.clone().fill_", "utils.to_cuda", "mass_trainer.EncDecTrainer.encoder", "enc1.transpose.transpose.transpose", "x1.ne", "enc_mask.transpose.transpose.transpose", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.stats[].append", "mass_trainer.EncDecTrainer.optimize", "len2.size", "int", "loss.item", "x1.clone", "x2.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.restricted_mask_sent", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "mass_step", "(", "self", ",", "lang", ",", "lambda_coeff", ")", ":", "\n", "        ", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang", "]", "\n", "x_", ",", "len_", "=", "self", ".", "get_batch", "(", "'mass'", ",", "lang", ")", "\n", "\n", "(", "x1", ",", "len1", ",", "x2", ",", "len2", ",", "y", ",", "pred_mask", ",", "positions", ")", "=", "self", ".", "restricted_mask_sent", "(", "x_", ",", "len_", ",", "int", "(", "params", ".", "lambda_span", ")", ")", "\n", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ",", "positions", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ",", "positions", ")", "\n", "\n", "enc1", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "enc_mask", "=", "x1", ".", "ne", "(", "params", ".", "mask_index", ")", "\n", "enc_mask", "=", "enc_mask", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "dec2", "=", "self", ".", "decoder", "(", "'fwd'", ",", "\n", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "\n", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ",", "positions", "=", "positions", ",", "enc_mask", "=", "enc_mask", ")", "\n", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'MA-%s'", "%", "lang", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "self", ".", "optimize", "(", "loss", ",", "[", "'encoder'", ",", "'decoder'", "]", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len2", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.EncDecTrainer.bmt_step": [[1004, 1052], ["mass_trainer.EncDecTrainer.encoder.train", "mass_trainer.EncDecTrainer.decoder.train", "mass_trainer.EncDecTrainer.get_batch", "x1.clone().fill_", "x2.clone().fill_", "torch.arange", "x2[].masked_select", "utils.to_cuda", "mass_trainer.EncDecTrainer.encoder", "enc1.transpose.transpose.transpose", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.decoder", "mass_trainer.EncDecTrainer.stats[].append", "mass_trainer.EncDecTrainer.optimize", "len2.size", "len2.max", "len", "loss.item", "x1.clone", "x2.clone"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.get_batch", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.Trainer.optimize"], ["", "def", "bmt_step", "(", "self", ",", "lang1", ",", "lang2", ",", "lambda_coeff", ")", ":", "\n", "        ", "\"\"\"\n        Machine translation step.\n        Can also be used for denoising auto-encoding.\n        \"\"\"", "\n", "assert", "lambda_coeff", ">=", "0", "\n", "if", "lambda_coeff", "==", "0", ":", "\n", "            ", "return", "\n", "", "params", "=", "self", ".", "params", "\n", "self", ".", "encoder", ".", "train", "(", ")", "\n", "self", ".", "decoder", ".", "train", "(", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# generate batch", "\n", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "self", ".", "get_batch", "(", "'bmt'", ",", "lang1", ",", "lang2", ",", "back", "=", "True", ")", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# target words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "self", ".", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# decode target sentence", "\n", "dec2", "=", "self", ".", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "\n", "# loss", "\n", "_", ",", "loss", "=", "self", ".", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "False", ")", "\n", "self", ".", "stats", "[", "(", "'BMT-%s-%s'", "%", "(", "lang1", ",", "lang2", ")", ")", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "loss", "=", "lambda_coeff", "*", "loss", "\n", "\n", "# optimize", "\n", "self", ".", "optimize", "(", "loss", ",", "[", "'encoder'", ",", "'decoder'", "]", ")", "\n", "\n", "# number of processed sentences / words", "\n", "self", ".", "n_sentences", "+=", "params", ".", "batch_size", "\n", "self", ".", "stats", "[", "'processed_s'", "]", "+=", "len2", ".", "size", "(", "0", ")", "\n", "self", ".", "stats", "[", "'processed_w'", "]", "+=", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.unfold_segments": [[1055, 1074], ["numpy.array", "pos.extend", "range"], "function", ["None"], ["", "", "def", "unfold_segments", "(", "segs", ")", ":", "\n", "    ", "\"\"\"Unfold the random mask segments, for example:\n       The shuffle segment is [2, 0, 0, 2, 0],\n       so the masked segment is like:\n       [1, 1, 0, 0, 1, 1, 0]\n       [1, 2, 3, 4, 5, 6, 7] (positions)\n       (1 means this token will be masked, otherwise not)\n       We return the position of the masked tokens like:\n       [1, 2, 5, 6]\n    \"\"\"", "\n", "pos", "=", "[", "]", "\n", "curr", "=", "1", "# We do not mask the start token", "\n", "for", "l", "in", "segs", ":", "\n", "        ", "if", "l", ">=", "1", ":", "\n", "            ", "pos", ".", "extend", "(", "[", "curr", "+", "i", "for", "i", "in", "range", "(", "l", ")", "]", ")", "\n", "curr", "+=", "l", "\n", "", "else", ":", "\n", "            ", "curr", "+=", "1", "\n", "", "", "return", "np", ".", "array", "(", "pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.random_mask_start": [[1076, 1084], ["numpy.random.random", "numpy.random.randint"], "function", ["None"], ["", "def", "random_mask_start", "(", "mlen_", ",", "len_", ")", ":", "\n", "    ", "p", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "p", ">=", "0.8", ":", "\n", "        ", "return", "0", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "        ", "return", "len_", "-", "mlen_", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "len_", "-", "mlen_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.mask_word": [[1085, 1099], ["torch.randint", "w.new().fill_", "torch.FloatTensor", "torch.multinomial().to", "print", "print", "torch.multinomial().to.size", "w.size", "w.new", "torch.multinomial", "w.new().fill_.size", "torch.randint.size", "_w_real.size", "w.size", "len"], "function", ["None"], ["", "", "def", "mask_word", "(", "w", ",", "n_words", "=", "100", ",", "mask_index", "=", "2", ")", ":", "\n", "    ", "_w_real", "=", "w", "\n", "# _w_rand = np.random.randint(self.params.n_words, size=w.shape)", "\n", "# _w_mask = np.full(w.shape, self.params.mask_index)", "\n", "_w_rand", "=", "torch", ".", "randint", "(", "n_words", ",", "size", "=", "w", ".", "size", "(", ")", ")", "\n", "_w_mask", "=", "w", ".", "new", "(", "w", ".", "size", "(", ")", ")", ".", "fill_", "(", "mask_index", ")", "\n", "pred_probs", "=", "torch", ".", "FloatTensor", "(", "[", "0.8", ",", "0.1", ",", "0.1", "]", ")", "\n", "probs", "=", "torch", ".", "multinomial", "(", "pred_probs", ",", "len", "(", "_w_real", ")", ",", "replacement", "=", "True", ")", ".", "to", "(", "w", ".", "device", ")", "\n", "# probs = probs.cpu()", "\n", "print", "(", "probs", ".", "size", "(", ")", ")", "\n", "print", "(", "'{},{},{}'", ".", "format", "(", "_w_mask", ".", "size", "(", ")", ",", "_w_rand", ".", "size", "(", ")", ",", "_w_real", ".", "size", "(", ")", ")", ")", "\n", "# _w = _w_mask * (probs == 0).numpy() + _w_real * (probs == 1).numpy() + _w_rand * (probs == 2).numpy()", "\n", "_w", "=", "_w_mask", "*", "(", "probs", "==", "0", ")", ".", "long", "(", ")", "+", "_w_real", "*", "(", "probs", "==", "1", ")", ".", "long", "(", ")", "+", "_w_rand", "*", "(", "probs", "==", "2", ")", ".", "long", "(", ")", "\n", "return", "_w", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.shuffle_segments": [[1103, 1117], ["numpy.random.random", "random.shuffle"], "function", ["None"], ["", "def", "shuffle_segments", "(", "segs", ",", "unmasked_tokens", ")", ":", "\n", "    ", "p", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "p", ">=", "0.8", ":", "\n", "        ", "shuf_segs", "=", "segs", "[", "1", ":", "]", "+", "unmasked_tokens", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "        ", "shuf_segs", "=", "segs", "[", ":", "-", "1", "]", "+", "unmasked_tokens", "\n", "", "else", ":", "\n", "        ", "shuf_segs", "=", "segs", "+", "unmasked_tokens", "\n", "", "random", ".", "shuffle", "(", "shuf_segs", ")", "\n", "if", "p", ">=", "0.8", ":", "\n", "        ", "shuf_segs", "=", "segs", "[", "0", ":", "1", "]", "+", "shuf_segs", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "        ", "shuf_segs", "=", "shuf_segs", "+", "segs", "[", "-", "1", ":", "]", "\n", "", "return", "shuf_segs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.get_segments": [[1119, 1127], ["segs.append", "segs.append"], "function", ["None"], ["", "def", "get_segments", "(", "mask_len", ",", "span_len", ")", ":", "\n", "    ", "segs", "=", "[", "]", "\n", "while", "mask_len", ">=", "span_len", ":", "\n", "        ", "segs", ".", "append", "(", "span_len", ")", "\n", "mask_len", "-=", "span_len", "\n", "", "if", "mask_len", "!=", "0", ":", "\n", "        ", "segs", ".", "append", "(", "mask_len", ")", "\n", "", "return", "segs", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.restricted_mask_sent_v2": [[1129, 1175], ["l.size", "mask_len.max", "x.clone", "x.new().fill_", "x.new().fill_", "x.new().fill_", "range", "y.masked_select.masked_select", "numpy.random.random", "mass_trainer.random_mask_start"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.random_mask_start"], ["", "def", "restricted_mask_sent_v2", "(", "x", ",", "l", ",", "span_len", "=", "100000", ",", "pad_index", "=", "0", ",", "word_mass", "=", "0.5", ")", ":", "\n", "    ", "if", "span_len", "<=", "0", ":", "\n", "        ", "span_len", "=", "1", "\n", "", "max_len", "=", "0", "\n", "bsz", "=", "l", ".", "size", "(", "0", ")", "\n", "# positions, inputs, targets, outputs, = [], [], [], []", "\n", "lens", "=", "l", "-", "2", "# remove eos", "\n", "device", "=", "x", ".", "device", "\n", "mask_len", "=", "(", "lens", ".", "float", "(", ")", "*", "word_mass", ")", ".", "int", "(", ")", "\n", "mask_len_max", "=", "mask_len", ".", "max", "(", ")", "\n", "# FIXME: only one segment is available", "\n", "# inputs = x.new(x.size(0), x.size(1)).fill_(self.params.pad_index)", "\n", "inputs", "=", "x", ".", "clone", "(", ")", "\n", "targets", "=", "x", ".", "new", "(", "mask_len_max", "+", "1", ",", "x", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "pad_index", ")", "\n", "outputs", "=", "x", ".", "new", "(", "mask_len_max", "+", "1", ",", "x", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "pad_index", ")", "\n", "positions", "=", "x", ".", "new", "(", "mask_len_max", "+", "1", ",", "x", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "0", ")", "\n", "def", "random_mask_start", "(", "mlen_", ",", "len_", ")", ":", "\n", "        ", "p", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "if", "p", ">=", "0.8", ":", "\n", "            ", "return", "0", "\n", "", "elif", "p", ">=", "0.6", ":", "\n", "            ", "return", "len_", "-", "mlen_", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "random", ".", "randint", "(", "0", ",", "len_", "-", "mlen_", ")", "\n", "", "", "mask_pos_rand", "=", "[", "random_mask_start", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "mask_len", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "lens", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "]", "\n", "include_indices", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "        ", "if", "mask_len", "[", "i", "]", "<=", "0", ":", "\n", "            ", "continue", "\n", "", "include_indices", ".", "append", "(", "i", ")", "\n", "mask_idx", "=", "torch", ".", "arange", "(", "\n", "1", "+", "mask_pos_rand", "[", "i", "]", ",", "1", "+", "mask_pos_rand", "[", "i", "]", "+", "mask_len", "[", "i", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "targets", "[", "1", ":", "mask_idx", ".", "size", "(", "0", ")", "+", "1", ",", "i", "]", "=", "x", "[", "mask_idx", "-", "1", ",", "i", "]", "\n", "outputs", "[", ":", "mask_idx", ".", "size", "(", "0", ")", ",", "i", "]", "=", "x", "[", "mask_idx", ",", "i", "]", "\n", "inputs", "[", "mask_idx", ",", "i", "]", "=", "mask_word", "(", "inputs", "[", "mask_idx", ",", "i", "]", ")", "\n", "positions", "[", "1", ":", "mask_idx", ".", "size", "(", "0", ")", "+", "1", ",", "i", "]", "=", "mask_idx", "-", "1", "\n", "", "x1", "=", "inputs", "[", ":", ",", "include_indices", "]", "\n", "l1", "=", "l", "[", "include_indices", "]", "\n", "x2", "=", "targets", "[", ":", ",", "include_indices", "]", "\n", "l2", "=", "(", "mask_len", "+", "1", ")", "[", "include_indices", "]", "\n", "positions", "=", "positions", "[", ":", ",", "include_indices", "]", "\n", "y", "=", "outputs", "[", ":", ",", "include_indices", "]", "\n", "pred_mask", "=", "y", "!=", "pad_index", "\n", "y", "=", "y", ".", "masked_select", "(", "pred_mask", ")", "\n", "# return inputs, l, targets, l2, y, pred_mask, positions", "\n", "return", "x1", ",", "l1", ",", "x2", ",", "l2", ",", "y", ",", "pred_mask", ",", "positions", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.run": [[1177, 1187], ["round", "mass_trainer.get_segments", "mass_trainer.shuffle_segments", "mass_trainer.unfold_segments", "range"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.get_segments", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.shuffle_segments", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_trainer.unfold_segments"], ["", "def", "run", "(", "l0", ")", ":", "\n", "    ", "span_len", "=", "10000", "\n", "length", "=", "30", "\n", "# l0 = 25", "\n", "mask_len", "=", "round", "(", "length", "*", "0.5", ")", "\n", "unmasked_tokens", "=", "[", "0", "for", "i", "in", "range", "(", "l0", "-", "mask_len", "-", "1", ")", "]", "\n", "segs", "=", "get_segments", "(", "mask_len", ",", "span_len", ")", "\n", "shuf_segs", "=", "shuffle_segments", "(", "segs", ",", "unmasked_tokens", ")", "\n", "pos_i", "=", "unfold_segments", "(", "shuf_segs", ")", "\n", "return", "shuf_segs", ",", "pos_i", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.check_model_params": [[18, 66], ["params.word_mask_keep_rand.split", "len", "float", "all", "len", "params.asm_cutoffs.split", "all", "sum", "int", "os.path.isfile", "params.reload_model.split", "all", "x.isdigit", "len", "os.path.isfile"], "function", ["None"], ["def", "check_model_params", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Check models parameters.\n    \"\"\"", "\n", "# masked language modeling task parameters", "\n", "assert", "params", ".", "bptt", ">=", "1", "\n", "assert", "0", "<=", "params", ".", "word_pred", "<", "1", "\n", "assert", "0", "<=", "params", ".", "sample_alpha", "<", "1", "\n", "s", "=", "params", ".", "word_mask_keep_rand", ".", "split", "(", "','", ")", "\n", "assert", "len", "(", "s", ")", "==", "3", "\n", "s", "=", "[", "float", "(", "x", ")", "for", "x", "in", "s", "]", "\n", "assert", "all", "(", "[", "0", "<=", "x", "<=", "1", "for", "x", "in", "s", "]", ")", "and", "sum", "(", "s", ")", "==", "1", "\n", "params", ".", "word_mask", "=", "s", "[", "0", "]", "\n", "params", ".", "word_keep", "=", "s", "[", "1", "]", "\n", "params", ".", "word_rand", "=", "s", "[", "2", "]", "\n", "\n", "# input sentence noise for DAE", "\n", "if", "len", "(", "params", ".", "ae_steps", ")", "==", "0", ":", "\n", "        ", "assert", "params", ".", "word_shuffle", "==", "0", "\n", "assert", "params", ".", "word_dropout", "==", "0", "\n", "assert", "params", ".", "word_blank", "==", "0", "\n", "", "else", ":", "\n", "        ", "assert", "params", ".", "word_shuffle", "==", "0", "or", "params", ".", "word_shuffle", ">", "1", "\n", "assert", "0", "<=", "params", ".", "word_dropout", "<", "1", "\n", "assert", "0", "<=", "params", ".", "word_blank", "<", "1", "\n", "\n", "# model dimensions", "\n", "", "assert", "params", ".", "emb_dim", "%", "params", ".", "n_heads", "==", "0", "\n", "\n", "# share input and output embeddings", "\n", "assert", "params", ".", "share_inout_emb", "is", "False", "or", "params", ".", "asm", "is", "False", "\n", "\n", "# adaptive softmax", "\n", "if", "params", ".", "asm", ":", "\n", "        ", "assert", "params", ".", "asm_div_value", ">", "1", "\n", "s", "=", "params", ".", "asm_cutoffs", ".", "split", "(", "','", ")", "\n", "assert", "all", "(", "[", "x", ".", "isdigit", "(", ")", "for", "x", "in", "s", "]", ")", "\n", "params", ".", "asm_cutoffs", "=", "[", "int", "(", "x", ")", "for", "x", "in", "s", "]", "\n", "assert", "params", ".", "max_vocab", "==", "-", "1", "or", "params", ".", "asm_cutoffs", "[", "-", "1", "]", "<", "params", ".", "max_vocab", "\n", "\n", "# reload a pretrained model", "\n", "", "if", "params", ".", "reload_model", "!=", "''", ":", "\n", "        ", "if", "params", ".", "encoder_only", ":", "\n", "            ", "assert", "os", ".", "path", ".", "isfile", "(", "params", ".", "reload_model", ")", "\n", "", "else", ":", "\n", "            ", "s", "=", "params", ".", "reload_model", ".", "split", "(", "','", ")", "\n", "assert", "len", "(", "s", ")", "==", "2", "\n", "assert", "all", "(", "[", "x", "==", "''", "or", "os", ".", "path", ".", "isfile", "(", "x", ")", "for", "x", "in", "s", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.set_pretrain_emb": [[68, 83], ["logger.info", "torch.no_grad", "range", "len", "word2id.get", "embeddings[].cuda", "embeddings[].cuda", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], ["", "", "", "def", "set_pretrain_emb", "(", "model", ",", "dico", ",", "word2id", ",", "embeddings", ")", ":", "\n", "    ", "\"\"\"\n    Pretrain word embeddings.\n    \"\"\"", "\n", "n_found", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "dico", ")", ")", ":", "\n", "            ", "idx", "=", "word2id", ".", "get", "(", "dico", "[", "i", "]", ",", "None", ")", "\n", "if", "idx", "is", "None", ":", "\n", "                ", "continue", "\n", "", "n_found", "+=", "1", "\n", "model", ".", "embeddings", ".", "weight", "[", "i", "]", "=", "embeddings", "[", "idx", "]", ".", "cuda", "(", ")", "\n", "model", ".", "pred_layer", ".", "proj", ".", "weight", "[", "i", "]", "=", "embeddings", "[", "idx", "]", ".", "cuda", "(", ")", "\n", "", "", "logger", ".", "info", "(", "\"Pretrained %i/%i words (%.3f%%).\"", "\n", "%", "(", "n_found", ",", "len", "(", "dico", ")", ",", "100.", "*", "n_found", "/", "len", "(", "dico", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_model_init.mass_build_model": [[85, 150], ["mass_transformer.TransformerModel", "logger.debug", "logger.info", "mass_transformer.TransformerModel.cuda", "mass_transformer.TransformerModel", "mass_transformer.TransformerModel", "logger.debug", "logger.debug", "logger.info", "logger.info", "logger.info", "all", "mass_transformer.TransformerModel.load_state_dict", "path.split", "mass_transformer.TransformerModel.cuda", "mass_transformer.TransformerModel.cuda", "torch.load", "sum", "logger.info", "torch.load", "all", "mass_transformer.TransformerModel.load_state_dict", "logger.info", "torch.load", "all", "mass_transformer.TransformerModel.load_state_dict", "sum", "sum", "k.startswith", "reloaded.keys", "reloaded.items", "p.numel", "k.startswith", "k.startswith", "p.numel", "p.numel", "storage.cuda", "len", "mass_transformer.TransformerModel.parameters", "storage.cuda", "torch.load.keys", "torch.load.items", "storage.cuda", "torch.load.keys", "torch.load.items", "mass_transformer.TransformerModel.parameters", "mass_transformer.TransformerModel.parameters", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda"], ["", "def", "mass_build_model", "(", "params", ",", "dico", ",", "checkpoint", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Build model.\n    \"\"\"", "\n", "if", "params", ".", "encoder_only", ":", "\n", "# build", "\n", "        ", "model", "=", "TransformerModel", "(", "params", ",", "dico", ",", "is_encoder", "=", "True", ",", "with_output", "=", "True", ")", "\n", "\n", "# reload a pretrained model", "\n", "if", "params", ".", "reload_model", "!=", "''", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reloading model from %s ...\"", "%", "params", ".", "reload_model", ")", "\n", "reloaded", "=", "torch", ".", "load", "(", "params", ".", "reload_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "params", ".", "local_rank", ")", ")", "[", "'model'", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "reloaded", ".", "keys", "(", ")", "]", ")", ":", "\n", "                ", "reloaded", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "reloaded", ".", "items", "(", ")", "}", "\n", "\n", "# # HACK to reload models with less layers", "\n", "# for i in range(12, 24):", "\n", "#     for k in TRANSFORMER_LAYER_PARAMS:", "\n", "#         k = k % i", "\n", "#         if k in model.state_dict() and k not in reloaded:", "\n", "#             logger.warning(\"Parameter %s not found. Ignoring ...\" % k)", "\n", "#             reloaded[k] = model.state_dict()[k]", "\n", "\n", "", "model", ".", "load_state_dict", "(", "reloaded", ")", "\n", "\n", "", "logger", ".", "debug", "(", "\"Model: {}\"", ".", "format", "(", "model", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of parameters (model): %i\"", "%", "sum", "(", "[", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ")", ")", "\n", "\n", "return", "model", ".", "cuda", "(", ")", "\n", "\n", "", "else", ":", "\n", "# build", "\n", "        ", "encoder", "=", "TransformerModel", "(", "params", ",", "dico", ",", "is_encoder", "=", "True", ",", "with_output", "=", "True", ")", "# TODO: only output when necessary - len(params.clm_steps + params.mlm_steps) > 0", "\n", "decoder", "=", "TransformerModel", "(", "params", ",", "dico", ",", "is_encoder", "=", "False", ",", "with_output", "=", "True", ")", "\n", "\n", "# reload a pretrained model", "\n", "if", "params", ".", "reload_model", "!=", "''", "or", "checkpoint", "is", "not", "None", ":", "\n", "            ", "path", "=", "checkpoint", "if", "checkpoint", "is", "not", "None", "else", "params", ".", "reload_model", "\n", "enc_path", ",", "dec_path", "=", "path", ".", "split", "(", "','", ")", "\n", "assert", "not", "(", "enc_path", "==", "''", "and", "dec_path", "==", "''", ")", "\n", "\n", "# reload encoder", "\n", "if", "enc_path", "!=", "''", ":", "\n", "                ", "logger", ".", "info", "(", "\"Reloading encoder from %s ...\"", "%", "enc_path", ")", "\n", "enc_reload", "=", "torch", ".", "load", "(", "enc_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "params", ".", "local_rank", ")", ")", "\n", "enc_reload", "=", "enc_reload", "[", "'model'", "if", "'model'", "in", "enc_reload", "else", "'encoder'", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "enc_reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                    ", "enc_reload", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "enc_reload", ".", "items", "(", ")", "}", "\n", "", "encoder", ".", "load_state_dict", "(", "enc_reload", ")", "\n", "\n", "# reload decoder", "\n", "", "if", "dec_path", "!=", "''", ":", "\n", "                ", "logger", ".", "info", "(", "\"Reloading decoder from %s ...\"", "%", "dec_path", ")", "\n", "dec_reload", "=", "torch", ".", "load", "(", "dec_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "params", ".", "local_rank", ")", ")", "\n", "dec_reload", "=", "dec_reload", "[", "'model'", "if", "'model'", "in", "dec_reload", "else", "'decoder'", "]", "\n", "if", "all", "(", "[", "k", ".", "startswith", "(", "'module.'", ")", "for", "k", "in", "dec_reload", ".", "keys", "(", ")", "]", ")", ":", "\n", "                    ", "dec_reload", "=", "{", "k", "[", "len", "(", "'module.'", ")", ":", "]", ":", "v", "for", "k", ",", "v", "in", "dec_reload", ".", "items", "(", ")", "}", "\n", "", "decoder", ".", "load_state_dict", "(", "dec_reload", ",", "strict", "=", "False", ")", "\n", "\n", "", "", "logger", ".", "debug", "(", "\"Encoder: {}\"", ".", "format", "(", "encoder", ")", ")", "\n", "logger", ".", "debug", "(", "\"Decoder: {}\"", ".", "format", "(", "decoder", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of parameters (encoder): %i\"", "%", "sum", "(", "[", "p", ".", "numel", "(", ")", "for", "p", "in", "encoder", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"Number of parameters (decoder): %i\"", "%", "sum", "(", "[", "p", ".", "numel", "(", ")", "for", "p", "in", "decoder", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", ")", ")", "\n", "\n", "return", "encoder", ".", "cuda", "(", ")", ",", "decoder", ".", "cuda", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.__init__": [[31, 41], ["dictionary.Dictionary.check_valid", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid"], ["    ", "def", "__init__", "(", "self", ",", "id2word", ",", "word2id", ",", "counts", ")", ":", "\n", "        ", "assert", "len", "(", "id2word", ")", "==", "len", "(", "word2id", ")", "==", "len", "(", "counts", ")", "\n", "self", ".", "id2word", "=", "id2word", "\n", "self", ".", "word2id", "=", "word2id", "\n", "self", ".", "counts", "=", "counts", "\n", "self", ".", "bos_index", "=", "word2id", "[", "BOS_WORD", "]", "\n", "self", ".", "eos_index", "=", "word2id", "[", "EOS_WORD", "]", "\n", "self", ".", "pad_index", "=", "word2id", "[", "PAD_WORD", "]", "\n", "self", ".", "unk_index", "=", "word2id", "[", "UNK_WORD", "]", "\n", "self", ".", "check_valid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.__len__": [[42, 47], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the number of words in the dictionary.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "id2word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.__getitem__": [[48, 53], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"\n        Returns the word of the specified index.\n        \"\"\"", "\n", "return", "self", ".", "id2word", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.__contains__": [[54, 59], ["None"], "methods", ["None"], ["", "def", "__contains__", "(", "self", ",", "w", ")", ":", "\n", "        ", "\"\"\"\n        Returns whether a word is in the dictionary.\n        \"\"\"", "\n", "return", "w", "in", "self", ".", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.__eq__": [[60, 69], ["dictionary.Dictionary.check_valid", "y.check_valid", "all", "len", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid"], ["", "def", "__eq__", "(", "self", ",", "y", ")", ":", "\n", "        ", "\"\"\"\n        Compare this dictionary with another one.\n        \"\"\"", "\n", "self", ".", "check_valid", "(", ")", "\n", "y", ".", "check_valid", "(", ")", "\n", "if", "len", "(", "self", ".", "id2word", ")", "!=", "len", "(", "y", ")", ":", "\n", "            ", "return", "False", "\n", "", "return", "all", "(", "self", ".", "id2word", "[", "i", "]", "==", "y", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "y", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid": [[70, 88], ["all", "range", "range", "len", "len", "len", "set", "set", "len", "dictionary.Dictionary.word2id.keys", "dictionary.Dictionary.counts.keys", "len", "range"], "methods", ["None"], ["", "def", "check_valid", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Check that the dictionary is valid.\n        \"\"\"", "\n", "assert", "self", ".", "bos_index", "==", "0", "\n", "assert", "self", ".", "eos_index", "==", "1", "\n", "assert", "self", ".", "pad_index", "==", "2", "\n", "assert", "self", ".", "unk_index", "==", "3", "\n", "assert", "all", "(", "self", ".", "id2word", "[", "4", "+", "i", "]", "==", "SPECIAL_WORD", "%", "i", "for", "i", "in", "range", "(", "SPECIAL_WORDS", ")", ")", "\n", "assert", "len", "(", "self", ".", "id2word", ")", "==", "len", "(", "self", ".", "word2id", ")", "==", "len", "(", "self", ".", "counts", ")", "\n", "assert", "set", "(", "self", ".", "word2id", ".", "keys", "(", ")", ")", "==", "set", "(", "self", ".", "counts", ".", "keys", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2word", ")", ")", ":", "\n", "            ", "assert", "self", ".", "word2id", "[", "self", ".", "id2word", "[", "i", "]", "]", "==", "i", "\n", "", "last_count", "=", "1e18", "\n", "for", "i", "in", "range", "(", "4", "+", "SPECIAL_WORDS", ",", "len", "(", "self", ".", "id2word", ")", "-", "1", ")", ":", "\n", "            ", "count", "=", "self", ".", "counts", "[", "self", ".", "id2word", "[", "i", "]", "]", "\n", "assert", "count", "<=", "last_count", "\n", "last_count", "=", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index": [[89, 97], ["dictionary.Dictionary.word2id.get"], "methods", ["None"], ["", "", "def", "index", "(", "self", ",", "word", ",", "no_unk", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the index of the specified word.\n        \"\"\"", "\n", "if", "no_unk", ":", "\n", "            ", "return", "self", ".", "word2id", "[", "word", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "word2id", ".", "get", "(", "word", ",", "self", ".", "unk_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.max_vocab": [[98, 110], ["len", "dictionary.Dictionary.check_valid", "logger.info", "dictionary.Dictionary.id2word.items", "dictionary.Dictionary.id2word.items", "dictionary.Dictionary.counts.items", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid"], ["", "", "def", "max_vocab", "(", "self", ",", "max_vocab", ")", ":", "\n", "        ", "\"\"\"\n        Limit the vocabulary size.\n        \"\"\"", "\n", "assert", "max_vocab", ">=", "1", "\n", "init_size", "=", "len", "(", "self", ")", "\n", "self", ".", "id2word", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "if", "k", "<", "max_vocab", "}", "\n", "self", ".", "word2id", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "}", "\n", "self", ".", "counts", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "counts", ".", "items", "(", ")", "if", "k", "in", "self", ".", "word2id", "}", "\n", "self", ".", "check_valid", "(", ")", "\n", "logger", ".", "info", "(", "\"Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words).\"", "\n", "%", "(", "max_vocab", ",", "init_size", ",", "len", "(", "self", ")", ",", "init_size", "-", "len", "(", "self", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.min_count": [[111, 123], ["len", "dictionary.Dictionary.check_valid", "logger.info", "dictionary.Dictionary.id2word.items", "dictionary.Dictionary.id2word.items", "dictionary.Dictionary.counts.items", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.check_valid"], ["", "def", "min_count", "(", "self", ",", "min_count", ")", ":", "\n", "        ", "\"\"\"\n        Threshold on the word frequency counts.\n        \"\"\"", "\n", "assert", "min_count", ">=", "0", "\n", "init_size", "=", "len", "(", "self", ")", "\n", "self", ".", "id2word", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "if", "self", ".", "counts", "[", "self", ".", "id2word", "[", "k", "]", "]", ">=", "min_count", "or", "k", "<", "4", "+", "SPECIAL_WORDS", "}", "\n", "self", ".", "word2id", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "id2word", ".", "items", "(", ")", "}", "\n", "self", ".", "counts", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "counts", ".", "items", "(", ")", "if", "k", "in", "self", ".", "word2id", "}", "\n", "self", ".", "check_valid", "(", ")", "\n", "logger", ".", "info", "(", "\"Minimum frequency count: %i. Dictionary size: %i -> %i (removed %i words).\"", "\n", "%", "(", "min_count", ",", "init_size", ",", "len", "(", "self", ")", ",", "init_size", "-", "len", "(", "self", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.read_vocab": [[124, 164], ["os.path.isfile", "range", "open", "enumerate", "open.close", "dictionary.Dictionary", "logger.info", "line.rstrip().split.rstrip().split.rstrip().split", "line[].isdigit", "int", "logger.warning", "word2id.keys", "len", "len", "print", "line[].isdigit", "print", "word2id.items", "len", "line.rstrip().split.rstrip().split.rstrip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "read_vocab", "(", "vocab_path", ")", ":", "\n", "        ", "\"\"\"\n        Create a dictionary from a vocabulary file.\n        \"\"\"", "\n", "skipped", "=", "0", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "vocab_path", ")", ",", "vocab_path", "\n", "word2id", "=", "{", "BOS_WORD", ":", "0", ",", "EOS_WORD", ":", "1", ",", "PAD_WORD", ":", "2", ",", "UNK_WORD", ":", "3", "}", "\n", "for", "i", "in", "range", "(", "SPECIAL_WORDS", ")", ":", "\n", "            ", "word2id", "[", "SPECIAL_WORD", "%", "i", "]", "=", "4", "+", "i", "\n", "", "counts", "=", "{", "k", ":", "0", "for", "k", "in", "word2id", ".", "keys", "(", ")", "}", "\n", "f", "=", "open", "(", "vocab_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "'\\u2028'", "in", "line", ":", "\n", "                ", "skipped", "+=", "1", "\n", "continue", "\n", "", "line", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line", ")", "!=", "2", ":", "\n", "                ", "skipped", "+=", "1", "\n", "continue", "\n", "", "assert", "len", "(", "line", ")", "==", "2", ",", "(", "i", ",", "line", ")", "\n", "# assert line[0] not in word2id and line[1].isdigit(), (i, line)", "\n", "assert", "line", "[", "1", "]", ".", "isdigit", "(", ")", ",", "(", "i", ",", "line", ")", "\n", "if", "line", "[", "0", "]", "in", "word2id", ":", "\n", "                ", "skipped", "+=", "1", "\n", "print", "(", "'%s already in vocab'", "%", "line", "[", "0", "]", ")", "\n", "continue", "\n", "", "if", "not", "line", "[", "1", "]", ".", "isdigit", "(", ")", ":", "\n", "                ", "skipped", "+=", "1", "\n", "print", "(", "'Empty word at line %s with count %s'", "%", "(", "i", ",", "line", ")", ")", "\n", "continue", "\n", "", "word2id", "[", "line", "[", "0", "]", "]", "=", "4", "+", "SPECIAL_WORDS", "+", "i", "-", "skipped", "# shift because of extra words", "\n", "counts", "[", "line", "[", "0", "]", "]", "=", "int", "(", "line", "[", "1", "]", ")", "\n", "", "f", ".", "close", "(", ")", "\n", "id2word", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "word2id", ".", "items", "(", ")", "}", "\n", "dico", "=", "Dictionary", "(", "id2word", ",", "word2id", ",", "counts", ")", "\n", "logger", ".", "info", "(", "\"Read %i words from the vocabulary file.\"", "%", "len", "(", "dico", ")", ")", "\n", "if", "skipped", ">", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Skipped %i empty lines!\"", "%", "skipped", ")", "\n", "", "return", "dico", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index_data": [[165, 229], ["open", "enumerate", "open.close", "numpy.int64", "os.path.isfile", "print", "torch.load", "line.rstrip().split", "numpy.int64.append", "numpy.int32.extend", "numpy.int32.append", "len", "numpy.uint16", "numpy.int32.min", "print", "torch.save", "print", "len", "print", "dico.index", "indexed.append", "len", "numpy.int32", "Exception", "line.rstrip", "logger.warning", "len", "unk_words.get", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["", "@", "staticmethod", "\n", "def", "index_data", "(", "path", ",", "bin_path", ",", "dico", ")", ":", "\n", "        ", "\"\"\"\n        Index sentences with a dictionary.\n        \"\"\"", "\n", "if", "bin_path", "is", "not", "None", "and", "os", ".", "path", ".", "isfile", "(", "bin_path", ")", ":", "\n", "            ", "print", "(", "\"Loading data from %s ...\"", "%", "bin_path", ")", "\n", "data", "=", "torch", ".", "load", "(", "bin_path", ")", "\n", "assert", "dico", "==", "data", "[", "'dico'", "]", "\n", "return", "data", "\n", "\n", "", "positions", "=", "[", "]", "\n", "sentences", "=", "[", "]", "\n", "unk_words", "=", "{", "}", "\n", "\n", "# index sentences", "\n", "f", "=", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "i", "%", "1000000", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "print", "(", "i", ")", "\n", "", "s", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", "\n", "# skip empty sentences", "\n", "if", "len", "(", "s", ")", "==", "0", ":", "\n", "                ", "print", "(", "\"Empty sentence in line %i.\"", "%", "i", ")", "\n", "# index sentence words", "\n", "", "count_unk", "=", "0", "\n", "indexed", "=", "[", "]", "\n", "for", "w", "in", "s", ":", "\n", "                ", "word_id", "=", "dico", ".", "index", "(", "w", ",", "no_unk", "=", "False", ")", "\n", "# if we find a special word which is not an unknown word, skip the sentence", "\n", "if", "0", "<=", "word_id", "<", "4", "+", "SPECIAL_WORDS", "and", "word_id", "!=", "3", ":", "\n", "                    ", "logger", ".", "warning", "(", "'Found unexpected special word \"%s\" (%i)!!'", "%", "(", "w", ",", "word_id", ")", ")", "\n", "continue", "\n", "", "assert", "word_id", ">=", "0", "\n", "indexed", ".", "append", "(", "word_id", ")", "\n", "if", "word_id", "==", "dico", ".", "unk_index", ":", "\n", "                    ", "unk_words", "[", "w", "]", "=", "unk_words", ".", "get", "(", "w", ",", "0", ")", "+", "1", "\n", "count_unk", "+=", "1", "\n", "# add sentence", "\n", "", "", "positions", ".", "append", "(", "[", "len", "(", "sentences", ")", ",", "len", "(", "sentences", ")", "+", "len", "(", "indexed", ")", "]", ")", "\n", "sentences", ".", "extend", "(", "indexed", ")", "\n", "sentences", ".", "append", "(", "1", ")", "# EOS index", "\n", "", "f", ".", "close", "(", ")", "\n", "\n", "# tensorize data", "\n", "positions", "=", "np", ".", "int64", "(", "positions", ")", "\n", "if", "len", "(", "dico", ")", "<", "1", "<<", "16", ":", "\n", "            ", "sentences", "=", "np", ".", "uint16", "(", "sentences", ")", "\n", "", "elif", "len", "(", "dico", ")", "<", "1", "<<", "31", ":", "\n", "            ", "sentences", "=", "np", ".", "int32", "(", "sentences", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Dictionary is too big.\"", ")", "\n", "", "assert", "sentences", ".", "min", "(", ")", ">=", "0", "\n", "data", "=", "{", "\n", "'dico'", ":", "dico", ",", "\n", "'positions'", ":", "positions", ",", "\n", "'sentences'", ":", "sentences", ",", "\n", "'unk_words'", ":", "unk_words", ",", "\n", "}", "\n", "if", "bin_path", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"Saving the data to %s ...\"", "%", "bin_path", ")", "\n", "torch", ".", "save", "(", "data", ",", "bin_path", ",", "pickle_protocol", "=", "4", ")", "\n", "\n", "", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.StreamDataset.__init__": [[19, 45], ["len", "math.ceil", "len", "torch.LongTensor().fill_", "len", "len", "numpy.zeros", "buffer.reshape", "numpy.zeros", "torch.LongTensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sent", ",", "pos", ",", "bs", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Prepare batches for data iterator.\n        \"\"\"", "\n", "bptt", "=", "params", ".", "bptt", "\n", "self", ".", "eos", "=", "params", ".", "eos_index", "\n", "\n", "# checks", "\n", "assert", "len", "(", "pos", ")", "==", "(", "sent", "==", "self", ".", "eos", ")", ".", "sum", "(", ")", "\n", "assert", "len", "(", "pos", ")", "==", "(", "sent", "[", "pos", "[", ":", ",", "1", "]", "]", "==", "self", ".", "eos", ")", ".", "sum", "(", ")", "\n", "\n", "n_tokens", "=", "len", "(", "sent", ")", "\n", "n_batches", "=", "math", ".", "ceil", "(", "n_tokens", "/", "(", "bs", "*", "bptt", ")", ")", "\n", "t_size", "=", "n_batches", "*", "bptt", "*", "bs", "\n", "\n", "buffer", "=", "np", ".", "zeros", "(", "t_size", ",", "dtype", "=", "sent", ".", "dtype", ")", "+", "self", ".", "eos", "\n", "buffer", "[", "t_size", "-", "n_tokens", ":", "]", "=", "sent", "\n", "buffer", "=", "buffer", ".", "reshape", "(", "(", "bs", ",", "n_batches", "*", "bptt", ")", ")", ".", "T", "\n", "self", ".", "data", "=", "np", ".", "zeros", "(", "(", "n_batches", "*", "bptt", "+", "1", ",", "bs", ")", ",", "dtype", "=", "sent", ".", "dtype", ")", "+", "self", ".", "eos", "\n", "self", ".", "data", "[", "1", ":", "]", "=", "buffer", "\n", "\n", "self", ".", "bptt", "=", "bptt", "\n", "self", ".", "n_tokens", "=", "n_tokens", "\n", "self", ".", "n_batches", "=", "n_batches", "\n", "self", ".", "n_sentences", "=", "len", "(", "pos", ")", "\n", "self", ".", "lengths", "=", "torch", ".", "LongTensor", "(", "bs", ")", ".", "fill_", "(", "bptt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.StreamDataset.__len__": [[46, 51], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of sentences in the dataset.\n        \"\"\"", "\n", "return", "self", ".", "n_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.StreamDataset.select_data": [[52, 66], ["logger.info", "logger.warning"], "methods", ["None"], ["", "def", "select_data", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "\"\"\"\n        Only select a subset of the dataset.\n        \"\"\"", "\n", "if", "not", "(", "0", "<=", "a", "<", "b", "<=", "self", ".", "n_batches", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Invalid split values: %i %i - %i\"", "%", "(", "a", ",", "b", ",", "self", ".", "n_batches", ")", ")", "\n", "return", "\n", "", "assert", "0", "<=", "a", "<", "b", "<=", "self", ".", "n_batches", "\n", "logger", ".", "info", "(", "\"Selecting batches from %i to %i ...\"", "%", "(", "a", ",", "b", ")", ")", "\n", "\n", "# sub-select", "\n", "self", ".", "data", "=", "self", ".", "data", "[", "a", "*", "self", ".", "bptt", ":", "b", "*", "self", ".", "bptt", "]", "\n", "self", ".", "n_batches", "=", "b", "-", "a", "\n", "self", ".", "n_sentences", "=", "(", "self", ".", "data", "==", "self", ".", "eos", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.StreamDataset.get_iterator": [[67, 76], ["torch.from_numpy", "dataset.StreamDataset.data[].astype"], "methods", ["None"], ["", "def", "get_iterator", "(", "self", ",", "shuffle", ",", "subsample", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Return a sentences iterator.\n        \"\"\"", "\n", "indexes", "=", "(", "np", ".", "random", ".", "permutation", "if", "shuffle", "else", "range", ")", "(", "self", ".", "n_batches", "//", "subsample", ")", "\n", "for", "i", "in", "indexes", ":", "\n", "            ", "a", "=", "self", ".", "bptt", "*", "i", "\n", "b", "=", "self", ".", "bptt", "*", "(", "i", "+", "1", ")", "\n", "yield", "torch", ".", "from_numpy", "(", "self", ".", "data", "[", "a", ":", "b", "]", ".", "astype", "(", "np", ".", "int64", ")", ")", ",", "self", ".", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.__init__": [[80, 100], ["dataset.Dataset.check", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["    ", "def", "__init__", "(", "self", ",", "sent", ",", "pos", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "eos_index", "=", "params", ".", "eos_index", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "self", ".", "batch_size", "=", "params", ".", "batch_size", "\n", "self", ".", "tokens_per_batch", "=", "params", ".", "tokens_per_batch", "\n", "self", ".", "max_batch_size", "=", "params", ".", "max_batch_size", "\n", "\n", "self", ".", "sent", "=", "sent", "\n", "self", ".", "pos", "=", "pos", "\n", "self", ".", "lengths", "=", "self", ".", "pos", "[", ":", ",", "1", "]", "-", "self", ".", "pos", "[", ":", ",", "0", "]", "\n", "\n", "# check number of sentences", "\n", "assert", "len", "(", "self", ".", "pos", ")", "==", "(", "self", ".", "sent", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "\n", "\n", "# # remove empty sentences", "\n", "# self.remove_empty_sentences()", "\n", "\n", "# sanity checks", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.__len__": [[101, 106], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of sentences in the dataset.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.check": [[107, 113], ["len"], "methods", ["None"], ["", "def", "check", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Sanity checks.\n        \"\"\"", "\n", "eos", "=", "self", ".", "eos_index", "\n", "assert", "len", "(", "self", ".", "pos", ")", "==", "(", "self", ".", "sent", "[", "self", ".", "pos", "[", ":", ",", "1", "]", "]", "==", "eos", ")", ".", "sum", "(", ")", "# check sentences indices", "\n", "# assert self.lengths.min() > 0                                     # check empty sentences", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.batch_sentences": [[115, 132], ["torch.LongTensor", "torch.LongTensor().fill_", "enumerate", "torch.LongTensor", "sent[].copy_", "len", "torch.LongTensor.max().item", "torch.LongTensor.size", "torch.from_numpy", "s.astype", "torch.LongTensor.max"], "methods", ["None"], ["", "def", "batch_sentences", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "\"\"\"\n        Take as input a list of n sentences (torch.LongTensor vectors) and return\n        a tensor of size (slen, n) where slen is the length of the longest\n        sentence, and a vector lengths containing the length of each sentence.\n        \"\"\"", "\n", "# sentences = sorted(sentences, key=lambda x: len(x), reverse=True)", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "[", "len", "(", "s", ")", "+", "2", "for", "s", "in", "sentences", "]", ")", "\n", "sent", "=", "torch", ".", "LongTensor", "(", "lengths", ".", "max", "(", ")", ".", "item", "(", ")", ",", "lengths", ".", "size", "(", "0", ")", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", "\n", "\n", "sent", "[", "0", "]", "=", "self", ".", "eos_index", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "if", "lengths", "[", "i", "]", ">", "2", ":", "# if sentence not empty", "\n", "                ", "sent", "[", "1", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "s", ".", "astype", "(", "np", ".", "int64", ")", ")", ")", "\n", "", "sent", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "=", "self", ".", "eos_index", "\n", "\n", "", "return", "sent", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.remove_empty_sentences": [[133, 144], ["len", "numpy.arange", "logger.info", "dataset.Dataset.check", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "remove_empty_sentences", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Remove empty sentences.\n        \"\"\"", "\n", "init_size", "=", "len", "(", "self", ".", "pos", ")", "\n", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "pos", ")", ")", "\n", "indices", "=", "indices", "[", "self", ".", "lengths", "[", "indices", "]", ">", "0", "]", "\n", "self", ".", "pos", "=", "self", ".", "pos", "[", "indices", "]", "\n", "self", ".", "lengths", "=", "self", ".", "pos", "[", ":", ",", "1", "]", "-", "self", ".", "pos", "[", ":", ",", "0", "]", "\n", "logger", ".", "info", "(", "\"Removed %i empty sentences.\"", "%", "(", "init_size", "-", "len", "(", "indices", ")", ")", ")", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.remove_long_sentences": [[145, 159], ["len", "numpy.arange", "logger.info", "dataset.Dataset.check", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "remove_long_sentences", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "\"\"\"\n        Remove sentences exceeding a certain length.\n        \"\"\"", "\n", "assert", "max_len", ">=", "0", "\n", "if", "max_len", "==", "0", ":", "\n", "            ", "return", "\n", "", "init_size", "=", "len", "(", "self", ".", "pos", ")", "\n", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "pos", ")", ")", "\n", "indices", "=", "indices", "[", "self", ".", "lengths", "[", "indices", "]", "<=", "max_len", "]", "\n", "self", ".", "pos", "=", "self", ".", "pos", "[", "indices", "]", "\n", "self", ".", "lengths", "=", "self", ".", "pos", "[", ":", ",", "1", "]", "-", "self", ".", "pos", "[", ":", ",", "0", "]", "\n", "logger", ".", "info", "(", "\"Removed %i too long sentences.\"", "%", "(", "init_size", "-", "len", "(", "indices", ")", ")", ")", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.select_data": [[160, 179], ["logger.info", "dataset.Dataset.pos.min", "dataset.Dataset.pos.max", "dataset.Dataset.check", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "select_data", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "\"\"\"\n        Only select a subset of the dataset.\n        \"\"\"", "\n", "assert", "0", "<=", "a", "<", "b", "<=", "len", "(", "self", ".", "pos", ")", "\n", "logger", ".", "info", "(", "\"Selecting sentences from %i to %i ...\"", "%", "(", "a", ",", "b", ")", ")", "\n", "\n", "# sub-select", "\n", "self", ".", "pos", "=", "self", ".", "pos", "[", "a", ":", "b", "]", "\n", "self", ".", "lengths", "=", "self", ".", "pos", "[", ":", ",", "1", "]", "-", "self", ".", "pos", "[", ":", ",", "0", "]", "\n", "\n", "# re-index", "\n", "min_pos", "=", "self", ".", "pos", ".", "min", "(", ")", "\n", "max_pos", "=", "self", ".", "pos", ".", "max", "(", ")", "\n", "self", ".", "pos", "-=", "min_pos", "\n", "self", ".", "sent", "=", "self", ".", "sent", "[", "min_pos", ":", "max_pos", "+", "1", "]", "\n", "\n", "# sanity checks", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.get_batches_iterator": [[180, 194], ["type", "dataset.Dataset.batch_sentences", "len", "numpy.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.batch_sentences"], ["", "def", "get_batches_iterator", "(", "self", ",", "batches", ",", "return_indices", ")", ":", "\n", "        ", "\"\"\"\n        Return a sentences iterator, given the associated sentence batches.\n        \"\"\"", "\n", "assert", "type", "(", "return_indices", ")", "is", "bool", "\n", "\n", "for", "sentence_ids", "in", "batches", ":", "\n", "            ", "if", "0", "<", "self", ".", "max_batch_size", "<", "len", "(", "sentence_ids", ")", ":", "\n", "                ", "np", ".", "random", ".", "shuffle", "(", "sentence_ids", ")", "\n", "sentence_ids", "=", "sentence_ids", "[", ":", "self", ".", "max_batch_size", "]", "\n", "", "pos", "=", "self", ".", "pos", "[", "sentence_ids", "]", "\n", "sent", "=", "[", "self", ".", "sent", "[", "a", ":", "b", "]", "for", "a", ",", "b", "in", "pos", "]", "\n", "sent", "=", "self", ".", "batch_sentences", "(", "sent", ")", "\n", "yield", "(", "sent", ",", "sentence_ids", ")", "if", "return_indices", "else", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.get_iterator": [[195, 248], ["numpy.random.RandomState", "dataset.Dataset.get_batches_iterator", "len", "len", "numpy.arange", "numpy.argsort", "numpy.array_split", "numpy.unique", "numpy.random.RandomState.shuffle", "sum", "lengths[].sum", "sum", "type", "type", "numpy.random.RandomState.permutation", "math.ceil", "numpy.cumsum", "len", "numpy.array_split.append", "type", "len", "range", "len", "lengths[].sum", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.get_batches_iterator"], ["", "", "def", "get_iterator", "(", "\n", "self", ",", "shuffle", ",", "group_by_size", "=", "False", ",", "n_sentences", "=", "-", "1", ",", "seed", "=", "None", ",", "return_indices", "=", "False", ",", "descending", "=", "False", ",", "\n", "infer_train", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Return a sentences iterator.\n        \"\"\"", "\n", "assert", "seed", "is", "None", "or", "shuffle", "is", "True", "and", "type", "(", "seed", ")", "is", "int", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", ")", "\n", "n_sentences", "=", "len", "(", "self", ".", "pos", ")", "if", "n_sentences", "==", "-", "1", "else", "n_sentences", "\n", "assert", "0", "<", "n_sentences", "<=", "len", "(", "self", ".", "pos", ")", "\n", "assert", "type", "(", "shuffle", ")", "is", "bool", "and", "type", "(", "group_by_size", ")", "is", "bool", "\n", "assert", "infer_train", "is", "True", "or", "group_by_size", "is", "False", "or", "shuffle", "is", "True", "\n", "\n", "# sentence lengths", "\n", "lengths", "=", "self", ".", "lengths", "+", "2", "\n", "\n", "# select sentences to iterate over", "\n", "if", "shuffle", ":", "\n", "            ", "indices", "=", "rng", ".", "permutation", "(", "len", "(", "self", ".", "pos", ")", ")", "[", ":", "n_sentences", "]", "\n", "", "else", ":", "\n", "            ", "indices", "=", "np", ".", "arange", "(", "n_sentences", ")", "\n", "\n", "# group sentences by lengths", "\n", "# if group_by_size:", "\n", "#     indices = indices[np.argsort(lengths[indices], kind='mergesort')]", "\n", "", "if", "group_by_size", ":", "\n", "            ", "order", "=", "np", ".", "argsort", "(", "lengths", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "\n", "if", "descending", ":", "\n", "                ", "order", "=", "order", "[", ":", ":", "-", "1", "]", "\n", "", "indices", "=", "indices", "[", "order", "]", "\n", "\n", "# create batches - either have a fixed number of sentences, or a similar number of tokens", "\n", "", "if", "self", ".", "tokens_per_batch", "==", "-", "1", ":", "\n", "            ", "batches", "=", "np", ".", "array_split", "(", "indices", ",", "math", ".", "ceil", "(", "len", "(", "indices", ")", "*", "1.", "/", "self", ".", "batch_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "batch_ids", "=", "np", ".", "cumsum", "(", "lengths", "[", "indices", "]", ")", "//", "self", ".", "tokens_per_batch", "\n", "_", ",", "bounds", "=", "np", ".", "unique", "(", "batch_ids", ",", "return_index", "=", "True", ")", "\n", "batches", "=", "[", "indices", "[", "bounds", "[", "i", "]", ":", "bounds", "[", "i", "+", "1", "]", "]", "for", "i", "in", "range", "(", "len", "(", "bounds", ")", "-", "1", ")", "]", "\n", "if", "bounds", "[", "-", "1", "]", "<", "len", "(", "indices", ")", ":", "\n", "                ", "batches", ".", "append", "(", "indices", "[", "bounds", "[", "-", "1", "]", ":", "]", ")", "\n", "\n", "# optionally shuffle batches", "\n", "", "", "if", "shuffle", ":", "\n", "            ", "rng", ".", "shuffle", "(", "batches", ")", "\n", "\n", "# sanity checks", "\n", "", "assert", "n_sentences", "==", "sum", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batches", "]", ")", "\n", "assert", "lengths", "[", "indices", "]", ".", "sum", "(", ")", "==", "sum", "(", "[", "lengths", "[", "x", "]", ".", "sum", "(", ")", "for", "x", "in", "batches", "]", ")", "\n", "# assert set.union(*[set(x.tolist()) for x in batches]) == set(range(n_sentences))  # slow", "\n", "\n", "# return the iterator", "\n", "return", "self", ".", "get_batches_iterator", "(", "batches", ",", "return_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.__init__": [[252, 276], ["dataset.ParallelDataset.remove_empty_sentences", "dataset.ParallelDataset.check", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_empty_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["    ", "def", "__init__", "(", "self", ",", "sent1", ",", "pos1", ",", "sent2", ",", "pos2", ",", "params", ")", ":", "\n", "\n", "        ", "self", ".", "eos_index", "=", "params", ".", "eos_index", "\n", "self", ".", "pad_index", "=", "params", ".", "pad_index", "\n", "self", ".", "batch_size", "=", "params", ".", "batch_size", "\n", "self", ".", "tokens_per_batch", "=", "params", ".", "tokens_per_batch", "\n", "self", ".", "max_batch_size", "=", "params", ".", "max_batch_size", "\n", "\n", "self", ".", "sent1", "=", "sent1", "\n", "self", ".", "sent2", "=", "sent2", "\n", "self", ".", "pos1", "=", "pos1", "\n", "self", ".", "pos2", "=", "pos2", "\n", "self", ".", "lengths1", "=", "self", ".", "pos1", "[", ":", ",", "1", "]", "-", "self", ".", "pos1", "[", ":", ",", "0", "]", "\n", "self", ".", "lengths2", "=", "self", ".", "pos2", "[", ":", ",", "1", "]", "-", "self", ".", "pos2", "[", ":", ",", "0", "]", "\n", "\n", "# check number of sentences", "\n", "assert", "len", "(", "self", ".", "pos1", ")", "==", "(", "self", ".", "sent1", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "\n", "assert", "len", "(", "self", ".", "pos2", ")", "==", "(", "self", ".", "sent2", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "\n", "\n", "# remove empty sentences", "\n", "self", ".", "remove_empty_sentences", "(", ")", "\n", "\n", "# sanity checks", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.__len__": [[277, 282], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of sentences in the dataset.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "pos1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check": [[283, 295], ["len", "len", "len", "len", "dataset.ParallelDataset.sent1.min", "dataset.ParallelDataset.sent1.max", "dataset.ParallelDataset.sent2.min", "dataset.ParallelDataset.sent2.max", "dataset.ParallelDataset.lengths1.min", "dataset.ParallelDataset.lengths2.min"], "methods", ["None"], ["", "def", "check", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Sanity checks.\n        \"\"\"", "\n", "eos", "=", "self", ".", "eos_index", "\n", "assert", "len", "(", "self", ".", "pos1", ")", "==", "len", "(", "self", ".", "pos2", ")", ">", "0", "# check number of sentences", "\n", "assert", "len", "(", "self", ".", "pos1", ")", "==", "(", "self", ".", "sent1", "[", "self", ".", "pos1", "[", ":", ",", "1", "]", "]", "==", "eos", ")", ".", "sum", "(", ")", "# check sentences indices", "\n", "assert", "len", "(", "self", ".", "pos2", ")", "==", "(", "self", ".", "sent2", "[", "self", ".", "pos2", "[", ":", ",", "1", "]", "]", "==", "eos", ")", ".", "sum", "(", ")", "# check sentences indices", "\n", "assert", "eos", "<=", "self", ".", "sent1", ".", "min", "(", ")", "<", "self", ".", "sent1", ".", "max", "(", ")", "# check dictionary indices", "\n", "assert", "eos", "<=", "self", ".", "sent2", ".", "min", "(", ")", "<", "self", ".", "sent2", ".", "max", "(", ")", "# check dictionary indices", "\n", "assert", "self", ".", "lengths1", ".", "min", "(", ")", ">", "0", "# check empty sentences", "\n", "assert", "self", ".", "lengths2", ".", "min", "(", ")", ">", "0", "# check empty sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_empty_sentences": [[296, 310], ["len", "numpy.arange", "logger.info", "dataset.ParallelDataset.check", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "remove_empty_sentences", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Remove empty sentences.\n        \"\"\"", "\n", "init_size", "=", "len", "(", "self", ".", "pos1", ")", "\n", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "pos1", ")", ")", "\n", "indices", "=", "indices", "[", "self", ".", "lengths1", "[", "indices", "]", ">", "0", "]", "\n", "indices", "=", "indices", "[", "self", ".", "lengths2", "[", "indices", "]", ">", "0", "]", "\n", "self", ".", "pos1", "=", "self", ".", "pos1", "[", "indices", "]", "\n", "self", ".", "pos2", "=", "self", ".", "pos2", "[", "indices", "]", "\n", "self", ".", "lengths1", "=", "self", ".", "pos1", "[", ":", ",", "1", "]", "-", "self", ".", "pos1", "[", ":", ",", "0", "]", "\n", "self", ".", "lengths2", "=", "self", ".", "pos2", "[", ":", ",", "1", "]", "-", "self", ".", "pos2", "[", ":", ",", "0", "]", "\n", "logger", ".", "info", "(", "\"Removed %i empty sentences.\"", "%", "(", "init_size", "-", "len", "(", "indices", ")", ")", ")", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_long_sentences": [[311, 328], ["len", "numpy.arange", "logger.info", "dataset.ParallelDataset.check", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "remove_long_sentences", "(", "self", ",", "max_len", ")", ":", "\n", "        ", "\"\"\"\n        Remove sentences exceeding a certain length.\n        \"\"\"", "\n", "assert", "max_len", ">=", "0", "\n", "if", "max_len", "==", "0", ":", "\n", "            ", "return", "\n", "", "init_size", "=", "len", "(", "self", ".", "pos1", ")", "\n", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "pos1", ")", ")", "\n", "indices", "=", "indices", "[", "self", ".", "lengths1", "[", "indices", "]", "<=", "max_len", "]", "\n", "indices", "=", "indices", "[", "self", ".", "lengths2", "[", "indices", "]", "<=", "max_len", "]", "\n", "self", ".", "pos1", "=", "self", ".", "pos1", "[", "indices", "]", "\n", "self", ".", "pos2", "=", "self", ".", "pos2", "[", "indices", "]", "\n", "self", ".", "lengths1", "=", "self", ".", "pos1", "[", ":", ",", "1", "]", "-", "self", ".", "pos1", "[", ":", ",", "0", "]", "\n", "self", ".", "lengths2", "=", "self", ".", "pos2", "[", ":", ",", "1", "]", "-", "self", ".", "pos2", "[", ":", ",", "0", "]", "\n", "logger", ".", "info", "(", "\"Removed %i too long sentences.\"", "%", "(", "init_size", "-", "len", "(", "indices", ")", ")", ")", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.select_data": [[329, 354], ["logger.info", "dataset.ParallelDataset.pos1.min", "dataset.ParallelDataset.pos1.max", "dataset.ParallelDataset.pos2.min", "dataset.ParallelDataset.pos2.max", "dataset.ParallelDataset.check", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.check"], ["", "def", "select_data", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "\"\"\"\n        Only select a subset of the dataset.\n        \"\"\"", "\n", "assert", "0", "<=", "a", "<", "b", "<=", "len", "(", "self", ".", "pos1", ")", "\n", "logger", ".", "info", "(", "\"Selecting sentences from %i to %i ...\"", "%", "(", "a", ",", "b", ")", ")", "\n", "\n", "# sub-select", "\n", "self", ".", "pos1", "=", "self", ".", "pos1", "[", "a", ":", "b", "]", "\n", "self", ".", "pos2", "=", "self", ".", "pos2", "[", "a", ":", "b", "]", "\n", "self", ".", "lengths1", "=", "self", ".", "pos1", "[", ":", ",", "1", "]", "-", "self", ".", "pos1", "[", ":", ",", "0", "]", "\n", "self", ".", "lengths2", "=", "self", ".", "pos2", "[", ":", ",", "1", "]", "-", "self", ".", "pos2", "[", ":", ",", "0", "]", "\n", "\n", "# re-index", "\n", "min_pos1", "=", "self", ".", "pos1", ".", "min", "(", ")", "\n", "max_pos1", "=", "self", ".", "pos1", ".", "max", "(", ")", "\n", "min_pos2", "=", "self", ".", "pos2", ".", "min", "(", ")", "\n", "max_pos2", "=", "self", ".", "pos2", ".", "max", "(", ")", "\n", "self", ".", "pos1", "-=", "min_pos1", "\n", "self", ".", "pos2", "-=", "min_pos2", "\n", "self", ".", "sent1", "=", "self", ".", "sent1", "[", "min_pos1", ":", "max_pos1", "+", "1", "]", "\n", "self", ".", "sent2", "=", "self", ".", "sent2", "[", "min_pos2", ":", "max_pos2", "+", "1", "]", "\n", "\n", "# sanity checks", "\n", "self", ".", "check", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.get_batches_iterator": [[355, 370], ["type", "dataset.ParallelDataset.batch_sentences", "dataset.ParallelDataset.batch_sentences", "len", "numpy.random.shuffle"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.batch_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.Dataset.batch_sentences"], ["", "def", "get_batches_iterator", "(", "self", ",", "batches", ",", "return_indices", ")", ":", "\n", "        ", "\"\"\"\n        Return a sentences iterator, given the associated sentence batches.\n        \"\"\"", "\n", "assert", "type", "(", "return_indices", ")", "is", "bool", "\n", "\n", "for", "sentence_ids", "in", "batches", ":", "\n", "            ", "if", "0", "<", "self", ".", "max_batch_size", "<", "len", "(", "sentence_ids", ")", ":", "\n", "                ", "np", ".", "random", ".", "shuffle", "(", "sentence_ids", ")", "\n", "sentence_ids", "=", "sentence_ids", "[", ":", "self", ".", "max_batch_size", "]", "\n", "", "pos1", "=", "self", ".", "pos1", "[", "sentence_ids", "]", "\n", "pos2", "=", "self", ".", "pos2", "[", "sentence_ids", "]", "\n", "sent1", "=", "self", ".", "batch_sentences", "(", "[", "self", ".", "sent1", "[", "a", ":", "b", "]", "for", "a", ",", "b", "in", "pos1", "]", ")", "\n", "sent2", "=", "self", ".", "batch_sentences", "(", "[", "self", ".", "sent2", "[", "a", ":", "b", "]", "for", "a", ",", "b", "in", "pos2", "]", ")", "\n", "yield", "(", "sent1", ",", "sent2", ",", "sentence_ids", ")", "if", "return_indices", "else", "(", "sent1", ",", "sent2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.get_iterator": [[371, 416], ["dataset.ParallelDataset.get_batches_iterator", "len", "len", "numpy.arange", "numpy.argsort", "numpy.array_split", "numpy.unique", "numpy.random.shuffle", "sum", "lengths[].sum", "sum", "type", "type", "numpy.random.permutation", "math.ceil", "numpy.cumsum", "len", "numpy.array_split.append", "len", "range", "len", "lengths[].sum", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.get_batches_iterator"], ["", "", "def", "get_iterator", "(", "self", ",", "shuffle", ",", "group_by_size", "=", "False", ",", "n_sentences", "=", "-", "1", ",", "return_indices", "=", "False", ",", "descending", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Return a sentences iterator.\n        \"\"\"", "\n", "n_sentences", "=", "len", "(", "self", ".", "pos1", ")", "if", "n_sentences", "==", "-", "1", "else", "n_sentences", "\n", "assert", "0", "<", "n_sentences", "<=", "len", "(", "self", ".", "pos1", ")", "\n", "assert", "type", "(", "shuffle", ")", "is", "bool", "and", "type", "(", "group_by_size", ")", "is", "bool", "\n", "\n", "# sentence lengths", "\n", "lengths", "=", "self", ".", "lengths1", "+", "self", ".", "lengths2", "+", "4", "\n", "\n", "# select sentences to iterate over", "\n", "if", "shuffle", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ".", "pos1", ")", ")", "[", ":", "n_sentences", "]", "\n", "", "else", ":", "\n", "            ", "indices", "=", "np", ".", "arange", "(", "n_sentences", ")", "\n", "\n", "# group sentences by lengths", "\n", "", "if", "group_by_size", ":", "\n", "            ", "order", "=", "np", ".", "argsort", "(", "lengths", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "\n", "if", "descending", ":", "\n", "                ", "order", "=", "order", "[", ":", ":", "-", "1", "]", "\n", "", "indices", "=", "indices", "[", "order", "]", "\n", "\n", "# create batches - either have a fixed number of sentences, or a similar number of tokens", "\n", "", "if", "self", ".", "tokens_per_batch", "==", "-", "1", ":", "\n", "            ", "batches", "=", "np", ".", "array_split", "(", "indices", ",", "math", ".", "ceil", "(", "len", "(", "indices", ")", "*", "1.", "/", "self", ".", "batch_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "batch_ids", "=", "np", ".", "cumsum", "(", "lengths", "[", "indices", "]", ")", "//", "self", ".", "tokens_per_batch", "\n", "_", ",", "bounds", "=", "np", ".", "unique", "(", "batch_ids", ",", "return_index", "=", "True", ")", "\n", "batches", "=", "[", "indices", "[", "bounds", "[", "i", "]", ":", "bounds", "[", "i", "+", "1", "]", "]", "for", "i", "in", "range", "(", "len", "(", "bounds", ")", "-", "1", ")", "]", "\n", "if", "bounds", "[", "-", "1", "]", "<", "len", "(", "indices", ")", ":", "\n", "                ", "batches", ".", "append", "(", "indices", "[", "bounds", "[", "-", "1", "]", ":", "]", ")", "\n", "\n", "# optionally shuffle batches", "\n", "", "", "if", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "batches", ")", "\n", "\n", "# sanity checks", "\n", "", "assert", "n_sentences", "==", "sum", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batches", "]", ")", "\n", "assert", "lengths", "[", "indices", "]", ".", "sum", "(", ")", "==", "sum", "(", "[", "lengths", "[", "x", "]", ".", "sum", "(", ")", "for", "x", "in", "batches", "]", ")", "\n", "# assert set.union(*[set(x.tolist()) for x in batches]) == set(range(n_sentences))  # slow", "\n", "\n", "# return the iterator", "\n", "return", "self", ".", "get_batches_iterator", "(", "batches", ",", "return_indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.process_binarized": [[20, 52], ["logger.info", "logger.info", "dico.max_vocab", "dico.index", "logger.info", "logger.info", "dico.min_count", "dico.index", "logger.info", "logger.info", "data[].astype", "len", "len", "len", "len", "len", "sum", "len", "len", "len", "data[].values", "dico.index", "len", "dico.index", "sum", "len", "len", "data[].values", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.max_vocab", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.min_count", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["def", "process_binarized", "(", "data", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Process a binarized dataset and log main statistics.\n    \"\"\"", "\n", "dico", "=", "data", "[", "'dico'", "]", "\n", "assert", "(", "(", "data", "[", "'sentences'", "]", ".", "dtype", "==", "np", ".", "uint16", ")", "and", "(", "len", "(", "dico", ")", "<", "1", "<<", "16", ")", "or", "\n", "(", "data", "[", "'sentences'", "]", ".", "dtype", "==", "np", ".", "int32", ")", "and", "(", "1", "<<", "16", "<=", "len", "(", "dico", ")", "<", "1", "<<", "31", ")", ")", "\n", "logger", ".", "info", "(", "\"%i words (%i unique) in %i sentences. %i unknown words (%i unique) covering %.2f%% of the data.\"", "%", "(", "\n", "len", "(", "data", "[", "'sentences'", "]", ")", "-", "len", "(", "data", "[", "'positions'", "]", ")", ",", "\n", "len", "(", "dico", ")", ",", "len", "(", "data", "[", "'positions'", "]", ")", ",", "\n", "sum", "(", "data", "[", "'unk_words'", "]", ".", "values", "(", ")", ")", ",", "len", "(", "data", "[", "'unk_words'", "]", ")", ",", "\n", "100.", "*", "sum", "(", "data", "[", "'unk_words'", "]", ".", "values", "(", ")", ")", "/", "(", "len", "(", "data", "[", "'sentences'", "]", ")", "-", "len", "(", "data", "[", "'positions'", "]", ")", ")", "\n", ")", ")", "\n", "if", "params", ".", "max_vocab", "!=", "-", "1", ":", "\n", "        ", "assert", "params", ".", "max_vocab", ">", "0", "\n", "logger", ".", "info", "(", "\"Selecting %i most frequent words ...\"", "%", "params", ".", "max_vocab", ")", "\n", "dico", ".", "max_vocab", "(", "params", ".", "max_vocab", ")", "\n", "data", "[", "'sentences'", "]", "[", "data", "[", "'sentences'", "]", ">=", "params", ".", "max_vocab", "]", "=", "dico", ".", "index", "(", "UNK_WORD", ")", "\n", "unk_count", "=", "(", "data", "[", "'sentences'", "]", "==", "dico", ".", "index", "(", "UNK_WORD", ")", ")", ".", "sum", "(", ")", "\n", "logger", ".", "info", "(", "\"Now %i unknown words covering %.2f%% of the data.\"", "\n", "%", "(", "unk_count", ",", "100.", "*", "unk_count", "/", "(", "len", "(", "data", "[", "'sentences'", "]", ")", "-", "len", "(", "data", "[", "'positions'", "]", ")", ")", ")", ")", "\n", "", "if", "params", ".", "min_count", ">", "0", ":", "\n", "        ", "logger", ".", "info", "(", "\"Selecting words with >= %i occurrences ...\"", "%", "params", ".", "min_count", ")", "\n", "dico", ".", "min_count", "(", "params", ".", "min_count", ")", "\n", "data", "[", "'sentences'", "]", "[", "data", "[", "'sentences'", "]", ">=", "len", "(", "dico", ")", "]", "=", "dico", ".", "index", "(", "UNK_WORD", ")", "\n", "unk_count", "=", "(", "data", "[", "'sentences'", "]", "==", "dico", ".", "index", "(", "UNK_WORD", ")", ")", ".", "sum", "(", ")", "\n", "logger", ".", "info", "(", "\"Now %i unknown words covering %.2f%% of the data.\"", "\n", "%", "(", "unk_count", ",", "100.", "*", "unk_count", "/", "(", "len", "(", "data", "[", "'sentences'", "]", ")", "-", "len", "(", "data", "[", "'positions'", "]", ")", ")", ")", ")", "\n", "", "if", "(", "data", "[", "'sentences'", "]", ".", "dtype", "==", "np", ".", "int32", ")", "and", "(", "len", "(", "dico", ")", "<", "1", "<<", "16", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Less than 65536 words. Moving data from int32 to uint16 ...\"", ")", "\n", "data", "[", "'sentences'", "]", "=", "data", "[", "'sentences'", "]", ".", "astype", "(", "np", ".", "uint16", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized": [[54, 71], ["path.replace.endswith", "getattr", "os.path.isfile", "logger.info", "torch.load", "loader.process_binarized", "path.replace.replace", "os.path.isfile"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.process_binarized"], ["", "def", "load_binarized", "(", "path", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    Load a binarized dataset.\n    \"\"\"", "\n", "assert", "path", ".", "endswith", "(", "'.pth'", ")", "\n", "if", "params", ".", "debug_train", ":", "\n", "        ", "path", "=", "path", ".", "replace", "(", "'train'", ",", "'valid'", ")", "\n", "", "if", "getattr", "(", "params", ",", "'multi_gpu'", ",", "False", ")", ":", "\n", "        ", "split_path", "=", "'%s.%i.pth'", "%", "(", "path", "[", ":", "-", "4", "]", ",", "params", ".", "local_rank", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "split_path", ")", ":", "\n", "            ", "assert", "params", ".", "split_data", "is", "False", "\n", "path", "=", "split_path", "\n", "", "", "assert", "os", ".", "path", ".", "isfile", "(", "path", ")", ",", "path", "\n", "logger", ".", "info", "(", "\"Loading data from %s ...\"", "%", "path", ")", "\n", "data", "=", "torch", ".", "load", "(", "path", ")", "\n", "data", "=", "process_binarized", "(", "data", ",", "params", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters": [[73, 102], ["len", "dico.index", "dico.index", "dico.index", "dico.index", "dico.index", "hasattr"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["", "def", "set_dico_parameters", "(", "params", ",", "data", ",", "dico", ")", ":", "\n", "    ", "\"\"\"\n    Update dictionary parameters.\n    \"\"\"", "\n", "if", "'dico'", "in", "data", ":", "\n", "        ", "assert", "data", "[", "'dico'", "]", "==", "dico", "\n", "", "else", ":", "\n", "        ", "data", "[", "'dico'", "]", "=", "dico", "\n", "\n", "", "n_words", "=", "len", "(", "dico", ")", "\n", "bos_index", "=", "dico", ".", "index", "(", "BOS_WORD", ")", "\n", "eos_index", "=", "dico", ".", "index", "(", "EOS_WORD", ")", "\n", "pad_index", "=", "dico", ".", "index", "(", "PAD_WORD", ")", "\n", "unk_index", "=", "dico", ".", "index", "(", "UNK_WORD", ")", "\n", "mask_index", "=", "dico", ".", "index", "(", "MASK_WORD", ")", "\n", "if", "hasattr", "(", "params", ",", "'bos_index'", ")", ":", "\n", "        ", "assert", "params", ".", "n_words", "==", "n_words", "\n", "assert", "params", ".", "bos_index", "==", "bos_index", "\n", "assert", "params", ".", "eos_index", "==", "eos_index", "\n", "assert", "params", ".", "pad_index", "==", "pad_index", "\n", "assert", "params", ".", "unk_index", "==", "unk_index", "\n", "assert", "params", ".", "mask_index", "==", "mask_index", "\n", "", "else", ":", "\n", "        ", "params", ".", "n_words", "=", "n_words", "\n", "params", ".", "bos_index", "=", "bos_index", "\n", "params", ".", "eos_index", "=", "eos_index", "\n", "params", ".", "pad_index", "=", "pad_index", "\n", "params", ".", "unk_index", "=", "unk_index", "\n", "params", ".", "mask_index", "=", "mask_index", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_mono_data": [[104, 163], ["params.mono_dataset.keys", "logger.info", "logger.info", "loader.load_binarized", "loader.set_dico_parameters", "dataset.StreamDataset", "logger.info", "[].select_data", "dataset.Dataset", "dataset.Dataset.remove_empty_sentences", "dataset.Dataset.remove_long_sentences", "dataset.Dataset.select_data", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.select_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_empty_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_long_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.select_data"], ["", "", "def", "load_mono_data", "(", "params", ",", "data", ")", ":", "\n", "    ", "\"\"\"\n    Load monolingual data.\n    \"\"\"", "\n", "data", "[", "'mono'", "]", "=", "{", "}", "\n", "data", "[", "'mono_stream'", "]", "=", "{", "}", "\n", "\n", "for", "lang", "in", "params", ".", "mono_dataset", ".", "keys", "(", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "'============ Monolingual data (%s)'", "%", "lang", ")", "\n", "\n", "assert", "lang", "in", "params", ".", "langs", "and", "lang", "not", "in", "data", "[", "'mono'", "]", "\n", "data", "[", "'mono'", "]", "[", "lang", "]", "=", "{", "}", "\n", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "=", "{", "}", "\n", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "# no need to load training data for evaluation", "\n", "            ", "if", "splt", "==", "'train'", "and", "params", ".", "eval_only", ":", "\n", "                ", "continue", "\n", "\n", "# load data / update dictionary parameters / update data", "\n", "", "mono_data", "=", "load_binarized", "(", "params", ".", "mono_dataset", "[", "lang", "]", "[", "splt", "]", ",", "params", ")", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "mono_data", "[", "'dico'", "]", ")", "\n", "\n", "# create stream dataset", "\n", "bs", "=", "params", ".", "batch_size", "if", "splt", "==", "'train'", "else", "1", "\n", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "[", "splt", "]", "=", "StreamDataset", "(", "mono_data", "[", "'sentences'", "]", ",", "mono_data", "[", "'positions'", "]", ",", "bs", ",", "params", ")", "\n", "\n", "# if there are several processes on the same machine, we can split the dataset", "\n", "if", "splt", "==", "'train'", "and", "params", ".", "split_data", "and", "1", "<", "params", ".", "n_gpu_per_node", "<=", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "[", "splt", "]", ".", "n_batches", ":", "\n", "                ", "n_batches", "=", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "[", "splt", "]", ".", "n_batches", "//", "params", ".", "n_gpu_per_node", "\n", "a", "=", "n_batches", "*", "params", ".", "local_rank", "\n", "b", "=", "n_batches", "*", "params", ".", "local_rank", "+", "n_batches", "\n", "data", "[", "'mono_stream'", "]", "[", "lang", "]", "[", "splt", "]", ".", "select_data", "(", "a", ",", "b", ")", "\n", "\n", "# for denoising auto-encoding and online back-translation, we need a non-stream (batched) dataset", "\n", "", "if", "lang", "in", "params", ".", "ae_steps", "or", "lang", "in", "params", ".", "bt_src_langs", ":", "\n", "\n", "# create batched dataset", "\n", "                ", "dataset", "=", "Dataset", "(", "mono_data", "[", "'sentences'", "]", ",", "mono_data", "[", "'positions'", "]", ",", "params", ")", "\n", "\n", "# remove empty and too long sentences", "\n", "if", "splt", "==", "'train'", ":", "\n", "                    ", "dataset", ".", "remove_empty_sentences", "(", ")", "\n", "dataset", ".", "remove_long_sentences", "(", "params", ".", "max_len", ")", "\n", "\n", "# if there are several processes on the same machine, we can split the dataset", "\n", "", "if", "splt", "==", "'train'", "and", "params", ".", "n_gpu_per_node", ">", "1", "and", "params", ".", "split_data", ":", "\n", "                    ", "n_sent", "=", "len", "(", "dataset", ")", "//", "params", ".", "n_gpu_per_node", "\n", "a", "=", "n_sent", "*", "params", ".", "local_rank", "\n", "b", "=", "n_sent", "*", "params", ".", "local_rank", "+", "n_sent", "\n", "dataset", ".", "select_data", "(", "a", ",", "b", ")", "\n", "\n", "", "data", "[", "'mono'", "]", "[", "lang", "]", "[", "splt", "]", "=", "dataset", "\n", "\n", "", "logger", ".", "info", "(", "\"\"", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_para_data": [[165, 229], ["set", "params.para_dataset.keys", "logger.info", "logger.info", "loader.load_binarized", "loader.load_binarized", "loader.set_dico_parameters", "loader.set_dico_parameters", "dataset.ParallelDataset", "logger.info", "dataset.ParallelDataset.remove_empty_sentences", "dataset.ParallelDataset.remove_long_sentences", "dataset.ParallelDataset.select_data", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_empty_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.remove_long_sentences", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dataset.ParallelDataset.select_data"], ["", "def", "load_para_data", "(", "params", ",", "data", ")", ":", "\n", "    ", "\"\"\"\n    Load parallel data.\n    \"\"\"", "\n", "data", "[", "'para'", "]", "=", "{", "}", "\n", "\n", "required_para_train", "=", "set", "(", "params", ".", "clm_steps", "+", "params", ".", "mlm_steps", "+", "params", ".", "pc_steps", "+", "params", ".", "mt_steps", ")", "\n", "\n", "for", "src", ",", "tgt", "in", "params", ".", "para_dataset", ".", "keys", "(", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "'============ Parallel data (%s-%s)'", "%", "(", "src", ",", "tgt", ")", ")", "\n", "\n", "assert", "(", "src", ",", "tgt", ")", "not", "in", "data", "[", "'para'", "]", "\n", "data", "[", "'para'", "]", "[", "(", "src", ",", "tgt", ")", "]", "=", "{", "}", "\n", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "# no need to load training data for evaluation", "\n", "            ", "if", "splt", "==", "'train'", "and", "params", ".", "eval_only", ":", "\n", "                ", "continue", "\n", "\n", "# for back-translation, we can't load training data", "\n", "", "if", "splt", "==", "'train'", "and", "(", "src", ",", "tgt", ")", "not", "in", "required_para_train", "and", "(", "tgt", ",", "src", ")", "not", "in", "required_para_train", ":", "\n", "                ", "continue", "\n", "\n", "# load binarized datasets", "\n", "", "src_path", ",", "tgt_path", "=", "params", ".", "para_dataset", "[", "(", "src", ",", "tgt", ")", "]", "[", "splt", "]", "\n", "src_data", "=", "load_binarized", "(", "src_path", ",", "params", ")", "\n", "tgt_data", "=", "load_binarized", "(", "tgt_path", ",", "params", ")", "\n", "\n", "# update dictionary parameters", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "src_data", "[", "'dico'", "]", ")", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "tgt_data", "[", "'dico'", "]", ")", "\n", "\n", "# create ParallelDataset", "\n", "dataset", "=", "ParallelDataset", "(", "\n", "src_data", "[", "'sentences'", "]", ",", "src_data", "[", "'positions'", "]", ",", "\n", "tgt_data", "[", "'sentences'", "]", ",", "tgt_data", "[", "'positions'", "]", ",", "\n", "params", "\n", ")", "\n", "\n", "# remove empty and too long sentences", "\n", "if", "splt", "==", "'train'", ":", "\n", "                ", "dataset", ".", "remove_empty_sentences", "(", ")", "\n", "dataset", ".", "remove_long_sentences", "(", "params", ".", "max_len", ")", "\n", "\n", "# for validation and test set, enumerate sentence per sentence", "\n", "", "if", "splt", "!=", "'train'", ":", "\n", "                ", "dataset", ".", "tokens_per_batch", "=", "-", "1", "\n", "\n", "# if there are several processes on the same machine, we can split the dataset", "\n", "", "if", "splt", "==", "'train'", "and", "params", ".", "n_gpu_per_node", ">", "1", "and", "params", ".", "split_data", ":", "\n", "                ", "n_sent", "=", "len", "(", "dataset", ")", "//", "params", ".", "n_gpu_per_node", "\n", "a", "=", "n_sent", "*", "params", ".", "local_rank", "\n", "b", "=", "n_sent", "*", "params", ".", "local_rank", "+", "n_sent", "\n", "dataset", ".", "select_data", "(", "a", ",", "b", ")", "\n", "logger", ".", "info", "(", "'Split data({}-{}) multi-gpu (rank={}): size: {}'", ".", "format", "(", "\n", "src", ",", "tgt", ",", "params", ".", "local_rank", ",", "len", "(", "dataset", ")", "\n", ")", ")", "\n", "\n", "", "data", "[", "'para'", "]", "[", "(", "src", ",", "tgt", ")", "]", "[", "splt", "]", "=", "dataset", "\n", "logger", ".", "info", "(", "\"\"", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.check_data_params": [[231, 324], ["os.path.isdir", "len", "all", "all", "all", "all", "all", "all", "all", "all", "all", "all", "all", "all", "set", "params.mono_dataset.values", "all", "set", "params.para_dataset.values", "all", "params.lgs.split", "len", "len", "s.split", "len", "len", "s.split", "len", "len", "tuple", "len", "len", "tuple", "len", "len", "len", "len", "tuple", "len", "len", "paths.values", "set", "paths.values", "set", "enumerate", "params.id2lang.items", "params.clm_steps.strip().split", "tuple", "set", "params.mlm_steps.strip().split", "tuple", "set", "s.split", "params.pc_steps.strip().split", "set", "s.split", "params.mt_steps.strip().split", "set", "len", "params.ae_steps.strip().split", "set", "len", "s.split", "params.bt_steps.strip().split", "set", "len", "os.path.join", "all", "all", "len", "sorted", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "os.path.isfile", "logger.error", "params.mono_dataset.values", "os.path.join", "os.path.join", "os.path.isfile", "logger.error", "os.path.isfile", "logger.error", "params.para_dataset.values", "params.clm_steps.strip", "params.mlm_steps.strip", "params.pc_steps.strip", "params.mt_steps.strip", "params.ae_steps.strip", "params.bt_steps.strip", "os.path.isfile", "paths.values", "os.path.isfile", "os.path.isfile", "paths.values"], "function", ["None"], ["", "def", "check_data_params", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Check datasets parameters.\n    \"\"\"", "\n", "# data path", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "params", ".", "data_path", ")", ",", "params", ".", "data_path", "\n", "\n", "# check languages", "\n", "params", ".", "langs", "=", "params", ".", "lgs", ".", "split", "(", "'-'", ")", "if", "params", ".", "lgs", "!=", "'debug'", "else", "[", "'en'", "]", "\n", "assert", "len", "(", "params", ".", "langs", ")", "==", "len", "(", "set", "(", "params", ".", "langs", ")", ")", ">=", "1", "\n", "# assert sorted(params.langs) == params.langs", "\n", "params", ".", "id2lang", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "enumerate", "(", "sorted", "(", "params", ".", "langs", ")", ")", "}", "\n", "params", ".", "lang2id", "=", "{", "k", ":", "v", "for", "v", ",", "k", "in", "params", ".", "id2lang", ".", "items", "(", ")", "}", "\n", "params", ".", "n_langs", "=", "len", "(", "params", ".", "langs", ")", "\n", "\n", "# CLM steps", "\n", "clm_steps", "=", "[", "s", ".", "split", "(", "'-'", ")", "for", "s", "in", "params", ".", "clm_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "params", ".", "clm_steps", "=", "[", "(", "s", "[", "0", "]", ",", "None", ")", "if", "len", "(", "s", ")", "==", "1", "else", "tuple", "(", "s", ")", "for", "s", "in", "clm_steps", "]", "\n", "assert", "all", "(", "[", "(", "l1", "in", "params", ".", "langs", ")", "and", "(", "l2", "in", "params", ".", "langs", "or", "l2", "is", "None", ")", "for", "l1", ",", "l2", "in", "params", ".", "clm_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "clm_steps", ")", "==", "len", "(", "set", "(", "params", ".", "clm_steps", ")", ")", "\n", "\n", "# MLM / TLM steps", "\n", "mlm_steps", "=", "[", "s", ".", "split", "(", "'-'", ")", "for", "s", "in", "params", ".", "mlm_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "params", ".", "mlm_steps", "=", "[", "(", "s", "[", "0", "]", ",", "None", ")", "if", "len", "(", "s", ")", "==", "1", "else", "tuple", "(", "s", ")", "for", "s", "in", "mlm_steps", "]", "\n", "assert", "all", "(", "[", "(", "l1", "in", "params", ".", "langs", ")", "and", "(", "l2", "in", "params", ".", "langs", "or", "l2", "is", "None", ")", "for", "l1", ",", "l2", "in", "params", ".", "mlm_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "mlm_steps", ")", "==", "len", "(", "set", "(", "params", ".", "mlm_steps", ")", ")", "\n", "\n", "# parallel classification steps", "\n", "params", ".", "pc_steps", "=", "[", "tuple", "(", "s", ".", "split", "(", "'-'", ")", ")", "for", "s", "in", "params", ".", "pc_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "2", "for", "x", "in", "params", ".", "pc_steps", "]", ")", "\n", "assert", "all", "(", "[", "l1", "in", "params", ".", "langs", "and", "l2", "in", "params", ".", "langs", "for", "l1", ",", "l2", "in", "params", ".", "pc_steps", "]", ")", "\n", "assert", "all", "(", "[", "l1", "!=", "l2", "for", "l1", ",", "l2", "in", "params", ".", "pc_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "pc_steps", ")", "==", "len", "(", "set", "(", "params", ".", "pc_steps", ")", ")", "\n", "\n", "# machine translation steps", "\n", "params", ".", "mt_steps", "=", "[", "tuple", "(", "s", ".", "split", "(", "'-'", ")", ")", "for", "s", "in", "params", ".", "mt_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "2", "for", "x", "in", "params", ".", "mt_steps", "]", ")", "\n", "assert", "all", "(", "[", "l1", "in", "params", ".", "langs", "and", "l2", "in", "params", ".", "langs", "for", "l1", ",", "l2", "in", "params", ".", "mt_steps", "]", ")", ",", "'{}'", ".", "format", "(", "params", ".", "mt_steps", ")", "\n", "assert", "all", "(", "[", "l1", "!=", "l2", "for", "l1", ",", "l2", "in", "params", ".", "mt_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "mt_steps", ")", "==", "len", "(", "set", "(", "params", ".", "mt_steps", ")", ")", "\n", "assert", "len", "(", "params", ".", "mt_steps", ")", "==", "0", "or", "not", "params", ".", "encoder_only", "\n", "\n", "# denoising auto-encoder steps", "\n", "params", ".", "ae_steps", "=", "[", "s", "for", "s", "in", "params", ".", "ae_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "assert", "all", "(", "[", "lang", "in", "params", ".", "langs", "for", "lang", "in", "params", ".", "ae_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "ae_steps", ")", "==", "len", "(", "set", "(", "params", ".", "ae_steps", ")", ")", "\n", "assert", "len", "(", "params", ".", "ae_steps", ")", "==", "0", "or", "not", "params", ".", "encoder_only", "\n", "\n", "# back-translation steps", "\n", "params", ".", "bt_steps", "=", "[", "tuple", "(", "s", ".", "split", "(", "'-'", ")", ")", "for", "s", "in", "params", ".", "bt_steps", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "3", "for", "x", "in", "params", ".", "bt_steps", "]", ")", "\n", "assert", "all", "(", "[", "l1", "in", "params", ".", "langs", "and", "l2", "in", "params", ".", "langs", "and", "l3", "in", "params", ".", "langs", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", "\n", "assert", "all", "(", "[", "l1", "==", "l3", "and", "l1", "!=", "l2", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", "\n", "assert", "len", "(", "params", ".", "bt_steps", ")", "==", "len", "(", "set", "(", "params", ".", "bt_steps", ")", ")", "\n", "assert", "len", "(", "params", ".", "bt_steps", ")", "==", "0", "or", "not", "params", ".", "encoder_only", "\n", "params", ".", "bt_src_langs", "=", "[", "l1", "for", "l1", ",", "_", ",", "_", "in", "params", ".", "bt_steps", "]", "\n", "\n", "# check monolingual datasets", "\n", "required_mono", "=", "set", "(", "[", "l1", "for", "l1", ",", "l2", "in", "(", "params", ".", "mlm_steps", "+", "params", ".", "clm_steps", ")", "if", "l2", "is", "None", "]", "+", "params", ".", "ae_steps", "+", "params", ".", "bt_src_langs", ")", "\n", "params", ".", "mono_dataset", "=", "{", "\n", "lang", ":", "{", "\n", "splt", ":", "os", ".", "path", ".", "join", "(", "params", ".", "data_path", ",", "'%s.%s.pth'", "%", "(", "splt", ",", "lang", ")", ")", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "\n", "}", "for", "lang", "in", "params", ".", "langs", "if", "lang", "in", "required_mono", "\n", "}", "\n", "for", "paths", "in", "params", ".", "mono_dataset", ".", "values", "(", ")", ":", "\n", "        ", "for", "p", "in", "paths", ".", "values", "(", ")", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "isfile", "(", "p", ")", ":", "\n", "                ", "logger", ".", "error", "(", "\"{} not found\"", ".", "format", "(", "p", ")", ")", "\n", "", "", "", "assert", "all", "(", "[", "all", "(", "[", "os", ".", "path", ".", "isfile", "(", "p", ")", "for", "p", "in", "paths", ".", "values", "(", ")", "]", ")", "for", "paths", "in", "params", ".", "mono_dataset", ".", "values", "(", ")", "]", ")", "\n", "\n", "# check parallel datasets", "\n", "required_para_train", "=", "set", "(", "params", ".", "clm_steps", "+", "params", ".", "mlm_steps", "+", "params", ".", "pc_steps", "+", "params", ".", "mt_steps", ")", "\n", "required_para", "=", "required_para_train", "|", "set", "(", "[", "(", "l2", ",", "l3", ")", "for", "_", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", "\n", "params", ".", "para_dataset", "=", "{", "\n", "(", "src", ",", "tgt", ")", ":", "{", "\n", "splt", ":", "(", "os", ".", "path", ".", "join", "(", "params", ".", "data_path", ",", "'%s.%s-%s.%s.pth'", "%", "(", "splt", ",", "src", ",", "tgt", ",", "src", ")", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "params", ".", "data_path", ",", "'%s.%s-%s.%s.pth'", "%", "(", "splt", ",", "src", ",", "tgt", ",", "tgt", ")", ")", ")", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "\n", "if", "splt", "!=", "'train'", "or", "(", "src", ",", "tgt", ")", "in", "required_para_train", "or", "(", "tgt", ",", "src", ")", "in", "required_para_train", "\n", "}", "for", "src", "in", "params", ".", "langs", "for", "tgt", "in", "params", ".", "langs", "\n", "if", "src", "<", "tgt", "and", "(", "(", "src", ",", "tgt", ")", "in", "required_para", "or", "(", "tgt", ",", "src", ")", "in", "required_para", ")", "\n", "}", "\n", "for", "paths", "in", "params", ".", "para_dataset", ".", "values", "(", ")", ":", "\n", "        ", "for", "p1", ",", "p2", "in", "paths", ".", "values", "(", ")", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "isfile", "(", "p1", ")", ":", "\n", "                ", "logger", ".", "error", "(", "\"{} not found\"", ".", "format", "(", "p1", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isfile", "(", "p2", ")", ":", "\n", "                ", "logger", ".", "error", "(", "\"{} not found\"", ".", "format", "(", "p2", ")", ")", "\n", "", "", "", "assert", "all", "(", "[", "all", "(", "[", "os", ".", "path", ".", "isfile", "(", "p1", ")", "and", "os", ".", "path", ".", "isfile", "(", "p2", ")", "for", "p1", ",", "p2", "in", "paths", ".", "values", "(", ")", "]", ")", "for", "paths", "in", "params", ".", "para_dataset", ".", "values", "(", ")", "]", ")", "\n", "\n", "# check that we can evaluate on BLEU", "\n", "assert", "params", ".", "eval_bleu", "is", "False", "or", "len", "(", "params", ".", "mt_steps", "+", "params", ".", "bt_steps", ")", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_data": [[326, 355], ["loader.load_mono_data", "loader.load_para_data", "logger.info", "data[].items", "data[].items", "logger.info", "v.keys", "v.keys", "logger.info", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_mono_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_para_data"], ["", "def", "load_data", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Load monolingual data.\n    The returned dictionary contains:\n        - dico (dictionary)\n        - vocab (FloatTensor)\n        - train / valid / test (monolingual datasets)\n    \"\"\"", "\n", "data", "=", "{", "}", "\n", "\n", "# monolingual datasets", "\n", "load_mono_data", "(", "params", ",", "data", ")", "\n", "\n", "# parallel datasets", "\n", "load_para_data", "(", "params", ",", "data", ")", "\n", "\n", "# monolingual data summary", "\n", "logger", ".", "info", "(", "'============ Data summary'", ")", "\n", "for", "lang", ",", "v", "in", "data", "[", "'mono_stream'", "]", ".", "items", "(", ")", ":", "\n", "        ", "for", "data_set", "in", "v", ".", "keys", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'{: <18} - {: >5} - {: >12}:{: >10}'", ".", "format", "(", "'Monolingual data'", ",", "data_set", ",", "lang", ",", "len", "(", "v", "[", "data_set", "]", ")", ")", ")", "\n", "\n", "# parallel data summary", "\n", "", "", "for", "(", "src", ",", "tgt", ")", ",", "v", "in", "data", "[", "'para'", "]", ".", "items", "(", ")", ":", "\n", "        ", "for", "data_set", "in", "v", ".", "keys", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'{: <18} - {: >5} - {: >12}:{: >10}'", ".", "format", "(", "'Parallel data'", ",", "data_set", ",", "'%s-%s'", "%", "(", "src", ",", "tgt", ")", ",", "len", "(", "v", "[", "data_set", "]", ")", ")", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"\"", ")", "\n", "return", "data", "\n", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.__init__": [[33, 41], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "embedder", ",", "scores", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize XNLI trainer / evaluator.\n        Initial `embedder` should be on CPU to save memory.\n        \"\"\"", "\n", "self", ".", "_embedder", "=", "embedder", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "scores", "=", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.get_iterator": [[42, 51], ["[].get_iterator"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_iterator", "(", "self", ",", "splt", ",", "lang", ")", ":", "\n", "        ", "\"\"\"\n        Get a monolingual data iterator.\n        \"\"\"", "\n", "assert", "splt", "in", "[", "'valid'", ",", "'test'", "]", "or", "splt", "==", "'train'", "and", "lang", "==", "'en'", "\n", "return", "self", ".", "data", "[", "lang", "]", "[", "splt", "]", "[", "'x'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "(", "splt", "==", "'train'", ")", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", ",", "\n", "return_indices", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.run": [[53, 95], ["xnli.XNLI.load_data", "copy.deepcopy", "xnli.XNLI.embedder.cuda", "torch.nn.Sequential().cuda", "torch.nn.Sequential().cuda", "optim.get_optimizer", "optim.get_optimizer", "range", "Exception", "list", "xnli.XNLI.proj.parameters", "logger.info", "xnli.XNLI.train", "logger.info", "torch.nn.Sequential", "torch.nn.Sequential", "xnli.XNLI.embedder.get_parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "xnli.XNLI.eval", "xnli.XNLI.scores.update", "len", "len", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Run XNLI training / evaluation.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "\n", "# load data", "\n", "self", ".", "data", "=", "self", ".", "load_data", "(", ")", "\n", "if", "not", "self", ".", "data", "[", "'dico'", "]", "==", "self", ".", "_embedder", ".", "dico", ":", "\n", "            ", "raise", "Exception", "(", "(", "\"Dictionary in evaluation data (%i words) seems different than the one \"", "+", "\n", "\"in the pretrained model (%i words). Please verify you used the same dictionary, \"", "+", "\n", "\"and the same values for max_vocab and min_count.\"", ")", "%", "(", "len", "(", "self", ".", "data", "[", "'dico'", "]", ")", ",", "len", "(", "self", ".", "_embedder", ".", "dico", ")", ")", ")", "\n", "\n", "# embedder", "\n", "", "self", ".", "embedder", "=", "copy", ".", "deepcopy", "(", "self", ".", "_embedder", ")", "\n", "self", ".", "embedder", ".", "cuda", "(", ")", "\n", "\n", "# projection layer", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "Dropout", "(", "params", ".", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embedder", ".", "out_dim", ",", "3", ")", "\n", "]", ")", ".", "cuda", "(", ")", "\n", "\n", "# optimizers", "\n", "self", ".", "optimizer_e", "=", "get_optimizer", "(", "list", "(", "self", ".", "embedder", ".", "get_parameters", "(", "params", ".", "finetune_layers", ")", ")", ",", "params", ".", "optimizer_e", ")", "\n", "self", ".", "optimizer_p", "=", "get_optimizer", "(", "self", ".", "proj", ".", "parameters", "(", ")", ",", "params", ".", "optimizer_p", ")", "\n", "\n", "# train and evaluate the model", "\n", "for", "epoch", "in", "range", "(", "params", ".", "n_epochs", ")", ":", "\n", "\n", "# update epoch", "\n", "            ", "self", ".", "epoch", "=", "epoch", "\n", "\n", "# training", "\n", "logger", ".", "info", "(", "\"XNLI - Training epoch %i ...\"", "%", "epoch", ")", "\n", "self", ".", "train", "(", ")", "\n", "\n", "# evaluation", "\n", "logger", ".", "info", "(", "\"XNLI - Evaluating epoch %i ...\"", "%", "epoch", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "scores", "=", "self", ".", "eval", "(", ")", "\n", "self", ".", "scores", ".", "update", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.train": [[96, 161], ["xnli.XNLI.embedder.train", "xnli.XNLI.proj.train", "time.time", "xnli.XNLI.get_iterator", "utils.truncate", "utils.truncate", "utils.concat_batches", "len", "utils.to_cuda", "xnli.XNLI.proj", "torch.cross_entropy", "torch.cross_entropy", "xnli.XNLI.optimizer_e.zero_grad", "xnli.XNLI.optimizer_p.zero_grad", "torch.cross_entropy.backward", "xnli.XNLI.optimizer_e.step", "xnli.XNLI.optimizer_p.step", "lengths.sum().item", "losses.append", "next", "xnli.XNLI.embedder.get_embeddings", "torch.cross_entropy.item", "logger.info", "lengths.sum", "time.time", "sum", "len", "time.time"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_embeddings"], ["", "", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Finetune for one epoch on the XNLI English training set.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "embedder", ".", "train", "(", ")", "\n", "self", ".", "proj", ".", "train", "(", ")", "\n", "\n", "# training variables", "\n", "losses", "=", "[", "]", "\n", "ns", "=", "0", "# number of sentences", "\n", "nw", "=", "0", "# number of words", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "iterator", "=", "self", ".", "get_iterator", "(", "'train'", ",", "'en'", ")", "\n", "lang_id", "=", "params", ".", "lang2id", "[", "'en'", "]", "\n", "\n", "while", "True", ":", "\n", "\n", "# batch", "\n", "            ", "try", ":", "\n", "                ", "batch", "=", "next", "(", "iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "break", "\n", "", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", ",", "idx", "=", "batch", "\n", "sent1", ",", "len1", "=", "truncate", "(", "sent1", ",", "len1", ",", "params", ".", "max_len", ",", "params", ".", "eos_index", ")", "\n", "sent2", ",", "len2", "=", "truncate", "(", "sent2", ",", "len2", ",", "params", ".", "max_len", ",", "params", ".", "eos_index", ")", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "\n", "sent1", ",", "len1", ",", "lang_id", ",", "\n", "sent2", ",", "len2", ",", "lang_id", ",", "\n", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "\n", "reset_positions", "=", "False", "\n", ")", "\n", "y", "=", "self", ".", "data", "[", "'en'", "]", "[", "'train'", "]", "[", "'y'", "]", "[", "idx", "]", "\n", "bs", "=", "len", "(", "len1", ")", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "y", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# loss", "\n", "output", "=", "self", ".", "proj", "(", "self", ".", "embedder", ".", "get_embeddings", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "output", ",", "y", ")", "\n", "\n", "# backward / optimization", "\n", "self", ".", "optimizer_e", ".", "zero_grad", "(", ")", "\n", "self", ".", "optimizer_p", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer_e", ".", "step", "(", ")", "\n", "self", ".", "optimizer_p", ".", "step", "(", ")", "\n", "\n", "# update statistics", "\n", "ns", "+=", "bs", "\n", "nw", "+=", "lengths", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "losses", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# log", "\n", "if", "ns", "%", "(", "100", "*", "bs", ")", "<", "bs", ":", "\n", "                ", "logger", ".", "info", "(", "\"XNLI - Epoch %i - Train iter %7i - %.1f words/s - Loss: %.4f\"", "%", "(", "self", ".", "epoch", ",", "ns", ",", "nw", "/", "(", "time", ".", "time", "(", ")", "-", "t", ")", ",", "sum", "(", "losses", ")", "/", "len", "(", "losses", ")", ")", ")", "\n", "nw", ",", "t", "=", "0", ",", "time", ".", "time", "(", ")", "\n", "losses", "=", "[", "]", "\n", "\n", "# epoch size", "\n", "", "if", "params", ".", "epoch_size", "!=", "-", "1", "and", "ns", ">=", "params", ".", "epoch_size", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.eval": [[162, 213], ["xnli.XNLI.embedder.eval", "xnli.XNLI.proj.eval", "collections.OrderedDict", "logger.info", "xnli.XNLI.get_iterator", "logger.info", "json.dumps", "utils.concat_batches", "utils.to_cuda", "xnli.XNLI.proj", "predictions.eq().sum().item", "len", "xnli.XNLI.embedder.get_embeddings", "xnli.XNLI.data.max", "predictions.eq().sum", "predictions.eq"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_embeddings"], ["", "", "", "def", "eval", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate on XNLI validation and test sets, for all languages.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "embedder", ".", "eval", "(", ")", "\n", "self", ".", "proj", ".", "eval", "(", ")", "\n", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "self", ".", "epoch", "}", ")", "\n", "\n", "for", "splt", "in", "[", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "            ", "for", "lang", "in", "XNLI_LANGS", ":", "\n", "                ", "if", "lang", "not", "in", "params", ".", "lang2id", ":", "\n", "                    ", "continue", "\n", "\n", "", "lang_id", "=", "params", ".", "lang2id", "[", "lang", "]", "\n", "valid", "=", "0", "\n", "total", "=", "0", "\n", "\n", "for", "batch", "in", "self", ".", "get_iterator", "(", "splt", ",", "lang", ")", ":", "\n", "\n", "# batch", "\n", "                    ", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", ",", "idx", "=", "batch", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "\n", "sent1", ",", "len1", ",", "lang_id", ",", "\n", "sent2", ",", "len2", ",", "lang_id", ",", "\n", "params", ".", "pad_index", ",", "\n", "params", ".", "eos_index", ",", "\n", "reset_positions", "=", "False", "\n", ")", "\n", "y", "=", "self", ".", "data", "[", "lang", "]", "[", "splt", "]", "[", "'y'", "]", "[", "idx", "]", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "y", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# forward", "\n", "output", "=", "self", ".", "proj", "(", "self", ".", "embedder", ".", "get_embeddings", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ")", ")", "\n", "predictions", "=", "output", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "\n", "# update statistics", "\n", "valid", "+=", "predictions", ".", "eq", "(", "y", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "total", "+=", "len", "(", "len1", ")", "\n", "\n", "# compute accuracy", "\n", "", "acc", "=", "100.0", "*", "valid", "/", "total", "\n", "scores", "[", "'xnli_%s_%s_acc'", "%", "(", "splt", ",", "lang", ")", "]", "=", "acc", "\n", "logger", ".", "info", "(", "\"XNLI - %s - %s - Epoch %i - Acc: %.1f%%\"", "%", "(", "splt", ",", "lang", ",", "self", ".", "epoch", ",", "acc", ")", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.xnli.XNLI.load_data": [[214, 255], ["os.path.join", "data.loader.load_binarized", "data.loader.load_binarized", "data.get", "data.loader.set_dico_parameters", "data.loader.set_dico_parameters", "data.dataset.ParallelDataset", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "os.path.join", "os.path.join", "open", "len", "len", "os.path.join", "l.rstrip"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Load XNLI cross-lingual classification data.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "data", "=", "{", "lang", ":", "{", "splt", ":", "{", "}", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "}", "for", "lang", "in", "XNLI_LANGS", "}", "\n", "label2id", "=", "{", "'contradiction'", ":", "0", ",", "'neutral'", ":", "1", ",", "'entailment'", ":", "2", "}", "\n", "dpath", "=", "os", ".", "path", ".", "join", "(", "params", ".", "data_path", ",", "'eval'", ",", "'XNLI'", ")", "\n", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "            ", "for", "lang", "in", "XNLI_LANGS", ":", "\n", "\n", "# only English has a training set", "\n", "                ", "if", "splt", "==", "'train'", "and", "lang", "!=", "'en'", ":", "\n", "                    ", "del", "data", "[", "lang", "]", "[", "'train'", "]", "\n", "continue", "\n", "\n", "# load data and dictionary", "\n", "", "data1", "=", "load_binarized", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.s1.%s.pth'", "%", "(", "splt", ",", "lang", ")", ")", ",", "params", ")", "\n", "data2", "=", "load_binarized", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.s2.%s.pth'", "%", "(", "splt", ",", "lang", ")", ")", ",", "params", ")", "\n", "data", "[", "'dico'", "]", "=", "data", ".", "get", "(", "'dico'", ",", "data1", "[", "'dico'", "]", ")", "\n", "\n", "# set dictionary parameters", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "data1", "[", "'dico'", "]", ")", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "data2", "[", "'dico'", "]", ")", "\n", "\n", "# create dataset", "\n", "data", "[", "lang", "]", "[", "splt", "]", "[", "'x'", "]", "=", "ParallelDataset", "(", "\n", "data1", "[", "'sentences'", "]", ",", "data1", "[", "'positions'", "]", ",", "\n", "data2", "[", "'sentences'", "]", ",", "data2", "[", "'positions'", "]", ",", "\n", "params", "\n", ")", "\n", "\n", "# load labels", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.label.%s'", "%", "(", "splt", ",", "lang", ")", ")", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "labels", "=", "[", "label2id", "[", "l", ".", "rstrip", "(", ")", "]", "for", "l", "in", "f", "]", "\n", "", "data", "[", "lang", "]", "[", "splt", "]", "[", "'y'", "]", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "assert", "len", "(", "data", "[", "lang", "]", "[", "splt", "]", "[", "'x'", "]", ")", "==", "len", "(", "data", "[", "lang", "]", "[", "splt", "]", "[", "'y'", "]", ")", "\n", "\n", "", "", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.__init__": [[49, 57], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "embedder", ",", "scores", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize GLUE trainer / evaluator.\n        Initial `embedder` should be on CPU to save memory.\n        \"\"\"", "\n", "self", ".", "_embedder", "=", "embedder", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "scores", "=", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.get_iterator": [[58, 66], ["[].get_iterator"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_iterator", "(", "self", ",", "splt", ")", ":", "\n", "        ", "\"\"\"\n        Build data iterator.\n        \"\"\"", "\n", "return", "self", ".", "data", "[", "splt", "]", "[", "'x'", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "(", "splt", "==", "'train'", ")", ",", "\n", "return_indices", "=", "True", ",", "\n", "group_by_size", "=", "self", ".", "params", ".", "group_by_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.run": [[68, 116], ["glue.GLUE.load_data", "copy.deepcopy", "glue.GLUE.embedder.cuda", "torch.nn.Sequential().cuda", "torch.nn.Sequential().cuda", "optim.get_optimizer", "optim.get_optimizer", "range", "Exception", "list", "glue.GLUE.proj.parameters", "logger.info", "glue.GLUE.train", "logger.info", "torch.nn.Sequential", "torch.nn.Sequential", "glue.GLUE.embedder.get_parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "glue.GLUE.eval", "glue.GLUE.scores.update", "glue.GLUE.eval", "len", "len", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.get_optimizer", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.Linear"], ["", "def", "run", "(", "self", ",", "task", ")", ":", "\n", "        ", "\"\"\"\n        Run GLUE training / evaluation.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "\n", "# task parameters", "\n", "self", ".", "task", "=", "task", "\n", "params", ".", "out_features", "=", "N_CLASSES", "[", "task", "]", "\n", "self", ".", "is_classif", "=", "task", "!=", "'STS-B'", "\n", "\n", "# load data", "\n", "self", ".", "data", "=", "self", ".", "load_data", "(", "task", ")", "\n", "if", "not", "self", ".", "data", "[", "'dico'", "]", "==", "self", ".", "_embedder", ".", "dico", ":", "\n", "            ", "raise", "Exception", "(", "(", "\"Dictionary in evaluation data (%i words) seems different than the one \"", "+", "\n", "\"in the pretrained model (%i words). Please verify you used the same dictionary, \"", "+", "\n", "\"and the same values for max_vocab and min_count.\"", ")", "%", "(", "len", "(", "self", ".", "data", "[", "'dico'", "]", ")", ",", "len", "(", "self", ".", "_embedder", ".", "dico", ")", ")", ")", "\n", "\n", "# embedder", "\n", "", "self", ".", "embedder", "=", "copy", ".", "deepcopy", "(", "self", ".", "_embedder", ")", "\n", "self", ".", "embedder", ".", "cuda", "(", ")", "\n", "\n", "# projection layer", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "nn", ".", "Dropout", "(", "params", ".", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embedder", ".", "out_dim", ",", "params", ".", "out_features", ")", "\n", "]", ")", ".", "cuda", "(", ")", "\n", "\n", "# optimizers", "\n", "self", ".", "optimizer_e", "=", "get_optimizer", "(", "list", "(", "self", ".", "embedder", ".", "get_parameters", "(", "params", ".", "finetune_layers", ")", ")", ",", "params", ".", "optimizer_e", ")", "\n", "self", ".", "optimizer_p", "=", "get_optimizer", "(", "self", ".", "proj", ".", "parameters", "(", ")", ",", "params", ".", "optimizer_p", ")", "\n", "\n", "# train and evaluate the model", "\n", "for", "epoch", "in", "range", "(", "params", ".", "n_epochs", ")", ":", "\n", "\n", "# update epoch", "\n", "            ", "self", ".", "epoch", "=", "epoch", "\n", "\n", "# training", "\n", "logger", ".", "info", "(", "\"GLUE - %s - Training epoch %i ...\"", "%", "(", "task", ",", "epoch", ")", ")", "\n", "self", ".", "train", "(", ")", "\n", "\n", "# evaluation", "\n", "logger", ".", "info", "(", "\"GLUE - %s - Evaluating epoch %i ...\"", "%", "(", "task", ",", "epoch", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "scores", "=", "self", ".", "eval", "(", "'valid'", ")", "\n", "self", ".", "scores", ".", "update", "(", "scores", ")", "\n", "self", ".", "eval", "(", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train": [[117, 186], ["glue.GLUE.embedder.train", "glue.GLUE.proj.train", "time.time", "glue.GLUE.get_iterator", "len", "utils.to_cuda", "glue.GLUE.proj", "glue.GLUE.optimizer_e.zero_grad", "glue.GLUE.optimizer_p.zero_grad", "torch.mse_loss.backward", "glue.GLUE.optimizer_e.step", "glue.GLUE.optimizer_p.step", "lengths.sum().item", "losses.append", "next", "utils.truncate", "utils.truncate", "utils.truncate", "utils.concat_batches", "glue.GLUE.embedder.get_embeddings", "torch.cross_entropy", "torch.cross_entropy", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss.item", "logger.info", "glue.GLUE.squeeze", "y.float", "lengths.sum", "time.time", "sum", "len", "time.time"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.train", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.optim.AdamCosineWithWarmup.step", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.truncate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_embeddings"], ["", "", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Finetune for one epoch on the training set.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "embedder", ".", "train", "(", ")", "\n", "self", ".", "proj", ".", "train", "(", ")", "\n", "\n", "# training variables", "\n", "losses", "=", "[", "]", "\n", "ns", "=", "0", "# number of sentences", "\n", "nw", "=", "0", "# number of words", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "iterator", "=", "self", ".", "get_iterator", "(", "'train'", ")", "\n", "lang_id", "=", "params", ".", "lang2id", "[", "'en'", "]", "\n", "\n", "while", "True", ":", "\n", "\n", "# batch", "\n", "            ", "try", ":", "\n", "                ", "batch", "=", "next", "(", "iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "break", "\n", "", "if", "self", ".", "n_sent", "==", "1", ":", "\n", "                ", "(", "x", ",", "lengths", ")", ",", "idx", "=", "batch", "\n", "x", ",", "lengths", "=", "truncate", "(", "x", ",", "lengths", ",", "params", ".", "max_len", ",", "params", ".", "eos_index", ")", "\n", "", "else", ":", "\n", "                ", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", ",", "idx", "=", "batch", "\n", "sent1", ",", "len1", "=", "truncate", "(", "sent1", ",", "len1", ",", "params", ".", "max_len", ",", "params", ".", "eos_index", ")", "\n", "sent2", ",", "len2", "=", "truncate", "(", "sent2", ",", "len2", ",", "params", ".", "max_len", ",", "params", ".", "eos_index", ")", "\n", "x", ",", "lengths", ",", "_", ",", "_", "=", "concat_batches", "(", "sent1", ",", "len1", ",", "lang_id", ",", "sent2", ",", "len2", ",", "lang_id", ",", "params", ".", "pad_index", ",", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "", "y", "=", "self", ".", "data", "[", "'train'", "]", "[", "'y'", "]", "[", "idx", "]", "\n", "bs", "=", "len", "(", "lengths", ")", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "lengths", "=", "to_cuda", "(", "x", ",", "y", ",", "lengths", ")", "\n", "\n", "# loss", "\n", "output", "=", "self", ".", "proj", "(", "self", ".", "embedder", ".", "get_embeddings", "(", "x", ",", "lengths", ",", "positions", "=", "None", ",", "langs", "=", "None", ")", ")", "\n", "if", "self", ".", "is_classif", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "output", ",", "y", ",", "weight", "=", "self", ".", "weights", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "F", ".", "mse_loss", "(", "output", ".", "squeeze", "(", "1", ")", ",", "y", ".", "float", "(", ")", ")", "\n", "\n", "# backward / optimization", "\n", "", "self", ".", "optimizer_e", ".", "zero_grad", "(", ")", "\n", "self", ".", "optimizer_p", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer_e", ".", "step", "(", ")", "\n", "self", ".", "optimizer_p", ".", "step", "(", ")", "\n", "\n", "# update statistics", "\n", "ns", "+=", "bs", "\n", "nw", "+=", "lengths", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "losses", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# log", "\n", "if", "ns", "!=", "0", "and", "ns", "%", "(", "10", "*", "bs", ")", "<", "bs", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"GLUE - %s - Epoch %s - Train iter %7i - %.1f words/s - %s Loss: %.4f\"", "\n", "%", "(", "self", ".", "task", ",", "self", ".", "epoch", ",", "ns", ",", "nw", "/", "(", "time", ".", "time", "(", ")", "-", "t", ")", ",", "'XE'", "if", "self", ".", "is_classif", "else", "'MSE'", ",", "sum", "(", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", ")", "\n", "nw", ",", "t", "=", "0", ",", "time", ".", "time", "(", ")", "\n", "losses", "=", "[", "]", "\n", "\n", "# epoch size", "\n", "", "if", "params", ".", "epoch_size", "!=", "-", "1", "and", "ns", ">=", "params", ".", "epoch_size", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval": [[187, 261], ["glue.GLUE.embedder.eval", "glue.GLUE.proj.eval", "collections.OrderedDict", "glue.GLUE.task.lower", "glue.GLUE.get_iterator", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "os.path.join", "logger.info", "utils.to_cuda", "glue.GLUE.proj", "numpy.concatenate.append", "numpy.concatenate.append", "numpy.concatenate.append", "len", "len", "len", "len", "numpy.concatenate", "logger.info", "open", "zip", "utils.concat_batches", "glue.GLUE.embedder.get_embeddings", "glue.GLUE.squeeze", "glue.GLUE.cpu().numpy", "p.cpu().numpy", "numpy.concatenate.append", "len", "len", "f.write", "glue.GLUE.data.max", "y.cpu().numpy", "len", "sklearn.metrics.f1_score", "sklearn.metrics.matthews_corrcoef", "json.dumps", "len", "glue.GLUE.cpu", "p.cpu", "scipy.stats.pearsonr", "scipy.stats.spearmanr", "y.cpu", "str"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.get_embeddings"], ["", "", "", "def", "eval", "(", "self", ",", "splt", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate on XNLI validation and test sets, for all languages.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "self", ".", "embedder", ".", "eval", "(", ")", "\n", "self", ".", "proj", ".", "eval", "(", ")", "\n", "\n", "assert", "splt", "in", "[", "'valid'", ",", "'test'", "]", "\n", "has_labels", "=", "'y'", "in", "self", ".", "data", "[", "splt", "]", "\n", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "self", ".", "epoch", "}", ")", "\n", "task", "=", "self", ".", "task", ".", "lower", "(", ")", "\n", "\n", "idxs", "=", "[", "]", "# sentence indices", "\n", "prob", "=", "[", "]", "# probabilities", "\n", "pred", "=", "[", "]", "# predicted values", "\n", "gold", "=", "[", "]", "# real values", "\n", "\n", "lang_id", "=", "params", ".", "lang2id", "[", "'en'", "]", "\n", "\n", "for", "batch", "in", "self", ".", "get_iterator", "(", "splt", ")", ":", "\n", "\n", "# batch", "\n", "            ", "if", "self", ".", "n_sent", "==", "1", ":", "\n", "                ", "(", "x", ",", "lengths", ")", ",", "idx", "=", "batch", "\n", "# x, lengths = truncate(x, lengths, params.max_len, params.eos_index)", "\n", "", "else", ":", "\n", "                ", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", ",", "idx", "=", "batch", "\n", "# sent1, len1 = truncate(sent1, len1, params.max_len, params.eos_index)", "\n", "# sent2, len2 = truncate(sent2, len2, params.max_len, params.eos_index)", "\n", "x", ",", "lengths", ",", "_", ",", "_", "=", "concat_batches", "(", "sent1", ",", "len1", ",", "lang_id", ",", "sent2", ",", "len2", ",", "lang_id", ",", "params", ".", "pad_index", ",", "params", ".", "eos_index", ",", "reset_positions", "=", "False", ")", "\n", "", "y", "=", "self", ".", "data", "[", "splt", "]", "[", "'y'", "]", "[", "idx", "]", "if", "has_labels", "else", "None", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "lengths", "=", "to_cuda", "(", "x", ",", "y", ",", "lengths", ")", "\n", "\n", "# prediction", "\n", "output", "=", "self", ".", "proj", "(", "self", ".", "embedder", ".", "get_embeddings", "(", "x", ",", "lengths", ",", "positions", "=", "None", ",", "langs", "=", "None", ")", ")", "\n", "p", "=", "output", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "if", "self", ".", "is_classif", "else", "output", ".", "squeeze", "(", "1", ")", "\n", "idxs", ".", "append", "(", "idx", ")", "\n", "prob", ".", "append", "(", "output", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "pred", ".", "append", "(", "p", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "if", "has_labels", ":", "\n", "                ", "gold", ".", "append", "(", "y", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# indices / predictions", "\n", "", "", "idxs", "=", "np", ".", "concatenate", "(", "idxs", ")", "\n", "prob", "=", "np", ".", "concatenate", "(", "prob", ")", "\n", "pred", "=", "np", ".", "concatenate", "(", "pred", ")", "\n", "assert", "len", "(", "idxs", ")", "==", "len", "(", "pred", ")", ",", "(", "len", "(", "idxs", ")", ",", "len", "(", "pred", ")", ")", "\n", "assert", "idxs", "[", "-", "1", "]", "==", "len", "(", "idxs", ")", "-", "1", ",", "(", "idxs", "[", "-", "1", "]", ",", "len", "(", "idxs", ")", "-", "1", ")", "\n", "\n", "# score the predictions if we have labels", "\n", "if", "has_labels", ":", "\n", "            ", "gold", "=", "np", ".", "concatenate", "(", "gold", ")", "\n", "prefix", "=", "f'{splt}_{task}'", "\n", "if", "self", ".", "is_classif", ":", "\n", "                ", "scores", "[", "'%s_acc'", "%", "prefix", "]", "=", "100.", "*", "(", "pred", "==", "gold", ")", ".", "sum", "(", ")", "/", "len", "(", "pred", ")", "\n", "scores", "[", "'%s_f1'", "%", "prefix", "]", "=", "100.", "*", "f1_score", "(", "gold", ",", "pred", ",", "average", "=", "'binary'", "if", "params", ".", "out_features", "==", "2", "else", "'micro'", ")", "\n", "scores", "[", "'%s_mc'", "%", "prefix", "]", "=", "100.", "*", "matthews_corrcoef", "(", "gold", ",", "pred", ")", "\n", "", "else", ":", "\n", "                ", "scores", "[", "'%s_prs'", "%", "prefix", "]", "=", "100.", "*", "pearsonr", "(", "pred", ",", "gold", ")", "[", "0", "]", "\n", "scores", "[", "'%s_spr'", "%", "prefix", "]", "=", "100.", "*", "spearmanr", "(", "pred", ",", "gold", ")", "[", "0", "]", "\n", "", "logger", ".", "info", "(", "\"__log__:%s\"", "%", "json", ".", "dumps", "(", "scores", ")", ")", "\n", "\n", "# output predictions", "\n", "", "pred_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "f'{splt}.pred.{self.epoch}'", ")", "\n", "with", "open", "(", "pred_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "i", ",", "p", "in", "zip", "(", "idxs", ",", "prob", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%i\\t%s\\n'", "%", "(", "i", ",", "','", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "p", "]", ")", ")", ")", "\n", "", "", "logger", ".", "info", "(", "f\"Wrote {len(idxs)} {splt} predictions to {pred_path}\"", ")", "\n", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.load_data": [[262, 326], ["os.path.join", "data.loader.load_binarized", "data.get", "data.loader.set_dico_parameters", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "os.path.join", "data.loader.load_binarized", "data.loader.set_dico_parameters", "data.dataset.Dataset", "data.dataset.ParallelDataset", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor().cuda.sum", "torch.FloatTensor().cuda.sum", "os.path.join", "open", "all", "len", "len", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "os.path.join", "l.rstrip", "float", "lab2id.get", "float", "enumerate", "enumerate", "range", "sorted", "sorted", "len", "set", "set", "set"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.embedder.SentenceEmbedder.cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.load_binarized", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.loader.set_dico_parameters"], ["", "def", "load_data", "(", "self", ",", "task", ")", ":", "\n", "        ", "\"\"\"\n        Load pair regression/classification bi-sentence tasks\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "data", "=", "{", "splt", ":", "{", "}", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "}", "\n", "dpath", "=", "os", ".", "path", ".", "join", "(", "params", ".", "data_path", ",", "'eval'", ",", "task", ")", "\n", "\n", "self", ".", "n_sent", "=", "1", "if", "task", "in", "[", "'SST-2'", ",", "'CoLA'", "]", "else", "2", "\n", "\n", "for", "splt", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "# load data and dictionary", "\n", "            ", "data1", "=", "load_binarized", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.s1.pth'", "%", "splt", ")", ",", "params", ")", "\n", "data2", "=", "load_binarized", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.s2.pth'", "%", "splt", ")", ",", "params", ")", "if", "self", ".", "n_sent", "==", "2", "else", "None", "\n", "data", "[", "'dico'", "]", "=", "data", ".", "get", "(", "'dico'", ",", "data1", "[", "'dico'", "]", ")", "\n", "\n", "# set dictionary parameters", "\n", "set_dico_parameters", "(", "params", ",", "data", ",", "data1", "[", "'dico'", "]", ")", "\n", "if", "self", ".", "n_sent", "==", "2", ":", "\n", "                ", "set_dico_parameters", "(", "params", ",", "data", ",", "data2", "[", "'dico'", "]", ")", "\n", "\n", "# create dataset", "\n", "", "if", "self", ".", "n_sent", "==", "1", ":", "\n", "                ", "data", "[", "splt", "]", "[", "'x'", "]", "=", "Dataset", "(", "data1", "[", "'sentences'", "]", ",", "data1", "[", "'positions'", "]", ",", "params", ")", "\n", "", "else", ":", "\n", "                ", "data", "[", "splt", "]", "[", "'x'", "]", "=", "ParallelDataset", "(", "\n", "data1", "[", "'sentences'", "]", ",", "data1", "[", "'positions'", "]", ",", "\n", "data2", "[", "'sentences'", "]", ",", "data2", "[", "'positions'", "]", ",", "\n", "params", "\n", ")", "\n", "\n", "# load labels", "\n", "", "if", "splt", "!=", "'test'", "or", "task", "in", "[", "'MRPC'", "]", ":", "\n", "# read labels from file", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "'%s.label'", "%", "splt", ")", ",", "'r'", ")", "as", "f", ":", "\n", "                    ", "lines", "=", "[", "l", ".", "rstrip", "(", ")", "for", "l", "in", "f", "]", "\n", "# STS-B task", "\n", "", "if", "task", "==", "'STS-B'", ":", "\n", "                    ", "assert", "all", "(", "0", "<=", "float", "(", "x", ")", "<=", "5", "for", "x", "in", "lines", ")", "\n", "y", "=", "[", "float", "(", "l", ")", "for", "l", "in", "lines", "]", "\n", "# QQP", "\n", "", "elif", "task", "==", "'QQP'", ":", "\n", "                    ", "UNK_LABEL", "=", "0", "\n", "lab2id", "=", "{", "x", ":", "i", "for", "i", ",", "x", "in", "enumerate", "(", "sorted", "(", "set", "(", "lines", ")", "-", "set", "(", "[", "''", "]", ")", ")", ")", "}", "\n", "y", "=", "[", "lab2id", ".", "get", "(", "x", ",", "UNK_LABEL", ")", "for", "x", "in", "lines", "]", "\n", "# other tasks", "\n", "", "else", ":", "\n", "                    ", "lab2id", "=", "{", "x", ":", "i", "for", "i", ",", "x", "in", "enumerate", "(", "sorted", "(", "set", "(", "lines", ")", ")", ")", "}", "\n", "y", "=", "[", "lab2id", "[", "x", "]", "for", "x", "in", "lines", "]", "\n", "", "data", "[", "splt", "]", "[", "'y'", "]", "=", "torch", ".", "LongTensor", "(", "y", ")", "\n", "assert", "len", "(", "data", "[", "splt", "]", "[", "'x'", "]", ")", "==", "len", "(", "data", "[", "splt", "]", "[", "'y'", "]", ")", "\n", "\n", "# compute weights for weighted training", "\n", "", "", "if", "task", "!=", "'STS-B'", "and", "params", ".", "weighted_training", ":", "\n", "            ", "weights", "=", "torch", ".", "FloatTensor", "(", "[", "\n", "1.0", "/", "(", "data", "[", "'train'", "]", "[", "'y'", "]", "==", "i", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "lab2id", ")", ")", "\n", "]", ")", ".", "cuda", "(", ")", "\n", "self", ".", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weights", "=", "None", "\n", "\n", "", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.__init__": [[88, 104], ["getattr", "logger.info", "os.path.join", "subprocess.Popen().wait", "evaluator.Evaluator.create_reference_files", "subprocess.Popen"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.create_reference_files"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "data", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Initialize evaluator.\n        \"\"\"", "\n", "self", ".", "trainer", "=", "trainer", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "dico", "=", "data", "[", "'dico'", "]", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "memory_list", "=", "trainer", ".", "memory_list", "\n", "self", ".", "is_sentencepiece", "=", "getattr", "(", "params", ",", "'is_sentencepiece'", ",", "False", ")", "\n", "logger", ".", "info", "(", "'Sentencepiece: {}'", ".", "format", "(", "self", ".", "is_sentencepiece", ")", ")", "\n", "# create directory to store hypotheses, and reference files for BLEU evaluation", "\n", "params", ".", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'hypotheses'", ")", "\n", "if", "self", ".", "params", ".", "is_master", ":", "\n", "            ", "subprocess", ".", "Popen", "(", "'mkdir -p %s'", "%", "params", ".", "hyp_path", ",", "shell", "=", "True", ")", ".", "wait", "(", ")", "\n", "self", ".", "create_reference_files", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.get_iterator": [[105, 152], ["len", "set", "set", "[].get_iterator", "len", "[].get_iterator", "[].get_iterator"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "", "def", "get_iterator", "(", "self", ",", "data_set", ",", "lang1", ",", "lang2", "=", "None", ",", "stream", "=", "False", ",", "allow_train_data", "=", "False", ",", "descending", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Create a new iterator for a dataset.\n        \"\"\"", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "allow_train_data", "\n", "assert", "lang1", "in", "self", ".", "params", ".", "langs", "\n", "assert", "lang2", "is", "None", "or", "lang2", "in", "self", ".", "params", ".", "langs", "\n", "assert", "stream", "is", "False", "or", "lang2", "is", "None", "\n", "\n", "# hacks to reduce evaluation time when using many languages", "\n", "if", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "30", ":", "\n", "            ", "eval_lgs", "=", "set", "(", "\n", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", ",", "\"ab\"", ",", "\"ay\"", ",", "\n", "\"bug\"", ",", "\"ha\"", ",", "\"ko\"", ",", "\"ln\"", ",", "\"min\"", ",", "\"nds\"", ",", "\"pap\"", ",", "\"pt\"", ",", "\"tg\"", ",", "\"to\"", ",", "\"udm\"", ",", "\"uk\"", ",", "\"zh_classical\"", "]", ")", "\n", "eval_lgs", "=", "set", "(", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", "]", ")", "\n", "subsample", "=", "10", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "5", "\n", "n_sentences", "=", "600", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "1500", "\n", "", "elif", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "5", ":", "\n", "            ", "subsample", "=", "10", "if", "data_set", "==", "'test'", "else", "5", "\n", "n_sentences", "=", "300", "if", "data_set", "==", "'test'", "else", "1500", "\n", "", "else", ":", "\n", "# n_sentences = -1 if data_set == 'valid' else 100", "\n", "            ", "n_sentences", "=", "-", "1", "\n", "subsample", "=", "1", "\n", "\n", "", "if", "lang2", "is", "None", ":", "\n", "            ", "if", "stream", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono_stream'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "shuffle", "=", "False", ",", "subsample", "=", "subsample", ")", "\n", "", "else", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "stream", "is", "False", "\n", "_lang1", ",", "_lang2", "=", "(", "lang1", ",", "lang2", ")", "if", "lang1", "<", "lang2", "else", "(", "lang2", ",", "lang1", ")", "\n", "iterator", "=", "self", ".", "data", "[", "'para'", "]", "[", "(", "_lang1", ",", "_lang2", ")", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", ",", "\n", ")", "\n", "\n", "", "for", "batch", "in", "iterator", ":", "\n", "            ", "yield", "batch", "if", "lang2", "is", "None", "or", "lang1", "<", "lang2", "else", "batch", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.create_reference_files": [[153, 206], ["evaluator.Evaluator.data[].items", "os.path.join", "os.path.join", "logger.info", "logger.info", "evaluator.Evaluator.get_iterator", "utils.restore_segmentation", "utils.restore_segmentation", "print", "lang1_txt.extend", "lang2_txt.extend", "x.replace", "x.replace", "open", "f.write", "open", "f.write", "os.path.exists", "os.path.exists", "evaluator.convert_to_text", "evaluator.convert_to_text"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text"], ["", "", "def", "create_reference_files", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create reference files for BLEU evaluation.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "params", ".", "ref_paths", "=", "{", "}", "\n", "measure_datasets", "=", "[", "'valid'", ",", "'test'", "]", "\n", "# if params.infer_train:", "\n", "#     measure_datasets = ['train'] + measure_datasets", "\n", "\n", "for", "(", "lang1", ",", "lang2", ")", ",", "v", "in", "self", ".", "data", "[", "'para'", "]", ".", "items", "(", ")", ":", "\n", "\n", "            ", "assert", "lang1", "<", "lang2", "\n", "\n", "for", "data_set", "in", "measure_datasets", ":", "\n", "\n", "# define data paths", "\n", "                ", "lang1_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "'ref.{0}-{1}.{2}.txt'", ".", "format", "(", "lang2", ",", "lang1", ",", "data_set", ")", ")", "\n", "lang2_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "'ref.{0}-{1}.{2}.txt'", ".", "format", "(", "lang1", ",", "lang2", ",", "data_set", ")", ")", "\n", "logger", ".", "info", "(", "\"| Load Ref File: {}\"", ".", "format", "(", "lang1_path", ")", ")", "\n", "logger", ".", "info", "(", "\"| Load Ref File: {}\"", ".", "format", "(", "lang2_path", ")", ")", "\n", "\n", "if", "data_set", "==", "'train'", "and", "(", "os", ".", "path", ".", "exists", "(", "lang1_path", ")", "and", "os", ".", "path", ".", "exists", "(", "lang2_path", ")", ")", ":", "\n", "                    ", "print", "(", "'Skip train dataset creating reference because they exist'", ")", "\n", "continue", "\n", "\n", "# store data paths", "\n", "", "params", ".", "ref_paths", "[", "(", "lang2", ",", "lang1", ",", "data_set", ")", "]", "=", "lang1_path", "\n", "params", ".", "ref_paths", "[", "(", "lang1", ",", "lang2", ",", "data_set", ")", "]", "=", "lang2_path", "\n", "\n", "# text sentences", "\n", "lang1_txt", "=", "[", "]", "\n", "lang2_txt", "=", "[", "]", "\n", "\n", "# convert to text", "\n", "for", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", "in", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "params", ".", "infer_train", ")", ":", "\n", "                    ", "lang1_txt", ".", "extend", "(", "convert_to_text", "(", "sent1", ",", "len1", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "lang2_txt", ".", "extend", "(", "convert_to_text", "(", "sent2", ",", "len2", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "\n", "# replace <unk> by <<unk>> as these tokens cannot be counted in BLEU", "\n", "", "lang1_txt", "=", "[", "x", ".", "replace", "(", "'<unk>'", ",", "'<<unk>>'", ")", "for", "x", "in", "lang1_txt", "]", "\n", "lang2_txt", "=", "[", "x", ".", "replace", "(", "'<unk>'", ",", "'<<unk>>'", ")", "for", "x", "in", "lang2_txt", "]", "\n", "\n", "# export hypothesis", "\n", "with", "open", "(", "lang1_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "lang1_txt", ")", "+", "'\\n'", ")", "\n", "", "with", "open", "(", "lang2_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "lang2_txt", ")", "+", "'\\n'", ")", "\n", "\n", "# restore original segmentation", "\n", "", "restore_segmentation", "(", "lang1_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "lang2_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.mask_out": [[207, 235], ["x.masked_scatter.masked_scatter.size", "range", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "_x_real.clone().fill_", "x.masked_scatter.masked_scatter.masked_scatter", "rng.rand", "to_predict.astype", "x.masked_scatter.masked_scatter.min", "x.masked_scatter.masked_scatter.max", "x.masked_scatter.masked_scatter.size", "torch.from_numpy.size", "torch.from_numpy.size", "torch.from_numpy.size", "numpy.any", "rng.randint", "_x_real.clone"], "methods", ["None"], ["", "", "", "def", "mask_out", "(", "self", ",", "x", ",", "lengths", ",", "rng", ")", ":", "\n", "        ", "\"\"\"\n        Decide of random words to mask out.\n        We specify the random generator to ensure that the test is the same at each epoch.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "slen", ",", "bs", "=", "x", ".", "size", "(", ")", "\n", "\n", "# words to predict - be sure there is at least one word per sentence", "\n", "to_predict", "=", "rng", ".", "rand", "(", "slen", ",", "bs", ")", "<=", "params", ".", "word_pred", "\n", "to_predict", "[", "0", "]", "=", "0", "\n", "for", "i", "in", "range", "(", "bs", ")", ":", "\n", "            ", "to_predict", "[", "lengths", "[", "i", "]", "-", "1", ":", ",", "i", "]", "=", "0", "\n", "if", "not", "np", ".", "any", "(", "to_predict", "[", ":", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", ")", ":", "\n", "                ", "v", "=", "rng", ".", "randint", "(", "1", ",", "lengths", "[", "i", "]", "-", "1", ")", "\n", "to_predict", "[", "v", ",", "i", "]", "=", "1", "\n", "", "", "pred_mask", "=", "torch", ".", "from_numpy", "(", "to_predict", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n", "# generate possible targets / update x input", "\n", "_x_real", "=", "x", "[", "pred_mask", "]", "\n", "_x_mask", "=", "_x_real", ".", "clone", "(", ")", ".", "fill_", "(", "params", ".", "mask_index", ")", "\n", "x", "=", "x", ".", "masked_scatter", "(", "pred_mask", ",", "_x_mask", ")", "\n", "\n", "assert", "0", "<=", "x", ".", "min", "(", ")", "<=", "x", ".", "max", "(", ")", "<", "params", ".", "n_words", "\n", "assert", "x", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "assert", "pred_mask", ".", "size", "(", ")", "==", "(", "slen", ",", "bs", ")", "\n", "\n", "return", "x", ",", "_x_real", ",", "pred_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.run_all_evals": [[236, 275], ["collections.OrderedDict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "set", "evaluator.Evaluator.evaluate_clm", "evaluator.Evaluator.evaluate_mlm", "evaluator.Evaluator.evaluate_mt", "len", "numpy.mean", "numpy.mean", "len", "numpy.mean", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_clm", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_mlm", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.evaluate_mt"], ["", "def", "run_all_evals", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "\"\"\"\n        Run all evaluations.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", ",", "'iter'", ":", "trainer", ".", "n_total_iter", "}", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "            ", "for", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "# causal prediction task (evaluate perplexity and accuracy)", "\n", "                ", "for", "lang1", ",", "lang2", "in", "params", ".", "clm_steps", ":", "\n", "                    ", "self", ".", "evaluate_clm", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", "\n", "\n", "# prediction task (evaluate perplexity and accuracy)", "\n", "", "for", "lang1", ",", "lang2", "in", "params", ".", "mlm_steps", ":", "\n", "                    ", "self", ".", "evaluate_mlm", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", "\n", "\n", "# machine translation task (evaluate perplexity and accuracy)", "\n", "", "for", "lang1", ",", "lang2", "in", "set", "(", "params", ".", "mt_steps", "+", "[", "(", "l2", ",", "l3", ")", "for", "_", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", ":", "\n", "                    ", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "self", ".", "evaluate_mt", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ")", "\n", "\n", "# report average metrics per language", "\n", "", "_clm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "clm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_clm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_clm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "scores", "[", "'%s_clm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "", "_mlm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "mlm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_mlm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_mlm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "scores", "[", "'%s_mlm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "\n", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_clm": [[276, 349], ["model.eval", "evaluator.Evaluator.get_iterator", "logger.info", "numpy.exp", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x[].masked_select", "utils.to_cuda", "model", "model", "x[].masked_select.size", "all_mem_att.items", "utils.concat_batches", "lengths.max", "pred_mask.sum().item", "x[].masked_select.size", "loss.item", "len", "evaluator.eval_memory_usage", "x.clone().fill_", "all_mem_att[].append", "pred_mask.sum", "x.clone", "word_scores.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_memory_usage"], ["", "def", "evaluate_clm", "(", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate perplexity and next word prediction accuracy.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "data_set", "in", "[", "'valid'", ",", "'test'", "]", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "or", "lang2", "is", "None", "\n", "\n", "model", "=", "self", ".", "model", "if", "params", ".", "encoder_only", "else", "self", ".", "decoder", "\n", "model", ".", "eval", "(", ")", "\n", "model", "=", "model", ".", "module", "if", "params", ".", "multi_gpu", "else", "model", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "if", "lang2", "is", "not", "None", "else", "None", "\n", "l1l2", "=", "lang1", "if", "lang2", "is", "None", "else", "\"{}-{}\"", ".", "format", "(", "lang1", ",", "lang2", ")", "\n", "\n", "n_words", "=", "0", "\n", "xe_loss", "=", "0", "\n", "n_valid", "=", "0", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "", "for", "batch", "in", "self", ".", "get_iterator", "(", "data_set", ",", "lang1", ",", "lang2", ",", "stream", "=", "(", "lang2", "is", "None", ")", ")", ":", "\n", "\n", "# batch", "\n", "            ", "if", "lang2", "is", "None", ":", "\n", "                ", "x", ",", "lengths", "=", "batch", "\n", "positions", "=", "None", "\n", "langs", "=", "x", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "if", "params", ".", "n_langs", ">", "1", "else", "None", "\n", "", "else", ":", "\n", "                ", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", "=", "batch", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "sent1", ",", "len1", ",", "lang1_id", ",", "sent2", ",", "len2", ",", "lang2_id", ",", "\n", "params", ".", "pad_index", ",", "params", ".", "eos_index", ",", "reset_positions", "=", "True", ")", "\n", "\n", "# words to predict", "\n", "", "alen", "=", "torch", ".", "arange", "(", "lengths", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "lengths", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "lengths", "[", "None", "]", "-", "1", "\n", "y", "=", "x", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "pred_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "y", ".", "size", "(", "0", ")", "\n", "\n", "# cuda", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "pred_mask", ",", "y", "=", "to_cuda", "(", "x", ",", "lengths", ",", "positions", ",", "langs", ",", "pred_mask", ",", "y", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "True", ")", "\n", "word_scores", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "True", ")", "\n", "\n", "# update stats", "\n", "n_words", "+=", "y", ".", "size", "(", "0", ")", "\n", "xe_loss", "+=", "loss", ".", "item", "(", ")", "*", "len", "(", "y", ")", "\n", "n_valid", "+=", "(", "word_scores", ".", "max", "(", "1", ")", "[", "1", "]", "==", "y", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "eval_memory", ":", "\n", "                ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                    ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "# log", "\n", "", "", "", "logger", ".", "info", "(", "\"Found %i words in %s. %i were predicted correctly.\"", "%", "(", "n_words", ",", "data_set", ",", "n_valid", ")", ")", "\n", "\n", "# compute perplexity and prediction accuracy", "\n", "ppl_name", "=", "'%s_%s_clm_ppl'", "%", "(", "data_set", ",", "l1l2", ")", "\n", "acc_name", "=", "'%s_%s_clm_acc'", "%", "(", "data_set", ",", "l1l2", ")", "\n", "scores", "[", "ppl_name", "]", "=", "np", ".", "exp", "(", "xe_loss", "/", "n_words", ")", "\n", "scores", "[", "acc_name", "]", "=", "100.", "*", "n_valid", "/", "n_words", "\n", "\n", "# compute memory usage", "\n", "if", "eval_memory", ":", "\n", "            ", "for", "mem_name", ",", "mem_att", "in", "all_mem_att", ".", "items", "(", ")", ":", "\n", "                ", "eval_memory_usage", "(", "scores", ",", "'%s_%s_%s'", "%", "(", "data_set", ",", "l1l2", ",", "mem_name", ")", ",", "mem_att", ",", "params", ".", "mem_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_mlm": [[350, 419], ["model.eval", "numpy.random.RandomState", "evaluator.Evaluator.get_iterator", "evaluator.Evaluator.mask_out", "utils.to_cuda", "model", "model", "len", "numpy.exp", "all_mem_att.items", "utils.concat_batches", "loss.item", "len", "evaluator.eval_memory_usage", "x.clone().fill_", "all_mem_att[].append", "x.clone", "word_scores.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.mask_out", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.concat_batches", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_memory_usage"], ["", "", "", "def", "evaluate_mlm", "(", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate perplexity and next word prediction accuracy.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "data_set", "in", "[", "'valid'", ",", "'test'", "]", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "or", "lang2", "is", "None", "\n", "\n", "model", "=", "self", ".", "model", "if", "params", ".", "encoder_only", "else", "self", ".", "encoder", "\n", "model", ".", "eval", "(", ")", "\n", "model", "=", "model", ".", "module", "if", "params", ".", "multi_gpu", "else", "model", "\n", "\n", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "0", ")", "\n", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "if", "lang2", "is", "not", "None", "else", "None", "\n", "l1l2", "=", "lang1", "if", "lang2", "is", "None", "else", "\"{}_{}\"", ".", "format", "(", "lang1", ",", "lang2", ")", "\n", "\n", "n_words", "=", "0", "\n", "xe_loss", "=", "0", "\n", "n_valid", "=", "0", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "", "for", "batch", "in", "self", ".", "get_iterator", "(", "data_set", ",", "lang1", ",", "lang2", ",", "stream", "=", "(", "lang2", "is", "None", ")", ")", ":", "\n", "\n", "# batch", "\n", "            ", "if", "lang2", "is", "None", ":", "\n", "                ", "x", ",", "lengths", "=", "batch", "\n", "positions", "=", "None", "\n", "langs", "=", "x", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "if", "params", ".", "n_langs", ">", "1", "else", "None", "\n", "", "else", ":", "\n", "                ", "(", "sent1", ",", "len1", ")", ",", "(", "sent2", ",", "len2", ")", "=", "batch", "\n", "x", ",", "lengths", ",", "positions", ",", "langs", "=", "concat_batches", "(", "sent1", ",", "len1", ",", "lang1_id", ",", "sent2", ",", "len2", ",", "lang2_id", ",", "\n", "params", ".", "pad_index", ",", "params", ".", "eos_index", ",", "reset_positions", "=", "True", ")", "\n", "\n", "# words to predict", "\n", "", "x", ",", "y", ",", "pred_mask", "=", "self", ".", "mask_out", "(", "x", ",", "lengths", ",", "rng", ")", "\n", "\n", "# cuda", "\n", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", "=", "to_cuda", "(", "x", ",", "y", ",", "pred_mask", ",", "lengths", ",", "positions", ",", "langs", ")", "\n", "\n", "# forward / loss", "\n", "tensor", "=", "model", "(", "'fwd'", ",", "x", "=", "x", ",", "lengths", "=", "lengths", ",", "positions", "=", "positions", ",", "langs", "=", "langs", ",", "causal", "=", "False", ")", "\n", "word_scores", ",", "loss", "=", "model", "(", "'predict'", ",", "tensor", "=", "tensor", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "True", ")", "\n", "\n", "# update stats", "\n", "n_words", "+=", "len", "(", "y", ")", "\n", "xe_loss", "+=", "loss", ".", "item", "(", ")", "*", "len", "(", "y", ")", "\n", "n_valid", "+=", "(", "word_scores", ".", "max", "(", "1", ")", "[", "1", "]", "==", "y", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "eval_memory", ":", "\n", "                ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                    ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "# compute perplexity and prediction accuracy", "\n", "", "", "", "ppl_name", "=", "'%s_%s_mlm_ppl'", "%", "(", "data_set", ",", "l1l2", ")", "\n", "acc_name", "=", "'%s_%s_mlm_acc'", "%", "(", "data_set", ",", "l1l2", ")", "\n", "scores", "[", "ppl_name", "]", "=", "np", ".", "exp", "(", "xe_loss", "/", "n_words", ")", "if", "n_words", ">", "0", "else", "1e9", "\n", "scores", "[", "acc_name", "]", "=", "100.", "*", "n_valid", "/", "n_words", "if", "n_words", ">", "0", "else", "0.", "\n", "\n", "# compute memory usage", "\n", "if", "eval_memory", ":", "\n", "            ", "for", "mem_name", ",", "mem_att", "in", "all_mem_att", ".", "items", "(", ")", ":", "\n", "                ", "eval_memory_usage", "(", "scores", ",", "'%s_%s_%s'", "%", "(", "data_set", ",", "l1l2", ",", "mem_name", ")", ",", "mem_att", ",", "params", ".", "mem_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.SingleEvaluator.__init__": [[423, 429], ["evaluator.Evaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "data", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Build language model evaluator.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "trainer", ",", "data", ",", "params", ")", "\n", "self", ".", "model", "=", "trainer", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.__init__": [[433, 440], ["evaluator.Evaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "data", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Build encoder / decoder evaluator.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "trainer", ",", "data", ",", "params", ")", "\n", "self", ".", "encoder", "=", "trainer", ".", "encoder", "\n", "self", ".", "decoder", "=", "trainer", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.get_iterator": [[441, 493], ["len", "set", "set", "[].get_iterator", "len", "[].get_iterator", "[].get_iterator"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_iterator", "(", "\n", "self", ",", "data_set", ",", "lang1", ",", "lang2", "=", "None", ",", "stream", "=", "False", ",", "allow_train_data", "=", "False", ",", "descending", "=", "False", ",", "\n", "force_mono", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Create a new iterator for a dataset.\n        \"\"\"", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "allow_train_data", "\n", "assert", "lang1", "in", "self", ".", "params", ".", "langs", "\n", "assert", "lang2", "is", "None", "or", "lang2", "in", "self", ".", "params", ".", "langs", "\n", "assert", "stream", "is", "False", "or", "lang2", "is", "None", "\n", "\n", "# hacks to reduce evaluation time when using many languages", "\n", "if", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "30", ":", "\n", "            ", "eval_lgs", "=", "set", "(", "\n", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", ",", "\"ab\"", ",", "\"ay\"", ",", "\n", "\"bug\"", ",", "\"ha\"", ",", "\"ko\"", ",", "\"ln\"", ",", "\"min\"", ",", "\"nds\"", ",", "\"pap\"", ",", "\"pt\"", ",", "\"tg\"", ",", "\"to\"", ",", "\"udm\"", ",", "\"uk\"", ",", "\"zh_classical\"", "]", ")", "\n", "eval_lgs", "=", "set", "(", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", "]", ")", "\n", "subsample", "=", "10", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "5", "\n", "n_sentences", "=", "600", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "1500", "\n", "", "elif", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "5", ":", "\n", "            ", "subsample", "=", "10", "if", "data_set", "==", "'test'", "else", "5", "\n", "n_sentences", "=", "300", "if", "data_set", "==", "'test'", "else", "1500", "\n", "", "else", ":", "\n", "# n_sentences = -1 if data_set == 'valid' else 100", "\n", "            ", "n_sentences", "=", "-", "1", "\n", "subsample", "=", "1", "\n", "\n", "# FIXME: forcefully get monolingual data despite mt steps", "\n", "\n", "", "if", "lang2", "is", "None", "or", "force_mono", ":", "\n", "            ", "if", "stream", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono_stream'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "shuffle", "=", "False", ",", "subsample", "=", "subsample", ")", "\n", "", "else", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", ",", "\n", "infer_train", "=", "True", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "stream", "is", "False", "\n", "_lang1", ",", "_lang2", "=", "(", "lang1", ",", "lang2", ")", "if", "lang1", "<", "lang2", "else", "(", "lang2", ",", "lang1", ")", "\n", "iterator", "=", "self", ".", "data", "[", "'para'", "]", "[", "(", "_lang1", ",", "_lang2", ")", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", "\n", ")", "\n", "\n", "", "for", "batch", "in", "iterator", ":", "\n", "            ", "yield", "batch", "if", "force_mono", "or", "lang2", "is", "None", "or", "lang1", "<", "lang2", "else", "batch", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.infer_train": [[494, 512], ["collections.OrderedDict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "set", "logger.info", "getattr", "evaluator.EncDecEvaluator.infer_to_pair_files_multibeam", "evaluator.EncDecEvaluator.infer_to_pair_files"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.infer_to_pair_files_multibeam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.infer_to_pair_files"], ["", "", "def", "infer_train", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", "}", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_set", "=", "'train'", "\n", "# for lang1, lang2 in set(params.mt_steps + [(l2, l3) for _, l2, l3 in params.bt_steps]):", "\n", "for", "lang1", ",", "lang2", "in", "set", "(", "params", ".", "mt_steps", "+", "[", "(", "l1", ",", "l2", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", ":", "\n", "                ", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "logger", ".", "info", "(", "'Perform 1BT: {} -> {} '", ".", "format", "(", "lang1", ",", "lang2", ")", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "if", "nbest", "is", "not", "None", "and", "nbest", ">", "0", ":", "\n", "                    ", "self", ".", "infer_to_pair_files_multibeam", "(", "\n", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "infer_train", "=", "True", ",", "force_mono", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "infer_to_pair_files", "(", "\n", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "infer_train", "=", "True", ",", "force_mono", "=", "True", ")", "\n", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.infer_to_pair_files": [[513, 637], ["getattr", "getattr", "logger.info", "evaluator.EncDecEvaluator.encoder.eval", "evaluator.EncDecEvaluator.decoder.eval", "os.makedirs", "os.path.join", "os.path.join", "logger.info", "logger.info", "utils.restore_segmentation", "utils.restore_segmentation", "logger.info", "open", "open", "enumerate", "evaluator.EncDecEvaluator.get_iterator", "x1.clone().fill_", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "int", "evaluator.convert_to_text", "evaluator.convert_to_text", "hypothesis.extend", "source.extend", "zip", "logger.info", "enc1.transpose.transpose.half", "decoder.generate", "decoder.generate_beam", "fh.write", "fs.write", "x1.clone", "all_mem_att[].append", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam"], ["", "def", "infer_to_pair_files", "(", "\n", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "infer_train", "=", "False", ",", "\n", "force_mono", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "infer_name", "=", "params", ".", "infer_name", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "assert", "infer_name", "!=", "''", "\n", "logger", ".", "info", "(", "'Order descending: {}, nbest={}'", ".", "format", "(", "descending", ",", "nbest", ")", ")", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "n_words", "=", "0", "\n", "xe_loss", "=", "0", "\n", "n_valid", "=", "0", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "# if eval_bleu:", "\n", "", "hypothesis", "=", "[", "]", "\n", "source", "=", "[", "]", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "if", "params", ".", "local_rank", ">", "-", "1", ":", "\n", "            ", "infer_name", "=", "\"{}.rank{}\"", ".", "format", "(", "infer_name", ",", "params", ".", "local_rank", ")", "\n", "\n", "", "src_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.out.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang1", "\n", ")", "\n", "hyp_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.out.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang2", "\n", ")", "\n", "os", ".", "makedirs", "(", "params", ".", "hyp_path", ",", "exist_ok", "=", "True", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "src_name", ")", "\n", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp_name", ")", "\n", "logger", ".", "info", "(", "'Write src to: {}'", ".", "format", "(", "src_path", ")", ")", "\n", "logger", ".", "info", "(", "'Write hyp to: {}'", ".", "format", "(", "hyp_path", ")", ")", "\n", "\n", "with", "open", "(", "hyp_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fh", ":", "\n", "            ", "with", "open", "(", "src_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fs", ":", "\n", "\n", "                ", "for", "index", ",", "batch", "in", "enumerate", "(", "\n", "self", ".", "get_iterator", "(", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ",", "\n", "force_mono", "=", "force_mono", ")", ")", ":", "\n", "\n", "                    ", "if", "index", "%", "200", "==", "0", ":", "\n", "                        ", "logger", ".", "info", "(", "'| Generate Index {}'", ".", "format", "(", "index", ")", ")", "\n", "\n", "# generate batch", "\n", "# (x1, len1), (x2, len2) = batch", "\n", "# (x1, len1), (_, __) = batch", "\n", "", "x1", ",", "len1", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "# langs2 = x2.clone().fill_(lang2_id)", "\n", "\n", "# target words to predict", "\n", "# alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)", "\n", "# pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word", "\n", "# y = x2[1:].masked_select(pred_mask[:-1])", "\n", "# assert len(y) == (len2 - 1).sum().item()", "\n", "\n", "# cuda", "\n", "# x1, len1, langs1, x2, len2, langs2, y = to_cuda(x1, len1, langs1, x2, len2, langs2, y)", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "\n", "# decode target sentence", "\n", "# dec2 = decoder('fwd', x=x2, lengths=len2, langs=langs2, causal=True, src_enc=enc1, src_len=len1)", "\n", "\n", "# loss", "\n", "# word_scores, loss = decoder('predict', tensor=dec2, pred_mask=pred_mask, y=y, get_scores=True)", "\n", "\n", "# update stats", "\n", "# n_words += y.size(0)", "\n", "# xe_loss += loss.item() * len(y)", "\n", "# n_valid += (word_scores.max(1)[1] == y).sum().item()", "\n", "if", "eval_memory", ":", "\n", "                        ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                            ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "# generate translation - translate / convert to text", "\n", "# if eval_bleu:", "\n", "", "", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                        ", "generated", ",", "lengths", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                        ", "generated", ",", "lengths", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "\n", "# nbest=nbest", "\n", ")", "\n", "", "hyp_texts", "=", "convert_to_text", "(", "generated", ",", "lengths", ",", "self", ".", "dico", ",", "params", ")", "\n", "src_texts", "=", "convert_to_text", "(", "x1", ",", "len1", ",", "self", ".", "dico", ",", "params", ")", "\n", "hypothesis", ".", "extend", "(", "hyp_texts", ")", "\n", "source", ".", "extend", "(", "src_texts", ")", "\n", "\n", "for", "h", ",", "s", "in", "zip", "(", "hyp_texts", ",", "src_texts", ")", ":", "\n", "                        ", "fh", ".", "write", "(", "'{}\\n'", ".", "format", "(", "h", ")", ")", "\n", "fs", ".", "write", "(", "'{}\\n'", ".", "format", "(", "s", ")", ")", "\n", "\n", "", "", "", "", "restore_segmentation", "(", "hyp_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "src_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n", "logger", ".", "info", "(", "\"FINISH GENERATION\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.infer_to_pair_files_multibeam": [[638, 747], ["getattr", "getattr", "logger.info", "evaluator.EncDecEvaluator.encoder.eval", "evaluator.EncDecEvaluator.decoder.eval", "os.makedirs", "os.path.join", "logger.info", "range", "open", "range", "enumerate", "open.close", "utils.restore_segmentation", "logger.info", "os.path.join", "logger.info", "hyp_paths.append", "hyp_fs.append", "evaluator.EncDecEvaluator.get_iterator", "x1.clone().fill_", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "int", "decoder.generate_beam", "evaluator.convert_to_text", "range", "f.close", "utils.restore_segmentation", "open", "logger.info", "enc1.transpose.transpose.half", "open.write", "evaluator.convert_to_text", "x1.clone", "all_mem_att[].append", "len", "len", "hyp_fs[].write", "len1.max().item", "s.strip", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text"], ["", "def", "infer_to_pair_files_multibeam", "(", "\n", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "infer_train", "=", "False", ",", "\n", "force_mono", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "infer_name", "=", "params", ".", "infer_name", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "assert", "infer_name", "!=", "''", "\n", "logger", ".", "info", "(", "'Order descending: {}, multi-beam nbest={}'", ".", "format", "(", "descending", ",", "nbest", ")", ")", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "# if eval_bleu:", "\n", "", "hypothesis", "=", "[", "]", "\n", "source", "=", "[", "]", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "if", "params", ".", "local_rank", ">", "-", "1", ":", "\n", "            ", "infer_name", "=", "\"{}.rank{}\"", ".", "format", "(", "infer_name", ",", "params", ".", "local_rank", ")", "\n", "\n", "", "src_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.src.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang1", "\n", ")", "\n", "os", ".", "makedirs", "(", "params", ".", "hyp_path", ",", "exist_ok", "=", "True", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "src_name", ")", "\n", "logger", ".", "info", "(", "'Write src to: {}'", ".", "format", "(", "src_path", ")", ")", "\n", "hyp_paths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "nbest", ")", ":", "\n", "            ", "hyp_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.out{}.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "i", ",", "lang2", "\n", ")", "\n", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp_name", ")", "\n", "logger", ".", "info", "(", "'Write hyp to: {}'", ".", "format", "(", "hyp_path", ")", ")", "\n", "hyp_paths", ".", "append", "(", "hyp_path", ")", "\n", "\n", "# with open(hyp_path, 'w', encoding='utf-8') as fh:", "\n", "", "src_f", "=", "open", "(", "src_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "hyp_fs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "nbest", ")", ":", "\n", "            ", "hyp_fs", ".", "append", "(", "open", "(", "hyp_paths", "[", "i", "]", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", ")", "\n", "\n", "", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ",", "force_mono", "=", "force_mono", ")", ")", ":", "\n", "\n", "            ", "if", "index", "%", "200", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'| Generate Index {}'", ".", "format", "(", "index", ")", ")", "\n", "\n", "", "x1", ",", "len1", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "\n", "if", "eval_memory", ":", "\n", "                ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                    ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "", "", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "assert", "params", ".", "beam_size", ">", "1", "\n", "# usually: generated: [len, bs], lengths [bs]", "\n", "# multibeam: generated: [len, nbest, bs], lengths [bs, nbest]", "\n", "generated", ",", "lengths", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "\n", "nbest", "=", "nbest", "\n", ")", "\n", "src_texts", "=", "convert_to_text", "(", "x1", ",", "len1", ",", "self", ".", "dico", ",", "params", ")", "\n", "for", "s", "in", "src_texts", ":", "\n", "                ", "src_f", ".", "write", "(", "'{}\\n'", ".", "format", "(", "s", ")", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "nbest", ")", ":", "\n", "                ", "generated_", ",", "lengths_", "=", "generated", "[", ":", ",", "i", "]", ",", "lengths", "[", ":", ",", "i", "]", "\n", "hyp_texts", "=", "convert_to_text", "(", "generated_", ",", "lengths_", ",", "self", ".", "dico", ",", "params", ",", "nbest", "=", "nbest", ")", "\n", "assert", "len", "(", "src_texts", ")", "==", "len", "(", "hyp_texts", ")", "\n", "for", "s", "in", "hyp_texts", ":", "\n", "                    ", "assert", "s", ".", "strip", "(", ")", "!=", "''", "\n", "hyp_fs", "[", "i", "]", ".", "write", "(", "'{}\\n'", ".", "format", "(", "s", ")", ")", "\n", "\n", "", "", "", "src_f", ".", "close", "(", ")", "\n", "for", "f", "in", "hyp_fs", ":", "\n", "            ", "f", ".", "close", "(", ")", "\n", "\n", "", "for", "p", "in", "hyp_paths", ":", "\n", "            ", "restore_segmentation", "(", "p", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "", "restore_segmentation", "(", "src_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n", "logger", ".", "info", "(", "\"FINISH GENERATION\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.evaluate_mt": [[748, 919], ["getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "logger.info", "evaluator.EncDecEvaluator.encoder.eval", "evaluator.EncDecEvaluator.decoder.eval", "enumerate", "numpy.exp", "evaluator.EncDecEvaluator.get_iterator", "x1.clone().fill_", "x2.clone().fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x2[].masked_select", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "decoder", "decoder", "x2[].masked_select.size", "all_mem_att.items", "os.path.join", "utils.restore_segmentation", "evaluator.eval_moses_bleu", "logger.info", "len2.max", "len", "enc1.transpose.transpose.half", "loss.item", "len", "int", "hypothesis.extend", "evaluator.eval_memory_usage", "open", "f.write", "x1.clone", "x2.clone", "all_mem_att[].append", "decoder.generate", "evaluator.convert_to_text", "decoder.generate_stochastic_beam", "len1.max().item", "decoder.generate_beam_efficient", "decoder.generate_beam", "word_scores.max", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_moses_bleu", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_memory_usage", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.TransformerModel.generate_stochastic_beam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.TransformerModel.generate_beam_efficient", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam"], ["", "def", "evaluate_mt", "(", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "infer_train", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate perplexity and next word prediction accuracy.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "# if not (data_set == 'valid' and lang1 == 'de'):", "\n", "#     return", "\n", "\n", "fast_beam", "=", "getattr", "(", "params", ",", "'fast_beam'", ",", "False", ")", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "stochastic_beam_infer", "=", "getattr", "(", "params", ",", "'stochastic_beam_infer'", ",", "False", ")", "\n", "stochastic_beam_temp", "=", "getattr", "(", "params", ",", "'stochastic_beam_temp'", ",", "1.0", ")", "\n", "beam_sample_temp", "=", "getattr", "(", "params", ",", "'beam_sample_temp'", ",", "1", ")", "\n", "sampling_topp", "=", "getattr", "(", "params", ",", "'sampling_topp'", ",", "1", ")", "\n", "hyps_size_multiple", "=", "getattr", "(", "params", ",", "'hyps_size_multiple'", ",", "1", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "1", ")", "\n", "logger", ".", "info", "(", "'Descending: {}, fast_beam={}, beam_size: {}, temp: {}'", ".", "format", "(", "\n", "descending", ",", "fast_beam", ",", "params", ".", "beam_size", ",", "beam_sample_temp", "\n", ")", ")", "\n", "# logger.info('hyps_size_multiple: {}'.format(hyps_size_multiple))", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "n_words", "=", "0", "\n", "xe_loss", "=", "0", "\n", "n_valid", "=", "0", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "", "if", "eval_bleu", ":", "\n", "            ", "hypothesis", "=", "[", "]", "\n", "\n", "", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ")", ")", ":", "\n", "\n", "# generate batch", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# target words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ")", "\n", "\n", "# encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "\n", "# decode target sentence", "\n", "dec2", "=", "decoder", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "enc1", ",", "src_len", "=", "len1", ")", "\n", "\n", "# loss", "\n", "word_scores", ",", "loss", "=", "decoder", "(", "'predict'", ",", "tensor", "=", "dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "True", ")", "\n", "\n", "# update stats", "\n", "n_words", "+=", "y", ".", "size", "(", "0", ")", "\n", "xe_loss", "+=", "loss", ".", "item", "(", ")", "*", "len", "(", "y", ")", "\n", "n_valid", "+=", "(", "word_scores", ".", "max", "(", "1", ")", "[", "1", "]", "==", "y", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "eval_memory", ":", "\n", "                ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                    ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "# generate translation - translate / convert to text", "\n", "", "", "if", "eval_bleu", ":", "\n", "                ", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "# if sampling_topp > 0:", "\n", "#     generated, lengths = decoder.generate_nucleus_sampling(", "\n", "#         enc1, len1, lang2_id, max_len=max_len, sampling_topp=sampling_topp)", "\n", "# else:", "\n", "                    ", "generated", ",", "lengths", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                    ", "if", "stochastic_beam_infer", ":", "\n", "                        ", "generated", ",", "lengths", "=", "decoder", ".", "generate_stochastic_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "\n", "sample_temp", "=", "stochastic_beam_temp", "\n", ")", "\n", "", "else", ":", "\n", "# turn on fast_beam by default", "\n", "                        ", "if", "fast_beam", ":", "\n", "# debug = True", "\n", "# debug = False", "\n", "# if debug:", "\n", "#     # generated, lengths = decoder.generate_beam_efficient_topn(", "\n", "#     #     # generated, lengths = decoder.generate_beam_efficient_validate_cpu(", "\n", "#     #     enc1, len1, lang2_id, beam_size=params.beam_size,", "\n", "#     #     length_penalty=params.length_penalty,", "\n", "#     #     early_stopping=params.early_stopping,", "\n", "#     #     max_len=max_len,", "\n", "#     #     sample_topn=100,", "\n", "#     # )", "\n", "#     generated, lengths = decoder.generate_beam_efficient_diverse(", "\n", "#         enc1, len1, lang2_id, beam_size=params.beam_size,", "\n", "#         length_penalty=params.length_penalty,", "\n", "#         early_stopping=params.early_stopping,", "\n", "#         max_len=max_len,", "\n", "#         num_groups=2, diversity_strength=0.5", "\n", "#     )", "\n", "#", "\n", "# else:", "\n", "                            ", "generated", ",", "lengths", "=", "decoder", ".", "generate_beam_efficient", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "\n", ")", "\n", "", "else", ":", "\n", "                            ", "generated", ",", "lengths", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "# generated, lengths = decoder.generate_beam_efficient(", "\n", "#     # generated, lengths, hyps = decoder.generate_beam_efficient_validate_cpu(", "\n", "#     enc1, len1, lang2_id, beam_size=params.beam_size,", "\n", "#     length_penalty=params.length_penalty,", "\n", "#     early_stopping=params.early_stopping,", "\n", "#     max_len=max_len", "\n", "# )", "\n", "\n", "", "", "", "hypothesis", ".", "extend", "(", "convert_to_text", "(", "generated", ",", "lengths", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "\n", "# compute perplexity and prediction accuracy", "\n", "", "", "scores", "[", "'%s_%s-%s_mt_ppl'", "%", "(", "data_set", ",", "lang1", ",", "lang2", ")", "]", "=", "np", ".", "exp", "(", "xe_loss", "/", "n_words", ")", "\n", "scores", "[", "'%s_%s-%s_mt_acc'", "%", "(", "data_set", ",", "lang1", ",", "lang2", ")", "]", "=", "100.", "*", "n_valid", "/", "n_words", "\n", "\n", "# compute memory usage", "\n", "if", "eval_memory", ":", "\n", "            ", "for", "mem_name", ",", "mem_att", "in", "all_mem_att", ".", "items", "(", ")", ":", "\n", "                ", "eval_memory_usage", "(", "scores", ",", "'%s_%s-%s_%s'", "%", "(", "data_set", ",", "lang1", ",", "lang2", ",", "mem_name", ")", ",", "mem_att", ",", "params", ".", "mem_size", ")", "\n", "\n", "# compute BLEU", "\n", "", "", "if", "eval_bleu", ":", "\n", "# hypothesis / reference paths", "\n", "            ", "hyp_name", "=", "'hyp{0}-{1}.{2}-{3}.{4}.txt'", ".", "format", "(", "scores", "[", "'epoch'", "]", ",", "scores", "[", "'iter'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ")", "\n", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp_name", ")", "\n", "ref_path", "=", "params", ".", "ref_paths", "[", "(", "lang1", ",", "lang2", ",", "data_set", ")", "]", "\n", "\n", "# export sentences to hypothesis file / restore BPE segmentation", "\n", "with", "open", "(", "hyp_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "hypothesis", ")", "+", "'\\n'", ")", "\n", "", "restore_segmentation", "(", "hyp_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n", "# evaluate BLEU score", "\n", "bleu", "=", "eval_moses_bleu", "(", "ref_path", ",", "hyp_path", ")", "\n", "logger", ".", "info", "(", "\"BLEU %s %s : %f\"", "%", "(", "hyp_path", ",", "ref_path", ",", "bleu", ")", ")", "\n", "scores", "[", "'%s_%s-%s_mt_bleu'", "%", "(", "data_set", ",", "lang1", ",", "lang2", ")", "]", "=", "bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.eval_evolve_train": [[920, 936], ["collections.OrderedDict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "set", "logger.info", "evaluator.EncDecEvaluator.eval_evolve_disruptive_event", "logger.info", "evaluator.EncDecEvaluator.eval_evolve_divergence_event"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.eval_evolve_disruptive_event", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.eval_evolve_divergence_event"], ["", "", "def", "eval_evolve_train", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", "}", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_set", "=", "'train'", "\n", "for", "lang1", ",", "lang2", "in", "set", "(", "params", ".", "mt_steps", "+", "[", "(", "l1", ",", "l2", ")", "for", "l1", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", ":", "\n", "                ", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "if", "params", ".", "eval_disruptive_event", ":", "\n", "                    ", "logger", ".", "info", "(", "'Perform Eval disruptive event 1BT: {} -> {} '", ".", "format", "(", "lang1", ",", "lang2", ")", ")", "\n", "self", ".", "eval_evolve_disruptive_event", "(", "\n", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "eval_train", "=", "True", ",", "force_mono", "=", "True", ")", "\n", "", "elif", "params", ".", "eval_divergence", ":", "\n", "                    ", "logger", ".", "info", "(", "'Perform Eval divergence event: {} -> {} '", ".", "format", "(", "lang1", ",", "lang2", ")", ")", "\n", "self", ".", "eval_evolve_divergence_event", "(", "\n", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "eval_train", "=", "True", ",", "force_mono", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_beam_from_prefix": [[937, 961], ["getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "int", "model.transformer.generate_beam_from_prefix", "min", "generated.size", "lengths.min"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.model.transformer.generate_beam_from_prefix"], ["", "", "", "", "def", "_compute_beam_from_prefix", "(", "self", ",", "generated", ",", "lengths", ",", "decoder", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "max_len", ",", "nbest", "=", "None", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "# nbest = getattr(params, 'nbest', None)", "\n", "\n", "limit_beam_size", "=", "getattr", "(", "params", ",", "'limit_beam_size'", ",", "1", ")", "\n", "limit_percent", "=", "getattr", "(", "params", ",", "'limit_percent'", ",", "0.5", ")", "\n", "limit_min_len", "=", "getattr", "(", "params", ",", "'limit_min_len'", ",", "10", ")", "\n", "limit_nbest", "=", "getattr", "(", "params", ",", "'limit_nbest'", ",", "None", ")", "\n", "limit_eval_default", "=", "getattr", "(", "params", ",", "'limit_eval_default'", ",", "False", ")", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "filter_len_limit", "=", "int", "(", "generated", ".", "size", "(", "0", ")", "*", "limit_percent", ")", "\n", "filter_len", "=", "min", "(", "lengths", ".", "min", "(", ")", ",", "filter_len_limit", ")", "-", "1", "\n", "assert", "filter_len", ">", "1", ",", "'len {}'", ".", "format", "(", "filter_len", ")", "\n", "fil_generated", "=", "generated", "[", ":", "filter_len", "]", "\n", "\n", "# generated, lengths is either ([t, b], [b]) or ([t, n, b], [b, n])", "\n", "\n", "generated", ",", "lengths", "=", "generate_beam_from_prefix", "(", "\n", "decoder", ",", "fil_generated", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "beam", ",", "lenpen", ",", "\n", "params", ".", "early_stopping", ",", "max_len", ",", "nbest", "\n", ")", "\n", "return", "generated", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_score": [[962, 992], ["getattr", "getattr", "x1_p.size", "generated.size", "x1_p.unsqueeze().expand().contiguous().view", "len1_p.unsqueeze().expand().contiguous().view", "x1_p.unsqueeze().expand().contiguous().view.clone().fill_", "generated.transpose().contiguous().view", "lengths.view", "generated.transpose().contiguous().view.clone().fill_", "encoder", "eval_enc1.transpose.transpose.transpose", "decoder", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "eval_out[].masked_select", "eval_dec2[].view", "decoder.pred_layer.proj().view", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy.detach", "eval_enc1.transpose.transpose.half", "lengths.view.max", "len", "x1_p.unsqueeze().expand().contiguous", "len1_p.unsqueeze().expand().contiguous", "x1_p.unsqueeze().expand().contiguous().view.clone", "generated.transpose().contiguous", "generated.transpose().contiguous().view.clone", "decoder.pred_layer.proj", "x1_p.unsqueeze().expand", "len1_p.unsqueeze().expand", "generated.transpose", "eval_pred_mask.unsqueeze().expand_as", "x1_p.unsqueeze", "len1_p.unsqueeze", "eval_pred_mask.unsqueeze"], "methods", ["None"], ["", "def", "_compute_score", "(", "self", ",", "encoder", ",", "decoder", ",", "generated", ",", "lengths", ",", "x1_p", ",", "len1_p", ",", "lang1_id", ",", "lang2_id", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "\n", "src_len", ",", "bsz", "=", "x1_p", ".", "size", "(", ")", "\n", "_tgt_len", ",", "_nb", ",", "_bsz", "=", "generated", ".", "size", "(", ")", "\n", "assert", "bsz", "==", "_bsz", ",", "'{} != {}'", ".", "format", "(", "bsz", ",", "_bsz", ")", "\n", "eval_x1_p", "=", "x1_p", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "src_len", ",", "bsz", ",", "nbest", ")", ".", "contiguous", "(", ")", ".", "view", "(", "src_len", ",", "bsz", "*", "nbest", ")", "\n", "eval_len1_p", "=", "len1_p", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bsz", ",", "nbest", ")", ".", "contiguous", "(", ")", ".", "view", "(", "bsz", "*", "nbest", ")", "\n", "eval_langs1_p", "=", "eval_x1_p", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "eval_out", "=", "generated", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "_tgt_len", ",", "bsz", "*", "nbest", ")", "\n", "eval_out_len", "=", "lengths", ".", "view", "(", "bsz", "*", "nbest", ")", "\n", "eval_out_langs", "=", "eval_out", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "eval_enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "eval_x1_p", ",", "lengths", "=", "eval_len1_p", ",", "langs", "=", "eval_langs1_p", ",", "causal", "=", "False", ")", "\n", "eval_enc1", "=", "eval_enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "eval_enc1", "=", "eval_enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "eval_enc1", "\n", "\n", "eval_dec2", "=", "decoder", "(", "'fwd'", ",", "x", "=", "eval_out", ",", "lengths", "=", "eval_out_len", ",", "langs", "=", "eval_out_langs", ",", "causal", "=", "True", ",", "\n", "src_enc", "=", "eval_enc1", ",", "src_len", "=", "eval_len1_p", ")", "\n", "eval_alen", "=", "torch", ".", "arange", "(", "eval_out_len", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "eval_out_len", ".", "device", ")", "\n", "eval_pred_mask", "=", "eval_alen", "[", ":", ",", "None", "]", "<", "eval_out_len", "[", "None", "]", "-", "1", "\n", "eval_y", "=", "eval_out", "[", "1", ":", "]", ".", "masked_select", "(", "eval_pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "eval_y", ")", "==", "(", "eval_out_len", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "masked_tensor", "=", "eval_dec2", "[", "eval_pred_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "eval_dec2", ")", "]", ".", "view", "(", "-", "1", ",", "decoder", ".", "dim", ")", "\n", "scores", "=", "decoder", ".", "pred_layer", ".", "proj", "(", "masked_tensor", ")", ".", "view", "(", "-", "1", ",", "decoder", ".", "pred_layer", ".", "n_words", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "eval_y", ",", "reduction", "=", "'none'", ")", "\n", "return", "loss", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._translate_evolve_back": [[993, 1017], ["getattr", "x1.clone().fill_", "encoder", "enc1.transpose.transpose.transpose", "int", "enc1.transpose.transpose.half", "decoder.generate", "decoder.generate_beam", "x1.clone", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam"], ["", "def", "_translate_evolve_back", "(", "self", ",", "encoder", ",", "decoder", ",", "x1", ",", "len1", ",", "lang1_id", ",", "lang2_id", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "# nbest = getattr(params, 'nbest', None)", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "\n", "# generate translation - translate / convert to text", "\n", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "            ", "generated", ",", "lengths", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "            ", "generated", ",", "lengths", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "", "return", "generated", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.eval_evolve_disruptive_event": [[1018, 1206], ["getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "logger.info", "evaluator.EncDecEvaluator.encoder.eval", "evaluator.EncDecEvaluator.decoder.eval", "enumerate", "evaluator.EncDecEvaluator.get_iterator", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "x1.clone().fill_", "x1.clone().fill_.clone", "utils.to_cuda", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "int", "decoder.generate_beam", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.mean", "torch.cat.mean", "torch.cat.mean", "torch.cat.mean", "torch.cat.mean", "torch.cat.mean", "logger.info", "os.path.join", "os.path.join", "os.path.join", "utils.restore_segmentation", "utils.restore_segmentation", "utils.restore_segmentation", "evaluator.eval_moses_bleu", "evaluator.eval_moses_bleu", "logger.info", "logger.info", "logger.info", "len1.min", "x1.size", "x1.size", "x1_p.size", "x1.size", "x1_p.size", "len1.size", "len1_p.size", "len1.size", "len1_p.size", "enc1.transpose.transpose.half", "enc1.transpose.transpose.size", "enc1_p.size", "enc1.transpose.transpose.size", "enc1_p.size", "decoder.generate", "generated_base.size", "enc1.transpose.transpose.size", "generated_base.size", "enc1.transpose.transpose.size", "generated_mbeam.size", "enc1.transpose.transpose.size", "generated_mbeam.size", "enc1.transpose.transpose.size", "open", "f.write", "open", "f.write", "open", "f.write", "len", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "logger.info", "len", "logger.info", "x1.clone", "lengths_base.min", "lengths_mbeam.min", "evaluator.EncDecEvaluator._compute_beam_from_prefix", "evaluator.EncDecEvaluator._compute_beam_from_prefix", "evaluator.EncDecEvaluator._compute_score", "evaluator.EncDecEvaluator._compute_score", "eval_scores_base.append", "eval_scores_mbeam.append", "evaluator.EncDecEvaluator._compute_beam_from_prefix", "evaluator.EncDecEvaluator._compute_beam_from_prefix", "evaluator.EncDecEvaluator._translate_evolve_back", "evaluator.EncDecEvaluator._translate_evolve_back", "eval_evol_refs.extend", "eval_evol_base_hyps.extend", "eval_evol_mbeam_hyps.extend", "torch.cat.mean.size", "torch.cat.mean.size", "len1.max().item", "evaluator.EncDecEvaluator.detach", "evaluator.EncDecEvaluator.detach", "evaluator.convert_to_text", "evaluator.convert_to_text", "evaluator.convert_to_text", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_moses_bleu", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_moses_bleu", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_beam_from_prefix", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_beam_from_prefix", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_score", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_score", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_beam_from_prefix", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._compute_beam_from_prefix", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._translate_evolve_back", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator._translate_evolve_back", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text"], ["", "def", "eval_evolve_disruptive_event", "(", "\n", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "eval_train", "=", "False", ",", "\n", "force_mono", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate evolution by causing disruptive event\n            * out = mt_st(src)\n            * out_dis = mt_st(src', prefix=out[:cut_off]) where src' is randomly sampled else \"where, but how?\n                * try select src' by shuffling the batches\n            * evaluate_score(out_dis)\n            * evaluate_BLEU(src', mt_ts(out_dis))\n        Compare the procedures between single vs multiple populations\n        :param scores:\n        :param data_set:\n        :param lang1:\n        :param lang2:\n        :param eval_bleu:\n        :param resolve_segmentation:\n        :param eval_train:\n        :param force_mono:\n        :return:\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "eval_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "\n", "limit_beam_size", "=", "getattr", "(", "params", ",", "'limit_beam_size'", ",", "1", ")", "\n", "limit_percent", "=", "getattr", "(", "params", ",", "'limit_percent'", ",", "0.5", ")", "\n", "limit_min_len", "=", "getattr", "(", "params", ",", "'limit_min_len'", ",", "10", ")", "\n", "limit_nbest", "=", "getattr", "(", "params", ",", "'limit_nbest'", ",", "None", ")", "\n", "limit_eval_default", "=", "getattr", "(", "params", ",", "'limit_eval_default'", ",", "False", ")", "\n", "eval_evolve_train_score", "=", "getattr", "(", "params", ",", "'eval_evolve_train_score'", ",", "True", ")", "\n", "eval_evolve_train_reconstruct", "=", "getattr", "(", "params", ",", "'eval_evolve_train_reconstruct'", ",", "True", ")", "\n", "assert", "eval_evolve_train_score", "or", "eval_evolve_train_reconstruct", "\n", "\n", "# assert infer_name != ''", "\n", "logger", ".", "info", "(", "'Order descending: {}, multi-beam nbest={}, limit_bs={}, limit_pc={}, limit_minlen={}'", ".", "format", "(", "\n", "descending", ",", "nbest", ",", "limit_beam_size", ",", "limit_percent", ",", "limit_min_len", ")", ")", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "# if eval_bleu:", "\n", "", "hypothesis", "=", "[", "]", "\n", "source", "=", "[", "]", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "\n", "eval_scores_base", "=", "[", "]", "\n", "eval_scores_mbeam", "=", "[", "]", "\n", "\n", "eval_evol_refs", "=", "[", "]", "\n", "eval_evol_base_hyps", "=", "[", "]", "\n", "eval_evol_mbeam_hyps", "=", "[", "]", "\n", "\n", "skip_batches", "=", "0", "\n", "skip_batches_gen", "=", "0", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "eval_train", ",", "descending", "=", "descending", ",", "force_mono", "=", "force_mono", ")", ")", ":", "\n", "            ", "if", "index", "%", "200", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'| Generate Index {}, skip batch {} / {}'", ".", "format", "(", "index", ",", "skip_batches", ",", "skip_batches_gen", ")", ")", "\n", "if", "len", "(", "eval_scores_base", ")", ">", "0", ":", "\n", "                    ", "_cur_avg_score_base", "=", "torch", ".", "cat", "(", "eval_scores_base", ")", ".", "mean", "(", ")", "\n", "_cur_avg_score_mbeam", "=", "torch", ".", "cat", "(", "eval_scores_mbeam", ")", ".", "mean", "(", ")", "\n", "logger", ".", "info", "(", "'| Current evolve score: {} / {}'", ".", "format", "(", "_cur_avg_score_base", ",", "_cur_avg_score_mbeam", ")", ")", "\n", "", "if", "len", "(", "eval_evol_refs", ")", ">", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "'| Current reconstruct len: {}'", ".", "format", "(", "len", "(", "eval_evol_refs", ")", ")", ")", "\n", "\n", "", "", "x1", ",", "len1", "=", "batch", "\n", "\n", "if", "len1", ".", "min", "(", ")", "<", "limit_min_len", ":", "\n", "                ", "skip_batches", "+=", "1", "\n", "continue", "\n", "\n", "", "randomperm", "=", "torch", ".", "randperm", "(", "x1", ".", "size", "(", "1", ")", ")", "\n", "x1_p", ",", "len1_p", "=", "x1", "[", ":", ",", "randomperm", "]", ",", "len1", "[", "randomperm", "]", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs1_p", "=", "langs1", ".", "clone", "(", ")", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "x1_p", ",", "len1_p", ",", "langs1_p", "=", "to_cuda", "(", "x1_p", ",", "len1_p", ",", "langs1_p", ")", "\n", "\n", "assert", "x1", ".", "size", "(", ")", "==", "x1_p", ".", "size", "(", ")", ",", "'{} != {}'", ".", "format", "(", "x1", ".", "size", "(", ")", ",", "x1_p", ".", "size", "(", ")", ")", "\n", "assert", "len1", ".", "size", "(", ")", "==", "len1_p", ".", "size", "(", ")", ",", "'{} != {}'", ".", "format", "(", "len1", ".", "size", "(", ")", ",", "len1_p", ".", "size", "(", ")", ")", "\n", "# encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "enc1_p", "=", "enc1", "[", "randomperm", "]", "\n", "assert", "enc1", ".", "size", "(", ")", "==", "enc1_p", ".", "size", "(", ")", ",", "'{} != {}'", ".", "format", "(", "enc1", ".", "size", "(", ")", ",", "enc1_p", ".", "size", "(", ")", ")", "\n", "\n", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "\n", "generated_mbeam", ",", "lengths_mbeam", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "beam", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", ",", "\n", "nbest", "=", "nbest", "\n", ")", "\n", "if", "limit_beam_size", "==", "1", ":", "\n", "                ", "generated_base", ",", "lengths_base", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                ", "generated_base", ",", "lengths_base", "=", "generated_mbeam", "[", ":", ",", "0", "]", ",", "lengths_mbeam", "[", ":", ",", "0", "]", "\n", "\n", "", "assert", "generated_base", ".", "size", "(", "-", "1", ")", "==", "enc1", ".", "size", "(", "0", ")", ",", "'{} != {}'", ".", "format", "(", "generated_base", ".", "size", "(", ")", ",", "enc1", ".", "size", "(", ")", ")", "\n", "assert", "generated_mbeam", ".", "size", "(", "-", "1", ")", "==", "enc1", ".", "size", "(", "0", ")", ",", "'{} != {}'", ".", "format", "(", "generated_mbeam", ".", "size", "(", ")", ",", "enc1", ".", "size", "(", ")", ")", "\n", "\n", "if", "lengths_base", ".", "min", "(", ")", ">", "limit_min_len", "and", "lengths_mbeam", ".", "min", "(", ")", ">", "limit_min_len", ":", "\n", "# todo: calculate score evaluate_score(generated, x1_p)", "\n", "                ", "if", "eval_evolve_train_score", ":", "\n", "                    ", "generated_base", ",", "lengths_base", "=", "self", ".", "_compute_beam_from_prefix", "(", "\n", "generated_base", ",", "lengths_base", ",", "decoder", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "max_len", ",", "nbest", ")", "\n", "generated_mbeam", ",", "lengths_mbeam", "=", "self", ".", "_compute_beam_from_prefix", "(", "\n", "generated_mbeam", ",", "lengths_mbeam", ",", "decoder", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "max_len", ",", "nbest", ")", "\n", "\n", "loss_base", "=", "self", ".", "_compute_score", "(", "\n", "encoder", ",", "decoder", ",", "generated_base", ",", "lengths_base", ",", "x1_p", ",", "len1_p", ",", "lang1_id", ",", "lang2_id", ")", "\n", "loss_mbeam", "=", "self", ".", "_compute_score", "(", "\n", "encoder", ",", "decoder", ",", "generated_mbeam", ",", "lengths_mbeam", ",", "x1_p", ",", "len1_p", ",", "lang1_id", ",", "lang2_id", ")", "\n", "\n", "eval_scores_base", ".", "append", "(", "loss_base", ".", "detach", "(", ")", ")", "\n", "eval_scores_mbeam", ".", "append", "(", "loss_mbeam", ".", "detach", "(", ")", ")", "\n", "\n", "# todo: calculate evaluate_BLEU(src', mt_ts(out_dis))", "\n", "", "if", "eval_evolve_train_reconstruct", ":", "\n", "                    ", "generated_base", ",", "lengths_base", "=", "self", ".", "_compute_beam_from_prefix", "(", "\n", "generated_base", ",", "lengths_base", ",", "decoder", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "max_len", ",", "nbest", "=", "None", ")", "\n", "generated_mbeam", ",", "lengths_mbeam", "=", "self", ".", "_compute_beam_from_prefix", "(", "\n", "generated_mbeam", ",", "lengths_mbeam", ",", "decoder", ",", "enc1_p", ",", "len1_p", ",", "lang2_id", ",", "max_len", ",", "nbest", "=", "None", ")", "\n", "\n", "# translate generated back to x1_p", "\n", "base_gen", ",", "base_len", "=", "self", ".", "_translate_evolve_back", "(", "\n", "encoder", ",", "decoder", ",", "generated_base", ",", "lengths_base", ",", "lang2_id", ",", "lang1_id", ")", "\n", "mbeam_gen", ",", "mbeam_len", "=", "self", ".", "_translate_evolve_back", "(", "\n", "encoder", ",", "decoder", ",", "generated_mbeam", ",", "lengths_mbeam", ",", "lang2_id", ",", "lang1_id", ")", "\n", "\n", "eval_evol_refs", ".", "extend", "(", "convert_to_text", "(", "x1_p", ",", "len1_p", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "eval_evol_base_hyps", ".", "extend", "(", "convert_to_text", "(", "base_gen", ",", "base_len", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "eval_evol_mbeam_hyps", ".", "extend", "(", "convert_to_text", "(", "mbeam_gen", ",", "mbeam_len", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "skip_batches_gen", "+=", "1", "\n", "# logger.info('lengths: {} / {} / {}'.format(lengths.min(), lengths.max(), lengths.float().mean()))", "\n", "\n", "# reporting", "\n", "", "", "if", "eval_evolve_train_score", ":", "\n", "            ", "evolve_scores_base", "=", "torch", ".", "cat", "(", "eval_scores_base", ")", "\n", "evolve_scores_mbeam", "=", "torch", ".", "cat", "(", "eval_scores_mbeam", ")", "\n", "avg_eval_score_base", "=", "evolve_scores_base", ".", "mean", "(", ")", "\n", "avg_eval_score_mbeam", "=", "evolve_scores_mbeam", ".", "mean", "(", ")", "\n", "logger", ".", "info", "(", "'eval_score: {}/{} base={}, mbeam={}'", ".", "format", "(", "\n", "avg_eval_score_base", ".", "size", "(", ")", ",", "avg_eval_score_mbeam", ".", "size", "(", ")", ",", "avg_eval_score_base", ",", "avg_eval_score_mbeam", ")", ")", "\n", "\n", "", "if", "eval_evolve_train_reconstruct", ":", "\n", "            ", "ref_name", "=", "'eval_evol_disruptive_event.new.ref'", "\n", "base_name", "=", "'eval_evol_disruptive_event.new.base'", "\n", "mbeam_name", "=", "'eval_evol_disruptive_event.new.mbeam'", "\n", "ref_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "ref_name", ")", "\n", "base_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "base_name", ")", "\n", "mbeam_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "mbeam_name", ")", "\n", "\n", "with", "open", "(", "ref_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "eval_evol_refs", ")", "+", "'\\n'", ")", "\n", "", "with", "open", "(", "base_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "eval_evol_base_hyps", ")", "+", "'\\n'", ")", "\n", "", "with", "open", "(", "mbeam_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "eval_evol_mbeam_hyps", ")", "+", "'\\n'", ")", "\n", "\n", "", "restore_segmentation", "(", "ref_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "base_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "mbeam_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "bleu_base", "=", "eval_moses_bleu", "(", "ref_path", ",", "base_path", ")", "\n", "bleu_mbeam", "=", "eval_moses_bleu", "(", "ref_path", ",", "mbeam_path", ")", "\n", "logger", ".", "info", "(", "\"BLEU BASE %s %s : %f\"", "%", "(", "base_path", ",", "ref_path", ",", "bleu_base", ")", ")", "\n", "logger", ".", "info", "(", "\"BLEU MBEAM %s %s : %f\"", "%", "(", "mbeam_path", ",", "ref_path", ",", "bleu_mbeam", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.eval_evolve_divergence_event": [[1207, 1330], ["getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "evaluator.EncDecEvaluator.encoder.eval", "evaluator.EncDecEvaluator.decoder.eval", "_encoder_para.eval", "_decoder_para.eval", "enumerate", "logger.info", "logger.info", "evaluator.EncDecEvaluator.get_iterator", "x1.clone().fill_", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "int", "target_gen.clone().fill_", "evaluator.EncDecEvaluator.eval_evolve_divergence_event.compute_bt_loss"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda"], ["", "", "def", "eval_evolve_divergence_event", "(", "\n", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "\n", "resolve_segmentation", "=", "None", ",", "eval_train", "=", "False", ",", "force_mono", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate evolution by divergence event\n            * Bio analogy: if a population is divided into 2 geographically and isolated, they will evolve into 2 different species that does not fit with each other anymore.\n            * Divide source dataset into S1 and S2, not divide target dataset T\n            * Train UMT1 = train(S1, T)\n            * Train UMT2 = train(S2, T)\n            * Bring S1 offspring and test fitness on S2: P_{UMT2}(S1 | UMT1(S1)) vs P_{UMT1}(S1 | UMT1(S1))\n        Note:\n            encode/decoder is S1, configure the data_generator correctly\n            encoder_para/decoder_para is S2, configure the data_gen correctly\n            e.g:\n                reload_model: 5340, reload_para_model: 5341 => split_evolve_diverge=0\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "eval_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "nbest", "=", "getattr", "(", "params", ",", "'nbest'", ",", "None", ")", "\n", "\n", "limit_beam_size", "=", "getattr", "(", "params", ",", "'limit_beam_size'", ",", "1", ")", "\n", "limit_percent", "=", "getattr", "(", "params", ",", "'limit_percent'", ",", "0.5", ")", "\n", "limit_min_len", "=", "getattr", "(", "params", ",", "'limit_min_len'", ",", "10", ")", "\n", "limit_nbest", "=", "getattr", "(", "params", ",", "'limit_nbest'", ",", "None", ")", "\n", "limit_eval_default", "=", "getattr", "(", "params", ",", "'limit_eval_default'", ",", "False", ")", "\n", "eval_evolve_train_score", "=", "getattr", "(", "params", ",", "'eval_evolve_train_score'", ",", "True", ")", "\n", "eval_evolve_train_reconstruct", "=", "getattr", "(", "params", ",", "'eval_evolve_train_reconstruct'", ",", "True", ")", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "split_evolve_diverge", "=", "self", ".", "params", ".", "split_evolve_diverge", "\n", "split_evolve_diverge_lang", "=", "self", ".", "params", ".", "split_evolve_diverge_lang", "\n", "if", "split_evolve_diverge_lang", "!=", "lang1", ":", "\n", "            ", "logger", ".", "info", "(", "\"Skip process because diver_lange({}) != lang1 ({})\"", ".", "format", "(", "split_evolve_diverge_lang", ",", "lang1", ")", ")", "\n", "return", "\n", "\n", "", "_encoder_para", "=", "self", ".", "trainer", ".", "encoder_para", "\n", "_decoder_para", "=", "self", ".", "trainer", ".", "decoder_para", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "_encoder_para", ".", "eval", "(", ")", "\n", "_decoder_para", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "try", ":", "\n", "            ", "encoder_para", "=", "_encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "_encoder_para", "\n", "decoder_para", "=", "_decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "_decoder_para", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "encoder_para", "=", "_encoder_para", "\n", "decoder_para", "=", "_decoder_para", "\n", "\n", "", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "", "skip_batches", "=", "0", "\n", "skip_batches_gen", "=", "0", "\n", "\n", "def", "compute_bt_loss", "(", "_enc", ",", "_dec", ",", "_x1", ",", "_len1", ",", "_langs1", ",", "_x2", ",", "_len2", ",", "_langs2", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "alen", "=", "torch", ".", "arange", "(", "_len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "_len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "_len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "_x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "_len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "_enc1", "=", "_enc", "(", "'fwd'", ",", "x", "=", "_x1", ",", "lengths", "=", "_len1", ",", "langs", "=", "_langs1", ",", "causal", "=", "False", ")", "\n", "_enc1", "=", "_enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "_enc1", "=", "_enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "_enc1", "\n", "_dec2", "=", "_dec", "(", "'fwd'", ",", "x", "=", "_x2", ",", "lengths", "=", "_len2", ",", "langs", "=", "_langs2", ",", "causal", "=", "True", ",", "src_enc", "=", "_enc1", ",", "src_len", "=", "_len1", ")", "\n", "word_scores", ",", "loss", "=", "decoder", "(", "'predict'", ",", "tensor", "=", "_dec2", ",", "pred_mask", "=", "pred_mask", ",", "y", "=", "y", ",", "get_scores", "=", "True", ")", "\n", "# batc_score = (word_scores.max(1)[1] == y).sum().item()", "\n", "return", "word_scores", ",", "loss", "\n", "\n", "", "", "self_losses", "=", "[", "]", "\n", "cross_losses", "=", "[", "]", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "eval_train", ",", "descending", "=", "descending", ",", "force_mono", "=", "force_mono", ")", ")", ":", "\n", "            ", "if", "index", "%", "200", "==", "0", ":", "\n", "                ", "_self_loss", "=", "0", "if", "len", "(", "self_losses", ")", "==", "0", "else", "torch", ".", "tensor", "(", "self_losses", ")", ".", "mean", "(", ")", "\n", "_cross_loss", "=", "0", "if", "len", "(", "cross_losses", ")", "==", "0", "else", "torch", ".", "tensor", "(", "cross_losses", ")", ".", "mean", "(", ")", "\n", "logger", ".", "info", "(", "'| Generate Index {}, selfloss={}  / crossloss={}'", ".", "format", "(", "index", ",", "_self_loss", ",", "_cross_loss", ")", ")", "\n", "\n", "# S1", "\n", "", "x1", ",", "len1", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# compute UMT1(S1) -> target language", "\n", "#   encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                ", "target_gen", ",", "target_len", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                ", "target_gen", ",", "target_len", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "", "langs2", "=", "target_gen", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# compute P_{UMT1} (S1 | UMT1(S1))", "\n", "self_score", ",", "self_loss", "=", "compute_bt_loss", "(", "encoder", ",", "decoder", ",", "target_gen", ",", "target_len", ",", "langs2", ",", "x1", ",", "len1", ",", "langs1", ")", "\n", "# compute P_{UMT2} (S1 | UMT1(S1))", "\n", "cross_score", ",", "cross_loss", "=", "compute_bt_loss", "(", "encoder_para", ",", "decoder_para", ",", "target_gen", ",", "target_len", ",", "langs2", ",", "x1", ",", "\n", "len1", ",", "langs1", ")", "\n", "\n", "self_losses", ".", "append", "(", "self_loss", ".", "item", "(", ")", ")", "\n", "cross_losses", ".", "append", "(", "cross_loss", ".", "item", "(", ")", ")", "\n", "\n", "", "_self_loss", "=", "0", "if", "len", "(", "self_losses", ")", "==", "0", "else", "torch", ".", "tensor", "(", "self_losses", ")", ".", "mean", "(", ")", "\n", "_cross_loss", "=", "0", "if", "len", "(", "cross_losses", ")", "==", "0", "else", "torch", ".", "tensor", "(", "cross_losses", ")", ".", "mean", "(", ")", "\n", "logger", ".", "info", "(", "'Final report divergence ({}->{}): selfloss={}  / crossloss={}'", ".", "format", "(", "\n", "lang1", ",", "lang2", ",", "_self_loss", ",", "_cross_loss", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.BinaryEnsembleDecoder.__init__": [[1334, 1342], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder1", ",", "decoder2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decoder1", "=", "decoder1", "\n", "self", ".", "decoder2", "=", "decoder2", "\n", "self", ".", "pad_index", "=", "self", ".", "decoder1", ".", "pad_index", "\n", "self", ".", "eos_index", "=", "self", ".", "decoder1", ".", "eos_index", "\n", "self", ".", "dim", "=", "self", ".", "decoder1", ".", "dim", "\n", "self", ".", "n_words", "=", "self", ".", "decoder1", ".", "n_words", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.BinaryEnsembleDecoder.forward": [[1343, 1355], ["evaluator.BinaryEnsembleDecoder.fwd", "NotImplementedError", "Exception"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd"], ["", "def", "forward", "(", "self", ",", "mode", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Forward function with different forward modes.\n        ### Small hack to handle PyTorch distributed.\n        \"\"\"", "\n", "if", "mode", "==", "'fwd'", ":", "\n", "            ", "return", "self", ".", "fwd", "(", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "'predict'", ":", "\n", "# return self.predict(**kwargs)", "\n", "            ", "raise", "NotImplementedError", "(", "'predict not impl'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown mode: %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.BinaryEnsembleDecoder.fwd": [[1356, 1362], ["evaluator.BinaryEnsembleDecoder.decoder1.fwd", "evaluator.BinaryEnsembleDecoder.decoder2.fwd", "src_enc1.size", "src_enc2.size", "src_enc1.size", "src_enc2.size"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd"], ["", "", "def", "fwd", "(", "self", ",", "x", ",", "lengths", ",", "causal", ",", "src_enc1", "=", "None", ",", "src_enc2", "=", "None", ",", "src_len", "=", "None", ",", "positions", "=", "None", ",", "langs", "=", "None", ",", "\n", "cache1", "=", "None", ",", "cache2", "=", "None", ")", ":", "\n", "        ", "assert", "src_enc1", ".", "size", "(", ")", "==", "src_enc2", ".", "size", "(", ")", ",", "'{} != {}'", ".", "format", "(", "src_enc1", ".", "size", "(", ")", ",", "src_enc2", ".", "size", "(", ")", ")", "\n", "tensor1", "=", "self", ".", "decoder1", ".", "fwd", "(", "x", ",", "lengths", ",", "causal", ",", "src_enc1", ",", "src_len", ",", "positions", ",", "langs", ",", "cache1", ")", "\n", "tensor2", "=", "self", ".", "decoder2", ".", "fwd", "(", "x", ",", "lengths", ",", "causal", ",", "src_enc2", ",", "src_len", ",", "positions", ",", "langs", ",", "cache2", ")", "\n", "return", "tensor1", ",", "tensor2", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.BinaryEnsembleDecoder.generate": [[1363, 1464], ["len", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "src_enc1.size", "src_enc2.size", "evaluator.BinaryEnsembleDecoder.forward", "tensor1.data[].type_as.data[].type_as.data[].type_as", "tensor2.data[].type_as.data[].type_as.data[].type_as", "evaluator.BinaryEnsembleDecoder.decoder1.pred_layer.get_scores", "evaluator.BinaryEnsembleDecoder.decoder2.pred_layer.get_scores", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "tensor1.data[].type_as.data[].type_as.size", "src_enc1.size", "tensor1.data[].type_as.data[].type_as.size", "tensor2.data[].type_as.data[].type_as.size", "src_enc2.size", "tensor2.data[].type_as.data[].type_as.size", "[].squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "src_len.clone().fill_.max", "generated[].masked_fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "src_len.clone().fill_.bool", "generated[].masked_fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "src_len.clone().fill_.byte", "evaluator.BinaryEnsembleDecoder.unsqueeze", "evaluator.BinaryEnsembleDecoder.unsqueeze", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate", "(", "self", ",", "src_enc1", ",", "src_enc2", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Decode a sentence given initial start.\n        `x`:\n            - LongTensor(bs, slen)\n                <EOS> W1 W2 W3 <EOS> <PAD>\n                <EOS> W1 W2 W3   W4  <EOS>\n        `lengths`:\n            - LongTensor(bs) [5, 6]\n        `positions`:\n            - False, for regular \"arange\" positions (LM)\n            - True, to reset positions from the new generation (MT)\n        `langs`:\n            - must be None if the model only supports one language\n            - lang_id if only one language is involved (LM)\n            - (lang_id1, lang_id2) if two languages are involved (MT)\n        \"\"\"", "\n", "\n", "# input batch", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "src_enc1", ".", "size", "(", "0", ")", "==", "bs", "\n", "assert", "src_enc2", ".", "size", "(", "0", ")", "==", "bs", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "# cache compute states", "\n", "cache1", "=", "{", "'slen'", ":", "0", "}", "\n", "cache2", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor1", ",", "tensor2", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc1", "=", "src_enc1", ",", "\n", "src_enc2", "=", "src_enc2", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache1", "=", "cache1", ",", "\n", "cache2", "=", "cache2", "\n", ")", "\n", "assert", "tensor1", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ",", "(", "\n", "cur_len", ",", "max_len", ",", "src_enc1", ".", "size", "(", ")", ",", "tensor1", ".", "size", "(", ")", ",", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ")", "\n", "assert", "tensor2", ".", "size", "(", ")", "==", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ",", "(", "\n", "cur_len", ",", "max_len", ",", "src_enc2", ".", "size", "(", ")", ",", "tensor2", ".", "size", "(", ")", ",", "(", "1", ",", "bs", ",", "self", ".", "dim", ")", ")", "\n", "tensor1", "=", "tensor1", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "src_enc1", ")", "# (bs, dim)", "\n", "tensor2", "=", "tensor2", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "src_enc2", ")", "# (bs, dim)", "\n", "scores1", "=", "self", ".", "decoder1", ".", "pred_layer", ".", "get_scores", "(", "tensor1", ")", "# (bs, n_words)", "\n", "scores2", "=", "self", ".", "decoder2", ".", "pred_layer", ".", "get_scores", "(", "tensor2", ")", "# (bs, n_words)", "\n", "scores", "=", "torch", ".", "cat", "(", "(", "scores1", ".", "unsqueeze", "(", "-", "1", ")", ",", "scores2", ".", "unsqueeze", "(", "-", "1", ")", ")", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", "\n", "\n", "# select next words: sample or greedy", "\n", "if", "sample_temperature", "is", "None", ":", "\n", "                ", "next_words", "=", "torch", ".", "topk", "(", "scores", ",", "1", ")", "[", "1", "]", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "1", ")", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "try", ":", "\n", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "bool", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# generated[-1].masked_fill_(unfinished_sents.bool(), self.eos_index)", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.BinaryEnsembleDecoder.generate_beam": [[1465, 1631], ["len", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_enc2.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.new.view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "enumerate", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "enumerate", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.size", "model.transformer.BeamHypotheses", "evaluator.BinaryEnsembleDecoder.forward", "evaluator.BinaryEnsembleDecoder.decoder1.pred_layer.get_scores", "evaluator.BinaryEnsembleDecoder.decoder2.pred_layer.get_scores", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "range", "beam_scores.new.new.new", "src_len.unsqueeze().expand().contiguous().view.new.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "cache1.keys", "cache2.keys", "all", "best.append", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_enc2.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "tensor1.size", "tensor2.size", "torch.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "zip", "next_batch_beam.extend", "len", "max", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_len.unsqueeze().expand().contiguous().view.new.max().item", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_enc2.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "evaluator.BinaryEnsembleDecoder.unsqueeze", "evaluator.BinaryEnsembleDecoder.unsqueeze", "generated[].clone", "value.item", "src_len.unsqueeze().expand().contiguous().view.new.max", "src_enc1.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "src_enc2.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_beam", "(", "self", ",", "src_enc1", ",", "src_enc2", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ")", ":", "\n", "# check inputs", "\n", "        ", "assert", "src_enc1", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "src_enc1", ".", "size", "(", "0", ")", "==", "src_len", ".", "size", "(", "0", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "# expand to beam size the source latent representations / source lengths", "\n", "src_enc1", "=", "src_enc1", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc1", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc1", ".", "shape", "[", "1", ":", "]", ")", "\n", "src_enc2", "=", "src_enc2", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "src_enc2", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "src_enc2", ".", "shape", "[", "1", ":", "]", ")", "\n", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_enc1", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "\n", "# cache compute states", "\n", "cache1", "=", "{", "'slen'", ":", "0", "}", "\n", "cache2", "=", "{", "'slen'", ":", "0", "}", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensor1", ",", "tensor2", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_enc1", "=", "src_enc1", ",", "\n", "src_enc2", "=", "src_enc2", ",", "\n", "src_len", "=", "src_len", ",", "\n", "cache1", "=", "cache1", ",", "\n", "cache2", "=", "cache2", ",", "\n", ")", "\n", "assert", "tensor1", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "assert", "tensor2", ".", "size", "(", ")", "==", "(", "1", ",", "bs", "*", "beam_size", ",", "self", ".", "dim", ")", "\n", "tensor1", "=", "tensor1", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "tensor2", "=", "tensor2", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", "# (bs * beam_size, dim)", "\n", "scores1", "=", "self", ".", "decoder1", ".", "pred_layer", ".", "get_scores", "(", "tensor1", ")", "# (bs * beam_size, n_words)", "\n", "scores2", "=", "self", ".", "decoder2", ".", "pred_layer", ".", "get_scores", "(", "tensor2", ")", "# (bs * beam_size, n_words)", "\n", "scores", "=", "torch", ".", "cat", "(", "(", "scores1", ".", "unsqueeze", "(", "-", "1", ")", ",", "scores2", ".", "unsqueeze", "(", "-", "1", ")", ")", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", "\n", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                        ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "\n", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "for", "k", "in", "cache1", ".", "keys", "(", ")", ":", "\n", "                ", "if", "k", "!=", "'slen'", ":", "\n", "                    ", "cache1", "[", "k", "]", "=", "(", "cache1", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache1", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "", "", "for", "k", "in", "cache2", ".", "keys", "(", ")", ":", "\n", "                ", "if", "k", "!=", "'slen'", ":", "\n", "                    ", "cache2", "[", "k", "]", "=", "(", "cache2", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache2", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# select the best hypotheses", "\n", "", "", "tgt_len", "=", "src_len", ".", "new", "(", "bs", ")", "\n", "best", "=", "[", "]", "\n", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyp", "=", "max", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", "1", "]", "\n", "tgt_len", "[", "i", "]", "=", "len", "(", "best_hyp", ")", "+", "1", "# +1 for the <EOS> symbol", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "src_len", ".", "new", "(", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ",", "bs", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "            ", "decoded", "[", ":", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "hypo", "\n", "decoded", "[", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "self", ".", "eos_index", "\n", "\n", "# sanity check", "\n", "", "assert", "(", "decoded", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "decoded", ",", "tgt_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.__init__": [[1634, 1647], ["torch.Module.__init__", "all", "all", "all", "all"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# self.decoder1 = decoder1", "\n", "# self.decoder2 = decoder2", "\n", "assert", "all", "(", "x", ".", "pad_index", "==", "decoders", "[", "0", "]", ".", "pad_index", "for", "x", "in", "decoders", ")", "\n", "assert", "all", "(", "x", ".", "eos_index", "==", "decoders", "[", "0", "]", ".", "eos_index", "for", "x", "in", "decoders", ")", "\n", "assert", "all", "(", "x", ".", "dim", "==", "decoders", "[", "0", "]", ".", "dim", "for", "x", "in", "decoders", ")", "\n", "assert", "all", "(", "x", ".", "n_words", "==", "decoders", "[", "0", "]", ".", "n_words", "for", "x", "in", "decoders", ")", "\n", "self", ".", "decoders", "=", "decoders", "\n", "self", ".", "pad_index", "=", "decoders", "[", "0", "]", ".", "pad_index", "\n", "self", ".", "eos_index", "=", "decoders", "[", "0", "]", ".", "eos_index", "\n", "self", ".", "dim", "=", "decoders", "[", "0", "]", ".", "dim", "\n", "self", ".", "n_words", "=", "decoders", "[", "0", "]", ".", "n_words", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward": [[1648, 1660], ["evaluator.MultiEnsembleDecoder.fwd", "NotImplementedError", "Exception"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd"], ["", "def", "forward", "(", "self", ",", "mode", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Forward function with different forward modes.\n        ### Small hack to handle PyTorch distributed.\n        \"\"\"", "\n", "if", "mode", "==", "'fwd'", ":", "\n", "            ", "return", "self", ".", "fwd", "(", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "'predict'", ":", "\n", "# return self.predict(**kwargs)", "\n", "            ", "raise", "NotImplementedError", "(", "'predict not impl'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unknown mode: %s\"", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd": [[1661, 1672], ["all", "isinstance", "isinstance", "dec.fwd", "zip", "e1.size", "e2.size", "zip"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.fwd"], ["", "", "def", "fwd", "(", "self", ",", "x", ",", "lengths", ",", "causal", ",", "src_encs", "=", "None", ",", "src_len", "=", "None", ",", "positions", "=", "None", ",", "langs", "=", "None", ",", "\n", "caches", "=", "None", ")", ":", "\n", "# assert src_enc1.size() == src_enc2.size(), '{} != {}'.format(src_enc1.size(), src_enc2.size())", "\n", "        ", "assert", "src_encs", "is", "not", "None", "and", "isinstance", "(", "src_encs", ",", "list", ")", "\n", "assert", "caches", "is", "None", "or", "isinstance", "(", "caches", ",", "list", ")", "\n", "assert", "all", "(", "e1", ".", "size", "(", ")", "==", "e2", ".", "size", "(", ")", "for", "e1", ",", "e2", "in", "zip", "(", "src_encs", "[", ":", "-", "1", "]", ",", "src_encs", "[", "1", ":", "]", ")", ")", "\n", "# tensor1 = self.decoder1.fwd(x, lengths, causal, src_enc1, src_len, positions, langs, cache1)", "\n", "# tensor2 = self.decoder2.fwd(x, lengths, causal, src_enc2, src_len, positions, langs, cache2)", "\n", "tensors", "=", "[", "dec", ".", "fwd", "(", "x", ",", "lengths", ",", "causal", ",", "src_enc", ",", "src_len", ",", "positions", ",", "langs", ",", "cache", ")", "\n", "for", "dec", ",", "src_enc", ",", "cache", "in", "zip", "(", "self", ".", "decoders", ",", "src_encs", ",", "caches", ")", "]", "\n", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate": [[1673, 1742], ["len", "all", "src_len.new", "src_len.new.fill_", "generated[].fill_", "src_len.new().long", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "torch.arange().unsqueeze().expand", "src_len.new().long().fill_", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "src_len.clone().fill_", "src_len.clone().fill_", "evaluator.MultiEnsembleDecoder.forward", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "src_len.clone().fill_.add_", "src_len.clone().fill_.mul_", "src_len.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "src_len.new().long", "langs.unsqueeze().expand.unsqueeze().expand.unsqueeze", "src_len.clone", "src_len.clone", "range", "t.data[].type_as", "dec.pred_layer.get_scores().unsqueeze", "[].squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.size", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "torch.multinomial().squeeze.ne().long", "src_len.clone().fill_.max", "generated[].masked_fill_", "src_enc.size", "len", "zip", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "src_len.clone().fill_.bool", "generated[].masked_fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "src_len.new", "dec.pred_layer.get_scores", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "torch.multinomial().squeeze.ne", "src_len.clone().fill_.byte", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores"], ["", "def", "generate", "(", "self", ",", "src_encs", ",", "src_len", ",", "tgt_lang_id", ",", "max_len", "=", "200", ",", "sample_temperature", "=", "None", ")", ":", "\n", "        ", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "all", "(", "src_enc", ".", "size", "(", "0", ")", "==", "bs", "for", "src_enc", "in", "src_encs", ")", "\n", "\n", "# generated sentences", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "langs", "=", "langs", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "max_len", ",", "bs", ")", "\n", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "cur_len", "=", "1", "\n", "gen_len", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "unfinished_sents", "=", "src_len", ".", "clone", "(", ")", ".", "fill_", "(", "1", ")", "\n", "\n", "caches", "=", "[", "{", "'slen'", ":", "0", "}", "for", "i", "in", "range", "(", "len", "(", "self", ".", "decoders", ")", ")", "]", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "            ", "tensors", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "gen_len", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_encs", "=", "src_encs", ",", "\n", "src_len", "=", "src_len", ",", "\n", "caches", "=", "caches", ",", "\n", ")", "\n", "# assert all(t.size() == (1, bs, self.dim), (cur_len, max_len, src_enc1.size(), tensor1.size(), (1, bs, self.dim)))", "\n", "tensors", "=", "[", "t", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "e", ")", "for", "t", ",", "e", "in", "zip", "(", "tensors", ",", "src_encs", ")", "]", "\n", "scores_t", "=", "[", "dec", ".", "pred_layer", ".", "get_scores", "(", "t", ")", ".", "unsqueeze", "(", "-", "1", ")", "for", "dec", ",", "t", "in", "zip", "(", "self", ".", "decoders", ",", "tensors", ")", "]", "\n", "scores", "=", "torch", ".", "cat", "(", "scores_t", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", "\n", "\n", "if", "sample_temperature", "is", "None", ":", "\n", "                ", "next_words", "=", "torch", ".", "topk", "(", "scores", ",", "1", ")", "[", "1", "]", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", "/", "sample_temperature", ",", "dim", "=", "1", ")", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "assert", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", ")", "\n", "\n", "# update generations / lengths / finished sentences / current length", "\n", "generated", "[", "cur_len", "]", "=", "next_words", "*", "unfinished_sents", "+", "self", ".", "pad_index", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "gen_len", ".", "add_", "(", "unfinished_sents", ")", "\n", "unfinished_sents", ".", "mul_", "(", "next_words", ".", "ne", "(", "self", ".", "eos_index", ")", ".", "long", "(", ")", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# add <EOS> to unfinished sentences", "\n", "", "", "if", "cur_len", "==", "max_len", ":", "\n", "            ", "try", ":", "\n", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "bool", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# generated[-1].masked_fill_(unfinished_sents.bool(), self.eos_index)", "\n", "                ", "generated", "[", "-", "1", "]", ".", "masked_fill_", "(", "unfinished_sents", ".", "byte", "(", ")", ",", "self", ".", "eos_index", ")", "\n", "\n", "# sanity check", "\n", "", "", "assert", "(", "generated", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "generated", "[", ":", "cur_len", "]", ",", "gen_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam": [[1743, 1894], ["len", "all", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "src_len.unsqueeze().expand().contiguous().view.new.fill_", "generated[].fill_", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().long", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "torch.arange().unsqueeze().expand_as.clone().fill_", "src_encs[].new().fill_", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "torch.tensor().type_as", "beam_scores.new.new.view", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "enumerate", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "enumerate", "e.unsqueeze().expand().contiguous().view", "model.transformer.BeamHypotheses", "evaluator.MultiEnsembleDecoder.forward", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "_scores.view.view.view", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "range", "beam_scores.new.new.new", "src_len.unsqueeze().expand().contiguous().view.new.new", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "all", "best.append", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "range", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "torch.arange().unsqueeze().expand_as.clone", "src_encs[].new", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "range", "t.data[].type_as", "dec.pred_layer.get_scores().unsqueeze", "torch.log_softmax.size", "beam_scores[].expand_as", "next_scores.size", "next_words.size", "zip", "next_batch_beam.extend", "len", "cache.keys", "max", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "e.size", "e.unsqueeze().expand().contiguous", "len", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new().fill_", "zip", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "src_len.unsqueeze().expand().contiguous().view.new.max().item", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "dec.pred_layer.get_scores", "next_scores[].max().item", "generated_hyps[].add", "next_sent_beam.append", "len", "len", "e.unsqueeze().expand", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.new", "generated[].clone", "value.item", "src_len.unsqueeze().expand().contiguous().view.new.max", "src_len.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "next_scores[].max", "e.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.forward", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.PredLayer.get_scores", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.mass.mass_transformer.BeamHypotheses.add"], ["", "def", "generate_beam", "(", "self", ",", "src_encs", ",", "src_len", ",", "tgt_lang_id", ",", "beam_size", ",", "length_penalty", ",", "early_stopping", ",", "\n", "max_len", "=", "200", ")", ":", "\n", "        ", "bs", "=", "len", "(", "src_len", ")", "\n", "assert", "all", "(", "e", ".", "size", "(", "0", ")", "==", "bs", "for", "e", "in", "src_encs", ")", "\n", "assert", "beam_size", ">=", "1", "\n", "\n", "# batch size / number of words", "\n", "bs", "=", "len", "(", "src_len", ")", "\n", "n_words", "=", "self", ".", "n_words", "\n", "\n", "src_encs", "=", "[", "e", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "(", "bs", ",", "beam_size", ")", "+", "e", ".", "shape", "[", "1", ":", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "(", "bs", "*", "beam_size", ",", ")", "+", "e", ".", "shape", "[", "1", ":", "]", ")", "for", "e", "in", "src_encs", "]", "\n", "\n", "src_len", "=", "src_len", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "bs", ",", "beam_size", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# generated sentences (batch with beam current hypotheses)", "\n", "generated", "=", "src_len", ".", "new", "(", "max_len", ",", "bs", "*", "beam_size", ")", "# upcoming output", "\n", "generated", ".", "fill_", "(", "self", ".", "pad_index", ")", "# fill upcoming ouput with <PAD>", "\n", "generated", "[", "0", "]", ".", "fill_", "(", "self", ".", "eos_index", ")", "# we use <EOS> for <BOS> everywhere", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "BeamHypotheses", "(", "beam_size", ",", "max_len", ",", "length_penalty", ",", "early_stopping", ")", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "# positions", "\n", "positions", "=", "src_len", ".", "new", "(", "max_len", ")", ".", "long", "(", ")", "\n", "positions", "=", "torch", ".", "arange", "(", "max_len", ",", "out", "=", "positions", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "generated", ")", "\n", "\n", "# language IDs", "\n", "langs", "=", "positions", ".", "clone", "(", ")", ".", "fill_", "(", "tgt_lang_id", ")", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "src_encs", "[", "0", "]", ".", "new", "(", "bs", ",", "beam_size", ")", ".", "fill_", "(", "0", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "torch", ".", "tensor", "(", "-", "1e9", ")", ".", "type_as", "(", "beam_scores", ")", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# current position", "\n", "cur_len", "=", "1", "\n", "caches", "=", "[", "{", "'slen'", ":", "0", "}", "for", "i", "in", "range", "(", "len", "(", "self", ".", "decoders", ")", ")", "]", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "bs", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_len", ":", "\n", "\n", "# compute word scores", "\n", "            ", "tensors", "=", "self", ".", "forward", "(", "\n", "'fwd'", ",", "\n", "x", "=", "generated", "[", ":", "cur_len", "]", ",", "\n", "lengths", "=", "src_len", ".", "new", "(", "bs", "*", "beam_size", ")", ".", "fill_", "(", "cur_len", ")", ",", "\n", "positions", "=", "positions", "[", ":", "cur_len", "]", ",", "\n", "langs", "=", "langs", "[", ":", "cur_len", "]", ",", "\n", "causal", "=", "True", ",", "\n", "src_encs", "=", "src_encs", ",", "\n", "src_len", "=", "src_len", ",", "\n", "caches", "=", "caches", ",", "\n", ")", "\n", "tensors", "=", "[", "t", ".", "data", "[", "-", "1", ",", ":", ",", ":", "]", ".", "type_as", "(", "e", ")", "for", "t", ",", "e", "in", "zip", "(", "tensors", ",", "src_encs", ")", "]", "\n", "scores_t", "=", "[", "dec", ".", "pred_layer", ".", "get_scores", "(", "t", ")", ".", "unsqueeze", "(", "-", "1", ")", "for", "dec", ",", "t", "in", "zip", "(", "self", ".", "decoders", ",", "tensors", ")", "]", "\n", "scores", "=", "torch", ".", "cat", "(", "scores_t", ",", "-", "1", ")", ".", "mean", "(", "-", "1", ")", "\n", "\n", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (bs * beam_size, n_words)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "bs", "*", "beam_size", ",", "n_words", ")", "\n", "\n", "# select next words with scores", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (bs * beam_size, n_words)", "\n", "_scores", "=", "_scores", ".", "view", "(", "bs", ",", "beam_size", "*", "n_words", ")", "# (bs, beam_size * n_words)", "\n", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "bs", ",", "2", "*", "beam_size", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "sent_id", "in", "range", "(", "bs", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "sent_id", "]", "=", "done", "[", "sent_id", "]", "or", "generated_hyps", "[", "sent_id", "]", ".", "is_done", "(", "next_scores", "[", "sent_id", "]", ".", "max", "(", ")", ".", "item", "(", ")", ")", "\n", "if", "done", "[", "sent_id", "]", ":", "\n", "                    ", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "value", "in", "zip", "(", "next_words", "[", "sent_id", "]", ",", "next_scores", "[", "sent_id", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "n_words", "\n", "word_id", "=", "idx", "%", "n_words", "\n", "\n", "# end of sentence, or next word", "\n", "if", "word_id", "==", "self", ".", "eos_index", "or", "cur_len", "+", "1", "==", "max_len", ":", "\n", "                        ", "generated_hyps", "[", "sent_id", "]", ".", "add", "(", "generated", "[", ":", "cur_len", ",", "sent_id", "*", "beam_size", "+", "beam_id", "]", ".", "clone", "(", ")", ",", "\n", "value", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "value", ",", "word_id", ",", "sent_id", "*", "beam_size", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "beam_size", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "0", "if", "cur_len", "+", "1", "==", "max_len", "else", "beam_size", "\n", "if", "len", "(", "next_sent_beam", ")", "==", "0", ":", "\n", "                    ", "next_sent_beam", "=", "[", "(", "0", ",", "self", ".", "pad_index", ",", "0", ")", "]", "*", "beam_size", "# pad the batch", "\n", "", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "beam_size", "*", "(", "sent_id", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "bs", "*", "beam_size", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "generated", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "src_len", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch and internal states", "\n", "generated", "=", "generated", "[", ":", ",", "beam_idx", "]", "\n", "generated", "[", "cur_len", "]", "=", "beam_words", "\n", "\n", "for", "cache", "in", "caches", ":", "\n", "                ", "for", "k", "in", "cache", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "k", "!=", "'slen'", ":", "\n", "                        ", "cache", "[", "k", "]", "=", "(", "cache", "[", "k", "]", "[", "0", "]", "[", "beam_idx", "]", ",", "cache", "[", "k", "]", "[", "1", "]", "[", "beam_idx", "]", ")", "\n", "\n", "# update current length", "\n", "", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "# select the best hypotheses", "\n", "", "", "tgt_len", "=", "src_len", ".", "new", "(", "bs", ")", "\n", "best", "=", "[", "]", "\n", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "best_hyp", "=", "max", "(", "hypotheses", ".", "hyp", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "[", "1", "]", "\n", "tgt_len", "[", "i", "]", "=", "len", "(", "best_hyp", ")", "+", "1", "# +1 for the <EOS> symbol", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# generate target batch", "\n", "", "decoded", "=", "src_len", ".", "new", "(", "tgt_len", ".", "max", "(", ")", ".", "item", "(", ")", ",", "bs", ")", ".", "fill_", "(", "self", ".", "pad_index", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "            ", "decoded", "[", ":", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "hypo", "\n", "decoded", "[", "tgt_len", "[", "i", "]", "-", "1", ",", "i", "]", "=", "self", ".", "eos_index", "\n", "\n", "# sanity check", "\n", "", "assert", "(", "decoded", "==", "self", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "\n", "return", "decoded", ",", "tgt_len", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.__init__": [[1898, 1902], ["evaluator.EncDecEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "data", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "trainer", ",", "data", ",", "params", ")", "\n", "self", ".", "encoder_para", "=", "trainer", ".", "encoder_para", "\n", "self", ".", "decoder_para", "=", "trainer", ".", "decoder_para", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.run_all_evals": [[1903, 1944], ["collections.OrderedDict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "set", "evaluator.EncDecParaPretrainedEvaluator.evaluate_clm", "evaluator.EncDecParaPretrainedEvaluator.evaluate_mlm", "evaluator.EncDecParaPretrainedEvaluator.evaluate_mt_ensemble_x2", "len", "numpy.mean", "numpy.mean", "len", "numpy.mean", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_clm", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.Evaluator.evaluate_mlm", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.evaluate_mt_ensemble_x2"], ["", "def", "run_all_evals", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "\"\"\"\n        Run all evaluations.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", "}", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# for data_set in ['valid', 'test']:", "\n", "# fixme: quick fix: run only test", "\n", "            ", "for", "data_set", "in", "[", "'test'", "]", ":", "\n", "\n", "# causal prediction task (evaluate perplexity and accuracy)", "\n", "                ", "for", "lang1", ",", "lang2", "in", "params", ".", "clm_steps", ":", "\n", "                    ", "self", ".", "evaluate_clm", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", "\n", "\n", "# prediction task (evaluate perplexity and accuracy)", "\n", "", "for", "lang1", ",", "lang2", "in", "params", ".", "mlm_steps", ":", "\n", "                    ", "self", ".", "evaluate_mlm", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ")", "\n", "\n", "# machine translation task (evaluate perplexity and accuracy)", "\n", "", "for", "lang1", ",", "lang2", "in", "set", "(", "params", ".", "mt_steps", "+", "[", "(", "l2", ",", "l3", ")", "for", "_", ",", "l2", ",", "l3", "in", "params", ".", "bt_steps", "]", ")", ":", "\n", "                    ", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "self", ".", "evaluate_mt_ensemble_x2", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ")", "\n", "\n", "# report average metrics per language", "\n", "", "_clm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "clm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_clm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_clm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "scores", "[", "'%s_clm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "", "_mlm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "mlm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_mlm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_mlm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "scores", "[", "'%s_mlm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "\n", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_train": [[1945, 1961], ["collections.OrderedDict", "getattr", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "set", "logger.info", "logger.info", "evaluator.EncDecParaPretrainedEvaluator.infer_mt_ensemble_x2", "logger.info", "evaluator.EncDecParaPretrainedEvaluator.infer_to_triple_files"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_mt_ensemble_x2", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_to_triple_files"], ["", "def", "infer_train", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", "}", ")", "\n", "infer_ens2x_train", "=", "getattr", "(", "params", ",", "'infer_ens2x_train'", ",", "False", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_set", "=", "'train'", "\n", "for", "lang1", ",", "lang2", ",", "lang3", "in", "set", "(", "params", ".", "bt_steps", ")", ":", "\n", "                ", "logger", ".", "info", "(", "'Perform 2BT: {} -> {} => {}'", ".", "format", "(", "lang1", ",", "lang2", ",", "lang3", ")", ")", "\n", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "if", "infer_ens2x_train", ":", "\n", "                    ", "logger", ".", "info", "(", "'Generate ensembles 1stage!'", ")", "\n", "self", ".", "infer_mt_ensemble_x2", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "infer_train", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "logger", ".", "info", "(", "'Generate MACD bt2stage!'", ")", "\n", "self", ".", "infer_to_triple_files", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "lang3", ",", "eval_bleu", ",", "infer_train", "=", "True", ")", "\n", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator": [[1962, 2014], ["len", "set", "set", "[].get_iterator", "len", "[].get_iterator", "[].get_iterator"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator"], ["", "def", "get_iterator", "(", "\n", "self", ",", "data_set", ",", "lang1", ",", "lang2", "=", "None", ",", "stream", "=", "False", ",", "allow_train_data", "=", "False", ",", "descending", "=", "False", ",", "\n", "force_mono", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Create a new iterator for a dataset.\n        \"\"\"", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "allow_train_data", "\n", "assert", "lang1", "in", "self", ".", "params", ".", "langs", "\n", "assert", "lang2", "is", "None", "or", "lang2", "in", "self", ".", "params", ".", "langs", "\n", "assert", "stream", "is", "False", "or", "lang2", "is", "None", "\n", "\n", "# hacks to reduce evaluation time when using many languages", "\n", "if", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "30", ":", "\n", "            ", "eval_lgs", "=", "set", "(", "\n", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", ",", "\"ab\"", ",", "\"ay\"", ",", "\n", "\"bug\"", ",", "\"ha\"", ",", "\"ko\"", ",", "\"ln\"", ",", "\"min\"", ",", "\"nds\"", ",", "\"pap\"", ",", "\"pt\"", ",", "\"tg\"", ",", "\"to\"", ",", "\"udm\"", ",", "\"uk\"", ",", "\"zh_classical\"", "]", ")", "\n", "eval_lgs", "=", "set", "(", "[", "\"ar\"", ",", "\"bg\"", ",", "\"de\"", ",", "\"el\"", ",", "\"en\"", ",", "\"es\"", ",", "\"fr\"", ",", "\"hi\"", ",", "\"ru\"", ",", "\"sw\"", ",", "\"th\"", ",", "\"tr\"", ",", "\"ur\"", ",", "\"vi\"", ",", "\"zh\"", "]", ")", "\n", "subsample", "=", "10", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "5", "\n", "n_sentences", "=", "600", "if", "(", "data_set", "==", "'test'", "or", "lang1", "not", "in", "eval_lgs", ")", "else", "1500", "\n", "", "elif", "len", "(", "self", ".", "params", ".", "langs", ")", ">", "5", ":", "\n", "            ", "subsample", "=", "10", "if", "data_set", "==", "'test'", "else", "5", "\n", "n_sentences", "=", "300", "if", "data_set", "==", "'test'", "else", "1500", "\n", "", "else", ":", "\n", "# n_sentences = -1 if data_set == 'valid' else 100", "\n", "            ", "n_sentences", "=", "-", "1", "\n", "subsample", "=", "1", "\n", "\n", "# FIXME: forcefully get monolingual data despite mt steps", "\n", "\n", "", "if", "lang2", "is", "None", "or", "force_mono", ":", "\n", "            ", "if", "stream", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono_stream'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "shuffle", "=", "False", ",", "subsample", "=", "subsample", ")", "\n", "", "else", ":", "\n", "                ", "iterator", "=", "self", ".", "data", "[", "'mono'", "]", "[", "lang1", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", ",", "\n", "infer_train", "=", "True", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "stream", "is", "False", "\n", "_lang1", ",", "_lang2", "=", "(", "lang1", ",", "lang2", ")", "if", "lang1", "<", "lang2", "else", "(", "lang2", ",", "lang1", ")", "\n", "iterator", "=", "self", ".", "data", "[", "'para'", "]", "[", "(", "_lang1", ",", "_lang2", ")", "]", "[", "data_set", "]", ".", "get_iterator", "(", "\n", "shuffle", "=", "False", ",", "\n", "group_by_size", "=", "True", ",", "\n", "n_sentences", "=", "n_sentences", ",", "\n", "descending", "=", "descending", "\n", ")", "\n", "\n", "", "for", "batch", "in", "iterator", ":", "\n", "            ", "yield", "batch", "if", "force_mono", "or", "lang2", "is", "None", "or", "lang1", "<", "lang2", "else", "batch", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_to_triple_files": [[2015, 2158], ["getattr", "logger.info", "evaluator.EncDecParaPretrainedEvaluator.encoder.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder.eval", "evaluator.EncDecParaPretrainedEvaluator.encoder_para.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder_para.eval", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "logger.info", "logger.info", "logger.info", "utils.restore_segmentation", "utils.restore_segmentation", "utils.restore_segmentation", "logger.info", "open", "open", "open", "enumerate", "evaluator.EncDecParaPretrainedEvaluator.get_iterator", "x1.clone().fill_", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "int", "evaluator.convert_to_text", "evaluator.convert_to_text", "hypothesis2.extend", "source.extend", "x2.clone().fill_", "encoder_para", "enc2.transpose.transpose.transpose", "int", "evaluator.convert_to_text", "hypothesis3.extend", "zip", "logger.info", "enc1.transpose.transpose.half", "decoder.generate", "decoder.generate_beam", "enc2.transpose.transpose.half", "decoder_para.generate", "decoder_para.generate_beam", "fh2.write", "fh3.write", "fs.write", "x1.clone", "all_mem_att[].append", "x2.clone", "all_mem_att2[].append", "len1.max().item", "len2.max().item", "len1.max", "len2.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam"], ["", "", "def", "infer_to_triple_files", "(", "\n", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "lang3", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "infer_train", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "infer_name", "=", "params", ".", "infer_name", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "logger", ".", "info", "(", "'Order descending: {}'", ".", "format", "(", "descending", ")", ")", "\n", "assert", "infer_name", "!=", "''", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "assert", "lang3", "in", "params", ".", "langs", "\n", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "try", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_para", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", "\n", "\n", "", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "lang3_id", "=", "params", ".", "lang2id", "[", "lang3", "]", "\n", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "all_mem_att2", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "# if eval_bleu:", "\n", "", "hypothesis2", "=", "[", "]", "\n", "hypothesis3", "=", "[", "]", "\n", "source", "=", "[", "]", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "# src_name = f\"infer{scores['epoch']}.{lang1}-{lang2}-{lang3}.{data_set}.b{beam}.lp{lenpen}.src.{lang1}\"", "\n", "# hyp2_name = f\"infer{scores['epoch']}.{lang1}-{lang2}-{lang3}.{data_set}.b{beam}.lp{lenpen}.hyp.{lang2}\"", "\n", "# hyp3_name = f\"infer{scores['epoch']}.{lang1}-{lang2}-{lang3}.{data_set}.b{beam}.lp{lenpen}.hyp.{lang3}\"", "\n", "if", "params", ".", "local_rank", ">", "-", "1", ":", "\n", "            ", "infer_name", "=", "\"{}.rank{}\"", ".", "format", "(", "infer_name", ",", "params", ".", "local_rank", ")", "\n", "", "src_name", "=", "\"{}.infer{}.{}-{}-{}.{}.b{}.lp{}.src.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "lang3", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang1", "\n", ")", "\n", "hyp2_name", "=", "\"{}.infer{}.{}-{}-{}.{}.b{}.lp{}.hyp.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "lang3", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang2", "\n", ")", "\n", "hyp3_name", "=", "\"{}.infer{}.{}-{}-{}.{}.b{}.lp{}.hyp.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "lang3", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang3", "\n", ")", "\n", "\n", "os", ".", "makedirs", "(", "params", ".", "hyp_path", ",", "exist_ok", "=", "True", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "src_name", ")", "\n", "hyp2_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp2_name", ")", "\n", "hyp3_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp3_name", ")", "\n", "logger", ".", "info", "(", "'Write src to  : {}'", ".", "format", "(", "src_path", ")", ")", "\n", "logger", ".", "info", "(", "'Write hypo2 to: {}'", ".", "format", "(", "hyp2_path", ")", ")", "\n", "logger", ".", "info", "(", "'Write hypo3 to: {}'", ".", "format", "(", "hyp3_path", ")", ")", "\n", "\n", "with", "open", "(", "hyp2_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fh2", ":", "\n", "            ", "with", "open", "(", "hyp3_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fh3", ":", "\n", "                ", "with", "open", "(", "src_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fs", ":", "\n", "\n", "                    ", "for", "index", ",", "batch", "in", "enumerate", "(", "\n", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ",", "\n", "force_mono", "=", "True", "\n", ")", ")", ":", "\n", "\n", "                        ", "if", "index", "%", "200", "==", "0", ":", "\n", "                            ", "logger", ".", "info", "(", "'| Generate Index {}'", ".", "format", "(", "index", ")", ")", "\n", "\n", "# todo: 1st stage: lang1 -> lang2 --------------------", "\n", "# (x1, len1), (_, __) = batch", "\n", "", "x1", ",", "len1", "=", "batch", "\n", "# print(\"x1={}, len1={}\".format(x1.size(), len1.size()))", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "# encode source sentence", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "\n", "if", "eval_memory", ":", "\n", "                            ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                                ", "all_mem_att", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "\n", "", "", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                            ", "generated2", ",", "lengths2", "=", "decoder", ".", "generate", "(", "enc1", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                            ", "generated2", ",", "lengths2", "=", "decoder", ".", "generate_beam", "(", "\n", "enc1", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "", "hyp2_texts", "=", "convert_to_text", "(", "generated2", ",", "lengths2", ",", "self", ".", "dico", ",", "params", ")", "\n", "src_texts", "=", "convert_to_text", "(", "x1", ",", "len1", ",", "self", ".", "dico", ",", "params", ")", "\n", "hypothesis2", ".", "extend", "(", "hyp2_texts", ")", "\n", "source", ".", "extend", "(", "src_texts", ")", "\n", "\n", "# todo: 2st stage: lang2 -> lang3 --------------------", "\n", "x2", "=", "generated2", "\n", "len2", "=", "lengths2", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "# encoder", "\n", "enc2", "=", "encoder_para", "(", "'fwd'", ",", "x", "=", "x2", ",", "lengths", "=", "len2", ",", "langs", "=", "langs2", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc2", "=", "enc2", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc2", "\n", "if", "eval_memory", ":", "\n", "                            ", "for", "k", ",", "v", "in", "self", ".", "memory_list", ":", "\n", "                                ", "all_mem_att2", "[", "k", "]", ".", "append", "(", "(", "v", ".", "last_indices", ",", "v", ".", "last_scores", ")", ")", "\n", "", "", "max_len2", "=", "int", "(", "1.5", "*", "len2", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                            ", "generated3", ",", "lengths3", "=", "decoder_para", ".", "generate", "(", "enc2", ",", "len2", ",", "lang3_id", ",", "max_len", "=", "max_len2", ")", "\n", "", "else", ":", "\n", "                            ", "generated3", ",", "lengths3", "=", "decoder_para", ".", "generate_beam", "(", "\n", "enc2", ",", "len2", ",", "lang3_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "", "hyp3_texts", "=", "convert_to_text", "(", "generated3", ",", "lengths3", ",", "self", ".", "dico", ",", "params", ")", "\n", "hypothesis3", ".", "extend", "(", "hyp3_texts", ")", "\n", "\n", "for", "h2", ",", "h3", ",", "s", "in", "zip", "(", "hyp2_texts", ",", "hyp3_texts", ",", "src_texts", ")", ":", "\n", "                            ", "fh2", ".", "write", "(", "'{}\\n'", ".", "format", "(", "h2", ")", ")", "\n", "fh3", ".", "write", "(", "'{}\\n'", ".", "format", "(", "h3", ")", ")", "\n", "fs", ".", "write", "(", "'{}\\n'", ".", "format", "(", "s", ")", ")", "\n", "\n", "", "", "", "", "", "restore_segmentation", "(", "hyp2_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "hyp3_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "src_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n", "logger", ".", "info", "(", "\"FINISH GENERATION\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.infer_mt_ensemble_x2": [[2159, 2266], ["getattr", "logger.info", "evaluator.EncDecParaPretrainedEvaluator.encoder.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder.eval", "evaluator.EncDecParaPretrainedEvaluator.encoder_para.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder_para.eval", "evaluator.BinaryEnsembleDecoder", "os.makedirs", "os.path.join", "os.path.join", "logger.info", "logger.info", "utils.restore_segmentation", "utils.restore_segmentation", "logger.info", "open", "open", "enumerate", "evaluator.EncDecParaPretrainedEvaluator.get_iterator", "x1.clone().fill_", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "encoder_para", "enc2.transpose.transpose.transpose", "int", "evaluator.convert_to_text", "evaluator.convert_to_text", "hypothesis.extend", "source.extend", "zip", "enc1.transpose.transpose.half", "enc2.transpose.transpose.half", "evaluator.BinaryEnsembleDecoder.generate", "evaluator.BinaryEnsembleDecoder.generate_beam", "fs.write", "fh.write", "x1.clone", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam"], ["", "def", "infer_mt_ensemble_x2", "(", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "\n", "infer_train", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "infer_name", "=", "params", ".", "infer_name", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "logger", ".", "info", "(", "'infer train _mt_ensemble 2x: Order descending: {}'", ".", "format", "(", "descending", ")", ")", "\n", "assert", "infer_name", "!=", "''", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "try", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_para", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", "\n", "\n", "", "decoder_wrapper", "=", "BinaryEnsembleDecoder", "(", "decoder", ",", "decoder_para", ")", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "\n", "", "hypothesis", "=", "[", "]", "\n", "source", "=", "[", "]", "\n", "beam", "=", "self", ".", "params", ".", "beam_size", "\n", "lenpen", "=", "self", ".", "params", ".", "length_penalty", "\n", "\n", "if", "params", ".", "local_rank", ">", "-", "1", ":", "\n", "            ", "infer_name", "=", "\"{}.rank{}\"", ".", "format", "(", "infer_name", ",", "params", ".", "local_rank", ")", "\n", "", "src_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.out.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang1", "\n", ")", "\n", "hyp_name", "=", "\"{}.infer{}.{}-{}.{}.b{}.lp{}.out.{}\"", ".", "format", "(", "\n", "infer_name", ",", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ",", "beam", ",", "lenpen", ",", "lang2", "\n", ")", "\n", "os", ".", "makedirs", "(", "params", ".", "hyp_path", ",", "exist_ok", "=", "True", ")", "\n", "src_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "src_name", ")", "\n", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp_name", ")", "\n", "logger", ".", "info", "(", "'Write src to: {}'", ".", "format", "(", "src_path", ")", ")", "\n", "logger", ".", "info", "(", "'Write hyp to: {}'", ".", "format", "(", "hyp_path", ")", ")", "\n", "\n", "with", "open", "(", "hyp_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fh", ":", "\n", "            ", "with", "open", "(", "src_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fs", ":", "\n", "                ", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ",", "force_mono", "=", "True", ")", ")", ":", "\n", "# generate batch", "\n", "# (x1, len1), (x2, len2) = batch", "\n", "                    ", "x1", ",", "len1", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "# langs2 = x2.clone().fill_(lang2_id)", "\n", "\n", "# target words to predict", "\n", "# alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)", "\n", "# pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word", "\n", "# y = x2[1:].masked_select(pred_mask[:-1])", "\n", "# assert len(y) == (len2 - 1).sum().item()", "\n", "\n", "# cuda", "\n", "# x1, len1, langs1, x2, len2, langs2, y = to_cuda(x1, len1, langs1, x2, len2, langs2, y)", "\n", "x1", ",", "len1", ",", "langs1", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ")", "\n", "\n", "# encode source sentences", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "enc2", "=", "encoder_para", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc2", "=", "enc2", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc2", "\n", "\n", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                        ", "generated", ",", "lengths", "=", "decoder_wrapper", ".", "generate", "(", "enc1", ",", "enc2", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                        ", "generated", ",", "lengths", "=", "decoder_wrapper", ".", "generate_beam", "(", "\n", "enc1", ",", "enc2", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "", "hyp_texts", "=", "convert_to_text", "(", "generated", ",", "lengths", ",", "self", ".", "dico", ",", "params", ")", "\n", "src_texts", "=", "convert_to_text", "(", "x1", ",", "len1", ",", "self", ".", "dico", ",", "params", ")", "\n", "hypothesis", ".", "extend", "(", "hyp_texts", ")", "\n", "source", ".", "extend", "(", "src_texts", ")", "\n", "\n", "for", "s", ",", "h", "in", "zip", "(", "src_texts", ",", "hyp_texts", ")", ":", "\n", "                        ", "fs", ".", "write", "(", "'{}\\n'", ".", "format", "(", "s", ")", ")", "\n", "fh", ".", "write", "(", "'{}\\n'", ".", "format", "(", "h", ")", ")", "\n", "", "", "", "", "restore_segmentation", "(", "hyp_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "restore_segmentation", "(", "src_path", ",", "True", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "logger", ".", "info", "(", "\"FINISH GENERATION ENSEMBLE\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.evaluate_mt_ensemble_x2": [[2267, 2373], ["getattr", "logger.info", "evaluator.EncDecParaPretrainedEvaluator.encoder.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder.eval", "evaluator.EncDecParaPretrainedEvaluator.encoder_para.eval", "evaluator.EncDecParaPretrainedEvaluator.decoder_para.eval", "evaluator.BinaryEnsembleDecoder", "enumerate", "evaluator.EncDecParaPretrainedEvaluator.get_iterator", "x1.clone().fill_", "x2.clone().fill_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x2[].masked_select", "utils.to_cuda", "encoder", "enc1.transpose.transpose.transpose", "encoder_para", "enc2.transpose.transpose.transpose", "os.path.join", "utils.restore_segmentation", "evaluator.eval_moses_bleu", "logger.info", "len2.max", "len", "enc1.transpose.transpose.half", "enc2.transpose.transpose.half", "int", "hypothesis.extend", "open", "f.write", "x1.clone", "x2.clone", "evaluator.BinaryEnsembleDecoder.generate", "evaluator.BinaryEnsembleDecoder.generate_beam", "evaluator.convert_to_text", "len1.max().item", "len1.max"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.glue.GLUE.eval", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecParaPretrainedEvaluator.get_iterator", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.to_cuda", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.src.utils.restore_segmentation", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_moses_bleu", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MultiEnsembleDecoder.generate_beam", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text"], ["", "def", "evaluate_mt_ensemble_x2", "(", "self", ",", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ",", "resolve_segmentation", "=", "None", ",", "\n", "infer_train", "=", "False", ")", ":", "\n", "        ", "params", "=", "self", ".", "params", "\n", "infer_name", "=", "params", ".", "infer_name", "\n", "descending", "=", "getattr", "(", "params", ",", "'order_descending'", ",", "False", ")", "\n", "logger", ".", "info", "(", "'eval_mt_ensemble 2x: Order descending: {}'", ".", "format", "(", "descending", ")", ")", "\n", "assert", "infer_name", "!=", "''", "\n", "assert", "(", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ")", "or", "infer_train", "\n", "assert", "lang1", "in", "params", ".", "langs", "\n", "assert", "lang2", "in", "params", ".", "langs", "\n", "# assert lang3 in params.langs", "\n", "\n", "self", ".", "encoder", ".", "eval", "(", ")", "\n", "self", ".", "decoder", ".", "eval", "(", ")", "\n", "self", ".", "encoder_para", ".", "eval", "(", ")", "\n", "self", ".", "decoder_para", ".", "eval", "(", ")", "\n", "encoder", "=", "self", ".", "encoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder", "\n", "decoder", "=", "self", ".", "decoder", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder", "\n", "try", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", ".", "module", "if", "params", ".", "multi_gpu", "else", "self", ".", "decoder_para", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "encoder_para", "=", "self", ".", "encoder_para", "\n", "decoder_para", "=", "self", ".", "decoder_para", "\n", "\n", "", "decoder_wrapper", "=", "BinaryEnsembleDecoder", "(", "decoder", ",", "decoder_para", ")", "\n", "\n", "params", "=", "params", "\n", "lang1_id", "=", "params", ".", "lang2id", "[", "lang1", "]", "\n", "lang2_id", "=", "params", ".", "lang2id", "[", "lang2", "]", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "all_mem_att2", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "", "n_words", "=", "0", "\n", "xe_loss", "=", "0", "\n", "n_valid", "=", "0", "\n", "\n", "# only save states / evaluate usage on the validation set", "\n", "eval_memory", "=", "params", ".", "use_memory", "and", "data_set", "==", "'valid'", "and", "self", ".", "params", ".", "is_master", "\n", "HashingMemory", ".", "EVAL_MEMORY", "=", "eval_memory", "\n", "if", "eval_memory", ":", "\n", "            ", "all_mem_att", "=", "{", "k", ":", "[", "]", "for", "k", ",", "_", "in", "self", ".", "memory_list", "}", "\n", "\n", "# store hypothesis to compute BLEU score", "\n", "", "if", "eval_bleu", ":", "\n", "            ", "hypothesis", "=", "[", "]", "\n", "\n", "", "for", "index", ",", "batch", "in", "enumerate", "(", "self", ".", "get_iterator", "(", "\n", "data_set", ",", "lang1", ",", "lang2", ",", "allow_train_data", "=", "infer_train", ",", "descending", "=", "descending", ")", ")", ":", "\n", "# generate batch", "\n", "            ", "(", "x1", ",", "len1", ")", ",", "(", "x2", ",", "len2", ")", "=", "batch", "\n", "langs1", "=", "x1", ".", "clone", "(", ")", ".", "fill_", "(", "lang1_id", ")", "\n", "langs2", "=", "x2", ".", "clone", "(", ")", ".", "fill_", "(", "lang2_id", ")", "\n", "\n", "# target words to predict", "\n", "alen", "=", "torch", ".", "arange", "(", "len2", ".", "max", "(", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "len2", ".", "device", ")", "\n", "pred_mask", "=", "alen", "[", ":", ",", "None", "]", "<", "len2", "[", "None", "]", "-", "1", "# do not predict anything given the last target word", "\n", "y", "=", "x2", "[", "1", ":", "]", ".", "masked_select", "(", "pred_mask", "[", ":", "-", "1", "]", ")", "\n", "assert", "len", "(", "y", ")", "==", "(", "len2", "-", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# cuda", "\n", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", "=", "to_cuda", "(", "x1", ",", "len1", ",", "langs1", ",", "x2", ",", "len2", ",", "langs2", ",", "y", ")", "\n", "\n", "# encode source sentences", "\n", "enc1", "=", "encoder", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc1", "=", "enc1", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc1", "=", "enc1", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc1", "\n", "enc2", "=", "encoder_para", "(", "'fwd'", ",", "x", "=", "x1", ",", "lengths", "=", "len1", ",", "langs", "=", "langs1", ",", "causal", "=", "False", ")", "\n", "enc2", "=", "enc2", ".", "transpose", "(", "0", ",", "1", ")", "\n", "enc2", "=", "enc2", ".", "half", "(", ")", "if", "params", ".", "fp16", "else", "enc2", "\n", "\n", "# # decode target sentence", "\n", "# dec2 = decoder('fwd', x=x2, lengths=len2, langs=langs2, causal=True, src_enc=enc1, src_len=len1)", "\n", "\n", "if", "eval_bleu", ":", "\n", "                ", "max_len", "=", "int", "(", "1.5", "*", "len1", ".", "max", "(", ")", ".", "item", "(", ")", "+", "10", ")", "\n", "if", "params", ".", "beam_size", "==", "1", ":", "\n", "                    ", "generated", ",", "lengths", "=", "decoder_wrapper", ".", "generate", "(", "enc1", ",", "enc2", ",", "len1", ",", "lang2_id", ",", "max_len", "=", "max_len", ")", "\n", "", "else", ":", "\n", "                    ", "generated", ",", "lengths", "=", "decoder_wrapper", ".", "generate_beam", "(", "\n", "enc1", ",", "enc2", ",", "len1", ",", "lang2_id", ",", "beam_size", "=", "params", ".", "beam_size", ",", "\n", "length_penalty", "=", "params", ".", "length_penalty", ",", "\n", "early_stopping", "=", "params", ".", "early_stopping", ",", "\n", "max_len", "=", "max_len", "\n", ")", "\n", "# raise NotImplementedError", "\n", "", "hypothesis", ".", "extend", "(", "convert_to_text", "(", "generated", ",", "lengths", ",", "self", ".", "dico", ",", "params", ")", ")", "\n", "\n", "", "", "if", "eval_bleu", ":", "\n", "# hypothesis / reference paths", "\n", "            ", "hyp_name", "=", "'hyp{0}.{1}-{2}.{3}.txt'", ".", "format", "(", "scores", "[", "'epoch'", "]", ",", "lang1", ",", "lang2", ",", "data_set", ")", "\n", "hyp_path", "=", "os", ".", "path", ".", "join", "(", "params", ".", "hyp_path", ",", "hyp_name", ")", "\n", "ref_path", "=", "params", ".", "ref_paths", "[", "(", "lang1", ",", "lang2", ",", "data_set", ")", "]", "\n", "\n", "# export sentences to hypothesis file / restore BPE segmentation", "\n", "with", "open", "(", "hyp_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "hypothesis", ")", "+", "'\\n'", ")", "\n", "", "restore_segmentation", "(", "hyp_path", ",", "sentencepiece", "=", "self", ".", "is_sentencepiece", ")", "\n", "\n", "# evaluate BLEU score", "\n", "bleu", "=", "eval_moses_bleu", "(", "ref_path", ",", "hyp_path", ")", "\n", "logger", ".", "info", "(", "\"BLEU %s %s : %f\"", "%", "(", "hyp_path", ",", "ref_path", ",", "bleu", ")", ")", "\n", "scores", "[", "'%s_%s-%s_mt_bleu'", "%", "(", "data_set", ",", "lang1", ",", "lang2", ")", "]", "=", "bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__": [[2377, 2379], ["evaluator.EncDecEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "trainer", ",", "data", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "trainer", ",", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.MACDOnlineEvaluator.run_all_evals": [[2380, 2432], ["collections.OrderedDict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "evaluator.MACDOnlineEvaluator.evaluate_mt", "len", "numpy.mean", "numpy.mean", "len", "numpy.mean", "numpy.mean", "evaluator.MACDOnlineEvaluator.evaluate_mt", "evaluator.MACDOnlineEvaluator.evaluate_mt", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.evaluate_mt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.evaluate_mt", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.EncDecEvaluator.evaluate_mt"], ["", "def", "run_all_evals", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "\"\"\"\n        Run all evaluations.\n        \"\"\"", "\n", "params", "=", "self", ".", "params", "\n", "scores", "=", "OrderedDict", "(", "{", "'epoch'", ":", "trainer", ".", "epoch", ",", "'iter'", ":", "trainer", ".", "n_total_iter", "}", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "            ", "for", "data_set", "in", "[", "'valid'", ",", "'test'", "]", ":", "\n", "\n", "# causal prediction task (evaluate perplexity and accuracy)", "\n", "# for lang1, lang2 in params.clm_steps:", "\n", "#     self.evaluate_clm(scores, data_set, lang1, lang2)", "\n", "\n", "# prediction task (evaluate perplexity and accuracy)", "\n", "# for lang1, lang2 in params.mlm_steps:", "\n", "#     self.evaluate_mlm(scores, data_set, lang1, lang2)", "\n", "\n", "# machine translation task (evaluate perplexity and accuracy)", "\n", "# for lang1, lang2 in set(params.mt_steps + [(l2, l3) for _, l2, l3 in params.bt_steps]):", "\n", "                ", "lang1", "=", "params", ".", "src_lang", "\n", "lang2", "=", "params", ".", "tgt_lang", "\n", "\n", "eval_bleu", "=", "params", ".", "eval_bleu", "and", "params", ".", "is_master", "\n", "\n", "if", "params", ".", "macd_version", "==", "1", ":", "\n", "                    ", "self", ".", "evaluate_mt", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ")", "\n", "", "elif", "params", ".", "macd_version", ">=", "2", ":", "\n", "                    ", "self", ".", "evaluate_mt", "(", "scores", ",", "data_set", ",", "lang1", ",", "lang2", ",", "eval_bleu", ")", "\n", "self", ".", "evaluate_mt", "(", "scores", ",", "data_set", ",", "lang2", ",", "lang1", ",", "eval_bleu", ")", "\n", "# elif params.macd_version == 3:", "\n", "#     self.evaluate_mt(scores, data_set, lang1, lang2, eval_bleu)", "\n", "#     self.evaluate_mt(scores, data_set, lang2, lang1, eval_bleu)", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "'macd version wrong'", ")", "\n", "\n", "# report average metrics per language", "\n", "", "_clm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "clm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_clm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_clm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "scores", "[", "'%s_clm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_clm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_clm_mono", "]", ")", "\n", "", "_mlm_mono", "=", "[", "l1", "for", "(", "l1", ",", "l2", ")", "in", "params", ".", "mlm_steps", "if", "l2", "is", "None", "]", "\n", "if", "len", "(", "_mlm_mono", ")", ">", "0", ":", "\n", "                    ", "scores", "[", "'%s_mlm_ppl'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_ppl'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "scores", "[", "'%s_mlm_acc'", "%", "data_set", "]", "=", "np", ".", "mean", "(", "\n", "[", "scores", "[", "'%s_%s_mlm_acc'", "%", "(", "data_set", ",", "lang", ")", "]", "for", "lang", "in", "_mlm_mono", "]", ")", "\n", "\n", "", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.kl_score": [[28, 33], ["x.copy", "numpy.log", "len", "numpy.log"], "function", ["None"], ["def", "kl_score", "(", "x", ")", ":", "\n", "# assert np.abs(np.sum(x) - 1) < 1e-5", "\n", "    ", "_x", "=", "x", ".", "copy", "(", ")", "\n", "_x", "[", "x", "==", "0", "]", "=", "1", "\n", "return", "np", ".", "log", "(", "len", "(", "x", ")", ")", "+", "(", "x", "*", "np", ".", "log", "(", "_x", ")", ")", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.gini_score": [[35, 39], ["numpy.cumsum().mean", "numpy.cumsum", "numpy.sort"], "function", ["None"], ["", "def", "gini_score", "(", "x", ")", ":", "\n", "# assert np.abs(np.sum(x) - 1) < 1e-5", "\n", "    ", "B", "=", "np", ".", "cumsum", "(", "np", ".", "sort", "(", "x", ")", ")", ".", "mean", "(", ")", "\n", "return", "1", "-", "2", "*", "B", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.tops": [[41, 46], ["numpy.cumsum", "numpy.sort", "numpy.searchsorted"], "function", ["None"], ["", "def", "tops", "(", "x", ")", ":", "\n", "# assert np.abs(np.sum(x) - 1) < 1e-5", "\n", "    ", "y", "=", "np", ".", "cumsum", "(", "np", ".", "sort", "(", "x", ")", ")", "\n", "top50", ",", "top90", ",", "top99", "=", "y", ".", "shape", "[", "0", "]", "-", "np", ".", "searchsorted", "(", "y", ",", "[", "0.5", ",", "0.1", ",", "0.01", "]", ")", "\n", "return", "top50", ",", "top90", ",", "top99", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_memory_usage": [[48, 84], ["numpy.zeros", "numpy.zeros", "float", "float", "float", "float", "float", "evaluator.tops", "float", "float", "float", "evaluator.tops", "float", "float", "float", "numpy.add.at", "numpy.add.at", "np.zeros.sum", "np.zeros.sum", "evaluator.kl_score", "evaluator.kl_score", "evaluator.gini_score", "evaluator.gini_score", "len"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.tops", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.tops", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.kl_score", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.kl_score", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.gini_score", "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.gini_score"], ["", "def", "eval_memory_usage", "(", "scores", ",", "name", ",", "mem_att", ",", "mem_size", ")", ":", "\n", "    ", "\"\"\"\n    Evaluate memory usage (HashingMemory / FFN).\n    \"\"\"", "\n", "# memory slot scores", "\n", "assert", "mem_size", ">", "0", "\n", "mem_scores_w", "=", "np", ".", "zeros", "(", "mem_size", ",", "dtype", "=", "np", ".", "float32", ")", "# weighted scores", "\n", "mem_scores_u", "=", "np", ".", "zeros", "(", "mem_size", ",", "dtype", "=", "np", ".", "float32", ")", "# unweighted scores", "\n", "\n", "# sum each slot usage", "\n", "for", "indices", ",", "weights", "in", "mem_att", ":", "\n", "        ", "np", ".", "add", ".", "at", "(", "mem_scores_w", ",", "indices", ",", "weights", ")", "\n", "np", ".", "add", ".", "at", "(", "mem_scores_u", ",", "indices", ",", "1", ")", "\n", "\n", "# compute the KL distance to the uniform distribution", "\n", "", "mem_scores_w", "=", "mem_scores_w", "/", "mem_scores_w", ".", "sum", "(", ")", "\n", "mem_scores_u", "=", "mem_scores_u", "/", "mem_scores_u", ".", "sum", "(", ")", "\n", "\n", "# store stats", "\n", "scores", "[", "'%s_mem_used'", "%", "name", "]", "=", "float", "(", "100", "*", "(", "mem_scores_w", "!=", "0", ")", ".", "sum", "(", ")", "/", "len", "(", "mem_scores_w", ")", ")", "\n", "\n", "scores", "[", "'%s_mem_kl_w'", "%", "name", "]", "=", "float", "(", "kl_score", "(", "mem_scores_w", ")", ")", "\n", "scores", "[", "'%s_mem_kl_u'", "%", "name", "]", "=", "float", "(", "kl_score", "(", "mem_scores_u", ")", ")", "\n", "\n", "scores", "[", "'%s_mem_gini_w'", "%", "name", "]", "=", "float", "(", "gini_score", "(", "mem_scores_w", ")", ")", "\n", "scores", "[", "'%s_mem_gini_u'", "%", "name", "]", "=", "float", "(", "gini_score", "(", "mem_scores_u", ")", ")", "\n", "\n", "top50", ",", "top90", ",", "top99", "=", "tops", "(", "mem_scores_w", ")", "\n", "scores", "[", "'%s_mem_top50_w'", "%", "name", "]", "=", "float", "(", "top50", ")", "\n", "scores", "[", "'%s_mem_top90_w'", "%", "name", "]", "=", "float", "(", "top90", ")", "\n", "scores", "[", "'%s_mem_top99_w'", "%", "name", "]", "=", "float", "(", "top99", ")", "\n", "\n", "top50", ",", "top90", ",", "top99", "=", "tops", "(", "mem_scores_u", ")", "\n", "scores", "[", "'%s_mem_top50_u'", "%", "name", "]", "=", "float", "(", "top50", ")", "\n", "scores", "[", "'%s_mem_top90_u'", "%", "name", "]", "=", "float", "(", "top90", ")", "\n", "scores", "[", "'%s_mem_top99_u'", "%", "name", "]", "=", "float", "(", "top99", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.convert_to_text": [[2433, 2454], ["batch.cpu().numpy.cpu().numpy", "lengths.cpu().numpy.cpu().numpy", "range", "range", "sentences.append", "batch.cpu().numpy.cpu", "lengths.cpu().numpy.cpu", "words.append", "lengths.cpu().numpy.max"], "function", ["None"], ["", "", "def", "convert_to_text", "(", "batch", ",", "lengths", ",", "dico", ",", "params", ",", "nbest", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert a batch of sentences to a list of text sentences.\n    \"\"\"", "\n", "batch", "=", "batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "lengths", "=", "lengths", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "slen", ",", "bs", "=", "batch", ".", "shape", "\n", "assert", "nbest", "is", "not", "None", "or", "(", "lengths", ".", "max", "(", ")", "==", "slen", "and", "lengths", ".", "shape", "[", "0", "]", "==", "bs", ")", "\n", "assert", "(", "batch", "[", "0", "]", "==", "params", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "bs", "\n", "assert", "(", "batch", "==", "params", ".", "eos_index", ")", ".", "sum", "(", ")", "==", "2", "*", "bs", "\n", "sentences", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "bs", ")", ":", "\n", "        ", "words", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "lengths", "[", "j", "]", ")", ":", "\n", "            ", "if", "batch", "[", "k", ",", "j", "]", "==", "params", ".", "eos_index", ":", "\n", "                ", "break", "\n", "", "words", ".", "append", "(", "dico", "[", "batch", "[", "k", ",", "j", "]", "]", ")", "\n", "", "sentences", ".", "append", "(", "\" \"", ".", "join", "(", "words", ")", ")", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.evaluation.evaluator.eval_moses_bleu": [[2456, 2472], ["os.path.isfile", "os.path.isfile", "subprocess.Popen", "[].decode", "[].decode.startswith", "os.path.isfile", "os.path.isfile", "float", "logger.warning", "subprocess.Popen.communicate", "[].decode.index"], "function", ["home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.data.dictionary.Dictionary.index"], ["", "def", "eval_moses_bleu", "(", "ref", ",", "hyp", ")", ":", "\n", "    ", "\"\"\"\n    Given a file of hypothesis and reference files,\n    evaluate the BLEU score using Moses scripts.\n    \"\"\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "hyp", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "ref", ")", "or", "os", ".", "path", ".", "isfile", "(", "ref", "+", "'0'", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "BLEU_SCRIPT_PATH", ")", "\n", "command", "=", "BLEU_SCRIPT_PATH", "+", "' %s < %s'", "\n", "p", "=", "subprocess", ".", "Popen", "(", "command", "%", "(", "ref", ",", "hyp", ")", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "shell", "=", "True", ")", "\n", "result", "=", "p", ".", "communicate", "(", ")", "[", "0", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "if", "result", ".", "startswith", "(", "'BLEU'", ")", ":", "\n", "        ", "return", "float", "(", "result", "[", "7", ":", "result", ".", "index", "(", "','", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "warning", "(", "'Impossible to parse BLEU score! \"%s\"'", "%", "result", ")", "\n", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.tools.lowercase_and_remove_accent.convert_to_unicode": [[13, 27], ["lowercase_and_remove_accent.convert_to_unicode.six_ensure_text"], "function", ["None"], ["def", "convert_to_unicode", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Converts `text` to Unicode (if it's not already), assuming UTF-8 input.\n    \"\"\"", "\n", "# six_ensure_text is copied from https://github.com/benjaminp/six", "\n", "def", "six_ensure_text", "(", "s", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'strict'", ")", ":", "\n", "        ", "if", "isinstance", "(", "s", ",", "six", ".", "binary_type", ")", ":", "\n", "            ", "return", "s", ".", "decode", "(", "encoding", ",", "errors", ")", "\n", "", "elif", "isinstance", "(", "s", ",", "six", ".", "text_type", ")", ":", "\n", "            ", "return", "s", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"not expecting type '%s'\"", "%", "type", "(", "s", ")", ")", "\n", "\n", "", "", "return", "six_ensure_text", "(", "text", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nxphi47_multiagent_crosstranslate.tools.lowercase_and_remove_accent.run_strip_accents": [[29, 41], ["unicodedata.normalize", "unicodedata.category", "output.append"], "function", ["None"], ["", "def", "run_strip_accents", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Strips accents from a piece of text.\n    \"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "        ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "            ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]]}