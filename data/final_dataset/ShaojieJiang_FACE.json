{"home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.HLoss.__init__": [[60, 63], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "ignore_index", "=", "-", "1", ")", ":", "\n", "        ", "super", "(", "HLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.HLoss.forward": [[64, 69], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "b.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "labels", ")", ":", "\n", "        ", "mask", "=", "(", "labels", "!=", "self", ".", "ignore_index", ")", ".", "float", "(", ")", "\n", "b", "=", "F", ".", "softmax", "(", "x", ",", "dim", "=", "1", ")", "*", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "1", ")", "\n", "b", "=", "-", "1.0", "*", "torch", ".", "matmul", "(", "mask", ",", "b", ".", "sum", "(", "dim", "=", "1", ")", ")", "\n", "return", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.__init__": [[76, 128], ["parlai.core.torch_generator_agent.TorchGeneratorModel.__init__", "modules.RNNDecoder", "modules.RNNEncoder", "modules.OutputLayer"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "\n", "self", ",", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "numlayers", "=", "2", ",", "dropout", "=", "0", ",", "\n", "bidirectional", "=", "False", ",", "rnn_class", "=", "'lstm'", ",", "lookuptable", "=", "'unique'", ",", "\n", "decoder", "=", "'same'", ",", "numsoftmax", "=", "1", ",", "\n", "attention", "=", "'none'", ",", "attention_length", "=", "48", ",", "attention_time", "=", "'post'", ",", "\n", "padding_idx", "=", "0", ",", "start_idx", "=", "1", ",", "end_idx", "=", "2", ",", "unknown_idx", "=", "3", ",", "input_dropout", "=", "0", ",", "\n", "longest_label", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize seq2seq model.\n\n        See cmdline args in Seq2seqAgent for description of arguments.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "padding_idx", "=", "padding_idx", ",", "\n", "start_idx", "=", "start_idx", ",", "\n", "end_idx", "=", "end_idx", ",", "\n", "unknown_idx", "=", "unknown_idx", ",", "\n", "input_dropout", "=", "input_dropout", ",", "\n", "longest_label", "=", "longest_label", ",", "\n", ")", "\n", "self", ".", "attn_type", "=", "attention", "\n", "\n", "rnn_class", "=", "Seq2seq", ".", "RNN_OPTS", "[", "rnn_class", "]", "\n", "self", ".", "decoder", "=", "RNNDecoder", "(", "\n", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "\n", "padding_idx", "=", "padding_idx", ",", "rnn_class", "=", "rnn_class", ",", "\n", "numlayers", "=", "numlayers", ",", "dropout", "=", "dropout", ",", "\n", "attn_type", "=", "attention", ",", "attn_length", "=", "attention_length", ",", "\n", "attn_time", "=", "attention_time", ",", "\n", "bidir_input", "=", "bidirectional", ")", "\n", "\n", "shared_lt", "=", "(", "self", ".", "decoder", ".", "lt", "# share embeddings between rnns", "\n", "if", "lookuptable", "in", "(", "'enc_dec'", ",", "'all'", ")", "else", "None", ")", "\n", "shared_rnn", "=", "self", ".", "decoder", ".", "rnn", "if", "decoder", "==", "'shared'", "else", "None", "\n", "self", ".", "encoder", "=", "RNNEncoder", "(", "\n", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "\n", "padding_idx", "=", "padding_idx", ",", "rnn_class", "=", "rnn_class", ",", "\n", "numlayers", "=", "numlayers", ",", "dropout", "=", "dropout", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", "shared_lt", "=", "shared_lt", ",", "shared_rnn", "=", "shared_rnn", ",", "\n", "unknown_idx", "=", "unknown_idx", ",", "input_dropout", "=", "input_dropout", ")", "\n", "\n", "shared_weight", "=", "(", "self", ".", "decoder", ".", "lt", "# use embeddings for projection", "\n", "if", "lookuptable", "in", "(", "'dec_out'", ",", "'all'", ")", "else", "None", ")", "\n", "\n", "self", ".", "output", "=", "OutputLayer", "(", "num_features", ",", "embeddingsize", ",", "\n", "hiddensize", ",", "\n", "dropout", "=", "dropout", ",", "\n", "numsoftmax", "=", "numsoftmax", ",", "\n", "shared_weight", "=", "shared_weight", ",", "\n", "padding_idx", "=", "padding_idx", ",", "\n", "unknown_idx", "=", "unknown_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.reorder_encoder_states": [[129, 161], ["modules._transpose_hidden_state", "isinstance", "hid.index_select.index_select.index_select", "modules._transpose_hidden_state", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "cell.index_select.index_select.index_select", "enc_out.index_select.index_select.index_select", "attn_mask.index_select.index_select.index_select", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state"], ["", "def", "reorder_encoder_states", "(", "self", ",", "encoder_states", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Reorder encoder states according to a new set of indices.\"\"\"", "\n", "enc_out", ",", "hidden", ",", "attn_mask", "=", "encoder_states", "\n", "\n", "# make sure we swap the hidden state around, apropos multigpu settings", "\n", "hidden", "=", "_transpose_hidden_state", "(", "hidden", ")", "\n", "\n", "# LSTM or GRU/RNN hidden state?", "\n", "if", "isinstance", "(", "hidden", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "hid", ",", "cell", "=", "hidden", ",", "None", "\n", "", "else", ":", "\n", "            ", "hid", ",", "cell", "=", "hidden", "\n", "\n", "", "if", "not", "torch", ".", "is_tensor", "(", "indices", ")", ":", "\n", "# cast indices to a tensor if needed", "\n", "            ", "indices", "=", "torch", ".", "LongTensor", "(", "indices", ")", ".", "to", "(", "hid", ".", "device", ")", "\n", "\n", "", "hid", "=", "hid", ".", "index_select", "(", "1", ",", "indices", ")", "\n", "if", "cell", "is", "None", ":", "\n", "            ", "hidden", "=", "hid", "\n", "", "else", ":", "\n", "            ", "cell", "=", "cell", ".", "index_select", "(", "1", ",", "indices", ")", "\n", "hidden", "=", "(", "hid", ",", "cell", ")", "\n", "\n", "", "if", "self", ".", "attn_type", "!=", "'none'", ":", "\n", "            ", "enc_out", "=", "enc_out", ".", "index_select", "(", "0", ",", "indices", ")", "\n", "attn_mask", "=", "attn_mask", ".", "index_select", "(", "0", ",", "indices", ")", "\n", "\n", "# and bring it back to multigpu friendliness", "\n", "", "hidden", "=", "_transpose_hidden_state", "(", "hidden", ")", "\n", "\n", "return", "enc_out", ",", "hidden", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.reorder_decoder_incremental_state": [[162, 170], ["torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "torch.index_select().contiguous", "isinstance", "tuple", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "modules.Seq2seq.reorder_decoder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.reorder_decoder_incremental_state"], ["", "def", "reorder_decoder_incremental_state", "(", "self", ",", "incremental_state", ",", "inds", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "incremental_state", ")", ":", "\n", "# gru or vanilla rnn", "\n", "            ", "return", "torch", ".", "index_select", "(", "incremental_state", ",", "0", ",", "inds", ")", ".", "contiguous", "(", ")", "\n", "", "elif", "isinstance", "(", "incremental_state", ",", "tuple", ")", ":", "\n", "            ", "return", "tuple", "(", "\n", "self", ".", "reorder_decoder_incremental_state", "(", "x", ",", "inds", ")", "\n", "for", "x", "in", "incremental_state", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.forward": [[172, 190], ["max", "modules.Seq2seq.encoder", "modules.Seq2seq.decode_forced", "xs.size", "modules.Seq2seq.decode_greedy", "ys.size"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.decode_forced", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.decode_greedy"], ["", "", "def", "forward", "(", "self", ",", "xs", ",", "ys", "=", "None", ",", "cand_params", "=", "None", ",", "prev_enc", "=", "None", ",", "maxlen", "=", "None", ")", ":", "\n", "        ", "if", "ys", "is", "not", "None", ":", "\n", "            ", "self", ".", "longest_label", "=", "max", "(", "self", ".", "longest_label", ",", "ys", ".", "size", "(", "1", ")", ")", "\n", "\n", "# use cached encoding if available", "\n", "", "encoder_states", "=", "prev_enc", "if", "prev_enc", "is", "not", "None", "else", "self", ".", "encoder", "(", "xs", ")", "\n", "if", "ys", "is", "not", "None", ":", "\n", "# use teacher forcing", "\n", "            ", "logits", ",", "preds", "=", "self", ".", "decode_forced", "(", "encoder_states", ",", "ys", ")", "\n", "", "else", ":", "\n", "            ", "bsz", "=", "xs", ".", "size", "(", "0", ")", "\n", "logits", ",", "preds", "=", "self", ".", "decode_greedy", "(", "\n", "encoder_states", ",", "\n", "bsz", ",", "\n", "maxlen", "or", "self", ".", "longest_label", "\n", ")", "\n", "\n", "", "return", "logits", ",", "preds", ",", "encoder_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.decode_forced": [[191, 200], ["ys.size", "ys.size", "ys.narrow", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modules.Seq2seq.decoder", "modules.Seq2seq.output", "modules.Seq2seq.max", "modules.Seq2seq._starts"], "methods", ["None"], ["", "def", "decode_forced", "(", "self", ",", "encoder_states", ",", "ys", ")", ":", "\n", "        ", "bsz", "=", "ys", ".", "size", "(", "0", ")", "\n", "seqlen", "=", "ys", ".", "size", "(", "1", ")", "\n", "inputs", "=", "ys", ".", "narrow", "(", "1", ",", "0", ",", "seqlen", "-", "1", ")", "\n", "inputs", "=", "torch", ".", "cat", "(", "[", "self", ".", "_starts", "(", "bsz", ")", ",", "inputs", "]", ",", "1", ")", "\n", "dec_out", ",", "_", "=", "self", ".", "decoder", "(", "inputs", ",", "encoder_states", ")", "\n", "logits", "=", "self", ".", "output", "(", "dec_out", ",", "forced", "=", "True", ")", "\n", "_", ",", "preds", "=", "logits", ".", "max", "(", "dim", "=", "2", ")", "\n", "return", "logits", ",", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.decode_greedy": [[201, 221], ["modules.Seq2seq._starts", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modules.Seq2seq.decoder", "modules.Seq2seq.output", "modules.Seq2seq.max", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "decode_greedy", "(", "self", ",", "encoder_states", ",", "bsz", ",", "maxlen", ")", ":", "\n", "        ", "xs", "=", "self", ".", "_starts", "(", "bsz", ")", "\n", "incr_state", "=", "None", "\n", "logits", "=", "[", "]", "\n", "# generate template", "\n", "for", "i", "in", "range", "(", "maxlen", ")", ":", "\n", "# todo, break early if all beams saw EOS", "\n", "            ", "dec_out", ",", "incr_state", "=", "self", ".", "decoder", "(", "xs", ",", "encoder_states", ",", "incremental_state", "=", "incr_state", ")", "\n", "scores", "=", "self", ".", "output", "(", "dec_out", ")", "\n", "_", ",", "preds", "=", "scores", ".", "max", "(", "dim", "=", "-", "1", ")", "\n", "logits", ".", "append", "(", "scores", ")", "\n", "xs", "=", "torch", ".", "cat", "(", "[", "xs", ",", "preds", "]", ",", "dim", "=", "1", ")", "\n", "# check if everyone has generated an end token", "\n", "all_finished", "=", "(", "(", "xs", "==", "self", ".", "END_IDX", ")", ".", "sum", "(", "dim", "=", "1", ")", ">", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "bsz", "\n", "if", "all_finished", ":", "\n", "                ", "break", "\n", "", "", "logits", "=", "torch", ".", "cat", "(", "logits", ",", "1", ")", "\n", "preds", "=", "xs", "\n", "\n", "return", "logits", ",", "preds", "[", ":", ",", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.UnknownDropout.__init__": [[230, 240], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "unknown_idx", ",", "probability", ")", ":", "\n", "        ", "\"\"\"Initialize layer.\n\n        :param unknown_idx: index of unknown token, replace tokens with this\n        :param probability: during training, replaces tokens with unknown token\n                            at this rate.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "unknown_idx", "=", "unknown_idx", "\n", "self", ".", "prob", "=", "probability", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.UnknownDropout.forward": [[241, 247], ["input.masked_fill_", "input.new().float().uniform_", "input.new().float", "input.new", "input.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"If training and dropout rate > 0, masks input with unknown token.\"\"\"", "\n", "if", "self", ".", "training", "and", "self", ".", "prob", ">", "0", ":", "\n", "            ", "mask", "=", "input", ".", "new", "(", "input", ".", "size", "(", ")", ")", ".", "float", "(", ")", ".", "uniform_", "(", "0", ",", "1", ")", "<", "self", ".", "prob", "\n", "input", ".", "masked_fill_", "(", "mask", ",", "self", ".", "unknown_idx", ")", "\n", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.RNNEncoder.__init__": [[252, 283], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "modules.UnknownDropout", "RuntimeError", "torch.Embedding", "torch.Embedding", "torch.Embedding", "rnn_class", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "\n", "padding_idx", "=", "0", ",", "rnn_class", "=", "'lstm'", ",", "numlayers", "=", "2", ",", "dropout", "=", "0.1", ",", "\n", "bidirectional", "=", "False", ",", "shared_lt", "=", "None", ",", "shared_rnn", "=", "None", ",", "\n", "input_dropout", "=", "0", ",", "unknown_idx", "=", "None", ",", "sparse", "=", "False", ")", ":", "\n", "        ", "\"\"\"Initialize recurrent encoder.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "layers", "=", "numlayers", "\n", "self", ".", "dirs", "=", "2", "if", "bidirectional", "else", "1", "\n", "self", ".", "hsz", "=", "hiddensize", "\n", "\n", "if", "input_dropout", ">", "0", "and", "unknown_idx", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "'input_dropout > 0 but unknown_idx not set'", ")", "\n", "", "self", ".", "input_dropout", "=", "UnknownDropout", "(", "unknown_idx", ",", "input_dropout", ")", "\n", "\n", "if", "shared_lt", "is", "None", ":", "\n", "            ", "self", ".", "lt", "=", "nn", ".", "Embedding", "(", "num_features", ",", "embeddingsize", ",", "\n", "padding_idx", "=", "padding_idx", ",", "\n", "sparse", "=", "sparse", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lt", "=", "shared_lt", "\n", "\n", "", "if", "shared_rnn", "is", "None", ":", "\n", "            ", "self", ".", "rnn", "=", "rnn_class", "(", "embeddingsize", ",", "hiddensize", ",", "numlayers", ",", "\n", "dropout", "=", "dropout", "if", "numlayers", ">", "1", "else", "0", ",", "\n", "batch_first", "=", "True", ",", "bidirectional", "=", "bidirectional", ")", "\n", "", "elif", "bidirectional", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Cannot share decoder with bidir encoder.'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn", "=", "shared_rnn", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.RNNEncoder.forward": [[284, 326], ["len", "modules.RNNEncoder.input_dropout", "modules.RNNEncoder.dropout", "modules.RNNEncoder.ne", "modules.RNNEncoder.rnn", "modules.RNNEncoder.lt", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "isinstance", "modules._transpose_hidden_state", "modules.RNNEncoder.ne.int", "hidden.view().sum.view().sum.view().sum", "modules.RNNEncoder.size", "hidden[].view().sum", "hidden[].view().sum", "hidden.view().sum.view().sum.view", "hidden[].view", "hidden[].view"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state"], ["", "", "def", "forward", "(", "self", ",", "xs", ")", ":", "\n", "        ", "\"\"\"Encode sequence.\n\n        :param xs: (bsz x seqlen) LongTensor of input token indices\n\n        :returns: encoder outputs, hidden state, attention mask\n            encoder outputs are the output state at each step of the encoding.\n            the hidden state is the final hidden state of the encoder.\n            the attention mask is a mask of which input values are nonzero.\n        \"\"\"", "\n", "bsz", "=", "len", "(", "xs", ")", "\n", "\n", "# embed input tokens", "\n", "xs", "=", "self", ".", "input_dropout", "(", "xs", ")", "\n", "xes", "=", "self", ".", "dropout", "(", "self", ".", "lt", "(", "xs", ")", ")", "\n", "attn_mask", "=", "xs", ".", "ne", "(", "0", ")", "\n", "try", ":", "\n", "            ", "x_lens", "=", "torch", ".", "sum", "(", "attn_mask", ".", "int", "(", ")", ",", "dim", "=", "1", ")", "\n", "xes", "=", "pack_padded_sequence", "(", "xes", ",", "x_lens", ",", "batch_first", "=", "True", ")", "\n", "packed", "=", "True", "\n", "", "except", "ValueError", ":", "\n", "# packing failed, don't pack then", "\n", "            ", "packed", "=", "False", "\n", "\n", "", "encoder_output", ",", "hidden", "=", "self", ".", "rnn", "(", "xes", ")", "\n", "if", "packed", ":", "\n", "# total_length to make sure we give the proper length in the case", "\n", "# of multigpu settings.", "\n", "# https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism", "\n", "            ", "encoder_output", ",", "_", "=", "pad_packed_sequence", "(", "\n", "encoder_output", ",", "batch_first", "=", "True", ",", "total_length", "=", "xs", ".", "size", "(", "1", ")", "\n", ")", "\n", "\n", "", "if", "self", ".", "dirs", ">", "1", ":", "\n", "# project to decoder dimension by taking sum of forward and back", "\n", "            ", "if", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ":", "\n", "                ", "hidden", "=", "(", "hidden", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "dirs", ",", "bsz", ",", "self", ".", "hsz", ")", ".", "sum", "(", "1", ")", ",", "\n", "hidden", "[", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "dirs", ",", "bsz", ",", "self", ".", "hsz", ")", ".", "sum", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "hidden", "=", "hidden", ".", "view", "(", "-", "1", ",", "self", ".", "dirs", ",", "bsz", ",", "self", ".", "hsz", ")", ".", "sum", "(", "1", ")", "\n", "\n", "", "", "return", "encoder_output", ",", "_transpose_hidden_state", "(", "hidden", ")", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.RNNDecoder.__init__": [[334, 365], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "rnn_class", "modules.AttentionLayer", "torch.Embedding", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "\n", "padding_idx", "=", "0", ",", "rnn_class", "=", "'lstm'", ",", "numlayers", "=", "2", ",", "\n", "dropout", "=", "0.1", ",", "bidir_input", "=", "False", ",", "shared_lt", "=", "None", ",", "\n", "shared_rnn", "=", "None", ",", "attn_type", "=", "'none'", ",", "attn_time", "=", "'pre'", ",", "\n", "attn_length", "=", "-", "1", ",", "sparse", "=", "False", ")", ":", "\n", "        ", "\"\"\"Initialize recurrent decoder.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "layers", "=", "numlayers", "\n", "self", ".", "hsz", "=", "hiddensize", "\n", "self", ".", "esz", "=", "embeddingsize", "\n", "\n", "if", "shared_lt", "is", "None", ":", "\n", "            ", "self", ".", "lt", "=", "nn", ".", "Embedding", "(", "num_features", ",", "embeddingsize", ",", "\n", "padding_idx", "=", "padding_idx", ",", "\n", "sparse", "=", "sparse", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lt", "=", "shared_lt", "\n", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "embeddingsize", ",", "hiddensize", ",", "numlayers", ",", "\n", "dropout", "=", "dropout", "if", "numlayers", ">", "1", "else", "0", ",", "\n", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "attn_type", "=", "attn_type", "\n", "self", ".", "attn_time", "=", "attn_time", "\n", "self", ".", "attention", "=", "AttentionLayer", "(", "attn_type", "=", "attn_type", ",", "\n", "hiddensize", "=", "hiddensize", ",", "\n", "embeddingsize", "=", "embeddingsize", ",", "\n", "bidirectional", "=", "bidir_input", ",", "\n", "attn_length", "=", "attn_length", ",", "\n", "attn_time", "=", "attn_time", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.RNNDecoder.forward": [[366, 429], ["isinstance", "xs.size", "modules.RNNDecoder.dropout", "modules._transpose_hidden_state", "modules._transpose_hidden_state", "tuple", "hidden.contiguous.contiguous.contiguous", "modules.RNNDecoder.lt", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "modules.RNNDecoder.rnn", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "modules._transpose_hidden_state", "modules.RNNDecoder.attention", "new_xes.append", "modules.RNNDecoder.rnn", "modules.RNNDecoder.attention", "torch.cat().to.append", "torch.cat().to.append", "torch.cat().to.append", "x.contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "xes[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state"], ["", "def", "forward", "(", "self", ",", "xs", ",", "encoder_output", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Decode from input tokens.\n\n        :param xs: (bsz x seqlen) LongTensor of input token indices\n        :param encoder_output: output from RNNEncoder. Tuple containing\n            (enc_out, enc_hidden, attn_mask) tuple.\n        :param incremental_state: most recent hidden state to the decoder.\n            If None, the hidden state of the encoder is used as initial state,\n            and the full sequence is computed. If not None, computes only the\n            next forward in the sequence.\n\n        :returns: (output, hidden_state) pair from the RNN.\n\n            - output is a bsz x time x latentdim matrix. If incremental_state is\n                given, the time dimension will be 1. This value must be passed to\n                the model's OutputLayer for a final softmax.\n            - hidden_state depends on the choice of RNN\n        \"\"\"", "\n", "enc_state", ",", "enc_hidden", ",", "attn_mask", "=", "encoder_output", "\n", "# in case of multi gpu, we need to transpose back out the hidden state", "\n", "attn_params", "=", "(", "enc_state", ",", "attn_mask", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# we're doing it piece by piece, so we have a more important hidden", "\n", "# seed, and we only need to compute for the final timestep", "\n", "            ", "hidden", "=", "_transpose_hidden_state", "(", "incremental_state", ")", "\n", "# only need the last timestep then", "\n", "xs", "=", "xs", "[", ":", ",", "-", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "hidden", "=", "_transpose_hidden_state", "(", "enc_hidden", ")", "\n", "\n", "", "if", "isinstance", "(", "hidden", ",", "tuple", ")", ":", "\n", "            ", "hidden", "=", "tuple", "(", "x", ".", "contiguous", "(", ")", "for", "x", "in", "hidden", ")", "\n", "", "else", ":", "\n", "            ", "hidden", "=", "hidden", ".", "contiguous", "(", ")", "\n", "\n", "", "seqlen", "=", "xs", ".", "size", "(", "1", ")", "\n", "xes", "=", "self", ".", "dropout", "(", "self", ".", "lt", "(", "xs", ")", ")", "\n", "\n", "if", "self", ".", "attn_time", "==", "'pre'", ":", "\n", "# modify input vectors with attention", "\n", "# attention module requires we do this one step at a time", "\n", "            ", "new_xes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "seqlen", ")", ":", "\n", "                ", "nx", ",", "_", "=", "self", ".", "attention", "(", "xes", "[", ":", ",", "i", ":", "i", "+", "1", "]", ",", "hidden", ",", "attn_params", ")", "\n", "new_xes", ".", "append", "(", "nx", ")", "\n", "", "xes", "=", "torch", ".", "cat", "(", "new_xes", ",", "1", ")", ".", "to", "(", "xes", ".", "device", ")", "\n", "\n", "", "if", "self", ".", "attn_time", "!=", "'post'", ":", "\n", "# no attn, we can just trust the rnn to run through", "\n", "            ", "output", ",", "new_hidden", "=", "self", ".", "rnn", "(", "xes", ",", "hidden", ")", "\n", "", "else", ":", "\n", "# uh oh, post attn, we need run through one at a time, and do the", "\n", "# attention modifications", "\n", "            ", "new_hidden", "=", "hidden", "\n", "output", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "seqlen", ")", ":", "\n", "                ", "o", ",", "new_hidden", "=", "self", ".", "rnn", "(", "xes", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ",", "new_hidden", ")", "\n", "o", ",", "_", "=", "self", ".", "attention", "(", "o", ",", "new_hidden", ",", "attn_params", ")", "\n", "output", ".", "append", "(", "o", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "output", ",", "dim", "=", "1", ")", ".", "to", "(", "xes", ".", "device", ")", "\n", "\n", "", "return", "output", ",", "_transpose_hidden_state", "(", "new_hidden", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Identity.forward": [[432, 434], ["None"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.OutputLayer.__init__": [[439, 504], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "torch.zeros().byte", "math.sqrt", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.Tensor().uniform_", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Tensor().normal_", "torch.Linear", "torch.Linear", "torch.Linear", "modules.Identity", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "embeddingsize", ",", "hiddensize", ",", "\n", "dropout", "=", "0", ",", "numsoftmax", "=", "1", ",", "shared_weight", "=", "None", ",", "\n", "padding_idx", "=", "-", "1", ",", "unknown_idx", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Initialize output layer.\n\n        :param num_features:  number of candidates to rank\n        :param hiddensize:    (last) dimension of the input vectors\n        :param embeddingsize: (last) dimension of the candidate vectors\n        :param numsoftmax:   (default 1) number of softmaxes to calculate.\n                              see arxiv.org/abs/1711.03953 for more info.\n                              increasing this slows down computation but can\n                              add more expressivity to the embeddings.\n        :param shared_weight: (num_features x esz) vector of weights to use as\n                              the final linear layer's weight matrix. default\n                              None starts with a new linear layer.\n        :param padding_idx:   model should output a large negative number for\n                              score at this index. if set to -1 (default),\n                              this is disabled. if >= 0, subtracts one from\n                              num_features and always outputs -1e20 at this\n                              index. only used when shared_weight is not None.\n                              setting this param helps protect gradient from\n                              entering shared embedding matrices.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "rng", "=", "1.", "/", "math", ".", "sqrt", "(", "num_features", ")", "\n", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "num_features", ")", ".", "uniform_", "(", "-", "rng", ",", "rng", ")", ")", "\n", "\n", "self", ".", "unk_mask", "=", "torch", ".", "zeros", "(", "num_features", ")", ".", "byte", "(", ")", "\n", "self", ".", "pad_mask", "=", "torch", ".", "zeros", "(", "num_features", ")", ".", "byte", "(", ")", "\n", "if", "unknown_idx", ">=", "0", ":", "\n", "            ", "self", ".", "unk_mask", "[", "unknown_idx", "]", "=", "1", "\n", "\n", "", "if", "self", ".", "padding_idx", ">=", "0", ":", "# score for padding token will be -INF", "\n", "            ", "self", ".", "pad_mask", "[", "self", ".", "padding_idx", "]", "=", "1", "\n", "\n", "# embedding to scores", "\n", "", "if", "shared_weight", "is", "None", ":", "\n", "# just a regular linear layer", "\n", "            ", "self", ".", "shared", "=", "False", "\n", "self", ".", "weight", "=", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "num_features", ",", "embeddingsize", ")", ".", "normal_", "(", "0", ",", "1", ")", "\n", ")", "\n", "", "else", ":", "\n", "# use shared weights and a bias layer instead", "\n", "            ", "self", ".", "shared", "=", "True", "\n", "self", ".", "weight", "=", "shared_weight", ".", "weight", "\n", "\n", "", "self", ".", "numsoftmax", "=", "numsoftmax", "\n", "if", "numsoftmax", ">", "1", ":", "\n", "            ", "self", ".", "esz", "=", "embeddingsize", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "prior", "=", "nn", ".", "Linear", "(", "hiddensize", ",", "numsoftmax", ",", "bias", "=", "False", ")", "\n", "self", ".", "latent", "=", "nn", ".", "Linear", "(", "hiddensize", ",", "numsoftmax", "*", "embeddingsize", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "", "else", ":", "\n", "# rnn output to embedding", "\n", "            ", "if", "hiddensize", "!=", "embeddingsize", ":", "\n", "# learn projection to correct dimensions", "\n", "                ", "self", ".", "o2e", "=", "nn", ".", "Linear", "(", "hiddensize", ",", "embeddingsize", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "# no need for any transformation here", "\n", "                ", "self", ".", "o2e", "=", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.OutputLayer.get_score": [[505, 534], ["input.size", "modules.OutputLayer.latent", "modules.OutputLayer.dropout", "torch.linear", "torch.linear", "torch.linear", "modules.OutputLayer.prior().view", "modules.OutputLayer.softmax", "modules.OutputLayer.softmax().view", "probs.log", "modules.OutputLayer.dropout", "torch.linear", "torch.linear", "torch.linear", "input.size", "modules.OutputLayer.activation", "modules.OutputLayer.view", "modules.OutputLayer.o2e", "input.dim", "modules.OutputLayer.prior", "modules.OutputLayer.softmax", "modules.OutputLayer.unsqueeze"], "methods", ["None"], ["", "", "", "def", "get_score", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "self", ".", "numsoftmax", ">", "1", ":", "\n", "            ", "bsz", "=", "input", ".", "size", "(", "0", ")", "\n", "seqlen", "=", "input", ".", "size", "(", "1", ")", "if", "input", ".", "dim", "(", ")", ">", "1", "else", "1", "\n", "\n", "# first compute different softmax scores based on input vec", "\n", "# hsz => numsoftmax * esz", "\n", "latent", "=", "self", ".", "latent", "(", "input", ")", "\n", "active", "=", "self", ".", "dropout", "(", "self", ".", "activation", "(", "latent", ")", ")", "\n", "# esz => num_features", "\n", "logit", "=", "F", ".", "linear", "(", "active", ".", "view", "(", "-", "1", ",", "self", ".", "esz", ")", ",", "self", ".", "weight", ",", "self", ".", "bias", ")", "\n", "\n", "# calculate priors: distribution over which softmax scores to use", "\n", "# hsz => numsoftmax", "\n", "prior_logit", "=", "self", ".", "prior", "(", "input", ")", ".", "view", "(", "-", "1", ",", "self", ".", "numsoftmax", ")", "\n", "# softmax over numsoftmax's", "\n", "prior", "=", "self", ".", "softmax", "(", "prior_logit", ")", "\n", "\n", "# now combine priors with logits", "\n", "prob", "=", "self", ".", "softmax", "(", "logit", ")", ".", "view", "(", "bsz", "*", "seqlen", ",", "self", ".", "numsoftmax", ",", "-", "1", ")", "\n", "probs", "=", "(", "prob", "*", "prior", ".", "unsqueeze", "(", "2", ")", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "bsz", ",", "seqlen", ",", "-", "1", ")", "\n", "scores", "=", "probs", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "# hsz => esz, good time for dropout", "\n", "            ", "e", "=", "self", ".", "dropout", "(", "self", ".", "o2e", "(", "input", ")", ")", "\n", "# esz => num_features", "\n", "scores", "=", "F", ".", "linear", "(", "e", ",", "self", ".", "weight", ",", "self", ".", "bias", ")", "\n", "\n", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.OutputLayer.forward": [[535, 554], ["modules.OutputLayer.get_score", "modules.OutputLayer.masked_fill_", "modules.OutputLayer.unk_mask.to", "modules.OutputLayer.pad_mask.to", "modules.OutputLayer.masked_fill_"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.OutputLayer.get_score"], ["", "def", "forward", "(", "self", ",", "input", ",", "forced", "=", "False", ")", ":", "\n", "        ", "\"\"\"Compute scores from inputs.\n\n        :param input: (bsz x seq_len x num_directions * hiddensize) tensor of\n                       states, e.g. the output states of an RNN\n\n        :returns: (bsz x seqlen x num_cands) scores for each candidate\n        \"\"\"", "\n", "# next compute scores over dictionary", "\n", "if", "self", ".", "unk_mask", ".", "device", "!=", "input", ".", "device", ":", "\n", "            ", "self", ".", "unk_mask", "=", "self", ".", "unk_mask", ".", "to", "(", "input", ".", "device", ")", "\n", "self", ".", "pad_mask", "=", "self", ".", "pad_mask", ".", "to", "(", "input", ".", "device", ")", "\n", "", "scores", "=", "self", ".", "get_score", "(", "input", ")", "\n", "\n", "if", "forced", "==", "False", ":", "\n", "            ", "scores", ".", "masked_fill_", "(", "self", ".", "unk_mask", ",", "-", "NEAR_INF", ")", "\n", "", "scores", ".", "masked_fill_", "(", "self", ".", "pad_mask", ",", "-", "NEAR_INF", ")", "\n", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.AttentionLayer.__init__": [[562, 598], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "RuntimeError", "RuntimeError", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["def", "__init__", "(", "self", ",", "attn_type", ",", "hiddensize", ",", "embeddingsize", ",", "\n", "bidirectional", "=", "False", ",", "attn_length", "=", "-", "1", ",", "attn_time", "=", "'pre'", ")", ":", "\n", "        ", "\"\"\"Initialize attention layer.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "attn_type", "\n", "\n", "if", "self", ".", "attention", "!=", "'none'", ":", "\n", "            ", "hsz", "=", "hiddensize", "\n", "hszXdirs", "=", "hsz", "*", "(", "2", "if", "bidirectional", "else", "1", ")", "\n", "if", "attn_time", "==", "'pre'", ":", "\n", "# attention happens on the input embeddings", "\n", "                ", "input_dim", "=", "embeddingsize", "\n", "", "elif", "attn_time", "==", "'post'", ":", "\n", "# attention happens on the output of the rnn", "\n", "                ", "input_dim", "=", "hsz", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "'unsupported attention time'", ")", "\n", "\n", "# linear layer for combining applied attention weights with input", "\n", "", "self", ".", "attn_combine", "=", "nn", ".", "Linear", "(", "hszXdirs", "+", "input_dim", ",", "input_dim", ",", "\n", "bias", "=", "False", ")", "\n", "\n", "if", "self", ".", "attention", "==", "'local'", ":", "\n", "# local attention over fixed set of output states", "\n", "                ", "if", "attn_length", "<", "0", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Set attention length to > 0.'", ")", "\n", "", "self", ".", "max_length", "=", "attn_length", "\n", "# combines input and previous hidden output layer", "\n", "self", ".", "attn", "=", "nn", ".", "Linear", "(", "hsz", "+", "input_dim", ",", "attn_length", ",", "bias", "=", "False", ")", "\n", "# combines attention weights with encoder outputs", "\n", "", "elif", "self", ".", "attention", "==", "'concat'", ":", "\n", "                ", "self", ".", "attn", "=", "nn", ".", "Linear", "(", "hsz", "+", "hszXdirs", ",", "hsz", ",", "bias", "=", "False", ")", "\n", "self", ".", "attn_v", "=", "nn", ".", "Linear", "(", "hsz", ",", "1", ",", "bias", "=", "False", ")", "\n", "", "elif", "self", ".", "attention", "==", "'general'", ":", "\n", "# equivalent to dot if attn is identity", "\n", "                ", "self", ".", "attn", "=", "nn", ".", "Linear", "(", "hsz", ",", "hszXdirs", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.AttentionLayer.forward": [[599, 676], ["enc_out.narrow.narrow.size", "last_hidden.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "type", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.softmax", "torch.softmax", "torch.softmax", "last_hidden.unsqueeze", "torch.softmax", "torch.softmax", "torch.softmax", "attn_weights.narrow.narrow.unsqueeze", "modules.AttentionLayer.attn_combine().unsqueeze", "modules.AttentionLayer.attn", "enc_out.narrow.narrow.narrow", "attn_weights.narrow.narrow.size", "attn_weights.narrow.narrow.narrow", "modules.AttentionLayer.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "modules.AttentionLayer.attn_v().squeeze", "torch.bmm().squeeze.masked_fill_", "torch.bmm().squeeze.masked_fill_", "torch.bmm().squeeze.masked_fill_", "xes.squeeze", "torch.bmm.squeeze", "torch.bmm.squeeze", "torch.bmm.squeeze", "xes.squeeze", "modules.AttentionLayer.attn", "enc_out.narrow.narrow.transpose", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "modules.AttentionLayer.attn_combine", "modules.AttentionLayer.attn_v", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modules.AttentionLayer.attn", "enc_out.narrow.narrow.transpose", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "xes", ",", "hidden", ",", "attn_params", ")", ":", "\n", "        ", "\"\"\"Compute attention over attn_params given input and hidden states.\n\n        :param xes:         input state. will be combined with applied\n                            attention.\n        :param hidden:      hidden state from model. will be used to select\n                            states to attend to in from the attn_params.\n        :param attn_params: tuple of encoder output states and a mask showing\n                            which input indices are nonzero.\n\n        :returns: output, attn_weights\n                  output is a new state of same size as input state `xes`.\n                  attn_weights are the weights given to each state in the\n                  encoder outputs.\n        \"\"\"", "\n", "if", "self", ".", "attention", "==", "'none'", ":", "\n", "# do nothing, no attention", "\n", "            ", "return", "xes", ",", "None", "\n", "\n", "", "if", "type", "(", "hidden", ")", "==", "tuple", ":", "\n", "# for lstms use the \"hidden\" state not the cell state", "\n", "            ", "hidden", "=", "hidden", "[", "0", "]", "\n", "", "last_hidden", "=", "hidden", "[", "-", "1", "]", "# select hidden state from last RNN layer", "\n", "\n", "enc_out", ",", "attn_mask", "=", "attn_params", "\n", "bsz", ",", "seqlen", ",", "hszXnumdir", "=", "enc_out", ".", "size", "(", ")", "\n", "numlayersXnumdir", "=", "last_hidden", ".", "size", "(", "1", ")", "\n", "\n", "if", "self", ".", "attention", "==", "'local'", ":", "\n", "# local attention weights aren't based on encoder states", "\n", "            ", "h_merged", "=", "torch", ".", "cat", "(", "(", "xes", ".", "squeeze", "(", "1", ")", ",", "last_hidden", ")", ",", "1", ")", "\n", "attn_weights", "=", "F", ".", "softmax", "(", "self", ".", "attn", "(", "h_merged", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# adjust state sizes to the fixed window size", "\n", "if", "seqlen", ">", "self", ".", "max_length", ":", "\n", "                ", "offset", "=", "seqlen", "-", "self", ".", "max_length", "\n", "enc_out", "=", "enc_out", ".", "narrow", "(", "1", ",", "offset", ",", "self", ".", "max_length", ")", "\n", "seqlen", "=", "self", ".", "max_length", "\n", "", "if", "attn_weights", ".", "size", "(", "1", ")", ">", "seqlen", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "narrow", "(", "1", ",", "0", ",", "seqlen", ")", "\n", "", "", "else", ":", "\n", "            ", "hid", "=", "last_hidden", ".", "unsqueeze", "(", "1", ")", "\n", "if", "self", ".", "attention", "==", "'concat'", ":", "\n", "# concat hidden state and encoder outputs", "\n", "                ", "hid", "=", "hid", ".", "expand", "(", "bsz", ",", "seqlen", ",", "numlayersXnumdir", ")", "\n", "h_merged", "=", "torch", ".", "cat", "(", "(", "enc_out", ",", "hid", ")", ",", "2", ")", "\n", "# then do linear combination of them with activation", "\n", "active", "=", "F", ".", "tanh", "(", "self", ".", "attn", "(", "h_merged", ")", ")", "\n", "attn_w_premask", "=", "self", ".", "attn_v", "(", "active", ")", ".", "squeeze", "(", "2", ")", "\n", "", "elif", "self", ".", "attention", "==", "'dot'", ":", "\n", "# dot product between hidden and encoder outputs", "\n", "                ", "if", "numlayersXnumdir", "!=", "hszXnumdir", ":", "\n", "# enc_out has two directions, so double hid", "\n", "                    ", "hid", "=", "torch", ".", "cat", "(", "[", "hid", ",", "hid", "]", ",", "2", ")", "\n", "", "enc_t", "=", "enc_out", ".", "transpose", "(", "1", ",", "2", ")", "\n", "attn_w_premask", "=", "torch", ".", "bmm", "(", "hid", ",", "enc_t", ")", ".", "squeeze", "(", "1", ")", "\n", "", "elif", "self", ".", "attention", "==", "'general'", ":", "\n", "# before doing dot product, transform hidden state with linear", "\n", "# same as dot if linear is identity", "\n", "                ", "hid", "=", "self", ".", "attn", "(", "hid", ")", "\n", "enc_t", "=", "enc_out", ".", "transpose", "(", "1", ",", "2", ")", "\n", "attn_w_premask", "=", "torch", ".", "bmm", "(", "hid", ",", "enc_t", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# calculate activation scores, apply mask if needed", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "# remove activation from NULL symbols", "\n", "                ", "attn_w_premask", ".", "masked_fill_", "(", "(", "1", "-", "attn_mask", ")", ",", "-", "NEAR_INF", ")", "\n", "", "attn_weights", "=", "F", ".", "softmax", "(", "attn_w_premask", ",", "dim", "=", "1", ")", "\n", "\n", "# apply the attention weights to the encoder states", "\n", "", "attn_applied", "=", "torch", ".", "bmm", "(", "attn_weights", ".", "unsqueeze", "(", "1", ")", ",", "enc_out", ")", "\n", "# concatenate the input and encoder states", "\n", "merged", "=", "torch", ".", "cat", "(", "(", "xes", ".", "squeeze", "(", "1", ")", ",", "attn_applied", ".", "squeeze", "(", "1", ")", ")", ",", "1", ")", "\n", "# combine them with a linear layer and tanh activation", "\n", "output", "=", "torch", ".", "tanh", "(", "self", ".", "attn_combine", "(", "merged", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "return", "output", ",", "attn_weights", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules._transpose_hidden_state": [[26, 41], ["isinstance", "tuple", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "map", "hidden_state.transpose", "ValueError"], "function", ["None"], ["def", "_transpose_hidden_state", "(", "hidden_state", ")", ":", "\n", "    ", "\"\"\"\n    Transpose the hidden state so that batch is the first dimension.\n\n    RNN modules produce (num_layers x batchsize x dim) hidden state, but\n    DataParallel expects batch size to be first. This helper is used to\n    ensure that we're always outputting batch-first, in case DataParallel\n    tries to stitch things back together.\n    \"\"\"", "\n", "if", "isinstance", "(", "hidden_state", ",", "tuple", ")", ":", "\n", "        ", "return", "tuple", "(", "map", "(", "_transpose_hidden_state", ",", "hidden_state", ")", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "hidden_state", ")", ":", "\n", "        ", "return", "hidden_state", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Don't know how to transpose {}\"", ".", "format", "(", "hidden_state", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.opt_to_kwargs": [[43, 53], ["None"], "function", ["None"], ["", "", "def", "opt_to_kwargs", "(", "opt", ")", ":", "\n", "    ", "\"\"\"Get kwargs for seq2seq from opt.\"\"\"", "\n", "kwargs", "=", "{", "}", "\n", "for", "k", "in", "[", "'numlayers'", ",", "'dropout'", ",", "'bidirectional'", ",", "'rnn_class'", ",", "\n", "'lookuptable'", ",", "'decoder'", ",", "'numsoftmax'", ",", "\n", "'attention'", ",", "'attention_length'", ",", "'attention_time'", ",", "\n", "'input_dropout'", "]", ":", "\n", "        ", "if", "k", "in", "opt", ":", "\n", "            ", "kwargs", "[", "k", "]", "=", "opt", "[", "k", "]", "\n", "", "", "return", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.add_cmdline_args": [[31, 106], ["argparser.add_argument_group", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "argparser.add_argument_group.add_argument", "super().add_cmdline_args", "FaceAgent.dictionary_class().add_cmdline_args", "modules.Seq2seq.RNN_OPTS.keys", "FaceAgent.dictionary_class"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.add_cmdline_args", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.add_cmdline_args"], ["    ", "@", "classmethod", "\n", "def", "add_cmdline_args", "(", "cls", ",", "argparser", ")", ":", "\n", "        ", "\"\"\"Add command-line arguments specifically for this agent.\"\"\"", "\n", "agent", "=", "argparser", ".", "add_argument_group", "(", "'Face Arguments'", ")", "\n", "agent", ".", "add_argument", "(", "'--init-model'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'load dict/model/opts from this path'", ")", "\n", "agent", ".", "add_argument", "(", "'-hs'", ",", "'--hiddensize'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'size of the hidden layers'", ")", "\n", "agent", ".", "add_argument", "(", "'-esz'", ",", "'--embeddingsize'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'size of the token embeddings'", ")", "\n", "agent", ".", "add_argument", "(", "'-nl'", ",", "'--numlayers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'number of hidden layers'", ")", "\n", "agent", ".", "add_argument", "(", "'-dr'", ",", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'dropout rate'", ")", "\n", "agent", ".", "add_argument", "(", "'-bi'", ",", "'--bidirectional'", ",", "type", "=", "'bool'", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "'whether to encode the context with a '", "\n", "'bidirectional rnn'", ")", "\n", "agent", ".", "add_argument", "(", "'-att'", ",", "'--attention'", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'none'", ",", "'concat'", ",", "'general'", ",", "'dot'", ",", "\n", "'local'", "]", ",", "\n", "help", "=", "'Choices: none, concat, general, local. '", "\n", "'If set local, also set attention-length. '", "\n", "'(see arxiv.org/abs/1508.04025)'", ")", "\n", "agent", ".", "add_argument", "(", "'-attl'", ",", "'--attention-length'", ",", "default", "=", "48", ",", "type", "=", "int", ",", "\n", "help", "=", "'Length of local attention.'", ")", "\n", "agent", ".", "add_argument", "(", "'--attention-time'", ",", "default", "=", "'post'", ",", "\n", "choices", "=", "[", "'pre'", ",", "'post'", "]", ",", "\n", "help", "=", "'Whether to apply attention before or after '", "\n", "'decoding.'", ")", "\n", "agent", ".", "add_argument", "(", "'-rnn'", ",", "'--rnn-class'", ",", "default", "=", "'lstm'", ",", "\n", "choices", "=", "Seq2seq", ".", "RNN_OPTS", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Choose between different types of RNNs.'", ")", "\n", "agent", ".", "add_argument", "(", "'-dec'", ",", "'--decoder'", ",", "default", "=", "'same'", ",", "\n", "choices", "=", "[", "'same'", ",", "'shared'", "]", ",", "\n", "help", "=", "'Choose between different decoder modules. '", "\n", "'Default \"same\" uses same class as encoder, '", "\n", "'while \"shared\" also uses the same weights. '", "\n", "'Note that shared disabled some encoder '", "\n", "'options--in particular, bidirectionality.'", ")", "\n", "agent", ".", "add_argument", "(", "'-lt'", ",", "'--lookuptable'", ",", "default", "=", "'unique'", ",", "\n", "choices", "=", "[", "'unique'", ",", "'enc_dec'", ",", "'dec_out'", ",", "'all'", "]", ",", "\n", "help", "=", "'The encoder, decoder, and output modules can '", "\n", "'share weights, or not. '", "\n", "'Unique has independent embeddings for each. '", "\n", "'Enc_dec shares the embedding for the encoder '", "\n", "'and decoder. '", "\n", "'Dec_out shares decoder embedding and output '", "\n", "'weights. '", "\n", "'All shares all three weights.'", ")", "\n", "agent", ".", "add_argument", "(", "'-soft'", ",", "'--numsoftmax'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'default 1, if greater then uses mixture of '", "\n", "'softmax (see arxiv.org/abs/1711.03953).'", ")", "\n", "agent", ".", "add_argument", "(", "'-idr'", ",", "'--input-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'Probability of replacing tokens with UNK in training.'", ")", "\n", "agent", ".", "add_argument", "(", "'-ft'", ",", "'--frequency-type'", ",", "default", "=", "'out'", ",", "\n", "choices", "=", "[", "'out'", ",", "'gt'", ",", "'none'", "]", ",", "\n", "help", "=", "'What to use for calculating token frequency.'", ")", "\n", "agent", ".", "add_argument", "(", "'-wt'", ",", "'--weighing-time'", ",", "default", "=", "'pre'", ",", "\n", "choices", "=", "[", "'pre'", ",", "'post'", ",", "'none'", "]", ",", "\n", "help", "=", "'When to apply weight to losses.'", ")", "\n", "agent", ".", "add_argument", "(", "'-cp'", ",", "'--confidence-penalty'", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'cp'", ",", "'cpf'", ",", "'cpfw'", ",", "'cpfwn'", ",", "'none'", "]", ",", "\n", "help", "=", "'Which kind of confidence penalty to use: '", "\n", "\"'cp' is the confidence-penalty function reported in https://arxiv.org/abs/1809.01941. \"", "\n", "\"'cpf' is the parameter-free version proposed in https://arxiv.org/abs/1902.09191. \"", "\n", "\"'cpfw' means using the parameter-free version as the weight of FACE. \"", "\n", "\"'cpfwn' is a new design that normalizes the weight to the range of [1, +inf], which is \"", "\n", "\"more favorable as the weight of FACE.\"", ")", "\n", "agent", ".", "add_argument", "(", "'-b'", ",", "'--beta'", ",", "type", "=", "float", ",", "default", "=", "2.5", ",", "\n", "help", "=", "'Penalty strength for type \"cp\".'", ")", "\n", "\n", "super", "(", "cls", ",", "FaceAgent", ")", ".", "add_cmdline_args", "(", "argparser", ")", "\n", "FaceAgent", ".", "dictionary_class", "(", ")", ".", "add_cmdline_args", "(", "argparser", ")", "\n", "return", "agent", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.model_version": [[107, 110], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "model_version", "(", ")", ":", "\n", "        ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__": [[111, 123], ["parlai.core.torch_generator_agent.TorchGeneratorAgent.__init__", "modules.HLoss", "math.log", "getattr", "numpy.zeros", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.__init__"], ["", "def", "__init__", "(", "self", ",", "opt", ",", "shared", "=", "None", ")", ":", "\n", "        ", "\"\"\"Set up model.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "opt", ",", "shared", ")", "\n", "self", ".", "id", "=", "'FACE'", "\n", "if", "getattr", "(", "self", ",", "'word_freq'", ",", "None", ")", "is", "None", ":", "\n", "            ", "self", ".", "word_freq", "=", "np", ".", "zeros", "(", "len", "(", "self", ".", "dict", ")", ")", "\n", "", "self", ".", "ft", "=", "opt", "[", "'frequency_type'", "]", "\n", "self", ".", "wt", "=", "opt", "[", "'weighing_time'", "]", "\n", "self", ".", "cp", "=", "opt", "[", "'confidence_penalty'", "]", "\n", "self", ".", "beta", "=", "opt", "[", "'beta'", "]", "\n", "self", ".", "masked_entropy", "=", "HLoss", "(", "ignore_index", "=", "self", ".", "NULL_IDX", ")", "\n", "self", ".", "ideal_entropy", "=", "math", ".", "log", "(", "1", "/", "len", "(", "self", ".", "dict", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.build_model": [[124, 168], ["modules.opt_to_kwargs", "modules.Seq2seq", "opt[].endswith", "len", "print", "face.FaceAgent.model.load_state_dict", "face.FaceAgent.model.cuda", "print", "face.FaceAgent.model.cuda", "states.get", "opt.get", "face.FaceAgent._copy_embeddings", "face.FaceAgent._copy_embeddings"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.opt_to_kwargs"], ["", "def", "build_model", "(", "self", ",", "states", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize model, override to change model setup.\"\"\"", "\n", "opt", "=", "self", ".", "opt", "\n", "if", "not", "states", ":", "\n", "            ", "states", "=", "{", "}", "\n", "\n", "", "kwargs", "=", "opt_to_kwargs", "(", "opt", ")", "\n", "self", ".", "model", "=", "Seq2seq", "(", "\n", "len", "(", "self", ".", "dict", ")", ",", "opt", "[", "'embeddingsize'", "]", ",", "opt", "[", "'hiddensize'", "]", ",", "\n", "padding_idx", "=", "self", ".", "NULL_IDX", ",", "start_idx", "=", "self", ".", "START_IDX", ",", "\n", "end_idx", "=", "self", ".", "END_IDX", ",", "unknown_idx", "=", "self", ".", "dict", "[", "self", ".", "dict", ".", "unk_token", "]", ",", "\n", "longest_label", "=", "states", ".", "get", "(", "'longest_label'", ",", "1", ")", ",", "\n", "**", "kwargs", ")", "\n", "\n", "if", "(", "opt", ".", "get", "(", "'dict_tokenizer'", ")", "==", "'bpe'", "and", "\n", "opt", "[", "'embedding_type'", "]", "!=", "'random'", ")", ":", "\n", "            ", "print", "(", "'skipping preinitialization of embeddings for bpe'", ")", "\n", "", "elif", "not", "states", "and", "opt", "[", "'embedding_type'", "]", "!=", "'random'", ":", "\n", "# `not states`: only set up embeddings if not loading model", "\n", "            ", "self", ".", "_copy_embeddings", "(", "self", ".", "model", ".", "decoder", ".", "lt", ".", "weight", ",", "\n", "opt", "[", "'embedding_type'", "]", ")", "\n", "if", "opt", "[", "'lookuptable'", "]", "in", "[", "'unique'", ",", "'dec_out'", "]", ":", "\n", "# also set encoder lt, since it's not shared", "\n", "                ", "self", ".", "_copy_embeddings", "(", "self", ".", "model", ".", "encoder", ".", "lt", ".", "weight", ",", "\n", "opt", "[", "'embedding_type'", "]", ",", "log", "=", "False", ")", "\n", "\n", "", "", "if", "states", ":", "\n", "# set loaded states if applicable", "\n", "            ", "self", ".", "model", ".", "load_state_dict", "(", "states", "[", "'model'", "]", ")", "\n", "\n", "", "if", "self", ".", "use_cuda", ":", "\n", "            ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "\n", "", "if", "opt", "[", "'embedding_type'", "]", ".", "endswith", "(", "'fixed'", ")", ":", "\n", "            ", "print", "(", "'Seq2seq: fixing embedding weights.'", ")", "\n", "self", ".", "model", ".", "decoder", ".", "lt", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "model", ".", "encoder", ".", "lt", ".", "weight", ".", "requires_grad", "=", "False", "\n", "if", "opt", "[", "'lookuptable'", "]", "in", "[", "'dec_out'", ",", "'all'", "]", ":", "\n", "                ", "self", ".", "model", ".", "decoder", ".", "e2s", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "if", "self", ".", "use_cuda", ":", "\n", "            ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "\n", "", "return", "self", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._init_cuda_buffer": [[169, 189], ["torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "face.FaceAgent.model", "face.FaceAgent.criterion", "face.FaceAgent.backward", "hasattr", "scores.view", "torch.ones().long().cuda.view", "torch.ones().long().cuda.view", "torch.ones().long().cuda.view", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "scores.size", "str", "RuntimeError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "_init_cuda_buffer", "(", "self", ",", "batchsize", ",", "maxlen", ",", "force", "=", "False", ")", ":", "\n", "        ", "\"\"\"Pre-initialize CUDA buffer by doing fake forward pass.\"\"\"", "\n", "if", "self", ".", "use_cuda", "and", "(", "force", "or", "not", "hasattr", "(", "self", ",", "'buffer_initialized'", ")", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "dummy_xs", "=", "torch", ".", "ones", "(", "batchsize", ",", "maxlen", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "dummy_ys", "=", "torch", ".", "ones", "(", "batchsize", ",", "2", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "scores", ",", "_", ",", "_", "=", "self", ".", "model", "(", "dummy_xs", ",", "dummy_ys", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "\n", "scores", ".", "view", "(", "-", "1", ",", "scores", ".", "size", "(", "-", "1", ")", ")", ",", "dummy_ys", ".", "view", "(", "-", "1", ")", "\n", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "buffer_initialized", "=", "True", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                    ", "m", "=", "(", "'CUDA OOM: Lower batch size (-bs) from {} or lower '", "\n", "' max sequence length (-tr) from {}'", "\n", "''", ".", "format", "(", "batchsize", ",", "maxlen", ")", ")", "\n", "raise", "RuntimeError", "(", "m", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.train_step": [[190, 258], ["batch.text_vec.size", "face.FaceAgent._init_cuda_buffer", "face.FaceAgent.model.train", "face.FaceAgent.zero_grad", "face.FaceAgent.model", "scores.view", "face.FaceAgent.clean_preds", "batch.label_vec.ne", "batch.label_vec.ne.long().sum().item", "face.FaceAgent.item", "face.FaceAgent.metrics[].extend", "face.FaceAgent.backward", "face.FaceAgent.update_params", "scores.size", "face.FaceAgent.update_frequency", "face.FaceAgent.loss_weight", "face.FaceAgent.criterion", "face.FaceAgent.masked_entropy", "face.FaceAgent.clean_preds", "face.FaceAgent.update_frequency", "batch.label_vec.view", "face.FaceAgent.criterion", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "face.FaceAgent.word_freq.sum", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "face.FaceAgent.criterion", "batch.label_vec.ne.long().sum", "batch.label_vec.view", "str", "print", "batch.label_vec.view", "batch.label_vec.view", "preds.view().cpu().numpy", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "batch.label_vec.view().cpu().numpy", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.relu", "torch.relu", "torch.relu", "batch.label_vec.ne.long", "preds.view().cpu", "batch.label_vec.view().cpu", "preds.view", "batch.label_vec.view"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._init_cuda_buffer", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.clean_preds", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.update_frequency", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.loss_weight", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.clean_preds", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.update_frequency"], ["", "", "", "", "def", "train_step", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"Train on a single batch of examples.\"\"\"", "\n", "batchsize", "=", "batch", ".", "text_vec", ".", "size", "(", "0", ")", "\n", "# helps with memory usage", "\n", "self", ".", "_init_cuda_buffer", "(", "batchsize", ",", "self", ".", "truncate", "or", "256", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n", "try", ":", "\n", "            ", "scores", ",", "preds", ",", "_", "=", "self", ".", "model", "(", "batch", ".", "text_vec", ",", "batch", ".", "label_vec", ")", "\n", "score_view", "=", "scores", ".", "view", "(", "-", "1", ",", "scores", ".", "size", "(", "-", "1", ")", ")", "\n", "preds_clean", "=", "self", ".", "clean_preds", "(", "preds", ")", "\n", "# Update token frequency, or not", "\n", "if", "self", ".", "ft", "==", "'gt'", ":", "\n", "                ", "self", ".", "update_frequency", "(", "self", ".", "clean_preds", "(", "batch", ".", "label_vec", ")", ")", "\n", "", "elif", "self", ".", "ft", "==", "'out'", ":", "\n", "                ", "self", ".", "update_frequency", "(", "preds_clean", ")", "\n", "# calculate loss w/ or w/o pre-/post-weight", "\n", "", "if", "self", ".", "wt", "==", "'pre'", ":", "\n", "                ", "self", ".", "criterion", ".", "weight", "=", "self", ".", "loss_weight", "(", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "score_view", ",", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "self", ".", "wt", "==", "'post'", ":", "\n", "                ", "self", ".", "criterion", ".", "reduction", "=", "'none'", "\n", "loss", "=", "self", ".", "criterion", "(", "score_view", ",", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ")", "\n", "device", "=", "loss", ".", "device", "\n", "freq_pred", "=", "self", ".", "word_freq", "[", "preds", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "freq_pred", "=", "torch", ".", "FloatTensor", "(", "freq_pred", ")", ".", "to", "(", "device", ")", "\n", "freq_GT", "=", "self", ".", "word_freq", "[", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "freq_GT", "=", "torch", ".", "FloatTensor", "(", "freq_GT", ")", ".", "to", "(", "device", ")", "\n", "total_freq", "=", "self", ".", "word_freq", ".", "sum", "(", ")", "\n", "weight", "=", "1", "+", "F", ".", "relu", "(", "freq_pred", "-", "freq_GT", ")", "/", "total_freq", "\n", "loss", "=", "torch", ".", "matmul", "(", "loss", ",", "weight", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "self", ".", "criterion", "(", "score_view", ",", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "notnull", "=", "batch", ".", "label_vec", ".", "ne", "(", "self", ".", "NULL_IDX", ")", "\n", "target_tokens", "=", "notnull", ".", "long", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Use confidence penalty or not", "\n", "if", "self", ".", "cp", "!=", "'none'", ":", "\n", "                ", "entropy", "=", "self", ".", "masked_entropy", "(", "score_view", ",", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ")", "\n", "mean_entropy", "=", "entropy", "/", "target_tokens", "\n", "if", "self", ".", "cp", "==", "'cp'", ":", "\n", "                    ", "loss", "-=", "self", ".", "beta", "*", "mean_entropy", "\n", "", "elif", "self", ".", "cp", "==", "'cpf'", ":", "\n", "                    ", "loss", "+=", "1", "/", "mean_entropy", "\n", "", "elif", "self", ".", "cp", "==", "'cpfw'", ":", "\n", "# TODO: normalize weight to [1, ++]?", "\n", "                    ", "loss", "*=", "(", "1", "+", "1", "/", "mean_entropy", ")", "\n", "", "elif", "self", ".", "cp", "==", "'cpfwn'", ":", "\n", "                    ", "loss", "*=", "(", "self", ".", "ideal_entropy", "/", "mean_entropy", ")", "\n", "# save loss to metrics", "\n", "", "", "correct", "=", "(", "(", "batch", ".", "label_vec", "==", "preds", ")", "*", "notnull", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "metrics", "[", "'correct_tokens'", "]", "+=", "correct", "\n", "self", ".", "metrics", "[", "'loss'", "]", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "metrics", "[", "'num_tokens'", "]", "+=", "target_tokens", "\n", "self", ".", "metrics", "[", "'preds'", "]", ".", "extend", "(", "preds_clean", ")", "\n", "loss", "=", "loss", "/", "target_tokens", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "update_params", "(", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "# catch out of memory exceptions during fwd/bck (skip batch)", "\n", "            ", "if", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                ", "print", "(", "'| WARNING: ran out of memory, skipping batch. '", "\n", "'if this happens frequently, decrease batchsize or '", "\n", "'truncate the inputs to the model.'", ")", "\n", "self", ".", "metrics", "[", "'total_skipped_batches'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "raise", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.eval_step": [[259, 335], ["batch.text_vec.size", "face.FaceAgent.model.eval", "face.FaceAgent.metrics[].extend", "parlai.core.torch_agent.Output", "parlai.core.utils.warn_once", "face.FaceAgent.model", "face.FaceAgent.model", "f_scores.view", "face.FaceAgent.criterion", "batch.label_vec.ne", "batch.label_vec.ne.long().sum().item", "face.FaceAgent.item", "face.FaceAgent.model.encoder", "range", "face.FaceAgent._v2t", "face.FaceAgent.clean_preds", "face.FaceAgent.model", "f_scores.size", "batch.label_vec.view", "len", "face.FaceAgent.model.reorder_encoder_states", "parlai.core.utils.padded_tensor", "face.FaceAgent.model.decode_forced", "torch.cross_entropy().view", "torch.cross_entropy().view", "torch.cross_entropy().view", "cand_scores.sort", "cand_choices.append", "face.FaceAgent.beam_search", "zip", "batch.label_vec.ne.long().sum", "cands.size", "face.FaceAgent._write_beam_dots", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "mask.sum", "batch.label_vec.ne.long", "scores.view", "cands.view", "cands.size"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._v2t", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.clean_preds", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.reorder_encoder_states", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.modules.Seq2seq.decode_forced"], ["", "", "", "def", "eval_step", "(", "self", ",", "batch", ")", ":", "\n", "        ", "\"\"\"Evaluate a single batch of examples.\"\"\"", "\n", "if", "batch", ".", "text_vec", "is", "None", ":", "\n", "            ", "return", "\n", "", "bsz", "=", "batch", ".", "text_vec", ".", "size", "(", "0", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "cand_scores", "=", "None", "\n", "\n", "if", "self", ".", "skip_generation", ":", "\n", "            ", "warn_once", "(", "\n", "\"--skip-generation does not produce accurate metrics beyond ppl\"", ",", "\n", "RuntimeWarning", "\n", ")", "\n", "scores", ",", "preds", ",", "_", "=", "self", ".", "model", "(", "batch", ".", "text_vec", ",", "batch", ".", "label_vec", ")", "\n", "", "elif", "self", ".", "beam_size", "==", "1", ":", "\n", "# greedy decode", "\n", "            ", "scores", ",", "preds", ",", "_", "=", "self", ".", "model", "(", "batch", ".", "text_vec", ")", "\n", "", "elif", "self", ".", "beam_size", ">", "1", ":", "\n", "            ", "out", "=", "self", ".", "beam_search", "(", "\n", "self", ".", "model", ",", "\n", "batch", ",", "\n", "self", ".", "beam_size", ",", "\n", "start", "=", "self", ".", "START_IDX", ",", "\n", "end", "=", "self", ".", "END_IDX", ",", "\n", "pad", "=", "self", ".", "NULL_IDX", ",", "\n", "min_length", "=", "self", ".", "beam_min_length", ",", "\n", "min_n_best", "=", "self", ".", "beam_min_n_best", ",", "\n", "block_ngram", "=", "self", ".", "beam_block_ngram", "\n", ")", "\n", "beam_preds_scores", ",", "_", ",", "beams", "=", "out", "\n", "preds", ",", "scores", "=", "zip", "(", "*", "beam_preds_scores", ")", "\n", "\n", "if", "self", ".", "beam_dot_log", "is", "True", ":", "\n", "                ", "self", ".", "_write_beam_dots", "(", "batch", ".", "text_vec", ",", "beams", ")", "\n", "\n", "", "", "if", "batch", ".", "label_vec", "is", "not", "None", ":", "\n", "# calculate loss on targets with teacher forcing", "\n", "            ", "f_scores", ",", "f_preds", ",", "_", "=", "self", ".", "model", "(", "batch", ".", "text_vec", ",", "batch", ".", "label_vec", ")", "\n", "score_view", "=", "f_scores", ".", "view", "(", "-", "1", ",", "f_scores", ".", "size", "(", "-", "1", ")", ")", "\n", "self", ".", "criterion", ".", "reduction", "=", "'sum'", "\n", "loss", "=", "self", ".", "criterion", "(", "score_view", ",", "batch", ".", "label_vec", ".", "view", "(", "-", "1", ")", ")", "\n", "# save loss to metrics", "\n", "notnull", "=", "batch", ".", "label_vec", ".", "ne", "(", "self", ".", "NULL_IDX", ")", "\n", "target_tokens", "=", "notnull", ".", "long", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "correct", "=", "(", "(", "batch", ".", "label_vec", "==", "f_preds", ")", "*", "notnull", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "metrics", "[", "'correct_tokens'", "]", "+=", "correct", "\n", "self", ".", "metrics", "[", "'loss'", "]", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "metrics", "[", "'num_tokens'", "]", "+=", "target_tokens", "\n", "\n", "", "cand_choices", "=", "None", "\n", "if", "self", ".", "rank_candidates", ":", "\n", "# compute roughly ppl to rank candidates", "\n", "            ", "cand_choices", "=", "[", "]", "\n", "encoder_states", "=", "self", ".", "model", ".", "encoder", "(", "batch", ".", "text_vec", ")", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "                ", "num_cands", "=", "len", "(", "batch", ".", "candidate_vecs", "[", "i", "]", ")", "\n", "enc", "=", "self", ".", "model", ".", "reorder_encoder_states", "(", "encoder_states", ",", "[", "i", "]", "*", "num_cands", ")", "\n", "cands", ",", "_", "=", "padded_tensor", "(", "\n", "batch", ".", "candidate_vecs", "[", "i", "]", ",", "self", ".", "NULL_IDX", ",", "self", ".", "use_cuda", "\n", ")", "\n", "scores", ",", "_", "=", "self", ".", "model", ".", "decode_forced", "(", "enc", ",", "cands", ")", "\n", "cand_losses", "=", "F", ".", "cross_entropy", "(", "\n", "scores", ".", "view", "(", "num_cands", "*", "cands", ".", "size", "(", "1", ")", ",", "-", "1", ")", ",", "\n", "cands", ".", "view", "(", "-", "1", ")", ",", "\n", "reduction", "=", "'none'", ",", "\n", ")", ".", "view", "(", "num_cands", ",", "cands", ".", "size", "(", "1", ")", ")", "\n", "# now cand_losses is cands x seqlen size, but we still need to", "\n", "# check padding and such", "\n", "mask", "=", "(", "cands", "!=", "self", ".", "NULL_IDX", ")", ".", "float", "(", ")", "\n", "cand_scores", "=", "(", "cand_losses", "*", "mask", ")", ".", "sum", "(", "dim", "=", "1", ")", "/", "(", "mask", ".", "sum", "(", "dim", "=", "1", ")", "+", "1e-9", ")", "\n", "_", ",", "ordering", "=", "cand_scores", ".", "sort", "(", ")", "\n", "cand_choices", ".", "append", "(", "[", "batch", ".", "candidates", "[", "i", "]", "[", "o", "]", "for", "o", "in", "ordering", "]", ")", "\n", "\n", "", "", "text", "=", "[", "self", ".", "_v2t", "(", "p", ")", "for", "p", "in", "preds", "]", "\n", "self", ".", "metrics", "[", "'preds'", "]", ".", "extend", "(", "self", ".", "clean_preds", "(", "preds", ")", ")", "\n", "return", "Output", "(", "text", ",", "cand_choices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.build_criterion": [[336, 347], ["face.FaceAgent.opt.get", "torch.NLLLoss", "torch.NLLLoss", "torch.NLLLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "face.FaceAgent.criterion.cuda"], "methods", ["None"], ["", "def", "build_criterion", "(", "self", ")", ":", "\n", "# set up criteria", "\n", "        ", "if", "self", ".", "opt", ".", "get", "(", "'numsoftmax'", ",", "1", ")", ">", "1", ":", "\n", "            ", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", "\n", "ignore_index", "=", "self", ".", "NULL_IDX", ",", "size_average", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "\n", "ignore_index", "=", "self", ".", "NULL_IDX", ",", "size_average", "=", "False", ")", "\n", "\n", "", "if", "self", ".", "use_cuda", ":", "\n", "            ", "self", ".", "criterion", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._v2t": [[348, 360], ["hasattr", "face.FaceAgent.dict.vec2txt", "vec.cpu.cpu.cpu", "new_vec.append"], "methods", ["None"], ["", "", "def", "_v2t", "(", "self", ",", "vec", ",", "end_early", "=", "True", ")", ":", "\n", "        ", "\"\"\"Convert token indices to string of tokens.\"\"\"", "\n", "new_vec", "=", "[", "]", "\n", "if", "hasattr", "(", "vec", ",", "'cpu'", ")", ":", "\n", "            ", "vec", "=", "vec", ".", "cpu", "(", ")", "\n", "", "for", "i", "in", "vec", ":", "\n", "            ", "if", "i", "==", "self", ".", "END_IDX", "and", "end_early", ":", "\n", "                ", "break", "\n", "", "if", "i", "==", "self", ".", "NULL_IDX", ":", "\n", "                ", "continue", "\n", "", "new_vec", ".", "append", "(", "i", ")", "\n", "", "return", "self", ".", "dict", ".", "vec2txt", "(", "new_vec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._vectorize_text": [[361, 365], ["super()._vectorize_text"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent._vectorize_text"], ["", "def", "_vectorize_text", "(", "self", ",", "text", ",", "add_start", "=", "False", ",", "add_end", "=", "False", ",", "\n", "truncate", "=", "None", ",", "truncate_left", "=", "False", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "_vectorize_text", "(", "text", ",", "add_start", "=", "add_start", ",", "add_end", "=", "add_end", ",", "\n", "truncate", "=", "truncate", ",", "truncate_left", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.vectorize": [[366, 371], ["super().vectorize"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.vectorize"], ["", "def", "vectorize", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Override vectorize for seq2seq.\"\"\"", "\n", "kwargs", "[", "'add_start'", "]", "=", "False", "# model does this in module code", "\n", "kwargs", "[", "'add_end'", "]", "=", "True", "# we do want this", "\n", "return", "super", "(", ")", ".", "vectorize", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.batchify": [[372, 376], ["super().batchify"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.batchify"], ["", "def", "batchify", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Override batchify options for seq2seq.\"\"\"", "\n", "kwargs", "[", "'sort'", "]", "=", "True", "# need sorted for pack_padded", "\n", "return", "super", "(", ")", ".", "batchify", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save": [[377, 401], ["face.FaceAgent.opt.get", "hasattr", "hasattr", "face.FaceAgent.optimizer.state_dict", "face.FaceAgent.model.module.state_dict", "face.FaceAgent.model.state_dict", "open", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "open", "face.FaceAgent.model_version", "json.dump"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.save", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.model_version"], ["", "def", "save", "(", "self", ",", "path", "=", "None", ")", ":", "\n", "        ", "\"\"\"Save model parameters if model_file is set.\"\"\"", "\n", "path", "=", "self", ".", "opt", ".", "get", "(", "'model_file'", ",", "None", ")", "if", "path", "is", "None", "else", "path", "\n", "\n", "if", "path", "and", "hasattr", "(", "self", ",", "'model'", ")", ":", "\n", "            ", "model", "=", "{", "}", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'module'", ")", ":", "\n", "                ", "model", "[", "'model'", "]", "=", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "model", "[", "'longest_label'", "]", "=", "self", ".", "model", ".", "module", ".", "longest_label", "\n", "", "else", ":", "\n", "                ", "model", "[", "'model'", "]", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "model", "[", "'longest_label'", "]", "=", "self", ".", "model", ".", "longest_label", "\n", "", "model", "[", "'optimizer'", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "model", "[", "'optimizer_type'", "]", "=", "self", ".", "opt", "[", "'optimizer'", "]", "\n", "model", "[", "'word_freq'", "]", "=", "self", ".", "word_freq", "\n", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "write", ":", "\n", "                ", "torch", ".", "save", "(", "model", ",", "write", ")", "\n", "\n", "# save opt file", "\n", "", "with", "open", "(", "path", "+", "'.opt'", ",", "'w'", ")", "as", "handle", ":", "\n", "# save version string", "\n", "                ", "self", ".", "opt", "[", "'model_version'", "]", "=", "self", ".", "model_version", "(", ")", "\n", "json", ".", "dump", "(", "self", ".", "opt", ",", "handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load": [[402, 412], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "face.FaceAgent.model.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load", "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.load"], ["", "", "", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"Return opt and model states.\"\"\"", "\n", "states", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "cpu", ",", "_", ":", "cpu", ")", "\n", "if", "'word_freq'", "in", "states", ":", "\n", "            ", "self", ".", "word_freq", "=", "states", "[", "'word_freq'", "]", "\n", "# set loaded states if applicable", "\n", "", "self", ".", "model", ".", "load_state_dict", "(", "states", "[", "'model'", "]", ")", "\n", "if", "'longest_label'", "in", "states", ":", "\n", "            ", "self", ".", "model", ".", "longest_label", "=", "states", "[", "'longest_label'", "]", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.reset_metrics": [[413, 420], ["super().reset_metrics"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.reset_metrics"], ["", "def", "reset_metrics", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset metrics for reporting loss and perplexity.\"\"\"", "\n", "super", "(", ")", ".", "reset_metrics", "(", ")", "\n", "self", ".", "metrics", "[", "'loss'", "]", "=", "0.0", "\n", "self", ".", "metrics", "[", "'num_tokens'", "]", "=", "0", "\n", "self", ".", "metrics", "[", "'correct_tokens'", "]", "=", "0", "\n", "self", ".", "metrics", "[", "'preds'", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.clean_preds": [[421, 434], ["preds.cpu().tolist.cpu().tolist.cpu().tolist", "res.append", "preds.cpu().tolist.cpu().tolist.cpu", "len", "pred.index"], "methods", ["None"], ["", "def", "clean_preds", "(", "self", ",", "preds", ")", ":", "\n", "        ", "res", "=", "[", "]", "\n", "preds", "=", "preds", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "for", "pred", "in", "preds", ":", "\n", "            ", "if", "self", ".", "END_IDX", "in", "pred", ":", "\n", "                ", "ind", "=", "pred", ".", "index", "(", "self", ".", "END_IDX", ")", "+", "1", "# end_idx included", "\n", "pred", "=", "pred", "[", ":", "ind", "]", "\n", "", "if", "len", "(", "pred", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "if", "pred", "[", "0", "]", "==", "self", ".", "START_IDX", ":", "\n", "                ", "pred", "=", "pred", "[", "1", ":", "]", "\n", "", "res", ".", "append", "(", "pred", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.calc_diversity": [[435, 450], ["set", "set", "round", "round", "len", "set.update", "set.update", "len", "len", "tuple", "len", "len", "range"], "methods", ["None"], ["", "def", "calc_diversity", "(", "self", ",", "metrics", ")", ":", "\n", "        ", "unigram", "=", "set", "(", ")", "\n", "bigram", "=", "set", "(", ")", "\n", "num_tok", "=", "0", "\n", "for", "vec", "in", "self", ".", "metrics", "[", "'preds'", "]", ":", "\n", "            ", "v_len", "=", "len", "(", "vec", ")", "\n", "num_tok", "+=", "v_len", "\n", "unigram", ".", "update", "(", "vec", ")", "\n", "bigram", ".", "update", "(", "[", "tuple", "(", "vec", "[", "i", ":", "i", "+", "2", "]", ")", "for", "i", "in", "range", "(", "v_len", "-", "1", ")", "]", ")", "\n", "", "metrics", "[", "'d_1'", "]", "=", "round", "(", "len", "(", "unigram", ")", "/", "num_tok", "*", "100", ",", "2", ")", "\n", "metrics", "[", "'d_2'", "]", "=", "round", "(", "len", "(", "bigram", ")", "/", "num_tok", "*", "100", ",", "2", ")", "\n", "if", "not", "self", ".", "model", ".", "training", ":", "\n", "            ", "metrics", "[", "'num_d1'", "]", "=", "len", "(", "unigram", ")", "\n", "metrics", "[", "'num_d2'", "]", "=", "len", "(", "bigram", ")", "\n", "metrics", "[", "'num_tok'", "]", "=", "num_tok", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.report": [[451, 475], ["m.items", "parlai.core.utils.round_sigfigs", "face.FaceAgent.calc_diversity", "math.exp", "float"], "methods", ["home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.calc_diversity"], ["", "", "def", "report", "(", "self", ")", ":", "\n", "        ", "\"\"\"Report loss and perplexity from model's perspective.\n\n        Note that this includes predicting __END__ and __UNK__ tokens and may\n        differ from a truly independent measurement.\n        \"\"\"", "\n", "m", "=", "{", "}", "\n", "num_tok", "=", "self", ".", "metrics", "[", "'num_tokens'", "]", "\n", "if", "num_tok", ">", "0", ":", "\n", "            ", "if", "self", ".", "metrics", "[", "'correct_tokens'", "]", ">", "0", ":", "\n", "                ", "m", "[", "'token_acc'", "]", "=", "self", ".", "metrics", "[", "'correct_tokens'", "]", "/", "num_tok", "\n", "", "m", "[", "'loss'", "]", "=", "self", ".", "metrics", "[", "'loss'", "]", "/", "num_tok", "\n", "try", ":", "\n", "                ", "m", "[", "'ppl'", "]", "=", "math", ".", "exp", "(", "m", "[", "'loss'", "]", ")", "\n", "", "except", "OverflowError", ":", "\n", "                ", "m", "[", "'ppl'", "]", "=", "float", "(", "'inf'", ")", "\n", "", "", "if", "self", ".", "metrics", "[", "'total_skipped_batches'", "]", ">", "0", ":", "\n", "            ", "m", "[", "'total_skipped_batches'", "]", "=", "self", ".", "metrics", "[", "'total_skipped_batches'", "]", "\n", "", "for", "k", ",", "v", "in", "m", ".", "items", "(", ")", ":", "\n", "# clean up: rounds to sigfigs and converts tensors to floats", "\n", "            ", "m", "[", "k", "]", "=", "round_sigfigs", "(", "v", ",", "4", ")", "\n", "", "if", "self", ".", "metrics", "[", "'preds'", "]", ":", "\n", "            ", "self", ".", "calc_diversity", "(", "m", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.update_frequency": [[476, 486], ["collections.Counter", "collections.Counter.items", "collections.Counter.update"], "methods", ["None"], ["", "def", "update_frequency", "(", "self", ",", "preds", ")", ":", "\n", "        ", "curr", "=", "Counter", "(", ")", "\n", "for", "pred", "in", "preds", ":", "\n", "            ", "curr", ".", "update", "(", "pred", ")", "\n", "\n", "# self.word_freq *= self.opt['decay_factor']", "\n", "", "for", "k", ",", "v", "in", "curr", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "==", "self", ".", "END_IDX", ":", "# do not suppress END token", "\n", "                 ", "continue", "\n", "", "self", ".", "word_freq", "[", "k", "]", "+=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.ShaojieJiang_FACE.face.face.FaceAgent.loss_weight": [[487, 496], ["face.FaceAgent.word_freq.sum", "RF.max", "len", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor().cuda", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "weight.sum", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["None"], ["", "", "def", "loss_weight", "(", "self", ")", ":", "\n", "        ", "RF", "=", "self", ".", "word_freq", "/", "self", ".", "word_freq", ".", "sum", "(", ")", "# relative frequency", "\n", "a", "=", "-", "1", "/", "RF", ".", "max", "(", ")", "\n", "weight", "=", "a", "*", "RF", "+", "1", "\n", "weight", "=", "weight", "/", "weight", ".", "sum", "(", ")", "*", "len", "(", "weight", ")", "# normalization", "\n", "if", "self", ".", "use_cuda", ":", "\n", "            ", "return", "torch", ".", "FloatTensor", "(", "weight", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "FloatTensor", "(", "weight", ")", "\n", "", "", "", ""]]}