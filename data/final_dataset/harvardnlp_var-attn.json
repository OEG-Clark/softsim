{"home.repos.pwc.inspect_result.harvardnlp_var-attn.None.translate.main": [[15, 19], ["onmt.translate.Translator.make_translator", "onmt.translate.Translator.make_translator.translate"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.make_translator", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.translate"], ["def", "main", "(", "opt", ")", ":", "\n", "    ", "translator", "=", "make_translator", "(", "opt", ",", "report_score", "=", "True", ")", "\n", "translator", ".", "translate", "(", "opt", ".", "src_dir", ",", "opt", ".", "src", ",", "opt", ".", "tgt", ",", "\n", "opt", ".", "batch_size", ",", "opt", ".", "attn_debug", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.__init__": [[128, 141], ["train.DatasetLazyIter._next_dataset_iterator"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter._next_dataset_iterator"], ["def", "__init__", "(", "self", ",", "datasets", ",", "fields", ",", "batch_size", ",", "batch_size_fn", ",", "\n", "device", ",", "is_train", ",", "random_state", "=", "None", ")", ":", "\n", "        ", "self", ".", "datasets", "=", "datasets", "\n", "self", ".", "fields", "=", "fields", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "batch_size_fn", "=", "batch_size_fn", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "is_train", "=", "is_train", "\n", "self", ".", "random_state", "=", "random_state", "\n", "\n", "self", ".", "cur_iter", "=", "self", ".", "_next_dataset_iterator", "(", "datasets", ")", "\n", "# We have at least one dataset.", "\n", "assert", "self", ".", "cur_iter", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.__iter__": [[142, 151], ["train.DatasetLazyIter._next_dataset_iterator"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter._next_dataset_iterator"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "dataset_iter", "=", "(", "d", "for", "d", "in", "self", ".", "datasets", ")", "\n", "while", "self", ".", "cur_iter", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "random_state", "is", "not", "None", ":", "\n", "                ", "self", ".", "cur_iter", ".", "random_shuffler", ".", "_random_state", "=", "self", ".", "random_state", "\n", "", "for", "batch", "in", "self", ".", "cur_iter", ":", "\n", "                ", "yield", "batch", "\n", "", "self", ".", "random_state", "=", "self", ".", "cur_iter", ".", "random_shuffler", ".", "_random_state", "\n", "self", ".", "cur_iter", "=", "self", ".", "_next_dataset_iterator", "(", "dataset_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.__len__": [[152, 158], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "# We return the len of cur_dataset, otherwise we need to load", "\n", "# all datasets to determine the real len, which loses the benefit", "\n", "# of lazy loading.", "\n", "        ", "assert", "self", ".", "cur_iter", "is", "not", "None", "\n", "return", "len", "(", "self", ".", "cur_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.get_cur_dataset": [[159, 161], ["None"], "methods", ["None"], ["", "def", "get_cur_dataset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cur_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter._next_dataset_iterator": [[162, 179], ["onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "next"], "methods", ["None"], ["", "def", "_next_dataset_iterator", "(", "self", ",", "dataset_iter", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "cur_dataset", "=", "next", "(", "dataset_iter", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "return", "None", "\n", "\n", "# We clear `fields` when saving, restore when loading.", "\n", "", "self", ".", "cur_dataset", ".", "fields", "=", "self", ".", "fields", "\n", "\n", "# Sort batch by decreasing lengths of sentence required by pytorch.", "\n", "# sort=False means \"Use dataset's sortkey instead of iterator's\".", "\n", "return", "onmt", ".", "io", ".", "OrderedIterator", "(", "\n", "dataset", "=", "self", ".", "cur_dataset", ",", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "batch_size_fn", "=", "self", ".", "batch_size_fn", ",", "\n", "device", "=", "self", ".", "device", ",", "train", "=", "self", ".", "is_train", ",", "\n", "sort", "=", "False", ",", "sort_within_batch", "=", "True", ",", "\n", "repeat", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.report_func": [[84, 113], ["onmt.Statistics.output", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics.log", "onmt.Statistics.log_tensorboard"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.output", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.log_tensorboard"], ["def", "report_func", "(", "epoch", ",", "batch", ",", "num_batches", ",", "\n", "progress_step", ",", "\n", "start_time", ",", "lr", ",", "report_stats", ")", ":", "\n", "    ", "\"\"\"\n    This is the user-defined batch-level traing progress\n    report function.\n\n    Args:\n        epoch(int): current epoch count.\n        batch(int): current batch count.\n        num_batches(int): total number of batches.\n        progress_step(int): the progress step.\n        start_time(float): last report time.\n        lr(float): current learning rate.\n        report_stats(Statistics): old Statistics instance.\n    Returns:\n        report_stats(Statistics): updated Statistics instance.\n    \"\"\"", "\n", "if", "batch", "%", "opt", ".", "report_every", "==", "-", "1", "%", "opt", ".", "report_every", ":", "\n", "        ", "report_stats", ".", "output", "(", "epoch", ",", "batch", "+", "1", ",", "num_batches", ",", "start_time", ")", "\n", "if", "opt", ".", "exp_host", ":", "\n", "            ", "report_stats", ".", "log", "(", "\"progress\"", ",", "experiment", ",", "lr", ")", "\n", "", "if", "opt", ".", "tensorboard", ":", "\n", "# Log the progress using the number of batches on the x-axis.", "\n", "            ", "report_stats", ".", "log_tensorboard", "(", "\n", "\"progress\"", ",", "writer", ",", "lr", ",", "progress_step", ")", "\n", "", "report_stats", "=", "onmt", ".", "Statistics", "(", ")", "\n", "\n", "", "return", "report_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_dataset_iter": [[181, 213], ["train.DatasetLazyIter", "max", "max", "max", "len", "len"], "function", ["None"], ["", "", "def", "make_dataset_iter", "(", "datasets", ",", "fields", ",", "opt", ",", "is_train", "=", "True", ",", "random_state", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    This returns user-defined train/validate data iterator for the trainer\n    to iterate over during each train epoch. We implement simple\n    ordered iterator strategy here, but more sophisticated strategy\n    like curriculum learning is ok too.\n    \"\"\"", "\n", "batch_size", "=", "opt", ".", "batch_size", "if", "is_train", "else", "opt", ".", "valid_batch_size", "\n", "batch_size_fn", "=", "None", "\n", "if", "is_train", "and", "opt", ".", "batch_type", "==", "\"tokens\"", ":", "\n", "# In token batching scheme, the number of sequences is limited", "\n", "# such that the total number of src/tgt tokens (including padding)", "\n", "# in a batch <= batch_size", "\n", "        ", "def", "batch_size_fn", "(", "new", ",", "count", ",", "sofar", ")", ":", "\n", "# Maintains the longest src and tgt length in the current batch", "\n", "            ", "global", "max_src_in_batch", ",", "max_tgt_in_batch", "\n", "# Reset current longest length at a new batch (count=1)", "\n", "if", "count", "==", "1", ":", "\n", "                ", "max_src_in_batch", "=", "0", "\n", "max_tgt_in_batch", "=", "0", "\n", "# Src: <bos> w1 ... wN <eos>", "\n", "", "max_src_in_batch", "=", "max", "(", "max_src_in_batch", ",", "len", "(", "new", ".", "src", ")", "+", "2", ")", "\n", "# Tgt: w1 ... wN <eos>", "\n", "max_tgt_in_batch", "=", "max", "(", "max_tgt_in_batch", ",", "len", "(", "new", ".", "tgt", ")", "+", "1", ")", "\n", "src_elements", "=", "count", "*", "max_src_in_batch", "\n", "tgt_elements", "=", "count", "*", "max_tgt_in_batch", "\n", "return", "max", "(", "src_elements", ",", "tgt_elements", ")", "\n", "\n", "", "", "device", "=", "'cuda'", "if", "opt", ".", "gpuid", "else", "'cpu'", "\n", "\n", "return", "DatasetLazyIter", "(", "datasets", ",", "fields", ",", "batch_size", ",", "batch_size_fn", ",", "\n", "device", ",", "is_train", ",", "random_state", "=", "random_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_loss_compute": [[215, 236], ["onmt.Utils.use_gpu", "onmt.modules.CopyGeneratorLossCompute", "onmt.modules.CopyGeneratorLossCompute", "onmt.modules.CopyGeneratorLossCompute", "onmt.modules.CopyGeneratorLossCompute", "onmt.modules.CopyGeneratorLossCompute", "onmt.modules.CopyGeneratorLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute", "onmt.Loss.NMTLossCompute.cuda"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu"], ["", "def", "make_loss_compute", "(", "model", ",", "tgt_vocab", ",", "opt", ",", "train", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    This returns user-defined LossCompute object, which is used to\n    compute loss in train/validate process. You can implement your\n    own *LossCompute class, by subclassing LossComputeBase.\n    \"\"\"", "\n", "if", "opt", ".", "copy_attn", ":", "\n", "        ", "compute", "=", "onmt", ".", "modules", ".", "CopyGeneratorLossCompute", "(", "\n", "model", ".", "generator", ",", "tgt_vocab", ",", "opt", ".", "copy_attn_force", ",", "\n", "opt", ".", "copy_loss_by_seqlength", ")", "\n", "", "else", ":", "\n", "        ", "compute", "=", "onmt", ".", "Loss", ".", "NMTLossCompute", "(", "\n", "model", ".", "generator", ",", "tgt_vocab", ",", "\n", "label_smoothing", "=", "opt", ".", "label_smoothing", "if", "train", "else", "0.0", ",", "\n", "train_baseline", "=", "opt", ".", "train_baseline", ">", "0", ",", "\n", ")", "\n", "\n", "", "if", "use_gpu", "(", "opt", ")", ":", "\n", "        ", "compute", ".", "cuda", "(", ")", "\n", "\n", "", "return", "compute", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.train_model": [[238, 332], ["train.make_loss_compute", "train.make_loss_compute", "onmt.Trainer", "onmt.Trainer", "onmt.Trainer", "onmt.Trainer", "onmt.Trainer", "onmt.Trainer", "print", "print", "print", "range", "print", "print", "train.make_dataset_iter", "onmt.Trainer.validate", "print", "print", "print", "print", "print", "print", "print", "range", "print", "train.make_dataset_iter", "onmt.Trainer.train", "print", "print", "print", "print", "print", "train.make_dataset_iter", "onmt.Trainer.validate", "print", "print", "print", "print", "print", "onmt.Trainer.epoch_step", "train.lazily_load_dataset", "print", "train.make_dataset_iter", "onmt.Trainer.validate", "print", "print", "print", "print", "print", "print", "train.lazily_load_dataset", "train.lazily_load_dataset", "trainer.train.log", "trainer.validate.log", "trainer.train.log_tensorboard", "trainer.train.log_tensorboard", "trainer.validate.expelbo", "onmt.Trainer.drop_checkpoint", "trainer.validate.expelbo", "trainer.validate.ppl", "trainer.validate.xent", "trainer.validate.kl", "trainer.validate.accuracy", "train.lazily_load_dataset", "trainer.train.expelbo", "trainer.train.ppl", "trainer.train.xent", "trainer.train.kl", "trainer.train.accuracy", "trainer.validate.expelbo", "trainer.validate.ppl", "trainer.validate.xent", "trainer.validate.kl", "trainer.validate.accuracy", "trainer.validate.expelbo", "trainer.validate.ppl", "trainer.validate.xent", "trainer.validate.kl", "trainer.validate.accuracy", "sum", "p.norm", "model.parameters"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_loss_compute", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_loss_compute", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_dataset_iter", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.validate", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_dataset_iter", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.train", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_dataset_iter", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.validate", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.epoch_step", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.make_dataset_iter", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.validate", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.log_tensorboard", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.log_tensorboard", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.drop_checkpoint", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy"], ["", "def", "train_model", "(", "model", ",", "fields", ",", "optim", ",", "data_type", ",", "model_opt", ")", ":", "\n", "    ", "train_loss", "=", "make_loss_compute", "(", "model", ",", "fields", "[", "\"tgt\"", "]", ".", "vocab", ",", "opt", ")", "\n", "valid_loss", "=", "make_loss_compute", "(", "model", ",", "fields", "[", "\"tgt\"", "]", ".", "vocab", ",", "opt", ",", "\n", "train", "=", "False", ")", "\n", "\n", "trunc_size", "=", "opt", ".", "truncated_decoder", "# Badly named...", "\n", "shard_size", "=", "opt", ".", "max_generator_batches", "\n", "norm_method", "=", "opt", ".", "normalization", "\n", "grad_accum_count", "=", "opt", ".", "accum_count", "\n", "\n", "trainer", "=", "onmt", ".", "Trainer", "(", "model", ",", "train_loss", ",", "valid_loss", ",", "optim", ",", "\n", "trunc_size", ",", "shard_size", ",", "data_type", ",", "\n", "norm_method", ",", "grad_accum_count", ")", "\n", "\n", "if", "model_opt", ".", "eval_only", ">", "0", ":", "\n", "        ", "print", "(", "\"|Param|: {}\"", ".", "format", "(", "sum", "(", "[", "p", ".", "norm", "(", ")", "**", "2", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ")", ".", "data", "[", "0", "]", "**", "0.5", ")", ")", "\n", "print", "(", "\"ELBO_q\"", ")", "\n", "model", ".", "use_prior", "=", "False", "\n", "valid_iter", "=", "make_dataset_iter", "(", "lazily_load_dataset", "(", "\"valid\"", ")", ",", "\n", "fields", ",", "opt", ",", "\n", "is_train", "=", "False", ")", "\n", "valid_stats", "=", "trainer", ".", "validate", "(", "valid_iter", ",", "\"enum\"", ")", "\n", "print", "(", "'Validation exp(elbo): %g'", "%", "valid_stats", ".", "expelbo", "(", ")", ")", "\n", "print", "(", "'Validation perplexity: %g'", "%", "valid_stats", ".", "ppl", "(", ")", ")", "\n", "print", "(", "'Validation xent: %g'", "%", "valid_stats", ".", "xent", "(", ")", ")", "\n", "print", "(", "'Validation kl: %g'", "%", "valid_stats", ".", "kl", "(", ")", ")", "\n", "print", "(", "'Validation accuracy: %g'", "%", "valid_stats", ".", "accuracy", "(", ")", ")", "\n", "print", "(", "\"N validation words: {}\"", ".", "format", "(", "valid_stats", ".", "_n_words", ")", ")", "\n", "print", "(", "\"p(x)\"", ")", "\n", "model", ".", "use_prior", "=", "True", "\n", "for", "k", "in", "range", "(", "6", ")", ":", "\n", "            ", "print", "(", "\"k-max: {}\"", ".", "format", "(", "k", ")", ")", "\n", "model", ".", "k", "=", "k", "\n", "valid_iter", "=", "make_dataset_iter", "(", "lazily_load_dataset", "(", "\"valid\"", ")", ",", "\n", "fields", ",", "opt", ",", "\n", "is_train", "=", "False", ")", "\n", "valid_stats", "=", "trainer", ".", "validate", "(", "valid_iter", ",", "\"exact\"", ")", "\n", "print", "(", "'Validation exp(elbo): %g'", "%", "valid_stats", ".", "expelbo", "(", ")", ")", "\n", "print", "(", "'Validation perplexity: %g'", "%", "valid_stats", ".", "ppl", "(", ")", ")", "\n", "print", "(", "'Validation xent: %g'", "%", "valid_stats", ".", "xent", "(", ")", ")", "\n", "print", "(", "'Validation kl: %g'", "%", "valid_stats", ".", "kl", "(", ")", ")", "\n", "print", "(", "'Validation accuracy: %g'", "%", "valid_stats", ".", "accuracy", "(", ")", ")", "\n", "print", "(", "\"N validation words: {}\"", ".", "format", "(", "valid_stats", ".", "_n_words", ")", ")", "\n", "", "return", "0", "\n", "\n", "\n", "", "print", "(", "'\\nStart training...'", ")", "\n", "print", "(", "' * number of epochs: %d, starting from Epoch %d'", "%", "\n", "(", "opt", ".", "epochs", "+", "1", "-", "opt", ".", "start_epoch", ",", "opt", ".", "start_epoch", ")", ")", "\n", "print", "(", "' * batch size: %d'", "%", "opt", ".", "batch_size", ")", "\n", "random_state", "=", "None", "\n", "for", "epoch", "in", "range", "(", "opt", ".", "start_epoch", ",", "opt", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "print", "(", "''", ")", "\n", "\n", "# 1. Train for one epoch on the training set.", "\n", "train_iter", "=", "make_dataset_iter", "(", "lazily_load_dataset", "(", "\"train\"", ")", ",", "\n", "fields", ",", "opt", ",", "random_state", "=", "random_state", ")", "\n", "train_stats", "=", "trainer", ".", "train", "(", "train_iter", ",", "epoch", ",", "report_func", ")", "\n", "random_state", "=", "train_iter", ".", "random_state", "\n", "print", "(", "'Train exp(elbo): %g'", "%", "train_stats", ".", "expelbo", "(", ")", ")", "\n", "print", "(", "'Train perplexity: %g'", "%", "train_stats", ".", "ppl", "(", ")", ")", "\n", "print", "(", "'Train xent: %g'", "%", "train_stats", ".", "xent", "(", ")", ")", "\n", "print", "(", "'Train kl: %g'", "%", "train_stats", ".", "kl", "(", ")", ")", "\n", "print", "(", "'Train accuracy: %g'", "%", "train_stats", ".", "accuracy", "(", ")", ")", "\n", "\n", "# 2. Validate on the validation set.", "\n", "valid_iter", "=", "make_dataset_iter", "(", "lazily_load_dataset", "(", "\"valid\"", ")", ",", "\n", "fields", ",", "opt", ",", "\n", "is_train", "=", "False", ")", "\n", "if", "model", ".", "mode", "==", "'sample'", "or", "model", ".", "mode", "==", "'wsram'", "or", "model", ".", "mode", "==", "'gumbel'", ":", "\n", "            ", "val_mode", "=", "'enum'", "\n", "", "else", ":", "\n", "            ", "val_mode", "=", "model", ".", "mode", "\n", "", "valid_stats", "=", "trainer", ".", "validate", "(", "valid_iter", ",", "val_mode", ")", "\n", "print", "(", "'Validation exp(elbo): %g'", "%", "valid_stats", ".", "expelbo", "(", ")", ")", "\n", "print", "(", "'Validation perplexity: %g'", "%", "valid_stats", ".", "ppl", "(", ")", ")", "\n", "print", "(", "'Validation xent: %g'", "%", "valid_stats", ".", "xent", "(", ")", ")", "\n", "print", "(", "'Validation kl: %g'", "%", "valid_stats", ".", "kl", "(", ")", ")", "\n", "print", "(", "'Validation accuracy: %g'", "%", "valid_stats", ".", "accuracy", "(", ")", ")", "\n", "\n", "# 3. Log to remote server.", "\n", "if", "opt", ".", "exp_host", ":", "\n", "            ", "train_stats", ".", "log", "(", "\"train\"", ",", "experiment", ",", "optim", ".", "lr", ")", "\n", "valid_stats", ".", "log", "(", "\"valid\"", ",", "experiment", ",", "optim", ".", "lr", ")", "\n", "", "if", "opt", ".", "tensorboard", ":", "\n", "            ", "train_stats", ".", "log_tensorboard", "(", "\"train\"", ",", "writer", ",", "optim", ".", "lr", ",", "epoch", ")", "\n", "train_stats", ".", "log_tensorboard", "(", "\"valid\"", ",", "writer", ",", "optim", ".", "lr", ",", "epoch", ")", "\n", "\n", "# 4. Update the learning rate", "\n", "", "trainer", ".", "epoch_step", "(", "valid_stats", ".", "expelbo", "(", ")", ",", "epoch", ")", "\n", "\n", "# 5. Drop a checkpoint if needed.", "\n", "if", "epoch", ">=", "opt", ".", "start_checkpoint_at", ":", "\n", "            ", "trainer", ".", "drop_checkpoint", "(", "model_opt", ",", "epoch", ",", "fields", ",", "valid_stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.check_save_model_path": [[334, 339], ["os.path.abspath", "os.path.dirname", "os.path.exists", "os.makedirs"], "function", ["None"], ["", "", "", "def", "check_save_model_path", "(", ")", ":", "\n", "    ", "save_model_path", "=", "os", ".", "path", ".", "abspath", "(", "opt", ".", "save_model", ")", "\n", "model_dirname", "=", "os", ".", "path", ".", "dirname", "(", "save_model_path", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "model_dirname", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "model_dirname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.tally_parameters": [[341, 353], ["sum", "print", "model.named_parameters", "print", "print", "p.nelement", "param.nelement", "model.parameters", "param.nelement"], "function", ["None"], ["", "", "def", "tally_parameters", "(", "model", ")", ":", "\n", "    ", "n_params", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'* number of parameters: %d'", "%", "n_params", ")", "\n", "enc", "=", "0", "\n", "dec", "=", "0", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'encoder'", "in", "name", ":", "\n", "            ", "enc", "+=", "param", ".", "nelement", "(", ")", "\n", "", "elif", "'decoder'", "or", "'generator'", "in", "name", ":", "\n", "            ", "dec", "+=", "param", ".", "nelement", "(", ")", "\n", "", "", "print", "(", "'encoder: '", ",", "enc", ")", "\n", "print", "(", "'decoder: '", ",", "dec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset": [[355, 382], ["sorted", "torch.load", "torch.load", "torch.load", "print", "glob.glob", "train.lazily_load_dataset.lazy_dataset_loader"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["", "def", "lazily_load_dataset", "(", "corpus_type", ")", ":", "\n", "    ", "\"\"\"\n    Dataset generator. Don't do extra stuff here, like printing,\n    because they will be postponed to the first loading time.\n\n    Args:\n        corpus_type: 'train' or 'valid'\n    Returns:\n        A list of dataset, the dataset(s) are lazily loaded.\n    \"\"\"", "\n", "assert", "corpus_type", "in", "[", "\"train\"", ",", "\"valid\"", "]", "\n", "\n", "def", "lazy_dataset_loader", "(", "pt_file", ",", "corpus_type", ")", ":", "\n", "        ", "dataset", "=", "torch", ".", "load", "(", "pt_file", ")", "\n", "print", "(", "'Loading %s dataset from %s, number of examples: %d'", "%", "\n", "(", "corpus_type", ",", "pt_file", ",", "len", "(", "dataset", ")", ")", ")", "\n", "return", "dataset", "\n", "\n", "# Sort the glob output by file name (by increasing indexes).", "\n", "", "pts", "=", "sorted", "(", "glob", ".", "glob", "(", "opt", ".", "data", "+", "'.'", "+", "corpus_type", "+", "'.[0-9]*.pt'", ")", ")", "\n", "if", "pts", ":", "\n", "        ", "for", "pt", "in", "pts", ":", "\n", "            ", "yield", "lazy_dataset_loader", "(", "pt", ",", "corpus_type", ")", "\n", "", "", "else", ":", "\n", "# Only one onmt.io.*Dataset, simple!", "\n", "        ", "pt", "=", "opt", ".", "data", "+", "'.'", "+", "corpus_type", "+", "'.pt'", "\n", "yield", "lazy_dataset_loader", "(", "pt", ",", "corpus_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.load_fields": [[384, 403], ["dict", "print", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "print", "print", "torch.load", "torch.load", "torch.load", "onmt.io.load_fields_from_vocab.items", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["", "", "def", "load_fields", "(", "dataset", ",", "data_type", ",", "checkpoint", ")", ":", "\n", "    ", "if", "checkpoint", "is", "not", "None", ":", "\n", "        ", "print", "(", "'Loading vocab from checkpoint at %s.'", "%", "opt", ".", "train_from", ")", "\n", "fields", "=", "onmt", ".", "io", ".", "load_fields_from_vocab", "(", "\n", "checkpoint", "[", "'vocab'", "]", ",", "data_type", ")", "\n", "", "else", ":", "\n", "        ", "fields", "=", "onmt", ".", "io", ".", "load_fields_from_vocab", "(", "\n", "torch", ".", "load", "(", "opt", ".", "data", "+", "'.vocab.pt'", ")", ",", "data_type", ")", "\n", "", "fields", "=", "dict", "(", "[", "(", "k", ",", "f", ")", "for", "(", "k", ",", "f", ")", "in", "fields", ".", "items", "(", ")", "\n", "if", "k", "in", "dataset", ".", "examples", "[", "0", "]", ".", "__dict__", "]", ")", "\n", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "print", "(", "' * vocabulary size. source = %d; target = %d'", "%", "\n", "(", "len", "(", "fields", "[", "'src'", "]", ".", "vocab", ")", ",", "len", "(", "fields", "[", "'tgt'", "]", ".", "vocab", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "' * vocabulary size. target = %d'", "%", "\n", "(", "len", "(", "fields", "[", "'tgt'", "]", ".", "vocab", ")", ")", ")", "\n", "\n", "", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.collect_report_features": [[405, 413], ["onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "onmt.io.collect_features", "enumerate", "enumerate", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features"], ["", "def", "collect_report_features", "(", "fields", ")", ":", "\n", "    ", "src_features", "=", "onmt", ".", "io", ".", "collect_features", "(", "fields", ",", "side", "=", "'src'", ")", "\n", "tgt_features", "=", "onmt", ".", "io", ".", "collect_features", "(", "fields", ",", "side", "=", "'tgt'", ")", "\n", "\n", "for", "j", ",", "feat", "in", "enumerate", "(", "src_features", ")", ":", "\n", "        ", "print", "(", "' * src feature %d size = %d'", "%", "(", "j", ",", "len", "(", "fields", "[", "feat", "]", ".", "vocab", ")", ")", ")", "\n", "", "for", "j", ",", "feat", "in", "enumerate", "(", "tgt_features", ")", ":", "\n", "        ", "print", "(", "' * tgt feature %d size = %d'", "%", "(", "j", ",", "len", "(", "fields", "[", "feat", "]", ".", "vocab", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.build_model": [[415, 425], ["print", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "print", "onmt.Utils.use_gpu", "len", "print", "torch.DataParallel"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu"], ["", "", "def", "build_model", "(", "model_opt", ",", "opt", ",", "fields", ",", "checkpoint", ")", ":", "\n", "    ", "print", "(", "'Building model...'", ")", "\n", "model", "=", "onmt", ".", "ModelConstructor", ".", "make_base_model", "(", "model_opt", ",", "fields", ",", "\n", "use_gpu", "(", "opt", ")", ",", "checkpoint", ")", "\n", "if", "len", "(", "opt", ".", "gpuid", ")", ">", "1", ":", "\n", "        ", "print", "(", "'Multi gpu training: '", ",", "opt", ".", "gpuid", ")", "\n", "model", "=", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "opt", ".", "gpuid", ",", "dim", "=", "1", ")", "\n", "", "print", "(", "model", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.build_optim": [[427, 497], ["onmt.Optim.set_parameters", "print", "train.show_optimizer_state", "print", "onmt.Optim.optimizer.state_dict", "print", "onmt.Optim", "onmt.Optim", "onmt.Optim", "onmt.Optim", "onmt.Optim", "onmt.Optim", "model.named_parameters", "onmt.Optim.optimizer.load_state_dict", "onmt.Utils.use_gpu", "print", "train.show_optimizer_state", "onmt.Optim.optimizer.state.values", "RuntimeError", "state.items", "len", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "v.cuda"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.set_parameters", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.show_optimizer_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.show_optimizer_state"], ["", "def", "build_optim", "(", "model", ",", "checkpoint", ")", ":", "\n", "    ", "saved_optimizer_state_dict", "=", "None", "\n", "\n", "if", "opt", ".", "train_from", ":", "\n", "        ", "print", "(", "'Loading optimizer from checkpoint.'", ")", "\n", "optim", "=", "checkpoint", "[", "'optim'", "]", "\n", "# We need to save a copy of optim.optimizer.state_dict() for setting", "\n", "# the, optimizer state later on in Stage 2 in this method, since", "\n", "# the method optim.set_parameters(model.parameters()) will overwrite", "\n", "# optim.optimizer, and with ith the values stored in", "\n", "# optim.optimizer.state_dict()", "\n", "saved_optimizer_state_dict", "=", "optim", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Making optimizer for training.'", ")", "\n", "optim", "=", "onmt", ".", "Optim", "(", "\n", "opt", ".", "optim", ",", "opt", ".", "learning_rate", ",", "opt", ".", "max_grad_norm", ",", "\n", "lr_decay", "=", "opt", ".", "learning_rate_decay", ",", "\n", "start_decay_at", "=", "opt", ".", "start_decay_at", ",", "\n", "beta1", "=", "opt", ".", "adam_beta1", ",", "\n", "beta2", "=", "opt", ".", "adam_beta2", ",", "\n", "eps", "=", "opt", ".", "adam_eps", ",", "\n", "adagrad_accum", "=", "opt", ".", "adagrad_accumulator_init", ",", "\n", "decay_method", "=", "opt", ".", "decay_method", ",", "\n", "warmup_steps", "=", "opt", ".", "warmup_steps", ",", "\n", "model_size", "=", "None", ")", "\n", "\n", "# Stage 1:", "\n", "# Essentially optim.set_parameters (re-)creates and optimizer using", "\n", "# model.paramters() as parameters that will be stored in the", "\n", "# optim.optimizer.param_groups field of the torch optimizer class.", "\n", "# Importantly, this method does not yet load the optimizer state, as", "\n", "# essentially it builds a new optimizer with empty optimizer state and", "\n", "# parameters from the model.", "\n", "", "optim", ".", "set_parameters", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "print", "(", "\n", "\"Stage 1: Keys after executing optim.set_parameters\"", "+", "\n", "\"(model.parameters())\"", ")", "\n", "show_optimizer_state", "(", "optim", ")", "\n", "\n", "if", "opt", ".", "train_from", ":", "\n", "# Stage 2: In this stage, which is only performed when loading an", "\n", "# optimizer from a checkpoint, we load the saved_optimizer_state_dict", "\n", "# into the re-created optimizer, to set the optim.optimizer.state", "\n", "# field, which was previously empty. For this, we use the optimizer", "\n", "# state saved in the \"saved_optimizer_state_dict\" variable for", "\n", "# this purpose.", "\n", "# See also: https://github.com/pytorch/pytorch/issues/2830", "\n", "        ", "optim", ".", "optimizer", ".", "load_state_dict", "(", "saved_optimizer_state_dict", ")", "\n", "# Convert back the state values to cuda type if applicable", "\n", "if", "use_gpu", "(", "opt", ")", ":", "\n", "            ", "for", "state", "in", "optim", ".", "optimizer", ".", "state", ".", "values", "(", ")", ":", "\n", "                ", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", ":", "\n", "                    ", "if", "torch", ".", "is_tensor", "(", "v", ")", ":", "\n", "                        ", "state", "[", "k", "]", "=", "v", ".", "cuda", "(", ")", "\n", "\n", "", "", "", "", "print", "(", "\n", "\"Stage 2: Keys after executing  optim.optimizer.load_state_dict\"", "+", "\n", "\"(saved_optimizer_state_dict)\"", ")", "\n", "show_optimizer_state", "(", "optim", ")", "\n", "\n", "# We want to make sure that indeed we have a non-empty optimizer state", "\n", "# when we loaded an existing model. This should be at least the case", "\n", "# for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state", "\n", "# (Exponential moving average of gradient and squared gradient values)", "\n", "if", "(", "optim", ".", "method", "==", "'adam'", ")", "and", "(", "len", "(", "optim", ".", "optimizer", ".", "state", ")", "<", "1", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"Error: loaded Adam optimizer from existing model\"", "+", "\n", "\" but optimizer state is empty\"", ")", "\n", "\n", "", "", "return", "optim", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.show_optimizer_state": [[500, 509], ["print", "[].keys", "print", "print", "optim.optimizer.state_dict", "print", "optim.optimizer.state_dict", "str", "str"], "function", ["None"], ["", "def", "show_optimizer_state", "(", "optim", ")", ":", "\n", "    ", "print", "(", "\"optim.optimizer.state_dict()['state'] keys: \"", ")", "\n", "for", "key", "in", "optim", ".", "optimizer", ".", "state_dict", "(", ")", "[", "'state'", "]", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "\"optim.optimizer.state_dict()['state'] key: \"", "+", "str", "(", "key", ")", ")", "\n", "\n", "", "print", "(", "\"optim.optimizer.state_dict()['param_groups'] elements: \"", ")", "\n", "for", "element", "in", "optim", ".", "optimizer", ".", "state_dict", "(", ")", "[", "'param_groups'", "]", ":", "\n", "        ", "print", "(", "\"optim.optimizer.state_dict()['param_groups'] element: \"", "+", "str", "(", "\n", "element", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.main": [[511, 566], ["vars().items", "next", "train.load_fields", "train.collect_report_features", "train.build_model", "train.tally_parameters", "train.check_save_model_path", "train.build_optim", "train.train_model", "print", "torch.load", "torch.load", "torch.load", "print", "train.lazily_load_dataset", "writer.close", "print", "torch.load", "torch.load", "torch.load", "vars", "print", "torch.load", "torch.load", "torch.load"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.load_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.collect_report_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.build_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.tally_parameters", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.check_save_model_path", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.build_optim", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.train_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.lazily_load_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["", "", "def", "main", "(", ")", ":", "\n", "# Load checkpoint if we resume from a previous training.", "\n", "    ", "if", "opt", ".", "train_from", ":", "\n", "        ", "print", "(", "'Loading checkpoint from %s'", "%", "opt", ".", "train_from", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "train_from", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model_opt", "=", "checkpoint", "[", "'opt'", "]", "\n", "# I don't like reassigning attributes of opt: it's not clear.", "\n", "opt", ".", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "", "elif", "opt", ".", "init_with", ":", "\n", "        ", "print", "(", "'Loading checkpoint from %s'", "%", "opt", ".", "init_with", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "init_with", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model_opt", "=", "opt", "\n", "", "elif", "opt", ".", "eval_with", ":", "\n", "        ", "print", "(", "'Loading checkpoint from %s'", "%", "opt", ".", "eval_with", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "eval_with", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model_opt", "=", "checkpoint", "[", "\"opt\"", "]", "\n", "model_opt", ".", "eval_only", "=", "1", "\n", "", "else", ":", "\n", "        ", "checkpoint", "=", "None", "\n", "model_opt", "=", "opt", "\n", "\n", "", "for", "k", ",", "v", "in", "vars", "(", "model_opt", ")", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "\"{}: {}\"", ".", "format", "(", "k", ",", "v", ")", ")", "\n", "\n", "# Peek the fisrt dataset to determine the data_type.", "\n", "# (All datasets have the same data_type).", "\n", "", "first_dataset", "=", "next", "(", "lazily_load_dataset", "(", "\"train\"", ")", ")", "\n", "data_type", "=", "first_dataset", ".", "data_type", "\n", "\n", "# Load fields generated from preprocess phase.", "\n", "fields", "=", "load_fields", "(", "first_dataset", ",", "data_type", ",", "checkpoint", ")", "\n", "\n", "# Report src/tgt features.", "\n", "collect_report_features", "(", "fields", ")", "\n", "\n", "# Build model.", "\n", "model", "=", "build_model", "(", "model_opt", ",", "opt", ",", "fields", ",", "checkpoint", ")", "\n", "# Remove bridge for tally params", "\n", "#model.encoder.bridge = None", "\n", "\n", "tally_parameters", "(", "model", ")", "\n", "check_save_model_path", "(", ")", "\n", "\n", "# Build optimizer.", "\n", "optim", "=", "build_optim", "(", "model", ",", "checkpoint", ")", "\n", "\n", "# Do training.", "\n", "train_model", "(", "model", ",", "fields", ",", "optim", ",", "data_type", ",", "model_opt", ")", "\n", "\n", "# If using tensorboard for logging, close the writer after training.", "\n", "if", "opt", ".", "tensorboard", ":", "\n", "        ", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.check_existing_pt_files": [[15, 25], ["glob.glob", "sys.stderr.write", "sys.exit"], "function", ["None"], ["def", "check_existing_pt_files", "(", "opt", ")", ":", "\n", "# We will use glob.glob() to find sharded {train|valid}.[0-9]*.pt", "\n", "# when training, so check to avoid tampering with existing pt files", "\n", "# or mixing them up.", "\n", "    ", "for", "t", "in", "[", "'train'", ",", "'valid'", ",", "'vocab'", "]", ":", "\n", "        ", "pattern", "=", "opt", ".", "save_data", "+", "'.'", "+", "t", "+", "'*.pt'", "\n", "if", "glob", ".", "glob", "(", "pattern", ")", ":", "\n", "            ", "sys", ".", "stderr", ".", "write", "(", "\"Please backup exisiting pt file: %s, \"", "\n", "\"to avoid tampering!\\n\"", "%", "pattern", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args": [[27, 41], ["argparse.ArgumentParser", "onmt.opts.add_md_help_argument", "onmt.opts.add_md_help_argument", "onmt.opts.preprocess_opts", "onmt.opts.preprocess_opts", "argparse.ArgumentParser.parse_args", "torch.manual_seed", "preprocess.check_existing_pt_files"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.add_md_help_argument", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.add_md_help_argument", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.preprocess_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.preprocess_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.check_existing_pt_files"], ["", "", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'preprocess.py'", ",", "\n", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "onmt", ".", "opts", ".", "add_md_help_argument", "(", "parser", ")", "\n", "onmt", ".", "opts", ".", "preprocess_opts", "(", "parser", ")", "\n", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "torch", ".", "manual_seed", "(", "opt", ".", "seed", ")", "\n", "\n", "check_existing_pt_files", "(", "opt", ")", "\n", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_text_dataset_in_shards": [[43, 113], ["os.path.getsize", "onmt.io.ShardedTextCorpusIterator", "onmt.io.ShardedTextCorpusIterator", "onmt.io.ShardedTextCorpusIterator", "onmt.io.ShardedTextCorpusIterator", "print", "print", "onmt.io.ShardedTextCorpusIterator.hit_end", "onmt.io.TextDataset", "onmt.io.TextDataset", "print", "torch.save", "ret_list.append"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator.hit_end"], ["", "def", "build_save_text_dataset_in_shards", "(", "src_corpus", ",", "tgt_corpus", ",", "fields", ",", "\n", "corpus_type", ",", "opt", ")", ":", "\n", "    ", "'''\n    Divide the big corpus into shards, and build dataset separately.\n    This is currently only for data_type=='text'.\n\n    The reason we do this is to avoid taking up too much memory due\n    to sucking in a huge corpus file.\n\n    To tackle this, we only read in part of the corpus file of size\n    `max_shard_size`(actually it is multiples of 64 bytes that equals\n    or is slightly larger than this size), and process it into dataset,\n    then write it to disk along the way. By doing this, we only focus on\n    part of the corpus at any moment, thus effectively reducing memory use.\n    According to test, this method can reduce memory footprint by ~50%.\n\n    Note! As we process along the shards, previous shards might still\n    stay in memory, but since we are done with them, and no more\n    reference to them, if there is memory tight situation, the OS could\n    easily reclaim these memory.\n\n    If `max_shard_size` is 0 or is larger than the corpus size, it is\n    effectively preprocessed into one dataset, i.e. no sharding.\n\n    NOTE! `max_shard_size` is measuring the input corpus size, not the\n    output pt file size. So a shard pt file consists of examples of size\n    2 * `max_shard_size`(source + target).\n    '''", "\n", "\n", "corpus_size", "=", "os", ".", "path", ".", "getsize", "(", "src_corpus", ")", "\n", "if", "corpus_size", ">", "10", "*", "(", "1024", "**", "2", ")", "and", "opt", ".", "max_shard_size", "==", "0", ":", "\n", "        ", "print", "(", "\"Warning. The corpus %s is larger than 10M bytes, you can \"", "\n", "\"set '-max_shard_size' to process it by small shards \"", "\n", "\"to use less memory.\"", "%", "src_corpus", ")", "\n", "\n", "", "if", "opt", ".", "max_shard_size", "!=", "0", ":", "\n", "        ", "print", "(", "' * divide corpus into shards and build dataset separately'", "\n", "'(shard_size = %d bytes).'", "%", "opt", ".", "max_shard_size", ")", "\n", "\n", "", "ret_list", "=", "[", "]", "\n", "src_iter", "=", "onmt", ".", "io", ".", "ShardedTextCorpusIterator", "(", "\n", "src_corpus", ",", "opt", ".", "src_seq_length_trunc", ",", "\n", "\"src\"", ",", "opt", ".", "max_shard_size", ")", "\n", "tgt_iter", "=", "onmt", ".", "io", ".", "ShardedTextCorpusIterator", "(", "\n", "tgt_corpus", ",", "opt", ".", "tgt_seq_length_trunc", ",", "\n", "\"tgt\"", ",", "opt", ".", "max_shard_size", ",", "\n", "assoc_iter", "=", "src_iter", ")", "\n", "\n", "index", "=", "0", "\n", "while", "not", "src_iter", ".", "hit_end", "(", ")", ":", "\n", "        ", "index", "+=", "1", "\n", "dataset", "=", "onmt", ".", "io", ".", "TextDataset", "(", "\n", "fields", ",", "src_iter", ",", "tgt_iter", ",", "\n", "src_iter", ".", "num_feats", ",", "tgt_iter", ".", "num_feats", ",", "\n", "src_seq_length", "=", "opt", ".", "src_seq_length", ",", "\n", "tgt_seq_length", "=", "opt", ".", "tgt_seq_length", ",", "\n", "dynamic_dict", "=", "opt", ".", "dynamic_dict", ",", "\n", "use_filter_pred", "=", "corpus_type", "==", "\"train\"", "or", "not", "opt", ".", "leave_valid", ")", "\n", "\n", "# We save fields in vocab.pt seperately, so make it empty.", "\n", "dataset", ".", "fields", "=", "[", "]", "\n", "\n", "pt_file", "=", "\"{:s}.{:s}.{:d}.pt\"", ".", "format", "(", "\n", "opt", ".", "save_data", ",", "corpus_type", ",", "index", ")", "\n", "print", "(", "\" * saving %s data shard to %s.\"", "%", "(", "corpus_type", ",", "pt_file", ")", ")", "\n", "torch", ".", "save", "(", "dataset", ",", "pt_file", ")", "\n", "\n", "ret_list", ".", "append", "(", "pt_file", ")", "\n", "\n", "", "return", "ret_list", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_dataset": [[115, 157], ["onmt.io.build_dataset", "onmt.io.build_dataset", "print", "torch.save", "preprocess.build_save_text_dataset_in_shards"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_text_dataset_in_shards"], ["", "def", "build_save_dataset", "(", "corpus_type", ",", "fields", ",", "opt", ")", ":", "\n", "    ", "assert", "corpus_type", "in", "[", "'train'", ",", "'valid'", "]", "\n", "\n", "if", "corpus_type", "==", "'train'", ":", "\n", "        ", "src_corpus", "=", "opt", ".", "train_src", "\n", "tgt_corpus", "=", "opt", ".", "train_tgt", "\n", "", "else", ":", "\n", "        ", "src_corpus", "=", "opt", ".", "valid_src", "\n", "tgt_corpus", "=", "opt", ".", "valid_tgt", "\n", "\n", "# Currently we only do preprocess sharding for corpus: data_type=='text'.", "\n", "", "if", "opt", ".", "data_type", "==", "'text'", ":", "\n", "        ", "return", "build_save_text_dataset_in_shards", "(", "\n", "src_corpus", ",", "tgt_corpus", ",", "fields", ",", "\n", "corpus_type", ",", "opt", ")", "\n", "\n", "# For data_type == 'img' or 'audio', currently we don't do", "\n", "# preprocess sharding. We only build a monolithic dataset.", "\n", "# But since the interfaces are uniform, it would be not hard", "\n", "# to do this should users need this feature.", "\n", "", "dataset", "=", "onmt", ".", "io", ".", "build_dataset", "(", "\n", "fields", ",", "opt", ".", "data_type", ",", "src_corpus", ",", "tgt_corpus", ",", "\n", "src_dir", "=", "opt", ".", "src_dir", ",", "\n", "src_seq_length", "=", "opt", ".", "src_seq_length", ",", "\n", "tgt_seq_length", "=", "opt", ".", "tgt_seq_length", ",", "\n", "src_seq_length_trunc", "=", "opt", ".", "src_seq_length_trunc", ",", "\n", "tgt_seq_length_trunc", "=", "opt", ".", "tgt_seq_length_trunc", ",", "\n", "dynamic_dict", "=", "opt", ".", "dynamic_dict", ",", "\n", "sample_rate", "=", "opt", ".", "sample_rate", ",", "\n", "window_size", "=", "opt", ".", "window_size", ",", "\n", "window_stride", "=", "opt", ".", "window_stride", ",", "\n", "window", "=", "opt", ".", "window", ",", "\n", "use_filter_pred", "=", "corpus_type", "==", "\"train\"", "or", "not", "opt", ".", "leave_valid", ")", "\n", "\n", "# We save fields in vocab.pt seperately, so make it empty.", "\n", "dataset", ".", "fields", "=", "[", "]", "\n", "\n", "pt_file", "=", "\"{:s}.{:s}.pt\"", ".", "format", "(", "opt", ".", "save_data", ",", "corpus_type", ")", "\n", "print", "(", "\" * saving %s dataset to %s.\"", "%", "(", "corpus_type", ",", "pt_file", ")", ")", "\n", "torch", ".", "save", "(", "dataset", ",", "pt_file", ")", "\n", "\n", "return", "[", "pt_file", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_vocab": [[159, 172], ["onmt.io.build_vocab", "onmt.io.build_vocab", "torch.save", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab"], ["", "def", "build_save_vocab", "(", "train_dataset", ",", "fields", ",", "opt", ")", ":", "\n", "    ", "fields", "=", "onmt", ".", "io", ".", "build_vocab", "(", "train_dataset", ",", "fields", ",", "opt", ".", "data_type", ",", "\n", "opt", ".", "share_vocab", ",", "\n", "opt", ".", "src_vocab", ",", "\n", "opt", ".", "src_vocab_size", ",", "\n", "opt", ".", "src_words_min_frequency", ",", "\n", "opt", ".", "tgt_vocab", ",", "\n", "opt", ".", "tgt_vocab_size", ",", "\n", "opt", ".", "tgt_words_min_frequency", ")", "\n", "\n", "# Can't save fields, so remove/reconstruct at training time.", "\n", "vocab_file", "=", "opt", ".", "save_data", "+", "'.vocab.pt'", "\n", "torch", ".", "save", "(", "onmt", ".", "io", ".", "save_fields_to_vocab", "(", "fields", ")", ",", "vocab_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.main": [[174, 194], ["preprocess.parse_args", "print", "onmt.io.get_num_features", "onmt.io.get_num_features", "onmt.io.get_num_features", "onmt.io.get_num_features", "print", "print", "print", "onmt.io.get_fields", "onmt.io.get_fields", "print", "preprocess.build_save_dataset", "print", "preprocess.build_save_vocab", "print", "preprocess.build_save_dataset"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_dataset"], ["", "def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "parse_args", "(", ")", "\n", "\n", "print", "(", "\"Extracting features...\"", ")", "\n", "src_nfeats", "=", "onmt", ".", "io", ".", "get_num_features", "(", "opt", ".", "data_type", ",", "opt", ".", "train_src", ",", "'src'", ")", "\n", "tgt_nfeats", "=", "onmt", ".", "io", ".", "get_num_features", "(", "opt", ".", "data_type", ",", "opt", ".", "train_tgt", ",", "'tgt'", ")", "\n", "print", "(", "\" * number of source features: %d.\"", "%", "src_nfeats", ")", "\n", "print", "(", "\" * number of target features: %d.\"", "%", "tgt_nfeats", ")", "\n", "\n", "print", "(", "\"Building `Fields` object...\"", ")", "\n", "fields", "=", "onmt", ".", "io", ".", "get_fields", "(", "opt", ".", "data_type", ",", "src_nfeats", ",", "tgt_nfeats", ")", "\n", "\n", "print", "(", "\"Building & saving training data...\"", ")", "\n", "train_dataset_files", "=", "build_save_dataset", "(", "'train'", ",", "fields", ",", "opt", ")", "\n", "\n", "print", "(", "\"Building & saving vocabulary...\"", ")", "\n", "build_save_vocab", "(", "train_dataset_files", ",", "fields", ",", "opt", ")", "\n", "\n", "print", "(", "\"Building & saving validation data...\"", ")", "\n", "build_save_dataset", "(", "'valid'", ",", "fields", ",", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.server.start": [[11, 103], ["flask.Flask", "server.start.prefix_route"], "function", ["None"], ["def", "start", "(", "config_file", ",", "\n", "url_root", "=", "\"./translator\"", ",", "\n", "host", "=", "\"0.0.0.0\"", ",", "\n", "port", "=", "5000", ",", "\n", "debug", "=", "True", ")", ":", "\n", "    ", "def", "prefix_route", "(", "route_function", ",", "prefix", "=", "''", ",", "mask", "=", "'{0}{1}'", ")", ":", "\n", "        ", "def", "newroute", "(", "route", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "return", "route_function", "(", "mask", ".", "format", "(", "prefix", ",", "route", ")", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "return", "newroute", "\n", "\n", "", "app", "=", "Flask", "(", "__name__", ")", "\n", "app", ".", "route", "=", "prefix_route", "(", "app", ".", "route", ",", "url_root", ")", "\n", "translation_server", "=", "TranslationServer", "(", ")", "\n", "translation_server", ".", "start", "(", "config_file", ")", "\n", "\n", "@", "app", ".", "route", "(", "'/models'", ",", "methods", "=", "[", "'GET'", "]", ")", "\n", "def", "get_models", "(", ")", ":", "\n", "        ", "out", "=", "translation_server", ".", "list_models", "(", ")", "\n", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/clone_model/<int:model_id>'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "clone_model", "(", "model_id", ")", ":", "\n", "        ", "out", "=", "{", "}", "\n", "data", "=", "request", ".", "get_json", "(", "force", "=", "True", ")", "\n", "timeout", "=", "-", "1", "\n", "if", "'timeout'", "in", "data", ":", "\n", "            ", "timeout", "=", "data", "[", "'timeout'", "]", "\n", "del", "data", "[", "'timeout'", "]", "\n", "\n", "", "opt", "=", "data", ".", "get", "(", "'opt'", ",", "None", ")", "\n", "try", ":", "\n", "            ", "model_id", ",", "load_time", "=", "translation_server", ".", "clone_model", "(", "\n", "model_id", ",", "opt", ",", "timeout", ")", "\n", "", "except", "ServerModelError", "as", "e", ":", "\n", "            ", "out", "[", "'status'", "]", "=", "STATUS_ERROR", "\n", "out", "[", "'error'", "]", "=", "str", "(", "e", ")", "\n", "", "else", ":", "\n", "            ", "out", "[", "'status'", "]", "=", "STATUS_OK", "\n", "out", "[", "'model_id'", "]", "=", "model_id", "\n", "out", "[", "'load_time'", "]", "=", "load_time", "\n", "\n", "", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/unload_model/<int:model_id>'", ",", "methods", "=", "[", "'GET'", "]", ")", "\n", "def", "unload_model", "(", "model_id", ")", ":", "\n", "        ", "out", "=", "{", "\"model_id\"", ":", "model_id", "}", "\n", "\n", "try", ":", "\n", "            ", "translation_server", ".", "unload_model", "(", "model_id", ")", "\n", "out", "[", "'status'", "]", "=", "STATUS_OK", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "out", "[", "'status'", "]", "=", "STATUS_ERROR", "\n", "out", "[", "'error'", "]", "=", "str", "(", "e", ")", "\n", "\n", "", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/translate'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "translate", "(", ")", ":", "\n", "        ", "inputs", "=", "request", ".", "get_json", "(", "force", "=", "True", ")", "\n", "out", "=", "{", "}", "\n", "try", ":", "\n", "            ", "translation", ",", "scores", ",", "n_best", ",", "times", "=", "translation_server", ".", "run", "(", "inputs", ")", "\n", "assert", "len", "(", "translation", ")", "==", "len", "(", "inputs", ")", "\n", "assert", "len", "(", "scores", ")", "==", "len", "(", "inputs", ")", "\n", "\n", "out", "=", "[", "[", "{", "\"src\"", ":", "inputs", "[", "i", "]", "[", "'src'", "]", ",", "\"tgt\"", ":", "translation", "[", "i", "]", ",", "\n", "\"n_best\"", ":", "n_best", ",", "\n", "\"pred_score\"", ":", "scores", "[", "i", "]", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "translation", ")", ")", "]", "]", "\n", "", "except", "ServerModelError", "as", "e", ":", "\n", "            ", "out", "[", "'error'", "]", "=", "str", "(", "e", ")", "\n", "out", "[", "'status'", "]", "=", "STATUS_ERROR", "\n", "\n", "", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/to_cpu/<int:model_id>'", ",", "methods", "=", "[", "'GET'", "]", ")", "\n", "def", "to_cpu", "(", "model_id", ")", ":", "\n", "        ", "out", "=", "{", "'model_id'", ":", "model_id", "}", "\n", "translation_server", ".", "models", "[", "model_id", "]", ".", "to_cpu", "(", ")", "\n", "\n", "out", "[", "'status'", "]", "=", "STATUS_OK", "\n", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/to_gpu/<int:model_id>'", ",", "methods", "=", "[", "'GET'", "]", ")", "\n", "def", "to_gpu", "(", "model_id", ")", ":", "\n", "        ", "out", "=", "{", "'model_id'", ":", "model_id", "}", "\n", "translation_server", ".", "models", "[", "model_id", "]", ".", "to_gpu", "(", ")", "\n", "\n", "out", "[", "'status'", "]", "=", "STATUS_OK", "\n", "return", "jsonify", "(", "out", ")", "\n", "\n", "", "app", ".", "run", "(", "debug", "=", "debug", ",", "host", "=", "host", ",", "port", "=", "port", ",", "use_reloader", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.__init__": [[26, 29], ["unittest.TestCase.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "TestModel", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab": [[32, 36], ["src.build_vocab", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "src", "=", "onmt", ".", "io", ".", "get_fields", "(", "\"text\"", ",", "0", ",", "0", ")", "[", "\"src\"", "]", "\n", "src", ".", "build_vocab", "(", "[", "]", ")", "\n", "return", "src", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch": [[37, 43], ["torch.autograd.Variable().long", "torch.autograd.Variable().long", "torch.ones().fill_().long", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ones().fill_", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "get_batch", "(", "self", ",", "source_l", "=", "3", ",", "bsize", "=", "1", ")", ":", "\n", "# len x batch x nfeat", "\n", "        ", "test_src", "=", "Variable", "(", "torch", ".", "ones", "(", "source_l", ",", "bsize", ",", "1", ")", ")", ".", "long", "(", ")", "\n", "test_tgt", "=", "Variable", "(", "torch", ".", "ones", "(", "source_l", ",", "bsize", ",", "1", ")", ")", ".", "long", "(", ")", "\n", "test_length", "=", "torch", ".", "ones", "(", "bsize", ")", ".", "fill_", "(", "source_l", ")", ".", "long", "(", ")", "\n", "return", "test_src", ",", "test_tgt", ",", "test_length", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch_image": [[44, 50], ["torch.autograd.Variable().float", "torch.autograd.Variable().long", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "get_batch_image", "(", "self", ",", "tgt_l", "=", "3", ",", "bsize", "=", "1", ",", "h", "=", "15", ",", "w", "=", "17", ")", ":", "\n", "# batch x c x h x w", "\n", "        ", "test_src", "=", "Variable", "(", "torch", ".", "ones", "(", "bsize", ",", "3", ",", "h", ",", "w", ")", ")", ".", "float", "(", ")", "\n", "test_tgt", "=", "Variable", "(", "torch", ".", "ones", "(", "tgt_l", ",", "bsize", ",", "1", ")", ")", ".", "long", "(", ")", "\n", "test_length", "=", "None", "\n", "return", "test_src", ",", "test_tgt", ",", "test_length", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch_audio": [[51, 59], ["int", "torch.autograd.Variable().float", "torch.autograd.Variable().long", "math.floor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "get_batch_audio", "(", "self", ",", "tgt_l", "=", "3", ",", "bsize", "=", "1", ",", "sample_rate", "=", "5500", ",", "\n", "window_size", "=", "0.03", ",", "t", "=", "37", ")", ":", "\n", "# batch x 1 x nfft x t", "\n", "        ", "nfft", "=", "int", "(", "math", ".", "floor", "(", "(", "sample_rate", "*", "window_size", ")", "/", "2", ")", "+", "1", ")", "\n", "test_src", "=", "Variable", "(", "torch", ".", "ones", "(", "bsize", ",", "1", ",", "nfft", ",", "t", ")", ")", ".", "float", "(", ")", "\n", "test_tgt", "=", "Variable", "(", "torch", ".", "ones", "(", "tgt_l", ",", "bsize", ",", "1", ")", ")", ".", "long", "(", ")", "\n", "test_length", "=", "None", "\n", "return", "test_src", ",", "test_tgt", ",", "test_length", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.embeddings_forward": [[60, 84], ["test_models.TestModel.get_vocab", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "test_models.TestModel.get_batch", "test_models.TestModel.assertEqual", "torch.cat", "onmt.ModelConstructor.make_embeddings.", "onmt.ModelConstructor.make_embeddings.", "onmt.ModelConstructor.make_embeddings.", "torch.zeros", "onmt.ModelConstructor.make_embeddings.", "onmt.ModelConstructor.make_embeddings.", "onmt.ModelConstructor.make_embeddings.", "torch.zeros", "onmt.ModelConstructor.make_embeddings.size", "torch.zeros.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch"], ["", "def", "embeddings_forward", "(", "self", ",", "opt", ",", "source_l", "=", "3", ",", "bsize", "=", "1", ")", ":", "\n", "        ", "'''\n        Tests if the embeddings works as expected\n\n        args:\n            opt: set of options\n            source_l: Length of generated input sentence\n            bsize: Batchsize of generated input\n        '''", "\n", "word_dict", "=", "self", ".", "get_vocab", "(", ")", "\n", "feature_dicts", "=", "[", "]", "\n", "emb", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ")", "\n", "test_src", ",", "_", ",", "__", "=", "self", ".", "get_batch", "(", "source_l", "=", "source_l", ",", "\n", "bsize", "=", "bsize", ")", "\n", "if", "opt", ".", "decoder_type", "==", "'transformer'", ":", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "[", "test_src", ",", "test_src", "]", ",", "0", ")", "\n", "res", "=", "emb", "(", "input", ")", "\n", "compare_to", "=", "torch", ".", "zeros", "(", "source_l", "*", "2", ",", "bsize", ",", "\n", "opt", ".", "src_word_vec_size", ")", "\n", "", "else", ":", "\n", "            ", "res", "=", "emb", "(", "test_src", ")", "\n", "compare_to", "=", "torch", ".", "zeros", "(", "source_l", ",", "bsize", ",", "opt", ".", "src_word_vec_size", ")", "\n", "\n", "", "self", ".", "assertEqual", "(", "res", ".", "size", "(", ")", ",", "compare_to", ".", "size", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.encoder_forward": [[85, 115], ["test_models.TestModel.get_vocab", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_encoder", "onmt.ModelConstructor.make_encoder", "onmt.ModelConstructor.make_encoder", "test_models.TestModel.get_batch", "onmt.ModelConstructor.make_encoder.", "onmt.ModelConstructor.make_encoder.", "onmt.ModelConstructor.make_encoder.", "torch.zeros", "torch.zeros", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "torch.zeros.size", "hidden_t[].size", "hidden_t[].size", "torch.zeros.size", "outputs.size", "type", "type"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch"], ["", "def", "encoder_forward", "(", "self", ",", "opt", ",", "source_l", "=", "3", ",", "bsize", "=", "1", ")", ":", "\n", "        ", "'''\n        Tests if the encoder works as expected\n\n        args:\n            opt: set of options\n            source_l: Length of generated input sentence\n            bsize: Batchsize of generated input\n        '''", "\n", "word_dict", "=", "self", ".", "get_vocab", "(", ")", "\n", "feature_dicts", "=", "[", "]", "\n", "embeddings", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ")", "\n", "enc", "=", "make_encoder", "(", "opt", ",", "embeddings", ")", "\n", "\n", "test_src", ",", "test_tgt", ",", "test_length", "=", "self", ".", "get_batch", "(", "source_l", "=", "source_l", ",", "\n", "bsize", "=", "bsize", ")", "\n", "\n", "hidden_t", ",", "outputs", "=", "enc", "(", "test_src", ",", "test_length", ")", "\n", "\n", "# Initialize vectors to compare size with", "\n", "test_hid", "=", "torch", ".", "zeros", "(", "self", ".", "opt", ".", "enc_layers", ",", "bsize", ",", "opt", ".", "rnn_size", ")", "\n", "test_out", "=", "torch", ".", "zeros", "(", "source_l", ",", "bsize", ",", "opt", ".", "rnn_size", ")", "\n", "\n", "# Ensure correct sizes and types", "\n", "self", ".", "assertEqual", "(", "test_hid", ".", "size", "(", ")", ",", "\n", "hidden_t", "[", "0", "]", ".", "size", "(", ")", ",", "\n", "hidden_t", "[", "1", "]", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "test_out", ".", "size", "(", ")", ",", "outputs", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ")", ",", "torch", ".", "autograd", ".", "Variable", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ".", "data", ")", ",", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.nmtmodel_forward": [[116, 148], ["test_models.TestModel.get_vocab", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_encoder", "onmt.ModelConstructor.make_encoder", "onmt.ModelConstructor.make_encoder", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "test_models.TestModel.get_batch", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "torch.zeros", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "outputs.size", "torch.zeros.size", "type", "type"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch"], ["", "def", "nmtmodel_forward", "(", "self", ",", "opt", ",", "source_l", "=", "3", ",", "bsize", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Creates a nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        \"\"\"", "\n", "word_dict", "=", "self", ".", "get_vocab", "(", ")", "\n", "feature_dicts", "=", "[", "]", "\n", "\n", "embeddings", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ")", "\n", "enc", "=", "make_encoder", "(", "opt", ",", "embeddings", ")", "\n", "\n", "embeddings", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ",", "\n", "for_encoder", "=", "False", ")", "\n", "dec", "=", "make_decoder", "(", "opt", ",", "embeddings", ")", "\n", "\n", "model", "=", "onmt", ".", "Models", ".", "NMTModel", "(", "enc", ",", "dec", ")", "\n", "\n", "test_src", ",", "test_tgt", ",", "test_length", "=", "self", ".", "get_batch", "(", "source_l", "=", "source_l", ",", "\n", "bsize", "=", "bsize", ")", "\n", "outputs", ",", "attn", ",", "_", "=", "model", "(", "test_src", ",", "\n", "test_tgt", ",", "\n", "test_length", ")", "\n", "outputsize", "=", "torch", ".", "zeros", "(", "source_l", "-", "1", ",", "bsize", ",", "opt", ".", "rnn_size", ")", "\n", "# Make sure that output has the correct size and type", "\n", "self", ".", "assertEqual", "(", "outputs", ".", "size", "(", ")", ",", "outputsize", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ")", ",", "torch", ".", "autograd", ".", "Variable", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ".", "data", ")", ",", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.imagemodel_forward": [[149, 188], ["test_models.TestModel.get_vocab", "onmt.modules.ImageEncoder", "onmt.modules.ImageEncoder", "onmt.modules.ImageEncoder", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "test_models.TestModel.get_batch_image", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "torch.zeros", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "outputs.size", "torch.zeros.size", "type", "type"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch_image"], ["", "def", "imagemodel_forward", "(", "self", ",", "opt", ",", "tgt_l", "=", "2", ",", "bsize", "=", "1", ",", "h", "=", "15", ",", "w", "=", "17", ")", ":", "\n", "        ", "\"\"\"\n        Creates an image-to-text nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        \"\"\"", "\n", "if", "opt", ".", "encoder_type", "==", "'transformer'", "or", "opt", ".", "encoder_type", "==", "'cnn'", ":", "\n", "            ", "return", "\n", "\n", "", "word_dict", "=", "self", ".", "get_vocab", "(", ")", "\n", "feature_dicts", "=", "[", "]", "\n", "\n", "enc", "=", "ImageEncoder", "(", "opt", ".", "enc_layers", ",", "\n", "opt", ".", "brnn", ",", "\n", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "dropout", ")", "\n", "\n", "embeddings", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ",", "\n", "for_encoder", "=", "False", ")", "\n", "dec", "=", "make_decoder", "(", "opt", ",", "embeddings", ")", "\n", "\n", "model", "=", "onmt", ".", "Models", ".", "NMTModel", "(", "enc", ",", "dec", ")", "\n", "\n", "test_src", ",", "test_tgt", ",", "test_length", "=", "self", ".", "get_batch_image", "(", "\n", "h", "=", "h", ",", "w", "=", "w", ",", "\n", "bsize", "=", "bsize", ",", "\n", "tgt_l", "=", "tgt_l", ")", "\n", "outputs", ",", "attn", ",", "_", "=", "model", "(", "test_src", ",", "\n", "test_tgt", ",", "\n", "test_length", ")", "\n", "outputsize", "=", "torch", ".", "zeros", "(", "tgt_l", "-", "1", ",", "bsize", ",", "opt", ".", "rnn_size", ")", "\n", "# Make sure that output has the correct size and type", "\n", "self", ".", "assertEqual", "(", "outputs", ".", "size", "(", ")", ",", "outputsize", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ")", ",", "torch", ".", "autograd", ".", "Variable", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ".", "data", ")", ",", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.audiomodel_forward": [[189, 231], ["test_models.TestModel.get_vocab", "onmt.modules.AudioEncoder", "onmt.modules.AudioEncoder", "onmt.modules.AudioEncoder", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_embeddings", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.ModelConstructor.make_decoder", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "onmt.Models.NMTModel", "test_models.TestModel.get_batch_audio", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "onmt.Models.NMTModel.", "torch.zeros", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "test_models.TestModel.assertEqual", "outputs.size", "torch.zeros.size", "type", "type"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models.TestModel.get_batch_audio"], ["", "def", "audiomodel_forward", "(", "self", ",", "opt", ",", "tgt_l", "=", "2", ",", "bsize", "=", "1", ",", "t", "=", "37", ")", ":", "\n", "        ", "\"\"\"\n        Creates a speech-to-text nmtmodel with a custom opt function.\n        Forwards a testbatch and checks output size.\n\n        Args:\n            opt: Namespace with options\n            source_l: length of input sequence\n            bsize: batchsize\n        \"\"\"", "\n", "if", "opt", ".", "encoder_type", "==", "'transformer'", "or", "opt", ".", "encoder_type", "==", "'cnn'", ":", "\n", "            ", "return", "\n", "\n", "", "word_dict", "=", "self", ".", "get_vocab", "(", ")", "\n", "feature_dicts", "=", "[", "]", "\n", "\n", "enc", "=", "AudioEncoder", "(", "opt", ".", "enc_layers", ",", "\n", "opt", ".", "brnn", ",", "\n", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "dropout", ",", "\n", "opt", ".", "sample_rate", ",", "\n", "opt", ".", "window_size", ")", "\n", "\n", "embeddings", "=", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ",", "\n", "for_encoder", "=", "False", ")", "\n", "dec", "=", "make_decoder", "(", "opt", ",", "embeddings", ")", "\n", "\n", "model", "=", "onmt", ".", "Models", ".", "NMTModel", "(", "enc", ",", "dec", ")", "\n", "\n", "test_src", ",", "test_tgt", ",", "test_length", "=", "self", ".", "get_batch_audio", "(", "\n", "bsize", "=", "bsize", ",", "\n", "sample_rate", "=", "opt", ".", "sample_rate", ",", "\n", "window_size", "=", "opt", ".", "window_size", ",", "\n", "t", "=", "t", ",", "tgt_l", "=", "tgt_l", ")", "\n", "outputs", ",", "attn", ",", "_", "=", "model", "(", "test_src", ",", "\n", "test_tgt", ",", "\n", "test_length", ")", "\n", "outputsize", "=", "torch", ".", "zeros", "(", "tgt_l", "-", "1", ",", "bsize", ",", "opt", ".", "rnn_size", ")", "\n", "# Make sure that output has the correct size and type", "\n", "self", ".", "assertEqual", "(", "outputs", ".", "size", "(", ")", ",", "outputsize", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ")", ",", "torch", ".", "autograd", ".", "Variable", ")", "\n", "self", ".", "assertEqual", "(", "type", "(", "outputs", ".", "data", ")", ",", "torch", ".", "FloatTensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_models._add_test": [[233, 257], ["setattr", "copy.deepcopy", "getattr", "setattr", "str().split", "str"], "function", ["None"], ["", "", "def", "_add_test", "(", "param_setting", ",", "methodname", ")", ":", "\n", "    ", "\"\"\"\n    Adds a Test to TestModel according to settings\n\n    Args:\n        param_setting: list of tuples of (param, setting)\n        methodname: name of the method that gets called\n    \"\"\"", "\n", "\n", "def", "test_method", "(", "self", ")", ":", "\n", "        ", "if", "param_setting", ":", "\n", "            ", "opt", "=", "copy", ".", "deepcopy", "(", "self", ".", "opt", ")", "\n", "for", "param", ",", "setting", "in", "param_setting", ":", "\n", "                ", "setattr", "(", "opt", ",", "param", ",", "setting", ")", "\n", "", "", "else", ":", "\n", "            ", "opt", "=", "self", ".", "opt", "\n", "", "getattr", "(", "self", ",", "methodname", ")", "(", "opt", ")", "\n", "", "if", "param_setting", ":", "\n", "        ", "name", "=", "'test_'", "+", "methodname", "+", "\"_\"", "+", "\"_\"", ".", "join", "(", "\n", "str", "(", "param_setting", ")", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "name", "=", "'test_'", "+", "methodname", "+", "'_standard'", "\n", "", "setattr", "(", "TestModel", ",", "name", ",", "test_method", ")", "\n", "test_method", ".", "__name__", "=", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_preprocess.TestData.__init__": [[36, 39], ["unittest.TestCase.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "TestData", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_preprocess.TestData.dataset_build": [[40, 63], ["onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "onmt.io.get_fields", "preprocess.build_save_dataset", "preprocess.build_save_vocab", "preprocess.build_save_dataset", "glob.glob", "hasattr", "hasattr", "os.remove", "hasattr", "os.path.exists", "os.remove", "hasattr", "os.path.exists", "os.remove", "len", "codecs.open", "f.write", "len", "codecs.open", "f.write"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.build_save_dataset"], ["", "def", "dataset_build", "(", "self", ",", "opt", ")", ":", "\n", "        ", "fields", "=", "onmt", ".", "io", ".", "get_fields", "(", "\"text\"", ",", "0", ",", "0", ")", "\n", "\n", "if", "hasattr", "(", "opt", ",", "'src_vocab'", ")", "and", "len", "(", "opt", ".", "src_vocab", ")", ">", "0", ":", "\n", "            ", "with", "codecs", ".", "open", "(", "opt", ".", "src_vocab", ",", "'w'", ",", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'a\\nb\\nc\\nd\\ne\\nf\\n'", ")", "\n", "", "", "if", "hasattr", "(", "opt", ",", "'tgt_vocab'", ")", "and", "len", "(", "opt", ".", "tgt_vocab", ")", ">", "0", ":", "\n", "            ", "with", "codecs", ".", "open", "(", "opt", ".", "tgt_vocab", ",", "'w'", ",", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'a\\nb\\nc\\nd\\ne\\nf\\n'", ")", "\n", "\n", "", "", "train_data_files", "=", "preprocess", ".", "build_save_dataset", "(", "'train'", ",", "fields", ",", "opt", ")", "\n", "\n", "preprocess", ".", "build_save_vocab", "(", "train_data_files", ",", "fields", ",", "opt", ")", "\n", "\n", "preprocess", ".", "build_save_dataset", "(", "'valid'", ",", "fields", ",", "opt", ")", "\n", "\n", "# Remove the generated *pt files.", "\n", "for", "pt", "in", "glob", ".", "glob", "(", "SAVE_DATA_PREFIX", "+", "'*.pt'", ")", ":", "\n", "            ", "os", ".", "remove", "(", "pt", ")", "\n", "", "if", "hasattr", "(", "opt", ",", "'src_vocab'", ")", "and", "os", ".", "path", ".", "exists", "(", "opt", ".", "src_vocab", ")", ":", "\n", "            ", "os", ".", "remove", "(", "opt", ".", "src_vocab", ")", "\n", "", "if", "hasattr", "(", "opt", ",", "'tgt_vocab'", ")", "and", "os", ".", "path", ".", "exists", "(", "opt", ".", "tgt_vocab", ")", ":", "\n", "            ", "os", ".", "remove", "(", "opt", ".", "tgt_vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_preprocess.TestData.test_merge_vocab": [[64, 75], ["torchtext.vocab.Vocab", "torchtext.vocab.Vocab", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "onmt.io.merge_vocabs", "test_preprocess.TestData.assertEqual", "test_preprocess.TestData.assertEqual", "test_preprocess.TestData.assertTrue", "collections.Counter", "collections.Counter", "collections.Counter", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs"], ["", "", "def", "test_merge_vocab", "(", "self", ")", ":", "\n", "        ", "va", "=", "torchtext", ".", "vocab", ".", "Vocab", "(", "Counter", "(", "'abbccc'", ")", ")", "\n", "vb", "=", "torchtext", ".", "vocab", ".", "Vocab", "(", "Counter", "(", "'eeabbcccf'", ")", ")", "\n", "\n", "merged", "=", "onmt", ".", "io", ".", "merge_vocabs", "(", "[", "va", ",", "vb", "]", ",", "2", ")", "\n", "\n", "self", ".", "assertEqual", "(", "Counter", "(", "{", "'c'", ":", "6", ",", "'b'", ":", "4", ",", "'a'", ":", "2", ",", "'e'", ":", "2", ",", "'f'", ":", "1", "}", ")", ",", "\n", "merged", ".", "freqs", ")", "\n", "# 4 specicials + 2 words (since we pass 2 to merge_vocabs)", "\n", "self", ".", "assertEqual", "(", "6", ",", "len", "(", "merged", ".", "itos", ")", ")", "\n", "self", ".", "assertTrue", "(", "'b'", "in", "merged", ".", "itos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_preprocess._add_test": [[77, 101], ["setattr", "copy.deepcopy", "getattr", "setattr", "str().split", "str"], "function", ["None"], ["", "", "def", "_add_test", "(", "param_setting", ",", "methodname", ")", ":", "\n", "    ", "\"\"\"\n    Adds a Test to TestData according to settings\n\n    Args:\n        param_setting: list of tuples of (param, setting)\n        methodname: name of the method that gets called\n    \"\"\"", "\n", "\n", "def", "test_method", "(", "self", ")", ":", "\n", "        ", "if", "param_setting", ":", "\n", "            ", "opt", "=", "copy", ".", "deepcopy", "(", "self", ".", "opt", ")", "\n", "for", "param", ",", "setting", "in", "param_setting", ":", "\n", "                ", "setattr", "(", "opt", ",", "param", ",", "setting", ")", "\n", "", "", "else", ":", "\n", "            ", "opt", "=", "self", ".", "opt", "\n", "", "getattr", "(", "self", ",", "methodname", ")", "(", "opt", ")", "\n", "", "if", "param_setting", ":", "\n", "        ", "name", "=", "'test_'", "+", "methodname", "+", "\"_\"", "+", "\"_\"", ".", "join", "(", "\n", "str", "(", "param_setting", ")", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "name", "=", "'test_'", "+", "methodname", "+", "'_standard'", "\n", "", "setattr", "(", "TestData", ",", "name", ",", "test_method", ")", "\n", "test_method", ".", "__name__", "=", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_simple.test_load": [[4, 7], ["None"], "function", ["None"], ["def", "test_load", "(", ")", ":", "\n", "    ", "onmt", "\n", "pass", "\n", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.test.test_attention.TestAttention.test_masked_global_attention": [[14, 33], ["torch.IntTensor", "torch.IntTensor.size", "torch.autograd.Variable", "torch.autograd.Variable", "onmt.modules.GlobalAttention", "onmt.modules.GlobalAttention.", "torch.randn", "torch.randn", "torch.IntTensor.max"], "methods", ["None"], ["    ", "def", "test_masked_global_attention", "(", "self", ")", ":", "\n", "        ", "source_lengths", "=", "torch", ".", "IntTensor", "(", "[", "7", ",", "3", ",", "5", ",", "2", "]", ")", "\n", "# illegal_weights_mask = torch.ByteTensor([", "\n", "#     [0, 0, 0, 0, 0, 0, 0],", "\n", "#     [0, 0, 0, 1, 1, 1, 1],", "\n", "#     [0, 0, 0, 0, 0, 1, 1],", "\n", "#     [0, 0, 1, 1, 1, 1, 1]])", "\n", "\n", "batch_size", "=", "source_lengths", ".", "size", "(", "0", ")", "\n", "dim", "=", "20", "\n", "\n", "memory_bank", "=", "Variable", "(", "torch", ".", "randn", "(", "batch_size", ",", "\n", "source_lengths", ".", "max", "(", ")", ",", "dim", ")", ")", "\n", "hidden", "=", "Variable", "(", "torch", ".", "randn", "(", "batch_size", ",", "dim", ")", ")", "\n", "\n", "attn", "=", "onmt", ".", "modules", ".", "GlobalAttention", "(", "dim", ")", "\n", "\n", "_", ",", "alignments", "=", "attn", "(", "hidden", ",", "memory_bank", ",", "\n", "memory_lengths", "=", "source_lengths", ")", "\n", "# TODO: fix for pytorch 0.3", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings": [[32, 76], ["len", "onmt.modules.Embeddings", "len"], "function", ["None"], ["def", "make_embeddings", "(", "opt", ",", "word_dict", ",", "feature_dicts", ",", "for_encoder", "=", "True", ",", "\n", "for_inference_network", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Make an Embeddings instance.\n    Args:\n        opt: the option in current environment.\n        word_dict(Vocab): words dictionary.\n        feature_dicts([Vocab], optional): a list of feature dictionary.\n        for_encoder(bool): make Embeddings for encoder or decoder?\n    \"\"\"", "\n", "if", "for_encoder", ":", "\n", "        ", "if", "not", "for_inference_network", ":", "\n", "            ", "embedding_dim", "=", "opt", ".", "src_word_vec_size", "\n", "", "else", ":", "\n", "            ", "embedding_dim", "=", "opt", ".", "inference_network_src_word_vec_size", "\n", "", "", "else", ":", "\n", "        ", "if", "not", "for_inference_network", ":", "\n", "            ", "embedding_dim", "=", "opt", ".", "tgt_word_vec_size", "\n", "", "else", ":", "\n", "            ", "embedding_dim", "=", "opt", ".", "inference_network_tgt_word_vec_size", "\n", "\n", "", "", "word_padding_idx", "=", "word_dict", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "\n", "num_word_embeddings", "=", "len", "(", "word_dict", ")", "\n", "\n", "feats_padding_idx", "=", "[", "feat_dict", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "\n", "for", "feat_dict", "in", "feature_dicts", "]", "\n", "num_feat_embeddings", "=", "[", "len", "(", "feat_dict", ")", "for", "feat_dict", "in", "\n", "feature_dicts", "]", "\n", "if", "not", "for_inference_network", ":", "\n", "        ", "dropout", "=", "opt", ".", "dropout", "\n", "", "else", ":", "\n", "        ", "dropout", "=", "opt", ".", "inference_network_dropout", "\n", "\n", "", "return", "Embeddings", "(", "word_vec_size", "=", "embedding_dim", ",", "\n", "position_encoding", "=", "opt", ".", "position_encoding", ",", "\n", "feat_merge", "=", "opt", ".", "feat_merge", ",", "\n", "feat_vec_exponent", "=", "opt", ".", "feat_vec_exponent", ",", "\n", "feat_vec_size", "=", "opt", ".", "feat_vec_size", ",", "\n", "dropout", "=", "dropout", ",", "\n", "word_padding_idx", "=", "word_padding_idx", ",", "\n", "feat_padding_idx", "=", "feats_padding_idx", ",", "\n", "word_vocab_size", "=", "num_word_embeddings", ",", "\n", "feat_vocab_sizes", "=", "num_feat_embeddings", ",", "\n", "sparse", "=", "opt", ".", "optim", "==", "\"sparseadam\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder": [[78, 99], ["onmt.modules.TransformerEncoder", "onmt.modules.CNNEncoder", "onmt.Models.MeanEncoder", "onmt.Models.RNNEncoder"], "function", ["None"], ["", "def", "make_encoder", "(", "opt", ",", "embeddings", ")", ":", "\n", "    ", "\"\"\"\n    Various encoder dispatcher function.\n    Args:\n        opt: the option in current environment.\n        embeddings (Embeddings): vocab embeddings for this encoder.\n    \"\"\"", "\n", "if", "opt", ".", "encoder_type", "==", "\"transformer\"", ":", "\n", "        ", "return", "TransformerEncoder", "(", "opt", ".", "enc_layers", ",", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "dropout", ",", "embeddings", ")", "\n", "", "elif", "opt", ".", "encoder_type", "==", "\"cnn\"", ":", "\n", "        ", "return", "CNNEncoder", "(", "opt", ".", "enc_layers", ",", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "cnn_kernel_width", ",", "\n", "opt", ".", "dropout", ",", "embeddings", ")", "\n", "", "elif", "opt", ".", "encoder_type", "==", "\"mean\"", ":", "\n", "        ", "return", "MeanEncoder", "(", "opt", ".", "enc_layers", ",", "embeddings", ")", "\n", "", "else", ":", "\n", "# \"rnn\" or \"brnn\"", "\n", "        ", "return", "RNNEncoder", "(", "opt", ".", "rnn_type", ",", "opt", ".", "brnn", ",", "opt", ".", "enc_layers", ",", "\n", "opt", ".", "memory_size", ",", "opt", ".", "decoder_rnn_size", ",", "opt", ".", "dropout", ",", "embeddings", ",", "\n", "opt", ".", "bridge", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_inference_network": [[101, 144], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "onmt.ViModels.InferenceNetwork", "print", "ModelConstructor.make_embeddings", "ModelConstructor.make_embeddings", "print"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings"], ["", "", "def", "make_inference_network", "(", "opt", ",", "src_embeddings", ",", "tgt_embeddings", ",", "\n", "src_dict", ",", "src_feature_dicts", ",", "\n", "tgt_dict", ",", "tgt_feature_dicts", ")", ":", "\n", "    ", "print", "(", "'Making inference network:'", ")", "\n", "if", "not", "opt", ".", "inference_network_share_embeddings", ":", "\n", "        ", "print", "(", "'    * share embeddings: False'", ")", "\n", "src_embeddings", "=", "make_embeddings", "(", "opt", ",", "src_dict", ",", "\n", "src_feature_dicts", ",", "\n", "for_inference_network", "=", "True", ")", "\n", "tgt_embeddings", "=", "make_embeddings", "(", "opt", ",", "tgt_dict", ",", "\n", "tgt_feature_dicts", ",", "for_encoder", "=", "False", ",", "\n", "for_inference_network", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'    * share embeddings: True'", ")", "\n", "\n", "", "inference_network_type", "=", "opt", ".", "inference_network_type", "\n", "inference_network_src_layers", "=", "opt", ".", "inference_network_src_layers", "\n", "inference_network_tgt_layers", "=", "opt", ".", "inference_network_tgt_layers", "\n", "rnn_type", "=", "opt", ".", "rnn_type", "\n", "rnn_size", "=", "opt", ".", "inference_network_rnn_size", "\n", "dropout", "=", "opt", ".", "inference_network_dropout", "\n", "scoresFstring", "=", "opt", ".", "alpha_transformation", "\n", "scoresF", "=", "scoresF_dict", "[", "scoresFstring", "]", "\n", "attn_type", "=", "opt", ".", "q_attn_type", "\n", "\n", "print", "(", "'    * inference network type: %s'", "%", "inference_network_type", ")", "\n", "print", "(", "'    * inference network RNN type: %s'", "%", "rnn_type", ")", "\n", "print", "(", "'    * inference network RNN size: %s'", "%", "rnn_size", ")", "\n", "print", "(", "'    * inference network dropout: %s'", "%", "dropout", ")", "\n", "print", "(", "'    * inference network src layers: %s'", "%", "inference_network_src_layers", ")", "\n", "print", "(", "'    * inference network tgt layers: %s'", "%", "inference_network_tgt_layers", ")", "\n", "print", "(", "'    * inference network alpha trans: %s'", "%", "scoresFstring", ")", "\n", "print", "(", "'    * inference network attn type: %s'", "%", "attn_type", ")", "\n", "print", "(", "'    * inference network dist type: %s'", "%", "opt", ".", "q_dist_type", ")", "\n", "print", "(", "'    * TODO: RNN\\'s could be possibly shared'", ")", "\n", "\n", "return", "InferenceNetwork", "(", "inference_network_type", ",", "\n", "src_embeddings", ",", "tgt_embeddings", ",", "\n", "rnn_type", ",", "inference_network_src_layers", ",", "\n", "inference_network_tgt_layers", ",", "rnn_size", ",", "dropout", ",", "\n", "attn_type", "=", "opt", ".", "q_attn_type", ",", "\n", "dist_type", "=", "opt", ".", "q_dist_type", ",", "\n", "scoresF", "=", "scoresF", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder": [[146, 212], ["onmt.modules.TransformerDecoder", "onmt.modules.CNNDecoder", "print", "onmt.Models.InputFeedRNNDecoder", "print", "onmt.ViModels.ViRNNDecoder", "onmt.Models.StdRNNDecoder"], "function", ["None"], ["", "def", "make_decoder", "(", "opt", ",", "embeddings", ")", ":", "\n", "    ", "\"\"\"\n    Various decoder dispatcher function.\n    Args:\n        opt: the option in current environment.\n        embeddings (Embeddings): vocab embeddings for this decoder.\n    \"\"\"", "\n", "if", "opt", ".", "decoder_type", "==", "\"transformer\"", ":", "\n", "        ", "return", "TransformerDecoder", "(", "opt", ".", "dec_layers", ",", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "global_attention", ",", "opt", ".", "copy_attn", ",", "\n", "opt", ".", "dropout", ",", "embeddings", ")", "\n", "", "elif", "opt", ".", "decoder_type", "==", "\"cnn\"", ":", "\n", "        ", "return", "CNNDecoder", "(", "opt", ".", "dec_layers", ",", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "global_attention", ",", "opt", ".", "copy_attn", ",", "\n", "opt", ".", "cnn_kernel_width", ",", "opt", ".", "dropout", ",", "\n", "embeddings", ")", "\n", "", "elif", "opt", ".", "input_feed", "and", "opt", ".", "inference_network_type", "==", "\"none\"", ":", "\n", "        ", "print", "(", "\"input feed\"", ")", "\n", "return", "InputFeedRNNDecoder", "(", "opt", ".", "rnn_type", ",", "opt", ".", "brnn", ",", "\n", "opt", ".", "dec_layers", ",", "\n", "opt", ".", "memory_size", ",", "\n", "opt", ".", "decoder_rnn_size", ",", "\n", "opt", ".", "attention_size", ",", "\n", "opt", ".", "global_attention", ",", "\n", "opt", ".", "coverage_attn", ",", "\n", "opt", ".", "context_gate", ",", "\n", "opt", ".", "copy_attn", ",", "\n", "opt", ".", "dropout", ",", "\n", "embeddings", ",", "\n", "opt", ".", "reuse_copy_attn", ")", "\n", "", "elif", "opt", ".", "input_feed", "and", "opt", ".", "inference_network_type", "!=", "\"none\"", ":", "\n", "        ", "print", "(", "\"VARIATIONAL DECODER\"", ")", "\n", "scoresFstring", "=", "opt", ".", "alpha_transformation", "\n", "scoresF", "=", "scoresF_dict", "[", "scoresFstring", "]", "\n", "\n", "return", "ViRNNDecoder", "(", "\n", "opt", ".", "rnn_type", ",", "opt", ".", "brnn", ",", "\n", "opt", ".", "dec_layers", ",", "\n", "memory_size", "=", "opt", ".", "memory_size", ",", "\n", "hidden_size", "=", "opt", ".", "decoder_rnn_size", ",", "\n", "attn_size", "=", "opt", ".", "attention_size", ",", "\n", "attn_type", "=", "opt", ".", "global_attention", ",", "\n", "coverage_attn", "=", "opt", ".", "coverage_attn", ",", "\n", "context_gate", "=", "opt", ".", "context_gate", ",", "\n", "copy_attn", "=", "opt", ".", "copy_attn", ",", "\n", "dropout", "=", "opt", ".", "dropout", ",", "\n", "embeddings", "=", "embeddings", ",", "\n", "reuse_copy_attn", "=", "opt", ".", "reuse_copy_attn", ",", "\n", "p_dist_type", "=", "opt", ".", "p_dist_type", ",", "\n", "q_dist_type", "=", "opt", ".", "q_dist_type", ",", "\n", "use_prior", "=", "opt", ".", "use_generative_model", ">", "0", ",", "\n", "scoresF", "=", "scoresF", ",", "\n", "n_samples", "=", "opt", ".", "n_samples", ",", "\n", "mode", "=", "opt", ".", "mode", ",", "\n", "temperature", "=", "opt", ".", "temperature", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "StdRNNDecoder", "(", "opt", ".", "rnn_type", ",", "opt", ".", "brnn", ",", "\n", "opt", ".", "dec_layers", ",", "opt", ".", "rnn_size", ",", "\n", "opt", ".", "global_attention", ",", "\n", "opt", ".", "coverage_attn", ",", "\n", "opt", ".", "context_gate", ",", "\n", "opt", ".", "copy_attn", ",", "\n", "opt", ".", "dropout", ",", "\n", "embeddings", ",", "\n", "opt", ".", "reuse_copy_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.load_test_model": [[214, 230], ["torch.load", "torch.load", "torch.load", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "ModelConstructor.make_base_model", "make_base_model.eval", "make_base_model.generator.eval", "onmt.Utils.use_gpu"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu"], ["", "", "def", "load_test_model", "(", "opt", ",", "dummy_opt", ")", ":", "\n", "    ", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "model", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "fields", "=", "onmt", ".", "io", ".", "load_fields_from_vocab", "(", "\n", "checkpoint", "[", "'vocab'", "]", ",", "data_type", "=", "opt", ".", "data_type", ")", "\n", "\n", "model_opt", "=", "checkpoint", "[", "'opt'", "]", "\n", "for", "arg", "in", "dummy_opt", ":", "\n", "        ", "if", "arg", "not", "in", "model_opt", ":", "\n", "            ", "model_opt", ".", "__dict__", "[", "arg", "]", "=", "dummy_opt", "[", "arg", "]", "\n", "\n", "", "", "model", "=", "make_base_model", "(", "model_opt", ",", "fields", ",", "\n", "use_gpu", "(", "opt", ")", ",", "checkpoint", ")", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "generator", ".", "eval", "(", ")", "\n", "return", "fields", ",", "model", ",", "model_opt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model": [[232, 361], ["onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "ModelConstructor.make_embeddings", "ModelConstructor.make_decoder", "onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "onmt.io.collect_feature_vocabs", "ModelConstructor.make_embeddings", "ModelConstructor.make_encoder", "ModelConstructor.make_inference_network", "onmt.Models.NMTModel", "onmt.ViModels.ViNMTModel", "onmt.Models.Generator", "onmt.modules.CopyGenerator", "print", "model.load_state_dict", "onmt.modules.CopyGenerator.load_state_dict", "hasattr", "hasattr", "model.cuda", "model.cpu", "onmt.modules.ImageEncoder", "AssertionError", "print", "model.parameters", "onmt.modules.CopyGenerator.parameters", "model.parameters", "onmt.modules.CopyGenerator.parameters", "model.encoder.embeddings.load_pretrained_vectors", "model.decoder.embeddings.load_pretrained_vectors", "onmt.modules.AudioEncoder", "len", "p.data.uniform_", "p.data.uniform_", "p.dim", "torch.nn.init.xavier_uniform", "p.dim", "torch.nn.init.xavier_uniform"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_decoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_encoder", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_inference_network", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.AudioEncoder.AudioEncoder.load_pretrained_vectors", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.AudioEncoder.AudioEncoder.load_pretrained_vectors"], ["", "def", "make_base_model", "(", "model_opt", ",", "fields", ",", "gpu", ",", "checkpoint", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        model_opt: the option loaded from checkpoint.\n        fields: `Field` objects for the model.\n        gpu(bool): whether to use gpu.\n        checkpoint: the model gnerated by train phase, or a resumed snapshot\n                    model from a stopped training.\n    Returns:\n        the NMTModel.\n    \"\"\"", "\n", "assert", "model_opt", ".", "model_type", "in", "[", "\"text\"", ",", "\"img\"", ",", "\"audio\"", "]", ",", "(", "\"Unsupported model type %s\"", "%", "(", "model_opt", ".", "model_type", ")", ")", "\n", "\n", "# Make encoder.", "\n", "if", "model_opt", ".", "model_type", "==", "\"text\"", ":", "\n", "        ", "src_dict", "=", "fields", "[", "\"src\"", "]", ".", "vocab", "\n", "src_feature_dicts", "=", "onmt", ".", "io", ".", "collect_feature_vocabs", "(", "fields", ",", "'src'", ")", "\n", "src_embeddings", "=", "make_embeddings", "(", "model_opt", ",", "src_dict", ",", "\n", "src_feature_dicts", ")", "\n", "encoder", "=", "make_encoder", "(", "model_opt", ",", "src_embeddings", ")", "\n", "", "elif", "model_opt", ".", "model_type", "==", "\"img\"", ":", "\n", "        ", "encoder", "=", "ImageEncoder", "(", "model_opt", ".", "enc_layers", ",", "\n", "model_opt", ".", "brnn", ",", "\n", "model_opt", ".", "rnn_size", ",", "\n", "model_opt", ".", "dropout", ")", "\n", "", "elif", "model_opt", ".", "model_type", "==", "\"audio\"", ":", "\n", "        ", "encoder", "=", "AudioEncoder", "(", "model_opt", ".", "enc_layers", ",", "\n", "model_opt", ".", "brnn", ",", "\n", "model_opt", ".", "rnn_size", ",", "\n", "model_opt", ".", "dropout", ",", "\n", "model_opt", ".", "sample_rate", ",", "\n", "model_opt", ".", "window_size", ")", "\n", "\n", "# Make decoder.", "\n", "", "tgt_dict", "=", "fields", "[", "\"tgt\"", "]", ".", "vocab", "\n", "tgt_feature_dicts", "=", "onmt", ".", "io", ".", "collect_feature_vocabs", "(", "fields", ",", "'tgt'", ")", "\n", "tgt_embeddings", "=", "make_embeddings", "(", "model_opt", ",", "tgt_dict", ",", "\n", "tgt_feature_dicts", ",", "for_encoder", "=", "False", ")", "\n", "\n", "# Share the embedding matrix - preprocess with share_vocab required.", "\n", "if", "model_opt", ".", "share_embeddings", ":", "\n", "# src/tgt vocab should be the same if `-share_vocab` is specified.", "\n", "        ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "            ", "raise", "AssertionError", "(", "'The `-share_vocab` should be set during '", "\n", "'preprocess if you use share_embeddings!'", ")", "\n", "\n", "", "tgt_embeddings", ".", "word_lut", ".", "weight", "=", "src_embeddings", ".", "word_lut", ".", "weight", "\n", "\n", "", "decoder", "=", "make_decoder", "(", "model_opt", ",", "tgt_embeddings", ")", "\n", "\n", "# Make inference network.", "\n", "inference_network", "=", "make_inference_network", "(", "\n", "model_opt", ",", "\n", "src_embeddings", ",", "tgt_embeddings", ",", "\n", "src_dict", ",", "src_feature_dicts", ",", "\n", "tgt_dict", ",", "tgt_feature_dicts", "\n", ")", "if", "model_opt", ".", "inference_network_type", "!=", "\"none\"", "else", "None", "\n", "\n", "# Make NMTModel(= encoder + decoder + inference network).", "\n", "model", "=", "(", "\n", "NMTModel", "(", "encoder", ",", "decoder", ")", "\n", "if", "inference_network", "is", "None", "\n", "else", "ViNMTModel", "(", "\n", "encoder", ",", "decoder", ",", "\n", "inference_network", ",", "\n", "n_samples", "=", "model_opt", ".", "n_samples", ",", "\n", "dist_type", "=", "model_opt", ".", "p_dist_type", ",", "\n", "dbg", "=", "model_opt", ".", "dbg_inf", ",", "\n", "use_prior", "=", "model_opt", ".", "use_generative_model", ">", "0", ")", "\n", ")", "\n", "model", ".", "model_type", "=", "model_opt", ".", "model_type", "\n", "\n", "# Make Generator.", "\n", "if", "not", "model_opt", ".", "copy_attn", ":", "\n", "        ", "\"\"\"\n        generator = nn.Sequential(\n            nn.Linear(model_opt.rnn_size, len(fields[\"tgt\"].vocab)),\n            nn.LogSoftmax(dim=1))\n        \"\"\"", "\n", "generator", "=", "Generator", "(", "\n", "in_dim", "=", "model_opt", ".", "decoder_rnn_size", ",", "\n", "out_dim", "=", "len", "(", "fields", "[", "\"tgt\"", "]", ".", "vocab", ")", ",", "\n", "mode", "=", "model_opt", ".", "mode", ",", "\n", ")", "\n", "if", "model_opt", ".", "share_decoder_embeddings", ":", "\n", "            ", "generator", "[", "0", "]", ".", "weight", "=", "decoder", ".", "embeddings", ".", "word_lut", ".", "weight", "\n", "", "", "else", ":", "\n", "        ", "generator", "=", "CopyGenerator", "(", "model_opt", ".", "rnn_size", ",", "\n", "fields", "[", "\"tgt\"", "]", ".", "vocab", ")", "\n", "\n", "# Load the model states from checkpoint or initialize them.", "\n", "", "if", "checkpoint", "is", "not", "None", ":", "\n", "        ", "print", "(", "'Loading model parameters.'", ")", "\n", "#model.load_state_dict(checkpoint['model'])", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ",", "strict", "=", "False", ")", "\n", "generator", ".", "load_state_dict", "(", "checkpoint", "[", "'generator'", "]", ")", "\n", "", "else", ":", "\n", "        ", "if", "model_opt", ".", "param_init", "!=", "0.0", ":", "\n", "            ", "print", "(", "'Intializing model parameters.'", ")", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "data", ".", "uniform_", "(", "-", "model_opt", ".", "param_init", ",", "model_opt", ".", "param_init", ")", "\n", "", "for", "p", "in", "generator", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "data", ".", "uniform_", "(", "-", "model_opt", ".", "param_init", ",", "model_opt", ".", "param_init", ")", "\n", "", "", "if", "model_opt", ".", "param_init_glorot", ":", "\n", "            ", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "xavier_uniform", "(", "p", ")", "\n", "", "", "for", "p", "in", "generator", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "xavier_uniform", "(", "p", ")", "\n", "\n", "", "", "", "if", "hasattr", "(", "model", ".", "encoder", ",", "'embeddings'", ")", ":", "\n", "            ", "model", ".", "encoder", ".", "embeddings", ".", "load_pretrained_vectors", "(", "\n", "model_opt", ".", "pre_word_vecs_enc", ",", "model_opt", ".", "fix_word_vecs_enc", ")", "\n", "", "if", "hasattr", "(", "model", ".", "decoder", ",", "'embeddings'", ")", ":", "\n", "            ", "model", ".", "decoder", ".", "embeddings", ".", "load_pretrained_vectors", "(", "\n", "model_opt", ".", "pre_word_vecs_dec", ",", "model_opt", ".", "fix_word_vecs_dec", ")", "\n", "\n", "# Add generator to model (this registers it as parameter of model).", "\n", "", "", "model", ".", "generator", "=", "generator", "\n", "\n", "# Make the whole model leverage GPU if indicated to do so.", "\n", "if", "gpu", ">=", "0", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "cpu", "(", ")", "\n", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter._format_usage": [[618, 620], ["None"], "methods", ["None"], ["def", "_format_usage", "(", "self", ",", "usage", ",", "actions", ",", "groups", ",", "prefix", ")", ":", "\n", "        ", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter.format_help": [[621, 625], ["print", "super().format_help"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter.format_help"], ["", "def", "format_help", "(", "self", ")", ":", "\n", "        ", "print", "(", "self", ".", "_prog", ")", "\n", "self", ".", "_root_section", ".", "heading", "=", "'# Options: %s'", "%", "self", ".", "_prog", "\n", "return", "super", "(", "MarkdownHelpFormatter", ",", "self", ")", ".", "format_help", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter.start_section": [[626, 629], ["super().start_section"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter.start_section"], ["", "def", "start_section", "(", "self", ",", "heading", ")", ":", "\n", "        ", "super", "(", "MarkdownHelpFormatter", ",", "self", ")", ".", "start_section", "(", "'### **%s**'", "%", "heading", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpFormatter._format_action": [[630, 642], ["lines.append", "lines.extend", "opts.MarkdownHelpFormatter._expand_help", "lines.extend", "opts.MarkdownHelpFormatter._split_lines"], "methods", ["None"], ["", "def", "_format_action", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "action", ".", "dest", "==", "\"help\"", "or", "action", ".", "dest", "==", "\"md\"", ":", "\n", "            ", "return", "\"\"", "\n", "", "lines", "=", "[", "]", "\n", "lines", ".", "append", "(", "'* **-%s %s** '", "%", "(", "action", ".", "dest", ",", "\n", "\"[%s]\"", "%", "action", ".", "default", "\n", "if", "action", ".", "default", "else", "\"[]\"", ")", ")", "\n", "if", "action", ".", "help", ":", "\n", "            ", "help_text", "=", "self", ".", "_expand_help", "(", "action", ")", "\n", "lines", ".", "extend", "(", "self", ".", "_split_lines", "(", "help_text", ",", "80", ")", ")", "\n", "", "lines", ".", "extend", "(", "[", "''", ",", "''", "]", ")", "\n", "return", "'\\n'", ".", "join", "(", "lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpAction.__init__": [[645, 654], ["argparse.Action.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "option_strings", ",", "\n", "dest", "=", "argparse", ".", "SUPPRESS", ",", "default", "=", "argparse", ".", "SUPPRESS", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "MarkdownHelpAction", ",", "self", ")", ".", "__init__", "(", "\n", "option_strings", "=", "option_strings", ",", "\n", "dest", "=", "dest", ",", "\n", "default", "=", "default", ",", "\n", "nargs", "=", "0", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.MarkdownHelpAction.__call__": [[655, 659], ["parser.print_help", "parser.exit"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "parser", ",", "namespace", ",", "values", ",", "option_string", "=", "None", ")", ":", "\n", "        ", "parser", ".", "formatter_class", "=", "MarkdownHelpFormatter", "\n", "parser", ".", "print_help", "(", ")", "\n", "parser", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.DeprecateAction.__init__": [[662, 665], ["argparse.Action.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "option_strings", ",", "dest", ",", "help", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "DeprecateAction", ",", "self", ")", ".", "__init__", "(", "option_strings", ",", "dest", ",", "nargs", "=", "0", ",", "\n", "help", "=", "help", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.DeprecateAction.__call__": [[666, 670], ["argparse.ArgumentTypeError"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "parser", ",", "namespace", ",", "values", ",", "flag_name", ")", ":", "\n", "        ", "help", "=", "self", ".", "help", "if", "self", ".", "help", "is", "not", "None", "else", "\"\"", "\n", "msg", "=", "\"Flag '%s' is deprecated. %s\"", "%", "(", "flag_name", ",", "help", ")", "\n", "raise", "argparse", ".", "ArgumentTypeError", "(", "msg", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts": [[5, 208], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["def", "model_opts", "(", "parser", ")", ":", "\n", "    ", "\"\"\"\n    These options are passed to the construction of the model.\n    Be careful with these as they will be used during translation.\n    \"\"\"", "\n", "\n", "# Embedding Options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Model-Embeddings'", ")", "\n", "group", ".", "add_argument", "(", "'-src_word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Word embedding size for src.'", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Word embedding size for tgt.'", ")", "\n", "group", ".", "add_argument", "(", "'-word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Word embedding size for src and tgt.'", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-share_decoder_embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Use a shared weight matrix for the input and\n                       output word  embeddings in the decoder.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-share_embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Share the word embeddings between encoder\n                       and decoder. Need to use shared dictionary for this\n                       option.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-position_encoding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Use a sin to mark relative words positions.\n                       Necessary for non-RNN style models.\n                       \"\"\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Model-Embedding Features'", ")", "\n", "group", ".", "add_argument", "(", "'-feat_merge'", ",", "type", "=", "str", ",", "default", "=", "'concat'", ",", "\n", "choices", "=", "[", "'concat'", ",", "'sum'", ",", "'mlp'", "]", ",", "\n", "help", "=", "\"\"\"Merge action for incorporating features embeddings.\n                       Options [concat|sum|mlp].\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-feat_vec_size'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"\"\"If specified, feature embedding sizes\n                       will be set to this. Otherwise, feat_vec_exponent\n                       will be used.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-feat_vec_exponent'", ",", "type", "=", "float", ",", "default", "=", "0.7", ",", "\n", "help", "=", "\"\"\"If -feat_merge_size is not set, feature\n                       embedding sizes will be set to N^feat_vec_exponent\n                       where N is the number of values the feature takes.\"\"\"", ")", "\n", "\n", "# Encoder-Deocder Options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Model- Encoder-Decoder'", ")", "\n", "group", ".", "add_argument", "(", "'-model_type'", ",", "default", "=", "'text'", ",", "\n", "help", "=", "\"\"\"Type of source model to use. Allows\n                       the system to incorporate non-text inputs.\n                       Options are [text|img|audio].\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-encoder_type'", ",", "type", "=", "str", ",", "default", "=", "'rnn'", ",", "\n", "choices", "=", "[", "'rnn'", ",", "'brnn'", ",", "'mean'", ",", "'transformer'", ",", "'cnn'", "]", ",", "\n", "help", "=", "\"\"\"Type of encoder layer to use. Non-RNN layers\n                       are experimental. Options are\n                       [rnn|brnn|mean|transformer|cnn].\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-decoder_type'", ",", "type", "=", "str", ",", "default", "=", "'rnn'", ",", "\n", "choices", "=", "[", "'rnn'", ",", "'transformer'", ",", "'cnn'", "]", ",", "\n", "help", "=", "\"\"\"Type of decoder layer to use. Non-RNN layers\n                       are experimental. Options are\n                       [rnn|transformer|cnn].\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-layers'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Number of layers in enc/dec.'", ")", "\n", "group", ".", "add_argument", "(", "'-enc_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'Number of layers in the encoder'", ")", "\n", "group", ".", "add_argument", "(", "'-dec_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'Number of layers in the decoder'", ")", "\n", "group", ".", "add_argument", "(", "'-memory_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Size of rnn hidden states'", ")", "\n", "group", ".", "add_argument", "(", "'-decoder_rnn_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Size of rnn hidden states'", ")", "\n", "group", ".", "add_argument", "(", "'-attention_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Size of decoder attention'", ")", "\n", "group", ".", "add_argument", "(", "'-cnn_kernel_width'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "\n", "help", "=", "\"\"\"Size of windows in the cnn, the kernel_size is\n                       (cnn_kernel_width, 1) in conv layer\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-input_feed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"Feed the context vector at each time step as\n                       additional input (via concatenation with the word\n                       embeddings) to the decoder.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-bridge'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"\"\"Have an additional layer between the last encoder\n                       state and the first decoder state\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-rnn_type'", ",", "type", "=", "str", ",", "default", "=", "'LSTM'", ",", "\n", "choices", "=", "[", "'LSTM'", ",", "'GRU'", ",", "'SRU'", "]", ",", "\n", "action", "=", "CheckSRU", ",", "\n", "help", "=", "\"\"\"The gate type to use in the RNNs\"\"\"", ")", "\n", "# group.add_argument('-residual',   action=\"store_true\",", "\n", "#                     help=\"Add residual connections between RNN layers.\")", "\n", "\n", "group", ".", "add_argument", "(", "'-brnn'", ",", "action", "=", "DeprecateAction", ",", "\n", "help", "=", "\"Deprecated, use `encoder_type`.\"", ")", "\n", "group", ".", "add_argument", "(", "'-brnn_merge'", ",", "default", "=", "'concat'", ",", "\n", "choices", "=", "[", "'concat'", ",", "'sum'", "]", ",", "\n", "help", "=", "\"Merge action for the bidir hidden states\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-context_gate'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "choices", "=", "[", "'source'", ",", "'target'", ",", "'both'", "]", ",", "\n", "help", "=", "\"\"\"Type of context gate to use.\n                       Do not select for no context gate.\"\"\"", ")", "\n", "\n", "# Inference Network options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Inference Network'", ")", "\n", "group", ".", "add_argument", "(", "\"-inference_network_normalization\"", ",", "default", "=", "\"none\"", ",", "\n", "choices", "=", "[", "\"none\"", ",", "\"bn\"", "]", ",", "\n", "help", "=", "\"\"\"Normalization type in the inference network.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-freeze_generative_model\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Freeze the generative model, except the attn prior.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-use_generative_model\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Use the generative model, namely the attn prior,\n                       instead of the inference network for the attention.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-p_input_feed\"", ",", "type", "=", "str", ",", "default", "=", "\"mean\"", ",", "\n", "choices", "=", "[", "\"q\"", ",", "\"p\"", ",", "\"p_mean\"", "]", ",", "\n", "help", "=", "\"\"\"Always perform input feeding with the prior P\n                       rather than approximate posterior Q even during training.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-mode\"", ",", "type", "=", "str", ",", "default", "=", "\"sample\"", ",", "\n", "choices", "=", "[", "\"sample\"", ",", "\"enum\"", ",", "\"exact\"", ",", "\"wsram\"", ",", "\"gumbel\"", "]", ",", "\n", "help", "=", "\"\"\"Sample or enumerate ELBO, or calculate reconstruction exactly.\n                       wsram: Ba et al \"Learning Wake-Sleep Recurrent Attention Models\".\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-n_samples\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"Number of samples to estimate log marginal.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-k_max\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"K max for attention evaluation.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-train_baseline\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Train the basline if > 0.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-alpha_transformation\"", ",", "type", "=", "str", ",", "default", "=", "\"exp\"", ",", "\n", "choices", "=", "[", "\"softplus\"", ",", "\"exp\"", ",", "\"relu\"", ",", "\"sm\"", "]", ",", "\n", "help", "=", "\"\"\"Transformation used to parameterize Dirichlet.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-min_clamp_val\"", ",", "type", "=", "float", ",", "default", "=", "1e-2", ",", "\n", "help", "=", "\"\"\"Use the generative model, namely the attn prior,\n                       instead of the inference network for the attention.\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-p_dist_type\"", ",", "type", "=", "str", ",", "default", "=", "\"categorical\"", ",", "\n", "choices", "=", "[", "\"categorical\"", ",", "\"none\"", "]", ",", "\n", "help", "=", "\"\"\"p_a distribution type.\n                        If 'none', then uses a softmax over scores.\n                        \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-q_attn_type\"", ",", "type", "=", "str", ",", "default", "=", "\"general\"", ",", "\n", "choices", "=", "[", "\"dot\"", ",", "\"general\"", ",", "\"mlp\"", "]", ",", "\n", "help", "=", "\"\"\"q and p_a distribution type.\n                        If 'none', then uses a softmax over scores.\n                        \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-q_dist_type\"", ",", "type", "=", "str", ",", "default", "=", "\"categorical\"", ",", "\n", "choices", "=", "[", "\"categorical\"", ",", "\"none\"", "]", ",", "\n", "help", "=", "\"\"\"q distribution type.\n                        If 'none', then uses a softmax over scores.\n                        \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-dbg_inf\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Feed dbg flag to inference network.\n                    \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-temperature'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"\"\"Gumbel Softmax temperature.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_type'", ",", "type", "=", "str", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'rnn'", ",", "'bigbrnn'", ",", "'brnn'", ",", "'embedding_only'", ",", "'none'", "]", ",", "\n", "help", "=", "\"\"\"Type of inference network to use.\n                       Options are\n                       [rnn|bigbrnn|brnn|embedding_only].\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_share_embeddings'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"Use src/tgt word embeddings for inference network.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_src_word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "\"\"\"Inference network src word vec size.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_tgt_word_vec_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "\"\"\"Inference network tgt word vec size.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_dropout'", ",", "type", "=", "float", ",", "default", "=", "0.3", ",", "\n", "help", "=", "\"\"\"Inference network dropout.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_src_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'Number of layers in the inference network src RNN'", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_tgt_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'Number of layers in the inference network tgt RNN'", ")", "\n", "group", ".", "add_argument", "(", "'-inference_network_rnn_size'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'Size of rnn hidden states in the inference network RNN'", ")", "\n", "\n", "# Attention options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Model- Attention'", ")", "\n", "group", ".", "add_argument", "(", "'-global_attention'", ",", "type", "=", "str", ",", "default", "=", "'general'", ",", "\n", "choices", "=", "[", "'dot'", ",", "'general'", ",", "'mlp'", "]", ",", "\n", "help", "=", "\"\"\"The attention type to use:\n                       dotprod or general (Luong) or MLP (Bahdanau)\"\"\"", ")", "\n", "\n", "# Genenerator and loss options.", "\n", "group", ".", "add_argument", "(", "'-copy_attn'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Train copy attention layer.'", ")", "\n", "group", ".", "add_argument", "(", "'-copy_attn_force'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'When available, train to copy.'", ")", "\n", "group", ".", "add_argument", "(", "'-reuse_copy_attn'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Reuse standard attention for copy\"", ")", "\n", "group", ".", "add_argument", "(", "'-copy_loss_by_seqlength'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Divide copy loss by length of sequence\"", ")", "\n", "group", ".", "add_argument", "(", "'-coverage_attn'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Train a coverage attention layer.'", ")", "\n", "group", ".", "add_argument", "(", "'-lambda_coverage'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "'Lambda value for coverage.'", ")", "\n", "group", ".", "add_argument", "(", "'-metric'", ",", "type", "=", "str", ",", "default", "=", "'ppl'", ",", "\n", "choices", "=", "[", "'ppl'", ",", "'xent_p'", ",", "'pppl'", ",", "'xent'", "]", ",", "\n", "help", "=", "\"\"\"The metric to use to determine lr annealing:\n                       ppl, xent_p, pppl, or xent\"\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.preprocess_opts": [[210, 295], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "preprocess_opts", "(", "parser", ")", ":", "\n", "# Data options", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Data'", ")", "\n", "group", ".", "add_argument", "(", "'-data_type'", ",", "default", "=", "\"text\"", ",", "\n", "help", "=", "\"\"\"Type of the source input.\n                       Options are [text|img].\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-train_src'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to the training source data\"", ")", "\n", "group", ".", "add_argument", "(", "'-train_tgt'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to the training target data\"", ")", "\n", "group", ".", "add_argument", "(", "'-valid_src'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to the validation source data\"", ")", "\n", "group", ".", "add_argument", "(", "'-valid_tgt'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to the validation target data\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-src_dir'", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Source directory for image or audio files.\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-save_data'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Output file for the prepared data\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-max_shard_size'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"For text corpus of large volume, it will\n                       be divided into shards of this size to preprocess.\n                       If 0, the data will be handled as a whole. The unit\n                       is in bytes. Optimal value should be multiples of\n                       64 bytes.\"\"\"", ")", "\n", "\n", "# Dictionary options, for text corpus", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Vocab'", ")", "\n", "group", ".", "add_argument", "(", "'-src_vocab'", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Path to an existing source vocabulary\"", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_vocab'", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Path to an existing target vocabulary\"", ")", "\n", "group", ".", "add_argument", "(", "'-features_vocabs_prefix'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Path prefix to existing features vocabularies\"", ")", "\n", "group", ".", "add_argument", "(", "'-src_vocab_size'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "\n", "help", "=", "\"Size of the source vocabulary\"", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_vocab_size'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "\n", "help", "=", "\"Size of the target vocabulary\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-src_words_min_frequency'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_words_min_frequency'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-dynamic_dict'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Create dynamic dictionaries\"", ")", "\n", "group", ".", "add_argument", "(", "'-share_vocab'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Share source and target vocabulary\"", ")", "\n", "\n", "# Truncation options, for text corpus", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Pruning'", ")", "\n", "group", ".", "add_argument", "(", "'-src_seq_length'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Maximum source sequence length\"", ")", "\n", "group", ".", "add_argument", "(", "'-src_seq_length_trunc'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate source sequence length.\"", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_seq_length'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Maximum target sequence length to keep.\"", ")", "\n", "group", ".", "add_argument", "(", "'-tgt_seq_length_trunc'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate target sequence length.\"", ")", "\n", "group", ".", "add_argument", "(", "'-lower'", ",", "action", "=", "'store_true'", ",", "help", "=", "'lowercase data'", ")", "\n", "group", ".", "add_argument", "(", "'-leave_valid'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Leave validation data as is.'", ")", "\n", "\n", "# Data processing options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Random'", ")", "\n", "group", ".", "add_argument", "(", "'-shuffle'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Shuffle data\"", ")", "\n", "group", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "3435", ",", "\n", "help", "=", "\"Random seed\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Logging'", ")", "\n", "group", ".", "add_argument", "(", "'-report_every'", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "\"Report status every this many sentences\"", ")", "\n", "\n", "# Options most relevant to speech", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Speech'", ")", "\n", "group", ".", "add_argument", "(", "'-sample_rate'", ",", "type", "=", "int", ",", "default", "=", "16000", ",", "\n", "help", "=", "\"Sample rate.\"", ")", "\n", "group", ".", "add_argument", "(", "'-window_size'", ",", "type", "=", "float", ",", "default", "=", ".02", ",", "\n", "help", "=", "\"Window size for spectrogram in seconds.\"", ")", "\n", "group", ".", "add_argument", "(", "'-window_stride'", ",", "type", "=", "float", ",", "default", "=", ".01", ",", "\n", "help", "=", "\"Window stride for spectrogram in seconds.\"", ")", "\n", "group", ".", "add_argument", "(", "'-window'", ",", "default", "=", "'hamming'", ",", "\n", "help", "=", "\"Window type for spectrogram generation.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.train_opts": [[297, 488], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "train_opts", "(", "parser", ")", ":", "\n", "# Model loading/saving options", "\n", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'General'", ")", "\n", "group", ".", "add_argument", "(", "\"-eval_only\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"Evaluation only.\"", ")", "\n", "group", ".", "add_argument", "(", "'-data'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Path prefix to the \".train.pt\" and\n                       \".valid.pt\" file path from preprocess.py\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-save_model'", ",", "default", "=", "'model'", ",", "\n", "help", "=", "\"\"\"Model filename (the model will be saved as\n                       <save_model>_epochN_PPL.pt where PPL is the\n                       validation perplexity\"\"\"", ")", "\n", "# GPU", "\n", "group", ".", "add_argument", "(", "'-gpuid'", ",", "default", "=", "[", "]", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Use CUDA on the listed devices.\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"\"\"Random seed used for the experiments\n                       reproducibility.\"\"\"", ")", "\n", "\n", "# Init options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Initialization'", ")", "\n", "group", ".", "add_argument", "(", "'-start_epoch'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'The epoch from which to start'", ")", "\n", "group", ".", "add_argument", "(", "'-param_init'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "\"\"\"Parameters are initialized over uniform distribution\n                       with support (-param_init, param_init).\n                       Use 0 to not use initialization\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-param_init_glorot'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Init parameters with xavier_uniform.\n                       Required for transfomer.\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-train_from'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "\"\"\"If training from a checkpoint then this is the\n                       path to the pretrained model's state_dict.\"\"\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-init_with'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "\"\"\"If initializing a variational model\n                       from a checkpoint then this is the\n                       path to the pretrained model's state_dict.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-eval_with'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "\"\"\"If initializing a variational model\n                       from a checkpoint then this is the\n                       path to the pretrained model's state_dict.\"\"\"", ")", "\n", "\n", "# Pretrained word vectors", "\n", "group", ".", "add_argument", "(", "'-pre_word_vecs_enc'", ",", "\n", "help", "=", "\"\"\"If a valid path is specified, then this will load\n                       pretrained word embeddings on the encoder side.\n                       See README for specific formatting instructions.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-pre_word_vecs_dec'", ",", "\n", "help", "=", "\"\"\"If a valid path is specified, then this will load\n                       pretrained word embeddings on the decoder side.\n                       See README for specific formatting instructions.\"\"\"", ")", "\n", "# Fixed word vectors", "\n", "group", ".", "add_argument", "(", "'-fix_word_vecs_enc'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Fix word embeddings on the encoder side.\"", ")", "\n", "group", ".", "add_argument", "(", "'-fix_word_vecs_dec'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Fix word embeddings on the encoder side.\"", ")", "\n", "\n", "# Optimization options", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Optimization- Type'", ")", "\n", "group", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'Maximum batch size for training'", ")", "\n", "group", ".", "add_argument", "(", "'-batch_type'", ",", "default", "=", "'sents'", ",", "\n", "choices", "=", "[", "\"sents\"", ",", "\"tokens\"", "]", ",", "\n", "help", "=", "\"\"\"Batch grouping for batch_size. Standard\n                               is sents. Tokens will do dynamic batching\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-normalization'", ",", "default", "=", "'sents'", ",", "\n", "choices", "=", "[", "\"sents\"", ",", "\"tokens\"", "]", ",", "\n", "help", "=", "'Normalization method of the gradient.'", ")", "\n", "group", ".", "add_argument", "(", "'-accum_count'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"Accumulate gradient this many times.\n                       Approximately equivalent to updating\n                       batch_size * accum_count batches at once.\n                       Recommended for Transformer.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-valid_batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Maximum batch size for validation'", ")", "\n", "group", ".", "add_argument", "(", "'-max_generator_batches'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"\"\"Maximum batches ds stra sequence to r\"mean\"n\n                       choices=[\"q\", \"p\", \"p_mean\"],\n                        the generator on in parallel. Higher is faster, but\n                        uses more memory.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-epochs'", ",", "type", "=", "int", ",", "default", "=", "13", ",", "\n", "help", "=", "'Number of training epochs'", ")", "\n", "group", ".", "add_argument", "(", "'-optim'", ",", "default", "=", "'sgd'", ",", "\n", "choices", "=", "[", "'sgd'", ",", "'adagrad'", ",", "'adadelta'", ",", "'adam'", ",", "\n", "'sparseadam'", "]", ",", "\n", "help", "=", "\"\"\"Optimization method.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-adagrad_accumulator_init'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Initializes the accumulator values in adagrad.\n                       Mirrors the initial_accumulator_value option\n                       in the tensorflow adagrad (use 0.1 for their default).\n                       \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-max_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "5", ",", "\n", "help", "=", "\"\"\"If the norm of the gradient vector exceeds this,\n                       renormalize it to have the norm equal to\n                       max_grad_norm\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.3", ",", "\n", "help", "=", "\"Dropout probability; applied in LSTM stacks.\"", ")", "\n", "group", ".", "add_argument", "(", "'-truncated_decoder'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Truncated bptt.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-adam_beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "\"\"\"The beta1 parameter used by Adam.\n                       Almost without exception a value of 0.9 is used in\n                       the literature, seemingly giving good results,\n                       so we would discourage changing this value from\n                       the default without due consideration.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-adam_beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ",", "\n", "help", "=", "\"\"\"The beta2 parameter used by Adam.\n                       Typically a value of 0.999 is recommended, as this is\n                       the value suggested by the original paper describing\n                       Adam, and is also the value adopted in other frameworks\n                       such as Tensorflow and Kerras, i.e. see:\n                       https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n                       https://keras.io/optimizers/ .\n                       Whereas recently the paper \"Attention is All You Need\"\n                       suggested a value of 0.98 for beta2, this parameter may\n                       not work well for normal models / default\n                       baselines.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-adam_eps'", ",", "type", "=", "float", ",", "default", "=", "1e-9", ")", "\n", "group", ".", "add_argument", "(", "'-label_smoothing'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"\"\"Label smoothing value epsilon.\n                       Probabilities of all non-true labels\n                       will be smoothed by epsilon / (vocab_size - 1).\n                       Set to zero to turn off label smoothing.\n                       For more detailed information, see:\n                       https://arxiv.org/abs/1512.00567\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-q_warmup_start\"", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "\"\"\"Number of warmup steps\n                    \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-q_warmup_steps\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Number of warmup steps\n                    \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-sample_kl\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Use sample instead of analytic KL.\n                    \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-detach_p_kl\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Use KL(Q || P.detach()).\n                    \"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-ignore_kl\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Do not backprop through KL.\n                    \"\"\"", ")", "\n", "# learning rate", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Optimization- Rate'", ")", "\n", "group", ".", "add_argument", "(", "'-learning_rate'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"\"\"Starting learning rate.\n                       Recommended settings: sgd = 1, adagrad = 0.1,\n                       adadelta = 1, adam = 0.001\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-learning_rate_decay'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"\"\"If update_learning_rate, decay learning rate by\n                       this much if (i) perplexity does not decrease on the\n                       validation set or (ii) epoch has gone past\n                       start_decay_at\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-start_decay_at'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "\"\"\"Start decaying every epoch after and including this\n                       epoch\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-start_checkpoint_at'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"\"\"Start checkpointing every epoch after and including\n                       this epoch\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-decay_method'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "choices", "=", "[", "'noam'", "]", ",", "help", "=", "\"Use a custom decay rate.\"", ")", "\n", "group", ".", "add_argument", "(", "'-warmup_steps'", ",", "type", "=", "int", ",", "default", "=", "4000", ",", "\n", "help", "=", "\"\"\"Number of warmup steps for custom decay.\"\"\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Logging'", ")", "\n", "group", ".", "add_argument", "(", "'-report_every'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Print stats at this interval.\"", ")", "\n", "group", ".", "add_argument", "(", "'-exp_host'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Send logs to this crayon server.\"", ")", "\n", "group", ".", "add_argument", "(", "'-exp'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Name of the experiment for logging.\"", ")", "\n", "# Use TensorboardX for visualization during training", "\n", "group", ".", "add_argument", "(", "'-tensorboard'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"\"\"Use tensorboardX for visualization during training.\n                       Must have the library tensorboardX.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "\"-tensorboard_log_dir\"", ",", "type", "=", "str", ",", "\n", "default", "=", "\"runs/onmt\"", ",", "\n", "help", "=", "\"\"\"Log directory for Tensorboard.\n                       This is also the name of the run.\n                       \"\"\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Speech'", ")", "\n", "# Options most relevant to speech", "\n", "group", ".", "add_argument", "(", "'-sample_rate'", ",", "type", "=", "int", ",", "default", "=", "16000", ",", "\n", "help", "=", "\"Sample rate.\"", ")", "\n", "group", ".", "add_argument", "(", "'-window_size'", ",", "type", "=", "float", ",", "default", "=", ".02", ",", "\n", "help", "=", "\"Window size for spectrogram in seconds.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts": [[490, 595], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "translate_opts", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Model'", ")", "\n", "group", ".", "add_argument", "(", "'-model'", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to model .pt file'", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Data'", ")", "\n", "group", ".", "add_argument", "(", "'-data_type'", ",", "default", "=", "\"text\"", ",", "\n", "help", "=", "\"Type of the source input. Options: [text|img].\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "'-src'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"\"\"Source sequence to decode (one line per\n                       sequence)\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-src_dir'", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'Source directory for image or audio files'", ")", "\n", "group", ".", "add_argument", "(", "'-tgt'", ",", "\n", "help", "=", "'True target sequence (optional)'", ")", "\n", "group", ".", "add_argument", "(", "'-output'", ",", "default", "=", "'pred.txt'", ",", "\n", "help", "=", "\"\"\"Path to output the predictions (each line will\n                       be the decoded sequence\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-report_bleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Report bleu score after translation,\n                       call tools/multi-bleu.perl on command line\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-report_rouge'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Report rouge 1/2/3/L/SU4 score after translation\n                       call tools/test_rouge.py on command line\"\"\"", ")", "\n", "\n", "# Options most relevant to summarization.", "\n", "group", ".", "add_argument", "(", "'-dynamic_dict'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Create dynamic dictionaries\"", ")", "\n", "group", ".", "add_argument", "(", "'-share_vocab'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Share source and target vocabulary\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Beam'", ")", "\n", "group", ".", "add_argument", "(", "'-beam_size'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Beam size'", ")", "\n", "group", ".", "add_argument", "(", "'-min_length'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Minimum prediction length'", ")", "\n", "group", ".", "add_argument", "(", "'-max_length'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Maximum prediction length.'", ")", "\n", "group", ".", "add_argument", "(", "'-max_sent_length'", ",", "action", "=", "DeprecateAction", ",", "\n", "help", "=", "\"Deprecated, use `-max_length` instead\"", ")", "\n", "\n", "# Alpha and Beta values for Google Length + Coverage penalty", "\n", "# Described here: https://arxiv.org/pdf/1609.08144.pdf, Section 7", "\n", "group", ".", "add_argument", "(", "'-stepwise_penalty'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"\"\"Apply penalty at every decoding step.\n                       Helpful for summary penalty.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-length_penalty'", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'avg'", "]", ",", "\n", "help", "=", "\"\"\"Length Penalty to use.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-coverage_penalty'", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'none'", ",", "'wu'", ",", "'summary'", "]", ",", "\n", "help", "=", "\"\"\"Coverage Penalty to use.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-alpha'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "\"\"\"Google NMT length penalty parameter\n                        (higher = longer generation)\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-beta'", ",", "type", "=", "float", ",", "default", "=", "-", "0.", ",", "\n", "help", "=", "\"\"\"Coverage penalty parameter\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-eos_norm'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "\"\"\"Google NMT length penalty parameter\n                        (higher = longer generation)\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-block_ngram_repeat'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Block repetition of ngrams during decoding.'", ")", "\n", "group", ".", "add_argument", "(", "'-ignore_when_blocking'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "\n", "default", "=", "[", "]", ",", "\n", "help", "=", "\"\"\"Ignore these strings when blocking repeats.\n                       You want to block sentence delimiters.\"\"\"", ")", "\n", "group", ".", "add_argument", "(", "'-replace_unk'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"\"\"Replace the generated UNK tokens with the\n                       source token that had highest attention weight. If\n                       phrase_table is provided, it will lookup the\n                       identified source token and give the corresponding\n                       target token. If it is not provided(or the identified\n                       source token does not exist in the table) then it\n                       will copy the source token\"\"\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Logging'", ")", "\n", "group", ".", "add_argument", "(", "'-verbose'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Print scores and predictions for each sentence'", ")", "\n", "group", ".", "add_argument", "(", "'-attn_debug'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'Print best attn for each word'", ")", "\n", "group", ".", "add_argument", "(", "'-dump_beam'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'File to dump beam information to.'", ")", "\n", "group", ".", "add_argument", "(", "'-n_best'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"\"\"If verbose is set, will output the n_best\n                       decoded sentences\"\"\"", ")", "\n", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Efficiency'", ")", "\n", "group", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "30", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "group", ".", "add_argument", "(", "'-k'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'K-max'", ")", "\n", "group", ".", "add_argument", "(", "'-gpu'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Device to run on\"", ")", "\n", "\n", "# Options most relevant to speech.", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "'Speech'", ")", "\n", "group", ".", "add_argument", "(", "'-sample_rate'", ",", "type", "=", "int", ",", "default", "=", "16000", ",", "\n", "help", "=", "\"Sample rate.\"", ")", "\n", "group", ".", "add_argument", "(", "'-window_size'", ",", "type", "=", "float", ",", "default", "=", ".02", ",", "\n", "help", "=", "'Window size for spectrogram in seconds'", ")", "\n", "group", ".", "add_argument", "(", "'-window_stride'", ",", "type", "=", "float", ",", "default", "=", ".01", ",", "\n", "help", "=", "'Window stride for spectrogram in seconds'", ")", "\n", "group", ".", "add_argument", "(", "'-window'", ",", "default", "=", "'hamming'", ",", "\n", "help", "=", "'Window type for spectrogram generation'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.add_md_help_argument": [[597, 600], ["parser.add_argument"], "function", ["None"], ["", "def", "add_md_help_argument", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "'-md'", ",", "action", "=", "MarkdownHelpAction", ",", "\n", "help", "=", "'print Markdown-formatted help text and exit.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.__init__": [[35, 42], ["time.time"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "xent", "=", "0", ",", "kl", "=", "0", ",", "n_words", "=", "0", ",", "n_correct", "=", "0", ")", ":", "\n", "        ", "self", ".", "_xent", "=", "xent", "\n", "self", ".", "_kl", "=", "kl", "\n", "self", ".", "_n_words", "=", "n_words", "\n", "self", ".", "_n_correct", "=", "n_correct", "\n", "self", ".", "_n_src_words", "=", "0", "\n", "self", ".", "_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update": [[43, 48], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "stat", ")", ":", "\n", "        ", "self", ".", "_xent", "+=", "stat", ".", "_xent", "\n", "self", ".", "_kl", "+=", "stat", ".", "_kl", "\n", "self", ".", "_n_words", "+=", "stat", ".", "_n_words", "\n", "self", ".", "_n_correct", "+=", "stat", ".", "_n_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy": [[49, 51], ["None"], "methods", ["None"], ["", "def", "accuracy", "(", "self", ")", ":", "\n", "        ", "return", "100", "*", "(", "self", ".", "_n_correct", "/", "self", ".", "_n_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent": [[52, 54], ["None"], "methods", ["None"], ["", "def", "xent", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_xent", "/", "self", ".", "_n_words", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl": [[55, 57], ["None"], "methods", ["None"], ["", "def", "kl", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_kl", "/", "self", ".", "_n_words", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl": [[58, 60], ["math.exp", "min"], "methods", ["None"], ["", "def", "ppl", "(", "self", ")", ":", "\n", "        ", "return", "math", ".", "exp", "(", "min", "(", "self", ".", "_xent", "/", "self", ".", "_n_words", ",", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo": [[61, 63], ["math.exp", "min"], "methods", ["None"], ["", "def", "expelbo", "(", "self", ")", ":", "\n", "        ", "return", "math", ".", "exp", "(", "min", "(", "(", "self", ".", "_xent", "+", "self", ".", "_kl", ")", "/", "self", ".", "_n_words", ",", "100", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.elapsed_time": [[64, 66], ["time.time"], "methods", ["None"], ["", "def", "elapsed_time", "(", "self", ")", ":", "\n", "        ", "return", "time", ".", "time", "(", ")", "-", "self", ".", "_start_time", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.output": [[67, 90], ["Trainer.Statistics.elapsed_time", "print", "sys.stdout.flush", "Trainer.Statistics.accuracy", "Trainer.Statistics.expelbo", "Trainer.Statistics.ppl", "Trainer.Statistics.xent", "Trainer.Statistics.kl", "time.time"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.elapsed_time", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.kl"], ["", "def", "output", "(", "self", ",", "epoch", ",", "batch", ",", "n_batches", ",", "start", ")", ":", "\n", "        ", "\"\"\"Write out statistics to stdout.\n\n        Args:\n           epoch (int): current epoch\n           batch (int): current batch\n           n_batch (int): total batches\n           start (int): start time of epoch.\n        \"\"\"", "\n", "t", "=", "self", ".", "elapsed_time", "(", ")", "\n", "print", "(", "(", "\"Epoch %2d, %5d/%5d; acc: %6.2f; \"", "+", "\n", "\"expelbo: %6.2f; ppl: %6.2f; xent: %6.2f; kl: %6.2f; \"", "+", "\n", "\"%3.0f src tok/s; %3.0f tgt tok/s; %6.0f s elapsed\"", ")", "%", "\n", "(", "epoch", ",", "batch", ",", "n_batches", ",", "\n", "self", ".", "accuracy", "(", ")", ",", "\n", "self", ".", "expelbo", "(", ")", ",", "\n", "self", ".", "ppl", "(", ")", ",", "\n", "self", ".", "xent", "(", ")", ",", "\n", "self", ".", "kl", "(", ")", ",", "\n", "self", ".", "_n_src_words", "/", "(", "t", "+", "1e-5", ")", ",", "\n", "self", ".", "_n_words", "/", "(", "t", "+", "1e-5", ")", ",", "\n", "time", ".", "time", "(", ")", "-", "start", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.log": [[91, 97], ["Trainer.Statistics.elapsed_time", "experiment.add_scalar_value", "experiment.add_scalar_value", "experiment.add_scalar_value", "experiment.add_scalar_value", "Trainer.Statistics.ppl", "Trainer.Statistics.accuracy"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.elapsed_time", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy"], ["", "def", "log", "(", "self", ",", "prefix", ",", "experiment", ",", "lr", ")", ":", "\n", "        ", "t", "=", "self", ".", "elapsed_time", "(", ")", "\n", "experiment", ".", "add_scalar_value", "(", "prefix", "+", "\"_ppl\"", ",", "self", ".", "ppl", "(", ")", ")", "\n", "experiment", ".", "add_scalar_value", "(", "prefix", "+", "\"_accuracy\"", ",", "self", ".", "accuracy", "(", ")", ")", "\n", "experiment", ".", "add_scalar_value", "(", "prefix", "+", "\"_tgtper\"", ",", "self", ".", "n_words", "/", "t", ")", "\n", "experiment", ".", "add_scalar_value", "(", "prefix", "+", "\"_lr\"", ",", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.log_tensorboard": [[98, 105], ["Trainer.Statistics.elapsed_time", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "Trainer.Statistics.xent", "Trainer.Statistics.ppl", "Trainer.Statistics.accuracy"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.elapsed_time", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.xent", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.ppl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy"], ["", "def", "log_tensorboard", "(", "self", ",", "prefix", ",", "writer", ",", "lr", ",", "step", ")", ":", "\n", "        ", "t", "=", "self", ".", "elapsed_time", "(", ")", "\n", "writer", ".", "add_scalar", "(", "prefix", "+", "\"/xent\"", ",", "self", ".", "xent", "(", ")", ",", "step", ")", "\n", "writer", ".", "add_scalar", "(", "prefix", "+", "\"/ppl\"", ",", "self", ".", "ppl", "(", ")", ",", "step", ")", "\n", "writer", ".", "add_scalar", "(", "prefix", "+", "\"/accuracy\"", ",", "self", ".", "accuracy", "(", ")", ",", "step", ")", "\n", "writer", ".", "add_scalar", "(", "prefix", "+", "\"/tgtper\"", ",", "self", ".", "n_words", "/", "t", ",", "step", ")", "\n", "writer", ".", "add_scalar", "(", "prefix", "+", "\"/lr\"", ",", "lr", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.__init__": [[127, 160], ["collections.defaultdict", "Trainer.Trainer.model.train", "enumerate", "torch.range().tolist", "torch.range().tolist", "torch.range().tolist", "torch.range().tolist", "torch.range", "torch.range", "torch.range", "torch.range"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.train"], ["def", "__init__", "(", "self", ",", "model", ",", "train_loss", ",", "valid_loss", ",", "optim", ",", "\n", "trunc_size", "=", "0", ",", "shard_size", "=", "32", ",", "data_type", "=", "'text'", ",", "\n", "norm_method", "=", "\"sents\"", ",", "grad_accum_count", "=", "1", ",", "\n", "q_warmup_start", "=", "0", ",", "q_warmup_steps", "=", "0", ",", "n_attn_samples", "=", "1", ")", ":", "\n", "\n", "# Basic attributes.", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "train_loss", "=", "train_loss", "\n", "self", ".", "valid_loss", "=", "valid_loss", "\n", "self", ".", "optim", "=", "optim", "\n", "self", ".", "trunc_size", "=", "trunc_size", "\n", "self", ".", "shard_size", "=", "shard_size", "\n", "self", ".", "data_type", "=", "data_type", "\n", "self", ".", "norm_method", "=", "norm_method", "\n", "self", ".", "grad_accum_count", "=", "grad_accum_count", "\n", "self", ".", "progress_step", "=", "0", "\n", "\n", "self", ".", "q_warmup_start", "=", "q_warmup_start", "\n", "self", ".", "q_warmup_steps", "=", "q_warmup_steps", "\n", "self", ".", "n_attn_samples", "=", "n_attn_samples", "\n", "self", ".", "alphas", "=", "defaultdict", "(", "lambda", ":", "1", ")", "\n", "if", "q_warmup_steps", ">", "0", ":", "\n", "            ", "for", "i", ",", "x", "in", "enumerate", "(", "torch", ".", "range", "(", "q_warmup_start", ",", "1", ",", "1", "/", "q_warmup_steps", ")", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "self", ".", "alphas", "[", "i", "]", "=", "x", "\n", "\n", "", "", "assert", "(", "grad_accum_count", ">", "0", ")", "\n", "if", "grad_accum_count", ">", "1", ":", "\n", "            ", "assert", "(", "self", ".", "trunc_size", "==", "0", ")", ",", "\"\"\"To enable accumulated gradients,\n                   you must disable target sequence truncating.\"\"\"", "\n", "\n", "# Set model in training mode.", "\n", "", "self", ".", "model", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.train": [[161, 227], ["Trainer.Statistics", "Trainer.Statistics", "enumerate", "train_iter.get_cur_dataset", "true_batchs.append", "len", "Trainer.Trainer._gradient_accumulation", "batch.tgt[].data.view().ne().sum", "batch.tgt[].data.view().ne().sum.item", "Trainer.Trainer._gradient_accumulation", "len", "len", "report_func", "sys.stdout.flush", "batch.tgt[].data.view().ne", "print", "batch.tgt[].data.view", "sum().item", "sum", "p.norm", "Trainer.Trainer.model.parameters"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.get_cur_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer._gradient_accumulation", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer._gradient_accumulation", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.report_func"], ["", "def", "train", "(", "self", ",", "train_iter", ",", "epoch", ",", "report_func", "=", "None", ")", ":", "\n", "        ", "\"\"\" Train next epoch.\n        Args:\n            train_iter: training data iterator\n            epoch(int): the epoch number\n            report_func(fn): function for logging\n\n        Returns:\n            stats (:obj:`onmt.Statistics`): epoch loss statistics\n        \"\"\"", "\n", "total_stats", "=", "Statistics", "(", ")", "\n", "report_stats", "=", "Statistics", "(", ")", "\n", "idx", "=", "0", "\n", "true_batchs", "=", "[", "]", "\n", "accum", "=", "0", "\n", "normalization", "=", "0", "\n", "try", ":", "\n", "            ", "add_on", "=", "0", "\n", "if", "len", "(", "train_iter", ")", "%", "self", ".", "grad_accum_count", ">", "0", ":", "\n", "                ", "add_on", "+=", "1", "\n", "", "num_batches", "=", "len", "(", "train_iter", ")", "/", "self", ".", "grad_accum_count", "+", "add_on", "\n", "", "except", "NotImplementedError", ":", "\n", "# Dynamic batching", "\n", "            ", "num_batches", "=", "-", "1", "\n", "\n", "", "for", "i", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "            ", "cur_dataset", "=", "train_iter", ".", "get_cur_dataset", "(", ")", "\n", "self", ".", "train_loss", ".", "cur_dataset", "=", "cur_dataset", "\n", "\n", "true_batchs", ".", "append", "(", "batch", ")", "\n", "accum", "+=", "1", "\n", "if", "self", ".", "norm_method", "==", "\"tokens\"", ":", "\n", "                ", "num_tokens", "=", "batch", ".", "tgt", "[", "1", ":", "]", ".", "data", ".", "view", "(", "-", "1", ")", ".", "ne", "(", "self", ".", "train_loss", ".", "padding_idx", ")", ".", "sum", "(", ")", "\n", "normalization", "+=", "num_tokens", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                ", "normalization", "+=", "batch", ".", "batch_size", "\n", "\n", "", "if", "accum", "==", "self", ".", "grad_accum_count", ":", "\n", "                ", "self", ".", "_gradient_accumulation", "(", "\n", "true_batchs", ",", "total_stats", ",", "\n", "report_stats", ",", "normalization", ")", "\n", "\n", "if", "report_func", "is", "not", "None", ":", "\n", "                    ", "if", "idx", "%", "1000", "==", "-", "1", "%", "1000", ":", "\n", "                        ", "print", "(", "\"|Param|: {}\"", ".", "format", "(", "sum", "(", "[", "p", ".", "norm", "(", ")", "**", "2", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "]", ")", ".", "item", "(", ")", "**", "0.5", ")", ")", "\n", "", "report_stats", "=", "report_func", "(", "\n", "epoch", ",", "idx", ",", "num_batches", ",", "\n", "self", ".", "progress_step", ",", "\n", "total_stats", ".", "_start_time", ",", "self", ".", "optim", ".", "lr", ",", "\n", "report_stats", ")", "\n", "self", ".", "progress_step", "+=", "1", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "true_batchs", "=", "[", "]", "\n", "accum", "=", "0", "\n", "normalization", "=", "0", "\n", "idx", "+=", "1", "\n", "\n", "", "", "if", "len", "(", "true_batchs", ")", ">", "0", ":", "\n", "            ", "self", ".", "_gradient_accumulation", "(", "\n", "true_batchs", ",", "total_stats", ",", "\n", "report_stats", ",", "normalization", ")", "\n", "true_batchs", "=", "[", "]", "\n", "\n", "", "return", "total_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.validate": [[228, 275], ["Trainer.Trainer.model.eval", "Trainer.Statistics", "Trainer.Trainer.model.train", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "sys.stdout.flush", "valid_iter.get_cur_dataset", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "Trainer.Trainer.model", "Trainer.Trainer.valid_loss.monolithic_compute_loss", "Trainer.Statistics.update"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.train", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.train.DatasetLazyIter.get_cur_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase.monolithic_compute_loss", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "validate", "(", "self", ",", "valid_iter", ",", "mode", "=", "None", ")", ":", "\n", "        ", "\"\"\" Validate model.\n            valid_iter: validate data iterator\n        Returns:\n            :obj:`onmt.Statistics`: validation loss statistics\n        \"\"\"", "\n", "# Set model in validating mode.", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "old_mode", "=", "self", ".", "model", ".", "mode", "\n", "if", "mode", "is", "None", ":", "\n", "            ", "mode", "=", "old_mode", "\n", "", "self", ".", "model", ".", "mode", "=", "mode", "\n", "self", ".", "valid_loss", ".", "generator", ".", "mode", "=", "mode", "\n", "# self.valid_loss.generator and self.train_loss.generator are references", "\n", "\n", "stats", "=", "Statistics", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "iii", ",", "batch", "in", "enumerate", "(", "valid_iter", ")", ":", "\n", "                ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "cur_dataset", "=", "valid_iter", ".", "get_cur_dataset", "(", ")", "\n", "self", ".", "valid_loss", ".", "cur_dataset", "=", "cur_dataset", "\n", "\n", "src", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'src'", ",", "self", ".", "data_type", ")", "\n", "if", "self", ".", "data_type", "==", "'text'", ":", "\n", "                    ", "_", ",", "src_lengths", "=", "batch", ".", "src", "\n", "", "else", ":", "\n", "                    ", "src_lengths", "=", "None", "\n", "\n", "", "tgt", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'tgt'", ")", "\n", "\n", "# F-prop through the model.", "\n", "outputs", ",", "attns", ",", "_", ",", "dist_info", ",", "outputs_baseline", "=", "self", ".", "model", "(", "src", ",", "tgt", ",", "src_lengths", ")", "\n", "\n", "# Compute loss.", "\n", "batch_stats", "=", "self", ".", "valid_loss", ".", "monolithic_compute_loss", "(", "\n", "batch", ",", "outputs", ",", "attns", ",", "dist_info", "=", "dist_info", ",", "output_baseline", "=", "outputs_baseline", ")", "\n", "\n", "# Update statistics.", "\n", "stats", ".", "update", "(", "batch_stats", ")", "\n", "\n", "# Set model back to training mode.", "\n", "", "", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "model", ".", "mode", "=", "old_mode", "\n", "self", ".", "train_loss", ".", "generator", ".", "mode", "=", "old_mode", "\n", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.epoch_step": [[276, 278], ["Trainer.Trainer.optim.update_learning_rate"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.update_learning_rate"], ["", "def", "epoch_step", "(", "self", ",", "ppl", ",", "epoch", ")", ":", "\n", "        ", "return", "self", ".", "optim", ".", "update_learning_rate", "(", "ppl", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer.drop_checkpoint": [[279, 311], ["real_model.state_dict", "real_generator.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "isinstance", "isinstance", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "onmt.io.save_fields_to_vocab", "real_model.state_dict.items", "valid_stats.accuracy", "valid_stats.expelbo"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.accuracy", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.expelbo"], ["", "def", "drop_checkpoint", "(", "self", ",", "opt", ",", "epoch", ",", "fields", ",", "valid_stats", ")", ":", "\n", "        ", "\"\"\" Save a resumable checkpoint.\n\n        Args:\n            opt (dict): option object\n            epoch (int): epoch number\n            fields (dict): fields and vocabulary\n            valid_stats : statistics of last validation run\n        \"\"\"", "\n", "real_model", "=", "(", "self", ".", "model", ".", "module", "\n", "if", "isinstance", "(", "self", ".", "model", ",", "nn", ".", "DataParallel", ")", "\n", "else", "self", ".", "model", ")", "\n", "real_generator", "=", "(", "real_model", ".", "generator", ".", "module", "\n", "if", "isinstance", "(", "real_model", ".", "generator", ",", "nn", ".", "DataParallel", ")", "\n", "else", "real_model", ".", "generator", ")", "\n", "\n", "model_state_dict", "=", "real_model", ".", "state_dict", "(", ")", "\n", "model_state_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "model_state_dict", ".", "items", "(", ")", "\n", "if", "'generator'", "not", "in", "k", "}", "\n", "generator_state_dict", "=", "real_generator", ".", "state_dict", "(", ")", "\n", "checkpoint", "=", "{", "\n", "'model'", ":", "model_state_dict", ",", "\n", "'generator'", ":", "generator_state_dict", ",", "\n", "'vocab'", ":", "onmt", ".", "io", ".", "save_fields_to_vocab", "(", "fields", ")", ",", "\n", "'opt'", ":", "opt", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'optim'", ":", "self", ".", "optim", ",", "\n", "}", "\n", "torch", ".", "save", "(", "checkpoint", ",", "\n", "'%s_acc_%.2f_ppl_%.2f_e%d.pt'", "\n", "%", "(", "opt", ".", "save_model", ",", "valid_stats", ".", "accuracy", "(", ")", ",", "\n", "valid_stats", ".", "expelbo", "(", ")", ",", "epoch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Trainer._gradient_accumulation": [[312, 376], ["Trainer.Trainer.model.zero_grad", "batch.tgt.size", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "range", "Trainer.Trainer.optim.step", "src_lengths.sum", "Trainer.Trainer.model", "Trainer.Trainer.train_loss.sharded_compute_loss", "total_stats.update", "report_stats.update", "Trainer.Trainer.model.zero_grad", "print", "print", "Trainer.Trainer.optim.step", "dec_state.detach", "Trainer.Trainer.model.named_parameters"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.zero_grad", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.step", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase.sharded_compute_loss", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.zero_grad", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.step", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.detach"], ["", "def", "_gradient_accumulation", "(", "self", ",", "true_batchs", ",", "total_stats", ",", "\n", "report_stats", ",", "normalization", ")", ":", "\n", "        ", "if", "self", ".", "grad_accum_count", ">", "1", ":", "\n", "            ", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "\n", "", "for", "batch", "in", "true_batchs", ":", "\n", "            ", "target_size", "=", "batch", ".", "tgt", ".", "size", "(", "0", ")", "\n", "# Truncated BPTT", "\n", "if", "self", ".", "trunc_size", ":", "\n", "                ", "trunc_size", "=", "self", ".", "trunc_size", "\n", "", "else", ":", "\n", "                ", "trunc_size", "=", "target_size", "\n", "\n", "", "dec_state", "=", "None", "\n", "src", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'src'", ",", "self", ".", "data_type", ")", "\n", "if", "self", ".", "data_type", "==", "'text'", ":", "\n", "                ", "_", ",", "src_lengths", "=", "batch", ".", "src", "\n", "report_stats", ".", "_n_src_words", "+=", "src_lengths", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "                ", "src_lengths", "=", "None", "\n", "\n", "", "tgt_outer", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'tgt'", ")", "\n", "\n", "for", "j", "in", "range", "(", "0", ",", "target_size", "-", "1", ",", "trunc_size", ")", ":", "\n", "# 1. Create truncated target.", "\n", "                ", "tgt", "=", "tgt_outer", "[", "j", ":", "j", "+", "trunc_size", "]", "\n", "\n", "# 2. F-prop all but generator.", "\n", "if", "self", ".", "grad_accum_count", "==", "1", ":", "\n", "                    ", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "", "outputs", ",", "attns", ",", "dec_state", ",", "dist_info", ",", "outputs_baseline", "=", "self", ".", "model", "(", "src", ",", "tgt", ",", "src_lengths", ",", "dec_state", ")", "\n", "\n", "# 3. Compute loss in shards for memory efficiency.", "\n", "self", ".", "train_loss", ".", "alpha", "=", "self", ".", "alphas", "[", "self", ".", "progress_step", "]", "\n", "batch_stats", "=", "self", ".", "train_loss", ".", "sharded_compute_loss", "(", "\n", "batch", ",", "outputs", ",", "attns", ",", "j", ",", "\n", "trunc_size", ",", "self", ".", "shard_size", ",", "normalization", ",", "\n", "dist_info", "=", "dist_info", ",", "output_baseline", "=", "outputs_baseline", ")", "\n", "\n", "# nan-check", "\n", "nans", "=", "[", "\n", "(", "name", ",", "param", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "\n", "if", "param", ".", "grad", "is", "not", "None", "and", "(", "param", ".", "grad", "!=", "param", ".", "grad", ")", ".", "any", "(", ")", "\n", "]", "\n", "if", "nans", ":", "\n", "                    ", "print", "(", "\"FOUND NANS\"", ")", "\n", "print", "(", "[", "x", "[", "0", "]", "for", "x", "in", "nans", "]", ")", "\n", "for", "_", ",", "param", "in", "nans", ":", "\n", "                        ", "param", ".", "grad", "[", "param", ".", "grad", "!=", "param", ".", "grad", "]", "=", "0", "\n", "\n", "# 4. Update the parameters and statistics.", "\n", "", "", "if", "self", ".", "grad_accum_count", "==", "1", ":", "\n", "                    ", "self", ".", "optim", ".", "step", "(", ")", "\n", "", "total_stats", ".", "update", "(", "batch_stats", ")", "\n", "report_stats", ".", "update", "(", "batch_stats", ")", "\n", "\n", "# If truncated, don't backprop fully.", "\n", "if", "dec_state", "is", "not", "None", ":", "\n", "                    ", "dec_state", ".", "detach", "(", ")", "\n", "\n", "", "", "", "if", "self", ".", "grad_accum_count", ">", "1", ":", "\n", "            ", "self", ".", "optim", ".", "step", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq": [[15, 23], ["next", "all", "str"], "function", ["None"], ["def", "aeq", "(", "*", "args", ")", ":", "\n", "    ", "\"\"\"\n    Assert all arguments have the same value\n    \"\"\"", "\n", "arguments", "=", "(", "arg", "for", "arg", "in", "args", ")", "\n", "first", "=", "next", "(", "arguments", ")", "\n", "assert", "all", "(", "arg", "==", "first", "for", "arg", "in", "arguments", ")", ",", "\"Not all arguments have the same value: \"", "+", "str", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.sequence_mask": [[25, 35], ["lengths.numel", "torch.arange().type_as().repeat().lt", "lengths.max", "lengths.unsqueeze", "torch.arange().type_as().repeat", "torch.arange().type_as", "torch.arange"], "function", ["None"], ["", "def", "sequence_mask", "(", "lengths", ",", "max_len", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Creates a boolean mask from sequence lengths.\n    \"\"\"", "\n", "batch_size", "=", "lengths", ".", "numel", "(", ")", "\n", "max_len", "=", "max_len", "or", "lengths", ".", "max", "(", ")", "\n", "return", "(", "torch", ".", "arange", "(", "0", ",", "max_len", ")", "\n", ".", "type_as", "(", "lengths", ")", "\n", ".", "repeat", "(", "batch_size", ",", "1", ")", "\n", ".", "lt", "(", "lengths", ".", "unsqueeze", "(", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu": [[37, 40], ["hasattr", "hasattr", "len"], "function", ["None"], ["", "def", "use_gpu", "(", "opt", ")", ":", "\n", "    ", "return", "(", "hasattr", "(", "opt", ",", "'gpuid'", ")", "and", "len", "(", "opt", ".", "gpuid", ")", ">", "0", ")", "or", "(", "hasattr", "(", "opt", ",", "'gpu'", ")", "and", "opt", ".", "gpu", ">", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase._check_args": [[52, 57], ["input.size", "lengths.size", "onmt.Utils.aeq"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["def", "_check_args", "(", "self", ",", "input", ",", "lengths", "=", "None", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "s_len", ",", "n_batch", ",", "n_feats", "=", "input", ".", "size", "(", ")", "\n", "if", "lengths", "is", "not", "None", ":", "\n", "            ", "n_batch_", ",", "=", "lengths", ".", "size", "(", ")", "\n", "aeq", "(", "n_batch", ",", "n_batch_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase.forward": [[58, 73], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "lengths", "=", "None", ",", "encoder_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src (:obj:`LongTensor`):\n               padded sequences of sparse indices `[src_len x batch x nfeat]`\n            lengths (:obj:`LongTensor`): length of each sequence `[batch]`\n            encoder_state (rnn-class specific):\n               initial encoder_state state.\n\n        Returns:\n            (tuple of :obj:`FloatTensor`, :obj:`FloatTensor`):\n                * final encoder state, used to initialize decoder\n                * memory bank for attention, `[src_len x batch x hidden]`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.MeanEncoder.__init__": [[82, 86], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "embeddings", ")", ":", "\n", "        ", "super", "(", "MeanEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.MeanEncoder.forward": [[87, 97], ["Models.MeanEncoder._check_args", "Models.MeanEncoder.embeddings", "Models.MeanEncoder.size", "Models.MeanEncoder.mean().expand", "Models.MeanEncoder.mean"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase._check_args"], ["", "def", "forward", "(", "self", ",", "src", ",", "lengths", "=", "None", ",", "encoder_state", "=", "None", ")", ":", "\n", "        ", "\"See :obj:`EncoderBase.forward()`\"", "\n", "self", ".", "_check_args", "(", "src", ",", "lengths", ",", "encoder_state", ")", "\n", "\n", "emb", "=", "self", ".", "embeddings", "(", "src", ")", "\n", "s_len", ",", "batch", ",", "emb_dim", "=", "emb", ".", "size", "(", ")", "\n", "mean", "=", "emb", ".", "mean", "(", "0", ")", ".", "expand", "(", "self", ".", "num_layers", ",", "batch", ",", "emb_dim", ")", "\n", "memory_bank", "=", "emb", "\n", "encoder_final", "=", "(", "mean", ",", "mean", ")", "\n", "return", "encoder_final", ",", "memory_bank", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder.__init__": [[111, 138], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "Models.rnn_factory", "Models.RNNEncoder._initialize_bridge"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.rnn_factory", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder._initialize_bridge"], ["def", "__init__", "(", "self", ",", "rnn_type", ",", "bidirectional", ",", "num_layers", ",", "\n", "hidden_size", ",", "dec_hidden_size", ",", "dropout", "=", "0.0", ",", "embeddings", "=", "None", ",", "\n", "use_bridge", "=", "False", ")", ":", "\n", "        ", "super", "(", "RNNEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "embeddings", "is", "not", "None", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "num_directions", "=", "2", "if", "bidirectional", "else", "1", "\n", "assert", "hidden_size", "%", "num_directions", "==", "0", "\n", "hidden_size", "=", "hidden_size", "//", "num_directions", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "self", ".", "rnn", ",", "self", ".", "no_pack_padded_seq", "=", "rnn_factory", "(", "rnn_type", ",", "\n", "input_size", "=", "embeddings", ".", "embedding_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "bidirectional", "=", "bidirectional", ")", "\n", "\n", "# Initialize the bridge layer", "\n", "self", ".", "use_bridge", "=", "use_bridge", "\n", "if", "self", ".", "use_bridge", ":", "\n", "            ", "self", ".", "_initialize_bridge", "(", "rnn_type", ",", "\n", "hidden_size", ",", "\n", "dec_hidden_size", ",", "\n", "num_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder.forward": [[139, 162], ["Models.RNNEncoder._check_args", "Models.RNNEncoder.size", "Models.RNNEncoder.rnn", "Models.RNNEncoder.dropout", "lengths.view().tolist.view().tolist.view().tolist", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "Models.RNNEncoder._bridge", "Models.RNNEncoder.embeddings", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "lengths.view().tolist.view().tolist.view"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase._check_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder._bridge"], ["", "", "def", "forward", "(", "self", ",", "src", ",", "lengths", "=", "None", ",", "encoder_state", "=", "None", ",", "emb", "=", "None", ")", ":", "\n", "        ", "\"See :obj:`EncoderBase.forward()`\"", "\n", "self", ".", "_check_args", "(", "src", ",", "lengths", ",", "encoder_state", ")", "\n", "\n", "if", "emb", "is", "None", ":", "\n", "            ", "emb", "=", "self", ".", "dropout", "(", "self", ".", "embeddings", "(", "src", ")", ")", "\n", "", "s_len", ",", "batch", ",", "emb_dim", "=", "emb", ".", "size", "(", ")", "\n", "self", ".", "emb_h", "=", "emb", "\n", "packed_emb", "=", "emb", "\n", "if", "lengths", "is", "not", "None", "and", "not", "self", ".", "no_pack_padded_seq", ":", "\n", "# Lengths data is wrapped inside a Variable.", "\n", "            ", "lengths", "=", "lengths", ".", "view", "(", "-", "1", ")", ".", "tolist", "(", ")", "\n", "packed_emb", "=", "pack", "(", "emb", ",", "lengths", ")", "\n", "\n", "", "memory_bank", ",", "encoder_final", "=", "self", ".", "rnn", "(", "packed_emb", ",", "encoder_state", ")", "\n", "\n", "if", "lengths", "is", "not", "None", "and", "not", "self", ".", "no_pack_padded_seq", ":", "\n", "            ", "memory_bank", "=", "unpack", "(", "memory_bank", ")", "[", "0", "]", "\n", "", "self", ".", "enc_h", "=", "memory_bank", "\n", "\n", "if", "self", ".", "use_bridge", ":", "\n", "            ", "encoder_final", "=", "self", ".", "_bridge", "(", "encoder_final", ")", "\n", "", "return", "encoder_final", ",", "memory_bank", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder._initialize_bridge": [[163, 179], ["torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "range"], "methods", ["None"], ["", "def", "_initialize_bridge", "(", "self", ",", "rnn_type", ",", "\n", "hidden_size", ",", "\n", "output_size", ",", "\n", "num_layers", ")", ":", "\n", "\n", "# LSTM has hidden and cell state, other only one", "\n", "        ", "number_of_states", "=", "2", "if", "rnn_type", "==", "\"LSTM\"", "else", "1", "\n", "# Total number of states", "\n", "self", ".", "total_hidden_dim", "=", "hidden_size", "*", "num_layers", "\n", "self", ".", "total_out_dim", "=", "output_size", "*", "num_layers", "\n", "\n", "# Build a linear layer for each", "\n", "self", ".", "bridge", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "self", ".", "total_hidden_dim", ",", "\n", "self", ".", "total_out_dim", "//", "number_of_states", ",", "\n", "bias", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "number_of_states", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNEncoder._bridge": [[180, 199], ["isinstance", "states.size", "linear", "linear.view", "tuple", "Models.RNNEncoder._bridge.bottle_hidden"], "methods", ["None"], ["", "def", "_bridge", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "\"\"\"\n        Forward hidden state through bridge\n        \"\"\"", "\n", "def", "bottle_hidden", "(", "linear", ",", "states", ")", ":", "\n", "            ", "\"\"\"\n            Transform from 3D to 2D, apply linear and return initial size\n            \"\"\"", "\n", "size", "=", "states", ".", "size", "(", ")", "\n", "result", "=", "linear", "(", "states", ".", "view", "(", "-", "1", ",", "self", ".", "total_hidden_dim", ")", ")", "\n", "#return F.relu(result).view(size)", "\n", "return", "result", ".", "view", "(", "size", "[", "0", "]", ",", "size", "[", "1", "]", ",", "-", "1", ")", "\n", "\n", "", "if", "isinstance", "(", "hidden", ",", "tuple", ")", ":", "# LSTM", "\n", "            ", "outs", "=", "tuple", "(", "[", "bottle_hidden", "(", "layer", ",", "hidden", "[", "ix", "]", ")", "\n", "for", "ix", ",", "layer", "in", "enumerate", "(", "self", ".", "bridge", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "outs", "=", "bottle_hidden", "(", "self", ".", "bridge", "[", "0", "]", ",", "hidden", ")", "\n", "", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderBase.__init__": [[247, 298], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "Models.RNNDecoderBase._build_rnn", "onmt.modules.GlobalAttention", "onmt.modules.context_gate_factory", "onmt.modules.GlobalAttention"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._build_rnn", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.context_gate_factory"], ["def", "__init__", "(", "self", ",", "rnn_type", ",", "bidirectional_encoder", ",", "num_layers", ",", "\n", "memory_size", ",", "hidden_size", ",", "attn_size", ",", "attn_type", "=", "\"general\"", ",", "\n", "coverage_attn", "=", "False", ",", "context_gate", "=", "None", ",", "\n", "copy_attn", "=", "False", ",", "dropout", "=", "0.0", ",", "embeddings", "=", "None", ",", "\n", "reuse_copy_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", "RNNDecoderBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Basic attributes.", "\n", "self", ".", "decoder_type", "=", "'rnn'", "\n", "self", ".", "bidirectional_encoder", "=", "bidirectional_encoder", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "memory_size", "=", "memory_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "attn_size", "=", "attn_size", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "# Build the RNN.", "\n", "self", ".", "rnn", "=", "self", ".", "_build_rnn", "(", "rnn_type", ",", "\n", "input_size", "=", "self", ".", "_input_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "# Set up the context gate.", "\n", "self", ".", "context_gate", "=", "None", "\n", "if", "context_gate", "is", "not", "None", ":", "\n", "            ", "self", ".", "context_gate", "=", "onmt", ".", "modules", ".", "context_gate_factory", "(", "\n", "context_gate", ",", "self", ".", "_input_size", ",", "\n", "hidden_size", ",", "hidden_size", ",", "hidden_size", "\n", ")", "\n", "\n", "# Set up the standard attention.", "\n", "", "self", ".", "_coverage", "=", "coverage_attn", "\n", "self", ".", "attn", "=", "onmt", ".", "modules", ".", "GlobalAttention", "(", "\n", "src_dim", "=", "memory_size", ",", "\n", "tgt_dim", "=", "hidden_size", ",", "\n", "attn_dim", "=", "attn_size", ",", "\n", "coverage", "=", "coverage_attn", ",", "\n", "attn_type", "=", "attn_type", ",", "\n", ")", "\n", "\n", "# Set up a separated copy attention layer, if needed.", "\n", "self", ".", "_copy", "=", "False", "\n", "if", "copy_attn", "and", "not", "reuse_copy_attn", ":", "\n", "            ", "self", ".", "copy_attn", "=", "onmt", ".", "modules", ".", "GlobalAttention", "(", "\n", "hidden_size", ",", "attn_type", "=", "attn_type", "\n", ")", "\n", "", "if", "copy_attn", ":", "\n", "            ", "self", ".", "_copy", "=", "True", "\n", "", "self", ".", "_reuse_copy_attn", "=", "reuse_copy_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderBase.forward": [[299, 342], ["isinstance", "tgt.size", "memory_bank.size", "onmt.Utils.aeq", "Models.RNNDecoderBase._run_forward_pass", "state.update_state", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "[].unsqueeze", "input_feed.unsqueeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._run_forward_pass", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.update_state"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tgt (`LongTensor`): sequences of padded tokens\n                                `[tgt_len x batch x nfeats]`.\n            memory_bank (`FloatTensor`): vectors from the encoder\n                 `[src_len x batch x hidden]`.\n            state (:obj:`onmt.Models.DecoderState`):\n                 decoder state object to initialize the decoder\n            memory_lengths (`LongTensor`): the padded source lengths\n                `[batch]`.\n        Returns:\n            (`FloatTensor`,:obj:`onmt.Models.DecoderState`,`FloatTensor`):\n                * decoder_outputs: output from the decoder (after attn)\n                         `[tgt_len x batch x hidden]`.\n                * decoder_state: final hidden state from the decoder\n                * attns: distribution over src at each tgt\n                        `[tgt_len x batch x src_len]`.\n        \"\"\"", "\n", "# Check", "\n", "assert", "isinstance", "(", "state", ",", "RNNDecoderState", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "_", ",", "memory_batch", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "memory_batch", ")", "\n", "# END", "\n", "\n", "# Run the forward pass of the RNN.", "\n", "decoder_final", ",", "decoder_outputs", ",", "input_feed", ",", "attns", "=", "self", ".", "_run_forward_pass", "(", "\n", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "memory_lengths", ")", "\n", "\n", "# Update the state with the result.", "\n", "final_output", "=", "decoder_outputs", "[", "-", "1", "]", "\n", "coverage", "=", "None", "\n", "if", "\"coverage\"", "in", "attns", ":", "\n", "            ", "coverage", "=", "attns", "[", "\"coverage\"", "]", "[", "-", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "", "state", ".", "update_state", "(", "decoder_final", ",", "input_feed", ".", "unsqueeze", "(", "0", ")", ",", "coverage", ")", "\n", "\n", "# Concatenates sequence of tensors along a new dimension.", "\n", "decoder_outputs", "=", "torch", ".", "stack", "(", "decoder_outputs", ")", "\n", "for", "k", "in", "attns", ":", "\n", "            ", "attns", "[", "k", "]", "=", "torch", ".", "stack", "(", "attns", "[", "k", "]", ")", "\n", "\n", "", "return", "decoder_outputs", ",", "state", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderBase.init_decoder_state": [[343, 367], ["isinstance", "encoder_final[].size", "Models.RNNDecoderState", "Models.RNNDecoderState", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Models.RNNDecoderBase.init_decoder_state._fix_enc_hidden"], "methods", ["None"], ["", "def", "init_decoder_state", "(", "self", ",", "src", ",", "memory_bank", ",", "encoder_final", ")", ":", "\n", "        ", "def", "_fix_enc_hidden", "(", "h", ")", ":", "\n", "# The encoder hidden is  (layers*directions) x batch x dim.", "\n", "# We need to convert it to layers x batch x (directions*dim).", "\n", "            ", "if", "self", ".", "bidirectional_encoder", ":", "\n", "                ", "h", "=", "torch", ".", "cat", "(", "[", "h", "[", "0", ":", "h", ".", "size", "(", "0", ")", ":", "2", "]", ",", "h", "[", "1", ":", "h", ".", "size", "(", "0", ")", ":", "2", "]", "]", ",", "2", ")", "\n", "", "return", "h", "\n", "\n", "", "if", "isinstance", "(", "encoder_final", ",", "tuple", ")", ":", "# LSTM", "\n", "# Zero out initial hidden state", "\n", "            ", "L", "=", "self", ".", "rnn", ".", "num_layers", "\n", "N", "=", "encoder_final", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "H", "=", "self", ".", "rnn", ".", "hidden_size", "\n", "return", "RNNDecoderState", "(", "self", ".", "hidden_size", ",", "\n", "#tuple([_fix_enc_hidden(enc_hid)", "\n", "#for enc_hid in encoder_final]),", "\n", "(", "\n", "torch", ".", "zeros", "(", "L", ",", "N", ",", "H", ")", ".", "to", "(", "encoder_final", "[", "0", "]", ")", ",", "\n", "torch", ".", "zeros", "(", "L", ",", "N", ",", "H", ")", ".", "to", "(", "encoder_final", "[", "0", "]", ")", ",", "\n", ")", ",", "\n", "self", ".", "memory_size", ")", "\n", "", "else", ":", "# GRU", "\n", "            ", "return", "RNNDecoderState", "(", "self", ".", "hidden_size", ",", "\n", "_fix_enc_hidden", "(", "encoder_final", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.StdRNNDecoder._run_forward_pass": [[384, 444], ["Models.StdRNNDecoder.dropout", "isinstance", "tgt.size", "rnn_output.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "Models.StdRNNDecoder.attn", "Models.StdRNNDecoder.dropout", "Models.StdRNNDecoder.embeddings", "Models.StdRNNDecoder.rnn", "Models.StdRNNDecoder.rnn", "rnn_output.transpose().contiguous", "memory_bank.transpose", "Models.StdRNNDecoder.context_gate", "decoder_outputs.view.view.view", "Models.StdRNNDecoder.view", "rnn_output.view", "decoder_outputs.view.view.view", "rnn_output.transpose", "Models.StdRNNDecoder.size", "rnn_output.size", "decoder_outputs.view.view.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["def", "_run_forward_pass", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Private helper for running the specific RNN forward pass.\n        Must be overriden by all subclasses.\n        Args:\n            tgt (LongTensor): a sequence of input tokens tensors\n                                 [len x batch x nfeats].\n            memory_bank (FloatTensor): output(tensor sequence) from the encoder\n                        RNN of size (src_len x batch x hidden_size).\n            state (FloatTensor): hidden state from the encoder RNN for\n                                 initializing the decoder.\n            memory_lengths (LongTensor): the source memory_bank lengths.\n        Returns:\n            decoder_final (Variable): final hidden state from the decoder.\n            decoder_outputs ([FloatTensor]): an array of output of every time\n                                     step from the decoder.\n            attns (dict of (str, [FloatTensor]): a dictionary of different\n                            type of attention Tensor array of every time\n                            step from the decoder.\n        \"\"\"", "\n", "assert", "not", "self", ".", "_copy", "# TODO, no support yet.", "\n", "assert", "not", "self", ".", "_coverage", "# TODO, no support yet.", "\n", "\n", "# Initialize local and return variables.", "\n", "attns", "=", "{", "}", "\n", "emb", "=", "self", ".", "dropout", "(", "self", ".", "embeddings", "(", "tgt", ")", ")", "\n", "\n", "# Run the forward pass of the RNN.", "\n", "if", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "GRU", ")", ":", "\n", "            ", "rnn_output", ",", "decoder_final", "=", "self", ".", "rnn", "(", "emb", ",", "state", ".", "hidden", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "rnn_output", ",", "decoder_final", "=", "self", ".", "rnn", "(", "emb", ",", "state", ".", "hidden", ")", "\n", "\n", "# Check", "\n", "", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "output_len", ",", "output_batch", ",", "_", "=", "rnn_output", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_len", ",", "output_len", ")", "\n", "aeq", "(", "tgt_batch", ",", "output_batch", ")", "\n", "# END", "\n", "\n", "# Calculate the attention.", "\n", "decoder_outputs", ",", "p_attn", "=", "self", ".", "attn", "(", "\n", "rnn_output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ",", "\n", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "memory_lengths", "=", "memory_lengths", "\n", ")", "\n", "attns", "[", "\"std\"", "]", "=", "p_attn", "\n", "\n", "# Calculate the context gate.", "\n", "if", "self", ".", "context_gate", "is", "not", "None", ":", "\n", "            ", "decoder_outputs", "=", "self", ".", "context_gate", "(", "\n", "emb", ".", "view", "(", "-", "1", ",", "emb", ".", "size", "(", "2", ")", ")", ",", "\n", "rnn_output", ".", "view", "(", "-", "1", ",", "rnn_output", ".", "size", "(", "2", ")", ")", ",", "\n", "decoder_outputs", ".", "view", "(", "-", "1", ",", "decoder_outputs", ".", "size", "(", "2", ")", ")", "\n", ")", "\n", "decoder_outputs", "=", "decoder_outputs", ".", "view", "(", "tgt_len", ",", "tgt_batch", ",", "self", ".", "hidden_size", ")", "\n", "\n", "", "decoder_outputs", "=", "self", ".", "dropout", "(", "decoder_outputs", ")", "\n", "return", "decoder_final", ",", "decoder_outputs", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.StdRNNDecoder._build_rnn": [[445, 448], ["Models.rnn_factory"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.rnn_factory"], ["", "def", "_build_rnn", "(", "self", ",", "rnn_type", ",", "**", "kwargs", ")", ":", "\n", "        ", "rnn", ",", "_", "=", "rnn_factory", "(", "rnn_type", ",", "**", "kwargs", ")", "\n", "return", "rnn", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.StdRNNDecoder._input_size": [[449, 455], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_input_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Private helper returning the number of expected features.\n        \"\"\"", "\n", "return", "self", ".", "embeddings", ".", "embedding_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.InputFeedRNNDecoder._run_forward_pass": [[484, 561], ["state.input_feed.squeeze", "state.input_feed.squeeze.size", "tgt.size", "onmt.Utils.aeq", "Models.InputFeedRNNDecoder.dropout", "enumerate", "Models.InputFeedRNNDecoder.embeddings", "Models.InputFeedRNNDecoder.dim", "state.coverage.squeeze", "Models.InputFeedRNNDecoder.split", "emb_t.squeeze.squeeze.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Models.InputFeedRNNDecoder.rnn", "rnn_output.squeeze.squeeze.squeeze", "Models.InputFeedRNNDecoder.attn", "Models.InputFeedRNNDecoder.dec_h.append", "Models.InputFeedRNNDecoder.p_attn_score.append", "Models.InputFeedRNNDecoder.src_context.append", "Models.InputFeedRNNDecoder.context.append", "Models.InputFeedRNNDecoder.dropout", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "memory_bank.transpose", "Models.InputFeedRNNDecoder.context_gate", "Models.InputFeedRNNDecoder.copy_attn", "memory_bank.transpose"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["def", "_run_forward_pass", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See StdRNNDecoder._run_forward_pass() for description\n        of arguments and return values.\n        \"\"\"", "\n", "# Additional args check.", "\n", "input_feed", "=", "state", ".", "input_feed", ".", "squeeze", "(", "0", ")", "\n", "input_feed_batch", ",", "_", "=", "input_feed", ".", "size", "(", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "input_feed_batch", ")", "\n", "# END Additional args check.", "\n", "\n", "# Initialize local and return variables.", "\n", "decoder_outputs", "=", "[", "]", "\n", "attns", "=", "{", "\"std\"", ":", "[", "]", "}", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "[", "]", "\n", "", "if", "self", ".", "_coverage", ":", "\n", "            ", "attns", "[", "\"coverage\"", "]", "=", "[", "]", "\n", "\n", "", "emb", "=", "self", ".", "dropout", "(", "self", ".", "embeddings", "(", "tgt", ")", ")", "\n", "assert", "emb", ".", "dim", "(", ")", "==", "3", "# len x batch x embedding_dim", "\n", "\n", "hidden", "=", "state", ".", "hidden", "\n", "coverage", "=", "state", ".", "coverage", ".", "squeeze", "(", "0", ")", "if", "state", ".", "coverage", "is", "not", "None", "else", "None", "\n", "\n", "# Input feed concatenates hidden state with", "\n", "# input at every time step.", "\n", "# DBG", "\n", "self", ".", "p_attn_score", "=", "[", "]", "\n", "self", ".", "dec_h", "=", "[", "]", "\n", "self", ".", "src_context", "=", "[", "]", "\n", "self", ".", "context", "=", "[", "]", "\n", "for", "i", ",", "emb_t", "in", "enumerate", "(", "emb", ".", "split", "(", "1", ")", ")", ":", "\n", "            ", "emb_t", "=", "emb_t", ".", "squeeze", "(", "0", ")", "\n", "decoder_input", "=", "torch", ".", "cat", "(", "[", "emb_t", ",", "input_feed", "]", ",", "1", ")", "\n", "\n", "rnn_output", ",", "hidden", "=", "self", ".", "rnn", "(", "decoder_input", ".", "unsqueeze", "(", "0", ")", ",", "hidden", ")", "\n", "rnn_output", "=", "rnn_output", ".", "squeeze", "(", "0", ")", "\n", "decoder_output", ",", "p_attn", ",", "input_feed", "=", "self", ".", "attn", "(", "\n", "rnn_output", ",", "\n", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "memory_lengths", "=", "memory_lengths", ")", "\n", "# DBG", "\n", "self", ".", "dec_h", ".", "append", "(", "rnn_output", ")", "\n", "self", ".", "p_attn_score", ".", "append", "(", "self", ".", "attn", ".", "p_attn_score", ")", "\n", "self", ".", "src_context", ".", "append", "(", "input_feed", ")", "\n", "self", ".", "context", ".", "append", "(", "decoder_output", ")", "\n", "\n", "if", "self", ".", "context_gate", "is", "not", "None", ":", "\n", "# TODO: context gate should be employed", "\n", "# instead of second RNN transform.", "\n", "                ", "decoder_output", "=", "self", ".", "context_gate", "(", "\n", "decoder_input", ",", "rnn_output", ",", "decoder_output", "\n", ")", "\n", "", "decoder_output", "=", "self", ".", "dropout", "(", "decoder_output", ")", "\n", "#input_feed = decoder_output", "\n", "\n", "decoder_outputs", "+=", "[", "decoder_output", "]", "\n", "attns", "[", "\"std\"", "]", "+=", "[", "p_attn", "]", "\n", "\n", "# Update the coverage attention.", "\n", "if", "self", ".", "_coverage", ":", "\n", "                ", "coverage", "=", "coverage", "+", "p_attn", "if", "coverage", "is", "not", "None", "else", "p_attn", "\n", "attns", "[", "\"coverage\"", "]", "+=", "[", "coverage", "]", "\n", "\n", "# Run the forward pass of the copy attention layer.", "\n", "", "if", "self", ".", "_copy", "and", "not", "self", ".", "_reuse_copy_attn", ":", "\n", "                ", "_", ",", "copy_attn", "=", "self", ".", "copy_attn", "(", "decoder_output", ",", "\n", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "attns", "[", "\"copy\"", "]", "+=", "[", "copy_attn", "]", "\n", "", "elif", "self", ".", "_copy", ":", "\n", "                ", "attns", "[", "\"copy\"", "]", "=", "attns", "[", "\"std\"", "]", "\n", "# Return result.", "\n", "", "", "return", "hidden", ",", "decoder_outputs", ",", "input_feed", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.InputFeedRNNDecoder._build_rnn": [[562, 572], ["torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["None"], ["", "def", "_build_rnn", "(", "self", ",", "rnn_type", ",", "input_size", ",", "\n", "hidden_size", ",", "num_layers", ",", "dropout", ")", ":", "\n", "        ", "assert", "not", "rnn_type", "==", "\"SRU\"", ",", "\"SRU doesn't support input feed! \"", "\"Please set -input_feed 0!\"", "\n", "if", "rnn_type", "==", "\"LSTM\"", ":", "\n", "            ", "stacked_cell", "=", "onmt", ".", "modules", ".", "StackedLSTM", "\n", "", "else", ":", "\n", "            ", "stacked_cell", "=", "onmt", ".", "modules", ".", "StackedGRU", "\n", "", "return", "nn", ".", "LSTM", "(", "num_layers", "=", "num_layers", ",", "input_size", "=", "input_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.InputFeedRNNDecoder._input_size": [[573, 579], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_input_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Using input feed by concatenating input with attention vectors.\n        \"\"\"", "\n", "return", "self", ".", "embeddings", ".", "embedding_size", "+", "self", ".", "memory_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.NMTModel.__init__": [[591, 597], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "multigpu", "=", "False", ")", ":", "\n", "        ", "self", ".", "multigpu", "=", "multigpu", "\n", "super", "(", "NMTModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "mode", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.NMTModel.forward": [[598, 635], ["Models.NMTModel.encoder", "Models.NMTModel.decoder.init_decoder_state", "Models.NMTModel.decoder"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.init_decoder_state"], ["", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "lengths", ",", "dec_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Forward propagate a `src` and `tgt` pair for training.\n        Possible initialized with a beginning decoder state.\n\n        Args:\n            src (:obj:`Tensor`):\n                a source sequence passed to encoder.\n                typically for inputs this will be a padded :obj:`LongTensor`\n                of size `[len x batch x features]`. however, may be an\n                image or other generic input depending on encoder.\n            tgt (:obj:`LongTensor`):\n                 a target sequence of size `[tgt_len x batch]`.\n            lengths(:obj:`LongTensor`): the src lengths, pre-padding `[batch]`.\n            dec_state (:obj:`DecoderState`, optional): initial decoder state\n        Returns:\n            (:obj:`FloatTensor`, `dict`, :obj:`onmt.Models.DecoderState`):\n\n                 * decoder output `[tgt_len x batch x hidden]`\n                 * dictionary attention dists of `[tgt_len x batch x src_len]`\n                 * final decoder state\n        \"\"\"", "\n", "tgt", "=", "tgt", "[", ":", "-", "1", "]", "# exclude last target from inputs", "\n", "\n", "enc_final", ",", "memory_bank", "=", "self", ".", "encoder", "(", "src", ",", "lengths", ")", "\n", "enc_state", "=", "self", ".", "decoder", ".", "init_decoder_state", "(", "src", ",", "memory_bank", ",", "enc_final", ")", "\n", "\n", "decoder_outputs", ",", "dec_state", ",", "attns", "=", "self", ".", "decoder", "(", "tgt", ",", "memory_bank", ",", "\n", "enc_state", "if", "dec_state", "is", "None", "\n", "else", "dec_state", ",", "\n", "memory_lengths", "=", "lengths", ")", "\n", "if", "self", ".", "multigpu", ":", "\n", "# Not yet supported on multi-gpu", "\n", "            ", "dec_state", "=", "None", "\n", "attns", "=", "None", "\n", "", "return", "decoder_outputs", ",", "attns", ",", "dec_state", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.detach": [[651, 660], ["Models.DecoderState.__new__", "type", "Models.DecoderState.detach.det"], "methods", ["None"], ["def", "detach", "(", "self", ")", ":", "\n", "        ", "def", "det", "(", "h", ")", ":", "\n", "            ", "if", "h", "is", "not", "None", ":", "\n", "                ", "return", "h", ".", "detach", "(", ")", "\n", "", "", "return", "self", ".", "__new__", "(", "\n", "type", "(", "self", ")", ",", "\n", "det", "(", "self", ".", "input_feed", ")", ",", "\n", "self", ".", "hidden_size", ",", "\n", "tuple", "(", "det", "(", "x", ")", "for", "x", "in", "self", ".", "hidden", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.beam_update": [[662, 677], ["e.size", "sent_states.data.copy_", "len", "sent_states.data.index_select", "e.view", "e.view"], "methods", ["None"], ["", "def", "beam_update", "(", "self", ",", "idx", ",", "positions", ",", "beam_size", ")", ":", "\n", "        ", "for", "e", "in", "self", ".", "_all", ":", "\n", "            ", "sizes", "=", "e", ".", "size", "(", ")", "\n", "br", "=", "sizes", "[", "1", "]", "\n", "if", "len", "(", "sizes", ")", "==", "3", ":", "\n", "                ", "sent_states", "=", "e", ".", "view", "(", "sizes", "[", "0", "]", ",", "beam_size", ",", "br", "//", "beam_size", ",", "\n", "sizes", "[", "2", "]", ")", "[", ":", ",", ":", ",", "idx", "]", "\n", "", "else", ":", "\n", "                ", "sent_states", "=", "e", ".", "view", "(", "sizes", "[", "0", "]", ",", "beam_size", ",", "\n", "br", "//", "beam_size", ",", "\n", "sizes", "[", "2", "]", ",", "\n", "sizes", "[", "3", "]", ")", "[", ":", ",", ":", ",", "idx", "]", "\n", "\n", "", "sent_states", ".", "data", ".", "copy_", "(", "\n", "sent_states", ".", "data", ".", "index_select", "(", "1", ",", "positions", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderState.__init__": [[680, 699], ["Models.RNNDecoderState.hidden[].size", "torch.autograd.Variable().unsqueeze", "torch.autograd.Variable().unsqueeze", "torch.autograd.Variable().unsqueeze", "isinstance", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "Models.RNNDecoderState.hidden[].data.new().zero_", "Models.RNNDecoderState.hidden[].data.new"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "rnnstate", ",", "context_size", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden_size (int): the size of hidden layer of the decoder.\n            rnnstate: final hidden state from the encoder.\n                transformed to shape: layers x batch x (directions*dim).\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "rnnstate", ",", "tuple", ")", ":", "\n", "            ", "self", ".", "hidden", "=", "(", "rnnstate", ",", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden", "=", "rnnstate", "\n", "", "self", ".", "coverage", "=", "None", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "# Init the input feed.", "\n", "batch_size", "=", "self", ".", "hidden", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "h_size", "=", "(", "batch_size", ",", "context_size", "if", "context_size", "is", "not", "None", "else", "hidden_size", ")", "\n", "self", ".", "input_feed", "=", "Variable", "(", "self", ".", "hidden", "[", "0", "]", ".", "data", ".", "new", "(", "*", "h_size", ")", ".", "zero_", "(", ")", ",", "\n", "requires_grad", "=", "False", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderState._all": [[700, 703], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_all", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "hidden", "+", "(", "self", ".", "input_feed", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderState.update_state": [[704, 711], ["isinstance"], "methods", ["None"], ["", "def", "update_state", "(", "self", ",", "rnnstate", ",", "input_feed", ",", "coverage", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "rnnstate", ",", "tuple", ")", ":", "\n", "            ", "self", ".", "hidden", "=", "(", "rnnstate", ",", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden", "=", "rnnstate", "\n", "", "self", ".", "input_feed", "=", "input_feed", "\n", "self", ".", "coverage", "=", "coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.RNNDecoderState.repeat_beam_size_times": [[712, 718], ["tuple", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "e.data.repeat"], "methods", ["None"], ["", "def", "repeat_beam_size_times", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\" Repeat beam_size times along batch dimension. \"\"\"", "\n", "vars", "=", "[", "Variable", "(", "e", ".", "data", ".", "repeat", "(", "1", ",", "beam_size", ",", "1", ")", ",", "volatile", "=", "True", ")", "\n", "for", "e", "in", "self", ".", "_all", "]", "\n", "self", ".", "hidden", "=", "tuple", "(", "vars", "[", ":", "-", "1", "]", ")", "\n", "self", ".", "input_feed", "=", "vars", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.Generator.__init__": [[721, 725], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "mode", "=", "\"sample\"", ")", ":", "\n", "        ", "super", "(", "Generator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.Generator.logsumexp": [[726, 733], ["x.max", "m.unsqueeze.unsqueeze.unsqueeze", "m.unsqueeze.unsqueeze.unsqueeze"], "methods", ["None"], ["", "def", "logsumexp", "(", "self", ",", "x", ",", "dim", "=", "0", ",", "keepdim", "=", "False", ")", ":", "\n", "        ", "m", "=", "x", ".", "max", "(", "dim", ")", "[", "0", "]", "\n", "if", "keepdim", ":", "\n", "            ", "m", "=", "m", ".", "unsqueeze", "(", "dim", ")", "\n", "return", "m", "+", "(", "x", "-", "m", ")", ".", "exp", "(", ")", ".", "sum", "(", "dim", ",", "keepdim", "=", "keepdim", ")", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "m", "+", "(", "x", "-", "m", ".", "unsqueeze", "(", "dim", ")", ")", ".", "exp", "(", ")", ".", "sum", "(", "dim", ",", "keepdim", "=", "keepdim", ")", ".", "log", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.Generator.forward": [[734, 757], ["torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "Models.Generator.proj", "input.dim", "Models.Generator.size", "Models.Generator.squeeze", "Models.Generator.logsumexp", "log_pa.transpose().unsqueeze", "Models.Generator.sum", "pa.transpose().unsqueeze", "Models.Generator.logsumexp", "log_pa.transpose", "math.log", "pa.transpose", "input.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.Generator.logsumexp", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.Generator.logsumexp", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "log_pa", "=", "None", ",", "pa", "=", "None", ")", ":", "\n", "# log_pa: T x N x S=K", "\n", "        ", "scores", "=", "F", ".", "log_softmax", "(", "self", ".", "proj", "(", "input", ")", ",", "dim", "=", "-", "1", ")", "# target, K, batch, vocab", "\n", "if", "input", ".", "dim", "(", ")", "==", "3", ":", "\n", "# Short-circuit", "\n", "            ", "return", "scores", "\n", "", "if", "scores", ".", "size", "(", "1", ")", "==", "1", ":", "\n", "            ", "scores", "=", "scores", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"exact\"", "and", "log_pa", "is", "not", "None", ":", "\n", "# for exact marginal", "\n", "                ", "scores", "=", "scores", "+", "log_pa", ".", "transpose", "(", "1", ",", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "scores", "=", "self", ".", "logsumexp", "(", "scores", ",", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "", "elif", "self", ".", "mode", "==", "\"enum\"", "and", "pa", "is", "not", "None", ":", "\n", "# for exact elbo", "\n", "                ", "scores", "=", "scores", "*", "pa", ".", "transpose", "(", "1", ",", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "scores", "=", "scores", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "", "elif", "self", ".", "mode", "==", "\"wsram\"", ":", "\n", "                ", "return", "scores", "\n", "", "else", ":", "\n", "                ", "scores", "=", "self", ".", "logsumexp", "(", "scores", ",", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "scores", "=", "scores", "-", "math", ".", "log", "(", "input", ".", "size", "(", "1", ")", ")", "\n", "", "", "return", "scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.rnn_factory": [[16, 26], ["onmt.modules.SRU", "getattr"], "function", ["None"], ["def", "rnn_factory", "(", "rnn_type", ",", "**", "kwargs", ")", ":", "\n", "# Use pytorch version when available.", "\n", "    ", "no_pack_padded_seq", "=", "False", "\n", "if", "rnn_type", "==", "\"SRU\"", ":", "\n", "# SRU doesn't support PackedSequence.", "\n", "        ", "no_pack_padded_seq", "=", "True", "\n", "rnn", "=", "onmt", ".", "modules", ".", "SRU", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "rnn", "=", "getattr", "(", "nn", ",", "rnn_type", ")", "(", "**", "kwargs", ")", "\n", "", "return", "rnn", ",", "no_pack_padded_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.__init__": [[6, 8], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "op", ")", ":", "\n", "        ", "self", ".", "optimizers", "=", "op", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.zero_grad": [[9, 12], ["op.zero_grad"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "for", "op", "in", "self", ".", "optimizers", ":", "\n", "            ", "op", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.MultipleOptimizer.step": [[13, 16], ["op.step"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.step"], ["", "", "def", "step", "(", "self", ")", ":", "\n", "        ", "for", "op", "in", "self", ".", "optimizers", ":", "\n", "            ", "op", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.__init__": [[47, 70], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "method", ",", "lr", ",", "max_grad_norm", ",", "\n", "lr_decay", "=", "1", ",", "start_decay_at", "=", "None", ",", "\n", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ",", "eps", "=", "1e-9", ",", "\n", "adagrad_accum", "=", "0.0", ",", "\n", "decay_method", "=", "None", ",", "\n", "warmup_steps", "=", "4000", ",", "\n", "model_size", "=", "None", ")", ":", "\n", "        ", "self", ".", "last_ppl", "=", "None", "\n", "self", ".", "best_ppl", "=", "None", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "original_lr", "=", "lr", "\n", "self", ".", "max_grad_norm", "=", "max_grad_norm", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "lr_decay", "=", "lr_decay", "\n", "self", ".", "start_decay_at", "=", "start_decay_at", "\n", "self", ".", "start_decay", "=", "False", "\n", "self", ".", "_step", "=", "0", "\n", "self", ".", "betas", "=", "[", "beta1", ",", "beta2", "]", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "adagrad_accum", "=", "adagrad_accum", "\n", "self", ".", "decay_method", "=", "decay_method", "\n", "self", ".", "warmup_steps", "=", "warmup_steps", "\n", "self", ".", "model_size", "=", "model_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.set_parameters": [[71, 102], ["torch.SGD", "torch.Adagrad", "Optim.Optim.params.append", "Optim.Optim.sparse_params.append", "print", "torch.Adadelta", "[].fill_", "torch.Adam", "Optim.MultipleOptimizer", "RuntimeError", "torch.Adam", "torch.SparseAdam"], "methods", ["None"], ["", "def", "set_parameters", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "params", "=", "[", "]", "\n", "self", ".", "sparse_params", "=", "[", "]", "\n", "for", "k", ",", "p", "in", "params", ":", "\n", "            ", "if", "p", ".", "requires_grad", ":", "\n", "                ", "if", "self", ".", "method", "!=", "'sparseadam'", "or", "\"embed\"", "not", "in", "k", ":", "\n", "                    ", "self", ".", "params", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "sparse_params", ".", "append", "(", "p", ")", "\n", "print", "(", "\"Sparse parameter {}\"", ".", "format", "(", "k", ")", ")", "\n", "", "", "", "if", "self", ".", "method", "==", "'sgd'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "SGD", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "elif", "self", ".", "method", "==", "'adagrad'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adagrad", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                    ", "self", ".", "optimizer", ".", "state", "[", "p", "]", "[", "'sum'", "]", "=", "self", ".", "optimizer", ".", "state", "[", "p", "]", "[", "'sum'", "]", ".", "fill_", "(", "self", ".", "adagrad_accum", ")", "\n", "", "", "", "elif", "self", ".", "method", "==", "'adadelta'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adadelta", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "", "elif", "self", ".", "method", "==", "'adam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ",", "\n", "betas", "=", "self", ".", "betas", ",", "eps", "=", "self", ".", "eps", ")", "\n", "", "elif", "self", ".", "method", "==", "'sparseadam'", ":", "\n", "            ", "self", ".", "optimizer", "=", "MultipleOptimizer", "(", "\n", "[", "optim", ".", "Adam", "(", "self", ".", "params", ",", "lr", "=", "self", ".", "lr", ",", "\n", "betas", "=", "self", ".", "betas", ",", "eps", "=", "1e-8", ")", ",", "\n", "optim", ".", "SparseAdam", "(", "self", ".", "sparse_params", ",", "lr", "=", "self", ".", "lr", ",", "\n", "betas", "=", "self", ".", "betas", ",", "eps", "=", "1e-8", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Invalid optim method: \"", "+", "self", ".", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim._set_rate": [[103, 110], ["None"], "methods", ["None"], ["", "", "def", "_set_rate", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "lr", "=", "lr", "\n", "if", "self", ".", "method", "!=", "'sparseadam'", ":", "\n", "            ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "self", ".", "lr", "\n", "", "else", ":", "\n", "            ", "for", "op", "in", "self", ".", "optimizer", ".", "optimizers", ":", "\n", "                ", "op", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "self", ".", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.step": [[111, 130], ["Optim.Optim.optimizer.step", "Optim.Optim._set_rate", "torch.nn.utils.clip_grad_norm", "min"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.step", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim._set_rate"], ["", "", "", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update the model parameters based on current gradients.\n\n        Optionally, will employ gradient modification or update learning\n        rate.\n        \"\"\"", "\n", "self", ".", "_step", "+=", "1", "\n", "\n", "# Decay method used in tensor2tensor.", "\n", "if", "self", ".", "decay_method", "==", "\"noam\"", ":", "\n", "            ", "self", ".", "_set_rate", "(", "\n", "self", ".", "original_lr", "*", "\n", "(", "self", ".", "model_size", "**", "(", "-", "0.5", ")", "*", "\n", "min", "(", "self", ".", "_step", "**", "(", "-", "0.5", ")", ",", "\n", "self", ".", "_step", "*", "self", ".", "warmup_steps", "**", "(", "-", "1.5", ")", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "max_grad_norm", ":", "\n", "            ", "clip_grad_norm", "(", "self", ".", "params", ",", "self", ".", "max_grad_norm", ")", "\n", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Optim.Optim.update_learning_rate": [[131, 149], ["print"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ",", "ppl", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"\n        Decay learning rate if val perf does not improve\n        or we hit the start_decay_at limit.\n        \"\"\"", "\n", "if", "self", ".", "best_ppl", "is", "not", "None", ":", "\n", "            ", "if", "ppl", ">", "self", ".", "best_ppl", ":", "\n", "                ", "if", "epoch", ">=", "self", ".", "start_decay_at", ":", "\n", "                    ", "self", ".", "lr", "=", "self", ".", "lr", "*", "self", ".", "lr_decay", "\n", "print", "(", "\"Decaying learning rate to %g\"", "%", "self", ".", "lr", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "best_ppl", "=", "ppl", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "best_ppl", "=", "ppl", "\n", "\n", "", "self", ".", "last_ppl", "=", "ppl", "\n", "if", "self", ".", "method", "!=", "'sparseadam'", ":", "\n", "            ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "self", ".", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.InferenceNetwork.__init__": [[19, 65], ["torch.Module.__init__", "float", "onmt.Models.MeanEncoder", "onmt.Models.MeanEncoder", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "Exception", "onmt.Models.RNNEncoder", "onmt.Models.RNNEncoder", "float", "onmt.Models.RNNEncoder", "onmt.Models.RNNEncoder", "onmt.Models.RNNEncoder", "onmt.Models.RNNEncoder"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inference_network_type", ",", "src_embeddings", ",", "tgt_embeddings", ",", "\n", "rnn_type", ",", "src_layers", ",", "tgt_layers", ",", "rnn_size", ",", "dropout", ",", "\n", "attn_type", "=", "\"general\"", ",", "\n", "dist_type", "=", "\"none\"", ",", "scoresF", "=", "F", ".", "softplus", ")", ":", "\n", "        ", "super", "(", "InferenceNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "inference_network_type", "=", "inference_network_type", "\n", "self", ".", "attn_type", "=", "attn_type", "\n", "self", ".", "dist_type", "=", "dist_type", "\n", "\n", "self", ".", "scoresF", "=", "scoresF", "\n", "\n", "if", "dist_type", "==", "\"none\"", ":", "\n", "            ", "self", ".", "mask_val", "=", "float", "(", "\"-inf\"", ")", "\n", "", "elif", "dist_type", "==", "\"categorical\"", ":", "\n", "            ", "self", ".", "mask_val", "=", "-", "float", "(", "'inf'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid distribution type\"", ")", "\n", "\n", "", "if", "inference_network_type", "==", "'embedding_only'", ":", "\n", "            ", "self", ".", "src_encoder", "=", "MeanEncoder", "(", "src_layers", ",", "src_embeddings", ")", "\n", "self", ".", "tgt_encoder", "=", "MeanEncoder", "(", "tgt_layers", ",", "tgt_embeddings", ")", "\n", "", "elif", "inference_network_type", "==", "'brnn'", ":", "\n", "            ", "self", ".", "src_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "True", ",", "src_layers", ",", "rnn_size", ",", "\n", "rnn_size", ",", "\n", "dropout", ",", "src_embeddings", ",", "False", ")", "\n", "self", ".", "tgt_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "True", ",", "tgt_layers", ",", "rnn_size", ",", "\n", "rnn_size", ",", "\n", "dropout", ",", "tgt_embeddings", ",", "False", ")", "\n", "", "elif", "inference_network_type", "==", "'bigbrnn'", ":", "\n", "            ", "self", ".", "src_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "True", ",", "src_layers", ",", "2", "*", "rnn_size", ",", "\n", "2", "*", "rnn_size", ",", "\n", "dropout", ",", "src_embeddings", ",", "False", ")", "\n", "self", ".", "tgt_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "True", ",", "tgt_layers", ",", "2", "*", "rnn_size", ",", "\n", "2", "*", "rnn_size", ",", "\n", "dropout", ",", "tgt_embeddings", ",", "False", ")", "\n", "", "elif", "inference_network_type", "==", "'rnn'", ":", "\n", "            ", "self", ".", "src_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "True", ",", "src_layers", ",", "rnn_size", ",", "\n", "dropout", ",", "src_embeddings", ",", "False", ")", "\n", "self", ".", "tgt_encoder", "=", "RNNEncoder", "(", "rnn_type", ",", "False", ",", "tgt_layers", ",", "rnn_size", ",", "\n", "dropout", ",", "tgt_embeddings", ",", "False", ")", "\n", "", "if", "inference_network_type", "==", "\"bigbrnn\"", ":", "\n", "            ", "self", ".", "W", "=", "torch", ".", "nn", ".", "Linear", "(", "rnn_size", "*", "2", ",", "rnn_size", "*", "2", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "W", "=", "torch", ".", "nn", ".", "Linear", "(", "rnn_size", ",", "rnn_size", ",", "bias", "=", "False", ")", "\n", "", "self", ".", "rnn_size", "=", "rnn_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.InferenceNetwork.forward": [[66, 115], ["ViModels.InferenceNetwork.src_encoder", "src_memory_bank.transpose.transpose.size", "ViModels.InferenceNetwork.tgt_encoder", "src_memory_bank.transpose.transpose.transpose", "src_memory_bank.transpose.transpose.transpose", "ViModels.InferenceNetwork.W", "ViModels.InferenceNetwork.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "onmt.Utils.Params.transpose", "log_scores.transpose.transpose.transpose", "onmt.Utils.Params", "onmt.Utils.sequence_mask", "mask.unsqueeze.unsqueeze.unsqueeze", "onmt.Utils.Params.data.masked_fill_", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "onmt.Utils.Params", "Exception", "float", "onmt.Utils.sequence_mask", "mask.unsqueeze.unsqueeze.unsqueeze", "onmt.Utils.Params.data.masked_fill_", "onmt.Utils.Params.transpose"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.sequence_mask", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.sequence_mask"], ["", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "src_lengths", "=", "None", ",", "src_emb", "=", "None", ",", "tgt_emb", "=", "None", ")", ":", "\n", "        ", "src_final", ",", "src_memory_bank", "=", "self", ".", "src_encoder", "(", "src", ",", "src_lengths", ",", "emb", "=", "src_emb", ")", "\n", "src_length", ",", "batch_size", ",", "rnn_size", "=", "src_memory_bank", ".", "size", "(", ")", "\n", "\n", "tgt_final", ",", "tgt_memory_bank", "=", "self", ".", "tgt_encoder", "(", "tgt", ",", "emb", "=", "tgt_emb", ")", "\n", "self", ".", "q_src_h", "=", "src_memory_bank", "\n", "self", ".", "q_tgt_h", "=", "tgt_memory_bank", "\n", "\n", "src_memory_bank", "=", "src_memory_bank", ".", "transpose", "(", "0", ",", "1", ")", "# batch_size, src_length, rnn_size", "\n", "src_memory_bank", "=", "src_memory_bank", ".", "transpose", "(", "1", ",", "2", ")", "# batch_size, rnn_size, src_length", "\n", "tgt_memory_bank", "=", "self", ".", "W", "(", "tgt_memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ")", "# batch_size, tgt_length, rnn_size", "\n", "\n", "if", "self", ".", "dist_type", "==", "\"categorical\"", ":", "\n", "            ", "scores", "=", "torch", ".", "bmm", "(", "tgt_memory_bank", ",", "src_memory_bank", ")", "\n", "# mask source attention", "\n", "assert", "(", "self", ".", "mask_val", "==", "-", "float", "(", "'inf'", ")", ")", "\n", "if", "src_lengths", "is", "not", "None", ":", "\n", "                ", "mask", "=", "sequence_mask", "(", "src_lengths", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "1", "-", "mask", ",", "self", ".", "mask_val", ")", "\n", "# scoresF should be softmax", "\n", "", "log_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "scores", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Make scores : T x N x S", "\n", "scores", "=", "scores", ".", "transpose", "(", "0", ",", "1", ")", "\n", "log_scores", "=", "log_scores", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "scores", "=", "Params", "(", "\n", "alpha", "=", "scores", ",", "\n", "log_alpha", "=", "log_scores", ",", "\n", "dist_type", "=", "self", ".", "dist_type", ",", "\n", ")", "\n", "", "elif", "self", ".", "dist_type", "==", "\"none\"", ":", "\n", "            ", "scores", "=", "torch", ".", "bmm", "(", "tgt_memory_bank", ",", "src_memory_bank", ")", "\n", "# mask source attention", "\n", "if", "src_lengths", "is", "not", "None", ":", "\n", "                ", "mask", "=", "sequence_mask", "(", "src_lengths", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "1", "-", "mask", ",", "self", ".", "mask_val", ")", "\n", "", "scores", "=", "Params", "(", "\n", "alpha", "=", "scores", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "dist_type", "=", "self", ".", "dist_type", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unsupported dist_type\"", ")", "\n", "\n", "# T x N x S", "\n", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder.__init__": [[118, 140], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "onmt.Models.InputFeedRNNDecoder.__init__", "onmt.modules.VariationalAttention"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# Hack to get get subclassing working", "\n", "        ", "p_dist_type", "=", "kwargs", ".", "pop", "(", "\"p_dist_type\"", ")", "\n", "q_dist_type", "=", "kwargs", ".", "pop", "(", "\"q_dist_type\"", ")", "\n", "use_prior", "=", "kwargs", ".", "pop", "(", "\"use_prior\"", ")", "\n", "scoresF", "=", "kwargs", ".", "pop", "(", "\"scoresF\"", ")", "\n", "n_samples", "=", "kwargs", ".", "pop", "(", "\"n_samples\"", ")", "\n", "mode", "=", "kwargs", ".", "pop", "(", "\"mode\"", ")", "\n", "temperature", "=", "kwargs", ".", "pop", "(", "\"temperature\"", ")", "\n", "super", "(", "ViRNNDecoder", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "attn", "=", "onmt", ".", "modules", ".", "VariationalAttention", "(", "\n", "src_dim", "=", "self", ".", "memory_size", ",", "\n", "tgt_dim", "=", "self", ".", "hidden_size", ",", "\n", "attn_dim", "=", "self", ".", "attn_size", ",", "\n", "temperature", "=", "temperature", ",", "\n", "p_dist_type", "=", "p_dist_type", ",", "\n", "q_dist_type", "=", "q_dist_type", ",", "\n", "use_prior", "=", "use_prior", ",", "\n", "scoresF", "=", "scoresF", ",", "\n", "n_samples", "=", "n_samples", ",", "\n", "mode", "=", "mode", ",", "\n", "attn_type", "=", "kwargs", "[", "\"attn_type\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._run_forward_pass": [[142, 264], ["state.input_feed.squeeze", "state.input_feed.squeeze.size", "tgt.size", "onmt.Utils.aeq", "memory_bank.size", "enumerate", "onmt.Utils.Params", "onmt.Utils.DistInfo", "ViModels.ViRNNDecoder.dropout", "emb.dim", "emb.size", "emb.size", "state.coverage.squeeze", "emb.split", "emb_t.squeeze.squeeze.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ViModels.ViRNNDecoder.rnn", "rnn_output.squeeze.squeeze.squeeze", "ViModels.ViRNNDecoder.attn", "ViModels.ViRNNDecoder.dropout", "onmt.Utils.Params", "ViModels.ViRNNDecoder.embeddings", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "onmt.Utils.Params", "memory_bank.transpose", "ViModels.ViRNNDecoder.context_gate", "ViModels.ViRNNDecoder.dropout", "ViModels.ViRNNDecoder.copy_attn", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "memory_bank.transpose", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "_run_forward_pass", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ",", "\n", "q_scores", "=", "None", ",", "tgt_emb", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See StdRNNDecoder._run_forward_pass() for description\n        of arguments and return values.\n        \"\"\"", "\n", "# Additional args check.", "\n", "input_feed", "=", "state", ".", "input_feed", ".", "squeeze", "(", "0", ")", "\n", "input_feed_batch", ",", "_", "=", "input_feed", ".", "size", "(", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "input_feed_batch", ")", "\n", "# END Additional args check.", "\n", "\n", "# Initialize local and return variables.", "\n", "decoder_outputs", "=", "[", "]", "\n", "decoder_outputs_baseline", "=", "[", "]", "\n", "dist_infos", "=", "[", "]", "\n", "attns", "=", "{", "\"std\"", ":", "[", "]", "}", "\n", "if", "q_scores", "is", "not", "None", ":", "\n", "            ", "attns", "[", "\"q\"", "]", "=", "[", "]", "\n", "", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "[", "]", "\n", "", "if", "self", ".", "_coverage", ":", "\n", "            ", "attns", "[", "\"coverage\"", "]", "=", "[", "]", "\n", "\n", "", "emb", "=", "self", ".", "dropout", "(", "self", ".", "embeddings", "(", "tgt", ")", ")", "if", "tgt_emb", "is", "None", "else", "tgt_emb", "\n", "assert", "emb", ".", "dim", "(", ")", "==", "3", "# len x batch x embedding_dim", "\n", "\n", "tgt_len", ",", "batch_size", "=", "emb", ".", "size", "(", "0", ")", ",", "emb", ".", "size", "(", "1", ")", "\n", "src_len", "=", "memory_bank", ".", "size", "(", "0", ")", "\n", "\n", "hidden", "=", "state", ".", "hidden", "\n", "coverage", "=", "state", ".", "coverage", ".", "squeeze", "(", "0", ")", "if", "state", ".", "coverage", "is", "not", "None", "else", "None", "\n", "\n", "# Input feed concatenates hidden state with", "\n", "# input at every time step.", "\n", "for", "i", ",", "emb_t", "in", "enumerate", "(", "emb", ".", "split", "(", "1", ")", ")", ":", "\n", "            ", "emb_t", "=", "emb_t", ".", "squeeze", "(", "0", ")", "\n", "decoder_input", "=", "torch", ".", "cat", "(", "[", "emb_t", ",", "input_feed", "]", ",", "-", "1", ")", "\n", "\n", "rnn_output", ",", "hidden", "=", "self", ".", "rnn", "(", "decoder_input", ".", "unsqueeze", "(", "0", ")", ",", "hidden", ")", "\n", "rnn_output", "=", "rnn_output", ".", "squeeze", "(", "0", ")", "\n", "if", "q_scores", "is", "not", "None", ":", "\n", "# map over tensor-like keys", "\n", "                ", "q_scores_i", "=", "Params", "(", "\n", "alpha", "=", "q_scores", ".", "alpha", "[", "i", "]", ",", "\n", "log_alpha", "=", "q_scores", ".", "log_alpha", "[", "i", "]", ",", "\n", "dist_type", "=", "q_scores", ".", "dist_type", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "q_scores_i", "=", "None", "\n", "", "decoder_output_y", ",", "decoder_output_c", ",", "context_c", ",", "attn_c", ",", "dist_info", "=", "self", ".", "attn", "(", "\n", "rnn_output", ",", "\n", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "memory_lengths", "=", "memory_lengths", ",", "\n", "q_scores", "=", "q_scores_i", ")", "\n", "\n", "dist_infos", "+=", "[", "dist_info", "]", "\n", "if", "self", ".", "context_gate", "is", "not", "None", "and", "decoder_output_c", "is", "not", "None", ":", "\n", "# TODO: context gate should be employed", "\n", "# instead of second RNN transform.", "\n", "                ", "decoder_output_c", "=", "self", ".", "context_gate", "(", "\n", "decoder_input", ",", "rnn_output", ",", "decoder_output_c", "\n", ")", "\n", "", "if", "decoder_output_c", "is", "not", "None", ":", "\n", "                ", "decoder_output_c", "=", "self", ".", "dropout", "(", "decoder_output_c", ")", "\n", "", "input_feed", "=", "context_c", "\n", "\n", "# decoder_output_y : K x N x H", "\n", "decoder_output_y", "=", "self", ".", "dropout", "(", "decoder_output_y", ")", "\n", "\n", "decoder_outputs", "+=", "[", "decoder_output_y", "]", "\n", "if", "decoder_output_c", "is", "not", "None", ":", "\n", "                ", "decoder_outputs_baseline", "+=", "[", "decoder_output_c", "]", "\n", "", "attns", "[", "\"std\"", "]", "+=", "[", "attn_c", "]", "\n", "if", "q_scores", "is", "not", "None", ":", "\n", "                ", "attns", "[", "\"q\"", "]", "+=", "[", "q_scores", ".", "alpha", "[", "i", "]", "]", "\n", "\n", "# Update the coverage attention.", "\n", "", "if", "self", ".", "_coverage", ":", "\n", "                ", "coverage", "=", "coverage", "+", "p_attn", "if", "coverage", "is", "not", "None", "else", "p_attn", "\n", "attns", "[", "\"coverage\"", "]", "+=", "[", "coverage", "]", "\n", "\n", "# Run the forward pass of the copy attention layer.", "\n", "", "if", "self", ".", "_copy", "and", "not", "self", ".", "_reuse_copy_attn", ":", "\n", "                ", "_", ",", "copy_attn", "=", "self", ".", "copy_attn", "(", "decoder_output", ",", "\n", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "attns", "[", "\"copy\"", "]", "+=", "[", "copy_attn", "]", "\n", "", "elif", "self", ".", "_copy", ":", "\n", "                ", "attns", "[", "\"copy\"", "]", "=", "attns", "[", "\"std\"", "]", "\n", "\n", "", "", "q_info", "=", "Params", "(", "\n", "alpha", "=", "q_scores", ".", "alpha", ",", "\n", "dist_type", "=", "q_scores", ".", "dist_type", ",", "\n", "samples", "=", "torch", ".", "stack", "(", "[", "d", ".", "q", ".", "samples", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "q", ".", "samples", "is", "not", "None", "else", "None", ",", "\n", "log_alpha", "=", "q_scores", ".", "log_alpha", ",", "\n", "sample_log_probs", "=", "torch", ".", "stack", "(", "[", "d", ".", "q", ".", "sample_log_probs", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "q", ".", "sample_log_probs", "is", "not", "None", "else", "None", ",", "\n", "sample_log_probs_q", "=", "torch", ".", "stack", "(", "[", "d", ".", "q", ".", "sample_log_probs_q", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "q", ".", "sample_log_probs_q", "is", "not", "None", "else", "None", ",", "\n", "sample_log_probs_p", "=", "torch", ".", "stack", "(", "[", "d", ".", "q", ".", "sample_log_probs_p", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "q", ".", "sample_log_probs_p", "is", "not", "None", "else", "None", ",", "\n", "sample_p_div_q_log", "=", "torch", ".", "stack", "(", "[", "d", ".", "q", ".", "sample_p_div_q_log", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "q", ".", "sample_p_div_q_log", "is", "not", "None", "else", "None", ",", "\n", ")", "if", "q_scores", "is", "not", "None", "else", "None", "\n", "p_info", "=", "Params", "(", "\n", "alpha", "=", "torch", ".", "stack", "(", "[", "d", ".", "p", ".", "alpha", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", ",", "\n", "dist_type", "=", "dist_infos", "[", "0", "]", ".", "p", ".", "dist_type", ",", "\n", "log_alpha", "=", "torch", ".", "stack", "(", "[", "d", ".", "p", ".", "log_alpha", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "p", ".", "log_alpha", "is", "not", "None", "else", "None", ",", "\n", "samples", "=", "torch", ".", "stack", "(", "[", "d", ".", "p", ".", "samples", "for", "d", "in", "dist_infos", "]", ",", "dim", "=", "0", ")", "\n", "if", "dist_infos", "[", "0", "]", ".", "p", ".", "samples", "is", "not", "None", "else", "None", ",", "\n", ")", "\n", "dist_info", "=", "DistInfo", "(", "\n", "q", "=", "q_info", ",", "\n", "p", "=", "p_info", ",", "\n", ")", "\n", "\n", "return", "hidden", ",", "decoder_outputs", ",", "input_feed", ",", "attns", ",", "dist_info", ",", "decoder_outputs_baseline", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder.forward": [[265, 296], ["isinstance", "tgt.size", "memory_bank.size", "onmt.Utils.aeq", "ViModels.ViRNNDecoder._run_forward_pass", "state.update_state", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "[].unsqueeze", "input_feed.unsqueeze", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._run_forward_pass", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.update_state"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ",", "q_scores", "=", "None", ",", "tgt_emb", "=", "None", ")", ":", "\n", "# Check", "\n", "        ", "assert", "isinstance", "(", "state", ",", "RNNDecoderState", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "_", ",", "memory_batch", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "memory_batch", ")", "\n", "# END", "\n", "\n", "# Run the forward pass of the RNN.", "\n", "decoder_final", ",", "decoder_outputs", ",", "input_feed", ",", "attns", ",", "dist_info", ",", "decoder_outputs_baseline", "=", "self", ".", "_run_forward_pass", "(", "\n", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "memory_lengths", ",", "\n", "q_scores", "=", "q_scores", ",", "tgt_emb", "=", "tgt_emb", ")", "\n", "\n", "# Update the state with the result.", "\n", "final_output", "=", "decoder_outputs", "[", "-", "1", "]", "\n", "coverage", "=", "None", "\n", "if", "\"coverage\"", "in", "attns", ":", "\n", "            ", "coverage", "=", "attns", "[", "\"coverage\"", "]", "[", "-", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "", "state", ".", "update_state", "(", "decoder_final", ",", "input_feed", ".", "unsqueeze", "(", "0", ")", ",", "coverage", ")", "\n", "\n", "# Concatenates sequence of tensors along a new dimension.", "\n", "# T x K x N x H", "\n", "decoder_outputs", "=", "torch", ".", "stack", "(", "decoder_outputs", ",", "dim", "=", "0", ")", "\n", "if", "len", "(", "decoder_outputs_baseline", ")", ">", "0", ":", "\n", "            ", "decoder_outputs_baseline", "=", "torch", ".", "stack", "(", "decoder_outputs_baseline", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "decoder_outputs_baseline", "=", "None", "\n", "", "for", "k", "in", "attns", ":", "\n", "            ", "attns", "[", "k", "]", "=", "torch", ".", "stack", "(", "attns", "[", "k", "]", ")", "\n", "\n", "", "return", "decoder_outputs", ",", "state", ",", "attns", ",", "dist_info", ",", "decoder_outputs_baseline", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._build_rnn": [[297, 308], ["torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["None"], ["", "def", "_build_rnn", "(", "self", ",", "rnn_type", ",", "input_size", ",", "\n", "hidden_size", ",", "num_layers", ",", "dropout", ")", ":", "\n", "        ", "assert", "not", "rnn_type", "==", "\"SRU\"", ",", "\"SRU doesn't support input feed! \"", "\"Please set -input_feed 0!\"", "\n", "if", "rnn_type", "==", "\"LSTM\"", ":", "\n", "            ", "stacked_cell", "=", "onmt", ".", "modules", ".", "StackedLSTM", "\n", "", "else", ":", "\n", "            ", "stacked_cell", "=", "onmt", ".", "modules", ".", "StackedGRU", "\n", "", "return", "nn", ".", "LSTM", "(", "\n", "num_layers", "=", "num_layers", ",", "input_size", "=", "input_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViRNNDecoder._input_size": [[309, 315], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_input_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Using input feed by concatenating input with attention vectors.\n        \"\"\"", "\n", "return", "self", ".", "embeddings", ".", "embedding_size", "+", "self", ".", "memory_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.__init__": [[327, 342], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "\n", "self", ",", "encoder", ",", "decoder", ",", "inference_network", ",", "\n", "multigpu", "=", "False", ",", "dist_type", "=", "\"categorical\"", ",", "dbg", "=", "False", ",", "use_prior", "=", "False", ",", "\n", "n_samples", "=", "1", ",", "k", "=", "0", ")", ":", "\n", "        ", "self", ".", "multigpu", "=", "multigpu", "\n", "super", "(", "ViNMTModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "inference_network", "=", "inference_network", "\n", "self", ".", "dist_type", "=", "dist_type", "\n", "self", ".", "dbg", "=", "dbg", "\n", "self", ".", "_use_prior", "=", "use_prior", "\n", "self", ".", "n_samples", "=", "n_samples", "\n", "self", ".", "silent", "=", "False", "\n", "self", ".", "k", "=", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.use_prior": [[347, 351], ["None"], "methods", ["None"], ["", "@", "use_prior", ".", "setter", "\n", "def", "use_prior", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_use_prior", "=", "value", "\n", "self", ".", "decoder", ".", "attn", ".", "use_prior", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.n_samples": [[356, 360], ["None"], "methods", ["None"], ["", "@", "n_samples", ".", "setter", "\n", "def", "n_samples", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_n_samples", "=", "value", "\n", "self", ".", "decoder", ".", "attn", ".", "n_samples", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.k": [[365, 369], ["None"], "methods", ["None"], ["", "@", "k", ".", "setter", "\n", "def", "k", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_k", "=", "value", "\n", "self", ".", "decoder", ".", "attn", ".", "k", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.mode": [[374, 380], ["print"], "methods", ["None"], ["", "@", "mode", ".", "setter", "\n", "def", "mode", "(", "self", ",", "value", ")", ":", "\n", "        ", "assert", "value", "in", "[", "\"sample\"", ",", "\"enum\"", ",", "\"exact\"", ",", "\"wsram\"", ",", "\"gumbel\"", "]", "\n", "if", "not", "self", ".", "silent", ":", "\n", "            ", "print", "(", "\"switching mode to {}\"", ".", "format", "(", "value", ")", ")", "\n", "", "self", ".", "decoder", ".", "attn", ".", "mode", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ViModels.ViNMTModel.forward": [[381, 441], ["ViModels.ViNMTModel.encoder.dropout", "ViModels.ViNMTModel.decoder.dropout", "tgt.size", "ViModels.ViNMTModel.encoder", "ViModels.ViNMTModel.decoder.init_decoder_state", "ViModels.ViNMTModel.decoder", "ViModels.ViNMTModel.encoder.embeddings", "ViModels.ViNMTModel.decoder.embeddings", "ViModels.ViNMTModel.inference_network"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.init_decoder_state"], ["", "def", "forward", "(", "self", ",", "src", ",", "tgt", ",", "lengths", ",", "dec_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Forward propagate a `src` and `tgt` pair for training.\n        Possible initialized with a beginning decoder state.\n\n        Args:\n            src (:obj:`Tensor`):\n                a source sequence passed to encoder.\n                typically for inputs this will be a padded :obj:`LongTensor`\n                of size `[len x batch x features]`. however, may be an\n                image or other generic input depending on encoder.\n            tgt (:obj:`LongTensor`):\n                 a target sequence of size `[tgt_len x batch]`.\n            lengths(:obj:`LongTensor`): the src lengths, pre-padding `[batch]`.\n            dec_state (:obj:`DecoderState`, optional): initial decoder state\n        Returns:\n            (:obj:`FloatTensor`, `dict`, :obj:`onmt.Models.DecoderState`):\n\n                 * decoder output `[tgt_len x batch x hidden]`\n                 * dictionary attention dists of `[tgt_len x batch x src_len]`\n                 * final decoder state\n        \"\"\"", "\n", "\n", "src_emb", "=", "self", ".", "encoder", ".", "dropout", "(", "self", ".", "encoder", ".", "embeddings", "(", "src", ")", ")", "\n", "tgt_emb", "=", "self", ".", "decoder", ".", "dropout", "(", "self", ".", "decoder", ".", "embeddings", "(", "tgt", ")", ")", "\n", "if", "self", ".", "dbg", ":", "\n", "# only see past", "\n", "            ", "inftgt", "=", "tgt", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "# see present", "\n", "            ", "inftgt", "=", "tgt", "[", "1", ":", "]", "\n", "inftgt_emb", "=", "tgt_emb", "[", "1", ":", "]", "\n", "", "tgt", "=", "tgt", "[", ":", "-", "1", "]", "# exclude last target from inputs", "\n", "tgt_emb", "=", "tgt_emb", "[", ":", "-", "1", "]", "# exclude last target from inputs", "\n", "tgt_length", ",", "batch_size", ",", "rnn_size", "=", "tgt", ".", "size", "(", ")", "\n", "\n", "enc_final", ",", "memory_bank", "=", "self", ".", "encoder", "(", "src", ",", "lengths", ",", "emb", "=", "src_emb", ")", "\n", "enc_state", "=", "self", ".", "decoder", ".", "init_decoder_state", "(", "\n", "src", ",", "memory_bank", ",", "enc_final", ")", "\n", "# enc_state.* should all be 0", "\n", "\n", "if", "self", ".", "inference_network", "is", "not", "None", "and", "not", "self", ".", "use_prior", ":", "\n", "# inference network q(z|x,y)", "\n", "            ", "q_scores", "=", "self", ".", "inference_network", "(", "\n", "src", ",", "inftgt", ",", "lengths", ",", "src_emb", "=", "src_emb", ",", "tgt_emb", "=", "inftgt_emb", ")", "# batch_size, tgt_length, src_length", "\n", "", "else", ":", "\n", "            ", "q_scores", "=", "None", "\n", "", "decoder_outputs", ",", "dec_state", ",", "attns", ",", "dist_info", ",", "decoder_outputs_baseline", "=", "self", ".", "decoder", "(", "tgt", ",", "memory_bank", ",", "\n", "enc_state", "if", "dec_state", "is", "None", "\n", "else", "dec_state", ",", "\n", "memory_lengths", "=", "lengths", ",", "\n", "q_scores", "=", "q_scores", ",", "\n", "tgt_emb", "=", "tgt_emb", ")", "\n", "\n", "if", "self", ".", "multigpu", ":", "\n", "# Not yet supported on multi-gpu", "\n", "            ", "dec_state", "=", "None", "\n", "attns", "=", "None", "\n", "\n", "", "return", "decoder_outputs", ",", "attns", ",", "dec_state", ",", "dist_info", ",", "decoder_outputs_baseline", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase.__init__": [[38, 43], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "generator", ",", "tgt_vocab", ")", ":", "\n", "        ", "super", "(", "LossComputeBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "generator", "=", "generator", "\n", "self", ".", "tgt_vocab", "=", "tgt_vocab", "\n", "self", ".", "padding_idx", "=", "tgt_vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._make_shard_state": [[44, 57], ["None"], "methods", ["None"], ["", "def", "_make_shard_state", "(", "self", ",", "batch", ",", "output", ",", "range_", ",", "attns", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Make shard state dictionary for shards() to return iterable\n        shards for efficient loss computation. Subclass must define\n        this method to match its own _compute_loss() interface.\n        Args:\n            batch: the current batch.\n            output: the predict output from the model.\n            range_: the range of examples for computing, the whole\n                    batch or a trunc of it?\n            attns: the attns dictionary returned from the model.\n        \"\"\"", "\n", "return", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._compute_loss": [[58, 70], ["None"], "methods", ["None"], ["", "def", "_compute_loss", "(", "self", ",", "batch", ",", "output", ",", "target", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Compute the loss. Subclass must define this method.\n\n        Args:\n\n            batch: the current batch.\n            output: the predict output from the model.\n            target: the validate target to compare output with.\n            **kwargs(optional): additional info for computing loss.\n        \"\"\"", "\n", "return", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase.monolithic_compute_loss": [[71, 92], ["Loss.LossComputeBase._make_shard_state", "Loss.LossComputeBase._compute_loss", "batch.tgt.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._make_shard_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._compute_loss"], ["", "def", "monolithic_compute_loss", "(", "self", ",", "batch", ",", "output", ",", "attns", ",", "dist_info", "=", "None", ",", "output_baseline", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Compute the forward loss for the batch.\n\n        Args:\n          batch (batch): batch of labeled examples\n          output (:obj:`FloatTensor`):\n              output of decoder model `[tgt_len x batch x hidden]`\n          attns (dict of :obj:`FloatTensor`) :\n              dictionary of attention distributions\n              `[tgt_len x batch x src_len]`\n        Returns:\n            :obj:`onmt.Statistics`: loss statistics\n        \"\"\"", "\n", "if", "dist_info", "is", "not", "None", ":", "\n", "            ", "self", ".", "dist_type", "=", "dist_info", ".", "p", ".", "dist_type", "\n", "", "range_", "=", "(", "0", ",", "batch", ".", "tgt", ".", "size", "(", "0", ")", ")", "\n", "shard_state", "=", "self", ".", "_make_shard_state", "(", "batch", ",", "output", ",", "range_", ",", "attns", ",", "dist_info", "=", "dist_info", ",", "output_baseline", "=", "output_baseline", ")", "\n", "_", ",", "batch_stats", "=", "self", ".", "_compute_loss", "(", "batch", ",", "**", "shard_state", ")", "\n", "\n", "return", "batch_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase.sharded_compute_loss": [[93, 136], ["onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "Loss.LossComputeBase._make_shard_state", "Loss.shards", "Loss.LossComputeBase._compute_loss", "loss.div().backward", "onmt.Statistics.update", "onmt.Statistics.update", "loss.div"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._make_shard_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.shards", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._compute_loss", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.backward", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "sharded_compute_loss", "(", "self", ",", "batch", ",", "output", ",", "attns", ",", "\n", "cur_trunc", ",", "trunc_size", ",", "shard_size", ",", "\n", "normalization", ",", "dist_info", "=", "None", ",", "\n", "output_baseline", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute the forward loss and backpropagate.  Computation is done\n        with shards and optionally truncation for memory efficiency.\n\n        Also supports truncated BPTT for long sequences by taking a\n        range in the decoder output sequence to back propagate in.\n        Range is from `(cur_trunc, cur_trunc + trunc_size)`.\n\n        Note sharding is an exact efficiency trick to relieve memory\n        required for the generation buffers. Truncation is an\n        approximate efficiency trick to relieve the memory required\n        in the RNN buffers.\n\n        Args:\n          batch (batch) : batch of labeled examples\n          output (:obj:`FloatTensor`) :\n              output of decoder model `[tgt_len x batch x hidden]`\n          attns (dict) : dictionary of attention distributions\n              `[tgt_len x batch x src_len]`\n          cur_trunc (int) : starting position of truncation window\n          trunc_size (int) : length of truncation window\n          shard_size (int) : maximum number of examples in a shard\n          normalization (int) : Loss is divided by this number\n\n        Returns:\n            :obj:`onmt.Statistics`: validation loss statistics\n\n        \"\"\"", "\n", "batch_stats", "=", "onmt", ".", "Statistics", "(", ")", "\n", "range_", "=", "(", "cur_trunc", ",", "cur_trunc", "+", "trunc_size", ")", "\n", "shard_state", "=", "self", ".", "_make_shard_state", "(", "batch", ",", "output", ",", "range_", ",", "attns", ",", "dist_info", "=", "dist_info", ",", "output_baseline", "=", "output_baseline", ")", "\n", "if", "dist_info", "is", "not", "None", ":", "\n", "            ", "self", ".", "dist_type", "=", "dist_info", ".", "p", ".", "dist_type", "\n", "\n", "", "for", "shard", "in", "shards", "(", "shard_state", ",", "shard_size", ")", ":", "\n", "            ", "loss", ",", "stats", "=", "self", ".", "_compute_loss", "(", "batch", ",", "**", "shard", ")", "\n", "loss", ".", "div", "(", "normalization", ")", ".", "backward", "(", ")", "\n", "batch_stats", ".", "update", "(", "stats", ")", "\n", "\n", "", "return", "batch_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._stats": [[137, 153], ["target.ne", "pred.eq().masked_select().sum", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "onmt.Statistics", "scores.max", "xent.item", "kl.item", "target.ne.sum().item", "pred.eq().masked_select().sum.item", "pred.eq().masked_select", "target.ne.sum", "pred.eq"], "methods", ["None"], ["", "def", "_stats", "(", "self", ",", "xent", ",", "kl", ",", "scores", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            loss (:obj:`FloatTensor`): the loss computed by the loss criterion.\n            scores (:obj:`FloatTensor`): a score for each possible output\n            target (:obj:`FloatTensor`): true targets\n\n        Returns:\n            :obj:`Statistics` : statistics for this batch.\n        \"\"\"", "\n", "pred", "=", "scores", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "non_padding", "=", "target", ".", "ne", "(", "self", ".", "padding_idx", ")", "\n", "num_correct", "=", "pred", ".", "eq", "(", "target", ")", ".", "masked_select", "(", "non_padding", ")", ".", "sum", "(", ")", "\n", "return", "onmt", ".", "Statistics", "(", "xent", ".", "item", "(", ")", ",", "kl", ".", "item", "(", ")", ",", "non_padding", ".", "sum", "(", ")", ".", "item", "(", ")", ",", "num_correct", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._bottle": [[154, 156], ["v.view", "v.size"], "methods", ["None"], ["", "def", "_bottle", "(", "self", ",", "v", ")", ":", "\n", "        ", "return", "v", ".", "view", "(", "-", "1", ",", "v", ".", "size", "(", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._unbottle": [[157, 159], ["v.view", "v.size"], "methods", ["None"], ["", "def", "_unbottle", "(", "self", ",", "v", ",", "batch_size", ")", ":", "\n", "        ", "return", "v", ".", "view", "(", "-", "1", ",", "batch_size", ",", "v", ".", "size", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.NMTLossCompute.__init__": [[165, 188], ["Loss.LossComputeBase.__init__", "torch.KLDivLoss", "torch.KLDivLoss", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn.fill_", "torch.randn.fill_", "Loss.NMTLossCompute.register_buffer", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.NLLLoss", "torch.NLLLoss", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "generator", ",", "tgt_vocab", ",", "normalization", "=", "\"sents\"", ",", "\n", "label_smoothing", "=", "0.0", ",", "train_baseline", "=", "False", ")", ":", "\n", "        ", "super", "(", "NMTLossCompute", ",", "self", ")", ".", "__init__", "(", "generator", ",", "tgt_vocab", ")", "\n", "assert", "(", "label_smoothing", ">=", "0.0", "and", "label_smoothing", "<=", "1.0", ")", "\n", "if", "label_smoothing", ">", "0", ":", "\n", "# When label smoothing is turned on,", "\n", "# KL-divergence between q_{smoothed ground truth prob.}(w)", "\n", "# and p_{prob. computed by model}(w) is minimized.", "\n", "# If label smoothing value is set to zero, the loss", "\n", "# is equivalent to NLLLoss or CrossEntropyLoss.", "\n", "# All non-true labels are uniformly set to low-confidence.", "\n", "            ", "self", ".", "criterion", "=", "nn", ".", "KLDivLoss", "(", "size_average", "=", "False", ")", "\n", "one_hot", "=", "torch", ".", "randn", "(", "1", ",", "len", "(", "tgt_vocab", ")", ")", "\n", "one_hot", ".", "fill_", "(", "label_smoothing", "/", "(", "len", "(", "tgt_vocab", ")", "-", "2", ")", ")", "\n", "one_hot", "[", "0", "]", "[", "self", ".", "padding_idx", "]", "=", "0", "\n", "self", ".", "register_buffer", "(", "'one_hot'", ",", "one_hot", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "torch", ".", "ones", "(", "len", "(", "tgt_vocab", ")", ")", "\n", "weight", "[", "self", ".", "padding_idx", "]", "=", "0", "\n", "self", ".", "criterion", "=", "nn", ".", "NLLLoss", "(", "weight", ",", "size_average", "=", "False", ")", "\n", "", "self", ".", "confidence", "=", "1.0", "-", "label_smoothing", "\n", "self", ".", "alpha", "=", "1", "\n", "self", ".", "train_baseline", "=", "train_baseline", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.NMTLossCompute._make_shard_state": [[189, 221], ["Exception", "Exception"], "methods", ["None"], ["", "def", "_make_shard_state", "(", "self", ",", "batch", ",", "output", ",", "range_", ",", "attns", "=", "None", ",", "\n", "dist_info", "=", "None", ",", "output_baseline", "=", "None", ")", ":", "\n", "        ", "state", "=", "{", "\n", "\"output\"", ":", "output", ",", "\n", "\"target\"", ":", "batch", ".", "tgt", "[", "range_", "[", "0", "]", "+", "1", ":", "range_", "[", "1", "]", "]", ",", "\n", "}", "\n", "\n", "if", "dist_info", "is", "not", "None", ":", "\n", "            ", "if", "dist_info", ".", "p", "is", "not", "None", ":", "\n", "                ", "state", "[", "\"p_samples\"", "]", "=", "dist_info", ".", "p", ".", "samples", "\n", "if", "dist_info", ".", "p", ".", "dist_type", "==", "\"categorical\"", ":", "\n", "                    ", "state", "[", "\"p_alpha\"", "]", "=", "dist_info", ".", "p", ".", "alpha", "\n", "state", "[", "\"p_log_alpha\"", "]", "=", "dist_info", ".", "p", ".", "log_alpha", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Unimplemented distribution\"", ")", "\n", "", "", "if", "dist_info", ".", "q", "is", "not", "None", ":", "\n", "                ", "state", "[", "\"q_samples\"", "]", "=", "dist_info", ".", "q", ".", "samples", "\n", "if", "dist_info", ".", "q", ".", "dist_type", "==", "\"categorical\"", ":", "\n", "                    ", "state", "[", "\"q_alpha\"", "]", "=", "dist_info", ".", "q", ".", "alpha", "\n", "state", "[", "\"q_log_alpha\"", "]", "=", "dist_info", ".", "q", ".", "log_alpha", "\n", "if", "self", ".", "generator", ".", "mode", "!=", "'wsram'", ":", "\n", "                        ", "state", "[", "\"q_sample_log_probs\"", "]", "=", "dist_info", ".", "q", ".", "sample_log_probs", "\n", "", "else", ":", "\n", "                        ", "state", "[", "\"sample_log_probs_q\"", "]", "=", "dist_info", ".", "q", ".", "sample_log_probs_q", "\n", "state", "[", "\"sample_log_probs_p\"", "]", "=", "dist_info", ".", "q", ".", "sample_log_probs_p", "\n", "state", "[", "\"sample_p_div_q_log\"", "]", "=", "dist_info", ".", "q", ".", "sample_p_div_q_log", "\n", "", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Unimplemented distribution\"", ")", "\n", "\n", "", "", "", "if", "output_baseline", "is", "not", "None", ":", "\n", "            ", "state", "[", "\"output_baseline\"", "]", "=", "output_baseline", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.NMTLossCompute._compute_loss": [[222, 357], ["Loss.NMTLossCompute.generator", "scores.view.view.view", "target.view", "Loss.NMTLossCompute.criterion", "torch.zeros().to.data.clone", "torch.zeros().to.data.clone", "Loss.NMTLossCompute._stats", "log_p_y.size", "log_p_y.gather().squeeze", "target.unsqueeze().expand().contiguous().view", "[].sum", "[].sum", "[].sum", "target.view", "q_alpha.contiguous().view.contiguous().view.contiguous().view", "p_alpha.contiguous().view.contiguous().view.contiguous().view", "torch.distributions.kl.kl_divergence().sum", "torch.distributions.kl.kl_divergence().sum", "scores_first.contiguous().view.contiguous().view.contiguous().view", "Loss.NMTLossCompute.criterion", "Loss.NMTLossCompute._stats", "scores.view.view.size", "output_baseline.unsqueeze.unsqueeze.unsqueeze", "Loss.NMTLossCompute.generator", "scores_baseline.view.view.view", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "Loss.NMTLossCompute.one_hot.repeat", "Loss.NMTLossCompute.scatter_", "torch.autograd.Variable", "torch.autograd.Variable", "Loss.NMTLossCompute.criterion", "scores_nopad.gather", "scores_baseline_nopad.gather", "q_sample_log_probs.view.view.view", "Loss.NMTLossCompute.data.clone", "Loss.NMTLossCompute.data.clone", "q_alpha.contiguous().view.contiguous().view.contiguous().view", "p_alpha.contiguous().view.contiguous().view.contiguous().view", "torch.distributions.kl.kl_divergence().sum", "torch.distributions.kl.kl_divergence().sum", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "w_unnormalized.sum", "q_alpha.contiguous().view.contiguous().view.size", "p_alpha.contiguous().view.contiguous().view.size", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "scores_first.contiguous().view.contiguous().view.size", "scores.view.view.size", "tdata.unsqueeze", "torch.autograd.Variable.size", "torch.autograd.Variable.size", "tdata.unsqueeze", "torch.nonzero().squeeze.dim", "torch.nonzero().squeeze.dim", "torch.gather.index_fill_", "torch.gather.index_fill_", "Loss.NMTLossCompute.index_fill_", "gtruth_nopad.unsqueeze", "gtruth_nopad.unsqueeze", "q_alpha.contiguous().view.contiguous().view.size", "p_alpha.contiguous().view.contiguous().view.size", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "target.view", "log_p_y.gather", "target.unsqueeze().expand().contiguous", "w_normalized.detach", "q_alpha.contiguous().view.contiguous().view.contiguous", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "p_alpha.contiguous().view.contiguous().view.contiguous", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence", "scores_first.contiguous().view.contiguous().view.contiguous", "target.view", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "q_alpha.contiguous().view.contiguous().view.contiguous", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "p_alpha.contiguous().view.contiguous().view.contiguous", "torch.autograd.Variable.ne", "torch.autograd.Variable.ne", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "target.unsqueeze().unsqueeze().expand", "[].sum.view", "[].sum.view", "[].sum.view", "tdata.eq", "scores_nopad.gather.detach", "scores_baseline_nopad.gather.detach", "target.unsqueeze().expand", "target.unsqueeze().expand().contiguous().view.ne", "target.unsqueeze().expand().contiguous().view.ne", "target.unsqueeze().expand().contiguous().view.ne", "target.unsqueeze().unsqueeze", "target.unsqueeze", "target.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._stats", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._stats", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.detach", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.detach", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.DecoderState.detach"], ["", "def", "_compute_loss", "(", "\n", "self", ",", "batch", ",", "output", ",", "target", ",", "\n", "p_samples", "=", "None", ",", "q_samples", "=", "None", ",", "\n", "p_alpha", "=", "None", ",", "q_alpha", "=", "None", ",", "\n", "q_log_alpha", "=", "None", ",", "\n", "q_sample_log_probs", "=", "None", ",", "\n", "p_log_alpha", "=", "None", ",", "\n", "output_baseline", "=", "None", ",", "\n", "sample_log_probs_q", "=", "None", ",", "\n", "sample_log_probs_p", "=", "None", ",", "\n", "sample_p_div_q_log", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "self", ".", "generator", ".", "mode", "in", "[", "\"enum\"", ",", "\"exact\"", ",", "\"wsram\"", ",", "\"gumbel\"", "]", ":", "\n", "            ", "output_baseline", "=", "None", "\n", "\n", "# Reconstruction", "\n", "# TODO(jchiu): hacky, want to set use_prior.", "\n", "", "scores", "=", "self", ".", "generator", "(", "\n", "output", ",", "\n", "log_pa", "=", "q_log_alpha", "if", "q_log_alpha", "is", "not", "None", "else", "p_log_alpha", ",", "\n", "pa", "=", "q_alpha", "if", "q_alpha", "is", "not", "None", "else", "p_alpha", ",", "\n", ")", "\n", "if", "self", ".", "generator", ".", "mode", "==", "'wsram'", ":", "\n", "            ", "log_p_y", "=", "scores", "# T, K, batch, S", "\n", "T", ",", "K", ",", "B", ",", "_", "=", "log_p_y", ".", "size", "(", ")", "\n", "#p_y = log_p_y.exp()", "\n", "log_p_y_sample", "=", "log_p_y", ".", "gather", "(", "3", ",", "target", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", ".", "expand", "(", "T", ",", "K", ",", "B", ",", "1", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "w_unnormalized", "=", "(", "sample_p_div_q_log", "+", "log_p_y_sample", ")", ".", "exp", "(", ")", "#T, K, B", "\n", "w_normalized", "=", "w_unnormalized", "/", "w_unnormalized", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "#bp = sample_p_div_q_log.exp()", "\n", "#bp = bp / bp.sum(dim=1, keepdim=True)", "\n", "#bq = 1. / K", "\n", "bp", "=", "0", "\n", "bq", "=", "0", "\n", "target_expand", "=", "target", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "T", ",", "K", ",", "B", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "# loss 1: w * log p (y)", "\n", "loss1", "=", "-", "w_normalized", ".", "detach", "(", ")", "*", "log_p_y_sample", "\n", "loss1", "=", "loss1", ".", "view", "(", "-", "1", ")", "[", "target_expand", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", ".", "sum", "(", ")", "\n", "# loss 2: (w - bp) * log p(a)", "\n", "loss2", "=", "-", "(", "w_normalized", "-", "bp", ")", ".", "detach", "(", ")", "*", "sample_log_probs_p", "\n", "loss2", "=", "loss2", ".", "view", "(", "-", "1", ")", "[", "target_expand", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", ".", "sum", "(", ")", "\n", "# loss 3: (w - bq) log q a", "\n", "loss3", "=", "-", "(", "w_normalized", "-", "bq", ")", ".", "detach", "(", ")", "*", "sample_log_probs_q", "\n", "loss3", "=", "loss3", ".", "view", "(", "-", "1", ")", "[", "target_expand", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", ".", "sum", "(", ")", "\n", "loss", "=", "loss1", "+", "loss2", "+", "loss3", "\n", "\n", "gtruth", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "q_alpha", "=", "q_alpha", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "q_alpha", ".", "size", "(", "2", ")", ")", "\n", "q_alpha", "=", "q_alpha", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "p_alpha", "=", "p_alpha", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "p_alpha", ".", "size", "(", "2", ")", ")", "\n", "p_alpha", "=", "p_alpha", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "if", "self", ".", "dist_type", "==", "'categorical'", ":", "\n", "                ", "q", "=", "Cat", "(", "q_alpha", ")", "\n", "p", "=", "Cat", "(", "p_alpha", ")", "\n", "", "else", ":", "\n", "                ", "assert", "(", "False", ")", "\n", "", "kl", "=", "kl_divergence", "(", "q", ",", "p", ")", ".", "sum", "(", ")", "\n", "kl_data", "=", "kl", ".", "data", "\n", "\n", "scores_first", "=", "log_p_y", "[", ":", ",", "0", ",", ":", ",", ":", "]", "\n", "scores_first", "=", "scores_first", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "scores_first", ".", "size", "(", "-", "1", ")", ")", "\n", "xent", "=", "self", ".", "criterion", "(", "scores_first", ",", "gtruth", ")", "\n", "xent_data", "=", "xent", ".", "data", "\n", "\n", "stats", "=", "self", ".", "_stats", "(", "xent_data", ",", "kl_data", ",", "scores_first", ".", "data", ",", "target", ".", "view", "(", "-", "1", ")", ".", "data", ")", "\n", "return", "loss", ",", "stats", "\n", "\n", "", "scores", "=", "scores", ".", "view", "(", "-", "1", ",", "scores", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "output_baseline", "is", "not", "None", ":", "\n", "            ", "output_baseline", "=", "output_baseline", ".", "unsqueeze", "(", "1", ")", "\n", "scores_baseline", "=", "self", ".", "generator", "(", "output_baseline", ")", "\n", "scores_baseline", "=", "scores_baseline", ".", "view", "(", "-", "1", ",", "scores", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "", "gtruth", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "if", "self", ".", "confidence", "<", "1", ":", "\n", "            ", "tdata", "=", "gtruth", ".", "data", "\n", "mask", "=", "torch", ".", "nonzero", "(", "tdata", ".", "eq", "(", "self", ".", "padding_idx", ")", ")", ".", "squeeze", "(", ")", "\n", "log_likelihood", "=", "torch", ".", "gather", "(", "scores", ".", "data", ",", "1", ",", "tdata", ".", "unsqueeze", "(", "1", ")", ")", "\n", "tmp_", "=", "self", ".", "one_hot", ".", "repeat", "(", "gtruth", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "tmp_", ".", "scatter_", "(", "1", ",", "tdata", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "\n", "if", "mask", ".", "dim", "(", ")", ">", "0", ":", "\n", "                ", "log_likelihood", ".", "index_fill_", "(", "0", ",", "mask", ",", "0", ")", "\n", "tmp_", ".", "index_fill_", "(", "0", ",", "mask", ",", "0", ")", "\n", "", "gtruth", "=", "Variable", "(", "tmp_", ",", "requires_grad", "=", "False", ")", "\n", "\n", "", "xent", "=", "self", ".", "criterion", "(", "scores", ",", "gtruth", ")", "\n", "if", "output_baseline", "is", "not", "None", ":", "\n", "            ", "xent_baseline", "=", "self", ".", "criterion", "(", "scores_baseline", ",", "gtruth", ")", "\n", "\n", "", "if", "q_sample_log_probs", "is", "not", "None", "and", "output_baseline", "is", "not", "None", ":", "\n", "# This code doesn't handle multiple samples", "\n", "            ", "scores_nopad", "=", "scores", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "scores_baseline_nopad", "=", "scores_baseline", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "gtruth_nopad", "=", "gtruth", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "llh_ind", "=", "scores_nopad", ".", "gather", "(", "1", ",", "gtruth_nopad", ".", "unsqueeze", "(", "1", ")", ")", "\n", "llh_baseline_ind", "=", "scores_baseline_nopad", ".", "gather", "(", "1", ",", "gtruth_nopad", ".", "unsqueeze", "(", "1", ")", ")", "\n", "reward", "=", "(", "llh_ind", ".", "detach", "(", ")", "-", "llh_baseline_ind", ".", "detach", "(", ")", ")", ".", "view", "(", "-", "1", ")", "# T*N", "\n", "q_sample_log_probs", "=", "q_sample_log_probs", ".", "view", "(", "-", "1", ")", "# T, N", "\n", "q_sample_log_probs", "=", "q_sample_log_probs", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "", "if", "self", ".", "confidence", "<", "1", ":", "\n", "# Default: report smoothed ppl.", "\n", "# loss_data = -log_likelihood.sum(0)", "\n", "            ", "xent_data", "=", "xent", ".", "data", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "            ", "xent_data", "=", "xent", ".", "data", ".", "clone", "(", ")", "\n", "\n", "# KL", "\n", "", "if", "q_alpha", "is", "not", "None", ":", "\n", "            ", "q_alpha", "=", "q_alpha", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "q_alpha", ".", "size", "(", "2", ")", ")", "\n", "q_alpha", "=", "q_alpha", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "p_alpha", "=", "p_alpha", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "p_alpha", ".", "size", "(", "2", ")", ")", "\n", "p_alpha", "=", "p_alpha", "[", "gtruth", ".", "ne", "(", "self", ".", "padding_idx", ")", "]", "\n", "if", "self", ".", "dist_type", "==", "'categorical'", ":", "\n", "                ", "q", "=", "Cat", "(", "q_alpha", ")", "\n", "p", "=", "Cat", "(", "p_alpha", ")", "\n", "", "else", ":", "\n", "                ", "assert", "(", "False", ")", "\n", "", "kl", "=", "kl_divergence", "(", "q", ",", "p", ")", ".", "sum", "(", ")", "\n", "loss", "=", "xent", "+", "self", ".", "alpha", "*", "kl", "\n", "", "else", ":", "\n", "            ", "kl", "=", "torch", ".", "zeros", "(", "1", ")", ".", "to", "(", "xent", ")", "\n", "loss", "=", "xent", "\n", "\n", "# subtract reward", "\n", "", "if", "self", ".", "generator", ".", "mode", "==", "'gumbel'", ":", "\n", "            ", "assert", "q_sample_log_probs", "is", "None", "\n", "", "if", "q_sample_log_probs", "is", "not", "None", ":", "\n", "            ", "loss", "=", "loss", "-", "(", "reward", "*", "q_sample_log_probs", ")", ".", "sum", "(", ")", "\n", "if", "self", ".", "train_baseline", ":", "\n", "                ", "loss", "=", "loss", "+", "xent_baseline", "\n", "\n", "", "", "kl_data", "=", "kl", ".", "data", ".", "clone", "(", ")", "\n", "stats", "=", "self", ".", "_stats", "(", "xent_data", ",", "kl_data", ",", "scores", ".", "data", ",", "target", ".", "view", "(", "-", "1", ")", ".", "data", ")", "\n", "return", "loss", ",", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.filter_shard_state": [[359, 372], ["state.items", "isinstance", "torch.split", "torch.split", "v_chunk.data.clone.data.clone", "v_split.append"], "function", ["None"], ["", "", "def", "filter_shard_state", "(", "state", ",", "shard_size", "=", "None", ")", ":", "\n", "    ", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", ":", "\n", "        ", "if", "shard_size", "is", "None", "and", "v", "is", "not", "None", ":", "\n", "            ", "yield", "k", ",", "v", "\n", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v_split", "=", "[", "]", "\n", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "for", "v_chunk", "in", "torch", ".", "split", "(", "v", ",", "shard_size", ")", ":", "\n", "                    ", "v_chunk", "=", "v_chunk", ".", "data", ".", "clone", "(", ")", "\n", "v_chunk", ".", "requires_grad", "=", "v", ".", "requires_grad", "\n", "v_split", ".", "append", "(", "v_chunk", ")", "\n", "", "", "yield", "k", ",", "(", "v", ",", "v_split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.shards": [[374, 421], ["dict", "zip", "zip", "dict.items", "zip", "torch.autograd.backward", "torch.autograd.backward", "Loss.filter_shard_state", "Loss.filter_shard_state", "dict", "isinstance", "zip", "variables.extend", "dict.items", "zip", "torch.split", "torch.split"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.backward", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.backward", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.filter_shard_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.filter_shard_state"], ["", "", "", "def", "shards", "(", "state", ",", "shard_size", ",", "eval", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        state: A dictionary which corresponds to the output of\n               *LossCompute._make_shard_state(). The values for\n               those keys are Tensor-like or None.\n        shard_size: The maximum size of the shards yielded by the model.\n        eval: If True, only yield the state, nothing else.\n              Otherwise, yield shards.\n    Yields:\n        Each yielded shard is a dict.\n    Side effect:\n        After the last shard, this function does back-propagation.\n    \"\"\"", "\n", "if", "eval", ":", "\n", "        ", "yield", "filter_shard_state", "(", "state", ")", "\n", "", "else", ":", "\n", "# non_none: the subdict of the state dictionary where the values", "\n", "# are not None.", "\n", "        ", "non_none", "=", "dict", "(", "filter_shard_state", "(", "state", ",", "shard_size", ")", ")", "\n", "\n", "# Now, the iteration:", "\n", "# state is a dictionary of sequences of tensor-like but we", "\n", "# want a sequence of dictionaries of tensors.", "\n", "# First, unzip the dictionary into a sequence of keys and a", "\n", "# sequence of tensor-like sequences.", "\n", "keys", ",", "values", "=", "zip", "(", "*", "(", "(", "k", ",", "[", "v_chunk", "for", "v_chunk", "in", "v_split", "]", ")", "\n", "for", "k", ",", "(", "_", ",", "v_split", ")", "in", "non_none", ".", "items", "(", ")", ")", ")", "\n", "\n", "# Now, yield a dictionary for each shard. The keys are always", "\n", "# the same. values is a sequence of length #keys where each", "\n", "# element is a sequence of length #shards. We want to iterate", "\n", "# over the shards, not over the keys: therefore, the values need", "\n", "# to be re-zipped by shard and then each shard can be paired", "\n", "# with the keys.", "\n", "for", "shard_tensors", "in", "zip", "(", "*", "values", ")", ":", "\n", "            ", "yield", "dict", "(", "zip", "(", "keys", ",", "shard_tensors", ")", ")", "\n", "\n", "# Assumed backprop'd", "\n", "", "variables", "=", "[", "]", "\n", "for", "k", ",", "(", "v", ",", "v_split", ")", "in", "non_none", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", "and", "state", "[", "k", "]", ".", "requires_grad", ":", "\n", "                ", "if", "v_split", "[", "0", "]", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "variables", ".", "extend", "(", "zip", "(", "torch", ".", "split", "(", "state", "[", "k", "]", ",", "shard_size", ")", ",", "\n", "[", "v_chunk", ".", "grad", "for", "v_chunk", "in", "v_split", "]", ")", ")", "\n", "", "", "", "inputs", ",", "grads", "=", "zip", "(", "*", "variables", ")", "\n", "torch", ".", "autograd", ".", "backward", "(", "inputs", ",", "grads", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.CheckSRU.__init__": [[16, 18], ["argparse.Action.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "option_strings", ",", "dest", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "CheckSRU", ",", "self", ")", ".", "__init__", "(", "option_strings", ",", "dest", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.CheckSRU.__call__": [[19, 24], ["setattr", "SRU.check_sru_requirement"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.check_sru_requirement"], ["", "def", "__call__", "(", "self", ",", "parser", ",", "namespace", ",", "values", ",", "option_string", "=", "None", ")", ":", "\n", "        ", "if", "values", "==", "'SRU'", ":", "\n", "            ", "check_sru_requirement", "(", "abort", "=", "True", ")", "\n", "# Check pass, set the args.", "\n", "", "setattr", "(", "namespace", ",", "self", ".", "dest", ",", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.__init__": [[372, 377], ["torch.autograd.Function.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "activation_type", ",", "d_out", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "super", "(", "SRU_Compute", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "activation_type", "=", "activation_type", "\n", "self", ".", "d_out", "=", "d_out", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.forward": [[378, 422], ["x.size", "min", "x.new", "x.new", "FUNC", "SRU.SRU_Compute.save_for_backward", "x.size", "u.size", "x.new().zero_", "x.dim", "x.dim", "x.dim", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x.new", "u.contiguous().data_ptr", "bias.data_ptr", "init_.contiguous().data_ptr", "x.new.data_ptr", "x.new.data_ptr", "x.contiguous().data_ptr", "mask_h.data_ptr", "u.contiguous", "init_.contiguous", "x.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "u", ",", "x", ",", "bias", ",", "init", "=", "None", ",", "mask_h", "=", "None", ")", ":", "\n", "        ", "bidir", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "length", "=", "x", ".", "size", "(", "0", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "1", "\n", "batch", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "d", "=", "self", ".", "d_out", "\n", "k", "=", "u", ".", "size", "(", "-", "1", ")", "//", "d", "\n", "k_", "=", "k", "//", "2", "if", "self", ".", "bidirectional", "else", "k", "\n", "ncols", "=", "batch", "*", "d", "*", "bidir", "\n", "thread_per_block", "=", "min", "(", "512", ",", "ncols", ")", "\n", "num_block", "=", "(", "ncols", "-", "1", ")", "//", "thread_per_block", "+", "1", "\n", "\n", "init_", "=", "x", ".", "new", "(", "ncols", ")", ".", "zero_", "(", ")", "if", "init", "is", "None", "else", "init", "\n", "size", "=", "(", "length", ",", "batch", ",", "d", "*", "bidir", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "(", "batch", ",", "d", "*", "bidir", ")", "\n", "c", "=", "x", ".", "new", "(", "*", "size", ")", "\n", "h", "=", "x", ".", "new", "(", "*", "size", ")", "\n", "\n", "FUNC", "=", "SRU_FWD_FUNC", "if", "not", "self", ".", "bidirectional", "else", "SRU_BiFWD_FUNC", "\n", "FUNC", "(", "args", "=", "[", "\n", "u", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "x", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", "if", "k_", "==", "3", "else", "0", ",", "\n", "bias", ".", "data_ptr", "(", ")", ",", "\n", "init_", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "mask_h", ".", "data_ptr", "(", ")", "if", "mask_h", "is", "not", "None", "else", "0", ",", "\n", "length", ",", "\n", "batch", ",", "\n", "d", ",", "\n", "k_", ",", "\n", "h", ".", "data_ptr", "(", ")", ",", "\n", "c", ".", "data_ptr", "(", ")", ",", "\n", "self", ".", "activation_type", "]", ",", "\n", "block", "=", "(", "thread_per_block", ",", "1", ",", "1", ")", ",", "grid", "=", "(", "num_block", ",", "1", ",", "1", ")", ",", "\n", "stream", "=", "SRU_STREAM", "\n", ")", "\n", "\n", "self", ".", "save_for_backward", "(", "u", ",", "x", ",", "bias", ",", "init", ",", "mask_h", ")", "\n", "self", ".", "intermediate", "=", "c", "\n", "if", "x", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "last_hidden", "=", "c", "\n", "", "elif", "self", ".", "bidirectional", ":", "\n", "# -> directions x batch x dim", "\n", "            ", "last_hidden", "=", "torch", ".", "stack", "(", "(", "c", "[", "-", "1", ",", ":", ",", ":", "d", "]", ",", "c", "[", "0", ",", ":", ",", "d", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "last_hidden", "=", "c", "[", "-", "1", "]", "\n", "", "return", "h", ",", "last_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU_Compute.backward": [[423, 474], ["x.size", "min", "u.new", "x.new", "x.new", "FUNC", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x.size", "u.size", "x.new().zero_", "x.new", "x.new.sum().view", "x.dim", "u.size", "x.new", "x.size", "u.contiguous().data_ptr", "bias.data_ptr", "init_.contiguous().data_ptr", "c.data_ptr", "grad_h.contiguous().data_ptr", "torch.cat.contiguous().data_ptr", "torch.cat.contiguous().data_ptr", "u.new.data_ptr", "x.new.data_ptr", "x.new.data_ptr", "x.new.sum", "x.contiguous().data_ptr", "mask_h.data_ptr", "grad_x.data_ptr", "u.contiguous", "init_.contiguous", "grad_h.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "x.contiguous"], "methods", ["None"], ["", "def", "backward", "(", "self", ",", "grad_h", ",", "grad_last", ")", ":", "\n", "        ", "if", "self", ".", "bidirectional", ":", "\n", "            ", "grad_last", "=", "torch", ".", "cat", "(", "(", "grad_last", "[", "0", "]", ",", "grad_last", "[", "1", "]", ")", ",", "1", ")", "\n", "", "bidir", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "u", ",", "x", ",", "bias", ",", "init", ",", "mask_h", "=", "self", ".", "saved_tensors", "\n", "c", "=", "self", ".", "intermediate", "\n", "length", "=", "x", ".", "size", "(", "0", ")", "if", "x", ".", "dim", "(", ")", "==", "3", "else", "1", "\n", "batch", "=", "x", ".", "size", "(", "-", "2", ")", "\n", "d", "=", "self", ".", "d_out", "\n", "k", "=", "u", ".", "size", "(", "-", "1", ")", "//", "d", "\n", "k_", "=", "k", "//", "2", "if", "self", ".", "bidirectional", "else", "k", "\n", "ncols", "=", "batch", "*", "d", "*", "bidir", "\n", "thread_per_block", "=", "min", "(", "512", ",", "ncols", ")", "\n", "num_block", "=", "(", "ncols", "-", "1", ")", "//", "thread_per_block", "+", "1", "\n", "\n", "init_", "=", "x", ".", "new", "(", "ncols", ")", ".", "zero_", "(", ")", "if", "init", "is", "None", "else", "init", "\n", "grad_u", "=", "u", ".", "new", "(", "*", "u", ".", "size", "(", ")", ")", "\n", "grad_bias", "=", "x", ".", "new", "(", "2", ",", "batch", ",", "d", "*", "bidir", ")", "\n", "grad_init", "=", "x", ".", "new", "(", "batch", ",", "d", "*", "bidir", ")", "\n", "\n", "# For DEBUG", "\n", "# size = (length, batch, x.size(-1)) \\", "\n", "#         if x.dim() == 3 else (batch, x.size(-1))", "\n", "# grad_x = x.new(*x.size()) if k_ == 3 else x.new(*size).zero_()", "\n", "\n", "# Normal use", "\n", "grad_x", "=", "x", ".", "new", "(", "*", "x", ".", "size", "(", ")", ")", "if", "k_", "==", "3", "else", "None", "\n", "\n", "FUNC", "=", "SRU_BWD_FUNC", "if", "not", "self", ".", "bidirectional", "else", "SRU_BiBWD_FUNC", "\n", "FUNC", "(", "args", "=", "[", "\n", "u", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "x", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", "if", "k_", "==", "3", "else", "0", ",", "\n", "bias", ".", "data_ptr", "(", ")", ",", "\n", "init_", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "mask_h", ".", "data_ptr", "(", ")", "if", "mask_h", "is", "not", "None", "else", "0", ",", "\n", "c", ".", "data_ptr", "(", ")", ",", "\n", "grad_h", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "grad_last", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "length", ",", "\n", "batch", ",", "\n", "d", ",", "\n", "k_", ",", "\n", "grad_u", ".", "data_ptr", "(", ")", ",", "\n", "grad_x", ".", "data_ptr", "(", ")", "if", "k_", "==", "3", "else", "0", ",", "\n", "grad_bias", ".", "data_ptr", "(", ")", ",", "\n", "grad_init", ".", "data_ptr", "(", ")", ",", "\n", "self", ".", "activation_type", "]", ",", "\n", "block", "=", "(", "thread_per_block", ",", "1", ",", "1", ")", ",", "grid", "=", "(", "num_block", ",", "1", ",", "1", ")", ",", "\n", "stream", "=", "SRU_STREAM", "\n", ")", "\n", "return", "grad_u", ",", "grad_x", ",", "grad_bias", ".", "sum", "(", "1", ")", ".", "view", "(", "-", "1", ")", ",", "grad_init", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.__init__": [[477, 498], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "SRU.SRUCell.init_weight", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.init_weight"], ["    ", "def", "__init__", "(", "self", ",", "n_in", ",", "n_out", ",", "dropout", "=", "0", ",", "rnn_dropout", "=", "0", ",", "\n", "bidirectional", "=", "False", ",", "use_tanh", "=", "1", ",", "use_relu", "=", "0", ")", ":", "\n", "        ", "super", "(", "SRUCell", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_in", "=", "n_in", "\n", "self", ".", "n_out", "=", "n_out", "\n", "self", ".", "rnn_dropout", "=", "rnn_dropout", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "activation_type", "=", "2", "if", "use_relu", "else", "(", "1", "if", "use_tanh", "else", "0", ")", "\n", "\n", "out_size", "=", "n_out", "*", "2", "if", "bidirectional", "else", "n_out", "\n", "k", "=", "4", "if", "n_in", "!=", "out_size", "else", "3", "\n", "self", ".", "size_per_dir", "=", "n_out", "*", "k", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "n_in", ",", "\n", "self", ".", "size_per_dir", "*", "2", "if", "bidirectional", "else", "self", ".", "size_per_dir", "\n", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "n_out", "*", "4", "if", "bidirectional", "else", "n_out", "*", "2", "\n", ")", ")", "\n", "self", ".", "init_weight", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.init_weight": [[499, 503], ["SRU.SRUCell.weight.data.uniform_", "SRU.SRUCell.bias.data.zero_"], "methods", ["None"], ["", "def", "init_weight", "(", "self", ")", ":", "\n", "        ", "val_range", "=", "(", "3.0", "/", "self", ".", "n_in", ")", "**", "0.5", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "val_range", ",", "val_range", ")", "\n", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.set_bias": [[504, 510], ["SRU.SRUCell.bias.data[].zero_().add_", "SRU.SRUCell.bias.data[].zero_().add_", "SRU.SRUCell.bias.data[].zero_", "SRU.SRUCell.bias.data[].zero_"], "methods", ["None"], ["", "def", "set_bias", "(", "self", ",", "bias_val", "=", "0", ")", ":", "\n", "        ", "n_out", "=", "self", ".", "n_out", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "bias", ".", "data", "[", "n_out", "*", "2", ":", "]", ".", "zero_", "(", ")", ".", "add_", "(", "bias_val", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", ".", "data", "[", "n_out", ":", "]", ".", "zero_", "(", ")", ".", "add_", "(", "bias_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.forward": [[511, 543], ["input.size", "x_2d.mm", "torch.autograd.Variable", "torch.autograd.Variable", "SRU.SRUCell.get_dropout_mask_", "x.contiguous().view", "SRU.SRUCell.get_dropout_mask_", "input.dim", "input.dim", "input.data.new().zero_", "SRU.SRUCell.expand_as", "x.dim", "SRU.SRU_Compute", "SRU.SRU_Compute", "x.contiguous", "input.data.new"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.get_dropout_mask_", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.get_dropout_mask_"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "c0", "=", "None", ")", ":", "\n", "        ", "assert", "input", ".", "dim", "(", ")", "==", "2", "or", "input", ".", "dim", "(", ")", "==", "3", "\n", "n_in", ",", "n_out", "=", "self", ".", "n_in", ",", "self", ".", "n_out", "\n", "batch", "=", "input", ".", "size", "(", "-", "2", ")", "\n", "if", "c0", "is", "None", ":", "\n", "            ", "c0", "=", "Variable", "(", "input", ".", "data", ".", "new", "(", "\n", "batch", ",", "n_out", "if", "not", "self", ".", "bidirectional", "else", "n_out", "*", "2", "\n", ")", ".", "zero_", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "training", "and", "(", "self", ".", "rnn_dropout", ">", "0", ")", ":", "\n", "            ", "mask", "=", "self", ".", "get_dropout_mask_", "(", "(", "batch", ",", "n_in", ")", ",", "self", ".", "rnn_dropout", ")", "\n", "x", "=", "input", "*", "mask", ".", "expand_as", "(", "input", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "input", "\n", "\n", "", "x_2d", "=", "x", "if", "x", ".", "dim", "(", ")", "==", "2", "else", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "n_in", ")", "\n", "u", "=", "x_2d", ".", "mm", "(", "self", ".", "weight", ")", "\n", "\n", "if", "self", ".", "training", "and", "(", "self", ".", "dropout", ">", "0", ")", ":", "\n", "            ", "bidir", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "mask_h", "=", "self", ".", "get_dropout_mask_", "(", "(", "batch", ",", "n_out", "*", "bidir", ")", ",", "self", ".", "dropout", ")", "\n", "h", ",", "c", "=", "SRU_Compute", "(", "self", ".", "activation_type", ",", "n_out", ",", "\n", "self", ".", "bidirectional", ")", "(", "\n", "u", ",", "input", ",", "self", ".", "bias", ",", "c0", ",", "mask_h", "\n", ")", "\n", "", "else", ":", "\n", "            ", "h", ",", "c", "=", "SRU_Compute", "(", "self", ".", "activation_type", ",", "n_out", ",", "\n", "self", ".", "bidirectional", ")", "(", "\n", "u", ",", "input", ",", "self", ".", "bias", ",", "c0", "\n", ")", "\n", "\n", "", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRUCell.get_dropout_mask_": [[544, 547], ["torch.autograd.Variable", "torch.autograd.Variable", "w.new().bernoulli_().div_", "w.new().bernoulli_", "w.new"], "methods", ["None"], ["", "def", "get_dropout_mask_", "(", "self", ",", "size", ",", "p", ")", ":", "\n", "        ", "w", "=", "self", ".", "weight", ".", "data", "\n", "return", "Variable", "(", "w", ".", "new", "(", "*", "size", ")", ".", "bernoulli_", "(", "1", "-", "p", ")", ".", "div_", "(", "1", "-", "p", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU.__init__": [[570, 597], ["SRU.check_sru_requirement", "torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "range", "SRU.SRUCell", "SRU.SRU.rnn_lst.append"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.check_sru_requirement", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "\n", "num_layers", "=", "2", ",", "dropout", "=", "0", ",", "rnn_dropout", "=", "0", ",", "\n", "bidirectional", "=", "False", ",", "use_tanh", "=", "1", ",", "use_relu", "=", "0", ")", ":", "\n", "# An entry check here, will catch on train side and translate side", "\n", "# if requirements are not satisfied.", "\n", "        ", "check_sru_requirement", "(", "abort", "=", "True", ")", "\n", "super", "(", "SRU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_in", "=", "input_size", "\n", "self", ".", "n_out", "=", "hidden_size", "\n", "self", ".", "depth", "=", "num_layers", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "rnn_dropout", "=", "rnn_dropout", "\n", "self", ".", "rnn_lst", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "out_size", "=", "hidden_size", "*", "2", "if", "bidirectional", "else", "hidden_size", "\n", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "sru_cell", "=", "SRUCell", "(", "\n", "n_in", "=", "self", ".", "n_in", "if", "i", "==", "0", "else", "self", ".", "out_size", ",", "\n", "n_out", "=", "self", ".", "n_out", ",", "\n", "dropout", "=", "dropout", "if", "i", "+", "1", "!=", "num_layers", "else", "0", ",", "\n", "rnn_dropout", "=", "rnn_dropout", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", "use_tanh", "=", "use_tanh", ",", "\n", "use_relu", "=", "use_relu", ",", "\n", ")", "\n", "self", ".", "rnn_lst", ".", "append", "(", "sru_cell", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU.set_bias": [[598, 601], ["l.set_bias"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU.set_bias"], ["", "", "def", "set_bias", "(", "self", ",", "bias_val", "=", "0", ")", ":", "\n", "        ", "for", "l", "in", "self", ".", "rnn_lst", ":", "\n", "            ", "l", ".", "set_bias", "(", "bias_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.SRU.forward": [[602, 634], ["enumerate", "input.dim", "torch.autograd.Variable", "torch.autograd.Variable", "isinstance", "rnn", "lstc.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "input.data.new().zero_", "c0.dim", "h.squeeze", "range", "c0.chunk", "input.data.new", "input.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "c0", "=", "None", ",", "return_hidden", "=", "True", ")", ":", "\n", "        ", "assert", "input", ".", "dim", "(", ")", "==", "3", "# (len, batch, n_in)", "\n", "dir_", "=", "2", "if", "self", ".", "bidirectional", "else", "1", "\n", "if", "c0", "is", "None", ":", "\n", "            ", "zeros", "=", "Variable", "(", "input", ".", "data", ".", "new", "(", "\n", "input", ".", "size", "(", "1", ")", ",", "self", ".", "n_out", "*", "dir_", "\n", ")", ".", "zero_", "(", ")", ")", "\n", "c0", "=", "[", "zeros", "for", "i", "in", "range", "(", "self", ".", "depth", ")", "]", "\n", "", "else", ":", "\n", "            ", "if", "isinstance", "(", "c0", ",", "tuple", ")", ":", "\n", "# RNNDecoderState wraps hidden as a tuple.", "\n", "                ", "c0", "=", "c0", "[", "0", "]", "\n", "", "assert", "c0", ".", "dim", "(", ")", "==", "3", "# (depth, batch, dir_*n_out)", "\n", "c0", "=", "[", "h", ".", "squeeze", "(", "0", ")", "for", "h", "in", "c0", ".", "chunk", "(", "self", ".", "depth", ",", "0", ")", "]", "\n", "\n", "", "prevx", "=", "input", "\n", "lstc", "=", "[", "]", "\n", "for", "i", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnn_lst", ")", ":", "\n", "            ", "h", ",", "c", "=", "rnn", "(", "prevx", ",", "c0", "[", "i", "]", ")", "\n", "prevx", "=", "h", "\n", "lstc", ".", "append", "(", "c", ")", "\n", "\n", "", "if", "self", ".", "bidirectional", ":", "\n", "# fh -> (layers*directions) x batch x dim", "\n", "            ", "fh", "=", "torch", ".", "cat", "(", "lstc", ")", "\n", "", "else", ":", "\n", "            ", "fh", "=", "torch", ".", "stack", "(", "lstc", ")", "\n", "\n", "", "if", "return_hidden", ":", "\n", "            ", "return", "prevx", ",", "fh", "\n", "", "else", ":", "\n", "            ", "return", "prevx", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.SRU.check_sru_requirement": [[31, 68], ["re.compile", "os.getenv", "torch.cuda.is_available", "torch.cuda.is_available", "AssertionError", "re.match", "AssertionError", "platform.system", "subprocess.check_output", "subprocess.check_output", "subprocess.check_output", "subprocess.check_output", "AssertionError"], "function", ["None"], ["", "", "def", "check_sru_requirement", "(", "abort", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Return True if check pass; if check fails and abort is True,\n    raise an Exception, othereise return False.\n    \"\"\"", "\n", "# Check 1.", "\n", "try", ":", "\n", "        ", "if", "platform", ".", "system", "(", ")", "==", "'Windows'", ":", "\n", "            ", "subprocess", ".", "check_output", "(", "'pip freeze | findstr cupy'", ",", "shell", "=", "True", ")", "\n", "subprocess", ".", "check_output", "(", "'pip freeze | findstr pynvrtc'", ",", "\n", "shell", "=", "True", ")", "\n", "", "else", ":", "# Unix-like systems", "\n", "            ", "subprocess", ".", "check_output", "(", "'pip freeze | grep -w cupy'", ",", "shell", "=", "True", ")", "\n", "subprocess", ".", "check_output", "(", "'pip freeze | grep -w pynvrtc'", ",", "\n", "shell", "=", "True", ")", "\n", "", "", "except", "subprocess", ".", "CalledProcessError", ":", "\n", "        ", "if", "not", "abort", ":", "\n", "            ", "return", "False", "\n", "", "raise", "AssertionError", "(", "\"Using SRU requires 'cupy' and 'pynvrtc' \"", "\n", "\"python packages installed.\"", ")", "\n", "\n", "# Check 2.", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "is", "False", ":", "\n", "        ", "if", "not", "abort", ":", "\n", "            ", "return", "False", "\n", "", "raise", "AssertionError", "(", "\"Using SRU requires pytorch built with cuda.\"", ")", "\n", "\n", "# Check 3.", "\n", "", "pattern", "=", "re", ".", "compile", "(", "\".*cuda/lib.*\"", ")", "\n", "ld_path", "=", "os", ".", "getenv", "(", "'LD_LIBRARY_PATH'", ",", "\"\"", ")", "\n", "if", "re", ".", "match", "(", "pattern", ",", "ld_path", ")", "is", "None", ":", "\n", "        ", "if", "not", "abort", ":", "\n", "            ", "return", "False", "\n", "", "raise", "AssertionError", "(", "\"Using SRU requires setting cuda lib path, e.g. \"", "\n", "\"export LD_LIBRARY_PATH=/usr/local/cuda/lib64.\"", ")", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.GlobalAttention.GlobalAttention.__init__": [[61, 86], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "src_dim", ",", "tgt_dim", ",", "attn_dim", ",", "coverage", "=", "False", ",", "attn_type", "=", "\"dot\"", ")", ":", "\n", "        ", "super", "(", "GlobalAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "src_dim", "=", "src_dim", "\n", "self", ".", "tgt_dim", "=", "tgt_dim", "\n", "self", ".", "dim", "=", "attn_dim", "\n", "self", ".", "attn_type", "=", "attn_type", "\n", "assert", "(", "self", ".", "attn_type", "in", "[", "\"dot\"", ",", "\"general\"", ",", "\"mlp\"", "]", ")", ",", "(", "\n", "\"Please select a valid attention type.\"", ")", "\n", "\n", "if", "self", ".", "attn_type", "==", "\"general\"", ":", "\n", "            ", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "tgt_dim", ",", "src_dim", ",", "bias", "=", "False", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "\"mlp\"", ":", "\n", "            ", "self", ".", "linear_context", "=", "nn", ".", "Linear", "(", "src_dim", ",", "attn_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_query", "=", "nn", ".", "Linear", "(", "tgt_dim", ",", "attn_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "attn_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "# mlp wants it with bias", "\n", "", "out_bias", "=", "self", ".", "attn_type", "==", "\"mlp\"", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "src_dim", "+", "tgt_dim", ",", "tgt_dim", ",", "bias", "=", "out_bias", ")", "\n", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "if", "coverage", ":", "\n", "            ", "self", ".", "linear_cover", "=", "nn", ".", "Linear", "(", "1", ",", "dim", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.GlobalAttention.GlobalAttention.score": [[87, 127], ["h_s.size", "GlobalAttention.GlobalAttention.view.size", "onmt.Utils.aeq", "h_s.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "GlobalAttention.GlobalAttention.linear_query", "wq.expand.expand.view", "wq.expand.expand.expand", "GlobalAttention.GlobalAttention.linear_context", "uh.expand.expand.view", "uh.expand.expand.expand", "GlobalAttention.GlobalAttention.tanh", "GlobalAttention.GlobalAttention.v().view", "GlobalAttention.GlobalAttention.view.view", "GlobalAttention.GlobalAttention.linear_in", "GlobalAttention.GlobalAttention.view", "GlobalAttention.GlobalAttention.view.view", "h_s.contiguous().view", "GlobalAttention.GlobalAttention.v", "h_s.contiguous", "GlobalAttention.GlobalAttention.view"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "", "def", "score", "(", "self", ",", "h_t", ",", "h_s", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n          h_t (`FloatTensor`): sequence of queries `[batch x tgt_len x dim]`\n          h_s (`FloatTensor`): sequence of sources `[batch x src_len x dim]`\n\n        Returns:\n          :obj:`FloatTensor`:\n           raw attention scores (unnormalized) for each src index\n          `[batch x tgt_len x src_len]`\n\n        \"\"\"", "\n", "\n", "# Check input sizes", "\n", "src_batch", ",", "src_len", ",", "src_dim", "=", "h_s", ".", "size", "(", ")", "\n", "tgt_batch", ",", "tgt_len", ",", "tgt_dim", "=", "h_t", ".", "size", "(", ")", "\n", "aeq", "(", "src_batch", ",", "tgt_batch", ")", "\n", "\n", "if", "self", ".", "attn_type", "in", "[", "\"general\"", ",", "\"dot\"", "]", ":", "\n", "            ", "if", "self", ".", "attn_type", "==", "\"general\"", ":", "\n", "                ", "h_t_", "=", "h_t", ".", "view", "(", "tgt_batch", "*", "tgt_len", ",", "tgt_dim", ")", "\n", "h_t_", "=", "self", ".", "linear_in", "(", "h_t_", ")", "\n", "h_t", "=", "h_t_", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "tgt_dim", ")", "\n", "", "h_s_", "=", "h_s", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# (batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)", "\n", "return", "torch", ".", "bmm", "(", "h_t", ",", "h_s_", ")", "\n", "", "else", ":", "\n", "            ", "dim", "=", "self", ".", "dim", "\n", "wq", "=", "self", ".", "linear_query", "(", "h_t", ".", "view", "(", "-", "1", ",", "self", ".", "tgt_dim", ")", ")", "\n", "wq", "=", "wq", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "1", ",", "dim", ")", "\n", "wq", "=", "wq", ".", "expand", "(", "tgt_batch", ",", "tgt_len", ",", "src_len", ",", "dim", ")", "\n", "\n", "uh", "=", "self", ".", "linear_context", "(", "h_s", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "src_dim", ")", ")", "\n", "uh", "=", "uh", ".", "view", "(", "src_batch", ",", "1", ",", "src_len", ",", "dim", ")", "\n", "uh", "=", "uh", ".", "expand", "(", "src_batch", ",", "tgt_len", ",", "src_len", ",", "dim", ")", "\n", "\n", "# (batch, t_len, s_len, d)", "\n", "wquh", "=", "self", ".", "tanh", "(", "wq", "+", "uh", ")", "\n", "\n", "return", "self", ".", "v", "(", "wquh", ".", "view", "(", "-", "1", ",", "dim", ")", ")", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "src_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.GlobalAttention.GlobalAttention.forward": [[128, 216], ["GlobalAttention.GlobalAttention.size", "input.unsqueeze.unsqueeze.size", "onmt.Utils.aeq", "GlobalAttention.GlobalAttention.score", "GlobalAttention.GlobalAttention.sm", "align_vectors.transpose().contiguous.transpose().contiguous.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "GlobalAttention.GlobalAttention.linear_out", "input.unsqueeze.unsqueeze.dim", "input.unsqueeze.unsqueeze.unsqueeze", "coverage.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "coverage.view().unsqueeze", "GlobalAttention.GlobalAttention.linear_cover().view_as", "GlobalAttention.GlobalAttention.tanh", "onmt.Utils.sequence_mask", "mask.unsqueeze.unsqueeze.unsqueeze", "GlobalAttention.GlobalAttention.data.masked_fill_", "GlobalAttention.GlobalAttention.view", "GlobalAttention.GlobalAttention.tanh", "attn_h.transpose().contiguous.transpose().contiguous.squeeze", "align_vectors.transpose().contiguous.transpose().contiguous.squeeze", "c.squeeze.squeeze.squeeze", "attn_h.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "align_vectors.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "attn_h.transpose().contiguous.transpose().contiguous.transpose().contiguous", "align_vectors.transpose().contiguous.transpose().contiguous.transpose().contiguous", "attn_h.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "align_vectors.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "coverage.view", "GlobalAttention.GlobalAttention.linear_cover", "float", "attn_h.transpose().contiguous.transpose().contiguous.transpose", "align_vectors.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.sequence_mask", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "memory_bank", ",", "memory_lengths", "=", "None", ",", "coverage", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n          input (`FloatTensor`): query vectors `[batch x tgt_len x dim]`\n          memory_bank (`FloatTensor`): source vectors `[batch x src_len x dim]`\n          memory_lengths (`LongTensor`): the source context lengths `[batch]`\n          coverage (`FloatTensor`): None (not supported yet)\n\n        Returns:\n          (`FloatTensor`, `FloatTensor`):\n\n          * Computed vector `[tgt_len x batch x dim]`\n          * Attention distribtutions for each query\n             `[tgt_len x batch x src_len]`\n        \"\"\"", "\n", "\n", "# one step input", "\n", "if", "input", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "one_step", "=", "True", "\n", "input", "=", "input", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "one_step", "=", "False", "\n", "\n", "", "batch", ",", "sourceL", ",", "dim", "=", "memory_bank", ".", "size", "(", ")", "\n", "batch_", ",", "targetL", ",", "dim_", "=", "input", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "\n", "if", "coverage", "is", "not", "None", ":", "\n", "            ", "batch_", ",", "sourceL_", "=", "coverage", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "sourceL", ",", "sourceL_", ")", "\n", "\n", "", "if", "coverage", "is", "not", "None", ":", "\n", "            ", "cover", "=", "coverage", ".", "view", "(", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "memory_bank", "+=", "self", ".", "linear_cover", "(", "cover", ")", ".", "view_as", "(", "memory_bank", ")", "\n", "memory_bank", "=", "self", ".", "tanh", "(", "memory_bank", ")", "\n", "\n", "# compute attention scores, as in Luong et al.", "\n", "", "align", "=", "self", ".", "score", "(", "input", ",", "memory_bank", ")", "\n", "self", ".", "p_attn_score", "=", "align", "\n", "if", "memory_lengths", "is", "not", "None", ":", "\n", "            ", "mask", "=", "sequence_mask", "(", "memory_lengths", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "# Make it broadcastable.", "\n", "align", ".", "data", ".", "masked_fill_", "(", "1", "-", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "# Softmax to normalize attention weights", "\n", "", "align_vectors", "=", "self", ".", "sm", "(", "align", ".", "view", "(", "batch", "*", "targetL", ",", "sourceL", ")", ")", "\n", "align_vectors", "=", "align_vectors", ".", "view", "(", "batch", ",", "targetL", ",", "sourceL", ")", "\n", "\n", "# each context vector c_t is the weighted average", "\n", "# over all the source hidden states", "\n", "c", "=", "torch", ".", "bmm", "(", "align_vectors", ",", "memory_bank", ")", "\n", "\n", "# concatenate", "\n", "#concat_c = torch.cat([c, input], -1)", "\n", "concat_c", "=", "torch", ".", "cat", "(", "[", "input", ",", "c", "]", ",", "-", "1", ")", "\n", "attn_h", "=", "self", ".", "linear_out", "(", "concat_c", ")", "\n", "#if self.attn_type in [\"general\", \"dot\"]:", "\n", "if", "True", "or", "self", ".", "attn_type", "in", "[", "\"general\"", ",", "\"dot\"", "]", ":", "\n", "            ", "attn_h", "=", "self", ".", "tanh", "(", "attn_h", ")", "\n", "\n", "", "if", "one_step", ":", "\n", "            ", "attn_h", "=", "attn_h", ".", "squeeze", "(", "1", ")", "\n", "align_vectors", "=", "align_vectors", ".", "squeeze", "(", "1", ")", "\n", "c", "=", "c", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Check output sizes", "\n", "batch_", ",", "dim_", "=", "attn_h", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "batch_", ",", "sourceL_", "=", "align_vectors", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "sourceL", ",", "sourceL_", ")", "\n", "", "else", ":", "\n", "            ", "attn_h", "=", "attn_h", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "align_vectors", "=", "align_vectors", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Check output sizes", "\n", "targetL_", ",", "batch_", ",", "dim_", "=", "attn_h", ".", "size", "(", ")", "\n", "aeq", "(", "targetL", ",", "targetL_", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "dim", ",", "dim_", ")", "\n", "targetL_", ",", "batch_", ",", "sourceL_", "=", "align_vectors", ".", "size", "(", ")", "\n", "aeq", "(", "targetL", ",", "targetL_", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "sourceL", ",", "sourceL_", ")", "\n", "\n", "", "return", "attn_h", ",", "align_vectors", ",", "c", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.PositionalEncoding.__init__": [[23, 35], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "pe.unsqueeze.unsqueeze.unsqueeze", "torch.Module.__init__", "Embeddings.PositionalEncoding.register_buffer", "torch.Dropout", "torch.Dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log"], ["def", "__init__", "(", "self", ",", "dropout", ",", "dim", ",", "max_len", "=", "5000", ")", ":", "\n", "        ", "pe", "=", "torch", ".", "zeros", "(", "max_len", ",", "dim", ")", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "0", ",", "dim", ",", "2", ")", "*", "\n", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "dim", ")", ")", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "1", ")", "\n", "super", "(", "PositionalEncoding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "register_buffer", "(", "'pe'", ",", "pe", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.PositionalEncoding.forward": [[36, 44], ["Embeddings.PositionalEncoding.dropout", "math.sqrt", "torch.autograd.Variable", "torch.autograd.Variable", "Embeddings.PositionalEncoding.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "emb", ")", ":", "\n", "# We must wrap the self.pe in Variable to compute, not the other", "\n", "# way - unwrap emb(i.e. emb.data). Otherwise the computation", "\n", "# wouldn't be watched to build the compute graph.", "\n", "        ", "emb", "=", "emb", "*", "math", ".", "sqrt", "(", "self", ".", "dim", ")", "\n", "emb", "=", "emb", "+", "Variable", "(", "self", ".", "pe", "[", ":", "emb", ".", "size", "(", "0", ")", "]", ",", "requires_grad", "=", "False", ")", "\n", "emb", "=", "self", ".", "dropout", "(", "emb", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.Embeddings.__init__": [[88, 151], ["vocab_sizes.extend", "emb_dims.extend", "pad_indices.extend", "zip", "onmt.modules.Elementwise", "torch.Module.__init__", "torch.Sequential", "torch.Sequential", "Embeddings.Embeddings.make_embedding.add_module", "torch.Embedding", "torch.Embedding", "sum", "sum", "torch.Sequential", "torch.Sequential", "Embeddings.Embeddings.make_embedding.add_module", "Embeddings.PositionalEncoding", "Embeddings.Embeddings.make_embedding.add_module", "len", "len", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "len", "int"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "word_vec_size", ",", "\n", "word_vocab_size", ",", "\n", "word_padding_idx", ",", "\n", "position_encoding", "=", "False", ",", "\n", "feat_merge", "=", "\"concat\"", ",", "\n", "feat_vec_exponent", "=", "0.7", ",", "feat_vec_size", "=", "-", "1", ",", "\n", "feat_padding_idx", "=", "[", "]", ",", "\n", "feat_vocab_sizes", "=", "[", "]", ",", "\n", "dropout", "=", "0", ",", "\n", "sparse", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "word_padding_idx", "=", "word_padding_idx", "\n", "\n", "# Dimensions and padding for constructing the word embedding matrix", "\n", "vocab_sizes", "=", "[", "word_vocab_size", "]", "\n", "emb_dims", "=", "[", "word_vec_size", "]", "\n", "pad_indices", "=", "[", "word_padding_idx", "]", "\n", "\n", "# Dimensions and padding for feature embedding matrices", "\n", "# (these have no effect if feat_vocab_sizes is empty)", "\n", "if", "feat_merge", "==", "'sum'", ":", "\n", "            ", "feat_dims", "=", "[", "word_vec_size", "]", "*", "len", "(", "feat_vocab_sizes", ")", "\n", "", "elif", "feat_vec_size", ">", "0", ":", "\n", "            ", "feat_dims", "=", "[", "feat_vec_size", "]", "*", "len", "(", "feat_vocab_sizes", ")", "\n", "", "else", ":", "\n", "            ", "feat_dims", "=", "[", "int", "(", "vocab", "**", "feat_vec_exponent", ")", "\n", "for", "vocab", "in", "feat_vocab_sizes", "]", "\n", "", "vocab_sizes", ".", "extend", "(", "feat_vocab_sizes", ")", "\n", "emb_dims", ".", "extend", "(", "feat_dims", ")", "\n", "pad_indices", ".", "extend", "(", "feat_padding_idx", ")", "\n", "\n", "# The embedding matrix look-up tables. The first look-up table", "\n", "# is for words. Subsequent ones are for features, if any exist.", "\n", "emb_params", "=", "zip", "(", "vocab_sizes", ",", "emb_dims", ",", "pad_indices", ")", "\n", "embeddings", "=", "[", "nn", ".", "Embedding", "(", "vocab", ",", "dim", ",", "padding_idx", "=", "pad", ",", "sparse", "=", "sparse", ")", "\n", "for", "vocab", ",", "dim", ",", "pad", "in", "emb_params", "]", "\n", "emb_luts", "=", "Elementwise", "(", "feat_merge", ",", "embeddings", ")", "\n", "\n", "# The final output size of word + feature vectors. This can vary", "\n", "# from the word vector size if and only if features are defined.", "\n", "# This is the attribute you should access if you need to know", "\n", "# how big your embeddings are going to be.", "\n", "self", ".", "embedding_size", "=", "(", "sum", "(", "emb_dims", ")", "if", "feat_merge", "==", "'concat'", "\n", "else", "word_vec_size", ")", "\n", "\n", "# The sequence of operations that converts the input sequence", "\n", "# into a sequence of embeddings. At minimum this consists of", "\n", "# looking up the embeddings for each word and feature in the", "\n", "# input. Model parameters may require the sequence to contain", "\n", "# additional operations as well.", "\n", "super", "(", "Embeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "make_embedding", "=", "nn", ".", "Sequential", "(", ")", "\n", "self", ".", "make_embedding", ".", "add_module", "(", "'emb_luts'", ",", "emb_luts", ")", "\n", "\n", "if", "feat_merge", "==", "'mlp'", "and", "len", "(", "feat_vocab_sizes", ")", ">", "0", ":", "\n", "            ", "in_dim", "=", "sum", "(", "emb_dims", ")", "\n", "out_dim", "=", "word_vec_size", "\n", "mlp", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "make_embedding", ".", "add_module", "(", "'mlp'", ",", "mlp", ")", "\n", "\n", "", "if", "position_encoding", ":", "\n", "            ", "pe", "=", "PositionalEncoding", "(", "dropout", ",", "self", ".", "embedding_size", ")", "\n", "self", ".", "make_embedding", ".", "add_module", "(", "'pe'", ",", "pe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.Embeddings.word_lut": [[152, 155], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "word_lut", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "make_embedding", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.Embeddings.emb_luts": [[156, 159], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "emb_luts", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "make_embedding", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.Embeddings.load_pretrained_vectors": [[160, 172], ["torch.load", "torch.load", "torch.load", "torch.load", "Embeddings.Embeddings.word_lut.weight.data.copy_"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["", "def", "load_pretrained_vectors", "(", "self", ",", "emb_file", ",", "fixed", ")", ":", "\n", "        ", "\"\"\"Load in pretrained embeddings.\n\n        Args:\n          emb_file (str) : path to torch serialized embeddings\n          fixed (bool) : if true, embeddings are not updated\n        \"\"\"", "\n", "if", "emb_file", ":", "\n", "            ", "pretrained", "=", "torch", ".", "load", "(", "emb_file", ")", "\n", "self", ".", "word_lut", ".", "weight", ".", "data", ".", "copy_", "(", "pretrained", ")", "\n", "if", "fixed", ":", "\n", "                ", "self", ".", "word_lut", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Embeddings.Embeddings.forward": [[173, 193], ["input.size", "onmt.Utils.aeq", "Embeddings.Embeddings.make_embedding", "Embeddings.Embeddings.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        Computes the embeddings for words and features.\n\n        Args:\n            input (`LongTensor`): index tensor `[len x batch x nfeat]`\n        Return:\n            `FloatTensor`: word embeddings `[len x batch x embedding_size]`\n        \"\"\"", "\n", "in_length", ",", "in_batch", ",", "nfeat", "=", "input", ".", "size", "(", ")", "\n", "aeq", "(", "nfeat", ",", "len", "(", "self", ".", "emb_luts", ")", ")", "\n", "\n", "emb", "=", "self", ".", "make_embedding", "(", "input", ")", "\n", "\n", "out_length", ",", "out_batch", ",", "emb_size", "=", "emb", ".", "size", "(", ")", "\n", "aeq", "(", "in_length", ",", "out_length", ")", "\n", "aeq", "(", "in_batch", ",", "out_batch", ")", "\n", "aeq", "(", "emb_size", ",", "self", ".", "embedding_size", ")", "\n", "\n", "return", "emb", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.ContextGate.__init__": [[26, 35], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", ":", "\n", "        ", "super", "(", "ContextGate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "input_size", "=", "embeddings_size", "+", "decoder_size", "+", "attention_size", "\n", "self", ".", "gate", "=", "nn", ".", "Linear", "(", "input_size", ",", "output_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "sig", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "source_proj", "=", "nn", ".", "Linear", "(", "attention_size", ",", "output_size", ")", "\n", "self", ".", "target_proj", "=", "nn", ".", "Linear", "(", "embeddings_size", "+", "decoder_size", ",", "\n", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.ContextGate.forward": [[36, 43], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "Gate.ContextGate.sig", "Gate.ContextGate.source_proj", "Gate.ContextGate.target_proj", "Gate.ContextGate.gate", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_emb", ",", "dec_state", ",", "attn_state", ")", ":", "\n", "        ", "input_tensor", "=", "torch", ".", "cat", "(", "(", "prev_emb", ",", "dec_state", ",", "attn_state", ")", ",", "dim", "=", "1", ")", "\n", "z", "=", "self", ".", "sig", "(", "self", ".", "gate", "(", "input_tensor", ")", ")", "\n", "proj_source", "=", "self", ".", "source_proj", "(", "attn_state", ")", "\n", "proj_target", "=", "self", ".", "target_proj", "(", "\n", "torch", ".", "cat", "(", "(", "prev_emb", ",", "dec_state", ")", ",", "dim", "=", "1", ")", ")", "\n", "return", "z", ",", "proj_source", ",", "proj_target", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.SourceContextGate.__init__": [[48, 54], ["torch.Module.__init__", "Gate.ContextGate", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", ":", "\n", "        ", "super", "(", "SourceContextGate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "context_gate", "=", "ContextGate", "(", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.SourceContextGate.forward": [[55, 59], ["Gate.SourceContextGate.context_gate", "Gate.SourceContextGate.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_emb", ",", "dec_state", ",", "attn_state", ")", ":", "\n", "        ", "z", ",", "source", ",", "target", "=", "self", ".", "context_gate", "(", "\n", "prev_emb", ",", "dec_state", ",", "attn_state", ")", "\n", "return", "self", ".", "tanh", "(", "target", "+", "z", "*", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.TargetContextGate.__init__": [[64, 70], ["torch.Module.__init__", "Gate.ContextGate", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", ":", "\n", "        ", "super", "(", "TargetContextGate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "context_gate", "=", "ContextGate", "(", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.TargetContextGate.forward": [[71, 74], ["Gate.TargetContextGate.context_gate", "Gate.TargetContextGate.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_emb", ",", "dec_state", ",", "attn_state", ")", ":", "\n", "        ", "z", ",", "source", ",", "target", "=", "self", ".", "context_gate", "(", "prev_emb", ",", "dec_state", ",", "attn_state", ")", "\n", "return", "self", ".", "tanh", "(", "z", "*", "target", "+", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.BothContextGate.__init__": [[79, 85], ["torch.Module.__init__", "Gate.ContextGate", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", ":", "\n", "        ", "super", "(", "BothContextGate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "context_gate", "=", "ContextGate", "(", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.BothContextGate.forward": [[86, 89], ["Gate.BothContextGate.context_gate", "Gate.BothContextGate.tanh"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_emb", ",", "dec_state", ",", "attn_state", ")", ":", "\n", "        ", "z", ",", "source", ",", "target", "=", "self", ".", "context_gate", "(", "prev_emb", ",", "dec_state", ",", "attn_state", ")", "\n", "return", "self", ".", "tanh", "(", "(", "1.", "-", "z", ")", "*", "target", "+", "z", "*", "source", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Gate.context_gate_factory": [[5, 16], ["None"], "function", ["None"], ["def", "context_gate_factory", "(", "type", ",", "embeddings_size", ",", "decoder_size", ",", "\n", "attention_size", ",", "output_size", ")", ":", "\n", "    ", "\"\"\"Returns the correct ContextGate class\"\"\"", "\n", "\n", "gate_types", "=", "{", "'source'", ":", "SourceContextGate", ",", "\n", "'target'", ":", "TargetContextGate", ",", "\n", "'both'", ":", "BothContextGate", "}", "\n", "\n", "assert", "type", "in", "gate_types", ",", "\"Not valid ContextGate type: {0}\"", ".", "format", "(", "type", ")", "\n", "return", "gate_types", "[", "type", "]", "(", "embeddings_size", ",", "decoder_size", ",", "attention_size", ",", "\n", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StackedRNN.StackedLSTM.__init__": [[10, 19], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "range", "StackedRNN.StackedLSTM.layers.append", "torch.LSTMCell", "torch.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "input_size", ",", "rnn_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "StackedLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "nn", ".", "LSTMCell", "(", "input_size", ",", "rnn_size", ")", ")", "\n", "input_size", "=", "rnn_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StackedRNN.StackedLSTM.forward": [[20, 35], ["enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "layer", "StackedRNN.StackedLSTM.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hidden", ")", ":", "\n", "        ", "h_0", ",", "c_0", "=", "hidden", "\n", "h_1", ",", "c_1", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "h_1_i", ",", "c_1_i", "=", "layer", "(", "input", ",", "(", "h_0", "[", "i", "]", ",", "c_0", "[", "i", "]", ")", ")", "\n", "input", "=", "h_1_i", "\n", "if", "i", "+", "1", "!=", "self", ".", "num_layers", ":", "\n", "                ", "input", "=", "self", ".", "dropout", "(", "input", ")", "\n", "", "h_1", "+=", "[", "h_1_i", "]", "\n", "c_1", "+=", "[", "c_1_i", "]", "\n", "\n", "", "h_1", "=", "torch", ".", "stack", "(", "h_1", ")", "\n", "c_1", "=", "torch", ".", "stack", "(", "c_1", ")", "\n", "\n", "return", "input", ",", "(", "h_1", ",", "c_1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StackedRNN.StackedGRU.__init__": [[39, 48], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "range", "StackedRNN.StackedGRU.layers.append", "torch.GRUCell", "torch.GRUCell"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_layers", ",", "input_size", ",", "rnn_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "StackedGRU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "nn", ".", "GRUCell", "(", "input_size", ",", "rnn_size", ")", ")", "\n", "input_size", "=", "rnn_size", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StackedRNN.StackedGRU.forward": [[49, 60], ["enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "layer", "StackedRNN.StackedGRU.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hidden", ")", ":", "\n", "        ", "h_1", "=", "[", "]", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "h_1_i", "=", "layer", "(", "input", ",", "hidden", "[", "0", "]", "[", "i", "]", ")", "\n", "input", "=", "h_1_i", "\n", "if", "i", "+", "1", "!=", "self", ".", "num_layers", ":", "\n", "                ", "input", "=", "self", ".", "dropout", "(", "input", ")", "\n", "", "h_1", "+=", "[", "h_1_i", "]", "\n", "\n", "", "h_1", "=", "torch", ".", "stack", "(", "h_1", ")", "\n", "return", "input", ",", "(", "h_1", ",", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.PositionwiseFeedForward.__init__": [[27, 36], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "onmt.modules.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "hidden_size", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "PositionwiseFeedForward", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "nn", ".", "Linear", "(", "size", ",", "hidden_size", ")", "\n", "self", ".", "w_2", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "size", ")", "\n", "self", ".", "layer_norm", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "size", ")", "\n", "# Save a little memory, by doing inplace.", "\n", "self", ".", "dropout_1", "=", "nn", ".", "Dropout", "(", "dropout", ",", "inplace", "=", "True", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "dropout_2", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.PositionwiseFeedForward.forward": [[37, 41], ["Transformer.PositionwiseFeedForward.dropout_1", "Transformer.PositionwiseFeedForward.dropout_2", "Transformer.PositionwiseFeedForward.relu", "Transformer.PositionwiseFeedForward.w_2", "Transformer.PositionwiseFeedForward.w_1", "Transformer.PositionwiseFeedForward.layer_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "inter", "=", "self", ".", "dropout_1", "(", "self", ".", "relu", "(", "self", ".", "w_1", "(", "self", ".", "layer_norm", "(", "x", ")", ")", ")", ")", "\n", "output", "=", "self", ".", "dropout_2", "(", "self", ".", "w_2", "(", "inter", ")", ")", "\n", "return", "output", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerEncoderLayer.__init__": [[56, 67], ["torch.Module.__init__", "onmt.modules.MultiHeadedAttention", "Transformer.PositionwiseFeedForward", "onmt.modules.LayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "dropout", ",", "\n", "head_count", "=", "8", ",", "hidden_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "self_attn", "=", "onmt", ".", "modules", ".", "MultiHeadedAttention", "(", "\n", "head_count", ",", "size", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "size", ",", "\n", "hidden_size", ",", "\n", "dropout", ")", "\n", "self", ".", "layer_norm", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerEncoderLayer.forward": [[68, 74], ["Transformer.TransformerEncoderLayer.layer_norm", "Transformer.TransformerEncoderLayer.self_attn", "Transformer.TransformerEncoderLayer.feed_forward", "Transformer.TransformerEncoderLayer.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "mask", ")", ":", "\n", "        ", "input_norm", "=", "self", ".", "layer_norm", "(", "inputs", ")", "\n", "context", ",", "_", "=", "self", ".", "self_attn", "(", "input_norm", ",", "input_norm", ",", "input_norm", ",", "\n", "mask", "=", "mask", ")", "\n", "out", "=", "self", ".", "dropout", "(", "context", ")", "+", "inputs", "\n", "return", "self", ".", "feed_forward", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerEncoder.__init__": [[101, 111], ["onmt.Models.EncoderBase.__init__", "torch.ModuleList", "torch.ModuleList", "onmt.modules.LayerNorm", "Transformer.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "hidden_size", ",", "\n", "dropout", ",", "embeddings", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "self", ".", "transformer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "TransformerEncoderLayer", "(", "hidden_size", ",", "dropout", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "self", ".", "layer_norm", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerEncoder.forward": [[112, 138], ["Transformer.TransformerEncoder._check_args", "Transformer.TransformerEncoder.embeddings", "Transformer.TransformerEncoder.size", "Transformer.TransformerEncoder.transpose().contiguous", "input[].transpose", "Transformer.TransformerEncoder.size", "input[].transpose.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "input[].transpose.data.eq().unsqueeze().expand", "range", "Transformer.TransformerEncoder.layer_norm", "torch.autograd.Variable", "torch.autograd.Variable", "Transformer.TransformerEncoder.transpose().contiguous", "Transformer.TransformerEncoder.transpose", "input[].transpose.data.eq().unsqueeze", "Transformer.TransformerEncoder.transpose", "input[].transpose.data.eq"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase._check_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "forward", "(", "self", ",", "input", ",", "lengths", "=", "None", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "\"\"\" See :obj:`EncoderBase.forward()`\"\"\"", "\n", "self", ".", "_check_args", "(", "input", ",", "lengths", ",", "hidden", ")", "\n", "\n", "emb", "=", "self", ".", "embeddings", "(", "input", ")", "\n", "s_len", ",", "n_batch", ",", "emb_dim", "=", "emb", ".", "size", "(", ")", "\n", "\n", "out", "=", "emb", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "words", "=", "input", "[", ":", ",", ":", ",", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", "\n", "# CHECKS", "\n", "out_batch", ",", "out_len", ",", "_", "=", "out", ".", "size", "(", ")", "\n", "w_batch", ",", "w_len", "=", "words", ".", "size", "(", ")", "\n", "aeq", "(", "out_batch", ",", "w_batch", ")", "\n", "aeq", "(", "out_len", ",", "w_len", ")", "\n", "# END CHECKS", "\n", "\n", "# Make mask.", "\n", "padding_idx", "=", "self", ".", "embeddings", ".", "word_padding_idx", "\n", "mask", "=", "words", ".", "data", ".", "eq", "(", "padding_idx", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "w_batch", ",", "w_len", ",", "w_len", ")", "\n", "# Run the forward pass of every layer of the tranformer.", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "out", "=", "self", ".", "transformer", "[", "i", "]", "(", "out", ",", "mask", ")", "\n", "", "out", "=", "self", ".", "layer_norm", "(", "out", ")", "\n", "\n", "return", "Variable", "(", "emb", ".", "data", ")", ",", "out", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderLayer.__init__": [[150, 168], ["torch.Module.__init__", "onmt.modules.MultiHeadedAttention", "onmt.modules.MultiHeadedAttention", "Transformer.PositionwiseFeedForward", "onmt.modules.LayerNorm", "onmt.modules.LayerNorm", "torch.Dropout", "torch.Dropout", "Transformer.TransformerDecoderLayer._get_attn_subsequent_mask", "Transformer.TransformerDecoderLayer.register_buffer"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderLayer._get_attn_subsequent_mask"], ["def", "__init__", "(", "self", ",", "size", ",", "dropout", ",", "\n", "head_count", "=", "8", ",", "hidden_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "onmt", ".", "modules", ".", "MultiHeadedAttention", "(", "\n", "head_count", ",", "size", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "context_attn", "=", "onmt", ".", "modules", ".", "MultiHeadedAttention", "(", "\n", "head_count", ",", "size", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "size", ",", "\n", "hidden_size", ",", "\n", "dropout", ")", "\n", "self", ".", "layer_norm_1", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "size", ")", "\n", "self", ".", "layer_norm_2", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "size", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "mask", "=", "self", ".", "_get_attn_subsequent_mask", "(", "MAX_SIZE", ")", "\n", "# Register self.mask as a buffer in TransformerDecoderLayer, so", "\n", "# it gets TransformerDecoderLayer's cuda behavior automatically.", "\n", "self", ".", "register_buffer", "(", "'mask'", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderLayer.forward": [[169, 215], ["inputs.size", "memory_bank.size", "onmt.Utils.aeq", "src_pad_mask.size", "tgt_pad_mask.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "torch.gt", "torch.gt", "torch.gt", "torch.gt", "Transformer.TransformerDecoderLayer.layer_norm_1", "Transformer.TransformerDecoderLayer.self_attn", "Transformer.TransformerDecoderLayer.layer_norm_2", "Transformer.TransformerDecoderLayer.context_attn", "Transformer.TransformerDecoderLayer.feed_forward", "Transformer.TransformerDecoderLayer.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "attn.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "previous_input.size", "onmt.Utils.aeq", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Transformer.TransformerDecoderLayer.drop", "Transformer.TransformerDecoderLayer.drop", "tgt_pad_mask.size", "tgt_pad_mask.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "memory_bank", ",", "src_pad_mask", ",", "tgt_pad_mask", ",", "\n", "previous_input", "=", "None", ")", ":", "\n", "# Args Checks", "\n", "        ", "input_batch", ",", "input_len", ",", "_", "=", "inputs", ".", "size", "(", ")", "\n", "if", "previous_input", "is", "not", "None", ":", "\n", "            ", "pi_batch", ",", "_", ",", "_", "=", "previous_input", ".", "size", "(", ")", "\n", "aeq", "(", "pi_batch", ",", "input_batch", ")", "\n", "", "contxt_batch", ",", "contxt_len", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "aeq", "(", "input_batch", ",", "contxt_batch", ")", "\n", "\n", "src_batch", ",", "t_len", ",", "s_len", "=", "src_pad_mask", ".", "size", "(", ")", "\n", "tgt_batch", ",", "t_len_", ",", "t_len__", "=", "tgt_pad_mask", ".", "size", "(", ")", "\n", "aeq", "(", "input_batch", ",", "contxt_batch", ",", "src_batch", ",", "tgt_batch", ")", "\n", "# aeq(t_len, t_len_, t_len__, input_len)", "\n", "aeq", "(", "s_len", ",", "contxt_len", ")", "\n", "# END Args Checks", "\n", "\n", "dec_mask", "=", "torch", ".", "gt", "(", "tgt_pad_mask", "+", "\n", "self", ".", "mask", "[", ":", ",", ":", "tgt_pad_mask", ".", "size", "(", "1", ")", ",", "\n", ":", "tgt_pad_mask", ".", "size", "(", "1", ")", "]", ",", "0", ")", "\n", "input_norm", "=", "self", ".", "layer_norm_1", "(", "inputs", ")", "\n", "all_input", "=", "input_norm", "\n", "if", "previous_input", "is", "not", "None", ":", "\n", "            ", "all_input", "=", "torch", ".", "cat", "(", "(", "previous_input", ",", "input_norm", ")", ",", "dim", "=", "1", ")", "\n", "dec_mask", "=", "None", "\n", "", "query", ",", "attn", "=", "self", ".", "self_attn", "(", "all_input", ",", "all_input", ",", "input_norm", ",", "\n", "mask", "=", "dec_mask", ")", "\n", "query", "=", "self", ".", "drop", "(", "query", ")", "+", "inputs", "\n", "\n", "query_norm", "=", "self", ".", "layer_norm_2", "(", "query", ")", "\n", "mid", ",", "attn", "=", "self", ".", "context_attn", "(", "memory_bank", ",", "memory_bank", ",", "query_norm", ",", "\n", "mask", "=", "src_pad_mask", ")", "\n", "output", "=", "self", ".", "feed_forward", "(", "self", ".", "drop", "(", "mid", ")", "+", "query", ")", "\n", "\n", "# CHECKS", "\n", "output_batch", ",", "output_len", ",", "_", "=", "output", ".", "size", "(", ")", "\n", "aeq", "(", "input_len", ",", "output_len", ")", "\n", "aeq", "(", "contxt_batch", ",", "output_batch", ")", "\n", "\n", "n_batch_", ",", "t_len_", ",", "s_len_", "=", "attn", ".", "size", "(", ")", "\n", "aeq", "(", "input_batch", ",", "n_batch_", ")", "\n", "aeq", "(", "contxt_len", ",", "s_len_", ")", "\n", "aeq", "(", "input_len", ",", "t_len_", ")", "\n", "# END CHECKS", "\n", "\n", "return", "output", ",", "attn", ",", "all_input", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderLayer._get_attn_subsequent_mask": [[216, 222], ["numpy.triu().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.triu", "numpy.ones"], "methods", ["None"], ["", "def", "_get_attn_subsequent_mask", "(", "self", ",", "size", ")", ":", "\n", "        ", "''' Get an attention mask to avoid using the subsequent info.'''", "\n", "attn_shape", "=", "(", "1", ",", "size", ",", "size", ")", "\n", "subsequent_mask", "=", "np", ".", "triu", "(", "np", ".", "ones", "(", "attn_shape", ")", ",", "k", "=", "1", ")", ".", "astype", "(", "'uint8'", ")", "\n", "subsequent_mask", "=", "torch", ".", "from_numpy", "(", "subsequent_mask", ")", "\n", "return", "subsequent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoder.__init__": [[252, 274], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "onmt.modules.LayerNorm", "onmt.modules.GlobalAttention", "Transformer.TransformerDecoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "hidden_size", ",", "attn_type", ",", "\n", "copy_attn", ",", "dropout", ",", "embeddings", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Basic attributes.", "\n", "self", ".", "decoder_type", "=", "'transformer'", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "# Build TransformerDecoder.", "\n", "self", ".", "transformer_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "TransformerDecoderLayer", "(", "hidden_size", ",", "dropout", ")", "\n", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "# TransformerDecoder has its own attention mechanism.", "\n", "# Set up a separated copy attention layer, if needed.", "\n", "self", ".", "_copy", "=", "False", "\n", "if", "copy_attn", ":", "\n", "            ", "self", ".", "copy_attn", "=", "onmt", ".", "modules", ".", "GlobalAttention", "(", "\n", "hidden_size", ",", "attn_type", "=", "attn_type", ")", "\n", "self", ".", "_copy", "=", "True", "\n", "", "self", ".", "layer_norm", "=", "onmt", ".", "modules", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoder.forward": [[275, 343], ["isinstance", "torch.cat.size", "torch.cat.size", "memory_bank.size", "onmt.Utils.aeq", "src[].transpose", "tgt[].transpose", "src[].transpose.size", "tgt[].transpose.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "Transformer.TransformerDecoder.embeddings", "Transformer.TransformerDecoder.transpose().contiguous", "memory_bank.transpose().contiguous", "src[].transpose.data.eq().unsqueeze().expand", "tgt[].transpose.data.eq().unsqueeze().expand", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "Transformer.TransformerDecoder.layer_norm", "Transformer.TransformerDecoder.transpose().contiguous", "attn.transpose().contiguous.transpose().contiguous.transpose().contiguous", "state.update_state.update_state.update_state", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Transformer.TransformerDecoder.dim", "torch.stack.append", "torch.stack.append", "Transformer.TransformerDecoder.transpose", "memory_bank.transpose", "src[].transpose.data.eq().unsqueeze", "tgt[].transpose.data.eq().unsqueeze", "Transformer.TransformerDecoder.transpose", "attn.transpose().contiguous.transpose().contiguous.transpose", "state.update_state.update_state.previous_input.size", "src[].transpose.data.eq", "tgt[].transpose.data.eq"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.update_state"], ["", "def", "forward", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See :obj:`onmt.modules.RNNDecoderBase.forward()`\n        \"\"\"", "\n", "# CHECKS", "\n", "assert", "isinstance", "(", "state", ",", "TransformerDecoderState", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "memory_len", ",", "memory_batch", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "memory_batch", ")", "\n", "\n", "src", "=", "state", ".", "src", "\n", "src_words", "=", "src", "[", ":", ",", ":", ",", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", "\n", "tgt_words", "=", "tgt", "[", ":", ",", ":", ",", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", "\n", "src_batch", ",", "src_len", "=", "src_words", ".", "size", "(", ")", "\n", "tgt_batch", ",", "tgt_len", "=", "tgt_words", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "memory_batch", ",", "src_batch", ",", "tgt_batch", ")", "\n", "aeq", "(", "memory_len", ",", "src_len", ")", "\n", "\n", "if", "state", ".", "previous_input", "is", "not", "None", ":", "\n", "            ", "tgt", "=", "torch", ".", "cat", "(", "[", "state", ".", "previous_input", ",", "tgt", "]", ",", "0", ")", "\n", "# END CHECKS", "\n", "\n", "# Initialize return variables.", "\n", "", "outputs", "=", "[", "]", "\n", "attns", "=", "{", "\"std\"", ":", "[", "]", "}", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "[", "]", "\n", "\n", "# Run the forward pass of the TransformerDecoder.", "\n", "", "emb", "=", "self", ".", "embeddings", "(", "tgt", ")", "\n", "if", "state", ".", "previous_input", "is", "not", "None", ":", "\n", "            ", "emb", "=", "emb", "[", "state", ".", "previous_input", ".", "size", "(", "0", ")", ":", ",", "]", "\n", "", "assert", "emb", ".", "dim", "(", ")", "==", "3", "# len x batch x embedding_dim", "\n", "\n", "output", "=", "emb", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "src_memory_bank", "=", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "padding_idx", "=", "self", ".", "embeddings", ".", "word_padding_idx", "\n", "src_pad_mask", "=", "src_words", ".", "data", ".", "eq", "(", "padding_idx", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "src_batch", ",", "tgt_len", ",", "src_len", ")", "\n", "tgt_pad_mask", "=", "tgt_words", ".", "data", ".", "eq", "(", "padding_idx", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "tgt_batch", ",", "tgt_len", ",", "tgt_len", ")", "\n", "\n", "saved_inputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "prev_layer_input", "=", "None", "\n", "if", "state", ".", "previous_input", "is", "not", "None", ":", "\n", "                ", "prev_layer_input", "=", "state", ".", "previous_layer_inputs", "[", "i", "]", "\n", "", "output", ",", "attn", ",", "all_input", "=", "self", ".", "transformer_layers", "[", "i", "]", "(", "output", ",", "src_memory_bank", ",", "\n", "src_pad_mask", ",", "tgt_pad_mask", ",", "\n", "previous_input", "=", "prev_layer_input", ")", "\n", "saved_inputs", ".", "append", "(", "all_input", ")", "\n", "\n", "", "saved_inputs", "=", "torch", ".", "stack", "(", "saved_inputs", ")", "\n", "output", "=", "self", ".", "layer_norm", "(", "output", ")", "\n", "\n", "# Process the result and update the attentions.", "\n", "outputs", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "attns", "[", "\"std\"", "]", "=", "attn", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "attn", "\n", "\n", "# Update the state.", "\n", "", "state", "=", "state", ".", "update_state", "(", "tgt", ",", "saved_inputs", ")", "\n", "return", "outputs", ",", "state", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoder.init_decoder_state": [[344, 346], ["Transformer.TransformerDecoderState"], "methods", ["None"], ["", "def", "init_decoder_state", "(", "self", ",", "src", ",", "memory_bank", ",", "enc_hidden", ")", ":", "\n", "        ", "return", "TransformerDecoderState", "(", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderState.__init__": [[349, 358], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "src", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src (FloatTensor): a sequence of source words tensors\n                    with optional feature tensors, of size (len x batch).\n        \"\"\"", "\n", "self", ".", "src", "=", "src", "\n", "self", ".", "previous_input", "=", "None", "\n", "self", ".", "previous_layer_inputs", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderState._all": [[359, 365], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_all", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Contains attributes that need to be updated in self.beam_update().\n        \"\"\"", "\n", "return", "(", "self", ".", "previous_input", ",", "self", ".", "previous_layer_inputs", ",", "self", ".", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderState.update_state": [[366, 372], ["Transformer.TransformerDecoderState"], "methods", ["None"], ["", "def", "update_state", "(", "self", ",", "input", ",", "previous_layer_inputs", ")", ":", "\n", "        ", "\"\"\" Called for every decoder forward pass. \"\"\"", "\n", "state", "=", "TransformerDecoderState", "(", "self", ".", "src", ")", "\n", "state", ".", "previous_input", "=", "input", "\n", "state", ".", "previous_layer_inputs", "=", "previous_layer_inputs", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Transformer.TransformerDecoderState.repeat_beam_size_times": [[373, 377], ["torch.autograd.Variable", "torch.autograd.Variable", "Transformer.TransformerDecoderState.src.data.repeat"], "methods", ["None"], ["", "def", "repeat_beam_size_times", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\" Repeat beam_size times along batch dimension. \"\"\"", "\n", "self", ".", "src", "=", "Variable", "(", "self", ".", "src", ".", "data", ".", "repeat", "(", "1", ",", "beam_size", ",", "1", ")", ",", "\n", "volatile", "=", "True", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ImageEncoder.ImageEncoder.__init__": [[18, 47], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Embedding", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "bidirectional", ",", "rnn_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "ImageEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "num_directions", "=", "2", "if", "bidirectional", "else", "1", "\n", "self", ".", "hidden_size", "=", "rnn_size", "\n", "\n", "self", ".", "layer1", "=", "nn", ".", "Conv2d", "(", "3", ",", "64", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "layer2", "=", "nn", ".", "Conv2d", "(", "64", ",", "128", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "layer3", "=", "nn", ".", "Conv2d", "(", "128", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "layer4", "=", "nn", ".", "Conv2d", "(", "256", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "layer5", "=", "nn", ".", "Conv2d", "(", "256", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "layer6", "=", "nn", ".", "Conv2d", "(", "512", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "batch_norm1", "=", "nn", ".", "BatchNorm2d", "(", "256", ")", "\n", "self", ".", "batch_norm2", "=", "nn", ".", "BatchNorm2d", "(", "512", ")", "\n", "self", ".", "batch_norm3", "=", "nn", ".", "BatchNorm2d", "(", "512", ")", "\n", "\n", "input_size", "=", "512", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", ",", "rnn_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "bidirectional", "=", "bidirectional", ")", "\n", "self", ".", "pos_lut", "=", "nn", ".", "Embedding", "(", "1000", ",", "input_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ImageEncoder.ImageEncoder.load_pretrained_vectors": [[48, 51], ["None"], "methods", ["None"], ["", "def", "load_pretrained_vectors", "(", "self", ",", "opt", ")", ":", "\n", "# Pass in needed options only when modify function definition.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ImageEncoder.ImageEncoder.forward": [[52, 108], ["torch.relu.size", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.relu", "torch.relu", "torch.relu", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ImageEncoder.ImageEncoder.layer1", "ImageEncoder.ImageEncoder.layer2", "ImageEncoder.ImageEncoder.batch_norm1", "ImageEncoder.ImageEncoder.layer4", "ImageEncoder.ImageEncoder.batch_norm2", "ImageEncoder.ImageEncoder.batch_norm3", "torch.relu.size", "input[].transpose().transpose", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "torch.Tensor().type_as().long().fill_", "ImageEncoder.ImageEncoder.pos_lut", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ImageEncoder.ImageEncoder.rnn", "all_outputs.append", "ImageEncoder.ImageEncoder.layer3", "ImageEncoder.ImageEncoder.layer5", "ImageEncoder.ImageEncoder.layer6", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "input[].transpose", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "torch.Tensor().type_as().long", "ImageEncoder.ImageEncoder.view", "ImageEncoder.ImageEncoder.size", "ImageEncoder.ImageEncoder.size", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor().type_as", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "lengths", "=", "None", ")", ":", "\n", "        ", "\"See :obj:`onmt.modules.EncoderBase.forward()`\"", "\n", "\n", "batch_size", "=", "input", ".", "size", "(", "0", ")", "\n", "# (batch_size, 64, imgH, imgW)", "\n", "# layer 1", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "layer1", "(", "input", "[", ":", ",", ":", ",", ":", ",", ":", "]", "-", "0.5", ")", ",", "True", ")", "\n", "\n", "# (batch_size, 64, imgH/2, imgW/2)", "\n", "input", "=", "F", ".", "max_pool2d", "(", "input", ",", "kernel_size", "=", "(", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "\n", "# (batch_size, 128, imgH/2, imgW/2)", "\n", "# layer 2", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "layer2", "(", "input", ")", ",", "True", ")", "\n", "\n", "# (batch_size, 128, imgH/2/2, imgW/2/2)", "\n", "input", "=", "F", ".", "max_pool2d", "(", "input", ",", "kernel_size", "=", "(", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "\n", "#  (batch_size, 256, imgH/2/2, imgW/2/2)", "\n", "# layer 3", "\n", "# batch norm 1", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "batch_norm1", "(", "self", ".", "layer3", "(", "input", ")", ")", ",", "True", ")", "\n", "\n", "# (batch_size, 256, imgH/2/2, imgW/2/2)", "\n", "# layer4", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "layer4", "(", "input", ")", ",", "True", ")", "\n", "\n", "# (batch_size, 256, imgH/2/2/2, imgW/2/2)", "\n", "input", "=", "F", ".", "max_pool2d", "(", "input", ",", "kernel_size", "=", "(", "1", ",", "2", ")", ",", "stride", "=", "(", "1", ",", "2", ")", ")", "\n", "\n", "# (batch_size, 512, imgH/2/2/2, imgW/2/2)", "\n", "# layer 5", "\n", "# batch norm 2", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "batch_norm2", "(", "self", ".", "layer5", "(", "input", ")", ")", ",", "True", ")", "\n", "\n", "# (batch_size, 512, imgH/2/2/2, imgW/2/2/2)", "\n", "input", "=", "F", ".", "max_pool2d", "(", "input", ",", "kernel_size", "=", "(", "2", ",", "1", ")", ",", "stride", "=", "(", "2", ",", "1", ")", ")", "\n", "\n", "# (batch_size, 512, imgH/2/2/2, imgW/2/2/2)", "\n", "input", "=", "F", ".", "relu", "(", "self", ".", "batch_norm3", "(", "self", ".", "layer6", "(", "input", ")", ")", ",", "True", ")", "\n", "\n", "# # (batch_size, 512, H, W)", "\n", "all_outputs", "=", "[", "]", "\n", "for", "row", "in", "range", "(", "input", ".", "size", "(", "2", ")", ")", ":", "\n", "            ", "inp", "=", "input", "[", ":", ",", ":", ",", "row", ",", ":", "]", ".", "transpose", "(", "0", ",", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "row_vec", "=", "torch", ".", "Tensor", "(", "batch_size", ")", ".", "type_as", "(", "inp", ".", "data", ")", ".", "long", "(", ")", ".", "fill_", "(", "row", ")", "\n", "pos_emb", "=", "self", ".", "pos_lut", "(", "Variable", "(", "row_vec", ")", ")", "\n", "with_pos", "=", "torch", ".", "cat", "(", "\n", "(", "pos_emb", ".", "view", "(", "1", ",", "pos_emb", ".", "size", "(", "0", ")", ",", "pos_emb", ".", "size", "(", "1", ")", ")", ",", "inp", ")", ",", "0", ")", "\n", "outputs", ",", "hidden_t", "=", "self", ".", "rnn", "(", "with_pos", ")", "\n", "all_outputs", ".", "append", "(", "outputs", ")", "\n", "", "out", "=", "torch", ".", "cat", "(", "all_outputs", ",", "0", ")", "\n", "\n", "return", "hidden_t", ",", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.GatedConv.__init__": [[26, 33], ["torch.Module.__init__", "onmt.modules.WeightNorm.WeightNormConv2d", "torch.xavier_uniform", "torch.xavier_uniform", "torch.xavier_uniform", "torch.xavier_uniform", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "width", "=", "3", ",", "dropout", "=", "0.2", ",", "nopad", "=", "False", ")", ":", "\n", "        ", "super", "(", "GatedConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "WeightNormConv2d", "(", "input_size", ",", "2", "*", "input_size", ",", "\n", "kernel_size", "=", "(", "width", ",", "1", ")", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "width", "//", "2", "*", "(", "1", "-", "nopad", ")", ",", "0", ")", ")", "\n", "init", ".", "xavier_uniform", "(", "self", ".", "conv", ".", "weight", ",", "gain", "=", "(", "4", "*", "(", "1", "-", "dropout", ")", ")", "**", "0.5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.GatedConv.forward": [[34, 40], ["Conv2Conv.GatedConv.dropout", "Conv2Conv.GatedConv.conv", "Conv2Conv.GatedConv.split", "int", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "Conv2Conv.GatedConv.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x_var", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "x_var", "=", "self", ".", "dropout", "(", "x_var", ")", "\n", "x_var", "=", "self", ".", "conv", "(", "x_var", ")", "\n", "out", ",", "gate", "=", "x_var", ".", "split", "(", "int", "(", "x_var", ".", "size", "(", "1", ")", "/", "2", ")", ",", "1", ")", "\n", "out", "=", "out", "*", "F", ".", "sigmoid", "(", "gate", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.StackedCNN.__init__": [[43, 52], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "Conv2Conv.StackedCNN.layers.append", "Conv2Conv.GatedConv"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_layers", ",", "input_size", ",", "cnn_kernel_width", "=", "3", ",", "\n", "dropout", "=", "0.2", ")", ":", "\n", "        ", "super", "(", "StackedCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "\n", "GatedConv", "(", "input_size", ",", "cnn_kernel_width", ",", "dropout", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.StackedCNN.forward": [[53, 58], ["conv"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "for", "conv", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "x", "+", "conv", "(", "x", ")", "\n", "x", "*=", "SCALE_WEIGHT", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNEncoder.__init__": [[65, 74], ["onmt.Models.EncoderBase.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "Conv2Conv.StackedCNN"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "hidden_size", ",", "\n", "cnn_kernel_width", ",", "dropout", ",", "embeddings", ")", ":", "\n", "        ", "super", "(", "CNNEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "input_size", "=", "embeddings", ".", "embedding_size", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", "\n", "self", ".", "cnn", "=", "StackedCNN", "(", "num_layers", ",", "hidden_size", ",", "\n", "cnn_kernel_width", ",", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNEncoder.forward": [[75, 91], ["Conv2Conv.CNNEncoder._check_args", "Conv2Conv.CNNEncoder.embeddings", "emb.transpose().contiguous.transpose().contiguous.size", "emb.transpose().contiguous.transpose().contiguous.transpose().contiguous", "emb.transpose().contiguous.transpose().contiguous.view", "Conv2Conv.CNNEncoder.linear", "shape_transform.view", "Conv2Conv.shape_transform", "Conv2Conv.CNNEncoder.cnn", "emb.transpose().contiguous.transpose().contiguous.size", "emb.transpose().contiguous.transpose().contiguous.size", "shape_transform.squeeze().transpose().contiguous", "Conv2Conv.CNNEncoder.squeeze().transpose().contiguous", "emb.transpose().contiguous.transpose().contiguous.transpose", "emb.transpose().contiguous.transpose().contiguous.size", "emb.transpose().contiguous.transpose().contiguous.size", "shape_transform.squeeze().transpose", "Conv2Conv.CNNEncoder.squeeze().transpose", "shape_transform.squeeze", "Conv2Conv.CNNEncoder.squeeze"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Models.EncoderBase._check_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.shape_transform"], ["", "def", "forward", "(", "self", ",", "input", ",", "lengths", "=", "None", ",", "hidden", "=", "None", ")", ":", "\n", "        ", "\"\"\" See :obj:`onmt.modules.EncoderBase.forward()`\"\"\"", "\n", "self", ".", "_check_args", "(", "input", ",", "lengths", ",", "hidden", ")", "\n", "\n", "emb", "=", "self", ".", "embeddings", "(", "input", ")", "\n", "s_len", ",", "batch", ",", "emb_dim", "=", "emb", ".", "size", "(", ")", "\n", "\n", "emb", "=", "emb", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "emb_reshape", "=", "emb", ".", "view", "(", "emb", ".", "size", "(", "0", ")", "*", "emb", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "emb_remap", "=", "self", ".", "linear", "(", "emb_reshape", ")", "\n", "emb_remap", "=", "emb_remap", ".", "view", "(", "emb", ".", "size", "(", "0", ")", ",", "emb", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "emb_remap", "=", "shape_transform", "(", "emb_remap", ")", "\n", "out", "=", "self", ".", "cnn", "(", "emb_remap", ")", "\n", "\n", "return", "emb_remap", ".", "squeeze", "(", "3", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ",", "out", ".", "squeeze", "(", "3", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.__init__": [[100, 133], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "Conv2Conv.CNNDecoder.conv_layers.append", "Conv2Conv.CNNDecoder.attn_layers.append", "onmt.modules.GlobalAttention", "Conv2Conv.GatedConv", "onmt.modules.ConvMultiStepAttention"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "hidden_size", ",", "attn_type", ",", "\n", "copy_attn", ",", "cnn_kernel_width", ",", "dropout", ",", "embeddings", ")", ":", "\n", "        ", "super", "(", "CNNDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Basic attributes.", "\n", "self", ".", "decoder_type", "=", "'cnn'", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "cnn_kernel_width", "=", "cnn_kernel_width", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "# Build the CNN.", "\n", "input_size", "=", "self", ".", "embeddings", ".", "embedding_size", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "hidden_size", ")", "\n", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "self", ".", "conv_layers", ".", "append", "(", "\n", "GatedConv", "(", "self", ".", "hidden_size", ",", "self", ".", "cnn_kernel_width", ",", "\n", "self", ".", "dropout", ",", "True", ")", ")", "\n", "\n", "", "self", ".", "attn_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "self", ".", "attn_layers", ".", "append", "(", "\n", "onmt", ".", "modules", ".", "ConvMultiStepAttention", "(", "self", ".", "hidden_size", ")", ")", "\n", "\n", "# CNNDecoder has its own attention mechanism.", "\n", "# Set up a separated copy attention layer, if needed.", "\n", "", "self", ".", "_copy", "=", "False", "\n", "if", "copy_attn", ":", "\n", "            ", "self", ".", "copy_attn", "=", "onmt", ".", "modules", ".", "GlobalAttention", "(", "\n", "hidden_size", ",", "attn_type", "=", "attn_type", ")", "\n", "self", ".", "_copy", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.forward": [[134, 196], ["isinstance", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "memory_bank.size", "onmt.Utils.aeq", "Conv2Conv.CNNDecoder.embeddings", "Conv2Conv.CNNDecoder.transpose().contiguous", "memory_bank.transpose().contiguous", "state.init_src.transpose().contiguous", "Conv2Conv.CNNDecoder.transpose().contiguous.contiguous().view", "Conv2Conv.CNNDecoder.linear", "Conv2Conv.CNNDecoder.view", "Conv2Conv.shape_transform", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "pad.type_as.type_as.type_as", "zip", "shape_transform.squeeze().transpose", "shape_transform.squeeze().transpose.transpose().contiguous", "state.update_state", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Conv2Conv.CNNDecoder.dim", "Conv2Conv.CNNDecoder.transpose().contiguous.size", "Conv2Conv.CNNDecoder.transpose().contiguous.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "conv", "attention", "attn[].squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "Conv2Conv.CNNDecoder.transpose", "memory_bank.transpose", "state.init_src.transpose", "Conv2Conv.CNNDecoder.transpose().contiguous.contiguous", "Conv2Conv.CNNDecoder.transpose().contiguous.size", "Conv2Conv.CNNDecoder.transpose().contiguous.size", "shape_transform.size", "shape_transform.size", "shape_transform.squeeze", "shape_transform.squeeze().transpose.transpose", "state.previous_input.size", "state.previous_input.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.shape_transform", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.update_state"], ["", "", "def", "forward", "(", "self", ",", "tgt", ",", "memory_bank", ",", "state", ",", "memory_lengths", "=", "None", ")", ":", "\n", "        ", "\"\"\" See :obj:`onmt.modules.RNNDecoderBase.forward()`\"\"\"", "\n", "# CHECKS", "\n", "assert", "isinstance", "(", "state", ",", "CNNDecoderState", ")", "\n", "tgt_len", ",", "tgt_batch", ",", "_", "=", "tgt", ".", "size", "(", ")", "\n", "contxt_len", ",", "contxt_batch", ",", "_", "=", "memory_bank", ".", "size", "(", ")", "\n", "aeq", "(", "tgt_batch", ",", "contxt_batch", ")", "\n", "# END CHECKS", "\n", "\n", "if", "state", ".", "previous_input", "is", "not", "None", ":", "\n", "            ", "tgt", "=", "torch", ".", "cat", "(", "[", "state", ".", "previous_input", ",", "tgt", "]", ",", "0", ")", "\n", "\n", "# Initialize return variables.", "\n", "", "outputs", "=", "[", "]", "\n", "attns", "=", "{", "\"std\"", ":", "[", "]", "}", "\n", "assert", "not", "self", ".", "_copy", ",", "\"Copy mechanism not yet tested in conv2conv\"", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "[", "]", "\n", "\n", "", "emb", "=", "self", ".", "embeddings", "(", "tgt", ")", "\n", "assert", "emb", ".", "dim", "(", ")", "==", "3", "# len x batch x embedding_dim", "\n", "\n", "tgt_emb", "=", "emb", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# The output of CNNEncoder.", "\n", "src_memory_bank_t", "=", "memory_bank", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# The combination of output of CNNEncoder and source embeddings.", "\n", "src_memory_bank_c", "=", "state", ".", "init_src", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Run the forward pass of the CNNDecoder.", "\n", "emb_reshape", "=", "tgt_emb", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "tgt_emb", ".", "size", "(", "0", ")", "*", "tgt_emb", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "linear_out", "=", "self", ".", "linear", "(", "emb_reshape", ")", "\n", "x", "=", "linear_out", ".", "view", "(", "tgt_emb", ".", "size", "(", "0", ")", ",", "tgt_emb", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "x", "=", "shape_transform", "(", "x", ")", "\n", "\n", "pad", "=", "Variable", "(", "torch", ".", "zeros", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ",", "\n", "self", ".", "cnn_kernel_width", "-", "1", ",", "1", ")", ")", "\n", "pad", "=", "pad", ".", "type_as", "(", "x", ")", "\n", "base_target_emb", "=", "x", "\n", "\n", "for", "conv", ",", "attention", "in", "zip", "(", "self", ".", "conv_layers", ",", "self", ".", "attn_layers", ")", ":", "\n", "            ", "new_target_input", "=", "torch", ".", "cat", "(", "[", "pad", ",", "x", "]", ",", "2", ")", "\n", "out", "=", "conv", "(", "new_target_input", ")", "\n", "c", ",", "attn", "=", "attention", "(", "base_target_emb", ",", "out", ",", "\n", "src_memory_bank_t", ",", "src_memory_bank_c", ")", "\n", "x", "=", "(", "x", "+", "(", "c", "+", "out", ")", "*", "SCALE_WEIGHT", ")", "*", "SCALE_WEIGHT", "\n", "", "output", "=", "x", ".", "squeeze", "(", "3", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# Process the result and update the attentions.", "\n", "outputs", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "if", "state", ".", "previous_input", "is", "not", "None", ":", "\n", "            ", "outputs", "=", "outputs", "[", "state", ".", "previous_input", ".", "size", "(", "0", ")", ":", "]", "\n", "attn", "=", "attn", "[", ":", ",", "state", ".", "previous_input", ".", "size", "(", "0", ")", ":", "]", ".", "squeeze", "(", ")", "\n", "attn", "=", "torch", ".", "stack", "(", "[", "attn", "]", ")", "\n", "", "attns", "[", "\"std\"", "]", "=", "attn", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "attn", "\n", "\n", "# Update the state.", "\n", "", "state", ".", "update_state", "(", "tgt", ")", "\n", "\n", "return", "outputs", ",", "state", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.init_decoder_state": [[197, 199], ["Conv2Conv.CNNDecoderState"], "methods", ["None"], ["", "def", "init_decoder_state", "(", "self", ",", "src", ",", "memory_bank", ",", "enc_hidden", ")", ":", "\n", "        ", "return", "CNNDecoderState", "(", "memory_bank", ",", "enc_hidden", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.__init__": [[202, 205], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "memory_bank", ",", "enc_hidden", ")", ":", "\n", "        ", "self", ".", "init_src", "=", "(", "memory_bank", "+", "enc_hidden", ")", "*", "SCALE_WEIGHT", "\n", "self", ".", "previous_input", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState._all": [[206, 212], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_all", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Contains attributes that need to be updated in self.beam_update().\n        \"\"\"", "\n", "return", "(", "self", ".", "previous_input", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.update_state": [[213, 216], ["None"], "methods", ["None"], ["", "def", "update_state", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\" Called for every decoder forward pass. \"\"\"", "\n", "self", ".", "previous_input", "=", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoderState.repeat_beam_size_times": [[217, 221], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "Conv2Conv.CNNDecoderState.init_src.data.repeat"], "methods", ["None"], ["", "def", "repeat_beam_size_times", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\" Repeat beam_size times along batch dimension. \"\"\"", "\n", "self", ".", "init_src", "=", "Variable", "(", "\n", "self", ".", "init_src", ".", "data", ".", "repeat", "(", "1", ",", "beam_size", ",", "1", ")", ",", "volatile", "=", "True", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.shape_transform": [[20, 23], ["torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose"], "function", ["None"], ["def", "shape_transform", "(", "x", ")", ":", "\n", "    ", "\"\"\" Tranform the size of the tensors to fit for conv input. \"\"\"", "\n", "return", "torch", ".", "unsqueeze", "(", "torch", ".", "transpose", "(", "x", ",", "1", ",", "2", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StructuredAttention.MatrixTree.__init__": [[15, 18], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "eps", "=", "1e-5", ")", ":", "\n", "        ", "self", ".", "eps", "=", "eps", "\n", "super", "(", "MatrixTree", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.StructuredAttention.MatrixTree.forward": [[19, 41], ["input.clone", "range", "input.exp", "input.size", "laplacian[].masked_fill", "input[].diag().exp", "laplacian[].masked_fill.inverse", "laplacian[].masked_fill.inverse.diag().unsqueeze().expand_as().transpose", "input[].exp().mul().clone", "input[].exp().mul().clone", "input[].diag().exp().mul", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "torch.eye().cuda().ne", "laplacian[].masked_fill.sum", "input[].diag", "laplacian[].masked_fill.inverse.diag().unsqueeze().expand_as", "input[].exp().mul", "input[].exp().mul", "input[].diag().exp", "laplacian[].masked_fill.inverse.transpose", "laplacian[].masked_fill.inverse.transpose", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "torch.eye().cuda", "laplacian[].masked_fill.inverse.diag().unsqueeze", "input[].exp", "input[].exp", "input[].diag", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "laplacian[].masked_fill.inverse.diag", "input.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "laplacian", "=", "input", ".", "exp", "(", ")", "+", "self", ".", "eps", "\n", "output", "=", "input", ".", "clone", "(", ")", "\n", "for", "b", "in", "range", "(", "input", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "lap", "=", "laplacian", "[", "b", "]", ".", "masked_fill", "(", "\n", "Variable", "(", "torch", ".", "eye", "(", "input", ".", "size", "(", "1", ")", ")", ".", "cuda", "(", ")", ".", "ne", "(", "0", ")", ")", ",", "0", ")", "\n", "lap", "=", "-", "lap", "+", "torch", ".", "diag", "(", "lap", ".", "sum", "(", "0", ")", ")", "\n", "# store roots on diagonal", "\n", "lap", "[", "0", "]", "=", "input", "[", "b", "]", ".", "diag", "(", ")", ".", "exp", "(", ")", "\n", "inv_laplacian", "=", "lap", ".", "inverse", "(", ")", "\n", "\n", "factor", "=", "inv_laplacian", ".", "diag", "(", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "input", "[", "b", "]", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "term1", "=", "input", "[", "b", "]", ".", "exp", "(", ")", ".", "mul", "(", "factor", ")", ".", "clone", "(", ")", "\n", "term2", "=", "input", "[", "b", "]", ".", "exp", "(", ")", ".", "mul", "(", "inv_laplacian", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "clone", "(", ")", "\n", "term1", "[", ":", ",", "0", "]", "=", "0", "\n", "term2", "[", "0", "]", "=", "0", "\n", "output", "[", "b", "]", "=", "term1", "-", "term2", "\n", "roots_output", "=", "input", "[", "b", "]", ".", "diag", "(", ")", ".", "exp", "(", ")", ".", "mul", "(", "\n", "inv_laplacian", ".", "transpose", "(", "0", ",", "1", ")", "[", "0", "]", ")", "\n", "output", "[", "b", "]", "=", "output", "[", "b", "]", "+", "torch", ".", "diag", "(", "roots_output", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.AudioEncoder.AudioEncoder.__init__": [[20, 42], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "int", "int", "int", "torch.LSTM", "torch.LSTM", "math.floor", "math.floor", "math.floor"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "bidirectional", ",", "rnn_size", ",", "dropout", ",", "\n", "sample_rate", ",", "window_size", ")", ":", "\n", "        ", "super", "(", "AudioEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "num_directions", "=", "2", "if", "bidirectional", "else", "1", "\n", "self", ".", "hidden_size", "=", "rnn_size", "\n", "\n", "self", ".", "layer1", "=", "nn", ".", "Conv2d", "(", "1", ",", "32", ",", "kernel_size", "=", "(", "41", ",", "11", ")", ",", "\n", "padding", "=", "(", "0", ",", "10", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "self", ".", "batch_norm1", "=", "nn", ".", "BatchNorm2d", "(", "32", ")", "\n", "self", ".", "layer2", "=", "nn", ".", "Conv2d", "(", "32", ",", "32", ",", "kernel_size", "=", "(", "21", ",", "11", ")", ",", "\n", "padding", "=", "(", "0", ",", "0", ")", ",", "stride", "=", "(", "2", ",", "1", ")", ")", "\n", "self", ".", "batch_norm2", "=", "nn", ".", "BatchNorm2d", "(", "32", ")", "\n", "\n", "input_size", "=", "int", "(", "math", ".", "floor", "(", "(", "sample_rate", "*", "window_size", ")", "/", "2", ")", "+", "1", ")", "\n", "input_size", "=", "int", "(", "math", ".", "floor", "(", "input_size", "-", "41", ")", "/", "2", "+", "1", ")", "\n", "input_size", "=", "int", "(", "math", ".", "floor", "(", "input_size", "-", "21", ")", "/", "2", "+", "1", ")", "\n", "input_size", "*=", "32", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "input_size", ",", "rnn_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "bidirectional", "=", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.AudioEncoder.AudioEncoder.load_pretrained_vectors": [[43, 46], ["None"], "methods", ["None"], ["", "def", "load_pretrained_vectors", "(", "self", ",", "opt", ")", ":", "\n", "# Pass in needed options only when modify function definition.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.AudioEncoder.AudioEncoder.forward": [[47, 71], ["AudioEncoder.AudioEncoder.batch_norm1", "torch.hardtanh", "torch.hardtanh", "AudioEncoder.AudioEncoder.batch_norm2", "torch.hardtanh", "torch.hardtanh", "input.transpose().transpose.transpose().transpose.size", "input.transpose().transpose.transpose().transpose.size", "input.transpose().transpose.transpose().transpose.view", "input.transpose().transpose.transpose().transpose.transpose().transpose", "AudioEncoder.AudioEncoder.rnn", "AudioEncoder.AudioEncoder.layer1", "AudioEncoder.AudioEncoder.layer2", "input.transpose().transpose.transpose().transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "lengths", "=", "None", ")", ":", "\n", "        ", "\"See :obj:`onmt.modules.EncoderBase.forward()`\"", "\n", "# (batch_size, 1, nfft, t)", "\n", "# layer 1", "\n", "input", "=", "self", ".", "batch_norm1", "(", "self", ".", "layer1", "(", "input", "[", ":", ",", ":", ",", ":", ",", ":", "]", ")", ")", "\n", "\n", "# (batch_size, 32, nfft/2, t/2)", "\n", "input", "=", "F", ".", "hardtanh", "(", "input", ",", "0", ",", "20", ",", "inplace", "=", "True", ")", "\n", "\n", "# (batch_size, 32, nfft/2/2, t/2)", "\n", "# layer 2", "\n", "input", "=", "self", ".", "batch_norm2", "(", "self", ".", "layer2", "(", "input", ")", ")", "\n", "\n", "# (batch_size, 32, nfft/2/2, t/2)", "\n", "input", "=", "F", ".", "hardtanh", "(", "input", ",", "0", ",", "20", ",", "inplace", "=", "True", ")", "\n", "\n", "batch_size", "=", "input", ".", "size", "(", "0", ")", "\n", "length", "=", "input", ".", "size", "(", "3", ")", "\n", "input", "=", "input", ".", "view", "(", "batch_size", ",", "-", "1", ",", "length", ")", "\n", "input", "=", "input", ".", "transpose", "(", "0", ",", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "input", ")", "\n", "\n", "return", "hidden", ",", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.__init__": [[31, 72], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "src_dim", ",", "tgt_dim", ",", "\n", "attn_dim", ",", "\n", "temperature", ",", "\n", "p_dist_type", "=", "\"categorical\"", ",", "\n", "q_dist_type", "=", "\"categorical\"", ",", "\n", "use_prior", "=", "False", ",", "\n", "scoresF", "=", "F", ".", "softplus", ",", "\n", "n_samples", "=", "1", ",", "\n", "mode", "=", "\"sample\"", ",", "\n", "attn_type", "=", "\"mlp\"", ",", "\n", ")", ":", "\n", "        ", "super", "(", "VariationalAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "src_dim", "=", "src_dim", "\n", "self", ".", "tgt_dim", "=", "tgt_dim", "\n", "self", ".", "attn_dim", "=", "attn_dim", "\n", "self", ".", "p_dist_type", "=", "p_dist_type", "\n", "self", ".", "q_dist_tyqe", "=", "q_dist_type", "\n", "self", ".", "use_prior", "=", "use_prior", "\n", "self", ".", "scoresF", "=", "scoresF", "\n", "self", ".", "n_samples", "=", "n_samples", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "attn_type", "=", "attn_type", "\n", "self", ".", "dim", "=", "attn_dim", "\n", "dim", "=", "self", ".", "dim", "\n", "self", ".", "k", "=", "0", "\n", "self", ".", "temperature", "=", "temperature", "\n", "\n", "if", "self", ".", "attn_type", "==", "\"general\"", ":", "\n", "            ", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "tgt_dim", ",", "src_dim", ",", "bias", "=", "False", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "\"mlp\"", ":", "\n", "            ", "self", ".", "linear_context", "=", "nn", ".", "Linear", "(", "src_dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_query", "=", "nn", ".", "Linear", "(", "tgt_dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "v", "=", "nn", ".", "Linear", "(", "dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "# mlp wants it with bias", "\n", "", "out_bias", "=", "self", ".", "attn_type", "==", "\"mlp\"", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "src_dim", "+", "tgt_dim", ",", "tgt_dim", ",", "bias", "=", "out_bias", ")", "\n", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.score": [[73, 112], ["h_s.size", "VariationalAttention.VariationalAttention.view.size", "onmt.Utils.aeq", "VariationalAttention.VariationalAttention.view.view", "VariationalAttention.VariationalAttention.linear_in", "VariationalAttention.VariationalAttention.view", "h_s.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "VariationalAttention.VariationalAttention.linear_query", "wq.expand.expand.view", "wq.expand.expand.expand", "VariationalAttention.VariationalAttention.linear_context", "uh.expand.expand.view", "uh.expand.expand.expand", "VariationalAttention.VariationalAttention.tanh", "VariationalAttention.VariationalAttention.v().view", "VariationalAttention.VariationalAttention.view.view", "h_s.contiguous().view", "VariationalAttention.VariationalAttention.v", "h_s.contiguous", "VariationalAttention.VariationalAttention.view"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "score", "(", "self", ",", "h_t", ",", "h_s", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n          h_t (`FloatTensor`): sequence of queries `[batch x tgt_len x dim]`\n          h_s (`FloatTensor`): sequence of sources `[batch x src_len x dim]`\n\n        Returns:\n          :obj:`FloatTensor`:\n           raw attention scores (unnormalized) for each src index\n          `[batch x tgt_len x src_len]`\n\n        \"\"\"", "\n", "\n", "# Check input sizes", "\n", "src_batch", ",", "src_len", ",", "src_dim", "=", "h_s", ".", "size", "(", ")", "\n", "tgt_batch", ",", "tgt_len", ",", "tgt_dim", "=", "h_t", ".", "size", "(", ")", "\n", "aeq", "(", "src_batch", ",", "tgt_batch", ")", "\n", "\n", "if", "self", ".", "attn_type", "==", "\"general\"", ":", "\n", "            ", "h_t_", "=", "h_t", ".", "view", "(", "tgt_batch", "*", "tgt_len", ",", "tgt_dim", ")", "\n", "h_t_", "=", "self", ".", "linear_in", "(", "h_t_", ")", "\n", "h_t", "=", "h_t_", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "src_dim", ")", "\n", "h_s_", "=", "h_s", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# (batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)", "\n", "return", "torch", ".", "bmm", "(", "h_t", ",", "h_s_", ")", "\n", "", "elif", "self", ".", "attn_type", "==", "\"mlp\"", ":", "\n", "            ", "dim", "=", "self", ".", "dim", "\n", "wq", "=", "self", ".", "linear_query", "(", "h_t", ".", "view", "(", "-", "1", ",", "self", ".", "tgt_dim", ")", ")", "\n", "wq", "=", "wq", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "1", ",", "dim", ")", "\n", "wq", "=", "wq", ".", "expand", "(", "tgt_batch", ",", "tgt_len", ",", "src_len", ",", "dim", ")", "\n", "\n", "uh", "=", "self", ".", "linear_context", "(", "h_s", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "src_dim", ")", ")", "\n", "uh", "=", "uh", ".", "view", "(", "src_batch", ",", "1", ",", "src_len", ",", "dim", ")", "\n", "uh", "=", "uh", ".", "expand", "(", "src_batch", ",", "tgt_len", ",", "src_len", ",", "dim", ")", "\n", "\n", "# (batch, t_len, s_len, d)", "\n", "wquh", "=", "self", ".", "tanh", "(", "wq", "+", "uh", ")", "\n", "\n", "return", "self", ".", "v", "(", "wquh", ".", "view", "(", "-", "1", ",", "dim", ")", ")", ".", "view", "(", "tgt_batch", ",", "tgt_len", ",", "src_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn": [[113, 137], ["alpha.size", "alpha.size", "alpha.size", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "attns.to.to.scatter_", "attns.to.to.to", "log_alpha.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "log_alpha.unsqueeze().expand.unsqueeze().expand.gather().squeeze", "Exception", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "log_alpha.unsqueeze().expand.unsqueeze().expand.unsqueeze", "log_alpha.unsqueeze().expand.unsqueeze().expand.gather", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "alpha.view"], "methods", ["None"], ["", "", "def", "sample_attn", "(", "self", ",", "params", ",", "n_samples", "=", "1", ",", "lengths", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "dist_type", "=", "params", ".", "dist_type", "\n", "if", "dist_type", "==", "\"categorical\"", ":", "\n", "            ", "alpha", "=", "params", ".", "alpha", "\n", "log_alpha", "=", "params", ".", "log_alpha", "\n", "K", "=", "n_samples", "\n", "N", "=", "alpha", ".", "size", "(", "0", ")", "\n", "T", "=", "alpha", ".", "size", "(", "1", ")", "\n", "S", "=", "alpha", ".", "size", "(", "2", ")", "\n", "attns_id", "=", "torch", ".", "distributions", ".", "categorical", ".", "Categorical", "(", "\n", "alpha", ".", "view", "(", "N", "*", "T", ",", "S", ")", "\n", ")", ".", "sample", "(", "\n", "torch", ".", "Size", "(", "[", "n_samples", "]", ")", "\n", ")", ".", "view", "(", "K", ",", "N", ",", "T", ",", "1", ")", "\n", "attns", "=", "torch", ".", "Tensor", "(", "K", ",", "N", ",", "T", ",", "S", ")", ".", "zero_", "(", ")", ".", "cuda", "(", ")", "\n", "attns", ".", "scatter_", "(", "3", ",", "attns_id", ",", "1", ")", "\n", "attns", "=", "attns", ".", "to", "(", "alpha", ")", "\n", "# log alpha: K, N, T, S", "\n", "log_alpha", "=", "log_alpha", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "K", ",", "N", ",", "T", ",", "S", ")", "\n", "sample_log_probs", "=", "log_alpha", ".", "gather", "(", "3", ",", "attns_id", ".", "to", "(", "log_alpha", ".", "device", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "return", "attns", ",", "sample_log_probs", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unsupported dist\"", ")", "\n", "", "return", "attns", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn_gumbel": [[138, 154], ["alpha.size", "alpha.size", "alpha.size", "VariationalAttention.gumbel_softmax_sample", "log_alpha.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "Exception", "log_alpha.unsqueeze().expand.unsqueeze().expand.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.gumbel_softmax_sample"], ["", "def", "sample_attn_gumbel", "(", "self", ",", "params", ",", "temperature", ",", "n_samples", "=", "1", ",", "lengths", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "dist_type", "=", "params", ".", "dist_type", "\n", "if", "dist_type", "==", "\"categorical\"", ":", "\n", "            ", "alpha", "=", "params", ".", "alpha", "\n", "log_alpha", "=", "params", ".", "log_alpha", "\n", "K", "=", "n_samples", "\n", "N", "=", "alpha", ".", "size", "(", "0", ")", "\n", "T", "=", "alpha", ".", "size", "(", "1", ")", "\n", "S", "=", "alpha", ".", "size", "(", "2", ")", "\n", "attns", "=", "gumbel_softmax_sample", "(", "log_alpha", ",", "K", ",", "temperature", ")", "# K, N, T, S", "\n", "# log alpha: K, N, T, S", "\n", "log_alpha", "=", "log_alpha", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "K", ",", "N", ",", "T", ",", "S", ")", "\n", "return", "attns", ",", "None", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unsupported dist\"", ")", "\n", "", "return", "attns", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn_wsram": [[155, 183], ["alpha_q.size", "alpha_q.size", "alpha_q.size", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.distributions.categorical.Categorical().sample().view", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda", "torch.Tensor().zero_().cuda.scatter_", "torch.Tensor().zero_().cuda.scatter_", "torch.Tensor().zero_().cuda.scatter_", "torch.Tensor().zero_().cuda.to", "torch.Tensor().zero_().cuda.to", "torch.Tensor().zero_().cuda.to", "log_alpha_q.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "log_alpha_q.unsqueeze().expand.unsqueeze().expand.gather().squeeze", "log_alpha_p.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "log_alpha_p.unsqueeze().expand.unsqueeze().expand.gather().squeeze", "Exception", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.distributions.categorical.Categorical().sample", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "torch.Tensor().zero_", "log_alpha_q.unsqueeze().expand.unsqueeze().expand.unsqueeze", "log_alpha_q.unsqueeze().expand.unsqueeze().expand.gather", "log_alpha_p.unsqueeze().expand.unsqueeze().expand.unsqueeze", "log_alpha_p.unsqueeze().expand.unsqueeze().expand.gather", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical().sample().view.to", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "alpha_q.view"], "methods", ["None"], ["", "def", "sample_attn_wsram", "(", "self", ",", "q_scores", ",", "p_scores", ",", "n_samples", "=", "1", ",", "lengths", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "dist_type", "=", "q_scores", ".", "dist_type", "\n", "assert", "p_scores", ".", "dist_type", "==", "dist_type", "\n", "if", "dist_type", "==", "\"categorical\"", ":", "\n", "            ", "alpha_q", "=", "q_scores", ".", "alpha", "\n", "log_alpha_q", "=", "q_scores", ".", "log_alpha", "\n", "K", "=", "n_samples", "\n", "N", "=", "alpha_q", ".", "size", "(", "0", ")", "\n", "T", "=", "alpha_q", ".", "size", "(", "1", ")", "\n", "S", "=", "alpha_q", ".", "size", "(", "2", ")", "\n", "attns_id", "=", "torch", ".", "distributions", ".", "categorical", ".", "Categorical", "(", "\n", "alpha_q", ".", "view", "(", "N", "*", "T", ",", "S", ")", "\n", ")", ".", "sample", "(", "\n", "torch", ".", "Size", "(", "[", "n_samples", "]", ")", "\n", ")", ".", "view", "(", "K", ",", "N", ",", "T", ",", "1", ")", "\n", "attns", "=", "torch", ".", "Tensor", "(", "K", ",", "N", ",", "T", ",", "S", ")", ".", "zero_", "(", ")", ".", "cuda", "(", ")", "\n", "attns", ".", "scatter_", "(", "3", ",", "attns_id", ",", "1", ")", "\n", "q_sample", "=", "attns", ".", "to", "(", "alpha_q", ")", "\n", "# log alpha: K, N, T, S", "\n", "log_alpha_q", "=", "log_alpha_q", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "K", ",", "N", ",", "T", ",", "S", ")", "\n", "sample_log_probs_q", "=", "log_alpha_q", ".", "gather", "(", "3", ",", "attns_id", ".", "to", "(", "log_alpha_q", ".", "device", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "log_alpha_p", "=", "p_scores", ".", "log_alpha", "\n", "log_alpha_p", "=", "log_alpha_p", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "K", ",", "N", ",", "T", ",", "S", ")", "\n", "sample_log_probs_p", "=", "log_alpha_p", ".", "gather", "(", "3", ",", "attns_id", ".", "to", "(", "log_alpha_p", ".", "device", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "sample_p_div_q_log", "=", "sample_log_probs_p", "-", "sample_log_probs_q", "\n", "return", "q_sample", ",", "sample_log_probs_q", ",", "sample_log_probs_p", ",", "sample_p_div_q_log", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unsupported dist\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.forward": [[184, 398], ["memory_bank.size", "input.unsqueeze.unsqueeze.size", "onmt.Utils.aeq", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "input.unsqueeze.unsqueeze.unsqueeze().repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "VariationalAttention.VariationalAttention.tanh", "onmt.Utils.DistInfo", "input.unsqueeze.unsqueeze.dim", "input.unsqueeze.unsqueeze.unsqueeze", "VariationalAttention.VariationalAttention.score", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.exp", "onmt.Utils.Params", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "VariationalAttention.VariationalAttention.tanh", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "memory_bank.unsqueeze().repeat().permute", "memory_bank.unsqueeze().repeat().permute.size", "VariationalAttention.VariationalAttention.linear_out", "c_align_vectors.transpose().contiguous.transpose().contiguous.squeeze", "context_c.squeeze.squeeze.squeeze", "h_y.permute().contiguous.permute().contiguous.squeeze", "onmt.Utils.Params", "h_c.squeeze.squeeze.transpose().contiguous", "c_align_vectors.transpose().contiguous.transpose().contiguous.transpose().contiguous", "h_y.permute().contiguous.permute().contiguous.permute().contiguous", "onmt.Utils.Params", "onmt.Utils.Params", "h_c.squeeze.squeeze.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "c_align_vectors.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.sequence_mask", "mask.unsqueeze.unsqueeze.unsqueeze", "F.log_softmax.exp.data.masked_fill_", "F.log_softmax.exp.data.topk", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "torch.zeros_like().fill_", "new_attn_score.scatter_.scatter_.scatter_", "VariationalAttention.VariationalAttention.linear_out", "VariationalAttention.VariationalAttention.sample_attn", "VariationalAttention.VariationalAttention.sample_attn", "input.unsqueeze.unsqueeze.unsqueeze", "h_c.squeeze.squeeze.squeeze", "onmt.Utils.Params", "h_c.squeeze.squeeze.size", "onmt.Utils.aeq", "c_align_vectors.transpose().contiguous.transpose().contiguous.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.Params", "F.log_softmax.exp.size", "float", "VariationalAttention.VariationalAttention.sample_attn_gumbel", "VariationalAttention.VariationalAttention.sample_attn_gumbel", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "memory_bank.unsqueeze().repeat", "onmt.Utils.Params.alpha.squeeze", "torch.log_softmax.squeeze", "h_c.squeeze.squeeze.transpose", "c_align_vectors.transpose().contiguous.transpose().contiguous.transpose", "h_y.permute().contiguous.permute().contiguous.permute", "onmt.Utils.Params.alpha.transpose().contiguous", "q_sample.permute().contiguous", "onmt.Utils.Params.alpha.transpose().contiguous", "log_alpha.transpose().contiguous", "p_sample.permute().contiguous", "float", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "VariationalAttention.VariationalAttention.sample_attn_wsram", "y_align_vectors.view", "memory_bank.unsqueeze().repeat().view", "p_sample.squeeze", "onmt.Utils.Params.alpha.unsqueeze", "onmt.Utils.Params.log_alpha.unsqueeze", "memory_bank.unsqueeze", "onmt.Utils.Params.alpha.squeeze", "q_sample.squeeze", "sample_log_probs.squeeze", "sample_log_probs_q.squeeze", "sample_log_probs_p.squeeze", "sample_p_div_q_log.squeeze", "onmt.Utils.Params.alpha.transpose", "q_sample.permute", "onmt.Utils.Params.alpha.transpose", "log_alpha.transpose", "p_sample.permute", "memory_bank.unsqueeze().repeat", "memory_bank.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.sequence_mask", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn_gumbel", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn_gumbel", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.VariationalAttention.sample_attn_wsram"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "memory_bank", ",", "memory_lengths", "=", "None", ",", "coverage", "=", "None", ",", "q_scores", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n          input (`FloatTensor`): query vectors `[batch x tgt_len x dim]`\n          memory_bank (`FloatTensor`): source vectors `[batch x src_len x dim]`\n          memory_lengths (`LongTensor`): the source context lengths `[batch]`\n          coverage (`FloatTensor`): None (not supported yet)\n          q_scores (`FloatTensor`): the attention params from the inference network\n\n        Returns:\n          (`FloatTensor`, `FloatTensor`):\n\n          * Weighted context vector `[tgt_len x batch x dim]`\n          * Attention distribtutions for each query\n             `[tgt_len x batch x src_len]`\n          * Unormalized attention scores for each query \n            `[batch x tgt_len x src_len]`\n        \"\"\"", "\n", "\n", "# one step input", "\n", "if", "input", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "one_step", "=", "True", "\n", "input", "=", "input", ".", "unsqueeze", "(", "1", ")", "\n", "if", "q_scores", "is", "not", "None", ":", "\n", "# oh, I guess this is super messy", "\n", "                ", "if", "q_scores", ".", "alpha", "is", "not", "None", ":", "\n", "                    ", "q_scores", "=", "Params", "(", "\n", "alpha", "=", "q_scores", ".", "alpha", ".", "unsqueeze", "(", "1", ")", ",", "\n", "log_alpha", "=", "q_scores", ".", "log_alpha", ".", "unsqueeze", "(", "1", ")", ",", "\n", "dist_type", "=", "q_scores", ".", "dist_type", ",", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "one_step", "=", "False", "\n", "\n", "", "batch", ",", "sourceL", ",", "dim", "=", "memory_bank", ".", "size", "(", ")", "\n", "batch_", ",", "targetL", ",", "dim_", "=", "input", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "\n", "# compute attention scores, as in Luong et al.", "\n", "# Params should be T x N x S", "\n", "if", "self", ".", "p_dist_type", "==", "\"categorical\"", ":", "\n", "            ", "scores", "=", "self", ".", "score", "(", "input", ",", "memory_bank", ")", "\n", "if", "memory_lengths", "is", "not", "None", ":", "\n", "# mask : N x T x S", "\n", "                ", "mask", "=", "sequence_mask", "(", "memory_lengths", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "# Make it broadcastable.", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "1", "-", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "", "if", "self", ".", "k", ">", "0", "and", "self", ".", "k", "<", "scores", ".", "size", "(", "-", "1", ")", ":", "\n", "                ", "topk", ",", "idx", "=", "scores", ".", "data", ".", "topk", "(", "self", ".", "k", ")", "\n", "new_attn_score", "=", "torch", ".", "zeros_like", "(", "scores", ".", "data", ")", ".", "fill_", "(", "float", "(", "\"-inf\"", ")", ")", "\n", "new_attn_score", "=", "new_attn_score", ".", "scatter_", "(", "2", ",", "idx", ",", "topk", ")", "\n", "scores", "=", "new_attn_score", "\n", "", "log_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "scores", "=", "log_scores", ".", "exp", "(", ")", "\n", "\n", "c_align_vectors", "=", "scores", "\n", "\n", "p_scores", "=", "Params", "(", "\n", "alpha", "=", "scores", ",", "\n", "log_alpha", "=", "log_scores", ",", "\n", "dist_type", "=", "self", ".", "p_dist_type", ",", "\n", ")", "\n", "\n", "# each context vector c_t is the weighted average", "\n", "# over all the source hidden states", "\n", "", "context_c", "=", "torch", ".", "bmm", "(", "c_align_vectors", ",", "memory_bank", ")", "\n", "if", "self", ".", "mode", "!=", "'wsram'", ":", "\n", "            ", "concat_c", "=", "torch", ".", "cat", "(", "[", "input", ",", "context_c", "]", ",", "-", "1", ")", "\n", "# N x T x H", "\n", "h_c", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "concat_c", ")", ")", "\n", "", "else", ":", "\n", "            ", "h_c", "=", "None", "\n", "\n", "# sample or enumerate", "\n", "# y_align_vectors: K x N x T x S", "\n", "", "q_sample", ",", "p_sample", ",", "sample_log_probs", "=", "None", ",", "None", ",", "None", "\n", "sample_log_probs_q", ",", "sample_log_probs_p", ",", "sample_p_div_q_log", "=", "None", ",", "None", ",", "None", "\n", "if", "self", ".", "mode", "==", "\"sample\"", ":", "\n", "            ", "if", "q_scores", "is", "None", "or", "self", ".", "use_prior", ":", "\n", "                ", "p_sample", ",", "sample_log_probs", "=", "self", ".", "sample_attn", "(", "\n", "p_scores", ",", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "lengths", "=", "memory_lengths", ",", "mask", "=", "mask", "if", "memory_lengths", "is", "not", "None", "else", "None", ")", "\n", "y_align_vectors", "=", "p_sample", "\n", "", "else", ":", "\n", "                ", "q_sample", ",", "sample_log_probs", "=", "self", ".", "sample_attn", "(", "\n", "q_scores", ",", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "lengths", "=", "memory_lengths", ",", "mask", "=", "mask", "if", "memory_lengths", "is", "not", "None", "else", "None", ")", "\n", "y_align_vectors", "=", "q_sample", "\n", "", "", "elif", "self", ".", "mode", "==", "\"gumbel\"", ":", "\n", "            ", "if", "q_scores", "is", "None", "or", "self", ".", "use_prior", ":", "\n", "                ", "p_sample", ",", "_", "=", "self", ".", "sample_attn_gumbel", "(", "\n", "p_scores", ",", "self", ".", "temperature", ",", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "lengths", "=", "memory_lengths", ",", "mask", "=", "mask", "if", "memory_lengths", "is", "not", "None", "else", "None", ")", "\n", "y_align_vectors", "=", "p_sample", "\n", "", "else", ":", "\n", "                ", "q_sample", ",", "_", "=", "self", ".", "sample_attn_gumbel", "(", "\n", "q_scores", ",", "self", ".", "temperature", ",", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "lengths", "=", "memory_lengths", ",", "mask", "=", "mask", "if", "memory_lengths", "is", "not", "None", "else", "None", ")", "\n", "y_align_vectors", "=", "q_sample", "\n", "", "", "elif", "self", ".", "mode", "==", "\"enum\"", "or", "self", ".", "mode", "==", "\"exact\"", ":", "\n", "            ", "y_align_vectors", "=", "None", "\n", "", "elif", "self", ".", "mode", "==", "\"wsram\"", ":", "\n", "            ", "assert", "q_scores", "is", "not", "None", "\n", "q_sample", ",", "sample_log_probs_q", ",", "sample_log_probs_p", ",", "sample_p_div_q_log", "=", "self", ".", "sample_attn_wsram", "(", "\n", "q_scores", ",", "p_scores", ",", "n_samples", "=", "self", ".", "n_samples", ",", "\n", "lengths", "=", "memory_lengths", ",", "mask", "=", "mask", "if", "memory_lengths", "is", "not", "None", "else", "None", ")", "\n", "y_align_vectors", "=", "q_sample", "\n", "\n", "\n", "# context_y: K x N x T x H", "\n", "", "if", "y_align_vectors", "is", "not", "None", ":", "\n", "            ", "context_y", "=", "torch", ".", "bmm", "(", "\n", "y_align_vectors", ".", "view", "(", "-", "1", ",", "targetL", ",", "sourceL", ")", ",", "\n", "memory_bank", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "self", ".", "n_samples", ",", "1", ",", "1", ",", "1", ")", ".", "view", "(", "-", "1", ",", "sourceL", ",", "dim", ")", "\n", ")", ".", "view", "(", "self", ".", "n_samples", ",", "batch", ",", "targetL", ",", "dim", ")", "\n", "", "else", ":", "\n", "# For enumerate, K = S.", "\n", "# memory_bank: N x S x H", "\n", "            ", "context_y", "=", "(", "memory_bank", "\n", ".", "unsqueeze", "(", "0", ")", "\n", ".", "repeat", "(", "targetL", ",", "1", ",", "1", ",", "1", ")", "# T, N, S, H", "\n", ".", "permute", "(", "2", ",", "1", ",", "0", ",", "3", ")", ")", "# S, N, T, H", "\n", "", "input", "=", "input", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "context_y", ".", "size", "(", "0", ")", ",", "1", ",", "1", ",", "1", ")", "\n", "concat_y", "=", "torch", ".", "cat", "(", "[", "input", ",", "context_y", "]", ",", "-", "1", ")", "\n", "# K x N x T x H", "\n", "h_y", "=", "self", ".", "tanh", "(", "self", ".", "linear_out", "(", "concat_y", ")", ")", "\n", "\n", "if", "one_step", ":", "\n", "            ", "if", "h_c", "is", "not", "None", ":", "\n", "# N x H", "\n", "                ", "h_c", "=", "h_c", ".", "squeeze", "(", "1", ")", "\n", "# N x S", "\n", "", "c_align_vectors", "=", "c_align_vectors", ".", "squeeze", "(", "1", ")", "\n", "context_c", "=", "context_c", ".", "squeeze", "(", "1", ")", "\n", "\n", "# K x N x H", "\n", "h_y", "=", "h_y", ".", "squeeze", "(", "2", ")", "\n", "# K x N x S", "\n", "#y_align_vectors = y_align_vectors.squeeze(2)", "\n", "\n", "q_scores", "=", "Params", "(", "\n", "alpha", "=", "q_scores", ".", "alpha", ".", "squeeze", "(", "1", ")", "if", "q_scores", ".", "alpha", "is", "not", "None", "else", "None", ",", "\n", "dist_type", "=", "q_scores", ".", "dist_type", ",", "\n", "samples", "=", "q_sample", ".", "squeeze", "(", "2", ")", "if", "q_sample", "is", "not", "None", "else", "None", ",", "\n", "sample_log_probs", "=", "sample_log_probs", ".", "squeeze", "(", "2", ")", "if", "sample_log_probs", "is", "not", "None", "else", "None", ",", "\n", "sample_log_probs_q", "=", "sample_log_probs_q", ".", "squeeze", "(", "2", ")", "if", "sample_log_probs_q", "is", "not", "None", "else", "None", ",", "\n", "sample_log_probs_p", "=", "sample_log_probs_p", ".", "squeeze", "(", "2", ")", "if", "sample_log_probs_p", "is", "not", "None", "else", "None", ",", "\n", "sample_p_div_q_log", "=", "sample_p_div_q_log", ".", "squeeze", "(", "2", ")", "if", "sample_p_div_q_log", "is", "not", "None", "else", "None", ",", "\n", ")", "if", "q_scores", "is", "not", "None", "else", "None", "\n", "p_scores", "=", "Params", "(", "\n", "alpha", "=", "p_scores", ".", "alpha", ".", "squeeze", "(", "1", ")", ",", "\n", "log_alpha", "=", "log_scores", ".", "squeeze", "(", "1", ")", ",", "\n", "dist_type", "=", "p_scores", ".", "dist_type", ",", "\n", "samples", "=", "p_sample", ".", "squeeze", "(", "2", ")", "if", "p_sample", "is", "not", "None", "else", "None", ",", "\n", ")", "\n", "\n", "if", "h_c", "is", "not", "None", ":", "\n", "# Check output sizes", "\n", "                ", "batch_", ",", "dim_", "=", "h_c", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "batch_", ",", "sourceL_", "=", "c_align_vectors", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "sourceL", ",", "sourceL_", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "False", "\n", "# Only support input feeding.", "\n", "# T x N x H", "\n", "h_c", "=", "h_c", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "# T x N x S", "\n", "c_align_vectors", "=", "c_align_vectors", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# T x K x N x H", "\n", "h_y", "=", "h_y", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "# T x K x N x S", "\n", "#y_align_vectors = y_align_vectors.permute(2, 0, 1, 3).contiguous()", "\n", "\n", "q_scores", "=", "Params", "(", "\n", "alpha", "=", "q_scores", ".", "alpha", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ",", "\n", "dist_type", "=", "q_scores", ".", "dist_type", ",", "\n", "samples", "=", "q_sample", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", ",", "\n", ")", "\n", "p_scores", "=", "Params", "(", "\n", "alpha", "=", "p_scores", ".", "alpha", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ",", "\n", "log_alpha", "=", "log_alpha", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ",", "\n", "dist_type", "=", "p_scores", ".", "dist_type", ",", "\n", "samples", "=", "p_sample", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", ",", "\n", ")", "\n", "\n", "# Check output sizes", "\n", "targetL_", ",", "batch_", ",", "dim_", "=", "h_c", ".", "size", "(", ")", "\n", "aeq", "(", "targetL", ",", "targetL_", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "dim", ",", "dim_", ")", "\n", "targetL_", ",", "batch_", ",", "sourceL_", "=", "c_align_vectors", ".", "size", "(", ")", "\n", "aeq", "(", "targetL", ",", "targetL_", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "sourceL", ",", "sourceL_", ")", "\n", "\n", "# For now, don't include samples.", "\n", "", "dist_info", "=", "DistInfo", "(", "\n", "q", "=", "q_scores", ",", "\n", "p", "=", "p_scores", ",", "\n", ")", "\n", "\n", "# h_y: samples from simplex", "\n", "#   either K x N x H, or T x K x N x H", "\n", "# h_c: convex combination of memory_bank for input feeding", "\n", "#   either N x H, or T x N x H", "\n", "# align_vectors: convex coefficients / boltzmann dist", "\n", "#   either N x S, or T x N x S", "\n", "# raw_scores: unnormalized scores", "\n", "#   either N x S, or T x N x S", "\n", "return", "h_y", ",", "h_c", ",", "context_c", ",", "c_align_vectors", ",", "dist_info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.sample_gumbel": [[12, 21], ["input.size", "input.size", "input.size", "torch.rand().to", "torch.rand().to", "torch.rand().to", "torch.rand().to.add_().log_().neg_", "torch.rand().to.add_().log_().neg_", "torch.rand", "torch.rand", "torch.rand", "torch.rand().to.add_().log_", "torch.rand().to.add_().log_", "torch.rand().to.add_", "torch.rand().to.add_"], "function", ["None"], ["def", "sample_gumbel", "(", "input", ",", "K", ")", ":", "\n", "    ", "N", "=", "input", ".", "size", "(", "0", ")", "\n", "T", "=", "input", ".", "size", "(", "1", ")", "\n", "S", "=", "input", ".", "size", "(", "2", ")", "\n", "noise", "=", "torch", ".", "rand", "(", "(", "K", ",", "N", ",", "T", ",", "S", ")", ")", ".", "to", "(", "input", ")", "\n", "eps", "=", "1e-20", "\n", "noise", ".", "add_", "(", "eps", ")", ".", "log_", "(", ")", ".", "neg_", "(", ")", "\n", "noise", ".", "add_", "(", "eps", ")", ".", "log_", "(", ")", ".", "neg_", "(", ")", "\n", "return", "noise", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.gumbel_softmax_sample": [[22, 28], ["VariationalAttention.sample_gumbel", "torch.softmax", "F.softmax.view_as", "log_probs.unsqueeze"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.VariationalAttention.sample_gumbel"], ["", "def", "gumbel_softmax_sample", "(", "log_probs", ",", "K", ",", "temperature", ")", ":", "\n", "#attns = gumbel_softmax_sample(log_alpha, K) # K, N, T, S", "\n", "    ", "noise", "=", "sample_gumbel", "(", "log_probs", ",", "K", ")", "# K, N, T, S", "\n", "x", "=", "(", "log_probs", ".", "unsqueeze", "(", "0", ")", "+", "noise", ")", "/", "temperature", "\n", "x", "=", "F", ".", "softmax", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "return", "x", ".", "view_as", "(", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ConvMultiStepAttention.ConvMultiStepAttention.__init__": [[28, 32], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ")", ":", "\n", "        ", "super", "(", "ConvMultiStepAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "input_size", ",", "input_size", ")", "\n", "self", ".", "mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ConvMultiStepAttention.ConvMultiStepAttention.apply_mask": [[33, 35], ["None"], "methods", ["None"], ["", "def", "apply_mask", "(", "self", ",", "mask", ")", ":", "\n", "        ", "self", ".", "mask", "=", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ConvMultiStepAttention.ConvMultiStepAttention.forward": [[36, 78], ["base_target_emb.size", "input.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "encoder_out_top.size", "encoder_out_combine.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "ConvMultiStepAttention.seq_linear", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "pre_attn.transpose.transpose.transpose", "torch.softmax", "torch.softmax", "torch.softmax", "attn.transpose().contiguous.transpose().contiguous.transpose().contiguous", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "pre_attn.transpose.transpose.data.masked_fill_", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "attn.transpose().contiguous.transpose().contiguous.transpose", "float"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ConvMultiStepAttention.seq_linear"], ["", "def", "forward", "(", "self", ",", "base_target_emb", ",", "input", ",", "encoder_out_top", ",", "\n", "encoder_out_combine", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            base_target_emb: target emb tensor\n            input: output of decode conv\n            encoder_out_t: the key matrix for calculation of attetion weight,\n                which is the top output of encode conv\n            encoder_out_combine:\n                the value matrix for the attention-weighted sum,\n                which is the combination of base emb and top output of encode\n\n        \"\"\"", "\n", "# checks", "\n", "batch", ",", "channel", ",", "height", ",", "width", "=", "base_target_emb", ".", "size", "(", ")", "\n", "batch_", ",", "channel_", ",", "height_", ",", "width_", "=", "input", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "height", ",", "height_", ")", "\n", "\n", "enc_batch", ",", "enc_channel", ",", "enc_height", "=", "encoder_out_top", ".", "size", "(", ")", "\n", "enc_batch_", ",", "enc_channel_", ",", "enc_height_", "=", "encoder_out_combine", ".", "size", "(", ")", "\n", "\n", "aeq", "(", "enc_batch", ",", "enc_batch_", ")", "\n", "aeq", "(", "enc_height", ",", "enc_height_", ")", "\n", "\n", "preatt", "=", "seq_linear", "(", "self", ".", "linear_in", ",", "input", ")", "\n", "target", "=", "(", "base_target_emb", "+", "preatt", ")", "*", "SCALE_WEIGHT", "\n", "target", "=", "torch", ".", "squeeze", "(", "target", ",", "3", ")", "\n", "target", "=", "torch", ".", "transpose", "(", "target", ",", "1", ",", "2", ")", "\n", "pre_attn", "=", "torch", ".", "bmm", "(", "target", ",", "encoder_out_top", ")", "\n", "\n", "if", "self", ".", "mask", "is", "not", "None", ":", "\n", "            ", "pre_attn", ".", "data", ".", "masked_fill_", "(", "self", ".", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "", "pre_attn", "=", "pre_attn", ".", "transpose", "(", "0", ",", "2", ")", "\n", "attn", "=", "F", ".", "softmax", "(", "pre_attn", ")", "\n", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "context_output", "=", "torch", ".", "bmm", "(", "\n", "attn", ",", "torch", ".", "transpose", "(", "encoder_out_combine", ",", "1", ",", "2", ")", ")", "\n", "context_output", "=", "torch", ".", "transpose", "(", "\n", "torch", ".", "unsqueeze", "(", "context_output", ",", "3", ")", ",", "1", ",", "2", ")", "\n", "return", "context_output", ",", "attn", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.ConvMultiStepAttention.seq_linear": [[10, 16], ["x.size", "linear", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose().contiguous().view", "torch.transpose().contiguous().view", "torch.transpose().contiguous().view", "linear.view", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.transpose", "torch.transpose", "torch.transpose"], "function", ["None"], ["def", "seq_linear", "(", "linear", ",", "x", ")", ":", "\n", "# linear transform for 3-d tensor", "\n", "    ", "batch", ",", "hidden_size", ",", "length", ",", "_", "=", "x", ".", "size", "(", ")", "\n", "h", "=", "linear", "(", "torch", ".", "transpose", "(", "x", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch", "*", "length", ",", "hidden_size", ")", ")", "\n", "return", "torch", ".", "transpose", "(", "h", ".", "view", "(", "batch", ",", "length", ",", "hidden_size", ",", "1", ")", ",", "1", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormLinear.__init__": [[39, 56], ["torch.Linear.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "WeightNorm.WeightNormLinear.register_buffer", "WeightNorm.WeightNormLinear.register_buffer", "WeightNorm.WeightNormLinear.register_buffer", "WeightNorm.WeightNormLinear.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters"], ["def", "__init__", "(", "self", ",", "in_features", ",", "out_features", ",", "\n", "init_scale", "=", "1.", ",", "polyak_decay", "=", "0.9995", ")", ":", "\n", "        ", "super", "(", "WeightNormLinear", ",", "self", ")", ".", "__init__", "(", "\n", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", "\n", "\n", "self", ".", "V", "=", "self", ".", "weight", "\n", "self", ".", "g", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "out_features", ")", ")", "\n", "self", ".", "b", "=", "self", ".", "bias", "\n", "\n", "self", ".", "register_buffer", "(", "\n", "'V_avg'", ",", "torch", ".", "zeros", "(", "out_features", ",", "in_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'g_avg'", ",", "torch", ".", "zeros", "(", "out_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'b_avg'", ",", "torch", ".", "zeros", "(", "out_features", ")", ")", "\n", "\n", "self", ".", "init_scale", "=", "init_scale", "\n", "self", ".", "polyak_decay", "=", "polyak_decay", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormLinear.reset_parameters": [[57, 59], ["None"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormLinear.forward": [[60, 94], ["WeightNorm.WeightNormLinear.V.data.copy_", "WeightNorm.WeightNormLinear.g.data.copy_", "WeightNorm.WeightNormLinear.b.data.copy_", "WeightNorm.WeightNormLinear.V_avg.copy_", "WeightNorm.WeightNormLinear.g_avg.copy_", "WeightNorm.WeightNormLinear.b_avg.copy_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "WeightNorm.get_vars_maybe_avg", "torch.linear", "torch.linear", "torch.linear", "WeightNorm.WeightNormLinear.V.data.norm().expand_as", "torch.linear", "torch.linear", "torch.linear", "x_init.mean().squeeze", "x_init.var().squeeze", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "scale_init.view().expand_as", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "b.view().expand_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "m_init.view().expand_as", "scalar.view().expand_as", "WeightNorm.WeightNormLinear.V.data.norm", "x_init.mean", "x_init.var", "scale_init.view", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "b.view", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "m_init.view", "scalar.view", "WeightNorm.WeightNormLinear.V.data.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_vars_maybe_avg"], ["", "def", "forward", "(", "self", ",", "x", ",", "init", "=", "False", ")", ":", "\n", "        ", "if", "init", "is", "True", ":", "\n", "# out_features * in_features", "\n", "            ", "self", ".", "V", ".", "data", ".", "copy_", "(", "torch", ".", "randn", "(", "self", ".", "V", ".", "data", ".", "size", "(", ")", ")", ".", "type_as", "(", "\n", "self", ".", "V", ".", "data", ")", "*", "0.05", ")", "\n", "# norm is out_features * 1", "\n", "v_norm", "=", "self", ".", "V", ".", "data", "/", "self", ".", "V", ".", "data", ".", "norm", "(", "2", ",", "1", ")", ".", "expand_as", "(", "self", ".", "V", ".", "data", ")", "\n", "# batch_size * out_features", "\n", "x_init", "=", "F", ".", "linear", "(", "x", ",", "Variable", "(", "v_norm", ")", ")", ".", "data", "\n", "# out_features", "\n", "m_init", ",", "v_init", "=", "x_init", ".", "mean", "(", "0", ")", ".", "squeeze", "(", "\n", "0", ")", ",", "x_init", ".", "var", "(", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "# out_features", "\n", "scale_init", "=", "self", ".", "init_scale", "/", "torch", ".", "sqrt", "(", "v_init", "+", "1e-10", ")", "\n", "self", ".", "g", ".", "data", ".", "copy_", "(", "scale_init", ")", "\n", "self", ".", "b", ".", "data", ".", "copy_", "(", "-", "m_init", "*", "scale_init", ")", "\n", "x_init", "=", "scale_init", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "x_init", ")", "*", "(", "x_init", "-", "m_init", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "x_init", ")", ")", "\n", "self", ".", "V_avg", ".", "copy_", "(", "self", ".", "V", ".", "data", ")", "\n", "self", ".", "g_avg", ".", "copy_", "(", "self", ".", "g", ".", "data", ")", "\n", "self", ".", "b_avg", ".", "copy_", "(", "self", ".", "b", ".", "data", ")", "\n", "return", "Variable", "(", "x_init", ")", "\n", "", "else", ":", "\n", "            ", "V", ",", "g", ",", "b", "=", "get_vars_maybe_avg", "(", "self", ",", "[", "'V'", ",", "'g'", ",", "'b'", "]", ",", "\n", "self", ".", "training", ",", "\n", "polyak_decay", "=", "self", ".", "polyak_decay", ")", "\n", "# batch_size * out_features", "\n", "x", "=", "F", ".", "linear", "(", "x", ",", "V", ")", "\n", "scalar", "=", "g", "/", "torch", ".", "norm", "(", "V", ",", "2", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "x", "=", "scalar", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "x", ")", "*", "x", "+", "b", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConv2d.__init__": [[97, 115], ["torch.Conv2d.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "WeightNorm.WeightNormConv2d.register_buffer", "WeightNorm.WeightNormConv2d.register_buffer", "WeightNorm.WeightNormConv2d.register_buffer", "WeightNorm.WeightNormConv2d.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "WeightNorm.WeightNormConv2d.V.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "init_scale", "=", "1.", ",", "\n", "polyak_decay", "=", "0.9995", ")", ":", "\n", "        ", "super", "(", "WeightNormConv2d", ",", "self", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "\n", "kernel_size", ",", "stride", ",", "padding", ",", "\n", "dilation", ",", "groups", ")", "\n", "\n", "self", ".", "V", "=", "self", ".", "weight", "\n", "self", ".", "g", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "self", ".", "b", "=", "self", ".", "bias", "\n", "\n", "self", ".", "register_buffer", "(", "'V_avg'", ",", "torch", ".", "zeros", "(", "self", ".", "V", ".", "size", "(", ")", ")", ")", "\n", "self", ".", "register_buffer", "(", "'g_avg'", ",", "torch", ".", "zeros", "(", "out_channels", ")", ")", "\n", "self", ".", "register_buffer", "(", "'b_avg'", ",", "torch", ".", "zeros", "(", "out_channels", ")", ")", "\n", "\n", "self", ".", "init_scale", "=", "init_scale", "\n", "self", ".", "polyak_decay", "=", "polyak_decay", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConv2d.reset_parameters": [[116, 118], ["None"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConv2d.forward": [[119, 165], ["WeightNorm.WeightNormConv2d.V.data.copy_", "x_init.transpose().contiguous().view", "WeightNorm.WeightNormConv2d.g.data.copy_", "WeightNorm.WeightNormConv2d.b.data.copy_", "scale_init.view", "m_init.view", "WeightNorm.WeightNormConv2d.V_avg.copy_", "WeightNorm.WeightNormConv2d.g_avg.copy_", "WeightNorm.WeightNormConv2d.b_avg.copy_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "WeightNorm.get_vars_maybe_avg", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.conv2d", "torch.conv2d", "torch.conv2d", "WeightNorm.WeightNormConv2d.V.data.view().norm().view().expand_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "x_init.transpose().contiguous().view.mean().squeeze", "x_init.transpose().contiguous().view.var().squeeze", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "scale_init.view.expand_as", "v.view", "len", "torch.norm.view().expand_as", "torch.norm.view().expand_as", "torch.norm.view().expand_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x_init.transpose().contiguous", "m_init.view.expand_as", "torch.norm.size", "torch.norm.size", "torch.norm.size", "torch.norm.squeeze", "torch.norm.squeeze", "torch.norm.squeeze", "WeightNorm.WeightNormConv2d.V.data.view().norm().view", "x_init.transpose().contiguous().view.mean", "x_init.transpose().contiguous().view.var", "torch.norm.view", "torch.norm.view", "torch.norm.view", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "x_init.transpose", "len", "len", "WeightNorm.WeightNormConv2d.V.data.size", "WeightNorm.WeightNormConv2d.V.data.view().norm", "x_init.size", "x_init.size", "WeightNorm.WeightNormConv2d.V.data.view", "len", "len", "v.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_vars_maybe_avg"], ["", "def", "forward", "(", "self", ",", "x", ",", "init", "=", "False", ")", ":", "\n", "        ", "if", "init", "is", "True", ":", "\n", "# out_channels, in_channels // groups, * kernel_size", "\n", "            ", "self", ".", "V", ".", "data", ".", "copy_", "(", "torch", ".", "randn", "(", "self", ".", "V", ".", "data", ".", "size", "(", ")", "\n", ")", ".", "type_as", "(", "self", ".", "V", ".", "data", ")", "*", "0.05", ")", "\n", "v_norm", "=", "self", ".", "V", ".", "data", "/", "self", ".", "V", ".", "data", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", ".", "norm", "(", "2", ",", "1", ")", ".", "view", "(", "self", ".", "out_channels", ",", "*", "(", "\n", "[", "1", "]", "*", "(", "len", "(", "self", ".", "kernel_size", ")", "+", "1", ")", ")", ")", ".", "expand_as", "(", "self", ".", "V", ".", "data", ")", "\n", "x_init", "=", "F", ".", "conv2d", "(", "x", ",", "Variable", "(", "v_norm", ")", ",", "None", ",", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", ".", "data", "\n", "t_x_init", "=", "x_init", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "self", ".", "out_channels", ",", "-", "1", ")", "\n", "m_init", ",", "v_init", "=", "t_x_init", ".", "mean", "(", "1", ")", ".", "squeeze", "(", "\n", "1", ")", ",", "t_x_init", ".", "var", "(", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "# out_features", "\n", "scale_init", "=", "self", ".", "init_scale", "/", "torch", ".", "sqrt", "(", "v_init", "+", "1e-10", ")", "\n", "self", ".", "g", ".", "data", ".", "copy_", "(", "scale_init", ")", "\n", "self", ".", "b", ".", "data", ".", "copy_", "(", "-", "m_init", "*", "scale_init", ")", "\n", "scale_init_shape", "=", "scale_init", ".", "view", "(", "\n", "1", ",", "self", ".", "out_channels", ",", "*", "(", "[", "1", "]", "*", "(", "len", "(", "x_init", ".", "size", "(", ")", ")", "-", "2", ")", ")", ")", "\n", "m_init_shape", "=", "m_init", ".", "view", "(", "\n", "1", ",", "self", ".", "out_channels", ",", "*", "(", "[", "1", "]", "*", "(", "len", "(", "x_init", ".", "size", "(", ")", ")", "-", "2", ")", ")", ")", "\n", "x_init", "=", "scale_init_shape", ".", "expand_as", "(", "\n", "x_init", ")", "*", "(", "x_init", "-", "m_init_shape", ".", "expand_as", "(", "x_init", ")", ")", "\n", "self", ".", "V_avg", ".", "copy_", "(", "self", ".", "V", ".", "data", ")", "\n", "self", ".", "g_avg", ".", "copy_", "(", "self", ".", "g", ".", "data", ")", "\n", "self", ".", "b_avg", ".", "copy_", "(", "self", ".", "b", ".", "data", ")", "\n", "return", "Variable", "(", "x_init", ")", "\n", "", "else", ":", "\n", "            ", "v", ",", "g", ",", "b", "=", "get_vars_maybe_avg", "(", "\n", "self", ",", "[", "'V'", ",", "'g'", ",", "'b'", "]", ",", "self", ".", "training", ",", "\n", "polyak_decay", "=", "self", ".", "polyak_decay", ")", "\n", "\n", "scalar", "=", "torch", ".", "norm", "(", "v", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", ",", "2", ",", "1", ")", "\n", "if", "len", "(", "scalar", ".", "size", "(", ")", ")", "==", "2", ":", "\n", "                ", "scalar", "=", "g", "/", "scalar", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "scalar", "=", "g", "/", "scalar", "\n", "\n", "", "w", "=", "scalar", ".", "view", "(", "self", ".", "out_channels", ",", "*", "\n", "(", "[", "1", "]", "*", "(", "len", "(", "v", ".", "size", "(", ")", ")", "-", "1", ")", ")", ")", ".", "expand_as", "(", "v", ")", "*", "v", "\n", "\n", "x", "=", "F", ".", "conv2d", "(", "x", ",", "w", ",", "b", ",", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.__init__": [[168, 188], ["torch.ConvTranspose2d.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "WeightNorm.WeightNormConvTranspose2d.register_buffer", "WeightNorm.WeightNormConvTranspose2d.register_buffer", "WeightNorm.WeightNormConvTranspose2d.register_buffer", "WeightNorm.WeightNormConvTranspose2d.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "WeightNorm.WeightNormConvTranspose2d.V.size"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "output_padding", "=", "0", ",", "groups", "=", "1", ",", "init_scale", "=", "1.", ",", "\n", "polyak_decay", "=", "0.9995", ")", ":", "\n", "        ", "super", "(", "WeightNormConvTranspose2d", ",", "self", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "\n", "kernel_size", ",", "stride", ",", "\n", "padding", ",", "output_padding", ",", "\n", "groups", ")", "\n", "# in_channels, out_channels, *kernel_size", "\n", "self", ".", "V", "=", "self", ".", "weight", "\n", "self", ".", "g", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "self", ".", "b", "=", "self", ".", "bias", "\n", "\n", "self", ".", "register_buffer", "(", "'V_avg'", ",", "torch", ".", "zeros", "(", "self", ".", "V", ".", "size", "(", ")", ")", ")", "\n", "self", ".", "register_buffer", "(", "'g_avg'", ",", "torch", ".", "zeros", "(", "out_channels", ")", ")", "\n", "self", ".", "register_buffer", "(", "'b_avg'", ",", "torch", ".", "zeros", "(", "out_channels", ")", ")", "\n", "\n", "self", ".", "init_scale", "=", "init_scale", "\n", "self", ".", "polyak_decay", "=", "polyak_decay", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.reset_parameters": [[189, 191], ["None"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.WeightNormConvTranspose2d.forward": [[192, 240], ["WeightNorm.WeightNormConvTranspose2d.V.data.copy_", "x_init.tranpose().contiguous().view", "WeightNorm.WeightNormConvTranspose2d.g.data.copy_", "WeightNorm.WeightNormConvTranspose2d.b.data.copy_", "scale_init.view", "m_init.view", "WeightNorm.WeightNormConvTranspose2d.V_avg.copy_", "WeightNorm.WeightNormConvTranspose2d.g_avg.copy_", "WeightNorm.WeightNormConvTranspose2d.b_avg.copy_", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "WeightNorm.get_vars_maybe_avg", "torch.conv_transpose2d", "torch.conv_transpose2d", "torch.conv_transpose2d", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose().contiguous().view().norm().view().expand_as", "torch.conv_transpose2d", "torch.conv_transpose2d", "torch.conv_transpose2d", "x_init.tranpose().contiguous().view.mean().squeeze", "x_init.tranpose().contiguous().view.var().squeeze", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "scale_init.view.expand_as", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "torch.norm().squeeze", "scalar.view().expand_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.randn().type_as", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x_init.tranpose().contiguous", "m_init.view.expand_as", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose().contiguous().view().norm().view", "x_init.tranpose().contiguous().view.mean", "x_init.tranpose().contiguous().view.var", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "scalar.view", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "x_init.tranpose", "len", "len", "V.transpose().contiguous().view", "WeightNorm.WeightNormConvTranspose2d.V.data.size", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose().contiguous().view().norm", "x_init.size", "x_init.size", "len", "V.transpose().contiguous", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose().contiguous().view", "len", "V.transpose", "V.size", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose().contiguous", "WeightNorm.WeightNormConvTranspose2d.V.data.transpose"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_vars_maybe_avg"], ["", "def", "forward", "(", "self", ",", "x", ",", "init", "=", "False", ")", ":", "\n", "        ", "if", "init", "is", "True", ":", "\n", "# in_channels, out_channels, *kernel_size", "\n", "            ", "self", ".", "V", ".", "data", ".", "copy_", "(", "torch", ".", "randn", "(", "self", ".", "V", ".", "data", ".", "size", "(", ")", ")", ".", "type_as", "(", "\n", "self", ".", "V", ".", "data", ")", "*", "0.05", ")", "\n", "v_norm", "=", "self", ".", "V", ".", "data", "/", "self", ".", "V", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", ".", "norm", "(", "2", ",", "1", ")", ".", "view", "(", "\n", "self", ".", "in_channels", ",", "self", ".", "out_channels", ",", "\n", "*", "(", "[", "1", "]", "*", "len", "(", "self", ".", "kernel_size", ")", ")", ")", ".", "expand_as", "(", "self", ".", "V", ".", "data", ")", "\n", "x_init", "=", "F", ".", "conv_transpose2d", "(", "\n", "x", ",", "Variable", "(", "v_norm", ")", ",", "None", ",", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "self", ".", "output_padding", ",", "self", ".", "groups", ")", ".", "data", "\n", "# self.out_channels, 1", "\n", "t_x_init", "=", "x_init", ".", "tranpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "self", ".", "out_channels", ",", "-", "1", ")", "\n", "# out_features", "\n", "m_init", ",", "v_init", "=", "t_x_init", ".", "mean", "(", "1", ")", ".", "squeeze", "(", "\n", "1", ")", ",", "t_x_init", ".", "var", "(", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "# out_features", "\n", "scale_init", "=", "self", ".", "init_scale", "/", "torch", ".", "sqrt", "(", "v_init", "+", "1e-10", ")", "\n", "self", ".", "g", ".", "data", ".", "copy_", "(", "scale_init", ")", "\n", "self", ".", "b", ".", "data", ".", "copy_", "(", "-", "m_init", "*", "scale_init", ")", "\n", "scale_init_shape", "=", "scale_init", ".", "view", "(", "\n", "1", ",", "self", ".", "out_channels", ",", "*", "(", "[", "1", "]", "*", "(", "len", "(", "x_init", ".", "size", "(", ")", ")", "-", "2", ")", ")", ")", "\n", "m_init_shape", "=", "m_init", ".", "view", "(", "\n", "1", ",", "self", ".", "out_channels", ",", "*", "(", "[", "1", "]", "*", "(", "len", "(", "x_init", ".", "size", "(", ")", ")", "-", "2", ")", ")", ")", "\n", "\n", "x_init", "=", "scale_init_shape", ".", "expand_as", "(", "x_init", ")", "*", "(", "x_init", "-", "m_init_shape", ".", "expand_as", "(", "x_init", ")", ")", "\n", "self", ".", "V_avg", ".", "copy_", "(", "self", ".", "V", ".", "data", ")", "\n", "self", ".", "g_avg", ".", "copy_", "(", "self", ".", "g", ".", "data", ")", "\n", "self", ".", "b_avg", ".", "copy_", "(", "self", ".", "b", ".", "data", ")", "\n", "return", "Variable", "(", "x_init", ")", "\n", "", "else", ":", "\n", "            ", "V", ",", "g", ",", "b", "=", "get_vars_maybe_avg", "(", "\n", "self", ",", "[", "'V'", ",", "'g'", ",", "'b'", "]", ",", "self", ".", "training", ",", "\n", "polyak_decay", "=", "self", ".", "polyak_decay", ")", "\n", "scalar", "=", "g", "/", "torch", ".", "norm", "(", "V", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "self", ".", "out_channels", ",", "-", "1", ")", ",", "2", ",", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "w", "=", "scalar", ".", "view", "(", "self", ".", "in_channels", ",", "self", ".", "out_channels", ",", "\n", "*", "(", "[", "1", "]", "*", "(", "len", "(", "V", ".", "size", "(", ")", ")", "-", "2", ")", ")", ")", ".", "expand_as", "(", "V", ")", "*", "V", "\n", "\n", "x", "=", "F", ".", "conv_transpose2d", "(", "x", ",", "w", ",", "b", ",", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "self", ".", "output_padding", ",", "\n", "self", ".", "groups", ")", "\n", "return", "x", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_var_maybe_avg": [[8, 19], ["getattr", "getattr", "torch.autograd.Variable"], "function", ["None"], ["def", "get_var_maybe_avg", "(", "namespace", ",", "var_name", ",", "training", ",", "polyak_decay", ")", ":", "\n", "# utility for retrieving polyak averaged params", "\n", "# Update average", "\n", "    ", "v", "=", "getattr", "(", "namespace", ",", "var_name", ")", "\n", "v_avg", "=", "getattr", "(", "namespace", ",", "var_name", "+", "'_avg'", ")", "\n", "v_avg", "-=", "(", "1", "-", "polyak_decay", ")", "*", "(", "v_avg", "-", "v", ".", "data", ")", "\n", "\n", "if", "training", ":", "\n", "        ", "return", "v", "\n", "", "else", ":", "\n", "        ", "return", "Variable", "(", "v_avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_vars_maybe_avg": [[21, 28], ["vars.append", "WeightNorm.get_var_maybe_avg"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.WeightNorm.get_var_maybe_avg"], ["", "", "def", "get_vars_maybe_avg", "(", "namespace", ",", "var_names", ",", "training", ",", "polyak_decay", ")", ":", "\n", "# utility for retrieving polyak averaged params", "\n", "    ", "vars", "=", "[", "]", "\n", "for", "vn", "in", "var_names", ":", "\n", "        ", "vars", ".", "append", "(", "get_var_maybe_avg", "(", "\n", "namespace", ",", "vn", ",", "training", ",", "polyak_decay", ")", ")", "\n", "", "return", "vars", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGenerator.__init__": [[61, 66], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", "CopyGenerator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "len", "(", "tgt_dict", ")", ")", "\n", "self", ".", "linear_copy", "=", "nn", ".", "Linear", "(", "input_size", ",", "1", ")", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGenerator.forward": [[67, 103], ["hidden.size", "attn.size", "src_map.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "CopyGenerator.CopyGenerator.linear", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "torch.bmm().transpose", "copy_prob.contiguous().view.contiguous().view.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "float", "CopyGenerator.CopyGenerator.linear_copy", "torch.sigmoid.expand_as", "torch.sigmoid.expand_as", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "copy_prob.contiguous().view.contiguous().view.contiguous", "torch.mul.view().transpose", "torch.mul.view().transpose", "torch.mul.view().transpose", "torch.mul.view().transpose", "src_map.transpose", "torch.mul.view", "torch.mul.view", "torch.mul.view", "torch.mul.view"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "forward", "(", "self", ",", "hidden", ",", "attn", ",", "src_map", ")", ":", "\n", "        ", "\"\"\"\n        Compute a distribution over the target dictionary\n        extended by the dynamic dictionary implied by compying\n        source words.\n\n        Args:\n           hidden (`FloatTensor`): hidden outputs `[batch*tlen, input_size]`\n           attn (`FloatTensor`): attn for each `[batch*tlen, input_size]`\n           src_map (`FloatTensor`):\n             A sparse indicator matrix mapping each source word to\n             its index in the \"extended\" vocab containing.\n             `[src_len, batch, extra_words]`\n        \"\"\"", "\n", "# CHECKS", "\n", "batch_by_tlen", ",", "_", "=", "hidden", ".", "size", "(", ")", "\n", "batch_by_tlen_", ",", "slen", "=", "attn", ".", "size", "(", ")", "\n", "slen_", ",", "batch", ",", "cvocab", "=", "src_map", ".", "size", "(", ")", "\n", "aeq", "(", "batch_by_tlen", ",", "batch_by_tlen_", ")", "\n", "aeq", "(", "slen", ",", "slen_", ")", "\n", "\n", "# Original probabilities.", "\n", "logits", "=", "self", ".", "linear", "(", "hidden", ")", "\n", "logits", "[", ":", ",", "self", ".", "tgt_dict", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "]", "=", "-", "float", "(", "'inf'", ")", "\n", "prob", "=", "F", ".", "softmax", "(", "logits", ")", "\n", "\n", "# Probability of copying p(z=1) batch.", "\n", "p_copy", "=", "F", ".", "sigmoid", "(", "self", ".", "linear_copy", "(", "hidden", ")", ")", "\n", "# Probibility of not copying: p_{word}(w) * (1 - p(z))", "\n", "out_prob", "=", "torch", ".", "mul", "(", "prob", ",", "1", "-", "p_copy", ".", "expand_as", "(", "prob", ")", ")", "\n", "mul_attn", "=", "torch", ".", "mul", "(", "attn", ",", "p_copy", ".", "expand_as", "(", "attn", ")", ")", "\n", "copy_prob", "=", "torch", ".", "bmm", "(", "mul_attn", ".", "view", "(", "-", "1", ",", "batch", ",", "slen", ")", "\n", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "src_map", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "copy_prob", "=", "copy_prob", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "cvocab", ")", "\n", "return", "torch", ".", "cat", "(", "[", "out_prob", ",", "copy_prob", "]", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorCriterion.__init__": [[106, 111], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "force_copy", ",", "pad", ",", "eps", "=", "1e-20", ")", ":", "\n", "        ", "self", ".", "force_copy", "=", "force_copy", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "offset", "=", "vocab_size", "\n", "self", ".", "pad", "=", "pad", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorCriterion.__call__": [[112, 139], ["align.eq().float", "align.ne().float", "target.eq().float", "target.ne().float", "scores.gather().view", "scores.gather().view", "scores.gather().view.mul", "scores.gather().view.log().mul", "align.eq", "align.ne", "target.eq", "target.ne", "scores.gather", "scores.gather", "scores.gather().view.mul", "scores.gather().view.mul().mul", "scores.gather().view.mul", "target.ne().float", "target.view", "scores.gather().view.log", "align.view", "scores.gather().view.mul", "target.ne"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log"], ["", "def", "__call__", "(", "self", ",", "scores", ",", "align", ",", "target", ")", ":", "\n", "# Compute unks in align and target for readability", "\n", "        ", "align_unk", "=", "align", ".", "eq", "(", "0", ")", ".", "float", "(", ")", "\n", "align_not_unk", "=", "align", ".", "ne", "(", "0", ")", ".", "float", "(", ")", "\n", "target_unk", "=", "target", ".", "eq", "(", "0", ")", ".", "float", "(", ")", "\n", "target_not_unk", "=", "target", ".", "ne", "(", "0", ")", ".", "float", "(", ")", "\n", "\n", "# Copy probability of tokens in source", "\n", "out", "=", "scores", ".", "gather", "(", "1", ",", "align", ".", "view", "(", "-", "1", ",", "1", ")", "+", "self", ".", "offset", ")", ".", "view", "(", "-", "1", ")", "\n", "# Set scores for unk to 0 and add eps", "\n", "out", "=", "out", ".", "mul", "(", "align_not_unk", ")", "+", "self", ".", "eps", "\n", "# Get scores for tokens in target", "\n", "tmp", "=", "scores", ".", "gather", "(", "1", ",", "target", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "# Regular prob (no unks and unks that can't be copied)", "\n", "if", "not", "self", ".", "force_copy", ":", "\n", "# Add score for non-unks in target", "\n", "            ", "out", "=", "out", "+", "tmp", ".", "mul", "(", "target_not_unk", ")", "\n", "# Add score for when word is unk in both align and tgt", "\n", "out", "=", "out", "+", "tmp", ".", "mul", "(", "align_unk", ")", ".", "mul", "(", "target_unk", ")", "\n", "", "else", ":", "\n", "# Forced copy. Add only probability for not-copied tokens", "\n", "            ", "out", "=", "out", "+", "tmp", ".", "mul", "(", "align_unk", ")", "\n", "\n", "# Drop padding.", "\n", "", "loss", "=", "-", "out", ".", "log", "(", ")", ".", "mul", "(", "target", ".", "ne", "(", "self", ".", "pad", ")", ".", "float", "(", ")", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute.__init__": [[145, 158], ["super().__init__", "CopyGenerator.CopyGeneratorCriterion", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "generator", ",", "tgt_vocab", ",", "\n", "force_copy", ",", "normalize_by_length", ",", "\n", "eps", "=", "1e-20", ")", ":", "\n", "        ", "super", "(", "CopyGeneratorLossCompute", ",", "self", ")", ".", "__init__", "(", "\n", "generator", ",", "tgt_vocab", ")", "\n", "\n", "# We lazily load datasets when there are more than one, so postpone", "\n", "# the setting of cur_dataset.", "\n", "self", ".", "cur_dataset", "=", "None", "\n", "self", ".", "force_copy", "=", "force_copy", "\n", "self", ".", "normalize_by_length", "=", "normalize_by_length", "\n", "self", ".", "criterion", "=", "CopyGeneratorCriterion", "(", "len", "(", "tgt_vocab", ")", ",", "force_copy", ",", "\n", "self", ".", "padding_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._make_shard_state": [[159, 170], ["getattr", "AssertionError", "attns.get"], "methods", ["None"], ["", "def", "_make_shard_state", "(", "self", ",", "batch", ",", "output", ",", "range_", ",", "attns", ")", ":", "\n", "        ", "\"\"\" See base class for args description. \"\"\"", "\n", "if", "getattr", "(", "batch", ",", "\"alignment\"", ",", "None", ")", "is", "None", ":", "\n", "            ", "raise", "AssertionError", "(", "\"using -copy_attn you need to pass in \"", "\n", "\"-dynamic_dict during preprocess stage.\"", ")", "\n", "\n", "", "return", "{", "\n", "\"output\"", ":", "output", ",", "\n", "\"target\"", ":", "batch", ".", "tgt", "[", "range_", "[", "0", "]", "+", "1", ":", "range_", "[", "1", "]", "]", ",", "\n", "\"copy_attn\"", ":", "attns", ".", "get", "(", "\"copy\"", ")", ",", "\n", "\"align\"", ":", "batch", ".", "alignment", "[", "range_", "[", "0", "]", "+", "1", ":", "range_", "[", "1", "]", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.CopyGenerator.CopyGeneratorLossCompute._compute_loss": [[172, 219], ["target.view.view.view", "align.view.view.view", "CopyGenerator.CopyGeneratorLossCompute.generator", "CopyGenerator.CopyGeneratorLossCompute.criterion", "CopyGenerator.CopyGeneratorLossCompute.data.clone", "onmt.io.TextDataset.collapse_copy_scores", "onmt.io.TextDataset.collapse_copy_scores", "onmt.io.TextDataset.collapse_copy_scores", "onmt.io.TextDataset.collapse_copy_scores", "CopyGenerator.CopyGeneratorLossCompute._bottle", "target.view.view.data.clone", "loss.sum.sum.sum().data.clone", "CopyGenerator.CopyGeneratorLossCompute._stats", "CopyGenerator.CopyGeneratorLossCompute._bottle", "CopyGenerator.CopyGeneratorLossCompute._bottle", "CopyGenerator.CopyGeneratorLossCompute._unbottle", "target.view.data.clone.eq", "align.view.view.data.ne", "correct_mask.long", "batch.tgt.ne().float().sum", "loss.sum.sum.view().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "torch.div().sum", "loss.sum.sum.sum", "len", "loss.sum.sum.sum", "batch.tgt.ne().float", "loss.sum.sum.view", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "torch.div", "batch.tgt.ne"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.collapse_copy_scores", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.collapse_copy_scores", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.collapse_copy_scores", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.collapse_copy_scores", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._bottle", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._stats", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._bottle", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._bottle", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Loss.LossComputeBase._unbottle"], ["", "def", "_compute_loss", "(", "self", ",", "batch", ",", "output", ",", "target", ",", "copy_attn", ",", "align", ")", ":", "\n", "        ", "\"\"\"\n        Compute the loss. The args must match self._make_shard_state().\n        Args:\n            batch: the current batch.\n            output: the predict output from the model.\n            target: the validate target to compare output with.\n            copy_attn: the copy attention value.\n            align: the align info.\n        \"\"\"", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "align", "=", "align", ".", "view", "(", "-", "1", ")", "\n", "scores", "=", "self", ".", "generator", "(", "self", ".", "_bottle", "(", "output", ")", ",", "\n", "self", ".", "_bottle", "(", "copy_attn", ")", ",", "\n", "batch", ".", "src_map", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "scores", ",", "align", ",", "target", ")", "\n", "scores_data", "=", "scores", ".", "data", ".", "clone", "(", ")", "\n", "scores_data", "=", "onmt", ".", "io", ".", "TextDataset", ".", "collapse_copy_scores", "(", "\n", "self", ".", "_unbottle", "(", "scores_data", ",", "batch", ".", "batch_size", ")", ",", "\n", "batch", ",", "self", ".", "tgt_vocab", ",", "self", ".", "cur_dataset", ".", "src_vocabs", ")", "\n", "scores_data", "=", "self", ".", "_bottle", "(", "scores_data", ")", "\n", "\n", "# Correct target copy token instead of <unk>", "\n", "# tgt[i] = align[i] + len(tgt_vocab)", "\n", "# for i such that tgt[i] == 0 and align[i] != 0", "\n", "target_data", "=", "target", ".", "data", ".", "clone", "(", ")", "\n", "correct_mask", "=", "target_data", ".", "eq", "(", "0", ")", "*", "align", ".", "data", ".", "ne", "(", "0", ")", "\n", "correct_copy", "=", "(", "align", ".", "data", "+", "len", "(", "self", ".", "tgt_vocab", ")", ")", "*", "correct_mask", ".", "long", "(", ")", "\n", "target_data", "=", "target_data", "+", "correct_copy", "\n", "\n", "# Compute sum of perplexities for stats", "\n", "loss_data", "=", "loss", ".", "sum", "(", ")", ".", "data", ".", "clone", "(", ")", "\n", "stats", "=", "self", ".", "_stats", "(", "loss_data", ",", "scores_data", ",", "target_data", ")", "\n", "\n", "if", "self", ".", "normalize_by_length", ":", "\n", "# Compute Loss as NLL divided by seq length", "\n", "# Compute Sequence Lengths", "\n", "            ", "pad_ix", "=", "batch", ".", "dataset", ".", "fields", "[", "'tgt'", "]", ".", "vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "\n", "tgt_lens", "=", "batch", ".", "tgt", ".", "ne", "(", "pad_ix", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "\n", "# Compute Total Loss per sequence in batch", "\n", "loss", "=", "loss", ".", "view", "(", "-", "1", ",", "batch", ".", "batch_size", ")", ".", "sum", "(", "0", ")", "\n", "# Divide by length of each sequence and sum", "\n", "loss", "=", "torch", ".", "div", "(", "loss", ",", "tgt_lens", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "\n", "", "return", "loss", ",", "stats", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.MultiHeadedAttn.MultiHeadedAttention.__init__": [[50, 67], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "head_count", ",", "model_dim", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "assert", "model_dim", "%", "head_count", "==", "0", "\n", "self", ".", "dim_per_head", "=", "model_dim", "//", "head_count", "\n", "self", ".", "model_dim", "=", "model_dim", "\n", "\n", "super", "(", "MultiHeadedAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "head_count", "=", "head_count", "\n", "\n", "self", ".", "linear_keys", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "linear_values", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "linear_query", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "model_dim", ",", "model_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.MultiHeadedAttn.MultiHeadedAttention.forward": [[68, 151], ["key.size", "value.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "query.size", "onmt.Utils.aeq", "onmt.Utils.aeq", "onmt.Utils.aeq", "key.size", "key.size", "query.size", "MultiHeadedAttn.MultiHeadedAttention.forward.shape"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq"], ["", "def", "forward", "(", "self", ",", "key", ",", "value", ",", "query", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Compute the context vector and the attention vectors.\n\n        Args:\n           key (`FloatTensor`): set of `key_len`\n                key vectors `[batch, key_len, dim]`\n           value (`FloatTensor`): set of `key_len`\n                value vectors `[batch, key_len, dim]`\n           query (`FloatTensor`): set of `query_len`\n                 query vectors  `[batch, query_len, dim]`\n           mask: binary mask indicating which keys have\n                 non-zero attention `[batch, query_len, key_len]`\n        Returns:\n           (`FloatTensor`, `FloatTensor`) :\n\n           * output context vectors `[batch, query_len, dim]`\n           * one of the attention vectors `[batch, query_len, key_len]`\n        \"\"\"", "\n", "\n", "# CHECKS", "\n", "batch", ",", "k_len", ",", "d", "=", "key", ".", "size", "(", ")", "\n", "batch_", ",", "k_len_", ",", "d_", "=", "value", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "k_len", ",", "k_len_", ")", "\n", "aeq", "(", "d", ",", "d_", ")", "\n", "batch_", ",", "q_len", ",", "d_", "=", "query", ".", "size", "(", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "d", ",", "d_", ")", "\n", "aeq", "(", "self", ".", "model_dim", "%", "8", ",", "0", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "batch_", ",", "q_len_", ",", "k_len_", "=", "mask", ".", "size", "(", ")", "\n", "aeq", "(", "batch_", ",", "batch", ")", "\n", "aeq", "(", "k_len_", ",", "k_len", ")", "\n", "aeq", "(", "q_len_", "==", "q_len", ")", "\n", "# END CHECKS", "\n", "\n", "", "batch_size", "=", "key", ".", "size", "(", "0", ")", "\n", "dim_per_head", "=", "self", ".", "dim_per_head", "\n", "head_count", "=", "self", ".", "head_count", "\n", "key_len", "=", "key", ".", "size", "(", "1", ")", "\n", "query_len", "=", "query", ".", "size", "(", "1", ")", "\n", "\n", "def", "shape", "(", "x", ")", ":", "\n", "            ", "return", "x", ".", "view", "(", "batch_size", ",", "-", "1", ",", "head_count", ",", "dim_per_head", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "", "def", "unshape", "(", "x", ")", ":", "\n", "            ", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "head_count", "*", "dim_per_head", ")", "\n", "\n", "# 1) Project key, value, and query.", "\n", "", "key_up", "=", "shape", "(", "self", ".", "linear_keys", "(", "key", ")", ")", "\n", "value_up", "=", "shape", "(", "self", ".", "linear_values", "(", "value", ")", ")", "\n", "query_up", "=", "shape", "(", "self", ".", "linear_query", "(", "query", ")", ")", "\n", "\n", "# 2) Calculate and scale scores.", "\n", "query_up", "=", "query_up", "/", "math", ".", "sqrt", "(", "dim_per_head", ")", "\n", "scores", "=", "torch", ".", "matmul", "(", "query_up", ",", "key_up", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "scores", ")", "\n", "scores", "=", "scores", ".", "masked_fill", "(", "Variable", "(", "mask", ")", ",", "-", "1e18", ")", "\n", "\n", "# 3) Apply attention dropout and compute context vectors.", "\n", "", "attn", "=", "self", ".", "sm", "(", "scores", ")", "\n", "drop_attn", "=", "self", ".", "dropout", "(", "attn", ")", "\n", "context", "=", "unshape", "(", "torch", ".", "matmul", "(", "drop_attn", ",", "value_up", ")", ")", "\n", "\n", "output", "=", "self", ".", "final_linear", "(", "context", ")", "\n", "# CHECK", "\n", "batch_", ",", "q_len_", ",", "d_", "=", "output", ".", "size", "(", ")", "\n", "aeq", "(", "q_len", ",", "q_len_", ")", "\n", "aeq", "(", "batch", ",", "batch_", ")", "\n", "aeq", "(", "d", ",", "d_", ")", "\n", "\n", "# Return one attn", "\n", "top_attn", "=", "attn", ".", "view", "(", "batch_size", ",", "head_count", ",", "\n", "query_len", ",", "key_len", ")", "[", ":", ",", "0", ",", ":", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "# END CHECK", "\n", "return", "output", ",", "top_attn", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.UtilClass.LayerNorm.__init__": [[6, 11], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["    ", "def", "__init__", "(", "self", ",", "features", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "a_2", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "features", ")", ")", "\n", "self", ".", "b_2", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "features", ")", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.UtilClass.LayerNorm.forward": [[12, 16], ["x.mean", "x.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "a_2", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "b_2", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.UtilClass.Elementwise.__init__": [[29, 33], ["torch.ModuleList.__init__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__"], ["def", "__init__", "(", "self", ",", "merge", "=", "None", ",", "*", "args", ")", ":", "\n", "        ", "assert", "merge", "in", "[", "None", ",", "'first'", ",", "'concat'", ",", "'sum'", ",", "'mlp'", "]", "\n", "self", ".", "merge", "=", "merge", "\n", "super", "(", "Elementwise", ",", "self", ")", ".", "__init__", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.UtilClass.Elementwise.forward": [[34, 46], ["feat.squeeze", "len", "len", "f", "input.split", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "inputs", "=", "[", "feat", ".", "squeeze", "(", "2", ")", "for", "feat", "in", "input", ".", "split", "(", "1", ",", "dim", "=", "2", ")", "]", "\n", "assert", "len", "(", "self", ")", "==", "len", "(", "inputs", ")", "\n", "outputs", "=", "[", "f", "(", "x", ")", "for", "f", ",", "x", "in", "zip", "(", "self", ",", "inputs", ")", "]", "\n", "if", "self", ".", "merge", "==", "'first'", ":", "\n", "            ", "return", "outputs", "[", "0", "]", "\n", "", "elif", "self", ".", "merge", "==", "'concat'", "or", "self", ".", "merge", "==", "'mlp'", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "outputs", ",", "2", ")", "\n", "", "elif", "self", ".", "merge", "==", "'sum'", ":", "\n", "            ", "return", "sum", "(", "outputs", ")", "\n", "", "else", ":", "\n", "            ", "return", "outputs", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.__init__": [[13, 16], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cov_pen", ",", "length_pen", ")", ":", "\n", "        ", "self", ".", "length_pen", "=", "length_pen", "\n", "self", ".", "cov_pen", "=", "cov_pen", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.coverage_penalty": [[17, 24], ["None"], "methods", ["None"], ["", "def", "coverage_penalty", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "cov_pen", "==", "\"wu\"", ":", "\n", "            ", "return", "self", ".", "coverage_wu", "\n", "", "elif", "self", ".", "cov_pen", "==", "\"summary\"", ":", "\n", "            ", "return", "self", ".", "coverage_summary", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "coverage_none", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_penalty": [[25, 32], ["None"], "methods", ["None"], ["", "", "def", "length_penalty", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "length_pen", "==", "\"wu\"", ":", "\n", "            ", "return", "self", ".", "length_wu", "\n", "", "elif", "self", ".", "length_pen", "==", "\"avg\"", ":", "\n", "            ", "return", "self", ".", "length_average", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "length_none", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.coverage_wu": [[37, 44], ["torch.min().log().sum", "torch.min().log", "torch.min", "cov.clone().fill_", "cov.clone"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log"], ["def", "coverage_wu", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        NMT coverage re-ranking score from\n        \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n        \"\"\"", "\n", "penalty", "=", "-", "torch", ".", "min", "(", "cov", ",", "cov", ".", "clone", "(", ")", ".", "fill_", "(", "1.0", ")", ")", ".", "log", "(", ")", ".", "sum", "(", "1", ")", "\n", "return", "beta", "*", "penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.coverage_summary": [[45, 52], ["torch.max().sum", "cov.size", "torch.max", "cov.clone().fill_", "cov.clone"], "methods", ["None"], ["", "def", "coverage_summary", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Our summary penalty.\n        \"\"\"", "\n", "penalty", "=", "torch", ".", "max", "(", "cov", ",", "cov", ".", "clone", "(", ")", ".", "fill_", "(", "1.0", ")", ")", ".", "sum", "(", "1", ")", "\n", "penalty", "-=", "cov", ".", "size", "(", "1", ")", "\n", "return", "beta", "*", "penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.coverage_none": [[53, 58], ["beam.scores.clone().fill_", "beam.scores.clone"], "methods", ["None"], ["", "def", "coverage_none", "(", "self", ",", "beam", ",", "cov", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        returns zero as penalty\n        \"\"\"", "\n", "return", "beam", ".", "scores", ".", "clone", "(", ")", ".", "fill_", "(", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_wu": [[59, 68], ["len"], "methods", ["None"], ["", "def", "length_wu", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        NMT length re-ranking score from\n        \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n        \"\"\"", "\n", "\n", "modifier", "=", "(", "(", "(", "5", "+", "len", "(", "beam", ".", "next_ys", ")", ")", "**", "alpha", ")", "/", "\n", "(", "(", "5", "+", "1", ")", "**", "alpha", ")", ")", "\n", "return", "(", "logprobs", "/", "modifier", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_average": [[69, 74], ["len"], "methods", ["None"], ["", "def", "length_average", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Returns the average probability of tokens in a sequence.\n        \"\"\"", "\n", "return", "logprobs", "/", "len", "(", "beam", ".", "next_ys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_none": [[75, 80], ["None"], "methods", ["None"], ["", "def", "length_none", "(", "self", ",", "beam", ",", "logprobs", ",", "alpha", "=", "0.", ",", "beta", "=", "0.", ")", ":", "\n", "        ", "\"\"\"\n        Returns unmodified scores.\n        \"\"\"", "\n", "return", "logprobs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.__init__": [[67, 132], ["set"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "fields", ",", "\n", "beam_size", ",", "\n", "n_best", "=", "1", ",", "\n", "max_length", "=", "100", ",", "\n", "global_scorer", "=", "None", ",", "\n", "eos_norm", "=", "0", ",", "\n", "copy_attn", "=", "False", ",", "\n", "gpu", "=", "False", ",", "\n", "dump_beam", "=", "\"\"", ",", "\n", "min_length", "=", "0", ",", "\n", "stepwise_penalty", "=", "False", ",", "\n", "block_ngram_repeat", "=", "0", ",", "\n", "ignore_when_blocking", "=", "[", "]", ",", "\n", "sample_rate", "=", "'16000'", ",", "\n", "window_size", "=", ".02", ",", "\n", "window_stride", "=", ".01", ",", "\n", "window", "=", "'hamming'", ",", "\n", "use_filter_pred", "=", "False", ",", "\n", "data_type", "=", "\"text\"", ",", "\n", "replace_unk", "=", "False", ",", "\n", "report_score", "=", "True", ",", "\n", "report_bleu", "=", "False", ",", "\n", "report_rouge", "=", "False", ",", "\n", "verbose", "=", "False", ",", "\n", "out_file", "=", "None", ")", ":", "\n", "        ", "self", ".", "gpu", "=", "gpu", "\n", "self", ".", "cuda", "=", "gpu", ">", "-", "1", "\n", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "fields", "=", "fields", "\n", "self", ".", "n_best", "=", "n_best", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "global_scorer", "=", "global_scorer", "\n", "self", ".", "eos_norm", "=", "eos_norm", "\n", "self", ".", "copy_attn", "=", "copy_attn", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "min_length", "=", "min_length", "\n", "self", ".", "stepwise_penalty", "=", "stepwise_penalty", "\n", "self", ".", "dump_beam", "=", "dump_beam", "\n", "self", ".", "block_ngram_repeat", "=", "block_ngram_repeat", "\n", "self", ".", "ignore_when_blocking", "=", "set", "(", "ignore_when_blocking", ")", "\n", "self", ".", "sample_rate", "=", "sample_rate", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_stride", "=", "window_stride", "\n", "self", ".", "window", "=", "window", "\n", "self", ".", "use_filter_pred", "=", "use_filter_pred", "\n", "self", ".", "replace_unk", "=", "replace_unk", "\n", "self", ".", "data_type", "=", "data_type", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "out_file", "=", "out_file", "\n", "self", ".", "report_score", "=", "report_score", "\n", "self", ".", "report_bleu", "=", "report_bleu", "\n", "self", ".", "report_rouge", "=", "report_rouge", "\n", "\n", "# for debugging", "\n", "self", ".", "beam_trace", "=", "self", ".", "dump_beam", "!=", "\"\"", "\n", "self", ".", "beam_accum", "=", "None", "\n", "if", "self", ".", "beam_trace", ":", "\n", "            ", "self", ".", "beam_accum", "=", "{", "\n", "\"predicted_ids\"", ":", "[", "]", ",", "\n", "\"beam_parent_ids\"", ":", "[", "]", ",", "\n", "\"scores\"", ":", "[", "]", ",", "\n", "\"log_probs\"", ":", "[", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.translate": [[133, 216], ["onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.build_dataset", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.io.OrderedIterator", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "onmt.translate.TranslationBuilder", "itertools.count", "Translator.Translator.translate_batch", "onmt.translate.TranslationBuilder.from_batch", "onmt.translate.TranslationBuilder.from_batch", "onmt.translate.TranslationBuilder.from_batch", "onmt.translate.TranslationBuilder.from_batch", "Translator.Translator._report_score", "json.dump", "len", "Translator.Translator.out_file.write", "Translator.Translator.out_file.flush", "Translator.Translator._report_score", "codecs.open", "next", "trans.log", "os.write", "preds.append", "trans.attns[].tolist", "zip", "os.write", "Translator.Translator._report_bleu", "Translator.Translator._report_rouge", "len", "trans.log.encode", "header_format.format", "row.index", "row_format.replace.replace.replace", "row_format.replace.replace.replace", "trans.log.encode", "len", "len", "max", "row_format.replace.replace.format", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.translate_batch", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.from_batch", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.from_batch", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.from_batch", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.from_batch", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_bleu", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_rouge", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.encode", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.encode"], ["", "", "def", "translate", "(", "self", ",", "src_dir", ",", "src_path", ",", "tgt_path", ",", "\n", "batch_size", ",", "attn_debug", "=", "False", ")", ":", "\n", "        ", "data", "=", "onmt", ".", "io", ".", "build_dataset", "(", "self", ".", "fields", ",", "\n", "self", ".", "data_type", ",", "\n", "src_path", ",", "\n", "tgt_path", ",", "\n", "src_dir", "=", "src_dir", ",", "\n", "sample_rate", "=", "self", ".", "sample_rate", ",", "\n", "window_size", "=", "self", ".", "window_size", ",", "\n", "window_stride", "=", "self", ".", "window_stride", ",", "\n", "window", "=", "self", ".", "window", ",", "\n", "use_filter_pred", "=", "self", ".", "use_filter_pred", ")", "\n", "\n", "data_iter", "=", "onmt", ".", "io", ".", "OrderedIterator", "(", "\n", "dataset", "=", "data", ",", "device", "=", "'cuda'", "if", "self", ".", "gpu", ">", "-", "1", "else", "'cpu'", ",", "\n", "batch_size", "=", "batch_size", ",", "train", "=", "False", ",", "sort", "=", "False", ",", "\n", "sort_within_batch", "=", "True", ",", "shuffle", "=", "False", ")", "\n", "\n", "builder", "=", "onmt", ".", "translate", ".", "TranslationBuilder", "(", "\n", "data", ",", "self", ".", "fields", ",", "\n", "self", ".", "n_best", ",", "self", ".", "replace_unk", ",", "tgt_path", ")", "\n", "\n", "# Statistics", "\n", "counter", "=", "count", "(", "1", ")", "\n", "pred_score_total", ",", "pred_words_total", "=", "0", ",", "0", "\n", "gold_score_total", ",", "gold_words_total", "=", "0", ",", "0", "\n", "\n", "all_scores", "=", "[", "]", "\n", "for", "batch", "in", "data_iter", ":", "\n", "            ", "batch_data", "=", "self", ".", "translate_batch", "(", "batch", ",", "data", ")", "\n", "translations", "=", "builder", ".", "from_batch", "(", "batch_data", ")", "\n", "\n", "for", "trans", "in", "translations", ":", "\n", "                ", "all_scores", "+=", "[", "trans", ".", "pred_scores", "[", "0", "]", "]", "\n", "pred_score_total", "+=", "trans", ".", "pred_scores", "[", "0", "]", "\n", "pred_words_total", "+=", "len", "(", "trans", ".", "pred_sents", "[", "0", "]", ")", "\n", "if", "tgt_path", "is", "not", "None", ":", "\n", "                    ", "gold_score_total", "+=", "trans", ".", "gold_score", "\n", "gold_words_total", "+=", "len", "(", "trans", ".", "gold_sent", ")", "+", "1", "\n", "\n", "", "n_best_preds", "=", "[", "\" \"", ".", "join", "(", "pred", ")", "\n", "for", "pred", "in", "trans", ".", "pred_sents", "[", ":", "self", ".", "n_best", "]", "]", "\n", "self", ".", "out_file", ".", "write", "(", "'\\n'", ".", "join", "(", "n_best_preds", ")", "+", "'\\n'", ")", "\n", "self", ".", "out_file", ".", "flush", "(", ")", "\n", "\n", "if", "self", ".", "verbose", ":", "\n", "                    ", "sent_number", "=", "next", "(", "counter", ")", "\n", "output", "=", "trans", ".", "log", "(", "sent_number", ")", "\n", "os", ".", "write", "(", "1", ",", "output", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "\n", "# Debug attention.", "\n", "", "if", "attn_debug", ":", "\n", "                    ", "srcs", "=", "trans", ".", "src_raw", "\n", "preds", "=", "trans", ".", "pred_sents", "[", "0", "]", "\n", "preds", ".", "append", "(", "'</s>'", ")", "\n", "attns", "=", "trans", ".", "attns", "[", "0", "]", ".", "tolist", "(", ")", "\n", "header_format", "=", "\"{:>10.10} \"", "+", "\"{:>10.7} \"", "*", "len", "(", "srcs", ")", "\n", "row_format", "=", "\"{:>10.10} \"", "+", "\"{:>10.7f} \"", "*", "len", "(", "srcs", ")", "\n", "output", "=", "header_format", ".", "format", "(", "\"\"", ",", "*", "trans", ".", "src_raw", ")", "+", "'\\n'", "\n", "for", "word", ",", "row", "in", "zip", "(", "preds", ",", "attns", ")", ":", "\n", "                        ", "max_index", "=", "row", ".", "index", "(", "max", "(", "row", ")", ")", "\n", "row_format", "=", "row_format", ".", "replace", "(", "\n", "\"{:>10.7f} \"", ",", "\"{:*>10.7f} \"", ",", "max_index", "+", "1", ")", "\n", "row_format", "=", "row_format", ".", "replace", "(", "\n", "\"{:*>10.7f} \"", ",", "\"{:>10.7f} \"", ",", "max_index", ")", "\n", "output", "+=", "row_format", ".", "format", "(", "word", ",", "*", "row", ")", "+", "'\\n'", "\n", "row_format", "=", "\"{:>10.10} \"", "+", "\"{:>10.7f} \"", "*", "len", "(", "srcs", ")", "\n", "", "os", ".", "write", "(", "1", ",", "output", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "\n", "", "", "", "if", "self", ".", "report_score", ":", "\n", "            ", "self", ".", "_report_score", "(", "'PRED'", ",", "pred_score_total", ",", "pred_words_total", ")", "\n", "if", "tgt_path", "is", "not", "None", ":", "\n", "                ", "self", ".", "_report_score", "(", "'GOLD'", ",", "gold_score_total", ",", "gold_words_total", ")", "\n", "if", "self", ".", "report_bleu", ":", "\n", "                    ", "self", ".", "_report_bleu", "(", "tgt_path", ")", "\n", "", "if", "self", ".", "report_rouge", ":", "\n", "                    ", "self", ".", "_report_rouge", "(", "tgt_path", ")", "\n", "\n", "", "", "", "if", "self", ".", "dump_beam", ":", "\n", "            ", "import", "json", "\n", "json", ".", "dump", "(", "self", ".", "translator", ".", "beam_accum", ",", "\n", "codecs", ".", "open", "(", "self", ".", "dump_beam", ",", "'w'", ",", "'utf-8'", ")", ")", "\n", "", "return", "all_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.translate_batch": [[217, 367], ["set", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "Translator.Translator.model.encoder", "Translator.Translator.model.decoder.init_decoder_state", "Translator.Translator.translate_batch.rvar"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.init_decoder_state"], ["", "def", "translate_batch", "(", "self", ",", "batch", ",", "data", ")", ":", "\n", "        ", "\"\"\"\n        Translate a batch of sentences.\n\n        Mostly a wrapper around :obj:`Beam`.\n\n        Args:\n           batch (:obj:`Batch`): a batch from a dataset object\n           data (:obj:`Dataset`): the dataset object\n\n\n        Todo:\n           Shouldn't need the original dataset.\n        \"\"\"", "\n", "\n", "# (0) Prep each of the components of the search.", "\n", "# And helper method for reducing verbosity.", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "batch_size", "=", "batch", ".", "batch_size", "\n", "data_type", "=", "data", ".", "data_type", "\n", "vocab", "=", "self", ".", "fields", "[", "\"tgt\"", "]", ".", "vocab", "\n", "\n", "# Define a list of tokens to exclude from ngram-blocking", "\n", "# exclusion_list = [\"<t>\", \"</t>\", \".\"]", "\n", "exclusion_tokens", "=", "set", "(", "[", "vocab", ".", "stoi", "[", "t", "]", "\n", "for", "t", "in", "self", ".", "ignore_when_blocking", "]", ")", "\n", "\n", "beam", "=", "[", "onmt", ".", "translate", ".", "Beam", "(", "beam_size", ",", "n_best", "=", "self", ".", "n_best", ",", "\n", "cuda", "=", "self", ".", "cuda", ",", "\n", "global_scorer", "=", "self", ".", "global_scorer", ",", "\n", "pad", "=", "vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", ",", "\n", "eos", "=", "vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "EOS_WORD", "]", ",", "\n", "bos", "=", "vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "BOS_WORD", "]", ",", "\n", "min_length", "=", "self", ".", "min_length", ",", "\n", "stepwise_penalty", "=", "self", ".", "stepwise_penalty", ",", "\n", "block_ngram_repeat", "=", "self", ".", "block_ngram_repeat", ",", "\n", "exclusion_tokens", "=", "exclusion_tokens", ",", "\n", "eos_norm", "=", "self", ".", "eos_norm", ")", "\n", "for", "__", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Help functions for working with beams and batches", "\n", "def", "var", "(", "a", ")", ":", "return", "Variable", "(", "a", ",", "volatile", "=", "True", ")", "\n", "\n", "def", "rvar", "(", "a", ")", ":", "return", "var", "(", "a", ".", "repeat", "(", "1", ",", "beam_size", ",", "1", ")", ")", "\n", "\n", "def", "bottle", "(", "m", ")", ":", "\n", "            ", "return", "m", ".", "view", "(", "batch_size", "*", "beam_size", ",", "-", "1", ")", "\n", "\n", "", "def", "unbottle", "(", "m", ")", ":", "\n", "            ", "return", "m", ".", "view", "(", "beam_size", ",", "batch_size", ",", "-", "1", ")", "\n", "\n", "# (1) Run the encoder on the src.", "\n", "", "src", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'src'", ",", "data_type", ")", "\n", "src_lengths", "=", "None", "\n", "if", "data_type", "==", "'text'", ":", "\n", "            ", "_", ",", "src_lengths", "=", "batch", ".", "src", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "beam", "[", "i", "]", ".", "src_len", "=", "src_lengths", "[", "i", "]", ".", "item", "(", ")", "\n", "", "", "enc_states", ",", "memory_bank", "=", "self", ".", "model", ".", "encoder", "(", "src", ",", "src_lengths", ")", "\n", "dec_states", "=", "self", ".", "model", ".", "decoder", ".", "init_decoder_state", "(", "\n", "src", ",", "memory_bank", ",", "enc_states", ")", "\n", "# zero out", "\n", "dec_states", ".", "hidden", "=", "(", "\n", "dec_states", ".", "hidden", "[", "0", "]", ".", "detach", "(", ")", ".", "fill_", "(", "0", ")", ",", "\n", "dec_states", ".", "hidden", "[", "1", "]", ".", "detach", "(", ")", ".", "fill_", "(", "0", ")", ",", "\n", ")", "\n", "\n", "if", "src_lengths", "is", "None", ":", "\n", "            ", "src_lengths", "=", "torch", ".", "Tensor", "(", "batch_size", ")", ".", "type_as", "(", "memory_bank", ".", "data", ")", ".", "long", "(", ")", ".", "fill_", "(", "memory_bank", ".", "size", "(", "0", ")", ")", "\n", "\n", "# (2) Repeat src objects `beam_size` times.", "\n", "", "src_map", "=", "rvar", "(", "batch", ".", "src_map", ".", "data", ")", "if", "data_type", "==", "'text'", "and", "self", ".", "copy_attn", "else", "None", "\n", "memory_bank", "=", "rvar", "(", "memory_bank", ".", "data", ")", "\n", "memory_lengths", "=", "src_lengths", ".", "repeat", "(", "beam_size", ")", "\n", "dec_states", ".", "repeat_beam_size_times", "(", "beam_size", ")", "\n", "\n", "# (3) run the decoder to generate sentences, using beam search.", "\n", "for", "i", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "            ", "if", "all", "(", "(", "b", ".", "done", "(", ")", "for", "b", "in", "beam", ")", ")", ":", "\n", "                ", "break", "\n", "\n", "# Construct batch x beam_size nxt words.", "\n", "# Get all the pending current beam words and arrange for forward.", "\n", "", "inp", "=", "var", "(", "torch", ".", "stack", "(", "[", "b", ".", "get_current_state", "(", ")", "for", "b", "in", "beam", "]", ")", "\n", ".", "t", "(", ")", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "\n", "# Turn any copied words to UNKs", "\n", "# 0 is unk", "\n", "if", "self", ".", "copy_attn", ":", "\n", "                ", "inp", "=", "inp", ".", "masked_fill", "(", "\n", "inp", ".", "gt", "(", "len", "(", "self", ".", "fields", "[", "\"tgt\"", "]", ".", "vocab", ")", "-", "1", ")", ",", "0", ")", "\n", "\n", "# Temporary kludge solution to handle changed dim expectation", "\n", "# in the decoder", "\n", "", "inp", "=", "inp", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Run one step.", "\n", "if", "isinstance", "(", "self", ".", "model", ",", "onmt", ".", "ViModels", ".", "ViNMTModel", ")", ":", "\n", "                ", "self", ".", "model", ".", "silent", "=", "True", "\n", "self", ".", "model", ".", "use_prior", "=", "True", "\n", "self", ".", "model", ".", "mode", "=", "\"exact\"", "\n", "self", ".", "model", ".", "generator", ".", "mode", "=", "\"exact\"", "\n", "# No Q", "\n", "dec_out", ",", "dec_states", ",", "attn", ",", "dist_info", ",", "_", "=", "self", ".", "model", ".", "decoder", "(", "\n", "inp", ",", "memory_bank", ",", "dec_states", ",", "memory_lengths", "=", "memory_lengths", ")", "\n", "", "else", ":", "\n", "                ", "dist_info", "=", "None", "\n", "dec_out", ",", "dec_states", ",", "attn", "=", "self", ".", "model", ".", "decoder", "(", "\n", "inp", ",", "memory_bank", ",", "dec_states", ",", "memory_lengths", "=", "memory_lengths", ")", "\n", "", "dec_out", "=", "dec_out", ".", "squeeze", "(", "0", ")", "\n", "# dec_out: beam x rnn_size", "\n", "\n", "# (b) Compute a vector of batch x beam word scores.", "\n", "if", "not", "self", ".", "copy_attn", ":", "\n", "                ", "dec_out", "=", "dec_out", ".", "unsqueeze", "(", "0", ")", "\n", "if", "isinstance", "(", "self", ".", "model", ",", "onmt", ".", "ViModels", ".", "ViNMTModel", ")", ":", "\n", "                    ", "out", "=", "self", ".", "model", ".", "generator", ".", "forward", "(", "dec_out", ",", "log_pa", "=", "dist_info", ".", "p", ".", "log_alpha", ")", ".", "data", "\n", "# huh?", "\n", "", "else", ":", "\n", "                    ", "out", "=", "self", ".", "model", ".", "generator", ".", "forward", "(", "dec_out", ")", ".", "data", "\n", "", "out", "=", "unbottle", "(", "out", ".", "squeeze", "(", "0", ")", ")", "\n", "# beam x tgt_vocab", "\n", "beam_attn", "=", "unbottle", "(", "attn", "[", "\"std\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "self", ".", "model", ".", "generator", ".", "forward", "(", "dec_out", ",", "\n", "attn", "[", "\"copy\"", "]", ".", "squeeze", "(", "0", ")", ",", "\n", "src_map", ")", "\n", "# beam x (tgt_vocab + extra_vocab)", "\n", "out", "=", "data", ".", "collapse_copy_scores", "(", "\n", "unbottle", "(", "out", ".", "data", ")", ",", "\n", "batch", ",", "self", ".", "fields", "[", "\"tgt\"", "]", ".", "vocab", ",", "data", ".", "src_vocabs", ")", "\n", "# beam x tgt_vocab", "\n", "out", "=", "out", ".", "log", "(", ")", "\n", "beam_attn", "=", "unbottle", "(", "attn", "[", "\"copy\"", "]", ")", "\n", "# (c) Advance each beam.", "\n", "", "for", "j", ",", "b", "in", "enumerate", "(", "beam", ")", ":", "\n", "                ", "b", ".", "advance", "(", "out", "[", ":", ",", "j", "]", ",", "\n", "beam_attn", ".", "data", "[", ":", ",", "j", ",", ":", "memory_lengths", "[", "j", "]", "]", ")", "\n", "dec_states", ".", "beam_update", "(", "j", ",", "b", ".", "get_current_origin", "(", ")", ",", "beam_size", ")", "\n", "\n", "# (4) Extract sentences from beam.", "\n", "", "", "ret", "=", "self", ".", "_from_beam", "(", "beam", ")", "\n", "ret", "[", "\"gold_score\"", "]", "=", "[", "0", "]", "*", "batch_size", "\n", "if", "\"tgt\"", "in", "batch", ".", "__dict__", ":", "\n", "            ", "ret", "[", "\"gold_score\"", "]", "=", "self", ".", "_run_target", "(", "batch", ",", "data", ")", "\n", "", "ret", "[", "\"batch\"", "]", "=", "batch", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._from_beam": [[368, 384], ["b.sort_finished", "enumerate", "ret[].append", "ret[].append", "ret[].append", "b.get_hyp", "hyps.append", "attn.append"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.sort_finished", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.get_hyp"], ["", "def", "_from_beam", "(", "self", ",", "beam", ")", ":", "\n", "        ", "ret", "=", "{", "\"predictions\"", ":", "[", "]", ",", "\n", "\"scores\"", ":", "[", "]", ",", "\n", "\"attention\"", ":", "[", "]", "}", "\n", "for", "b", "in", "beam", ":", "\n", "            ", "n_best", "=", "self", ".", "n_best", "\n", "scores", ",", "ks", "=", "b", ".", "sort_finished", "(", "minimum", "=", "n_best", ")", "\n", "hyps", ",", "attn", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "(", "times", ",", "k", ")", "in", "enumerate", "(", "ks", "[", ":", "n_best", "]", ")", ":", "\n", "                ", "hyp", ",", "att", "=", "b", ".", "get_hyp", "(", "times", ",", "k", ")", "\n", "hyps", ".", "append", "(", "hyp", ")", "\n", "attn", ".", "append", "(", "att", ")", "\n", "", "ret", "[", "\"predictions\"", "]", ".", "append", "(", "hyps", ")", "\n", "ret", "[", "\"scores\"", "]", ".", "append", "(", "scores", ")", "\n", "ret", "[", "\"attention\"", "]", ".", "append", "(", "attn", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._run_target": [[385, 415], ["onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "Translator.Translator.model.encoder", "Translator.Translator.model.decoder.init_decoder_state", "tt.FloatTensor().fill_", "Translator.Translator.model.decoder", "zip", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "onmt.io.make_features", "Translator.Translator.model.generator.forward", "tgt.unsqueeze.unsqueeze.unsqueeze", "Translator.Translator.data.gather", "Translator.Translator.data.gather.masked_fill_", "tt.FloatTensor", "tgt.unsqueeze.unsqueeze.eq"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.Conv2Conv.CNNDecoder.init_decoder_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.modules.UtilClass.Elementwise.forward"], ["", "def", "_run_target", "(", "self", ",", "batch", ",", "data", ")", ":", "\n", "        ", "data_type", "=", "data", ".", "data_type", "\n", "if", "data_type", "==", "'text'", ":", "\n", "            ", "_", ",", "src_lengths", "=", "batch", ".", "src", "\n", "", "else", ":", "\n", "            ", "src_lengths", "=", "None", "\n", "", "src", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'src'", ",", "data_type", ")", "\n", "tgt_in", "=", "onmt", ".", "io", ".", "make_features", "(", "batch", ",", "'tgt'", ")", "[", ":", "-", "1", "]", "\n", "\n", "#  (1) run the encoder on the src", "\n", "enc_states", ",", "memory_bank", "=", "self", ".", "model", ".", "encoder", "(", "src", ",", "src_lengths", ")", "\n", "dec_states", "=", "self", ".", "model", ".", "decoder", ".", "init_decoder_state", "(", "src", ",", "memory_bank", ",", "enc_states", ")", "\n", "\n", "#  (2) if a target is specified, compute the 'goldScore'", "\n", "#  (i.e. log likelihood) of the target under the model", "\n", "tt", "=", "torch", ".", "cuda", "if", "self", ".", "cuda", "else", "torch", "\n", "gold_scores", "=", "tt", ".", "FloatTensor", "(", "batch", ".", "batch_size", ")", ".", "fill_", "(", "0", ")", "\n", "dec_out", ",", "_", ",", "_", "=", "self", ".", "model", ".", "decoder", "(", "\n", "tgt_in", ",", "memory_bank", ",", "dec_states", ",", "memory_lengths", "=", "src_lengths", ")", "\n", "\n", "tgt_pad", "=", "self", ".", "fields", "[", "\"tgt\"", "]", ".", "vocab", ".", "stoi", "[", "onmt", ".", "io", ".", "PAD_WORD", "]", "\n", "for", "dec", ",", "tgt", "in", "zip", "(", "dec_out", ",", "batch", ".", "tgt", "[", "1", ":", "]", ".", "data", ")", ":", "\n", "# Log prob of each word.", "\n", "            ", "out", "=", "self", ".", "model", ".", "generator", ".", "forward", "(", "dec", ")", "\n", "tgt", "=", "tgt", ".", "unsqueeze", "(", "1", ")", "\n", "scores", "=", "out", ".", "data", ".", "gather", "(", "1", ",", "tgt", ")", "\n", "scores", ".", "masked_fill_", "(", "tgt", ".", "eq", "(", "tgt_pad", ")", ",", "0", ")", "\n", "gold_scores", "+=", "scores", "\n", "", "return", "gold_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_score": [[416, 420], ["print", "math.exp"], "methods", ["None"], ["", "def", "_report_score", "(", "self", ",", "name", ",", "score_total", ",", "words_total", ")", ":", "\n", "        ", "print", "(", "\"%s AVG SCORE: %.4f, %s PPL: %.4f\"", "%", "(", "\n", "name", ",", "score_total", "/", "words_total", ",", "\n", "name", ",", "math", ".", "exp", "(", "-", "score_total", "/", "words_total", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_bleu": [[421, 432], ["print", "subprocess.check_output().decode", "print", "os.path.split", "os.path.realpath", "subprocess.check_output", "subprocess.check_output().decode.strip"], "methods", ["None"], ["", "def", "_report_bleu", "(", "self", ",", "tgt_path", ")", ":", "\n", "        ", "import", "subprocess", "\n", "path", "=", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ")", "[", "0", "]", "\n", "print", "(", ")", "\n", "\n", "res", "=", "subprocess", ".", "check_output", "(", "\"perl %s/tools/multi-bleu.perl %s\"", "\n", "%", "(", "path", ",", "tgt_path", ",", "self", ".", "output", ")", ",", "\n", "stdin", "=", "self", ".", "out_file", ",", "\n", "shell", "=", "True", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "print", "(", "\">> \"", "+", "res", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator._report_rouge": [[433, 442], ["subprocess.check_output().decode", "print", "os.path.split", "subprocess.check_output().decode.strip", "os.path.realpath", "subprocess.check_output"], "methods", ["None"], ["", "def", "_report_rouge", "(", "self", ",", "tgt_path", ")", ":", "\n", "        ", "import", "subprocess", "\n", "path", "=", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ")", "[", "0", "]", "\n", "res", "=", "subprocess", ".", "check_output", "(", "\n", "\"python %s/tools/test_rouge.py -r %s -c STDIN\"", "\n", "%", "(", "path", ",", "tgt_path", ")", ",", "\n", "shell", "=", "True", ",", "\n", "stdin", "=", "self", ".", "out_file", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "print", "(", "res", ".", "strip", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.make_translator": [[16, 46], ["argparse.ArgumentParser", "onmt.opts.model_opts", "onmt.opts.model_opts", "onmt.opts.model_opts", "onmt.opts.model_opts", "onmt.ModelConstructor.load_test_model", "onmt.ModelConstructor.load_test_model", "onmt.ModelConstructor.load_test_model", "onmt.ModelConstructor.load_test_model", "onmt.translate.GNMTGlobalScorer", "onmt.translate.GNMTGlobalScorer", "onmt.translate.GNMTGlobalScorer", "onmt.translate.GNMTGlobalScorer", "Translator.Translator", "codecs.open", "torch.cuda.set_device", "argparse.ArgumentParser.parse_known_args", "getattr"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.load_test_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.load_test_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.load_test_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.load_test_model"], ["def", "make_translator", "(", "opt", ",", "report_score", "=", "True", ",", "out_file", "=", "None", ")", ":", "\n", "    ", "if", "out_file", "is", "None", ":", "\n", "        ", "out_file", "=", "codecs", ".", "open", "(", "opt", ".", "output", ",", "'w'", ",", "'utf-8'", ")", "\n", "\n", "", "if", "opt", ".", "gpu", ">", "-", "1", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "opt", ".", "gpu", ")", "\n", "\n", "", "dummy_parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'train.py'", ")", "\n", "onmt", ".", "opts", ".", "model_opts", "(", "dummy_parser", ")", "\n", "dummy_opt", "=", "dummy_parser", ".", "parse_known_args", "(", "[", "]", ")", "[", "0", "]", "\n", "\n", "fields", ",", "model", ",", "model_opt", "=", "onmt", ".", "ModelConstructor", ".", "load_test_model", "(", "opt", ",", "dummy_opt", ".", "__dict__", ")", "\n", "model", ".", "k", "=", "opt", ".", "k", "\n", "scorer", "=", "onmt", ".", "translate", ".", "GNMTGlobalScorer", "(", "opt", ".", "alpha", ",", "\n", "opt", ".", "beta", ",", "\n", "opt", ".", "coverage_penalty", ",", "\n", "opt", ".", "length_penalty", ")", "\n", "\n", "kwargs", "=", "{", "k", ":", "getattr", "(", "opt", ",", "k", ")", "\n", "for", "k", "in", "[", "\"beam_size\"", ",", "\"n_best\"", ",", "\"max_length\"", ",", "\"min_length\"", ",", "\n", "\"stepwise_penalty\"", ",", "\"block_ngram_repeat\"", ",", "\n", "\"ignore_when_blocking\"", ",", "\"dump_beam\"", ",", "\n", "\"data_type\"", ",", "\"replace_unk\"", ",", "\"gpu\"", ",", "\"verbose\"", "]", "}", "\n", "\n", "translator", "=", "Translator", "(", "model", ",", "fields", ",", "global_scorer", "=", "scorer", ",", "\n", "out_file", "=", "out_file", ",", "report_score", "=", "report_score", ",", "\n", "copy_attn", "=", "model_opt", ".", "copy_attn", ",", "\n", "eos_norm", "=", "opt", ".", "eos_norm", ",", "**", "kwargs", ")", "\n", "return", "translator", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.__init__": [[22, 29], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data", ",", "fields", ",", "n_best", "=", "1", ",", "replace_unk", "=", "False", ",", "\n", "has_tgt", "=", "False", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "fields", "=", "fields", "\n", "self", ".", "n_best", "=", "n_best", "\n", "self", ".", "replace_unk", "=", "replace_unk", "\n", "self", ".", "has_tgt", "=", "has_tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder._build_target_tokens": [[30, 47], ["range", "len", "tokens.append", "tokens.append", "len", "attn[].max", "len"], "methods", ["None"], ["", "def", "_build_target_tokens", "(", "self", ",", "src", ",", "src_vocab", ",", "src_raw", ",", "pred", ",", "attn", ")", ":", "\n", "        ", "vocab", "=", "self", ".", "fields", "[", "\"tgt\"", "]", ".", "vocab", "\n", "tokens", "=", "[", "]", "\n", "for", "tok", "in", "pred", ":", "\n", "            ", "if", "tok", "<", "len", "(", "vocab", ")", ":", "\n", "                ", "tokens", ".", "append", "(", "vocab", ".", "itos", "[", "tok", "]", ")", "\n", "", "else", ":", "\n", "                ", "tokens", ".", "append", "(", "src_vocab", ".", "itos", "[", "tok", "-", "len", "(", "vocab", ")", "]", ")", "\n", "", "if", "tokens", "[", "-", "1", "]", "==", "onmt", ".", "io", ".", "EOS_WORD", ":", "\n", "                ", "tokens", "=", "tokens", "[", ":", "-", "1", "]", "\n", "break", "\n", "", "", "if", "self", ".", "replace_unk", "and", "(", "attn", "is", "not", "None", ")", "and", "(", "src", "is", "not", "None", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "                ", "if", "tokens", "[", "i", "]", "==", "vocab", ".", "itos", "[", "onmt", ".", "io", ".", "UNK", "]", ":", "\n", "                    ", "_", ",", "maxIndex", "=", "attn", "[", "i", "]", ".", "max", "(", "0", ")", "\n", "tokens", "[", "i", "]", "=", "src_raw", "[", "maxIndex", "[", "0", "]", "]", "\n", "", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder.from_batch": [[48, 103], ["list", "torch.sort", "range", "len", "len", "zip", "batch.src[].data.index_select", "batch.tgt.data.index_select", "Translation.Translation", "translations.append", "Translation.TranslationBuilder._build_target_tokens", "Translation.TranslationBuilder._build_target_tokens", "sorted", "range", "zip"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder._build_target_tokens", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.TranslationBuilder._build_target_tokens"], ["", "def", "from_batch", "(", "self", ",", "translation_batch", ")", ":", "\n", "        ", "batch", "=", "translation_batch", "[", "\"batch\"", "]", "\n", "assert", "(", "len", "(", "translation_batch", "[", "\"gold_score\"", "]", ")", "==", "\n", "len", "(", "translation_batch", "[", "\"predictions\"", "]", ")", ")", "\n", "batch_size", "=", "batch", ".", "batch_size", "\n", "\n", "preds", ",", "pred_score", ",", "attn", ",", "gold_score", ",", "indices", "=", "list", "(", "zip", "(", "\n", "*", "sorted", "(", "zip", "(", "translation_batch", "[", "\"predictions\"", "]", ",", "\n", "translation_batch", "[", "\"scores\"", "]", ",", "\n", "translation_batch", "[", "\"attention\"", "]", ",", "\n", "translation_batch", "[", "\"gold_score\"", "]", ",", "\n", "batch", ".", "indices", ".", "data", ")", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "# Sorting", "\n", "inds", ",", "perm", "=", "torch", ".", "sort", "(", "batch", ".", "indices", ".", "data", ")", "\n", "data_type", "=", "self", ".", "data", ".", "data_type", "\n", "if", "data_type", "==", "'text'", ":", "\n", "            ", "src", "=", "batch", ".", "src", "[", "0", "]", ".", "data", ".", "index_select", "(", "1", ",", "perm", ")", "\n", "", "else", ":", "\n", "            ", "src", "=", "None", "\n", "\n", "", "if", "self", ".", "has_tgt", ":", "\n", "            ", "tgt", "=", "batch", ".", "tgt", ".", "data", ".", "index_select", "(", "1", ",", "perm", ")", "\n", "", "else", ":", "\n", "            ", "tgt", "=", "None", "\n", "\n", "", "translations", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "data_type", "==", "'text'", ":", "\n", "                ", "src_vocab", "=", "self", ".", "data", ".", "src_vocabs", "[", "inds", "[", "b", "]", "]", "if", "self", ".", "data", ".", "src_vocabs", "else", "None", "\n", "src_raw", "=", "self", ".", "data", ".", "examples", "[", "inds", "[", "b", "]", "]", ".", "src", "\n", "", "else", ":", "\n", "                ", "src_vocab", "=", "None", "\n", "src_raw", "=", "None", "\n", "", "pred_sents", "=", "[", "self", ".", "_build_target_tokens", "(", "\n", "src", "[", ":", ",", "b", "]", "if", "src", "is", "not", "None", "else", "None", ",", "\n", "src_vocab", ",", "src_raw", ",", "\n", "preds", "[", "b", "]", "[", "n", "]", ",", "attn", "[", "b", "]", "[", "n", "]", ")", "\n", "for", "n", "in", "range", "(", "self", ".", "n_best", ")", "]", "\n", "gold_sent", "=", "None", "\n", "if", "tgt", "is", "not", "None", ":", "\n", "                ", "gold_sent", "=", "self", ".", "_build_target_tokens", "(", "\n", "src", "[", ":", ",", "b", "]", "if", "src", "is", "not", "None", "else", "None", ",", "\n", "src_vocab", ",", "src_raw", ",", "\n", "tgt", "[", "1", ":", ",", "b", "]", "if", "tgt", "is", "not", "None", "else", "None", ",", "None", ")", "\n", "\n", "", "translation", "=", "Translation", "(", "src", "[", ":", ",", "b", "]", "if", "src", "is", "not", "None", "else", "None", ",", "\n", "src_raw", ",", "pred_sents", ",", "\n", "attn", "[", "b", "]", ",", "pred_score", "[", "b", "]", ",", "gold_sent", ",", "\n", "gold_score", "[", "b", "]", ")", "\n", "translations", ".", "append", "(", "translation", ")", "\n", "\n", "", "return", "translations", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.__init__": [[120, 129], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "src", ",", "src_raw", ",", "pred_sents", ",", "\n", "attn", ",", "pred_scores", ",", "tgt_sent", ",", "gold_score", ")", ":", "\n", "        ", "self", ".", "src", "=", "src", "\n", "self", ".", "src_raw", "=", "src_raw", "\n", "self", ".", "pred_sents", "=", "pred_sents", "\n", "self", ".", "attns", "=", "attn", "\n", "self", ".", "pred_scores", "=", "pred_scores", "\n", "self", ".", "gold_sent", "=", "tgt_sent", "\n", "self", ".", "gold_score", "=", "gold_score", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translation.Translation.log": [[130, 153], ["print", "print", "len", "print", "zip"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "sent_number", ")", ":", "\n", "        ", "\"\"\"\n        Log translation to stdout.\n        \"\"\"", "\n", "output", "=", "'\\nSENT {}: {}\\n'", ".", "format", "(", "sent_number", ",", "self", ".", "src_raw", ")", "\n", "\n", "best_pred", "=", "self", ".", "pred_sents", "[", "0", "]", "\n", "best_score", "=", "self", ".", "pred_scores", "[", "0", "]", "\n", "pred_sent", "=", "' '", ".", "join", "(", "best_pred", ")", "\n", "output", "+=", "'PRED {}: {}\\n'", ".", "format", "(", "sent_number", ",", "pred_sent", ")", "\n", "print", "(", "\"PRED SCORE: {:.4f}\"", ".", "format", "(", "best_score", ")", ")", "\n", "\n", "if", "self", ".", "gold_sent", "is", "not", "None", ":", "\n", "            ", "tgt_sent", "=", "' '", ".", "join", "(", "self", ".", "gold_sent", ")", "\n", "output", "+=", "'GOLD {}: {}\\n'", ".", "format", "(", "sent_number", ",", "tgt_sent", ")", "\n", "# output += (\"GOLD SCORE: {:.4f}\".format(self.gold_score))", "\n", "print", "(", "\"GOLD SCORE: {:.4f}\"", ".", "format", "(", "self", ".", "gold_score", ")", ")", "\n", "", "if", "len", "(", "self", ".", "pred_sents", ")", ">", "1", ":", "\n", "            ", "print", "(", "'\\nBEST HYP:'", ")", "\n", "for", "score", ",", "sent", "in", "zip", "(", "self", ".", "pred_scores", ",", "self", ".", "pred_sents", ")", ":", "\n", "                ", "output", "+=", "\"[{:.4f}] {}\\n\"", ".", "format", "(", "score", ",", "sent", ")", "\n", "\n", "", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.__init__": [[20, 26], ["TranslationServer.Timer.start"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.start"], ["    ", "def", "__init__", "(", "self", ",", "start", "=", "False", ")", ":", "\n", "        ", "self", ".", "stime", "=", "-", "1", "\n", "self", ".", "prev", "=", "-", "1", "\n", "self", ".", "times", "=", "{", "}", "\n", "if", "start", ":", "\n", "            ", "self", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.start": [[27, 31], ["time.time"], "methods", ["None"], ["", "", "def", "start", "(", "self", ")", ":", "\n", "        ", "self", ".", "stime", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "prev", "=", "self", ".", "stime", "\n", "self", ".", "times", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick": [[32, 43], ["time.time"], "methods", ["None"], ["", "def", "tick", "(", "self", ",", "name", "=", "None", ",", "tot", "=", "False", ")", ":", "\n", "        ", "t", "=", "time", ".", "time", "(", ")", "\n", "if", "not", "tot", ":", "\n", "            ", "elapsed", "=", "t", "-", "self", ".", "prev", "\n", "", "else", ":", "\n", "            ", "elapsed", "=", "t", "-", "self", ".", "stime", "\n", "", "self", ".", "prev", "=", "t", "\n", "\n", "if", "name", "is", "not", "None", ":", "\n", "            ", "self", ".", "times", "[", "name", "]", "=", "elapsed", "\n", "", "return", "elapsed", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.__init__": [[50, 53], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "models", "=", "{", "}", "\n", "self", ".", "next_id", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.start": [[54, 77], ["TranslationServer.TranslationServer.confs.get", "enumerate", "open", "json.load", "conf.get", "TranslationServer.TranslationServer.preload_model", "ValueError", "conf.get", "conf.get", "conf.get", "conf.get", "conf.get", "kwargs.items"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.preload_model"], ["", "def", "start", "(", "self", ",", "config_file", ")", ":", "\n", "        ", "\"\"\"Read the config file and pre-/load the models\n        \"\"\"", "\n", "self", ".", "config_file", "=", "config_file", "\n", "with", "open", "(", "self", ".", "config_file", ")", "as", "f", ":", "\n", "            ", "self", ".", "confs", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "self", ".", "models_root", "=", "self", ".", "confs", ".", "get", "(", "'models_root'", ",", "'./available_models'", ")", "\n", "for", "i", ",", "conf", "in", "enumerate", "(", "self", ".", "confs", "[", "\"models\"", "]", ")", ":", "\n", "            ", "if", "\"model\"", "not", "in", "conf", ":", "\n", "                ", "raise", "ValueError", "(", "\"\"\"Incorrect config file: missing 'model'\n                                    parameter for model #%d\"\"\"", "%", "i", ")", "\n", "", "kwargs", "=", "{", "'timeout'", ":", "conf", ".", "get", "(", "'timeout'", ",", "None", ")", ",", "\n", "'load'", ":", "conf", ".", "get", "(", "'load'", ",", "None", ")", ",", "\n", "'tokenizer_opt'", ":", "conf", ".", "get", "(", "'tokenizer'", ",", "None", ")", ",", "\n", "'on_timeout'", ":", "conf", ".", "get", "(", "'on_timeout'", ",", "None", ")", ",", "\n", "'model_root'", ":", "conf", ".", "get", "(", "'model_root'", ",", "self", ".", "models_root", ")", "\n", "}", "\n", "kwargs", "=", "{", "k", ":", "v", "for", "(", "k", ",", "v", ")", "in", "kwargs", ".", "items", "(", ")", "if", "v", "is", "not", "None", "}", "\n", "model_id", "=", "conf", ".", "get", "(", "\"id\"", ",", "None", ")", "\n", "opt", "=", "conf", "[", "\"opt\"", "]", "\n", "opt", "[", "\"model\"", "]", "=", "conf", "[", "\"model\"", "]", "\n", "self", ".", "preload_model", "(", "opt", ",", "model_id", "=", "model_id", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.clone_model": [[78, 90], ["TranslationServer.TranslationServer.load_model", "TranslationServer.ServerModelError", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.load_model"], ["", "", "def", "clone_model", "(", "self", ",", "model_id", ",", "opt", ",", "timeout", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Clone a model `model_id`.\n           Different options may be passed. If `opt` is None, it will use the\n           same set of options\n        \"\"\"", "\n", "if", "model_id", "in", "self", ".", "models", ":", "\n", "            ", "if", "opt", "is", "None", ":", "\n", "                ", "opt", "=", "self", ".", "models", "[", "model_id", "]", ".", "user_opt", "\n", "", "opt", "[", "\"model\"", "]", "=", "self", ".", "models", "[", "model_id", "]", ".", "opt", ".", "model", "\n", "return", "self", ".", "load_model", "(", "opt", ",", "timeout", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ServerModelError", "(", "\"No such model '%s'\"", "%", "str", "(", "model_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.load_model": [[91, 98], ["TranslationServer.TranslationServer.preload_model"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.preload_model"], ["", "", "def", "load_model", "(", "self", ",", "opt", ",", "model_id", "=", "None", ",", "**", "model_kwargs", ")", ":", "\n", "        ", "\"\"\"Loading a model given a set of options\n        \"\"\"", "\n", "model_id", "=", "self", ".", "preload_model", "(", "opt", ",", "model_id", "=", "model_id", ",", "**", "model_kwargs", ")", "\n", "load_time", "=", "self", ".", "models", "[", "model_id", "]", ".", "load_time", "\n", "\n", "return", "model_id", ",", "load_time", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.preload_model": [[99, 116], ["print", "TranslationServer.ServerModel", "TranslationServer.TranslationServer.models.keys", "ValueError", "TranslationServer.TranslationServer.models.keys"], "methods", ["None"], ["", "def", "preload_model", "(", "self", ",", "opt", ",", "model_id", "=", "None", ",", "**", "model_kwargs", ")", ":", "\n", "        ", "\"\"\"Preloading the model: updating internal datastructure\n           It will effectively load the model if `load` is set\n        \"\"\"", "\n", "if", "model_id", "is", "not", "None", ":", "\n", "            ", "if", "model_id", "in", "self", ".", "models", ".", "keys", "(", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Model ID %d already exists\"", "%", "model_id", ")", "\n", "", "", "else", ":", "\n", "            ", "model_id", "=", "self", ".", "next_id", "\n", "while", "model_id", "in", "self", ".", "models", ".", "keys", "(", ")", ":", "\n", "                ", "model_id", "+=", "1", "\n", "", "self", ".", "next_id", "=", "model_id", "+", "1", "\n", "", "print", "(", "\"Pre-loading model %d\"", "%", "model_id", ")", "\n", "model", "=", "ServerModel", "(", "opt", ",", "model_id", ",", "**", "model_kwargs", ")", "\n", "self", ".", "models", "[", "model_id", "]", "=", "model", "\n", "\n", "return", "model_id", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.run": [[117, 130], ["inputs[].get", "TranslationServer.TranslationServer.models[].run", "print", "TranslationServer.ServerModelError", "str", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.run"], ["", "def", "run", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Translate `inputs`\n           We keep the same format as the Lua version i.e.\n             [{\"id\": model_id, \"src\": \"sequence to translate\"},{ ...}]\n\n           We use inputs[0][\"id\"] as the model id\n        \"\"\"", "\n", "model_id", "=", "inputs", "[", "0", "]", ".", "get", "(", "\"id\"", ",", "0", ")", "\n", "if", "model_id", "in", "self", ".", "models", "and", "self", ".", "models", "[", "model_id", "]", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "models", "[", "model_id", "]", ".", "run", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Error No such model '%s'\"", "%", "str", "(", "model_id", ")", ")", "\n", "raise", "ServerModelError", "(", "\"No such model '%s'\"", "%", "str", "(", "model_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.unload_model": [[131, 139], ["TranslationServer.TranslationServer.models[].unload", "TranslationServer.ServerModelError", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.unload"], ["", "", "def", "unload_model", "(", "self", ",", "model_id", ")", ":", "\n", "        ", "\"\"\"Manually unload a model.\n           It will free the memory and cancel the timer\n        \"\"\"", "\n", "if", "model_id", "in", "self", ".", "models", "and", "self", ".", "models", "[", "model_id", "]", "is", "not", "None", ":", "\n", "            ", "self", ".", "models", "[", "model_id", "]", ".", "unload", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ServerModelError", "(", "\"No such model '%s'\"", "%", "str", "(", "model_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.list_models": [[140, 147], ["TranslationServer.TranslationServer.models.items", "model.to_dict"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_dict"], ["", "", "def", "list_models", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the list of available models\n        \"\"\"", "\n", "models", "=", "[", "]", "\n", "for", "i", ",", "model", "in", "self", ".", "models", ".", "items", "(", ")", ":", "\n", "            ", "models", "+=", "[", "model", ".", "to_dict", "(", ")", "]", "\n", "", "return", "models", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.__init__": [[150, 182], ["TranslationServer.ServerModel.parse_opt", "ValueError", "TranslationServer.ServerModel.load"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.parse_opt", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "model_id", ",", "tokenizer_opt", "=", "None", ",", "load", "=", "False", ",", "\n", "timeout", "=", "-", "1", ",", "on_timeout", "=", "\"to_cpu\"", ",", "model_root", "=", "\"./\"", ")", ":", "\n", "        ", "\"\"\"\n            Args:\n                opt: (dict) options for the Translator\n                model_id: (int) model id\n                tokenizer_opt: (dict) options for the tokenizer or None\n                load: (bool) whether to load the model during __init__\n                timeout: (int) seconds before running `do_timeout`\n                         Negative values means no timeout\n                on_timeout: (str) in [\"to_cpu\", \"unload\"] set what to do on\n                            timeout (see function `do_timeout`)\n                model_root: (str) path to the model directory\n                            it must contain de model and tokenizer file\n\n        \"\"\"", "\n", "self", ".", "model_root", "=", "model_root", "\n", "self", ".", "opt", "=", "self", ".", "parse_opt", "(", "opt", ")", "\n", "if", "self", ".", "opt", ".", "n_best", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Values of n_best > 1 are not supported\"", ")", "\n", "\n", "", "self", ".", "model_id", "=", "model_id", "\n", "self", ".", "tokenizer_opt", "=", "tokenizer_opt", "\n", "self", ".", "timeout", "=", "timeout", "\n", "self", ".", "on_timeout", "=", "on_timeout", "\n", "\n", "self", ".", "unload_timer", "=", "None", "\n", "self", ".", "user_opt", "=", "opt", "\n", "self", ".", "tokenizer", "=", "None", "\n", "\n", "if", "load", ":", "\n", "            ", "self", ".", "load", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.parse_opt": [[183, 207], ["argparse.ArgumentParser", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "onmt.opts.translate_opts", "os.path.join", "argparse.ArgumentParser.parse_args.items", "argparse.ArgumentParser.parse_args", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.translate_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args"], ["", "", "def", "parse_opt", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Parse the option set passed by the user using `onmt.opts`\n           Args:\n               opt: (dict) options passed by the user\n\n           Returns:\n               opt: (Namespace) full set of options for the Translator\n        \"\"\"", "\n", "prec_argv", "=", "sys", ".", "argv", "\n", "sys", ".", "argv", "=", "sys", ".", "argv", "[", ":", "1", "]", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "onmt", ".", "opts", ".", "translate_opts", "(", "parser", ")", "\n", "\n", "opt", "[", "'model'", "]", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_root", ",", "opt", "[", "'model'", "]", ")", "\n", "opt", "[", "'src'", "]", "=", "\"dummy_src\"", "\n", "\n", "for", "(", "k", ",", "v", ")", "in", "opt", ".", "items", "(", ")", ":", "\n", "            ", "sys", ".", "argv", "+=", "[", "'-%s'", "%", "k", ",", "str", "(", "v", ")", "]", "\n", "\n", "", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", ".", "cuda", "=", "opt", ".", "gpu", ">", "-", "1", "\n", "\n", "sys", ".", "argv", "=", "prec_argv", "\n", "return", "opt", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.loaded": [[208, 211], ["hasattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "loaded", "(", "self", ")", ":", "\n", "        ", "return", "hasattr", "(", "self", ",", "'translator'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load": [[212, 244], ["TranslationServer.Timer", "print", "TranslationServer.Timer.start", "io.StringIO", "TranslationServer.Timer.tick", "TranslationServer.Timer.tick", "TranslationServer.ServerModel.reset_unload_timer", "onmt.translate.Translator.make_translator", "onmt.translate.Translator.make_translator", "onmt.translate.Translator.make_translator", "print", "TranslationServer.ServerModelError", "spm.SentencePieceProcessor", "os.path.join", "spm.SentencePieceProcessor.Load", "ValueError", "ValueError", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.start", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.reset_unload_timer", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.make_translator", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.make_translator", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.make_translator"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "timer", "=", "Timer", "(", ")", "\n", "print", "(", "\"Loading model %d\"", "%", "self", ".", "model_id", ")", "\n", "timer", ".", "start", "(", ")", "\n", "self", ".", "out_file", "=", "io", ".", "StringIO", "(", ")", "\n", "try", ":", "\n", "            ", "self", ".", "translator", "=", "make_translator", "(", "self", ".", "opt", ",", "\n", "report_score", "=", "False", ",", "\n", "out_file", "=", "self", ".", "out_file", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "raise", "ServerModelError", "(", "\"Runtime Error: %s\"", "%", "str", "(", "e", ")", ")", "\n", "\n", "", "timer", ".", "tick", "(", "\"model_loading\"", ")", "\n", "if", "self", ".", "tokenizer_opt", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"Loading tokenizer\"", ")", "\n", "mandatory", "=", "[", "\"type\"", ",", "\"model\"", "]", "\n", "for", "m", "in", "mandatory", ":", "\n", "                ", "if", "m", "not", "in", "self", ".", "tokenizer_opt", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Missing mandatory tokenizer option '%s'\"", "\n", "%", "m", ")", "\n", "", "", "if", "self", ".", "tokenizer_opt", "[", "'type'", "]", "==", "'sentencepiece'", ":", "\n", "                ", "import", "sentencepiece", "as", "spm", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_root", ",", "\n", "self", ".", "tokenizer_opt", "[", "'model'", "]", ")", "\n", "sp", ".", "Load", "(", "model_path", ")", "\n", "self", ".", "tokenizer", "=", "sp", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid value for tokenizer type\"", ")", "\n", "\n", "", "", "self", ".", "load_time", "=", "timer", ".", "tick", "(", ")", "\n", "self", ".", "reset_unload_timer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.run": [[245, 316], ["TranslationServer.Timer", "print", "TranslationServer.Timer.start", "os.path.join", "TranslationServer.Timer.tick", "TranslationServer.Timer.tick", "print", "TranslationServer.ServerModel.reset_unload_timer", "TranslationServer.ServerModel.out_file.getvalue().split", "print", "TranslationServer.ServerModel.clear_out_file", "TranslationServer.ServerModel.load", "TranslationServer.Timer.tick", "os.path.exists", "os.makedirs", "codecs.open", "enumerate", "TranslationServer.ServerModel.translator.translate", "len", "TranslationServer.ServerModel.to_gpu", "TranslationServer.Timer.tick", "src.split", "slice", "len", "TranslationServer.ServerModelError", "TranslationServer.ServerModel.out_file.getvalue", "sorted", "sum", "sum", "sorted", "TranslationServer.ServerModel.maybe_tokenize", "f.write", "len", "TranslationServer.ServerModel.maybe_detokenize", "subsegment.keys", "subsegment.items", "len", "len", "str", "TranslationServer.ServerModel.split", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.start", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.reset_unload_timer", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.clear_out_file", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Translator.Translator.translate", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_gpu", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.Timer.tick", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.maybe_tokenize", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.maybe_detokenize"], ["", "def", "run", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"Translate `inputs` using this model\n\n            Args:\n                inputs: [{\"src\": \"...\"},{\"src\": ...}]\n\n            Returns:\n                result: (list) translations\n                times: (dict) containing times\n        \"\"\"", "\n", "timer", "=", "Timer", "(", ")", "\n", "print", "(", "\"\\nRunning translation using %d\"", "%", "self", ".", "model_id", ")", "\n", "\n", "timer", ".", "start", "(", ")", "\n", "if", "not", "self", ".", "loaded", ":", "\n", "            ", "self", ".", "load", "(", ")", "\n", "timer", ".", "tick", "(", "name", "=", "\"load\"", ")", "\n", "", "elif", "self", ".", "opt", ".", "cuda", ":", "\n", "            ", "self", ".", "to_gpu", "(", ")", "\n", "timer", ".", "tick", "(", "name", "=", "\"to_gpu\"", ")", "\n", "\n", "# NOTE: the translator exept a filepath as parameter", "\n", "#       therefore we write the data as a temp file.", "\n", "", "tmp_root", "=", "\"/tmp/onmt_server\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "tmp_root", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "tmp_root", ")", "\n", "", "src_path", "=", "os", ".", "path", ".", "join", "(", "tmp_root", ",", "\"tmp_src\"", ")", "\n", "with", "codecs", ".", "open", "(", "src_path", ",", "'w'", ",", "'utf-8'", ")", "as", "f", ":", "\n", "# NOTE: If an input contains an line separator \\n we split it", "\n", "#       into subsegments that we translate independantly", "\n", "#       we then merge the translations together with the same", "\n", "#       line breaks", "\n", "            ", "subsegment", "=", "{", "}", "\n", "sscount", "=", "0", "\n", "sslength", "=", "[", "]", "\n", "for", "(", "i", ",", "inp", ")", "in", "enumerate", "(", "inputs", ")", ":", "\n", "                ", "src", "=", "inp", "[", "'src'", "]", "\n", "lines", "=", "src", ".", "split", "(", "\"\\n\"", ")", "\n", "subsegment", "[", "i", "]", "=", "slice", "(", "sscount", ",", "sscount", "+", "len", "(", "lines", ")", ")", "\n", "sscount", "+=", "len", "(", "lines", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                    ", "tok", "=", "self", ".", "maybe_tokenize", "(", "line", ")", "\n", "f", ".", "write", "(", "tok", "+", "\"\\n\"", ")", "\n", "sslength", "+=", "[", "len", "(", "tok", ".", "split", "(", ")", ")", "]", "\n", "", "", "", "timer", ".", "tick", "(", "name", "=", "\"writing\"", ")", "\n", "try", ":", "\n", "            ", "scores", "=", "self", ".", "translator", ".", "translate", "(", "None", ",", "src_path", ",", "None", ",", "\n", "self", ".", "opt", ".", "batch_size", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "raise", "ServerModelError", "(", "\"Runtime Error: %s\"", "%", "str", "(", "e", ")", ")", "\n", "\n", "", "timer", ".", "tick", "(", "name", "=", "\"translation\"", ")", "\n", "print", "(", "\"\"\"Using model #%d\\t%d inputs (%d subsegment)\n               \\ttranslation time: %f\"\"\"", "%", "(", "self", ".", "model_id", ",", "len", "(", "subsegment", ")", ",", "\n", "sscount", ",", "\n", "timer", ".", "times", "[", "'translation'", "]", ")", ")", "\n", "self", ".", "reset_unload_timer", "(", ")", "\n", "results", "=", "self", ".", "out_file", ".", "getvalue", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "print", "(", "\"Results: \"", ",", "len", "(", "results", ")", ")", "\n", "results", "=", "[", "'\\n'", ".", "join", "(", "[", "self", ".", "maybe_detokenize", "(", "_", ")", "\n", "for", "_", "in", "results", "[", "subsegment", "[", "i", "]", "]", "\n", "if", "len", "(", "_", ")", ">", "0", "]", ")", "\n", "for", "i", "in", "sorted", "(", "subsegment", ".", "keys", "(", ")", ")", "]", "\n", "\n", "avg_scores", "=", "[", "sum", "(", "[", "s", "*", "l", "for", "s", ",", "l", "in", "zip", "(", "scores", "[", "sub", "]", ",", "sslength", "[", "sub", "]", ")", "]", ")", "\n", "/", "sum", "(", "sslength", "[", "sub", "]", ")", "\n", "for", "k", ",", "sub", "\n", "in", "sorted", "(", "subsegment", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "]", "\n", "\n", "self", ".", "clear_out_file", "(", ")", "\n", "return", "results", ",", "avg_scores", ",", "self", ".", "opt", ".", "n_best", ",", "timer", ".", "times", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.do_timeout": [[317, 327], ["print", "TranslationServer.ServerModel.unload", "print", "TranslationServer.ServerModel.to_cpu"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.unload", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_cpu"], ["", "def", "do_timeout", "(", "self", ")", ":", "\n", "        ", "\"\"\"Timeout function that free GPU memory by moving the model to CPU\n           or unloading it; depending on `self.on_timemout` value\n        \"\"\"", "\n", "if", "self", ".", "on_timeout", "==", "\"unload\"", ":", "\n", "            ", "print", "(", "\"Timeout: unloading model %d\"", "%", "self", ".", "model_id", ")", "\n", "self", ".", "unload", "(", ")", "\n", "", "if", "self", ".", "on_timeout", "==", "\"to_cpu\"", ":", "\n", "            ", "print", "(", "\"Timeout: sending model %d to CPU\"", "%", "self", ".", "model_id", ")", "\n", "self", ".", "to_cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.unload": [[328, 334], ["print", "torch.cuda.empty_cache"], "methods", ["None"], ["", "", "def", "unload", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Unloading model %d\"", "%", "self", ".", "model_id", ")", "\n", "del", "self", ".", "translator", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "self", ".", "unload_timer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.reset_unload_timer": [[335, 343], ["threading.Timer", "TranslationServer.ServerModel.unload_timer.start", "TranslationServer.ServerModel.unload_timer.cancel"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.TranslationServer.start"], ["", "def", "reset_unload_timer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "timeout", "<", "0", ":", "\n", "            ", "return", "\n", "\n", "", "if", "self", ".", "unload_timer", "is", "not", "None", ":", "\n", "            ", "self", ".", "unload_timer", ".", "cancel", "(", ")", "\n", "", "self", ".", "unload_timer", "=", "threading", ".", "Timer", "(", "self", ".", "timeout", ",", "self", ".", "do_timeout", ")", "\n", "self", ".", "unload_timer", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_dict": [[344, 356], ["TranslationServer.ServerModel.user_opt.keys"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "hide_opt", "=", "[", "\"model\"", ",", "\"src\"", "]", "\n", "d", "=", "{", "\"model_id\"", ":", "self", ".", "model_id", ",", "\n", "\"opt\"", ":", "{", "k", ":", "self", ".", "user_opt", "[", "k", "]", "for", "k", "in", "self", ".", "user_opt", ".", "keys", "(", ")", "\n", "if", "k", "not", "in", "hide_opt", "}", ",", "\n", "\"model\"", ":", "self", ".", "user_opt", "[", "\"model\"", "]", ",", "\n", "\"loaded\"", ":", "self", ".", "loaded", ",", "\n", "\"timeout\"", ":", "self", ".", "timeout", ",", "\n", "}", "\n", "if", "self", ".", "tokenizer_opt", "is", "not", "None", ":", "\n", "            ", "d", "[", "\"tokenizer\"", "]", "=", "self", ".", "tokenizer_opt", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_cpu": [[357, 363], ["TranslationServer.ServerModel.translator.model.cpu", "torch.cuda.empty_cache"], "methods", ["None"], ["", "def", "to_cpu", "(", "self", ")", ":", "\n", "        ", "\"\"\"Move the model to CPU and clear CUDA cache\n        \"\"\"", "\n", "self", ".", "translator", ".", "model", ".", "cpu", "(", ")", "\n", "if", "self", ".", "opt", ".", "cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.to_gpu": [[364, 369], ["torch.cuda.set_device", "TranslationServer.ServerModel.translator.model.cuda"], "methods", ["None"], ["", "", "def", "to_gpu", "(", "self", ")", ":", "\n", "        ", "\"\"\"Move the model to GPU\n        \"\"\"", "\n", "torch", ".", "cuda", ".", "set_device", "(", "self", ".", "opt", ".", "gpu", ")", "\n", "self", ".", "translator", ".", "model", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.clear_out_file": [[370, 374], ["io.StringIO"], "methods", ["None"], ["", "def", "clear_out_file", "(", "self", ")", ":", "\n", "# Creating a new object is faster", "\n", "        ", "self", ".", "out_file", "=", "io", ".", "StringIO", "(", ")", "\n", "self", ".", "translator", ".", "out_file", "=", "self", ".", "out_file", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.maybe_tokenize": [[375, 383], ["TranslationServer.ServerModel.tokenize"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.tokenize"], ["", "def", "maybe_tokenize", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"Tokenize the sequence (or not)\n\n           Same args/returns as `tokenize`\n        \"\"\"", "\n", "if", "self", ".", "tokenizer_opt", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "tokenize", "(", "sequence", ")", "\n", "", "return", "sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.tokenize": [[384, 401], ["ValueError", "TranslationServer.ServerModel.tokenizer.EncodeAsPieces"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"Tokenize a single sequence\n\n            Args:\n                sequence: (str) the sequence to tokenize\n\n            Returns:\n                tok: (str) the tokenized sequence\n\n        \"\"\"", "\n", "if", "self", ".", "tokenizer", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"No tokenizer loaded\"", ")", "\n", "\n", "", "if", "self", ".", "tokenizer_opt", "[", "\"type\"", "]", "==", "\"sentencepiece\"", ":", "\n", "            ", "tok", "=", "self", ".", "tokenizer", ".", "EncodeAsPieces", "(", "sequence", ")", "\n", "tok", "=", "\" \"", ".", "join", "(", "tok", ")", "\n", "", "return", "tok", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.maybe_detokenize": [[402, 410], ["TranslationServer.ServerModel.detokenize"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.detokenize"], ["", "def", "maybe_detokenize", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"De-tokenize the sequence (or not)\n\n           Same args/returns as `tokenize`\n        \"\"\"", "\n", "if", "self", ".", "tokenizer_opt", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "detokenize", "(", "sequence", ")", "\n", "", "return", "sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.detokenize": [[411, 422], ["ValueError", "TranslationServer.ServerModel.tokenizer.DecodePieces", "sequence.split"], "methods", ["None"], ["", "def", "detokenize", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"Detokenize a single sequence\n\n           Same args/returns as `tokenize`\n        \"\"\"", "\n", "if", "self", ".", "tokenizer", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"No tokenizer loaded\"", ")", "\n", "\n", "", "if", "self", ".", "tokenizer_opt", "[", "\"type\"", "]", "==", "\"sentencepiece\"", ":", "\n", "            ", "detok", "=", "self", ".", "tokenizer", ".", "DecodePieces", "(", "sequence", ".", "split", "(", ")", ")", "\n", "", "return", "detok", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.__init__": [[19, 68], ["set", "Beam.Beam.tt.FloatTensor().zero_", "Beam.Beam.tt.LongTensor().fill_", "Beam.Beam.tt.FloatTensor", "Beam.Beam.tt.LongTensor"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ",", "pad", ",", "bos", ",", "eos", ",", "\n", "n_best", "=", "1", ",", "cuda", "=", "False", ",", "\n", "eos_norm", "=", "0", ",", "\n", "global_scorer", "=", "None", ",", "\n", "min_length", "=", "0", ",", "\n", "stepwise_penalty", "=", "False", ",", "\n", "block_ngram_repeat", "=", "0", ",", "\n", "exclusion_tokens", "=", "set", "(", ")", ")", ":", "\n", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "if", "cuda", "else", "torch", "\n", "\n", "# The eos norm.", "\n", "self", ".", "eos_norm", "=", "eos_norm", "\n", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "self", ".", "all_scores", "=", "[", "]", "\n", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prev_ks", "=", "[", "]", "\n", "\n", "# The outputs at each time-step.", "\n", "self", ".", "next_ys", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", "\n", ".", "fill_", "(", "pad", ")", "]", "\n", "self", ".", "next_ys", "[", "0", "]", "[", "0", "]", "=", "bos", "\n", "\n", "# Has EOS topped the beam yet.", "\n", "self", ".", "_eos", "=", "eos", "\n", "self", ".", "eos_top", "=", "False", "\n", "\n", "# The attentions (matrix) for each time.", "\n", "self", ".", "attn", "=", "[", "]", "\n", "\n", "# Time and k pair for finished.", "\n", "self", ".", "finished", "=", "[", "]", "\n", "self", ".", "n_best", "=", "n_best", "\n", "\n", "# Information for global scoring.", "\n", "self", ".", "global_scorer", "=", "global_scorer", "\n", "self", ".", "global_state", "=", "{", "}", "\n", "\n", "# Minimum prediction length", "\n", "self", ".", "min_length", "=", "min_length", "\n", "\n", "# Apply Penalty at every step", "\n", "self", ".", "stepwise_penalty", "=", "stepwise_penalty", "\n", "self", ".", "block_ngram_repeat", "=", "block_ngram_repeat", "\n", "self", ".", "exclusion_tokens", "=", "exclusion_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.get_current_state": [[69, 72], ["None"], "methods", ["None"], ["", "def", "get_current_state", "(", "self", ")", ":", "\n", "        ", "\"Get the outputs for the current timestep.\"", "\n", "return", "self", ".", "next_ys", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.get_current_origin": [[73, 76], ["None"], "methods", ["None"], ["", "def", "get_current_origin", "(", "self", ")", ":", "\n", "        ", "\"Get the backpointers for the current timestep.\"", "\n", "return", "self", ".", "prev_ks", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.advance": [[77, 157], ["word_probs.size", "len", "beam_scores.view", "beam_scores.view.topk", "Beam.Beam.all_scores.append", "Beam.Beam.prev_ks.append", "Beam.Beam.next_ys.append", "Beam.Beam.attn.append", "Beam.Beam.global_scorer.update_global_state", "range", "Beam.Beam.global_scorer.update_score", "range", "len", "range", "attn_out.index_select", "Beam.Beam.next_ys[].size", "Beam.Beam.all_scores.append", "len", "range", "Beam.Beam.scores.unsqueeze().expand_as", "Beam.Beam.next_ys[].size", "len", "range", "Beam.Beam.global_scorer.score", "Beam.Beam.finished.append", "len", "Beam.Beam.next_ys[].size", "Beam.Beam.get_hyp", "set", "range", "Beam.Beam.scores.unsqueeze", "set.add", "set", "tuple", "tuple", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.update_global_state", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.update_score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.score", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.get_hyp"], ["", "def", "advance", "(", "self", ",", "word_probs", ",", "attn_out", ")", ":", "\n", "        ", "\"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attn_out`: Compute and update the beam search.\n\n        Parameters:\n\n        * `word_probs`- probs of advancing from the last step (K x words)\n        * `attn_out`- attention at the last step\n\n        Returns: True if beam search is complete.\n        \"\"\"", "\n", "num_words", "=", "word_probs", ".", "size", "(", "1", ")", "\n", "if", "self", ".", "stepwise_penalty", ":", "\n", "            ", "self", ".", "global_scorer", ".", "update_score", "(", "self", ",", "attn_out", ")", "\n", "# force the output to be longer than self.min_length", "\n", "", "cur_len", "=", "len", "(", "self", ".", "next_ys", ")", "\n", "if", "cur_len", "<", "self", ".", "min_length", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "word_probs", ")", ")", ":", "\n", "                ", "word_probs", "[", "k", "]", "[", "self", ".", "_eos", "]", "=", "-", "1e20", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "eos_norm", ">", "0", ":", "\n", "                ", "for", "k", "in", "range", "(", "len", "(", "word_probs", ")", ")", ":", "\n", "                    ", "word_probs", "[", "k", "]", "[", "self", ".", "_eos", "]", "-=", "self", ".", "eos_norm", "*", "self", ".", "src_len", "/", "(", "cur_len", "+", "1", ")", "\n", "# Sum the previous scores.", "\n", "", "", "", "if", "len", "(", "self", ".", "prev_ks", ")", ">", "0", ":", "\n", "            ", "beam_scores", "=", "word_probs", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "word_probs", ")", "\n", "# Don't let EOS have children.", "\n", "for", "i", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                    ", "beam_scores", "[", "i", "]", "=", "-", "1e20", "\n", "\n", "# Block ngram repeats", "\n", "", "", "if", "self", ".", "block_ngram_repeat", ">", "0", ":", "\n", "                ", "ngrams", "=", "[", "]", "\n", "le", "=", "len", "(", "self", ".", "next_ys", ")", "\n", "for", "j", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                    ", "hyp", ",", "_", "=", "self", ".", "get_hyp", "(", "le", "-", "1", ",", "j", ")", "\n", "ngrams", "=", "set", "(", ")", "\n", "fail", "=", "False", "\n", "gram", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "le", "-", "1", ")", ":", "\n", "# Last n tokens, n = block_ngram_repeat", "\n", "                        ", "gram", "=", "(", "gram", "+", "[", "hyp", "[", "i", "]", "]", ")", "[", "-", "self", ".", "block_ngram_repeat", ":", "]", "\n", "# Skip the blocking if it is in the exclusion list", "\n", "if", "set", "(", "gram", ")", "&", "self", ".", "exclusion_tokens", ":", "\n", "                            ", "continue", "\n", "", "if", "tuple", "(", "gram", ")", "in", "ngrams", ":", "\n", "                            ", "fail", "=", "True", "\n", "", "ngrams", ".", "add", "(", "tuple", "(", "gram", ")", ")", "\n", "", "if", "fail", ":", "\n", "                        ", "beam_scores", "[", "j", "]", "=", "-", "10e20", "\n", "", "", "", "", "else", ":", "\n", "            ", "beam_scores", "=", "word_probs", "[", "0", "]", "\n", "", "flat_beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "\n", "best_scores", ",", "best_scores_id", "=", "flat_beam_scores", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "\n", "True", ",", "True", ")", "\n", "\n", "self", ".", "all_scores", ".", "append", "(", "self", ".", "scores", ")", "\n", "self", ".", "scores", "=", "best_scores", "\n", "\n", "# best_scores_id is flattened beam x word array, so calculate which", "\n", "# word and beam each score came from", "\n", "prev_k", "=", "best_scores_id", "/", "num_words", "\n", "self", ".", "prev_ks", ".", "append", "(", "prev_k", ")", "\n", "self", ".", "next_ys", ".", "append", "(", "(", "best_scores_id", "-", "prev_k", "*", "num_words", ")", ")", "\n", "self", ".", "attn", ".", "append", "(", "attn_out", ".", "index_select", "(", "0", ",", "prev_k", ")", ")", "\n", "self", ".", "global_scorer", ".", "update_global_state", "(", "self", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "next_ys", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                ", "global_scores", "=", "self", ".", "global_scorer", ".", "score", "(", "self", ",", "self", ".", "scores", ")", "\n", "s", "=", "global_scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "next_ys", ")", "-", "1", ",", "i", ")", ")", "\n", "\n", "# End condition is when top-of-beam is EOS and no global score.", "\n", "", "", "if", "self", ".", "next_ys", "[", "-", "1", "]", "[", "0", "]", "==", "self", ".", "_eos", ":", "\n", "            ", "self", ".", "all_scores", ".", "append", "(", "self", ".", "scores", ")", "\n", "self", ".", "eos_top", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.done": [[158, 160], ["len"], "methods", ["None"], ["", "", "def", "done", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eos_top", "and", "len", "(", "self", ".", "finished", ")", ">=", "self", ".", "n_best", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.sort_finished": [[161, 175], ["Beam.Beam.finished.sort", "len", "Beam.Beam.global_scorer.score", "Beam.Beam.finished.append", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.score"], ["", "def", "sort_finished", "(", "self", ",", "minimum", "=", "None", ")", ":", "\n", "        ", "if", "minimum", "is", "not", "None", ":", "\n", "            ", "i", "=", "0", "\n", "# Add from beam until we have minimum outputs.", "\n", "while", "len", "(", "self", ".", "finished", ")", "<", "minimum", ":", "\n", "                ", "global_scores", "=", "self", ".", "global_scorer", ".", "score", "(", "self", ",", "self", ".", "scores", ")", "\n", "s", "=", "global_scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "next_ys", ")", "-", "1", ",", "i", ")", ")", "\n", "i", "+=", "1", "\n", "\n", "", "", "self", ".", "finished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "scores", "=", "[", "sc", "for", "sc", ",", "_", ",", "_", "in", "self", ".", "finished", "]", "\n", "ks", "=", "[", "(", "t", ",", "k", ")", "for", "_", ",", "t", ",", "k", "in", "self", ".", "finished", "]", "\n", "return", "scores", ",", "ks", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.Beam.get_hyp": [[176, 186], ["range", "hyp.append", "attn.append", "torch.stack", "len"], "methods", ["None"], ["", "def", "get_hyp", "(", "self", ",", "timestep", ",", "k", ")", ":", "\n", "        ", "\"\"\"\n        Walk back to construct the full hypothesis.\n        \"\"\"", "\n", "hyp", ",", "attn", "=", "[", "]", ",", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prev_ks", "[", ":", "timestep", "]", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "hyp", ".", "append", "(", "self", ".", "next_ys", "[", "j", "+", "1", "]", "[", "k", "]", ")", "\n", "attn", ".", "append", "(", "self", ".", "attn", "[", "j", "]", "[", "k", "]", ")", "\n", "k", "=", "self", ".", "prev_ks", "[", "j", "]", "[", "k", "]", "\n", "", "return", "hyp", "[", ":", ":", "-", "1", "]", ",", "torch", ".", "stack", "(", "attn", "[", ":", ":", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.__init__": [[197, 206], ["onmt.translate.Penalties.PenaltyBuilder", "onmt.translate.Penalties.PenaltyBuilder.coverage_penalty", "onmt.translate.Penalties.PenaltyBuilder.length_penalty"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.coverage_penalty", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_penalty"], ["def", "__init__", "(", "self", ",", "alpha", ",", "beta", ",", "cov_penalty", ",", "length_penalty", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "penalty_builder", "=", "Penalties", ".", "PenaltyBuilder", "(", "cov_penalty", ",", "\n", "length_penalty", ")", "\n", "# Term will be subtracted from probability", "\n", "self", ".", "cov_penalty", "=", "penalty_builder", ".", "coverage_penalty", "(", ")", "\n", "# Probability will be divided by this", "\n", "self", ".", "length_penalty", "=", "penalty_builder", ".", "length_penalty", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.score": [[207, 221], ["Beam.GNMTGlobalScorer.length_penalty", "Beam.GNMTGlobalScorer.cov_penalty"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Penalties.PenaltyBuilder.length_penalty"], ["", "def", "score", "(", "self", ",", "beam", ",", "logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Rescores a prediction based on penalty functions\n        \"\"\"", "\n", "normalized_probs", "=", "self", ".", "length_penalty", "(", "beam", ",", "\n", "logprobs", ",", "\n", "self", ".", "alpha", ")", "\n", "if", "not", "beam", ".", "stepwise_penalty", ":", "\n", "            ", "penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", ",", "\n", "self", ".", "beta", ")", "\n", "normalized_probs", "-=", "penalty", "\n", "\n", "", "return", "normalized_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.update_score": [[222, 232], ["beam.global_state.keys", "beam.scores.add_", "Beam.GNMTGlobalScorer.cov_penalty", "beam.scores.sub_"], "methods", ["None"], ["", "def", "update_score", "(", "self", ",", "beam", ",", "attn", ")", ":", "\n", "        ", "\"\"\"\n        Function to update scores of a Beam that is not finished\n        \"\"\"", "\n", "if", "\"prev_penalty\"", "in", "beam", ".", "global_state", ".", "keys", "(", ")", ":", "\n", "            ", "beam", ".", "scores", ".", "add_", "(", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", ")", "\n", "penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "+", "attn", ",", "\n", "self", ".", "beta", ")", "\n", "beam", ".", "scores", ".", "sub_", "(", "penalty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.Beam.GNMTGlobalScorer.update_global_state": [[233, 249], ["len", "beam.scores.clone().fill_", "beam.attn[].sum", "torch.min().sum", "beam.global_state[].index_select().add", "Beam.GNMTGlobalScorer.cov_penalty", "beam.scores.clone", "torch.min", "beam.global_state[].index_select"], "methods", ["None"], ["", "", "def", "update_global_state", "(", "self", ",", "beam", ")", ":", "\n", "        ", "\"Keeps the coverage vector as sum of attentions\"", "\n", "if", "len", "(", "beam", ".", "prev_ks", ")", "==", "1", ":", "\n", "            ", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", "=", "beam", ".", "scores", ".", "clone", "(", ")", ".", "fill_", "(", "0.0", ")", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "=", "beam", ".", "attn", "[", "-", "1", "]", "\n", "self", ".", "cov_total", "=", "beam", ".", "attn", "[", "-", "1", "]", ".", "sum", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cov_total", "+=", "torch", ".", "min", "(", "beam", ".", "attn", "[", "-", "1", "]", ",", "\n", "beam", ".", "global_state", "[", "'coverage'", "]", ")", ".", "sum", "(", "1", ")", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", "=", "beam", ".", "global_state", "[", "\"coverage\"", "]", ".", "index_select", "(", "0", ",", "beam", ".", "prev_ks", "[", "-", "1", "]", ")", ".", "add", "(", "beam", ".", "attn", "[", "-", "1", "]", ")", "\n", "\n", "prev_penalty", "=", "self", ".", "cov_penalty", "(", "beam", ",", "\n", "beam", ".", "global_state", "[", "\"coverage\"", "]", ",", "\n", "self", ".", "beta", ")", "\n", "beam", ".", "global_state", "[", "\"prev_penalty\"", "]", "=", "prev_penalty", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.__getstate__": [[27, 29], ["None"], "methods", ["None"], ["def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__dict__", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.__setstate__": [[30, 32], ["DatasetBase.ONMTDatasetBase.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "__setstate__", "(", "self", ",", "d", ")", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.__reduce_ex__": [[33, 36], ["super().__reduce_ex__"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.__reduce_ex__"], ["", "def", "__reduce_ex__", "(", "self", ",", "proto", ")", ":", "\n", "        ", "\"This is a hack. Something is broken with torch pickle.\"", "\n", "return", "super", "(", "ONMTDatasetBase", ",", "self", ")", ".", "__reduce_ex__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.load_fields": [[37, 48], ["load_fields_from_vocab", "dict", "vocab_dict.items", "load_fields_from_vocab.items"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab"], ["", "def", "load_fields", "(", "self", ",", "vocab_dict", ")", ":", "\n", "        ", "\"\"\" Load fields from vocab.pt, and set the `fields` attribute.\n\n        Args:\n            vocab_dict (dict): a dict of loaded vocab from vocab.pt file.\n        \"\"\"", "\n", "from", "onmt", ".", "io", ".", "IO", "import", "load_fields_from_vocab", "\n", "\n", "fields", "=", "load_fields_from_vocab", "(", "vocab_dict", ".", "items", "(", ")", ",", "self", ".", "data_type", ")", "\n", "self", ".", "fields", "=", "dict", "(", "[", "(", "k", ",", "f", ")", "for", "(", "k", ",", "f", ")", "in", "fields", ".", "items", "(", ")", "\n", "if", "k", "in", "self", ".", "examples", "[", "0", "]", ".", "__dict__", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features": [[49, 72], ["len", "all", "list", "token.split", "zip", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "extract_text_features", "(", "tokens", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tokens: A list of tokens, where each token consists of a word,\n                optionally followed by u\"\uffe8\"-delimited features.\n        Returns:\n            A sequence of words, a sequence of features, and num of features.\n        \"\"\"", "\n", "if", "not", "tokens", ":", "\n", "            ", "return", "[", "]", ",", "[", "]", ",", "-", "1", "\n", "\n", "", "split_tokens", "=", "[", "token", ".", "split", "(", "u\"\uffe8\"", ")", "for", "token", "in", "tokens", "]", "\n", "split_tokens", "=", "[", "token", "for", "token", "in", "split_tokens", "if", "token", "[", "0", "]", "]", "\n", "token_size", "=", "len", "(", "split_tokens", "[", "0", "]", ")", "\n", "\n", "assert", "all", "(", "len", "(", "token", ")", "==", "token_size", "for", "token", "in", "split_tokens", ")", ",", "\"all words must have the same number of features\"", "\n", "words_and_features", "=", "list", "(", "zip", "(", "*", "split_tokens", ")", ")", "\n", "words", "=", "words_and_features", "[", "0", "]", "\n", "features", "=", "words_and_features", "[", "1", ":", "]", "\n", "\n", "return", "words", ",", "features", ",", "token_size", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._join_dicts": [[75, 84], ["dict", "itertools.chain", "d.items"], "methods", ["None"], ["", "def", "_join_dicts", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dictionaries with disjoint keys.\n\n        Returns:\n            a single dictionary that has the union of these keys.\n        \"\"\"", "\n", "return", "dict", "(", "chain", "(", "*", "[", "d", ".", "items", "(", ")", "for", "d", "in", "args", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._peek": [[85, 97], ["next", "itertools.chain"], "methods", ["None"], ["", "def", "_peek", "(", "self", ",", "seq", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            seq: an iterator.\n\n        Returns:\n            the first thing returned by calling next() on the iterator\n            and an iterator created by re-chaining that value to the beginning\n            of the iterator.\n        \"\"\"", "\n", "first", "=", "next", "(", "seq", ")", "\n", "return", "first", ",", "chain", "(", "[", "first", "]", ",", "seq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._construct_example_fromlist": [[98, 117], ["torchtext.data.Example", "zip", "setattr", "setattr", "field.preprocess"], "methods", ["None"], ["", "def", "_construct_example_fromlist", "(", "self", ",", "data", ",", "fields", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            data: the data to be set as the value of the attributes of\n                the to-be-created `Example`, associating with respective\n                `Field` objects with same key.\n            fields: a dict of `torchtext.data.Field` objects. The keys\n                are attributes of the to-be-created `Example`.\n\n        Returns:\n            the created `Example` object.\n        \"\"\"", "\n", "ex", "=", "torchtext", ".", "data", ".", "Example", "(", ")", "\n", "for", "(", "name", ",", "field", ")", ",", "val", "in", "zip", "(", "fields", ",", "data", ")", ":", "\n", "            ", "if", "field", "is", "not", "None", ":", "\n", "                ", "setattr", "(", "ex", ",", "name", ",", "field", ".", "preprocess", "(", "val", ")", ")", "\n", "", "else", ":", "\n", "                ", "setattr", "(", "ex", ",", "name", ",", "val", ")", "\n", "", "", "return", "ex", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.__init__": [[38, 95], ["TextDataset.TextDataset._peek", "ex.keys", "print", "onmt.io.DatasetBase.ONMTDatasetBase.__init__", "TextDataset.TextDataset._dynamic_dict", "TextDataset.TextDataset._construct_example_fromlist", "len", "out_examples.append", "len", "TextDataset.TextDataset._join_dicts", "len", "zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._peek", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset._dynamic_dict", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._construct_example_fromlist", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._join_dicts"], ["def", "__init__", "(", "self", ",", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", "=", "0", ",", "num_tgt_feats", "=", "0", ",", "\n", "src_seq_length", "=", "0", ",", "tgt_seq_length", "=", "0", ",", "\n", "dynamic_dict", "=", "True", ",", "use_filter_pred", "=", "True", ")", ":", "\n", "        ", "self", ".", "data_type", "=", "'text'", "\n", "\n", "# self.src_vocabs: mutated in dynamic_dict, used in", "\n", "# collapse_copy_scores and in Translator.py", "\n", "self", ".", "src_vocabs", "=", "[", "]", "\n", "\n", "self", ".", "n_src_feats", "=", "num_src_feats", "\n", "self", ".", "n_tgt_feats", "=", "num_tgt_feats", "\n", "\n", "# Each element of an example is a dictionary whose keys represents", "\n", "# at minimum the src tokens and their indices and potentially also", "\n", "# the src and tgt features and alignment information.", "\n", "if", "tgt_examples_iter", "is", "not", "None", ":", "\n", "            ", "examples_iter", "=", "(", "self", ".", "_join_dicts", "(", "src", ",", "tgt", ")", "for", "src", ",", "tgt", "in", "\n", "zip", "(", "src_examples_iter", ",", "tgt_examples_iter", ")", ")", "\n", "", "else", ":", "\n", "            ", "examples_iter", "=", "src_examples_iter", "\n", "\n", "", "if", "dynamic_dict", ":", "\n", "            ", "examples_iter", "=", "self", ".", "_dynamic_dict", "(", "examples_iter", ")", "\n", "\n", "# Peek at the first to see which fields are used.", "\n", "", "ex", ",", "examples_iter", "=", "self", ".", "_peek", "(", "examples_iter", ")", "\n", "keys", "=", "ex", ".", "keys", "(", ")", "\n", "\n", "out_fields", "=", "[", "(", "k", ",", "fields", "[", "k", "]", ")", "if", "k", "in", "fields", "else", "(", "k", ",", "None", ")", "\n", "for", "k", "in", "keys", "]", "\n", "example_values", "=", "(", "[", "ex", "[", "k", "]", "for", "k", "in", "keys", "]", "for", "ex", "in", "examples_iter", ")", "\n", "\n", "# If out_examples is a generator, we need to save the filter_pred", "\n", "# function in serialization too, which would cause a problem when", "\n", "# `torch.save()`. Thus we materialize it as a list.", "\n", "src_size", "=", "0", "\n", "tgt_size", "=", "0", "\n", "out_examples", "=", "[", "]", "\n", "for", "ex_values", "in", "example_values", ":", "\n", "            ", "example", "=", "self", ".", "_construct_example_fromlist", "(", "\n", "ex_values", ",", "out_fields", ")", "\n", "src_size", "+=", "len", "(", "example", ".", "src", ")", "\n", "out_examples", ".", "append", "(", "example", ")", "\n", "#tgt_size += len(example.tgt)", "\n", "", "print", "(", "\"average src size\"", ",", "src_size", "/", "len", "(", "out_examples", ")", ",", "\n", "len", "(", "out_examples", ")", ")", "\n", "#print(\"Number of target tokens: {}\".format(tgt_size))", "\n", "\n", "def", "filter_pred", "(", "example", ")", ":", "\n", "            ", "return", "0", "<", "len", "(", "example", ".", "src", ")", "<=", "src_seq_length", "and", "0", "<", "len", "(", "example", ".", "tgt", ")", "<=", "tgt_seq_length", "\n", "\n", "", "filter_pred", "=", "filter_pred", "if", "use_filter_pred", "else", "lambda", "x", ":", "True", "\n", "\n", "super", "(", "TextDataset", ",", "self", ")", ".", "__init__", "(", "\n", "out_examples", ",", "out_fields", ",", "filter_pred", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.sort_key": [[97, 104], ["hasattr", "len", "len", "len"], "methods", ["None"], ["", "def", "sort_key", "(", "self", ",", "ex", ")", ":", "\n", "        ", "\"\"\" Sort using length of source sentences. \"\"\"", "\n", "# Default to a balanced sort, prioritizing tgt len match.", "\n", "# TODO: make this configurable.", "\n", "if", "hasattr", "(", "ex", ",", "\"tgt\"", ")", ":", "\n", "            ", "return", "len", "(", "ex", ".", "src", ")", ",", "len", "(", "ex", ".", "tgt", ")", "\n", "", "return", "len", "(", "ex", ".", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.collapse_copy_scores": [[105, 131], ["len", "range", "range", "len", "torch.Tensor().type_as", "torch.Tensor().type_as", "scores[].index_add_", "scores[].index_fill_", "torch.Tensor().type_as.append", "torch.Tensor().type_as.append", "scores[].index_select", "torch.Tensor", "torch.Tensor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collapse_copy_scores", "(", "scores", ",", "batch", ",", "tgt_vocab", ",", "src_vocabs", ")", ":", "\n", "        ", "\"\"\"\n        Given scores from an expanded dictionary\n        corresponeding to a batch, sums together copies,\n        with a dictionary word when it is ambigious.\n        \"\"\"", "\n", "offset", "=", "len", "(", "tgt_vocab", ")", "\n", "for", "b", "in", "range", "(", "batch", ".", "batch_size", ")", ":", "\n", "            ", "blank", "=", "[", "]", "\n", "fill", "=", "[", "]", "\n", "index", "=", "batch", ".", "indices", ".", "data", "[", "b", "]", "\n", "src_vocab", "=", "src_vocabs", "[", "index", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "src_vocab", ")", ")", ":", "\n", "                ", "sw", "=", "src_vocab", ".", "itos", "[", "i", "]", "\n", "ti", "=", "tgt_vocab", ".", "stoi", "[", "sw", "]", "\n", "if", "ti", "!=", "0", ":", "\n", "                    ", "blank", ".", "append", "(", "offset", "+", "i", ")", "\n", "fill", ".", "append", "(", "ti", ")", "\n", "", "", "if", "blank", ":", "\n", "                ", "blank", "=", "torch", ".", "Tensor", "(", "blank", ")", ".", "type_as", "(", "batch", ".", "indices", ".", "data", ")", "\n", "fill", "=", "torch", ".", "Tensor", "(", "fill", ")", ".", "type_as", "(", "batch", ".", "indices", ".", "data", ")", "\n", "scores", "[", ":", ",", "b", "]", ".", "index_add_", "(", "1", ",", "fill", ",", "\n", "scores", "[", ":", ",", "b", "]", ".", "index_select", "(", "1", ",", "blank", ")", ")", "\n", "scores", "[", ":", ",", "b", "]", ".", "index_fill_", "(", "1", ",", "blank", ",", "1e-10", ")", "\n", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.make_text_examples_nfeats_tpl": [[132, 161], ["TextDataset.read_text_file", "next", "itertools.chain"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.read_text_file"], ["", "@", "staticmethod", "\n", "def", "make_text_examples_nfeats_tpl", "(", "path", ",", "truncate", ",", "side", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src or tgt file.\n            truncate (int): maximum sequence length (0 for unlimited).\n            side (str): \"src\" or \"tgt\".\n\n        Returns:\n            (example_dict iterator, num_feats) tuple.\n        \"\"\"", "\n", "assert", "side", "in", "[", "'src'", ",", "'tgt'", "]", "\n", "\n", "if", "path", "is", "None", ":", "\n", "            ", "return", "(", "None", ",", "0", ")", "\n", "\n", "# All examples have same number of features, so we peek first one", "\n", "# to get the num_feats.", "\n", "", "examples_nfeats_iter", "=", "TextDataset", ".", "read_text_file", "(", "path", ",", "truncate", ",", "side", ")", "\n", "\n", "first_ex", "=", "next", "(", "examples_nfeats_iter", ")", "\n", "num_feats", "=", "first_ex", "[", "1", "]", "\n", "\n", "# Chain back the first element - we only want to peek it.", "\n", "examples_nfeats_iter", "=", "chain", "(", "[", "first_ex", "]", ",", "examples_nfeats_iter", ")", "\n", "examples_iter", "=", "(", "ex", "for", "ex", ",", "nfeats", "in", "examples_nfeats_iter", ")", "\n", "\n", "return", "(", "examples_iter", ",", "num_feats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.read_text_file": [[162, 188], ["codecs.open", "enumerate", "line.strip().split.strip().split.strip().split", "TextDataset.extract_text_features", "example_dict.update", "line.strip().split.strip().split.strip", "enumerate", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "@", "staticmethod", "\n", "def", "read_text_file", "(", "path", ",", "truncate", ",", "side", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src or tgt file.\n            truncate (int): maximum sequence length (0 for unlimited).\n            side (str): \"src\" or \"tgt\".\n\n        Yields:\n            (word, features, nfeat) triples for each line.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "path", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "corpus_file", ":", "\n", "            ", "for", "i", ",", "line", "in", "enumerate", "(", "corpus_file", ")", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "truncate", ":", "\n", "                    ", "line", "=", "line", "[", ":", "truncate", "]", "\n", "\n", "", "words", ",", "feats", ",", "n_feats", "=", "TextDataset", ".", "extract_text_features", "(", "line", ")", "\n", "\n", "example_dict", "=", "{", "side", ":", "words", ",", "\"indices\"", ":", "i", "}", "\n", "if", "feats", ":", "\n", "                    ", "prefix", "=", "side", "+", "\"_feat_\"", "\n", "example_dict", ".", "update", "(", "(", "prefix", "+", "str", "(", "j", ")", ",", "f", ")", "\n", "for", "j", ",", "f", "in", "enumerate", "(", "feats", ")", ")", "\n", "", "yield", "example_dict", ",", "n_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.get_fields": [[189, 250], ["torchtext.data.Field", "range", "torchtext.data.Field", "range", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "max", "torch.zeros", "enumerate", "max", "torch.zeros().long", "enumerate", "max", "len", "enumerate", "t.size", "t.size", "torch.zeros", "str", "str", "t.max", "len", "sent.size"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            n_src_features (int): the number of source features to\n                create `torchtext.data.Field` for.\n            n_tgt_features (int): the number of target features to\n                create `torchtext.data.Field` for.\n\n        Returns:\n            A dictionary whose keys are strings and whose values\n            are the corresponding Field objects.\n        \"\"\"", "\n", "fields", "=", "{", "}", "\n", "\n", "fields", "[", "\"src\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "pad_token", "=", "PAD_WORD", ",", "\n", "include_lengths", "=", "True", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_src_features", ")", ":", "\n", "            ", "fields", "[", "\"src_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "fields", "[", "\"tgt\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_tgt_features", ")", ":", "\n", "            ", "fields", "[", "\"tgt_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "def", "make_src", "(", "data", ",", "vocab", ")", ":", "\n", "            ", "src_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "src_vocab_size", "=", "max", "(", "[", "t", ".", "max", "(", ")", "for", "t", "in", "data", "]", ")", "+", "1", "\n", "alignment", "=", "torch", ".", "zeros", "(", "src_size", ",", "len", "(", "data", ")", ",", "src_vocab_size", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "for", "j", ",", "t", "in", "enumerate", "(", "sent", ")", ":", "\n", "                    ", "alignment", "[", "j", ",", "i", ",", "t", "]", "=", "1", "\n", "", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"src_map\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "dtype", "=", "torch", ".", "float", ",", "\n", "postprocessing", "=", "make_src", ",", "sequential", "=", "False", ")", "\n", "\n", "def", "make_tgt", "(", "data", ",", "vocab", ")", ":", "\n", "            ", "tgt_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "alignment", "=", "torch", ".", "zeros", "(", "tgt_size", ",", "len", "(", "data", ")", ")", ".", "long", "(", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "alignment", "[", ":", "sent", ".", "size", "(", "0", ")", ",", "i", "]", "=", "sent", "\n", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"alignment\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "dtype", "=", "torch", ".", "long", ",", "\n", "postprocessing", "=", "make_tgt", ",", "sequential", "=", "False", ")", "\n", "\n", "fields", "[", "\"indices\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "dtype", "=", "torch", ".", "long", ",", "\n", "sequential", "=", "False", ")", "\n", "\n", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.get_num_features": [[251, 271], ["codecs.open", "cf.readline().strip().split", "TextDataset.extract_text_features", "cf.readline().strip", "cf.readline"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features"], ["", "@", "staticmethod", "\n", "def", "get_num_features", "(", "corpus_file", ",", "side", ")", ":", "\n", "        ", "\"\"\"\n        Peek one line and get number of features of it.\n        (All lines must have same number of features).\n        For text corpus, both sides are in text form, thus\n        it works the same.\n\n        Args:\n            corpus_file (str): file path to get the features.\n            side (str): 'src' or 'tgt'.\n\n        Returns:\n            number of features on `side`.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "corpus_file", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "cf", ":", "\n", "            ", "f_line", "=", "cf", ".", "readline", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "_", ",", "_", ",", "num_feats", "=", "TextDataset", ".", "extract_text_features", "(", "f_line", ")", "\n", "\n", "", "return", "num_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset._dynamic_dict": [[273, 289], ["torchtext.vocab.Vocab", "TextDataset.TextDataset.src_vocabs.append", "torch.LongTensor", "collections.Counter", "torch.LongTensor"], "methods", ["None"], ["", "def", "_dynamic_dict", "(", "self", ",", "examples_iter", ")", ":", "\n", "        ", "for", "example", "in", "examples_iter", ":", "\n", "            ", "src", "=", "example", "[", "\"src\"", "]", "\n", "src_vocab", "=", "torchtext", ".", "vocab", ".", "Vocab", "(", "Counter", "(", "src", ")", ",", "\n", "specials", "=", "[", "UNK_WORD", ",", "PAD_WORD", "]", ")", "\n", "self", ".", "src_vocabs", ".", "append", "(", "src_vocab", ")", "\n", "# Mapping source tokens to indices in the dynamic dict.", "\n", "src_map", "=", "torch", ".", "LongTensor", "(", "[", "src_vocab", ".", "stoi", "[", "w", "]", "for", "w", "in", "src", "]", ")", "\n", "example", "[", "\"src_map\"", "]", "=", "src_map", "\n", "\n", "if", "\"tgt\"", "in", "example", ":", "\n", "                ", "tgt", "=", "example", "[", "\"tgt\"", "]", "\n", "mask", "=", "torch", ".", "LongTensor", "(", "\n", "[", "0", "]", "+", "[", "src_vocab", ".", "stoi", "[", "w", "]", "for", "w", "in", "tgt", "]", "+", "[", "0", "]", ")", "\n", "example", "[", "\"alignment\"", "]", "=", "mask", "\n", "", "yield", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator.__init__": [[300, 327], ["io.open", "sys.stderr.write", "sys.exit"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "corpus_path", ",", "line_truncate", ",", "side", ",", "shard_size", ",", "\n", "assoc_iter", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            corpus_path: the corpus file path.\n            line_truncate: the maximum length of a line to read.\n                            0 for unlimited.\n            side: \"src\" or \"tgt\".\n            shard_size: the shard size, 0 means not sharding the file.\n            assoc_iter: if not None, it is the associate iterator that\n                        this iterator should align its step with.\n        \"\"\"", "\n", "try", ":", "\n", "# The codecs module seems to have bugs with seek()/tell(),", "\n", "# so we use io.open().", "\n", "            ", "self", ".", "corpus", "=", "io", ".", "open", "(", "corpus_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "", "except", "IOError", ":", "\n", "            ", "sys", ".", "stderr", ".", "write", "(", "\"Failed to open corpus file: %s\"", "%", "corpus_path", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "self", ".", "line_truncate", "=", "line_truncate", "\n", "self", ".", "side", "=", "side", "\n", "self", ".", "shard_size", "=", "shard_size", "\n", "self", ".", "assoc_iter", "=", "assoc_iter", "\n", "self", ".", "last_pos", "=", "0", "\n", "self", ".", "line_index", "=", "-", "1", "\n", "self", ".", "eof", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator.__iter__": [[328, 375], ["TextDataset.ShardedTextCorpusIterator.corpus.seek", "TextDataset.ShardedTextCorpusIterator.corpus.readline", "TextDataset.ShardedTextCorpusIterator.corpus.close", "TextDataset.ShardedTextCorpusIterator.corpus.readline", "AssertionError", "TextDataset.ShardedTextCorpusIterator._example_dict_iter", "TextDataset.ShardedTextCorpusIterator.corpus.tell", "TextDataset.ShardedTextCorpusIterator.corpus.close", "TextDataset.ShardedTextCorpusIterator._example_dict_iter"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator._example_dict_iter", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator._example_dict_iter"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Iterator of (example_dict, nfeats).\n        On each call, it iterates over as many (example_dict, nfeats) tuples\n        until this shard's size equals to or approximates `self.shard_size`.\n        \"\"\"", "\n", "iteration_index", "=", "-", "1", "\n", "if", "self", ".", "assoc_iter", "is", "not", "None", ":", "\n", "# We have associate iterator, just yields tuples", "\n", "# util we run parallel with it.", "\n", "            ", "while", "self", ".", "line_index", "<", "self", ".", "assoc_iter", ".", "line_index", ":", "\n", "                ", "line", "=", "self", ".", "corpus", ".", "readline", "(", ")", "\n", "if", "line", "==", "''", ":", "\n", "                    ", "raise", "AssertionError", "(", "\n", "\"Two corpuses must have same number of lines!\"", ")", "\n", "\n", "", "self", ".", "line_index", "+=", "1", "\n", "iteration_index", "+=", "1", "\n", "yield", "self", ".", "_example_dict_iter", "(", "line", ",", "iteration_index", ")", "\n", "\n", "", "if", "self", ".", "assoc_iter", ".", "eof", ":", "\n", "                ", "self", ".", "eof", "=", "True", "\n", "self", ".", "corpus", ".", "close", "(", ")", "\n", "", "", "else", ":", "\n", "# Yield tuples util this shard's size reaches the threshold.", "\n", "            ", "self", ".", "corpus", ".", "seek", "(", "self", ".", "last_pos", ")", "\n", "while", "True", ":", "\n", "                ", "if", "self", ".", "shard_size", "!=", "0", "and", "self", ".", "line_index", "%", "64", "==", "0", ":", "\n", "# This part of check is time consuming on Py2 (but", "\n", "# it is quite fast on Py3, weird!). So we don't bother", "\n", "# to check for very line. Instead we chekc every 64", "\n", "# lines. Thus we are not dividing exactly per", "\n", "# `shard_size`, but it is not too much difference.", "\n", "                    ", "cur_pos", "=", "self", ".", "corpus", ".", "tell", "(", ")", "\n", "if", "cur_pos", ">=", "self", ".", "last_pos", "+", "self", ".", "shard_size", ":", "\n", "                        ", "self", ".", "last_pos", "=", "cur_pos", "\n", "raise", "StopIteration", "\n", "\n", "", "", "line", "=", "self", ".", "corpus", ".", "readline", "(", ")", "\n", "if", "line", "==", "''", ":", "\n", "                    ", "self", ".", "eof", "=", "True", "\n", "self", ".", "corpus", ".", "close", "(", ")", "\n", "raise", "StopIteration", "\n", "\n", "", "self", ".", "line_index", "+=", "1", "\n", "iteration_index", "+=", "1", "\n", "yield", "self", ".", "_example_dict_iter", "(", "line", ",", "iteration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator.hit_end": [[376, 378], ["None"], "methods", ["None"], ["", "", "", "def", "hit_end", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eof", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator.num_feats": [[379, 393], ["TextDataset.ShardedTextCorpusIterator.corpus.tell", "TextDataset.ShardedTextCorpusIterator.corpus.readline().split", "TextDataset.extract_text_features", "TextDataset.ShardedTextCorpusIterator.corpus.seek", "TextDataset.ShardedTextCorpusIterator.corpus.readline"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features"], ["", "@", "property", "\n", "def", "num_feats", "(", "self", ")", ":", "\n", "# We peek the first line and seek back to", "\n", "# the beginning of the file.", "\n", "        ", "saved_pos", "=", "self", ".", "corpus", ".", "tell", "(", ")", "\n", "\n", "line", "=", "self", ".", "corpus", ".", "readline", "(", ")", ".", "split", "(", ")", "\n", "if", "self", ".", "line_truncate", ":", "\n", "            ", "line", "=", "line", "[", ":", "self", ".", "line_truncate", "]", "\n", "", "_", ",", "_", ",", "self", ".", "n_feats", "=", "TextDataset", ".", "extract_text_features", "(", "line", ")", "\n", "\n", "self", ".", "corpus", ".", "seek", "(", "saved_pos", ")", "\n", "\n", "return", "self", ".", "n_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.ShardedTextCorpusIterator._example_dict_iter": [[394, 409], ["line.split.split.split", "TextDataset.extract_text_features", "onmt.Utils.aeq", "example_dict.update", "enumerate", "str"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.aeq", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "_example_dict_iter", "(", "self", ",", "line", ",", "index", ")", ":", "\n", "        ", "line", "=", "line", ".", "split", "(", ")", "\n", "if", "self", ".", "line_truncate", ":", "\n", "            ", "line", "=", "line", "[", ":", "self", ".", "line_truncate", "]", "\n", "", "words", ",", "feats", ",", "n_feats", "=", "TextDataset", ".", "extract_text_features", "(", "line", ")", "\n", "example_dict", "=", "{", "self", ".", "side", ":", "words", ",", "\"indices\"", ":", "index", "}", "\n", "if", "feats", ":", "\n", "# All examples must have same number of features.", "\n", "            ", "aeq", "(", "self", ".", "n_feats", ",", "n_feats", ")", "\n", "\n", "prefix", "=", "self", ".", "side", "+", "\"_feat_\"", "\n", "example_dict", ".", "update", "(", "(", "prefix", "+", "str", "(", "j", ")", ",", "f", ")", "\n", "for", "j", ",", "f", "in", "enumerate", "(", "feats", ")", ")", "\n", "\n", "", "return", "example_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.OrderedIterator.create_batches": [[360, 375], ["IO.OrderedIterator.create_batches.pool"], "methods", ["None"], ["    ", "def", "create_batches", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "train", ":", "\n", "            ", "def", "pool", "(", "data", ",", "random_shuffler", ")", ":", "\n", "                ", "for", "p", "in", "torchtext", ".", "data", ".", "batch", "(", "data", ",", "self", ".", "batch_size", "*", "100", ")", ":", "\n", "                    ", "p_batch", "=", "torchtext", ".", "data", ".", "batch", "(", "\n", "sorted", "(", "p", ",", "key", "=", "self", ".", "sort_key", ")", ",", "\n", "self", ".", "batch_size", ",", "self", ".", "batch_size_fn", ")", "\n", "for", "b", "in", "random_shuffler", "(", "list", "(", "p_batch", ")", ")", ":", "\n", "                        ", "yield", "b", "\n", "", "", "", "self", ".", "batches", "=", "pool", "(", "self", ".", "data", "(", ")", ",", "self", ".", "random_shuffler", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "batches", "=", "[", "]", "\n", "for", "b", "in", "torchtext", ".", "data", ".", "batch", "(", "self", ".", "data", "(", ")", ",", "self", ".", "batch_size", ",", "\n", "self", ".", "batch_size_fn", ")", ":", "\n", "                ", "self", ".", "batches", ".", "append", "(", "sorted", "(", "b", ",", "key", "=", "self", ".", "sort_key", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._getstate": [[18, 20], ["dict", "dict"], "function", ["None"], ["def", "_getstate", "(", "self", ")", ":", "\n", "    ", "return", "dict", "(", "self", ".", "__dict__", ",", "stoi", "=", "dict", "(", "self", ".", "stoi", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._setstate": [[22, 25], ["IO..__dict__.update", "collections.defaultdict"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "_setstate", "(", "self", ",", "state", ")", ":", "\n", "    ", "self", ".", "__dict__", ".", "update", "(", "state", ")", "\n", "self", ".", "stoi", "=", "defaultdict", "(", "lambda", ":", "0", ",", "self", ".", "stoi", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.get_fields": [[31, 50], ["onmt.io.TextDataset.TextDataset.get_fields", "onmt.io.ImageDataset.ImageDataset.get_fields", "onmt.io.AudioDataset.AudioDataset.get_fields"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields"], ["def", "get_fields", "(", "data_type", ",", "n_src_features", ",", "n_tgt_features", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        data_type: type of the source input. Options are [text|img|audio].\n        n_src_features: the number of source features to\n            create `torchtext.data.Field` for.\n        n_tgt_features: the number of target features to\n            create `torchtext.data.Field` for.\n\n    Returns:\n        A dictionary whose keys are strings and whose values are the\n        corresponding Field objects.\n    \"\"\"", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "return", "TextDataset", ".", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", "\n", "", "elif", "data_type", "==", "'img'", ":", "\n", "        ", "return", "ImageDataset", ".", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", "\n", "", "elif", "data_type", "==", "'audio'", ":", "\n", "        ", "return", "AudioDataset", ".", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab": [[52, 65], ["dict", "len", "len", "IO.get_fields", "dict.items", "IO.collect_features", "IO.collect_features", "collections.defaultdict"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features"], ["", "", "def", "load_fields_from_vocab", "(", "vocab", ",", "data_type", "=", "\"text\"", ")", ":", "\n", "    ", "\"\"\"\n    Load Field objects from `vocab.pt` file.\n    \"\"\"", "\n", "vocab", "=", "dict", "(", "vocab", ")", "\n", "n_src_features", "=", "len", "(", "collect_features", "(", "vocab", ",", "'src'", ")", ")", "\n", "n_tgt_features", "=", "len", "(", "collect_features", "(", "vocab", ",", "'tgt'", ")", ")", "\n", "fields", "=", "get_fields", "(", "data_type", ",", "n_src_features", ",", "n_tgt_features", ")", "\n", "for", "k", ",", "v", "in", "vocab", ".", "items", "(", ")", ":", "\n", "# Hack. Can't pickle defaultdict :(", "\n", "        ", "v", ".", "stoi", "=", "defaultdict", "(", "lambda", ":", "0", ",", "v", ".", "stoi", ")", "\n", "fields", "[", "k", "]", ".", "vocab", "=", "v", "\n", "", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.save_fields_to_vocab": [[67, 77], ["fields.items", "dict", "vocab.append"], "function", ["None"], ["", "def", "save_fields_to_vocab", "(", "fields", ")", ":", "\n", "    ", "\"\"\"\n    Save Vocab objects in Field objects to `vocab.pt` file.\n    \"\"\"", "\n", "vocab", "=", "[", "]", "\n", "for", "k", ",", "f", "in", "fields", ".", "items", "(", ")", ":", "\n", "        ", "if", "f", "is", "not", "None", "and", "'vocab'", "in", "f", ".", "__dict__", ":", "\n", "            ", "f", ".", "vocab", ".", "stoi", "=", "dict", "(", "f", ".", "vocab", ".", "stoi", ")", "\n", "vocab", ".", "append", "(", "(", "k", ",", "f", ".", "vocab", ")", ")", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs": [[79, 95], ["sum", "torchtext.vocab.Vocab", "torchtext.vocab.Vocab", "collections.Counter"], "function", ["None"], ["", "def", "merge_vocabs", "(", "vocabs", ",", "vocab_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Merge individual vocabularies (assumed to be generated from disjoint\n    documents) into a larger vocabulary.\n\n    Args:\n        vocabs: `torchtext.vocab.Vocab` vocabularies to be merged\n        vocab_size: `int` the final vocabulary size. `None` for no limit.\n    Return:\n        `torchtext.vocab.Vocab`\n    \"\"\"", "\n", "merged", "=", "sum", "(", "[", "vocab", ".", "freqs", "for", "vocab", "in", "vocabs", "]", ",", "Counter", "(", ")", ")", "\n", "return", "torchtext", ".", "vocab", ".", "Vocab", "(", "merged", ",", "\n", "specials", "=", "[", "UNK_WORD", ",", "PAD_WORD", ",", "\n", "BOS_WORD", ",", "EOS_WORD", "]", ",", "\n", "max_size", "=", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.get_num_features": [[97, 116], ["onmt.io.TextDataset.TextDataset.get_num_features", "onmt.io.ImageDataset.ImageDataset.get_num_features", "onmt.io.AudioDataset.AudioDataset.get_num_features"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features"], ["", "def", "get_num_features", "(", "data_type", ",", "corpus_file", ",", "side", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        data_type (str): type of the source input.\n            Options are [text|img|audio].\n        corpus_file (str): file path to get the features.\n        side (str): for source or for target.\n\n    Returns:\n        number of features on `side`.\n    \"\"\"", "\n", "assert", "side", "in", "[", "\"src\"", ",", "\"tgt\"", "]", "\n", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "return", "TextDataset", ".", "get_num_features", "(", "corpus_file", ",", "side", ")", "\n", "", "elif", "data_type", "==", "'img'", ":", "\n", "        ", "return", "ImageDataset", ".", "get_num_features", "(", "corpus_file", ",", "side", ")", "\n", "", "elif", "data_type", "==", "'audio'", ":", "\n", "        ", "return", "AudioDataset", ".", "get_num_features", "(", "corpus_file", ",", "side", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.make_features": [[118, 144], ["isinstance", "sorted", "torch.cat", "level.unsqueeze"], "function", ["None"], ["", "", "def", "make_features", "(", "batch", ",", "side", ",", "data_type", "=", "'text'", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        batch (Variable): a batch of source or target data.\n        side (str): for source or for target.\n        data_type (str): type of the source input.\n            Options are [text|img|audio].\n    Returns:\n        A sequence of src/tgt tensors with optional feature tensors\n        of size (len x batch).\n    \"\"\"", "\n", "assert", "side", "in", "[", "'src'", ",", "'tgt'", "]", "\n", "if", "isinstance", "(", "batch", ".", "__dict__", "[", "side", "]", ",", "tuple", ")", ":", "\n", "        ", "data", "=", "batch", ".", "__dict__", "[", "side", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "data", "=", "batch", ".", "__dict__", "[", "side", "]", "\n", "\n", "", "feat_start", "=", "side", "+", "\"_feat_\"", "\n", "keys", "=", "sorted", "(", "[", "k", "for", "k", "in", "batch", ".", "__dict__", "if", "feat_start", "in", "k", "]", ")", "\n", "features", "=", "[", "batch", ".", "__dict__", "[", "k", "]", "for", "k", "in", "keys", "]", "\n", "levels", "=", "[", "data", "]", "+", "features", "\n", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "[", "level", ".", "unsqueeze", "(", "2", ")", "for", "level", "in", "levels", "]", ",", "2", ")", "\n", "", "else", ":", "\n", "        ", "return", "levels", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_features": [[146, 158], ["itertools.count", "feats.append", "str"], "function", ["None"], ["", "", "def", "collect_features", "(", "fields", ",", "side", "=", "\"src\"", ")", ":", "\n", "    ", "\"\"\"\n    Collect features from Field object.\n    \"\"\"", "\n", "assert", "side", "in", "[", "\"src\"", ",", "\"tgt\"", "]", "\n", "feats", "=", "[", "]", "\n", "for", "j", "in", "count", "(", ")", ":", "\n", "        ", "key", "=", "side", "+", "\"_feat_\"", "+", "str", "(", "j", ")", "\n", "if", "key", "not", "in", "fields", ":", "\n", "            ", "break", "\n", "", "feats", ".", "append", "(", "key", ")", "\n", "", "return", "feats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.collect_feature_vocabs": [[160, 172], ["itertools.count", "feature_vocabs.append", "str"], "function", ["None"], ["", "def", "collect_feature_vocabs", "(", "fields", ",", "side", ")", ":", "\n", "    ", "\"\"\"\n    Collect feature Vocab objects from Field object.\n    \"\"\"", "\n", "assert", "side", "in", "[", "'src'", ",", "'tgt'", "]", "\n", "feature_vocabs", "=", "[", "]", "\n", "for", "j", "in", "count", "(", ")", ":", "\n", "        ", "key", "=", "side", "+", "\"_feat_\"", "+", "str", "(", "j", ")", "\n", "if", "key", "not", "in", "fields", ":", "\n", "            ", "break", "\n", "", "feature_vocabs", ".", "append", "(", "fields", "[", "key", "]", ".", "vocab", ")", "\n", "", "return", "feature_vocabs", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_dataset": [[174, 220], ["IO._make_examples_nfeats_tpl", "onmt.io.TextDataset.TextDataset.make_text_examples_nfeats_tpl", "onmt.io.TextDataset.TextDataset", "onmt.io.ImageDataset.ImageDataset", "onmt.io.AudioDataset.AudioDataset"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._make_examples_nfeats_tpl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.make_text_examples_nfeats_tpl"], ["", "def", "build_dataset", "(", "fields", ",", "data_type", ",", "src_path", ",", "tgt_path", ",", "src_dir", "=", "None", ",", "\n", "src_seq_length", "=", "0", ",", "tgt_seq_length", "=", "0", ",", "\n", "src_seq_length_trunc", "=", "0", ",", "tgt_seq_length_trunc", "=", "0", ",", "\n", "dynamic_dict", "=", "True", ",", "sample_rate", "=", "0", ",", "\n", "window_size", "=", "0", ",", "window_stride", "=", "0", ",", "window", "=", "None", ",", "\n", "normalize_audio", "=", "True", ",", "use_filter_pred", "=", "True", ")", ":", "\n", "\n", "# Build src/tgt examples iterator from corpus files, also extract", "\n", "# number of features.", "\n", "    ", "src_examples_iter", ",", "num_src_feats", "=", "_make_examples_nfeats_tpl", "(", "data_type", ",", "src_path", ",", "src_dir", ",", "\n", "src_seq_length_trunc", ",", "sample_rate", ",", "\n", "window_size", ",", "window_stride", ",", "\n", "window", ",", "normalize_audio", ")", "\n", "\n", "# For all data types, the tgt side corpus is in form of text.", "\n", "tgt_examples_iter", ",", "num_tgt_feats", "=", "TextDataset", ".", "make_text_examples_nfeats_tpl", "(", "\n", "tgt_path", ",", "tgt_seq_length_trunc", ",", "\"tgt\"", ")", "\n", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "dataset", "=", "TextDataset", "(", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", ",", "num_tgt_feats", ",", "\n", "src_seq_length", "=", "src_seq_length", ",", "\n", "tgt_seq_length", "=", "tgt_seq_length", ",", "\n", "dynamic_dict", "=", "dynamic_dict", ",", "\n", "use_filter_pred", "=", "use_filter_pred", ")", "\n", "\n", "", "elif", "data_type", "==", "'img'", ":", "\n", "        ", "dataset", "=", "ImageDataset", "(", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", ",", "num_tgt_feats", ",", "\n", "tgt_seq_length", "=", "tgt_seq_length", ",", "\n", "use_filter_pred", "=", "use_filter_pred", ")", "\n", "\n", "", "elif", "data_type", "==", "'audio'", ":", "\n", "        ", "dataset", "=", "AudioDataset", "(", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", ",", "num_tgt_feats", ",", "\n", "tgt_seq_length", "=", "tgt_seq_length", ",", "\n", "sample_rate", "=", "sample_rate", ",", "\n", "window_size", "=", "window_size", ",", "\n", "window_stride", "=", "window_stride", ",", "\n", "window", "=", "window", ",", "\n", "normalize_audio", "=", "normalize_audio", ",", "\n", "use_filter_pred", "=", "use_filter_pred", ")", "\n", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._build_field_vocab": [[222, 228], ["list", "field.vocab_cls", "collections.OrderedDict.fromkeys"], "function", ["None"], ["", "def", "_build_field_vocab", "(", "field", ",", "counter", ",", "**", "kwargs", ")", ":", "\n", "    ", "specials", "=", "list", "(", "OrderedDict", ".", "fromkeys", "(", "\n", "tok", "for", "tok", "in", "[", "field", ".", "unk_token", ",", "field", ".", "pad_token", ",", "field", ".", "init_token", ",", "\n", "field", ".", "eos_token", "]", "\n", "if", "tok", "is", "not", "None", ")", ")", "\n", "field", ".", "vocab", "=", "field", ".", "vocab_cls", "(", "counter", ",", "specials", "=", "specials", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.build_vocab": [[230, 328], ["IO._build_field_vocab", "print", "range", "collections.Counter", "len", "set", "print", "os.path.exists", "len", "set", "print", "os.path.exists", "torch.load", "print", "IO._build_field_vocab", "print", "IO._build_field_vocab", "print", "range", "open", "open", "len", "str", "IO._build_field_vocab", "print", "print", "IO.merge_vocabs", "set.add", "set.add", "getattr", "counter[].update", "len", "str", "line.strip().split", "line.strip().split", "len", "len", "line.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._build_field_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._build_field_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._build_field_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._build_field_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.merge_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Trainer.Statistics.update"], ["", "def", "build_vocab", "(", "train_dataset_files", ",", "fields", ",", "data_type", ",", "share_vocab", ",", "\n", "src_vocab_path", ",", "src_vocab_size", ",", "src_words_min_frequency", ",", "\n", "tgt_vocab_path", ",", "tgt_vocab_size", ",", "tgt_words_min_frequency", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        train_dataset_files: a list of train dataset pt file.\n        fields (dict): fields to build vocab for.\n        data_type: \"text\", \"img\" or \"audio\"?\n        share_vocab(bool): share source and target vocabulary?\n        src_vocab_path(string): Path to src vocabulary file.\n        src_vocab_size(int): size of the source vocabulary.\n        src_words_min_frequency(int): the minimum frequency needed to\n                include a source word in the vocabulary.\n        tgt_vocab_path(string): Path to tgt vocabulary file.\n        tgt_vocab_size(int): size of the target vocabulary.\n        tgt_words_min_frequency(int): the minimum frequency needed to\n                include a target word in the vocabulary.\n\n    Returns:\n        Dict of Fields\n    \"\"\"", "\n", "counter", "=", "{", "}", "\n", "for", "k", "in", "fields", ":", "\n", "        ", "counter", "[", "k", "]", "=", "Counter", "(", ")", "\n", "\n", "# Load vocabulary", "\n", "", "src_vocab", "=", "None", "\n", "if", "len", "(", "src_vocab_path", ")", ">", "0", ":", "\n", "        ", "src_vocab", "=", "set", "(", "[", "]", ")", "\n", "print", "(", "'Loading source vocab from %s'", "%", "src_vocab_path", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "src_vocab_path", ")", ",", "'src vocab %s not found!'", "%", "src_vocab_path", "\n", "with", "open", "(", "src_vocab_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "word", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "src_vocab", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "tgt_vocab", "=", "None", "\n", "if", "len", "(", "tgt_vocab_path", ")", ">", "0", ":", "\n", "        ", "tgt_vocab", "=", "set", "(", "[", "]", ")", "\n", "print", "(", "'Loading target vocab from %s'", "%", "tgt_vocab_path", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "tgt_vocab_path", ")", ",", "'tgt vocab %s not found!'", "%", "tgt_vocab_path", "\n", "with", "open", "(", "tgt_vocab_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "word", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "0", "]", "\n", "tgt_vocab", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "for", "path", "in", "train_dataset_files", ":", "\n", "        ", "dataset", "=", "torch", ".", "load", "(", "path", ")", "\n", "print", "(", "\" * reloading %s.\"", "%", "path", ")", "\n", "for", "ex", "in", "dataset", ".", "examples", ":", "\n", "            ", "for", "k", "in", "fields", ":", "\n", "                ", "val", "=", "getattr", "(", "ex", ",", "k", ",", "None", ")", "\n", "if", "val", "is", "not", "None", "and", "not", "fields", "[", "k", "]", ".", "sequential", ":", "\n", "                    ", "val", "=", "[", "val", "]", "\n", "", "elif", "k", "==", "'src'", "and", "src_vocab", ":", "\n", "                    ", "val", "=", "[", "item", "for", "item", "in", "val", "if", "item", "in", "src_vocab", "]", "\n", "", "elif", "k", "==", "'tgt'", "and", "tgt_vocab", ":", "\n", "                    ", "val", "=", "[", "item", "for", "item", "in", "val", "if", "item", "in", "tgt_vocab", "]", "\n", "", "counter", "[", "k", "]", ".", "update", "(", "val", ")", "\n", "\n", "", "", "", "_build_field_vocab", "(", "fields", "[", "\"tgt\"", "]", ",", "counter", "[", "\"tgt\"", "]", ",", "\n", "max_size", "=", "tgt_vocab_size", ",", "\n", "min_freq", "=", "tgt_words_min_frequency", ")", "\n", "print", "(", "\" * tgt vocab size: %d.\"", "%", "len", "(", "fields", "[", "\"tgt\"", "]", ".", "vocab", ")", ")", "\n", "\n", "# All datasets have same num of n_tgt_features,", "\n", "# getting the last one is OK.", "\n", "for", "j", "in", "range", "(", "dataset", ".", "n_tgt_feats", ")", ":", "\n", "        ", "key", "=", "\"tgt_feat_\"", "+", "str", "(", "j", ")", "\n", "_build_field_vocab", "(", "fields", "[", "key", "]", ",", "counter", "[", "key", "]", ")", "\n", "print", "(", "\" * %s vocab size: %d.\"", "%", "(", "key", ",", "len", "(", "fields", "[", "key", "]", ".", "vocab", ")", ")", ")", "\n", "\n", "", "if", "data_type", "==", "'text'", ":", "\n", "        ", "_build_field_vocab", "(", "fields", "[", "\"src\"", "]", ",", "counter", "[", "\"src\"", "]", ",", "\n", "max_size", "=", "src_vocab_size", ",", "\n", "min_freq", "=", "src_words_min_frequency", ")", "\n", "print", "(", "\" * src vocab size: %d.\"", "%", "len", "(", "fields", "[", "\"src\"", "]", ".", "vocab", ")", ")", "\n", "\n", "# All datasets have same num of n_src_features,", "\n", "# getting the last one is OK.", "\n", "for", "j", "in", "range", "(", "dataset", ".", "n_src_feats", ")", ":", "\n", "            ", "key", "=", "\"src_feat_\"", "+", "str", "(", "j", ")", "\n", "_build_field_vocab", "(", "fields", "[", "key", "]", ",", "counter", "[", "key", "]", ")", "\n", "print", "(", "\" * %s vocab size: %d.\"", "%", "(", "key", ",", "len", "(", "fields", "[", "key", "]", ".", "vocab", ")", ")", ")", "\n", "\n", "# Merge the input and output vocabularies.", "\n", "", "if", "share_vocab", ":", "\n", "# `tgt_vocab_size` is ignored when sharing vocabularies", "\n", "            ", "print", "(", "\" * merging src and tgt vocab...\"", ")", "\n", "merged_vocab", "=", "merge_vocabs", "(", "\n", "[", "fields", "[", "\"src\"", "]", ".", "vocab", ",", "fields", "[", "\"tgt\"", "]", ".", "vocab", "]", ",", "\n", "vocab_size", "=", "src_vocab_size", ")", "\n", "fields", "[", "\"src\"", "]", ".", "vocab", "=", "merged_vocab", "\n", "fields", "[", "\"tgt\"", "]", ".", "vocab", "=", "merged_vocab", "\n", "\n", "", "", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO._make_examples_nfeats_tpl": [[330, 357], ["onmt.io.TextDataset.TextDataset.make_text_examples_nfeats_tpl", "onmt.io.ImageDataset.ImageDataset.make_image_examples_nfeats_tpl", "onmt.io.AudioDataset.AudioDataset.make_audio_examples_nfeats_tpl"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.TextDataset.TextDataset.make_text_examples_nfeats_tpl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.make_image_examples_nfeats_tpl", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.make_audio_examples_nfeats_tpl"], ["", "def", "_make_examples_nfeats_tpl", "(", "data_type", ",", "src_path", ",", "src_dir", ",", "\n", "src_seq_length_trunc", ",", "sample_rate", ",", "\n", "window_size", ",", "window_stride", ",", "\n", "window", ",", "normalize_audio", ")", ":", "\n", "    ", "\"\"\"\n    Process the corpus into (example_dict iterator, num_feats) tuple\n    on source side for different 'data_type'.\n    \"\"\"", "\n", "\n", "if", "data_type", "==", "'text'", ":", "\n", "        ", "src_examples_iter", ",", "num_src_feats", "=", "TextDataset", ".", "make_text_examples_nfeats_tpl", "(", "\n", "src_path", ",", "src_seq_length_trunc", ",", "\"src\"", ")", "\n", "\n", "", "elif", "data_type", "==", "'img'", ":", "\n", "        ", "src_examples_iter", ",", "num_src_feats", "=", "ImageDataset", ".", "make_image_examples_nfeats_tpl", "(", "\n", "src_path", ",", "src_dir", ")", "\n", "\n", "", "elif", "data_type", "==", "'audio'", ":", "\n", "        ", "src_examples_iter", ",", "num_src_feats", "=", "AudioDataset", ".", "make_audio_examples_nfeats_tpl", "(", "\n", "src_path", ",", "src_dir", ",", "sample_rate", ",", "\n", "window_size", ",", "window_stride", ",", "window", ",", "\n", "normalize_audio", ")", "\n", "\n", "", "return", "src_examples_iter", ",", "num_src_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.__init__": [[36, 83], ["AudioDataset.AudioDataset._peek", "ex.keys", "list", "onmt.io.DatasetBase.ONMTDatasetBase.__init__", "AudioDataset.AudioDataset._construct_example_fromlist", "AudioDataset.AudioDataset._join_dicts", "zip", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._peek", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._construct_example_fromlist", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._join_dicts"], ["def", "__init__", "(", "self", ",", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", "=", "0", ",", "num_tgt_feats", "=", "0", ",", "\n", "tgt_seq_length", "=", "0", ",", "sample_rate", "=", "0", ",", "\n", "window_size", "=", "0.0", ",", "window_stride", "=", "0.0", ",", "window", "=", "None", ",", "\n", "normalize_audio", "=", "True", ",", "use_filter_pred", "=", "True", ")", ":", "\n", "        ", "self", ".", "data_type", "=", "'audio'", "\n", "\n", "self", ".", "sample_rate", "=", "sample_rate", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "window_stride", "=", "window_stride", "\n", "self", ".", "window", "=", "window", "\n", "self", ".", "normalize_audio", "=", "normalize_audio", "\n", "\n", "self", ".", "n_src_feats", "=", "num_src_feats", "\n", "self", ".", "n_tgt_feats", "=", "num_tgt_feats", "\n", "\n", "if", "tgt_examples_iter", "is", "not", "None", ":", "\n", "            ", "examples_iter", "=", "(", "self", ".", "_join_dicts", "(", "src", ",", "tgt", ")", "for", "src", ",", "tgt", "in", "\n", "zip", "(", "src_examples_iter", ",", "tgt_examples_iter", ")", ")", "\n", "", "else", ":", "\n", "            ", "examples_iter", "=", "src_examples_iter", "\n", "\n", "# Peek at the first to see which fields are used.", "\n", "", "ex", ",", "examples_iter", "=", "self", ".", "_peek", "(", "examples_iter", ")", "\n", "keys", "=", "ex", ".", "keys", "(", ")", "\n", "\n", "out_fields", "=", "[", "(", "k", ",", "fields", "[", "k", "]", ")", "if", "k", "in", "fields", "else", "(", "k", ",", "None", ")", "\n", "for", "k", "in", "keys", "]", "\n", "example_values", "=", "(", "[", "ex", "[", "k", "]", "for", "k", "in", "keys", "]", "for", "ex", "in", "examples_iter", ")", "\n", "out_examples", "=", "(", "self", ".", "_construct_example_fromlist", "(", "\n", "ex_values", ",", "out_fields", ")", "\n", "for", "ex_values", "in", "example_values", ")", "\n", "# If out_examples is a generator, we need to save the filter_pred", "\n", "# function in serialization too, which would cause a problem when", "\n", "# `torch.save()`. Thus we materialize it as a list.", "\n", "out_examples", "=", "list", "(", "out_examples", ")", "\n", "\n", "def", "filter_pred", "(", "example", ")", ":", "\n", "            ", "if", "tgt_examples_iter", "is", "not", "None", ":", "\n", "                ", "return", "0", "<", "len", "(", "example", ".", "tgt", ")", "<=", "tgt_seq_length", "\n", "", "else", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "filter_pred", "=", "filter_pred", "if", "use_filter_pred", "else", "lambda", "x", ":", "True", "\n", "\n", "super", "(", "AudioDataset", ",", "self", ")", ".", "__init__", "(", "\n", "out_examples", ",", "out_fields", ",", "filter_pred", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.sort_key": [[85, 88], ["ex.src.size"], "methods", ["None"], ["", "def", "sort_key", "(", "self", ",", "ex", ")", ":", "\n", "        ", "\"\"\" Sort using duration time of the sound spectrogram. \"\"\"", "\n", "return", "ex", ".", "src", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.make_audio_examples_nfeats_tpl": [[89, 116], ["AudioDataset.read_audio_file"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.read_audio_file"], ["", "@", "staticmethod", "\n", "def", "make_audio_examples_nfeats_tpl", "(", "path", ",", "audio_dir", ",", "\n", "sample_rate", ",", "window_size", ",", "\n", "window_stride", ",", "window", ",", "\n", "normalize_audio", ",", "truncate", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src file containing audio paths.\n            audio_dir (str): location of source audio files.\n            sample_rate (int): sample_rate.\n            window_size (float) : window size for spectrogram in seconds.\n            window_stride (float): window stride for spectrogram in seconds.\n            window (str): window type for spectrogram generation.\n            normalize_audio (bool): subtract spectrogram by mean and divide\n                by std or not.\n            truncate (int): maximum audio length (0 or None for unlimited).\n\n        Returns:\n            (example_dict iterator, num_feats) tuple\n        \"\"\"", "\n", "examples_iter", "=", "AudioDataset", ".", "read_audio_file", "(", "\n", "path", ",", "audio_dir", ",", "\"src\"", ",", "sample_rate", ",", "\n", "window_size", ",", "window_stride", ",", "window", ",", "\n", "normalize_audio", ",", "truncate", ")", "\n", "num_feats", "=", "0", "# Source side(audio) has no features.", "\n", "\n", "return", "(", "examples_iter", ",", "num_feats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.read_audio_file": [[117, 192], ["os.path.exists", "codecs.open", "os.path.join", "os.path.exists", "torchaudio.load", "sound.mean.mean.numpy", "int", "int", "librosa.stft", "librosa.magphase", "np.log1p", "torch.FloatTensor", "line.strip", "os.path.exists", "line.strip", "len", "torch.FloatTensor.mean", "torch.FloatTensor.std", "torch.FloatTensor.add_", "torch.FloatTensor.div_", "line.strip", "sound.mean.mean.size", "sound.mean.mean.squeeze", "sound.mean.mean.mean"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["", "@", "staticmethod", "\n", "def", "read_audio_file", "(", "path", ",", "src_dir", ",", "side", ",", "sample_rate", ",", "window_size", ",", "\n", "window_stride", ",", "window", ",", "normalize_audio", ",", "\n", "truncate", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src file containing audio paths.\n            src_dir (str): location of source audio files.\n            side (str): 'src' or 'tgt'.\n            sample_rate (int): sample_rate.\n            window_size (float) : window size for spectrogram in seconds.\n            window_stride (float): window stride for spectrogram in seconds.\n            window (str): window type for spectrogram generation.\n            normalize_audio (bool): subtract spectrogram by mean and divide\n                by std or not.\n            truncate (int): maximum audio length (0 or None for unlimited).\n\n        Yields:\n            a dictionary containing audio data for each line.\n        \"\"\"", "\n", "assert", "(", "src_dir", "is", "not", "None", ")", "and", "os", ".", "path", ".", "exists", "(", "src_dir", ")", ",", "\"src_dir must be a valid directory if data_type is audio\"", "\n", "\n", "global", "torchaudio", ",", "librosa", ",", "np", "\n", "import", "torchaudio", "\n", "import", "librosa", "\n", "import", "numpy", "as", "np", "\n", "\n", "with", "codecs", ".", "open", "(", "path", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "corpus_file", ":", "\n", "            ", "index", "=", "0", "\n", "for", "line", "in", "corpus_file", ":", "\n", "                ", "audio_path", "=", "os", ".", "path", ".", "join", "(", "src_dir", ",", "line", ".", "strip", "(", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "audio_path", ")", ":", "\n", "                    ", "audio_path", "=", "line", "\n", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "audio_path", ")", ",", "'audio path %s not found'", "%", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "sound", ",", "sample_rate", "=", "torchaudio", ".", "load", "(", "audio_path", ")", "\n", "if", "truncate", "and", "truncate", ">", "0", ":", "\n", "                    ", "if", "sound", ".", "size", "(", "0", ")", ">", "truncate", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "assert", "sample_rate", "==", "sample_rate", ",", "'Sample rate of %s != -sample_rate (%d vs %d)'", "%", "(", "audio_path", ",", "sample_rate", ",", "sample_rate", ")", "\n", "\n", "sound", "=", "sound", ".", "numpy", "(", ")", "\n", "if", "len", "(", "sound", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "if", "sound", ".", "shape", "[", "1", "]", "==", "1", ":", "\n", "                        ", "sound", "=", "sound", ".", "squeeze", "(", ")", "\n", "", "else", ":", "\n", "                        ", "sound", "=", "sound", ".", "mean", "(", "axis", "=", "1", ")", "# average multiple channels", "\n", "\n", "", "", "n_fft", "=", "int", "(", "sample_rate", "*", "window_size", ")", "\n", "win_length", "=", "n_fft", "\n", "hop_length", "=", "int", "(", "sample_rate", "*", "window_stride", ")", "\n", "# STFT", "\n", "d", "=", "librosa", ".", "stft", "(", "sound", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "\n", "win_length", "=", "win_length", ",", "window", "=", "window", ")", "\n", "spect", ",", "_", "=", "librosa", ".", "magphase", "(", "d", ")", "\n", "spect", "=", "np", ".", "log1p", "(", "spect", ")", "\n", "spect", "=", "torch", ".", "FloatTensor", "(", "spect", ")", "\n", "if", "normalize_audio", ":", "\n", "                    ", "mean", "=", "spect", ".", "mean", "(", ")", "\n", "std", "=", "spect", ".", "std", "(", ")", "\n", "spect", ".", "add_", "(", "-", "mean", ")", "\n", "spect", ".", "div_", "(", "std", ")", "\n", "\n", "", "example_dict", "=", "{", "side", ":", "spect", ",", "\n", "side", "+", "'_path'", ":", "line", ".", "strip", "(", ")", ",", "\n", "'indices'", ":", "index", "}", "\n", "index", "+=", "1", "\n", "\n", "yield", "example_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.get_fields": [[193, 262], ["torchtext.data.Field", "range", "torchtext.data.Field", "range", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "data[].size", "max", "torch.zeros", "enumerate", "torchtext.data.Field", "torchtext.data.Field", "max", "torch.zeros", "enumerate", "max", "torch.zeros().long", "enumerate", "len", "max", "len", "enumerate", "max.size", "max.size", "max.size", "torch.zeros", "str", "str", "max.max", "len", "spect.size", "sent.size"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            n_src_features: the number of source features to\n                create `torchtext.data.Field` for.\n            n_tgt_features: the number of target features to\n                create `torchtext.data.Field` for.\n\n        Returns:\n            A dictionary whose keys are strings and whose values\n            are the corresponding Field objects.\n        \"\"\"", "\n", "fields", "=", "{", "}", "\n", "\n", "def", "make_audio", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "nfft", "=", "data", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "t", "=", "max", "(", "[", "t", ".", "size", "(", "1", ")", "for", "t", "in", "data", "]", ")", "\n", "sounds", "=", "torch", ".", "zeros", "(", "len", "(", "data", ")", ",", "1", ",", "nfft", ",", "t", ")", "\n", "for", "i", ",", "spect", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "sounds", "[", "i", ",", ":", ",", ":", ",", "0", ":", "spect", ".", "size", "(", "1", ")", "]", "=", "spect", "\n", "", "return", "sounds", "\n", "\n", "", "fields", "[", "\"src\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "FloatTensor", ",", "\n", "postprocessing", "=", "make_audio", ",", "sequential", "=", "False", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_src_features", ")", ":", "\n", "            ", "fields", "[", "\"src_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "fields", "[", "\"tgt\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_tgt_features", ")", ":", "\n", "            ", "fields", "[", "\"tgt_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "def", "make_src", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "src_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "src_vocab_size", "=", "max", "(", "[", "t", ".", "max", "(", ")", "for", "t", "in", "data", "]", ")", "+", "1", "\n", "alignment", "=", "torch", ".", "zeros", "(", "src_size", ",", "len", "(", "data", ")", ",", "src_vocab_size", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "for", "j", ",", "t", "in", "enumerate", "(", "sent", ")", ":", "\n", "                    ", "alignment", "[", "j", ",", "i", ",", "t", "]", "=", "1", "\n", "", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"src_map\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "FloatTensor", ",", "\n", "postprocessing", "=", "make_src", ",", "sequential", "=", "False", ")", "\n", "\n", "def", "make_tgt", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "tgt_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "alignment", "=", "torch", ".", "zeros", "(", "tgt_size", ",", "len", "(", "data", ")", ")", ".", "long", "(", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "alignment", "[", ":", "sent", ".", "size", "(", "0", ")", ",", "i", "]", "=", "sent", "\n", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"alignment\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "LongTensor", ",", "\n", "postprocessing", "=", "make_tgt", ",", "sequential", "=", "False", ")", "\n", "\n", "fields", "[", "\"indices\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "LongTensor", ",", "\n", "sequential", "=", "False", ")", "\n", "\n", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.AudioDataset.AudioDataset.get_num_features": [[263, 285], ["codecs.open", "cf.readline().strip().split", "AudioDataset.extract_text_features", "cf.readline().strip", "cf.readline"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features"], ["", "@", "staticmethod", "\n", "def", "get_num_features", "(", "corpus_file", ",", "side", ")", ":", "\n", "        ", "\"\"\"\n        For audio corpus, source side is in form of audio, thus\n        no feature; while target side is in form of text, thus\n        we can extract its text features.\n\n        Args:\n            corpus_file (str): file path to get the features.\n            side (str): 'src' or 'tgt'.\n\n        Returns:\n            number of features on `side`.\n        \"\"\"", "\n", "if", "side", "==", "'src'", ":", "\n", "            ", "num_feats", "=", "0", "\n", "", "else", ":", "\n", "            ", "with", "codecs", ".", "open", "(", "corpus_file", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "cf", ":", "\n", "                ", "f_line", "=", "cf", ".", "readline", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "_", ",", "_", ",", "num_feats", "=", "AudioDataset", ".", "extract_text_features", "(", "f_line", ")", "\n", "\n", "", "", "return", "num_feats", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.__init__": [[30, 69], ["ImageDataset.ImageDataset._peek", "ex.keys", "list", "onmt.io.DatasetBase.ONMTDatasetBase.__init__", "ImageDataset.ImageDataset._construct_example_fromlist", "ImageDataset.ImageDataset._join_dicts", "zip", "len"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._peek", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._construct_example_fromlist", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase._join_dicts"], ["def", "__init__", "(", "self", ",", "fields", ",", "src_examples_iter", ",", "tgt_examples_iter", ",", "\n", "num_src_feats", "=", "0", ",", "num_tgt_feats", "=", "0", ",", "\n", "tgt_seq_length", "=", "0", ",", "use_filter_pred", "=", "True", ")", ":", "\n", "        ", "self", ".", "data_type", "=", "'img'", "\n", "\n", "self", ".", "n_src_feats", "=", "num_src_feats", "\n", "self", ".", "n_tgt_feats", "=", "num_tgt_feats", "\n", "\n", "if", "tgt_examples_iter", "is", "not", "None", ":", "\n", "            ", "examples_iter", "=", "(", "self", ".", "_join_dicts", "(", "src", ",", "tgt", ")", "for", "src", ",", "tgt", "in", "\n", "zip", "(", "src_examples_iter", ",", "tgt_examples_iter", ")", ")", "\n", "", "else", ":", "\n", "            ", "examples_iter", "=", "src_examples_iter", "\n", "\n", "# Peek at the first to see which fields are used.", "\n", "", "ex", ",", "examples_iter", "=", "self", ".", "_peek", "(", "examples_iter", ")", "\n", "keys", "=", "ex", ".", "keys", "(", ")", "\n", "\n", "out_fields", "=", "[", "(", "k", ",", "fields", "[", "k", "]", ")", "if", "k", "in", "fields", "else", "(", "k", ",", "None", ")", "\n", "for", "k", "in", "keys", "]", "\n", "example_values", "=", "(", "[", "ex", "[", "k", "]", "for", "k", "in", "keys", "]", "for", "ex", "in", "examples_iter", ")", "\n", "out_examples", "=", "(", "self", ".", "_construct_example_fromlist", "(", "\n", "ex_values", ",", "out_fields", ")", "\n", "for", "ex_values", "in", "example_values", ")", "\n", "# If out_examples is a generator, we need to save the filter_pred", "\n", "# function in serialization too, which would cause a problem when", "\n", "# `torch.save()`. Thus we materialize it as a list.", "\n", "out_examples", "=", "list", "(", "out_examples", ")", "\n", "\n", "def", "filter_pred", "(", "example", ")", ":", "\n", "            ", "if", "tgt_examples_iter", "is", "not", "None", ":", "\n", "                ", "return", "0", "<", "len", "(", "example", ".", "tgt", ")", "<=", "tgt_seq_length", "\n", "", "else", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "filter_pred", "=", "filter_pred", "if", "use_filter_pred", "else", "lambda", "x", ":", "True", "\n", "\n", "super", "(", "ImageDataset", ",", "self", ")", ".", "__init__", "(", "\n", "out_examples", ",", "out_fields", ",", "filter_pred", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.sort_key": [[71, 74], ["ex.src.size", "ex.src.size"], "methods", ["None"], ["", "def", "sort_key", "(", "self", ",", "ex", ")", ":", "\n", "        ", "\"\"\" Sort using the size of the image: (width, height).\"\"\"", "\n", "return", "(", "ex", ".", "src", ".", "size", "(", "2", ")", ",", "ex", ".", "src", ".", "size", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.make_image_examples_nfeats_tpl": [[75, 89], ["ImageDataset.read_img_file"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.read_img_file"], ["", "@", "staticmethod", "\n", "def", "make_image_examples_nfeats_tpl", "(", "path", ",", "img_dir", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src file containing image paths\n            src_dir (str): location of source images\n\n        Returns:\n            (example_dict iterator, num_feats) tuple\n        \"\"\"", "\n", "examples_iter", "=", "ImageDataset", ".", "read_img_file", "(", "path", ",", "img_dir", ",", "'src'", ")", "\n", "num_feats", "=", "0", "# Source side(img) has no features.", "\n", "\n", "return", "(", "examples_iter", ",", "num_feats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.read_img_file": [[90, 131], ["os.path.exists", "codecs.open", "os.path.join", "os.path.exists", "line.strip", "os.path.exists", "line.strip", "transforms.ToTensor", "Image.open", "line.strip", "img.size", "img.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "read_img_file", "(", "path", ",", "src_dir", ",", "side", ",", "truncate", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            path (str): location of a src file containing image paths\n            src_dir (str): location of source images\n            side (str): 'src' or 'tgt'\n            truncate: maximum img size ((0,0) or None for unlimited)\n\n        Yields:\n            a dictionary containing image data, path and index for each line.\n        \"\"\"", "\n", "assert", "(", "src_dir", "is", "not", "None", ")", "and", "os", ".", "path", ".", "exists", "(", "src_dir", ")", ",", "'src_dir must be a valid directory if data_type is img'", "\n", "\n", "global", "Image", ",", "transforms", "\n", "from", "PIL", "import", "Image", "\n", "from", "torchvision", "import", "transforms", "\n", "\n", "with", "codecs", ".", "open", "(", "path", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "corpus_file", ":", "\n", "            ", "index", "=", "0", "\n", "for", "line", "in", "corpus_file", ":", "\n", "                ", "img_path", "=", "os", ".", "path", ".", "join", "(", "src_dir", ",", "line", ".", "strip", "(", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "img_path", ")", ":", "\n", "                    ", "img_path", "=", "line", "\n", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "img_path", ")", ",", "'img path %s not found'", "%", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "img", "=", "transforms", ".", "ToTensor", "(", ")", "(", "Image", ".", "open", "(", "img_path", ")", ")", "\n", "if", "truncate", "and", "truncate", "!=", "(", "0", ",", "0", ")", ":", "\n", "                    ", "if", "not", "(", "img", ".", "size", "(", "1", ")", "<=", "truncate", "[", "0", "]", "\n", "and", "img", ".", "size", "(", "2", ")", "<=", "truncate", "[", "1", "]", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "example_dict", "=", "{", "side", ":", "img", ",", "\n", "side", "+", "'_path'", ":", "line", ".", "strip", "(", ")", ",", "\n", "'indices'", ":", "index", "}", "\n", "index", "+=", "1", "\n", "\n", "yield", "example_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_fields": [[132, 202], ["torchtext.data.Field", "range", "torchtext.data.Field", "range", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "data[].size", "max", "max", "torch.zeros().fill_", "enumerate", "torchtext.data.Field", "torchtext.data.Field", "max", "torch.zeros", "enumerate", "max", "torch.zeros().long", "enumerate", "max", "len", "enumerate", "t.size", "t.size", "torch.zeros", "t.size", "t.size", "torch.zeros", "len", "str", "str", "t.max", "len", "img.size", "img.size", "sent.size"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "get_fields", "(", "n_src_features", ",", "n_tgt_features", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            n_src_features: the number of source features to\n                create `torchtext.data.Field` for.\n            n_tgt_features: the number of target features to\n                create `torchtext.data.Field` for.\n\n        Returns:\n            A dictionary whose keys are strings and whose values\n            are the corresponding Field objects.\n        \"\"\"", "\n", "fields", "=", "{", "}", "\n", "\n", "def", "make_img", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "c", "=", "data", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "h", "=", "max", "(", "[", "t", ".", "size", "(", "1", ")", "for", "t", "in", "data", "]", ")", "\n", "w", "=", "max", "(", "[", "t", ".", "size", "(", "2", ")", "for", "t", "in", "data", "]", ")", "\n", "imgs", "=", "torch", ".", "zeros", "(", "len", "(", "data", ")", ",", "c", ",", "h", ",", "w", ")", ".", "fill_", "(", "1", ")", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "imgs", "[", "i", ",", ":", ",", "0", ":", "img", ".", "size", "(", "1", ")", ",", "0", ":", "img", ".", "size", "(", "2", ")", "]", "=", "img", "\n", "", "return", "imgs", "\n", "\n", "", "fields", "[", "\"src\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "FloatTensor", ",", "\n", "postprocessing", "=", "make_img", ",", "sequential", "=", "False", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_src_features", ")", ":", "\n", "            ", "fields", "[", "\"src_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "fields", "[", "\"tgt\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "for", "j", "in", "range", "(", "n_tgt_features", ")", ":", "\n", "            ", "fields", "[", "\"tgt_feat_\"", "+", "str", "(", "j", ")", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "init_token", "=", "BOS_WORD", ",", "eos_token", "=", "EOS_WORD", ",", "\n", "pad_token", "=", "PAD_WORD", ")", "\n", "\n", "", "def", "make_src", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "src_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "src_vocab_size", "=", "max", "(", "[", "t", ".", "max", "(", ")", "for", "t", "in", "data", "]", ")", "+", "1", "\n", "alignment", "=", "torch", ".", "zeros", "(", "src_size", ",", "len", "(", "data", ")", ",", "src_vocab_size", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "for", "j", ",", "t", "in", "enumerate", "(", "sent", ")", ":", "\n", "                    ", "alignment", "[", "j", ",", "i", ",", "t", "]", "=", "1", "\n", "", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"src_map\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "FloatTensor", ",", "\n", "postprocessing", "=", "make_src", ",", "sequential", "=", "False", ")", "\n", "\n", "def", "make_tgt", "(", "data", ",", "vocab", ",", "is_train", ")", ":", "\n", "            ", "tgt_size", "=", "max", "(", "[", "t", ".", "size", "(", "0", ")", "for", "t", "in", "data", "]", ")", "\n", "alignment", "=", "torch", ".", "zeros", "(", "tgt_size", ",", "len", "(", "data", ")", ")", ".", "long", "(", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "data", ")", ":", "\n", "                ", "alignment", "[", ":", "sent", ".", "size", "(", "0", ")", ",", "i", "]", "=", "sent", "\n", "", "return", "alignment", "\n", "\n", "", "fields", "[", "\"alignment\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "LongTensor", ",", "\n", "postprocessing", "=", "make_tgt", ",", "sequential", "=", "False", ")", "\n", "\n", "fields", "[", "\"indices\"", "]", "=", "torchtext", ".", "data", ".", "Field", "(", "\n", "use_vocab", "=", "False", ",", "tensor_type", "=", "torch", ".", "LongTensor", ",", "\n", "sequential", "=", "False", ")", "\n", "\n", "return", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.ImageDataset.ImageDataset.get_num_features": [[203, 225], ["codecs.open", "cf.readline().strip().split", "ImageDataset.extract_text_features", "cf.readline().strip", "cf.readline"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.io.DatasetBase.ONMTDatasetBase.extract_text_features"], ["", "@", "staticmethod", "\n", "def", "get_num_features", "(", "corpus_file", ",", "side", ")", ":", "\n", "        ", "\"\"\"\n        For image corpus, source side is in form of image, thus\n        no feature; while target side is in form of text, thus\n        we can extract its text features.\n\n        Args:\n            corpus_file (str): file path to get the features.\n            side (str): 'src' or 'tgt'.\n\n        Returns:\n            number of features on `side`.\n        \"\"\"", "\n", "if", "side", "==", "'src'", ":", "\n", "            ", "num_feats", "=", "0", "\n", "", "else", ":", "\n", "            ", "with", "codecs", ".", "open", "(", "corpus_file", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "cf", ":", "\n", "                ", "f_line", "=", "cf", ".", "readline", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "_", ",", "_", ",", "num_feats", "=", "ImageDataset", ".", "extract_text_features", "(", "f_line", ")", "\n", "\n", "", "", "return", "num_feats", "\n", "", "", ""]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.test_rouge.test_rouge": [[10, 46], ["time.strftime", "time.localtime", "len", "range", "pyrouge.Rouge155", "pyrouge.Rouge155.convert_and_evaluate", "pyrouge.Rouge155.output_to_dict", "os.path.isdir", "os.path.isdir", "os.mkdir", "os.mkdir", "os.mkdir", "line.strip", "line.strip", "len", "len", "shutil.rmtree", "len", "open", "f.write", "open", "f.write"], "function", ["None"], ["def", "test_rouge", "(", "cand", ",", "ref", ")", ":", "\n", "    ", "\"\"\"Calculate ROUGE scores of sequences passed as an iterator\n       e.g. a list of str, an open file, StringIO or even sys.stdin\n    \"\"\"", "\n", "current_time", "=", "time", ".", "strftime", "(", "'%Y-%m-%d-%H-%M-%S'", ",", "time", ".", "localtime", "(", ")", ")", "\n", "tmp_dir", "=", "\".rouge-tmp-{}\"", ".", "format", "(", "current_time", ")", "\n", "try", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "tmp_dir", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/candidate\"", ")", "\n", "os", ".", "mkdir", "(", "tmp_dir", "+", "\"/reference\"", ")", "\n", "", "candidates", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "cand", "]", "\n", "references", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "ref", "]", "\n", "assert", "len", "(", "candidates", ")", "==", "len", "(", "references", ")", "\n", "cnt", "=", "len", "(", "candidates", ")", "\n", "for", "i", "in", "range", "(", "cnt", ")", ":", "\n", "            ", "if", "len", "(", "references", "[", "i", "]", ")", "<", "1", ":", "\n", "                ", "continue", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/candidate/cand.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "candidates", "[", "i", "]", ")", "\n", "", "with", "open", "(", "tmp_dir", "+", "\"/reference/ref.{}.txt\"", ".", "format", "(", "i", ")", ",", "\"w\"", ",", "\n", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "references", "[", "i", "]", ")", "\n", "", "", "r", "=", "pyrouge", ".", "Rouge155", "(", ")", "\n", "r", ".", "model_dir", "=", "tmp_dir", "+", "\"/reference/\"", "\n", "r", ".", "system_dir", "=", "tmp_dir", "+", "\"/candidate/\"", "\n", "r", ".", "model_filename_pattern", "=", "'ref.#ID#.txt'", "\n", "r", ".", "system_filename_pattern", "=", "'cand.(\\d+).txt'", "\n", "rouge_results", "=", "r", ".", "convert_and_evaluate", "(", ")", "\n", "results_dict", "=", "r", ".", "output_to_dict", "(", "rouge_results", ")", "\n", "return", "results_dict", "\n", "", "finally", ":", "\n", "        ", "pass", "\n", "if", "os", ".", "path", ".", "isdir", "(", "tmp_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "tmp_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.test_rouge.rouge_results_to_str": [[48, 55], ["None"], "function", ["None"], ["", "", "", "def", "rouge_results_to_str", "(", "results_dict", ")", ":", "\n", "    ", "return", "\">> ROUGE(1/2/3/L/SU4): {:.2f}/{:.2f}/{:.2f}/{:.2f}/{:.2f}\"", ".", "format", "(", "\n", "results_dict", "[", "\"rouge_1_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_2_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_3_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_l_f_score\"", "]", "*", "100", ",", "\n", "results_dict", "[", "\"rouge_su*_f_score\"", "]", "*", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.get_vocabs": [[12, 31], ["torch.load", "print", "print", "print", "type", "type", "type", "len", "len"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["def", "get_vocabs", "(", "dict_file", ")", ":", "\n", "    ", "vocabs", "=", "torch", ".", "load", "(", "dict_file", ")", "\n", "\n", "enc_vocab", ",", "dec_vocab", "=", "None", ",", "None", "\n", "\n", "# the vocab object is a list of tuple (name, torchtext.Vocab)", "\n", "# we iterate over this list and associate vocabularies based on the name", "\n", "for", "vocab", "in", "vocabs", ":", "\n", "        ", "if", "vocab", "[", "0", "]", "==", "'src'", ":", "\n", "            ", "enc_vocab", "=", "vocab", "[", "1", "]", "\n", "", "if", "vocab", "[", "0", "]", "==", "'tgt'", ":", "\n", "            ", "dec_vocab", "=", "vocab", "[", "1", "]", "\n", "", "", "assert", "type", "(", "None", ")", "not", "in", "[", "type", "(", "enc_vocab", ")", ",", "type", "(", "dec_vocab", ")", "]", "\n", "\n", "print", "(", "\"From: %s\"", "%", "dict_file", ")", "\n", "print", "(", "\"\\t* source vocab: %d words\"", "%", "len", "(", "enc_vocab", ")", ")", "\n", "print", "(", "\"\\t* target vocab: %d words\"", "%", "len", "(", "dec_vocab", ")", ")", "\n", "\n", "return", "enc_vocab", ",", "dec_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.get_embeddings": [[33, 66], ["dict", "enumerate", "print", "enumerate", "print", "open", "l.decode().strip().split", "open", "l.decode().strip().split", "len", "len", "float", "len", "len", "len", "float", "len", "l.decode().strip", "l.decode().strip", "l.decode", "l.decode"], "function", ["None"], ["", "def", "get_embeddings", "(", "file_enc", ",", "opt", ",", "flag", ")", ":", "\n", "    ", "embs", "=", "dict", "(", ")", "\n", "if", "flag", "==", "'enc'", ":", "\n", "        ", "for", "(", "i", ",", "l", ")", "in", "enumerate", "(", "open", "(", "file_enc", ",", "'rb'", ")", ")", ":", "\n", "            ", "if", "i", "<", "opt", ".", "skip_lines", ":", "\n", "                ", "continue", "\n", "", "if", "not", "l", ":", "\n", "                ", "break", "\n", "", "if", "len", "(", "l", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "l_split", "=", "l", ".", "decode", "(", "'utf8'", ")", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "l_split", ")", "==", "2", ":", "\n", "                ", "continue", "\n", "", "embs", "[", "l_split", "[", "0", "]", "]", "=", "[", "float", "(", "em", ")", "for", "em", "in", "l_split", "[", "1", ":", "]", "]", "\n", "", "print", "(", "\"Got {} encryption embeddings from {}\"", ".", "format", "(", "len", "(", "embs", ")", ",", "\n", "file_enc", ")", ")", "\n", "", "else", ":", "\n", "\n", "        ", "for", "(", "i", ",", "l", ")", "in", "enumerate", "(", "open", "(", "file_enc", ",", "'rb'", ")", ")", ":", "\n", "            ", "if", "not", "l", ":", "\n", "                ", "break", "\n", "", "if", "len", "(", "l", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "l_split", "=", "l", ".", "decode", "(", "'utf8'", ")", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "l_split", ")", "==", "2", ":", "\n", "                ", "continue", "\n", "", "embs", "[", "l_split", "[", "0", "]", "]", "=", "[", "float", "(", "em", ")", "for", "em", "in", "l_split", "[", "1", ":", "]", "]", "\n", "", "print", "(", "\"Got {} decryption embeddings from {}\"", ".", "format", "(", "len", "(", "embs", ")", ",", "\n", "file_enc", ")", ")", "\n", "\n", "", "return", "embs", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.match_embeddings": [[68, 82], ["len", "numpy.zeros", "vocab.stoi.items", "six.next", "torch.Tensor", "six.itervalues", "len", "print"], "function", ["None"], ["", "def", "match_embeddings", "(", "vocab", ",", "emb", ",", "opt", ")", ":", "\n", "    ", "dim", "=", "len", "(", "six", ".", "next", "(", "six", ".", "itervalues", "(", "emb", ")", ")", ")", "\n", "filtered_embeddings", "=", "np", ".", "zeros", "(", "(", "len", "(", "vocab", ")", ",", "dim", ")", ")", "\n", "count", "=", "{", "\"match\"", ":", "0", ",", "\"miss\"", ":", "0", "}", "\n", "for", "w", ",", "w_id", "in", "vocab", ".", "stoi", ".", "items", "(", ")", ":", "\n", "        ", "if", "w", "in", "emb", ":", "\n", "            ", "filtered_embeddings", "[", "w_id", "]", "=", "emb", "[", "w", "]", "\n", "count", "[", "'match'", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "if", "opt", ".", "verbose", ":", "\n", "                ", "print", "(", "u\"not found:\\t{}\"", ".", "format", "(", "w", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "count", "[", "'miss'", "]", "+=", "1", "\n", "\n", "", "", "return", "torch", ".", "Tensor", "(", "filtered_embeddings", ")", ",", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.main": [[87, 138], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "embeddings_to_torch.get_vocabs", "embeddings_to_torch.get_embeddings", "embeddings_to_torch.get_embeddings", "embeddings_to_torch.match_embeddings", "embeddings_to_torch.match_embeddings", "print", "print", "print", "print", "print", "print", "print", "torch.save", "torch.save", "print", "filtered_enc_embeddings.size", "filtered_dec_embeddings.size"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.get_vocabs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.get_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.get_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.match_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.embeddings_to_torch.match_embeddings"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'embeddings_to_torch.py'", ")", "\n", "parser", ".", "add_argument", "(", "'-emb_file_enc'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"source Embeddings from this file\"", ")", "\n", "parser", ".", "add_argument", "(", "'-emb_file_dec'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"target Embeddings from this file\"", ")", "\n", "parser", ".", "add_argument", "(", "'-output_file'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Output file for the prepared data\"", ")", "\n", "parser", ".", "add_argument", "(", "'-dict_file'", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Dictionary file\"", ")", "\n", "parser", ".", "add_argument", "(", "'-verbose'", ",", "action", "=", "\"store_true\"", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-skip_lines'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Skip first lines of the embedding file\"", ")", "\n", "parser", ".", "add_argument", "(", "'-type'", ",", "choices", "=", "TYPES", ",", "default", "=", "\"GloVe\"", ")", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "enc_vocab", ",", "dec_vocab", "=", "get_vocabs", "(", "opt", ".", "dict_file", ")", "\n", "if", "opt", ".", "type", "==", "\"word2vec\"", ":", "\n", "        ", "opt", ".", "skip_lines", "=", "1", "\n", "\n", "", "embeddings_enc", "=", "get_embeddings", "(", "opt", ".", "emb_file_enc", ",", "opt", ",", "flag", "=", "'enc'", ")", "\n", "embeddings_dec", "=", "get_embeddings", "(", "opt", ".", "emb_file_dec", ",", "opt", ",", "flag", "=", "'dec'", ")", "\n", "\n", "filtered_enc_embeddings", ",", "enc_count", "=", "match_embeddings", "(", "enc_vocab", ",", "\n", "embeddings_enc", ",", "\n", "opt", ")", "\n", "filtered_dec_embeddings", ",", "dec_count", "=", "match_embeddings", "(", "dec_vocab", ",", "\n", "embeddings_dec", ",", "\n", "opt", ")", "\n", "print", "(", "\"\\nMatching: \"", ")", "\n", "match_percent", "=", "[", "_", "[", "'match'", "]", "/", "(", "_", "[", "'match'", "]", "+", "_", "[", "'miss'", "]", ")", "*", "100", "\n", "for", "_", "in", "[", "enc_count", ",", "dec_count", "]", "]", "\n", "print", "(", "\"\\t* enc: %d match, %d missing, (%.2f%%)\"", "%", "(", "enc_count", "[", "'match'", "]", ",", "\n", "enc_count", "[", "'miss'", "]", ",", "\n", "match_percent", "[", "0", "]", ")", ")", "\n", "print", "(", "\"\\t* dec: %d match, %d missing, (%.2f%%)\"", "%", "(", "dec_count", "[", "'match'", "]", ",", "\n", "dec_count", "[", "'miss'", "]", ",", "\n", "match_percent", "[", "1", "]", ")", ")", "\n", "\n", "print", "(", "\"\\nFiltered embeddings:\"", ")", "\n", "print", "(", "\"\\t* enc: \"", ",", "filtered_enc_embeddings", ".", "size", "(", ")", ")", "\n", "print", "(", "\"\\t* dec: \"", ",", "filtered_dec_embeddings", ".", "size", "(", ")", ")", "\n", "\n", "enc_output_file", "=", "opt", ".", "output_file", "+", "\".enc.pt\"", "\n", "dec_output_file", "=", "opt", ".", "output_file", "+", "\".dec.pt\"", "\n", "print", "(", "\"\\nSaving embedding as:\\n\\t* enc: %s\\n\\t* dec: %s\"", "\n", "%", "(", "enc_output_file", ",", "dec_output_file", ")", ")", "\n", "torch", ".", "save", "(", "filtered_enc_embeddings", ",", "enc_output_file", ")", "\n", "torch", ".", "save", "(", "filtered_dec_embeddings", ",", "dec_output_file", ")", "\n", "print", "(", "\"\\nDone.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.create_parser": [[29, 56], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.FileType", "argparse.FileType"], "function", ["None"], ["def", "create_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "formatter_class", "=", "argparse", ".", "RawDescriptionHelpFormatter", ",", "\n", "description", "=", "\"learn BPE-based word segmentation\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input'", ",", "'-i'", ",", "type", "=", "argparse", ".", "FileType", "(", "'r'", ")", ",", "default", "=", "sys", ".", "stdin", ",", "\n", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "\"Input text (default: standard input).\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output'", ",", "'-o'", ",", "type", "=", "argparse", ".", "FileType", "(", "'w'", ")", ",", "default", "=", "sys", ".", "stdout", ",", "\n", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "\"Output file for BPE codes (default: standard output)\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--symbols'", ",", "'-s'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"Create this many new symbols (each representing a character n-gram) (default: %(default)s))\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--min-frequency'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "metavar", "=", "'FREQ'", ",", "\n", "help", "=", "'Stop if no symbol pair has frequency >= FREQ (default: %(default)s))'", ")", "\n", "parser", ".", "add_argument", "(", "'--dict-input'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If set, input file is interpreted as a dictionary where each line contains a word-count pair\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--verbose'", ",", "'-v'", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"verbose mode.\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.get_vocabulary": [[57, 69], ["collections.Counter", "line.strip().split", "int", "line.split", "line.strip"], "function", ["None"], ["", "def", "get_vocabulary", "(", "fobj", ",", "is_dict", "=", "False", ")", ":", "\n", "    ", "\"\"\"Read text and return dictionary that encodes vocabulary\n    \"\"\"", "\n", "vocab", "=", "Counter", "(", ")", "\n", "for", "line", "in", "fobj", ":", "\n", "        ", "if", "is_dict", ":", "\n", "            ", "word", ",", "count", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "vocab", "[", "word", "]", "=", "int", "(", "count", ")", "\n", "", "else", ":", "\n", "            ", "for", "word", "in", "line", ".", "split", "(", ")", ":", "\n", "                ", "vocab", "[", "word", "]", "+=", "1", "\n", "", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.update_pair_statistics": [[70, 127], ["collections.defaultdict", "old_word.index", "word.index", "len", "len", "len", "len"], "function", ["None"], ["", "def", "update_pair_statistics", "(", "pair", ",", "changed", ",", "stats", ",", "indices", ")", ":", "\n", "    ", "\"\"\"Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    \"\"\"", "\n", "stats", "[", "pair", "]", "=", "0", "\n", "indices", "[", "pair", "]", "=", "defaultdict", "(", "int", ")", "\n", "first", ",", "second", "=", "pair", "\n", "new_pair", "=", "first", "+", "second", "\n", "for", "j", ",", "word", ",", "old_word", ",", "freq", "in", "changed", ":", "\n", "\n", "# find all instances of pair, and update frequency/indices around it", "\n", "        ", "i", "=", "0", "\n", "while", "True", ":", "\n", "# find first symbol", "\n", "            ", "try", ":", "\n", "                ", "i", "=", "old_word", ".", "index", "(", "first", ",", "i", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "break", "\n", "# if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])", "\n", "", "if", "i", "<", "len", "(", "old_word", ")", "-", "1", "and", "old_word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "# assuming a symbol sequence \"A B C\", if \"B C\" is merged, reduce the frequency of \"A B\"", "\n", "                ", "if", "i", ":", "\n", "                    ", "prev", "=", "old_word", "[", "i", "-", "1", ":", "i", "+", "1", "]", "\n", "stats", "[", "prev", "]", "-=", "freq", "\n", "indices", "[", "prev", "]", "[", "j", "]", "-=", "1", "\n", "", "if", "i", "<", "len", "(", "old_word", ")", "-", "2", ":", "\n", "# assuming a symbol sequence \"A B C B\", if \"B C\" is merged, reduce the frequency of \"C B\".", "\n", "# however, skip this if the sequence is A B C B C, because the frequency of \"C B\" will be reduced by the previous code block", "\n", "                    ", "if", "old_word", "[", "i", "+", "2", "]", "!=", "first", "or", "i", ">=", "len", "(", "old_word", ")", "-", "3", "or", "old_word", "[", "i", "+", "3", "]", "!=", "second", ":", "\n", "                        ", "nex", "=", "old_word", "[", "i", "+", "1", ":", "i", "+", "3", "]", "\n", "stats", "[", "nex", "]", "-=", "freq", "\n", "indices", "[", "nex", "]", "[", "j", "]", "-=", "1", "\n", "", "", "i", "+=", "2", "\n", "", "else", ":", "\n", "                ", "i", "+=", "1", "\n", "\n", "", "", "i", "=", "0", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "# find new pair", "\n", "                ", "i", "=", "word", ".", "index", "(", "new_pair", ",", "i", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "break", "\n", "# assuming a symbol sequence \"A BC D\", if \"B C\" is merged, increase the frequency of \"A BC\"", "\n", "", "if", "i", ":", "\n", "                ", "prev", "=", "word", "[", "i", "-", "1", ":", "i", "+", "1", "]", "\n", "stats", "[", "prev", "]", "+=", "freq", "\n", "indices", "[", "prev", "]", "[", "j", "]", "+=", "1", "\n", "# assuming a symbol sequence \"A BC B\", if \"B C\" is merged, increase the frequency of \"BC B\"", "\n", "# however, if the sequence is A BC BC, skip this step because the count of \"BC BC\" will be incremented by the previous code block", "\n", "", "if", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "!=", "new_pair", ":", "\n", "                ", "nex", "=", "word", "[", "i", ":", "i", "+", "2", "]", "\n", "stats", "[", "nex", "]", "+=", "freq", "\n", "indices", "[", "nex", "]", "[", "j", "]", "+=", "1", "\n", "", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.get_pair_statistics": [[129, 146], ["collections.defaultdict", "collections.defaultdict", "enumerate", "collections.defaultdict"], "function", ["None"], ["", "", "", "def", "get_pair_statistics", "(", "vocab", ")", ":", "\n", "    ", "\"\"\"Count frequency of all symbol pairs, and create index\"\"\"", "\n", "\n", "# data structure of pair frequencies", "\n", "stats", "=", "defaultdict", "(", "int", ")", "\n", "\n", "#index from pairs to words", "\n", "indices", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "\n", "for", "i", ",", "(", "word", ",", "freq", ")", "in", "enumerate", "(", "vocab", ")", ":", "\n", "        ", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "            ", "stats", "[", "prev_char", ",", "char", "]", "+=", "freq", "\n", "indices", "[", "prev_char", ",", "char", "]", "[", "i", "]", "+=", "1", "\n", "prev_char", "=", "char", "\n", "\n", "", "", "return", "stats", ",", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.replace_pair": [[148, 171], ["pair_str.replace.replace", "re.compile", "indices[].iteritems", "indices[].items", "re.compile.sub", "tuple", "changes.append", "tuple.split", "re.escape"], "function", ["None"], ["", "def", "replace_pair", "(", "pair", ",", "vocab", ",", "indices", ")", ":", "\n", "    ", "\"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"", "\n", "first", ",", "second", "=", "pair", "\n", "pair_str", "=", "''", ".", "join", "(", "pair", ")", "\n", "pair_str", "=", "pair_str", ".", "replace", "(", "'\\\\'", ",", "'\\\\\\\\'", ")", "\n", "changes", "=", "[", "]", "\n", "pattern", "=", "re", ".", "compile", "(", "r'(?<!\\S)'", "+", "re", ".", "escape", "(", "first", "+", "' '", "+", "second", ")", "+", "r'(?!\\S)'", ")", "\n", "if", "sys", ".", "version_info", "<", "(", "3", ",", "0", ")", ":", "\n", "        ", "iterator", "=", "indices", "[", "pair", "]", ".", "iteritems", "(", ")", "\n", "", "else", ":", "\n", "        ", "iterator", "=", "indices", "[", "pair", "]", ".", "items", "(", ")", "\n", "", "for", "j", ",", "freq", "in", "iterator", ":", "\n", "        ", "if", "freq", "<", "1", ":", "\n", "            ", "continue", "\n", "", "word", ",", "freq", "=", "vocab", "[", "j", "]", "\n", "new_word", "=", "' '", ".", "join", "(", "word", ")", "\n", "new_word", "=", "pattern", ".", "sub", "(", "pair_str", ",", "new_word", ")", "\n", "new_word", "=", "tuple", "(", "new_word", ".", "split", "(", ")", ")", "\n", "\n", "vocab", "[", "j", "]", "=", "(", "new_word", ",", "freq", ")", "\n", "changes", ".", "append", "(", "(", "j", ",", "new_word", ",", "word", ",", "freq", ")", ")", "\n", "\n", "", "return", "changes", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.prune_stats": [[172, 186], ["list", "stats.items"], "function", ["None"], ["", "def", "prune_stats", "(", "stats", ",", "big_stats", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    \"\"\"", "\n", "for", "item", ",", "freq", "in", "list", "(", "stats", ".", "items", "(", ")", ")", ":", "\n", "        ", "if", "freq", "<", "threshold", ":", "\n", "            ", "del", "stats", "[", "item", "]", "\n", "if", "freq", "<", "0", ":", "\n", "                ", "big_stats", "[", "item", "]", "+=", "freq", "\n", "", "else", ":", "\n", "                ", "big_stats", "[", "item", "]", "=", "freq", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.main": [[188, 229], ["outfile.write", "learn_bpe.get_vocabulary", "dict", "sorted", "learn_bpe.get_pair_statistics", "copy.deepcopy", "range", "dict.items", "max", "outfile.write", "learn_bpe.replace_pair", "learn_bpe.update_pair_statistics", "copy.deepcopy.values", "max", "learn_bpe.prune_stats", "copy.deepcopy", "max", "learn_bpe.prune_stats", "sys.stderr.write", "sys.stderr.write", "learn_bpe.prune_stats", "dict.items", "tuple"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.get_vocabulary", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.get_pair_statistics", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.replace_pair", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.update_pair_statistics", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.prune_stats", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.prune_stats", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.learn_bpe.prune_stats"], ["", "", "", "", "def", "main", "(", "infile", ",", "outfile", ",", "num_symbols", ",", "min_frequency", "=", "2", ",", "verbose", "=", "False", ",", "is_dict", "=", "False", ")", ":", "\n", "    ", "\"\"\"Learn num_symbols BPE operations from vocabulary, and write to outfile.\n    \"\"\"", "\n", "\n", "# version 0.2 changes the handling of the end-of-word token ('</w>');", "\n", "# version numbering allows bckward compatibility", "\n", "outfile", ".", "write", "(", "'#version: 0.2\\n'", ")", "\n", "\n", "vocab", "=", "get_vocabulary", "(", "infile", ",", "is_dict", ")", "\n", "vocab", "=", "dict", "(", "[", "(", "tuple", "(", "x", "[", ":", "-", "1", "]", ")", "+", "(", "x", "[", "-", "1", "]", "+", "'</w>'", ",", ")", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "vocab", ".", "items", "(", ")", "]", ")", "\n", "sorted_vocab", "=", "sorted", "(", "vocab", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "stats", ",", "indices", "=", "get_pair_statistics", "(", "sorted_vocab", ")", "\n", "big_stats", "=", "copy", ".", "deepcopy", "(", "stats", ")", "\n", "# threshold is inspired by Zipfian assumption, but should only affect speed", "\n", "threshold", "=", "max", "(", "stats", ".", "values", "(", ")", ")", "/", "10", "\n", "for", "i", "in", "range", "(", "num_symbols", ")", ":", "\n", "        ", "if", "stats", ":", "\n", "            ", "most_frequent", "=", "max", "(", "stats", ",", "key", "=", "lambda", "x", ":", "(", "stats", "[", "x", "]", ",", "x", ")", ")", "\n", "\n", "# we probably missed the best pair because of pruning; go back to full statistics", "\n", "", "if", "not", "stats", "or", "(", "i", "and", "stats", "[", "most_frequent", "]", "<", "threshold", ")", ":", "\n", "            ", "prune_stats", "(", "stats", ",", "big_stats", ",", "threshold", ")", "\n", "stats", "=", "copy", ".", "deepcopy", "(", "big_stats", ")", "\n", "most_frequent", "=", "max", "(", "stats", ",", "key", "=", "lambda", "x", ":", "(", "stats", "[", "x", "]", ",", "x", ")", ")", "\n", "# threshold is inspired by Zipfian assumption, but should only affect speed", "\n", "threshold", "=", "stats", "[", "most_frequent", "]", "*", "i", "/", "(", "i", "+", "10000.0", ")", "\n", "prune_stats", "(", "stats", ",", "big_stats", ",", "threshold", ")", "\n", "\n", "", "if", "stats", "[", "most_frequent", "]", "<", "min_frequency", ":", "\n", "            ", "sys", ".", "stderr", ".", "write", "(", "'no pair has frequency >= {0}. Stopping\\n'", ".", "format", "(", "min_frequency", ")", ")", "\n", "break", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "sys", ".", "stderr", ".", "write", "(", "'pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'", ".", "format", "(", "i", ",", "most_frequent", "[", "0", "]", ",", "most_frequent", "[", "1", "]", ",", "stats", "[", "most_frequent", "]", ")", ")", "\n", "", "outfile", ".", "write", "(", "'{0} {1}\\n'", ".", "format", "(", "*", "most_frequent", ")", ")", "\n", "changes", "=", "replace_pair", "(", "most_frequent", ",", "sorted_vocab", ",", "indices", ")", "\n", "update_pair_statistics", "(", "most_frequent", ",", "changes", ",", "stats", ",", "indices", ")", "\n", "stats", "[", "most_frequent", "]", "=", "0", "\n", "if", "not", "i", "%", "100", ":", "\n", "            ", "prune_stats", "(", "stats", ",", "big_stats", ",", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.average_models.average_models": [[6, 32], ["enumerate", "torch.load", "avg_model.items", "avg_generator.items", "avg_model[].mul_().add_().div_", "avg_generator[].mul_().add_().div_", "avg_model[].mul_().add_", "avg_generator[].mul_().add_", "avg_model[].mul_", "avg_generator[].mul_"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load"], ["def", "average_models", "(", "model_files", ")", ":", "\n", "    ", "vocab", "=", "None", "\n", "opt", "=", "None", "\n", "epoch", "=", "None", "\n", "avg_model", "=", "None", "\n", "avg_generator", "=", "None", "\n", "\n", "for", "i", ",", "model_file", "in", "enumerate", "(", "model_files", ")", ":", "\n", "        ", "m", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "model_weights", "=", "m", "[", "'model'", "]", "\n", "generator_weights", "=", "m", "[", "'generator'", "]", "\n", "\n", "if", "i", "==", "0", ":", "\n", "            ", "vocab", ",", "opt", ",", "epoch", "=", "m", "[", "'vocab'", "]", ",", "m", "[", "'opt'", "]", ",", "m", "[", "'epoch'", "]", "\n", "avg_model", "=", "model_weights", "\n", "avg_generator", "=", "generator_weights", "\n", "", "else", ":", "\n", "            ", "for", "(", "k", ",", "v", ")", "in", "avg_model", ".", "items", "(", ")", ":", "\n", "                ", "avg_model", "[", "k", "]", ".", "mul_", "(", "i", ")", ".", "add_", "(", "model_weights", "[", "k", "]", ")", ".", "div_", "(", "i", "+", "1", ")", "\n", "\n", "", "for", "(", "k", ",", "v", ")", "in", "avg_generator", ".", "items", "(", ")", ":", "\n", "                ", "avg_generator", "[", "k", "]", ".", "mul_", "(", "i", ")", ".", "add_", "(", "generator_weights", "[", "k", "]", ")", ".", "div_", "(", "i", "+", "1", ")", "\n", "\n", "", "", "", "final", "=", "{", "\"vocab\"", ":", "vocab", ",", "\"opt\"", ":", "opt", ",", "\"epoch\"", ":", "epoch", ",", "\"optim\"", ":", "None", ",", "\n", "\"generator\"", ":", "avg_generator", ",", "\"model\"", ":", "avg_model", "}", "\n", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.average_models.main": [[34, 44], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "average_models.average_models", "torch.save"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.average_models.average_models"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-models\"", ",", "\"-m\"", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"List of models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-output\"", ",", "\"-o\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Output file\"", ")", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "final", "=", "average_models", "(", "opt", ".", "models", ")", "\n", "torch", ".", "save", "(", "final", ",", "opt", ".", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.__init__": [[32, 56], ["codes.readline", "codes.readline.startswith", "dict", "dict", "tuple", "codes.seek", "tuple", "item.split", "int", "reversed", "apply_bpe.BPE.bpe_codes.items", "re.sub().split", "list", "enumerate", "re.sub", "codes.readline.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "codes", ",", "separator", "=", "'@@'", ",", "vocab", "=", "None", ",", "glossaries", "=", "None", ")", ":", "\n", "\n", "# check version information", "\n", "        ", "firstline", "=", "codes", ".", "readline", "(", ")", "\n", "if", "firstline", ".", "startswith", "(", "'#version:'", ")", ":", "\n", "            ", "self", ".", "version", "=", "tuple", "(", "[", "int", "(", "x", ")", "for", "x", "in", "re", ".", "sub", "(", "r'(\\.0+)*$'", ",", "''", ",", "firstline", ".", "split", "(", ")", "[", "-", "1", "]", ")", ".", "split", "(", "\".\"", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "version", "=", "(", "0", ",", "1", ")", "\n", "codes", ".", "seek", "(", "0", ")", "\n", "\n", "", "self", ".", "bpe_codes", "=", "[", "tuple", "(", "item", ".", "split", "(", ")", ")", "for", "item", "in", "codes", "]", "\n", "\n", "# some hacking to deal with duplicates (only consider first instance)", "\n", "self", ".", "bpe_codes", "=", "dict", "(", "[", "(", "code", ",", "i", ")", "for", "(", "i", ",", "code", ")", "in", "reversed", "(", "list", "(", "enumerate", "(", "self", ".", "bpe_codes", ")", ")", ")", "]", ")", "\n", "\n", "self", ".", "bpe_codes_reverse", "=", "dict", "(", "[", "(", "pair", "[", "0", "]", "+", "pair", "[", "1", "]", ",", "pair", ")", "for", "pair", ",", "i", "in", "self", ".", "bpe_codes", ".", "items", "(", ")", "]", ")", "\n", "\n", "self", ".", "separator", "=", "separator", "\n", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "self", ".", "glossaries", "=", "glossaries", "if", "glossaries", "else", "[", "]", "\n", "\n", "self", ".", "cache", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE.segment": [[57, 76], ["sentence.split", "output.append", "output.append", "apply_bpe.BPE._isolate_glossaries", "apply_bpe.encode"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE._isolate_glossaries", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.encode"], ["", "def", "segment", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "\"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "word", "in", "sentence", ".", "split", "(", ")", ":", "\n", "            ", "new_word", "=", "[", "out", "for", "segment", "in", "self", ".", "_isolate_glossaries", "(", "word", ")", "\n", "for", "out", "in", "encode", "(", "segment", ",", "\n", "self", ".", "bpe_codes", ",", "\n", "self", ".", "bpe_codes_reverse", ",", "\n", "self", ".", "vocab", ",", "\n", "self", ".", "separator", ",", "\n", "self", ".", "version", ",", "\n", "self", ".", "cache", ",", "\n", "self", ".", "glossaries", ")", "]", "\n", "\n", "for", "item", "in", "new_word", "[", ":", "-", "1", "]", ":", "\n", "                ", "output", ".", "append", "(", "item", "+", "self", ".", "separator", ")", "\n", "", "output", ".", "append", "(", "new_word", "[", "-", "1", "]", ")", "\n", "\n", "", "return", "' '", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.BPE._isolate_glossaries": [[77, 83], ["apply_bpe.isolate_glossary"], "methods", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.isolate_glossary"], ["", "def", "_isolate_glossaries", "(", "self", ",", "word", ")", ":", "\n", "        ", "word_segments", "=", "[", "word", "]", "\n", "for", "gloss", "in", "self", ".", "glossaries", ":", "\n", "            ", "word_segments", "=", "[", "out_segments", "for", "segment", "in", "word_segments", "\n", "for", "out_segments", "in", "isolate_glossary", "(", "segment", ",", "gloss", ")", "]", "\n", "", "return", "word_segments", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.create_parser": [[84, 119], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.FileType", "argparse.FileType", "argparse.FileType", "argparse.FileType"], "function", ["None"], ["", "", "def", "create_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "formatter_class", "=", "argparse", ".", "RawDescriptionHelpFormatter", ",", "\n", "description", "=", "\"learn BPE-based word segmentation\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input'", ",", "'-i'", ",", "type", "=", "argparse", ".", "FileType", "(", "'r'", ")", ",", "default", "=", "sys", ".", "stdin", ",", "\n", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "\"Input file (default: standard input).\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--codes'", ",", "'-c'", ",", "type", "=", "argparse", ".", "FileType", "(", "'r'", ")", ",", "metavar", "=", "'PATH'", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"File with BPE codes (created by learn_bpe.py).\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--output'", ",", "'-o'", ",", "type", "=", "argparse", ".", "FileType", "(", "'w'", ")", ",", "default", "=", "sys", ".", "stdout", ",", "\n", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "\"Output file (default: standard output)\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--separator'", ",", "'-s'", ",", "type", "=", "str", ",", "default", "=", "'@@'", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "\"Separator between non-final subword units (default: '%(default)s'))\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--vocabulary'", ",", "type", "=", "argparse", ".", "FileType", "(", "'r'", ")", ",", "default", "=", "None", ",", "\n", "metavar", "=", "\"PATH\"", ",", "\n", "help", "=", "\"Vocabulary file (built with get_vocab.py). If provided, this script reverts any merge operations that produce an OOV.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--vocabulary-threshold'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "metavar", "=", "\"INT\"", ",", "\n", "help", "=", "\"Vocabulary threshold. If vocabulary is provided, any word with frequency < threshold will be treated as OOV\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--glossaries'", ",", "type", "=", "str", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "\n", "metavar", "=", "\"STR\"", ",", "\n", "help", "=", "\"Glossaries. The strings provided in glossaries will not be affected\"", "+", "\n", "\"by the BPE (i.e. they will neither be broken into subwords, nor concatenated with other subwords\"", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.get_pairs": [[120, 131], ["set", "set.add"], "function", ["None"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.encode": [[132, 195], ["apply_bpe.get_pairs", "min", "tuple", "word[].endswith", "apply_bpe.check_vocab_and_split", "tuple", "len", "len", "apply_bpe.get_pairs", "tuple", "check_vocab_and_split.index", "tuple.extend", "tuple.append", "tuple.append", "bpe_codes.get", "tuple.extend", "word[].replace", "float", "len"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.get_pairs", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.check_vocab_and_split", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.get_pairs"], ["", "def", "encode", "(", "orig", ",", "bpe_codes", ",", "bpe_codes_reverse", ",", "vocab", ",", "separator", ",", "version", ",", "cache", ",", "glossaries", "=", "None", ")", ":", "\n", "    ", "\"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n    \"\"\"", "\n", "\n", "if", "orig", "in", "cache", ":", "\n", "        ", "return", "cache", "[", "orig", "]", "\n", "\n", "", "if", "orig", "in", "glossaries", ":", "\n", "        ", "cache", "[", "orig", "]", "=", "(", "orig", ",", ")", "\n", "return", "(", "orig", ",", ")", "\n", "\n", "", "if", "version", "==", "(", "0", ",", "1", ")", ":", "\n", "        ", "word", "=", "tuple", "(", "orig", ")", "+", "(", "'</w>'", ",", ")", "\n", "", "elif", "version", "==", "(", "0", ",", "2", ")", ":", "# more consistent handling of word-final segments", "\n", "        ", "word", "=", "tuple", "(", "orig", "[", ":", "-", "1", "]", ")", "+", "(", "orig", "[", "-", "1", "]", "+", "'</w>'", ",", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "        ", "return", "orig", "\n", "\n", "", "while", "True", ":", "\n", "        ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "bpe_codes", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "bpe_codes", ":", "\n", "            ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "            ", "break", "\n", "", "else", ":", "\n", "            ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "# don't print end-of-word symbols", "\n", "", "", "if", "word", "[", "-", "1", "]", "==", "'</w>'", ":", "\n", "        ", "word", "=", "word", "[", ":", "-", "1", "]", "\n", "", "elif", "word", "[", "-", "1", "]", ".", "endswith", "(", "'</w>'", ")", ":", "\n", "        ", "word", "=", "word", "[", ":", "-", "1", "]", "+", "(", "word", "[", "-", "1", "]", ".", "replace", "(", "'</w>'", ",", "''", ")", ",", ")", "\n", "\n", "", "if", "vocab", ":", "\n", "        ", "word", "=", "check_vocab_and_split", "(", "word", ",", "bpe_codes_reverse", ",", "vocab", ",", "separator", ")", "\n", "\n", "", "cache", "[", "orig", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.recursive_split": [[196, 222], ["apply_bpe.recursive_split", "apply_bpe.recursive_split"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.recursive_split", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.recursive_split"], ["", "def", "recursive_split", "(", "segment", ",", "bpe_codes", ",", "vocab", ",", "separator", ",", "final", "=", "False", ")", ":", "\n", "    ", "\"\"\"Recursively split segment into smaller units (by reversing BPE merges)\n    until all units are either in-vocabulary, or cannot be split futher.\"\"\"", "\n", "\n", "try", ":", "\n", "        ", "if", "final", ":", "\n", "            ", "left", ",", "right", "=", "bpe_codes", "[", "segment", "+", "'</w>'", "]", "\n", "right", "=", "right", "[", ":", "-", "4", "]", "\n", "", "else", ":", "\n", "            ", "left", ",", "right", "=", "bpe_codes", "[", "segment", "]", "\n", "", "", "except", ":", "\n", "#sys.stderr.write('cannot split {0} further.\\n'.format(segment))", "\n", "        ", "yield", "segment", "\n", "return", "\n", "\n", "", "if", "left", "+", "separator", "in", "vocab", ":", "\n", "        ", "yield", "left", "\n", "", "else", ":", "\n", "        ", "for", "item", "in", "recursive_split", "(", "left", ",", "bpe_codes", ",", "vocab", ",", "separator", ",", "False", ")", ":", "\n", "            ", "yield", "item", "\n", "\n", "", "", "if", "(", "final", "and", "right", "in", "vocab", ")", "or", "(", "not", "final", "and", "right", "+", "separator", "in", "vocab", ")", ":", "\n", "        ", "yield", "right", "\n", "", "else", ":", "\n", "        ", "for", "item", "in", "recursive_split", "(", "right", ",", "bpe_codes", ",", "vocab", ",", "separator", ",", "final", ")", ":", "\n", "            ", "yield", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.check_vocab_and_split": [[223, 246], ["out.append", "apply_bpe.recursive_split", "out.append", "apply_bpe.recursive_split", "out.append", "out.append"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.recursive_split", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.recursive_split"], ["", "", "", "def", "check_vocab_and_split", "(", "orig", ",", "bpe_codes", ",", "vocab", ",", "separator", ")", ":", "\n", "    ", "\"\"\"Check for each segment in word if it is in-vocabulary,\n    and segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"", "\n", "\n", "out", "=", "[", "]", "\n", "\n", "for", "segment", "in", "orig", "[", ":", "-", "1", "]", ":", "\n", "        ", "if", "segment", "+", "separator", "in", "vocab", ":", "\n", "            ", "out", ".", "append", "(", "segment", ")", "\n", "", "else", ":", "\n", "#sys.stderr.write('OOV: {0}\\n'.format(segment))", "\n", "            ", "for", "item", "in", "recursive_split", "(", "segment", ",", "bpe_codes", ",", "vocab", ",", "separator", ",", "False", ")", ":", "\n", "                ", "out", ".", "append", "(", "item", ")", "\n", "\n", "", "", "", "segment", "=", "orig", "[", "-", "1", "]", "\n", "if", "segment", "in", "vocab", ":", "\n", "        ", "out", ".", "append", "(", "segment", ")", "\n", "", "else", ":", "\n", "#sys.stderr.write('OOV: {0}\\n'.format(segment))", "\n", "        ", "for", "item", "in", "recursive_split", "(", "segment", ",", "bpe_codes", ",", "vocab", ",", "separator", ",", "True", ")", ":", "\n", "            ", "out", ".", "append", "(", "item", ")", "\n", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.read_vocabulary": [[248, 261], ["set", "line.split", "int", "set.add"], "function", ["None"], ["", "def", "read_vocabulary", "(", "vocab_file", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n    \"\"\"", "\n", "\n", "vocabulary", "=", "set", "(", ")", "\n", "\n", "for", "line", "in", "vocab_file", ":", "\n", "        ", "word", ",", "freq", "=", "line", ".", "split", "(", ")", "\n", "freq", "=", "int", "(", "freq", ")", "\n", "if", "threshold", "==", "None", "or", "freq", ">=", "threshold", ":", "\n", "            ", "vocabulary", ".", "add", "(", "word", ")", "\n", "\n", "", "", "return", "vocabulary", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.isolate_glossary": [[262, 277], ["word.split", "segment.strip", "splits[].strip"], "function", ["None"], ["", "def", "isolate_glossary", "(", "word", ",", "glossary", ")", ":", "\n", "    ", "\"\"\"\n    Isolate a glossary present inside a word.\n\n    Returns a list of subwords. In which all 'glossary' glossaries are isolated \n\n    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n        ['1934', 'USA', 'B', 'USA']\n    \"\"\"", "\n", "if", "word", "==", "glossary", "or", "glossary", "not", "in", "word", ":", "\n", "        ", "return", "[", "word", "]", "\n", "", "else", ":", "\n", "        ", "splits", "=", "word", ".", "split", "(", "glossary", ")", "\n", "segments", "=", "[", "segment", ".", "strip", "(", ")", "for", "split", "in", "splits", "[", ":", "-", "1", "]", "for", "segment", "in", "[", "split", ",", "glossary", "]", "if", "segment", "!=", "''", "]", "\n", "return", "segments", "+", "[", "splits", "[", "-", "1", "]", ".", "strip", "(", ")", "]", "if", "splits", "[", "-", "1", "]", "!=", "''", "else", "segments", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.extract_embeddings.write_embeddings": [[20, 27], ["open", "range", "min", "dict.itos[].encode", "range", "file.write", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.apply_bpe.encode"], ["def", "write_embeddings", "(", "filename", ",", "dict", ",", "embeddings", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "file", ":", "\n", "        ", "for", "i", "in", "range", "(", "min", "(", "len", "(", "embeddings", ")", ",", "len", "(", "dict", ".", "itos", ")", ")", ")", ":", "\n", "            ", "str", "=", "dict", ".", "itos", "[", "i", "]", ".", "encode", "(", "\"utf-8\"", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "embeddings", "[", "0", "]", ")", ")", ":", "\n", "                ", "str", "=", "str", "+", "(", "\" %5f\"", "%", "(", "embeddings", "[", "i", "]", "[", "j", "]", ")", ")", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "file", ".", "write", "(", "str", "+", "b\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.extract_embeddings.main": [[29, 70], ["argparse.ArgumentParser", "onmt.opts.model_opts", "onmt.opts.model_opts", "onmt.opts.model_opts", "onmt.opts.model_opts", "parser.parse_args", "torch.load", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.io.load_fields_from_vocab", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "onmt.ModelConstructor.make_base_model", "encoder.embeddings.word_lut.weight.data.tolist", "decoder.embeddings.word_lut.weight.data.tolist", "print", "extract_embeddings.write_embeddings", "print", "extract_embeddings.write_embeddings", "print", "print", "argparse.ArgumentParser.parse_known_args", "torch.cuda.set_device", "onmt.Utils.use_gpu"], "function", ["home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.opts.model_opts", "home.repos.pwc.inspect_result.harvardnlp_var-attn.None.preprocess.parse_args", "home.repos.pwc.inspect_result.harvardnlp_var-attn.translate.TranslationServer.ServerModel.load", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.io.IO.load_fields_from_vocab", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.ModelConstructor.make_base_model", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.extract_embeddings.write_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.tools.extract_embeddings.write_embeddings", "home.repos.pwc.inspect_result.harvardnlp_var-attn.onmt.Utils.use_gpu"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "dummy_parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'train.py'", ")", "\n", "onmt", ".", "opts", ".", "model_opts", "(", "dummy_parser", ")", "\n", "dummy_opt", "=", "dummy_parser", ".", "parse_known_args", "(", "[", "]", ")", "[", "0", "]", "\n", "opt", "=", "parser", ".", "parse_args", "(", ")", "\n", "opt", ".", "cuda", "=", "opt", ".", "gpu", ">", "-", "1", "\n", "if", "opt", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "opt", ".", "gpu", ")", "\n", "\n", "# Add in default model arguments, possibly added since training.", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "model", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model_opt", "=", "checkpoint", "[", "'opt'", "]", "\n", "src_dict", "=", "checkpoint", "[", "'vocab'", "]", "[", "1", "]", "[", "1", "]", "\n", "tgt_dict", "=", "checkpoint", "[", "'vocab'", "]", "[", "0", "]", "[", "1", "]", "\n", "\n", "fields", "=", "onmt", ".", "io", ".", "load_fields_from_vocab", "(", "checkpoint", "[", "'vocab'", "]", ")", "\n", "\n", "model_opt", "=", "checkpoint", "[", "'opt'", "]", "\n", "for", "arg", "in", "dummy_opt", ".", "__dict__", ":", "\n", "        ", "if", "arg", "not", "in", "model_opt", ":", "\n", "            ", "model_opt", ".", "__dict__", "[", "arg", "]", "=", "dummy_opt", ".", "__dict__", "[", "arg", "]", "\n", "\n", "", "", "model", "=", "onmt", ".", "ModelConstructor", ".", "make_base_model", "(", "\n", "model_opt", ",", "fields", ",", "use_gpu", "(", "opt", ")", ",", "checkpoint", ")", "\n", "encoder", "=", "model", ".", "encoder", "\n", "decoder", "=", "model", ".", "decoder", "\n", "\n", "encoder_embeddings", "=", "encoder", ".", "embeddings", ".", "word_lut", ".", "weight", ".", "data", ".", "tolist", "(", ")", "\n", "decoder_embeddings", "=", "decoder", ".", "embeddings", ".", "word_lut", ".", "weight", ".", "data", ".", "tolist", "(", ")", "\n", "\n", "print", "(", "\"Writing source embeddings\"", ")", "\n", "write_embeddings", "(", "opt", ".", "output_dir", "+", "\"/src_embeddings.txt\"", ",", "src_dict", ",", "\n", "encoder_embeddings", ")", "\n", "\n", "print", "(", "\"Writing target embeddings\"", ")", "\n", "write_embeddings", "(", "opt", ".", "output_dir", "+", "\"/tgt_embeddings.txt\"", ",", "tgt_dict", ",", "\n", "decoder_embeddings", ")", "\n", "\n", "print", "(", "'... done.'", ")", "\n", "print", "(", "'Converting model...'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.harvardnlp_var-attn.source.conf.setup": [[194, 200], ["print", "app.add_config_value", "app.add_transform"], "function", ["None"], ["def", "setup", "(", "app", ")", ":", "\n", "    ", "print", "(", "\"hello\"", ")", "\n", "app", ".", "add_config_value", "(", "'recommonmark_config'", ",", "{", "\n", "'enable_eval_rst'", ":", "True", "\n", "}", ",", "True", ")", "\n", "app", ".", "add_transform", "(", "AutoStructify", ")", "\n", "", ""]]}