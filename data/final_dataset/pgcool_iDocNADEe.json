{"home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.loadGloveModel": [[27, 54], ["print", "open", "print", "line.split", "numpy.array", "len", "os.path.join", "os.path.join", "float", "os.path.join", "os.path.join", "print", "exit"], "function", ["None"], ["def", "loadGloveModel", "(", "gloveFile", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "    ", "'''\n    This function loads GloVe embeddings as per hidden size of DocNADE or iDocNADE.\n    '''", "\n", "if", "gloveFile", "is", "None", ":", "\n", "        ", "if", "params", ".", "hidden_size", "==", "50", ":", "\n", "            ", "gloveFile", "=", "os", ".", "path", ".", "join", "(", "home_dir", ",", "\"resources/pretrained_embeddings/glove.6B.50d.txt\"", ")", "\n", "", "elif", "params", ".", "hidden_size", "==", "100", ":", "\n", "            ", "gloveFile", "=", "os", ".", "path", ".", "join", "(", "home_dir", ",", "\"resources/pretrained_embeddings/glove.6B.100d.txt\"", ")", "\n", "", "elif", "params", ".", "hidden_size", "==", "200", ":", "\n", "            ", "gloveFile", "=", "os", ".", "path", ".", "join", "(", "home_dir", ",", "\"resources/pretrained_embeddings/glove.6B.200d.txt\"", ")", "\n", "", "elif", "params", ".", "hidden_size", "==", "300", ":", "\n", "            ", "gloveFile", "=", "os", ".", "path", ".", "join", "(", "home_dir", ",", "\"resources/pretrained_embeddings/glove.6B.300d.txt\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Invalid dimension [%d] for Glove pretrained embedding matrix!!'", "%", "params", ".", "hidden_size", ")", "\n", "exit", "(", ")", "\n", "\n", "", "", "print", "(", "\"Loading Glove Model\"", ")", "\n", "f", "=", "open", "(", "gloveFile", ",", "'r'", ")", "\n", "model", "=", "{", "}", "\n", "for", "line", "in", "f", ":", "\n", "        ", "splitLine", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "splitLine", "[", "0", "]", "\n", "embedding", "=", "np", ".", "array", "(", "[", "float", "(", "val", ")", "for", "val", "in", "splitLine", "[", "1", ":", "]", "]", ")", "\n", "model", "[", "word", "]", "=", "embedding", "\n", "", "print", "(", "\"Done.\"", ",", "len", "(", "model", ")", ",", "\" words loaded!\"", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.train": [[56, 566], ["os.path.join", "os.path.join", "os.path.join", "tensorflow.Session", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.FileWriter", "tensorflow.summary.merge_all", "tensorflow.train.Saver", "tensorflow.local_variables_initializer().run", "tensorflow.global_variables_initializer().run", "numpy.array", "numpy.array", "numpy.array", "print", "range", "print", "tensorflow.global_variables", "dataset.batches_bidirectional", "dataset.batches", "tensorflow.ConfigProto", "tensorflow.local_variables_initializer", "tensorflow.global_variables_initializer", "next", "session.run", "losses.append", "next", "session.run", "losses.append", "print", "print", "print", "print", "tf.summary.FileWriter.add_summary", "tf.summary.FileWriter.flush", "print", "print", "dataset.rows", "dataset.rows", "dataset.rows", "dataset.batches_bidirectional", "dataset.batches", "numpy.mean", "numpy.exp", "print", "tf.train.Saver.save", "open", "f.write", "print", "dataset.batches_bidirectional", "dataset.batches", "numpy.mean", "numpy.exp", "model.vectors_bidirectional", "model.vectors_bidirectional", "model.vectors", "model.vectors", "model.evaluate", "print", "tf.train.Saver.save", "open", "f.write", "session.run", "session.run", "print", "dataset.batches_bidirectional", "dataset.batches", "numpy.mean", "numpy.exp", "open", "f.write", "dataset.batches_bidirectional", "dataset.batches", "model.vectors_bidirectional", "model.vectors_bidirectional", "model.vectors", "model.vectors", "model.evaluate", "tensorflow.GPUOptions", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "this_val_nll_bw.append", "this_val_loss_normed_bw.append", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "numpy.mean", "os.path.join", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "this_val_nll_bw.append", "this_val_loss_normed_bw.append", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "numpy.mean", "dataset.batches_bidirectional", "dataset.batches_bidirectional", "dataset.batches", "dataset.batches", "os.path.join", "session.run", "this_test_nll.append", "this_test_loss_normed.append", "this_test_nll_bw.append", "this_test_loss_normed_bw.append", "session.run", "this_test_nll.append", "this_test_loss_normed.append", "numpy.mean", "os.path.join", "session.run", "session.run", "dataset.batches_bidirectional", "dataset.batches_bidirectional", "dataset.batches", "dataset.batches", "numpy.mean", "numpy.mean", "numpy.exp", "numpy.exp", "numpy.mean", "numpy.mean", "numpy.exp", "numpy.exp", "numpy.mean", "numpy.mean", "numpy.exp", "numpy.exp", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.average", "numpy.average", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches"], ["", "def", "train", "(", "model", ",", "dataset", ",", "params", ")", ":", "\n", "    ", "'''\n    This function runs training of DocNADE/iDocNADE\n    based on the given parameters.\n    Also logs the training and validation PPL and IR scores in \n    log directory.\n    For information about various training parameters see ReadME.md file\n    '''", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "params", ".", "model", ",", "'logs'", ")", "\n", "model_dir_ir", "=", "os", ".", "path", ".", "join", "(", "params", ".", "model", ",", "'model_ir'", ")", "\n", "model_dir_ppl", "=", "os", ".", "path", ".", "join", "(", "params", ".", "model", ",", "'model_ppl'", ")", "\n", "\n", "with", "tf", ".", "Session", "(", "config", "=", "tf", ".", "ConfigProto", "(", "\n", "inter_op_parallelism_threads", "=", "params", ".", "num_cores", ",", "\n", "intra_op_parallelism_threads", "=", "params", ".", "num_cores", ",", "\n", "gpu_options", "=", "tf", ".", "GPUOptions", "(", "allow_growth", "=", "True", ")", "\n", ")", ")", "as", "session", ":", "\n", "        ", "avg_loss", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "'loss_ph'", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'loss'", ",", "avg_loss", ")", "\n", "\n", "validation", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "'validation_ph'", ")", "\n", "validation_accuracy", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "'validation_acc'", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'validation'", ",", "validation", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'validation_accuracy'", ",", "validation_accuracy", ")", "\n", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "log_dir", ",", "session", ".", "graph", ")", "\n", "summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "global_variables", "(", ")", ")", "\n", "\n", "tf", ".", "local_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "tf", ".", "global_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "\n", "losses", "=", "[", "]", "\n", "\n", "# Shuffle: the order of words in the sentence for DocNADE to avoid overfitting", "\n", "if", "params", ".", "bidirectional", ":", "\n", "            ", "training_data", "=", "dataset", ".", "batches_bidirectional", "(", "'training_docnade'", ",", "params", ".", "batch_size", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", "\n", "", "else", ":", "\n", "            ", "training_data", "=", "dataset", ".", "batches", "(", "'training_docnade'", ",", "params", ".", "batch_size", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", "\n", "\n", "# Global variables for best training, validation and test scores", "\n", "", "best_val_IR", "=", "0.0", "\n", "best_val_nll", "=", "np", ".", "inf", "\n", "best_val_ppl", "=", "np", ".", "inf", "\n", "best_val_disc_accuracy", "=", "0.0", "\n", "\n", "best_test_IR", "=", "0.0", "\n", "best_test_nll", "=", "np", ".", "inf", "\n", "best_test_ppl", "=", "np", ".", "inf", "\n", "best_test_disc_accuracy", "=", "0.0", "\n", "\n", "# Patience is set differently for DocNADE and iDocNADE", "\n", "#if params.bidirectional or params.initialize_docnade:", "\n", "#    patience = 30", "\n", "#else:", "\n", "#    patience = params.patience", "\n", "patience", "=", "params", ".", "patience", "\n", "\n", "patience_count", "=", "0", "\n", "#patience_count_ir = 0", "\n", "best_train_nll", "=", "np", ".", "inf", "\n", "\n", "# Loading labels for Information Retrieval (IR)", "\n", "training_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'training_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "validation_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'validation_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "test_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'test_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "\n", "# Start of training", "\n", "print", "(", "\"Training started.\\n\"", ")", "\n", "\n", "for", "step", "in", "range", "(", "params", ".", "num_steps", "+", "1", ")", ":", "\n", "            ", "this_loss", "=", "-", "1.", "\n", "\n", "if", "params", ".", "bidirectional", ":", "\n", "# Getting data batch by batch", "\n", "                ", "y", ",", "x", ",", "x_bw", ",", "seq_lengths", "=", "next", "(", "training_data", ")", "\n", "\n", "_", ",", "loss_normed", ",", "loss_unnormed", ",", "loss_normed_bw", ",", "loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", ",", "\n", "model", ".", "loss_normed_bw", ",", "model", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "x_bw", ":", "x_bw", ",", "\n", "model", ".", "y", ":", "y", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "\n", "this_loss", "=", "0.5", "*", "(", "loss_unnormed", "+", "loss_unnormed_bw", ")", "\n", "losses", ".", "append", "(", "this_loss", ")", "\n", "", "else", ":", "\n", "# Getting data batch by batch", "\n", "                ", "y", ",", "x", ",", "seq_lengths", "=", "next", "(", "training_data", ")", "\n", "\n", "_", ",", "loss", ",", "loss_unnormed", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "y", ":", "y", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "\n", "this_loss", "=", "loss", "\n", "losses", ".", "append", "(", "this_loss", ")", "\n", "\n", "# Printing training loss", "\n", "", "if", "(", "step", "%", "params", ".", "log_every", "==", "0", ")", ":", "\n", "                ", "print", "(", "'{}: {:.6f}'", ".", "format", "(", "step", ",", "this_loss", ")", ")", "\n", "\n", "\n", "# Calculating PPL for validation set as per \"params.validation_ppl_freq\" parameter", "\n", "", "if", "step", "and", "(", "step", "%", "params", ".", "validation_ppl_freq", ")", "==", "0", ":", "\n", "# val_loss_unnormed is for Negative Log Likelihood (NLL)", "\n", "# val_loss_normed is for Perplexity (PPL)", "\n", "\n", "                ", "this_val_nll", "=", "[", "]", "\n", "this_val_loss_normed", "=", "[", "]", "\n", "this_val_nll_bw", "=", "[", "]", "\n", "this_val_loss_normed_bw", "=", "[", "]", "\n", "\n", "if", "params", ".", "bidirectional", ":", "\n", "# Getting iDocNADE validation data as per validation batch size parameter \"params.validation_bs\"", "\n", "                    ", "for", "val_y", ",", "val_x", ",", "val_x_bw", ",", "val_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'validation_docnade'", ",", "params", ".", "validation_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "val_loss_normed", ",", "val_loss_unnormed", ",", "val_loss_normed_bw", ",", "val_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", ",", "\n", "model", ".", "loss_normed_bw", ",", "model", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "val_x", ",", "\n", "model", ".", "x_bw", ":", "val_x_bw", ",", "\n", "model", ".", "y", ":", "val_y", ",", "\n", "model", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "this_val_nll_bw", ".", "append", "(", "val_loss_unnormed_bw", ")", "\n", "this_val_loss_normed_bw", ".", "append", "(", "val_loss_normed_bw", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE validation data as per validation batch size parameter \"params.validation_bs\"", "\n", "                    ", "for", "val_y", ",", "val_x", ",", "val_seq_lengths", "in", "dataset", ".", "batches", "(", "'validation_docnade'", ",", "params", ".", "validation_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "val_loss_normed", ",", "val_loss_unnormed", "=", "session", ".", "run", "(", "[", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "val_x", ",", "\n", "model", ".", "y", ":", "val_y", ",", "\n", "model", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "\n", "# Calculating PPL and NLL on full validation set", "\n", "", "", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "total_val_nll", "=", "0.5", "*", "(", "np", ".", "mean", "(", "this_val_nll", ")", "+", "np", ".", "mean", "(", "this_val_nll_bw", ")", ")", "\n", "total_val_ppl", "=", "0.5", "*", "(", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "+", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed_bw", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "total_val_nll", "=", "np", ".", "mean", "(", "this_val_nll", ")", "\n", "total_val_ppl", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "\n", "\n", "# Only saving latest best model", "\n", "", "if", "total_val_ppl", "<", "best_val_ppl", ":", "\n", "                    ", "best_val_ppl", "=", "total_val_ppl", "\n", "print", "(", "'saving: {}'", ".", "format", "(", "model_dir_ppl", ")", ")", "\n", "saver", ".", "save", "(", "session", ",", "model_dir_ppl", "+", "'/model_ppl'", ",", "global_step", "=", "1", ")", "\n", "\n", "# Early stopping", "\n", "", "if", "total_val_nll", "<", "best_val_nll", ":", "\n", "                    ", "best_val_nll", "=", "total_val_nll", "\n", "patience_count", "=", "0", "\n", "", "else", ":", "\n", "                    ", "patience_count", "+=", "1", "\n", "\n", "# Print validation PPL and IR statistics", "\n", "", "print", "(", "'This val PPL: {:.3f} (best val PPL: {:.3f},  best val loss: {:.3f})'", ".", "format", "(", "\n", "total_val_ppl", ",", "\n", "best_val_ppl", "or", "0.0", ",", "\n", "best_val_nll", "\n", ")", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"training_info.txt\"", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"Step: %i,    val PPL: %s,     best val PPL: %s,    best val loss: %s\\n\"", "%", "\n", "(", "step", ",", "total_val_ppl", ",", "best_val_ppl", ",", "best_val_nll", ")", ")", "\n", "\n", "", "if", "patience_count", ">", "patience", ":", "\n", "                    ", "print", "(", "\"Early stopping criterion satisfied.\"", ")", "\n", "break", "\n", "\n", "# Calculating IR for validation set as per \"params.validation_ir_freq\" parameter", "\n", "", "", "if", "step", ">=", "1", "and", "step", "%", "params", ".", "validation_ir_freq", "==", "0", ":", "\n", "\n", "                ", "this_val_nll", "=", "[", "]", "\n", "this_val_loss_normed", "=", "[", "]", "\n", "this_val_nll_bw", "=", "[", "]", "\n", "this_val_loss_normed_bw", "=", "[", "]", "\n", "\n", "if", "params", ".", "bidirectional", ":", "\n", "# Getting iDocNADE validation data as per validation batch size parameter \"params.validation_bs\"", "\n", "                    ", "for", "val_y", ",", "val_x", ",", "val_x_bw", ",", "val_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'validation_docnade'", ",", "params", ".", "validation_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "_", ",", "val_loss_normed", ",", "val_loss_unnormed", ",", "val_loss_normed_bw", ",", "val_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", ",", "\n", "model", ".", "loss_normed_bw", ",", "model", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "val_x", ",", "\n", "model", ".", "x_bw", ":", "val_x_bw", ",", "\n", "model", ".", "y", ":", "val_y", ",", "\n", "model", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "this_val_nll_bw", ".", "append", "(", "val_loss_unnormed_bw", ")", "\n", "this_val_loss_normed_bw", ".", "append", "(", "val_loss_normed_bw", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE validation data as per validation batch size parameter \"params.validation_bs\"", "\n", "                    ", "for", "val_y", ",", "val_x", ",", "val_seq_lengths", "in", "dataset", ".", "batches", "(", "'validation_docnade'", ",", "params", ".", "validation_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "_", ",", "val_loss_normed", ",", "val_loss_unnormed", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "val_x", ",", "\n", "model", ".", "y", ":", "val_y", ",", "\n", "model", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "\n", "# Calculating PPL and NLL on full validation set", "\n", "", "", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "total_val_nll", "=", "0.5", "*", "(", "np", ".", "mean", "(", "this_val_nll", ")", "+", "np", ".", "mean", "(", "this_val_nll_bw", ")", ")", "\n", "total_val_ppl", "=", "0.5", "*", "(", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "+", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed_bw", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "total_val_nll", "=", "np", ".", "mean", "(", "this_val_nll", ")", "\n", "total_val_ppl", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "\n", "\n", "# Only saving latest best model", "\n", "", "if", "total_val_ppl", "<", "best_val_ppl", ":", "\n", "                    ", "best_val_ppl", "=", "total_val_ppl", "\n", "\n", "# Early stopping", "\n", "", "if", "total_val_nll", "<", "best_val_nll", ":", "\n", "                    ", "best_val_nll", "=", "total_val_nll", "\n", "patience_count", "=", "0", "\n", "", "else", ":", "\n", "                    ", "patience_count", "+=", "1", "\n", "\n", "# Print validation PPL and IR statistics", "\n", "", "print", "(", "'This val PPL: {:.3f} (best val PPL: {:.3f},  best val loss: {:.3f})'", ".", "format", "(", "\n", "total_val_ppl", ",", "\n", "best_val_ppl", "or", "0.0", ",", "\n", "best_val_nll", "\n", ")", ")", "\n", "\n", "# Getting hidden vectors using iDocNADE/DocNADE model for all documents in validation set", "\n", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "validation_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'validation_docnade'", ",", "\n", "params", ".", "validation_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", ",", "\n", "params", ".", "combination_type", "\n", ")", "\n", "\n", "training_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'training_docnade'", ",", "\n", "params", ".", "validation_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", ",", "\n", "params", ".", "combination_type", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "validation_vectors", "=", "m", ".", "vectors", "(", "\n", "model", ",", "\n", "dataset", ".", "batches", "(", "\n", "'validation_docnade'", ",", "\n", "params", ".", "validation_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "training_vectors", "=", "m", ".", "vectors", "(", "\n", "model", ",", "\n", "dataset", ".", "batches", "(", "\n", "'training_docnade'", ",", "\n", "params", ".", "validation_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "# Calculating validation IR using \"evaluate.py\" in \"model\" directory", "\n", "", "val", "=", "eval", ".", "evaluate", "(", "\n", "training_vectors", ",", "\n", "validation_vectors", ",", "\n", "training_labels", ",", "\n", "validation_labels", ",", "\n", "recall", "=", "[", "0.02", "]", ",", "\n", "num_classes", "=", "params", ".", "num_classes", ",", "\n", "multi_label", "=", "params", ".", "multi_label", "\n", ")", "[", "0", "]", "\n", "\n", "# Saving best IR model", "\n", "if", "val", ">", "best_val_IR", ":", "\n", "                    ", "best_val_IR", "=", "val", "\n", "print", "(", "'saving: {}'", ".", "format", "(", "model_dir_ir", ")", ")", "\n", "saver", ".", "save", "(", "session", ",", "model_dir_ir", "+", "'/model_ir'", ",", "global_step", "=", "1", ")", "\n", "#    patience_count = 0", "\n", "#else:", "\n", "#    patience_count += 1", "\n", "\n", "", "print", "(", "'This val IR: {:.3f} (best val IR: {:.3f})'", ".", "format", "(", "\n", "val", ",", "\n", "best_val_IR", "or", "0.0", "\n", ")", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"training_info.txt\"", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"Step: %i,    val IR: %s,    best val IR: %s\\n\"", "%", "\n", "(", "step", ",", "val", ",", "best_val_IR", ")", ")", "\n", "\n", "# Saving summaries", "\n", "", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "summary", ",", "=", "session", ".", "run", "(", "[", "summaries", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "x_bw", ":", "x_bw", ",", "\n", "model", ".", "y", ":", "y", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", ",", "\n", "validation", ":", "val", ",", "\n", "validation_accuracy", ":", "0.0", ",", "\n", "avg_loss", ":", "np", ".", "average", "(", "losses", ")", "\n", "}", ")", "\n", "", "else", ":", "\n", "                    ", "summary", ",", "=", "session", ".", "run", "(", "[", "summaries", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "y", ":", "y", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", ",", "\n", "validation", ":", "val", ",", "\n", "validation_accuracy", ":", "0.0", ",", "\n", "avg_loss", ":", "np", ".", "average", "(", "losses", ")", "\n", "}", ")", "\n", "", "summary_writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "summary_writer", ".", "flush", "(", ")", "\n", "losses", "=", "[", "]", "\n", "\n", "if", "patience_count", ">", "patience", ":", "\n", "                    ", "print", "(", "\"Early stopping criterion satisfied.\"", ")", "\n", "break", "\n", "\n", "# Calculating PPL for test set as per \"params.test_ppl_freq\" parameter", "\n", "", "", "if", "step", "and", "(", "step", "%", "params", ".", "test_ppl_freq", ")", "==", "0", ":", "\n", "# test_loss_unnormed is for Negative Log Likelihood (NLL)", "\n", "# test_loss_normed is for Perplexity (PPL)", "\n", "\n", "                ", "this_test_nll", "=", "[", "]", "\n", "this_test_loss_normed", "=", "[", "]", "\n", "this_test_nll_bw", "=", "[", "]", "\n", "this_test_loss_normed_bw", "=", "[", "]", "\n", "\n", "if", "params", ".", "bidirectional", ":", "\n", "# Getting iDocNADE test set as per test batch size parameter \"params.test_bs\"", "\n", "                    ", "for", "test_y", ",", "test_x", ",", "test_x_bw", ",", "test_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'test_docnade'", ",", "params", ".", "test_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "test_loss_normed", ",", "test_loss_unnormed", ",", "test_loss_normed_bw", ",", "test_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", ",", "\n", "model", ".", "loss_normed_bw", ",", "model", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "test_x", ",", "\n", "model", ".", "x_bw", ":", "test_x_bw", ",", "\n", "model", ".", "y", ":", "test_y", ",", "\n", "model", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "this_test_nll", ".", "append", "(", "test_loss_unnormed", ")", "\n", "this_test_loss_normed", ".", "append", "(", "test_loss_normed", ")", "\n", "this_test_nll_bw", ".", "append", "(", "test_loss_unnormed_bw", ")", "\n", "this_test_loss_normed_bw", ".", "append", "(", "test_loss_normed_bw", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE test set as per test batch size parameter \"params.test_bs\"", "\n", "                    ", "for", "test_y", ",", "test_x", ",", "test_seq_lengths", "in", "dataset", ".", "batches", "(", "'test_docnade'", ",", "params", ".", "test_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "test_loss_normed", ",", "test_loss_unnormed", "=", "session", ".", "run", "(", "[", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "test_x", ",", "\n", "model", ".", "y", ":", "test_y", ",", "\n", "model", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "this_test_nll", ".", "append", "(", "test_loss_unnormed", ")", "\n", "this_test_loss_normed", ".", "append", "(", "test_loss_normed", ")", "\n", "\n", "# Calculating PPl and NLL on full test set", "\n", "", "", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "total_test_nll", "=", "0.5", "*", "(", "np", ".", "mean", "(", "this_test_nll", ")", "+", "np", ".", "mean", "(", "this_test_nll_bw", ")", ")", "\n", "total_test_ppl", "=", "0.5", "*", "(", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed", ")", ")", "+", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed_bw", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "total_test_nll", "=", "np", ".", "mean", "(", "this_test_nll", ")", "\n", "total_test_ppl", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed", ")", ")", "\n", "\n", "# Saving best test set values", "\n", "", "if", "total_test_ppl", "<", "best_test_ppl", ":", "\n", "                    ", "best_test_ppl", "=", "total_test_ppl", "\n", "\n", "", "if", "total_test_nll", "<", "best_test_nll", ":", "\n", "                    ", "best_test_nll", "=", "total_test_nll", "\n", "\n", "", "print", "(", "'This test PPL: {:.3f} (best test PPL: {:.3f},  best test loss: {:.3f})'", ".", "format", "(", "\n", "total_test_ppl", ",", "\n", "best_test_ppl", "or", "0.0", ",", "\n", "best_test_nll", "\n", ")", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"training_info.txt\"", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"Step: %i,    test PPL: %s,    best test PPL: %s,    best test loss: %s\\n\"", "%", "\n", "(", "step", ",", "total_test_ppl", ",", "best_test_ppl", ",", "best_test_nll", ")", ")", "\n", "\n", "# Calculating IR for test set as per \"params.test_ir_freq\" parameter", "\n", "", "", "if", "step", ">=", "1", "and", "(", "step", "%", "params", ".", "test_ir_freq", ")", "==", "0", ":", "\n", "\n", "                ", "if", "params", ".", "bidirectional", ":", "\n", "# Getting iDocNADE test set as per test batch size parameter \"params.test_bs\"", "\n", "                    ", "for", "test_y", ",", "test_x", ",", "test_x_bw", ",", "test_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'test_docnade'", ",", "params", ".", "test_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "_", ",", "test_loss_normed", ",", "test_loss_unnormed", ",", "test_loss_normed_bw", ",", "test_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", ",", "\n", "model", ".", "loss_normed_bw", ",", "model", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "test_x", ",", "\n", "model", ".", "x_bw", ":", "test_x_bw", ",", "\n", "model", ".", "y", ":", "test_y", ",", "\n", "model", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE test set as per test batch size parameter \"params.test_bs\"", "\n", "                    ", "for", "test_y", ",", "test_x", ",", "test_seq_lengths", "in", "dataset", ".", "batches", "(", "'test_docnade'", ",", "params", ".", "test_bs", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", ".", "multi_label", ")", ":", "\n", "                        ", "_", ",", "test_loss_normed", ",", "test_loss_unnormed", "=", "session", ".", "run", "(", "[", "model", ".", "opt", ",", "model", ".", "loss_normed", ",", "model", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "test_x", ",", "\n", "model", ".", "y", ":", "test_y", ",", "\n", "model", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "\n", "# Getting hidden vectors using iDocNADE/DocNADE model for all documents in test set", "\n", "", "", "if", "params", ".", "bidirectional", ":", "\n", "                    ", "test_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'test_docnade'", ",", "\n", "params", ".", "test_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", ",", "\n", "params", ".", "combination_type", "\n", ")", "\n", "\n", "training_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'training_docnade'", ",", "\n", "params", ".", "test_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", ",", "\n", "params", ".", "combination_type", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "test_vectors", "=", "m", ".", "vectors", "(", "\n", "model", ",", "\n", "dataset", ".", "batches", "(", "\n", "'test_docnade'", ",", "\n", "params", ".", "test_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "training_vectors", "=", "m", ".", "vectors", "(", "\n", "model", ",", "\n", "dataset", ".", "batches", "(", "\n", "'training_docnade'", ",", "\n", "params", ".", "test_bs", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", ".", "multi_label", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "# Calculating test IR using \"evaluate.py\" in \"model\" directory", "\n", "", "test", "=", "eval", ".", "evaluate", "(", "\n", "training_vectors", ",", "\n", "test_vectors", ",", "\n", "training_labels", ",", "\n", "test_labels", ",", "\n", "recall", "=", "[", "0.02", "]", ",", "\n", "num_classes", "=", "params", ".", "num_classes", ",", "\n", "multi_label", "=", "params", ".", "multi_label", "\n", ")", "[", "0", "]", "\n", "\n", "# Saving and printing best test IR value", "\n", "if", "test", ">", "best_test_IR", ":", "\n", "                    ", "best_test_IR", "=", "test", "\n", "\n", "", "print", "(", "'This test IR: {:.3f} (best test IR: {:.3f})'", ".", "format", "(", "\n", "test", ",", "\n", "best_test_IR", "or", "0.0", "\n", ")", ")", "\n", "\n", "", "", "print", "(", "\"Training finished.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.compute_coherence": [[574, 641], ["gensim.corpora.dictionary.Dictionary", "print", "print", "gensim.corpora.dictionary.Dictionary.doc2bow", "open", "len", "print", "print", "f.write", "f.write", "print", "print", "f.write", "f.write", "print", "print", "f.write", "gensim.models.CoherenceModel().get_coherence", "avg_coh_scores_dict[].append", "print", "f.write", "print", "numpy.mean", "numpy.mean", "gensim.models.CoherenceModel"], "function", ["None"], ["def", "compute_coherence", "(", "texts", ",", "list_of_topics", ",", "top_n_word_in_each_topic_list", ",", "reload_model_dir", ")", ":", "\n", "    ", "'''\n    Function to compute Topic Coherence based on different types available.\n\n    list_of_topics:                 list of list of topic words\n    top_n_word_in_each_topic_list:  list of number of words to count from beginning\n                                    to calculate topic coherence\n    reload_model_dir:               model directory created when running the experiments\n    '''", "\n", "\n", "dictionary", "=", "Dictionary", "(", "texts", ")", "\n", "corpus", "=", "[", "dictionary", ".", "doc2bow", "(", "text", ")", "for", "text", "in", "texts", "]", "\n", "\n", "print", "(", "'corpus len:%s'", "%", "len", "(", "corpus", ")", ")", "\n", "print", "(", "'dictionary:%s'", "%", "dictionary", ")", "\n", "# https://github.com/earthquakesan/palmetto-py", "\n", "# compute_topic_coherence: PMI and other coherence types", "\n", "# from palmettopy.palmetto import Palmetto", "\n", "# palmetto = Palmetto()", "\n", "\n", "# coherence_types = [\"ca\", \"cp\", \"cv\", \"npmi\", \"uci\", \"umass\"] # for palmetto library", "\n", "coherence_types", "=", "[", "\"c_v\"", "]", "#, 'u_mass', 'c_v', 'c_uci', 'c_npmi'] # [\"c_v\"] # 'u_mass', 'c_v', 'c_uci', 'c_npmi',", "\n", "avg_coh_scores_dict", "=", "{", "}", "\n", "\n", "best_coh_type_value_topci_indx", "=", "{", "}", "\n", "for", "top_n", "in", "top_n_word_in_each_topic_list", ":", "\n", "        ", "avg_coh_scores_dict", "[", "top_n", "]", "=", "[", "]", "\n", "best_coh_type_value_topci_indx", "[", "top_n", "]", "=", "[", "0", ",", "0", ",", "[", "]", "]", "# score, topic_indx, topics words", "\n", "\n", "", "h_num", "=", "0", "\n", "with", "open", "(", "reload_model_dir", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "topic_words_all", "in", "list_of_topics", ":", "\n", "            ", "h_num", "+=", "1", "\n", "for", "top_n", "in", "top_n_word_in_each_topic_list", ":", "\n", "                ", "topic_words", "=", "[", "topic_words_all", "[", ":", "top_n", "]", "]", "\n", "for", "coh_type", "in", "coherence_types", ":", "\n", "                    ", "try", ":", "\n", "                        ", "print", "(", "'top_n: %s Topic Num: %s \\nTopic Words: %s'", "%", "(", "top_n", ",", "h_num", ",", "topic_words", ")", ")", "\n", "f", ".", "write", "(", "'top_n: %s Topic Num: %s \\nTopic Words: %s\\n'", "%", "(", "top_n", ",", "h_num", ",", "topic_words", ")", ")", "\n", "# print('topic_words_top_10_abs[%s]:%s' % (h_num, topic_words_top_10_abs[h_num]))", "\n", "# PMI = palmetto.get_coherence(topic_words_top_10[h_num], coherence_type=coh_type)", "\n", "PMI", "=", "CoherenceModel", "(", "topics", "=", "topic_words", ",", "texts", "=", "texts", ",", "dictionary", "=", "dictionary", ",", "coherence", "=", "coh_type", ",", "processes", "=", "2", ")", ".", "get_coherence", "(", ")", "\n", "\n", "avg_coh_scores_dict", "[", "top_n", "]", ".", "append", "(", "PMI", ")", "\n", "\n", "if", "PMI", ">", "best_coh_type_value_topci_indx", "[", "top_n", "]", "[", "0", "]", ":", "\n", "                            ", "best_coh_type_value_topci_indx", "[", "top_n", "]", "=", "[", "PMI", ",", "top_n", ",", "topic_words", "]", "\n", "\n", "", "print", "(", "'Coh_type:%s  Topic Num:%s COH score:%s'", "%", "(", "coh_type", ",", "h_num", ",", "PMI", ")", ")", "\n", "f", ".", "write", "(", "'Coh_type:%s  Topic Num:%s COH score:%s\\n'", "%", "(", "coh_type", ",", "h_num", ",", "PMI", ")", ")", "\n", "\n", "print", "(", "'--------------------------------------------------------------'", ")", "\n", "", "except", ":", "\n", "                        ", "continue", "\n", "", "", "print", "(", "'========================================================================================================'", ")", "\n", "\n", "", "", "for", "top_n", "in", "top_n_word_in_each_topic_list", ":", "\n", "            ", "print", "(", "'top scores for top_%s:%s'", "%", "(", "top_n", ",", "best_coh_type_value_topci_indx", "[", "top_n", "]", ")", ")", "\n", "print", "(", "'-------------------------------------------------------------------'", ")", "\n", "f", ".", "write", "(", "'top scores for top_%s:%s\\n'", "%", "(", "top_n", ",", "best_coh_type_value_topci_indx", "[", "top_n", "]", ")", ")", "\n", "f", ".", "write", "(", "'-------------------------------------------------------------------\\n'", ")", "\n", "\n", "", "for", "top_n", "in", "top_n_word_in_each_topic_list", ":", "\n", "            ", "print", "(", "'Avg COH for top_%s topic words: %s'", "%", "(", "top_n", ",", "np", ".", "mean", "(", "avg_coh_scores_dict", "[", "top_n", "]", ")", ")", ")", "\n", "print", "(", "'-------------------------------------------------------------------'", ")", "\n", "f", ".", "write", "(", "'Avg COH for top_%s topic words: %s\\n'", "%", "(", "top_n", ",", "np", ".", "mean", "(", "avg_coh_scores_dict", "[", "top_n", "]", ")", ")", ")", "\n", "f", ".", "write", "(", "'-------------------------------------------------------------------\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.get_vectors_from_matrix": [[643, 660], ["numpy.array", "numpy.zeros", "vecs.append"], "function", ["None"], ["", "", "", "def", "get_vectors_from_matrix", "(", "matrix", ",", "batches", ")", ":", "\n", "    ", "'''\n    Function to get document representation vectors using embedding\n    matrix of iDocNADE/DocNADE\n\n    matrix: embedding matrix of shape = [vocab_size X embedding_size]\n    batches: instance data.batches/data.batches_bidirectional function\n    '''", "\n", "\n", "vecs", "=", "[", "]", "\n", "for", "_", ",", "x", ",", "seq_length", "in", "batches", ":", "\n", "        ", "temp_vec", "=", "np", ".", "zeros", "(", "(", "matrix", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "indices", "=", "x", "[", "0", ",", ":", "seq_length", "[", "0", "]", "]", "\n", "for", "index", "in", "indices", ":", "\n", "            ", "temp_vec", "+=", "matrix", "[", "index", ",", ":", "]", "\n", "", "vecs", ".", "append", "(", "temp_vec", ")", "\n", "", "return", "np", ".", "array", "(", "vecs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.softmax": [[662, 704], ["numpy.atleast_2d", "numpy.exp", "numpy.expand_dims", "next", "float", "numpy.expand_dims", "numpy.sum", "len", "p.flatten.flatten", "numpy.max", "enumerate"], "function", ["None"], ["", "def", "softmax", "(", "X", ",", "theta", "=", "1.0", ",", "axis", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Compute the softmax of each element along an axis of X.\n\n    Parameters\n    ----------\n    X: ND-Array. Probably should be floats. \n    theta (optional): float parameter, used as a multiplier\n        prior to exponentiation. Default = 1.0\n    axis (optional): axis to compute values along. Default is the \n        first non-singleton axis.\n\n    Returns an array the same size as X. The result will sum to 1\n    along the specified axis.\n    \"\"\"", "\n", "\n", "# make X at least 2d", "\n", "y", "=", "np", ".", "atleast_2d", "(", "X", ")", "\n", "\n", "# find axis", "\n", "if", "axis", "is", "None", ":", "\n", "        ", "axis", "=", "next", "(", "j", "[", "0", "]", "for", "j", "in", "enumerate", "(", "y", ".", "shape", ")", "if", "j", "[", "1", "]", ">", "1", ")", "\n", "\n", "# multiply y against the theta parameter, ", "\n", "", "y", "=", "y", "*", "float", "(", "theta", ")", "\n", "\n", "# subtract the max for numerical stability", "\n", "y", "=", "y", "-", "np", ".", "expand_dims", "(", "np", ".", "max", "(", "y", ",", "axis", "=", "axis", ")", ",", "axis", ")", "\n", "\n", "# exponentiate y", "\n", "y", "=", "np", ".", "exp", "(", "y", ")", "\n", "\n", "# take the sum along the specified axis", "\n", "ax_sum", "=", "np", ".", "expand_dims", "(", "np", ".", "sum", "(", "y", ",", "axis", "=", "axis", ")", ",", "axis", ")", "\n", "\n", "# finally: divide elementwise", "\n", "p", "=", "y", "/", "ax_sum", "\n", "\n", "# flatten if X was 1D", "\n", "if", "len", "(", "X", ".", "shape", ")", "==", "1", ":", "p", "=", "p", ".", "flatten", "(", ")", "\n", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.square_rooted": [[706, 713], ["round", "sqrt", "sum"], "function", ["None"], ["", "def", "square_rooted", "(", "x", ")", ":", "\n", "    ", "'''\n    Function to calculate L2 norm of a vector\n\n    x: a vector or a list\n    '''", "\n", "return", "round", "(", "sqrt", "(", "sum", "(", "[", "a", "*", "a", "for", "a", "in", "x", "]", ")", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.cosine_similarity": [[715, 725], ["sum", "round", "train_model.square_rooted", "train_model.square_rooted", "float", "zip"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.square_rooted", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.square_rooted"], ["", "def", "cosine_similarity", "(", "x", ",", "y", ")", ":", "\n", "    ", "'''\n    Function to calculate cosine similarity between x and y.\n\n    x: a vector or a list\n    y: a vector or a list\n    '''", "\n", "numerator", "=", "sum", "(", "a", "*", "b", "for", "a", ",", "b", "in", "zip", "(", "x", ",", "y", ")", ")", "\n", "denominator", "=", "square_rooted", "(", "x", ")", "*", "square_rooted", "(", "y", ")", "\n", "return", "round", "(", "numerator", "/", "float", "(", "denominator", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.reload_evaluation_ir": [[727, 1011], ["tensorflow.Session", "os.path.join", "numpy.array", "numpy.array", "numpy.array", "tensorflow.local_variables_initializer().run", "tensorflow.global_variables_initializer().run", "model.evaluate", "print", "model.evaluate", "print", "model.vectors_bidirectional", "model.vectors_bidirectional", "model.vectors_bidirectional", "model.vectors", "model.vectors", "model.vectors", "open", "f.write", "f.write", "open", "f.write", "f.write", "tensorflow.ConfigProto", "tensorflow.local_variables_initializer", "tensorflow.global_variables_initializer", "dataset.batches_bidirectional", "dataset.batches_bidirectional", "dataset.batches_bidirectional", "dataset.batches", "dataset.batches", "dataset.batches", "os.path.join", "os.path.join", "dataset.rows", "dataset.rows", "dataset.rows", "tensorflow.GPUOptions"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows"], ["", "def", "reload_evaluation_ir", "(", "model_ir", ",", "dataset", ",", "params", ")", ":", "\n", "    ", "'''\n    Main function which does evaluation on reloaded model.\n\n    model_ir:  iDocNADE/DocNADE model instance with weights/biases from best IR model\n    datset:    an instance of data object\n    params:    parameters saved during training of the reloaded model\n    '''", "\n", "with", "tf", ".", "Session", "(", "config", "=", "tf", ".", "ConfigProto", "(", "\n", "inter_op_parallelism_threads", "=", "params", "[", "'num_cores'", "]", ",", "\n", "intra_op_parallelism_threads", "=", "params", "[", "'num_cores'", "]", ",", "\n", "gpu_options", "=", "tf", ".", "GPUOptions", "(", "allow_growth", "=", "True", ")", "\n", ")", ")", "as", "session", ":", "\n", "        ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "params", "[", "'model'", "]", ",", "'logs'", ")", "\n", "\n", "# List of ratios on which IR has to be calculated", "\n", "ir_ratio_list", "=", "[", "0.0001", ",", "0.0005", ",", "0.001", ",", "0.002", ",", "0.005", ",", "0.01", ",", "0.02", ",", "0.05", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.5", ",", "0.8", ",", "1.0", "]", "\n", "\n", "# List of \"reciprocal of regularization strength\" to be used ", "\n", "# in classification using LogiticRegression module of sklearn", "\n", "c_list", "=", "[", "0.0001", ",", "0.001", ",", "0.01", ",", "0.1", ",", "0.5", ",", "1.0", ",", "3.0", ",", "5.0", ",", "10.0", ",", "100.0", ",", "500.0", ",", "1000.0", ",", "10000.0", "]", "\n", "\n", "# Loading labels for IR and Classification", "\n", "training_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'training_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "validation_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'validation_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "test_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'test_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "\n", "tf", ".", "local_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "tf", ".", "global_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "\n", "# Calculating hidden vector representation for training, validation and test documents", "\n", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "            ", "training_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'training_docnade'", ",", "\n", "params", "[", "'validation_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", ",", "\n", "params", "[", "'combination_type'", "]", "\n", ")", "\n", "\n", "validation_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'validation_docnade'", ",", "\n", "params", "[", "'validation_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", ",", "\n", "params", "[", "'combination_type'", "]", "\n", ")", "\n", "\n", "test_vectors", "=", "m", ".", "vectors_bidirectional", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches_bidirectional", "(", "\n", "'test_docnade'", ",", "\n", "params", "[", "'test_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", ",", "\n", "params", "[", "'combination_type'", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "training_vectors", "=", "m", ".", "vectors", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches", "(", "\n", "'training_docnade'", ",", "\n", "params", "[", "'validation_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "validation_vectors", "=", "m", ".", "vectors", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches", "(", "\n", "'validation_docnade'", ",", "\n", "params", "[", "'validation_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "test_vectors", "=", "m", ".", "vectors", "(", "\n", "model_ir", ",", "\n", "dataset", ".", "batches", "(", "\n", "'test_docnade'", ",", "\n", "params", "[", "'test_bs'", "]", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "multilabel", "=", "params", "[", "'multi_label'", "]", "\n", ")", ",", "\n", "session", "\n", ")", "\n", "\n", "\n", "\n", "# Calculating IR for validation set using \"evaluate.py\" in \"model\" directory", "\n", "", "val_list", "=", "eval", ".", "evaluate", "(", "\n", "training_vectors", ",", "\n", "validation_vectors", ",", "\n", "training_labels", ",", "\n", "validation_labels", ",", "\n", "recall", "=", "ir_ratio_list", ",", "\n", "num_classes", "=", "params", "[", "'num_classes'", "]", ",", "\n", "multi_label", "=", "params", "[", "'multi_label'", "]", "\n", ")", "\n", "\n", "print", "(", "'Val IR: '", ",", "val_list", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"reload_info_ir.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\\n\\nFractions list: %s\"", "%", "(", "ir_ratio_list", ")", ")", "\n", "f", ".", "write", "(", "\"\\nVal IR: %s\"", "%", "(", "val_list", ")", ")", "\n", "\n", "\n", "\n", "# Calculating IR for test set using \"evaluate.py\" in \"model\" directory", "\n", "", "test_list", "=", "eval", ".", "evaluate", "(", "\n", "training_vectors", ",", "\n", "test_vectors", ",", "\n", "training_labels", ",", "\n", "test_labels", ",", "\n", "recall", "=", "ir_ratio_list", ",", "\n", "num_classes", "=", "params", "[", "'num_classes'", "]", ",", "\n", "multi_label", "=", "params", "[", "'multi_label'", "]", "\n", ")", "\n", "\n", "print", "(", "'Test IR: '", ",", "test_list", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"reload_info_ir.txt\"", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\\n\\nFractions list: %s\"", "%", "(", "ir_ratio_list", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\\nTest IR: %s\"", "%", "(", "test_list", ")", ")", "\n", "\n", "", "\"\"\"\n        # Document classification using LogisticRegression from scikit-learn\n        test_acc = []\n        test_f1 = []\n        val_acc = []\n        val_f1 = []\n\n        test_acc_W = []\n        test_f1_W = []\n        val_acc_W = []\n        val_f1_W = []\n\n        # Loading train, validation and test set labels for classification\n        y_train = np.array(\n            [y for y, _ in dataset.rows('training_docnade', num_epochs=1)]\n        )\n        y_val = np.array(\n            [y for y, _ in dataset.rows('validation_docnade', num_epochs=1)]\n        )\n        y_test = np.array(\n            [y for y, _ in dataset.rows('test_docnade', num_epochs=1)]\n        )\n\n        # Getting document representation using embedding matrix of DocNADE/iDocNADE\n        training_vectors_W = get_vectors_from_matrix(model_ir.W, dataset.batches('training_docnade', 1, num_epochs=1, shuffle=False, multilabel=params['multi_label']))\n        validation_vectors_W = get_vectors_from_matrix(model_ir.W, dataset.batches('validation_docnade', 1, num_epochs=1, shuffle=False, multilabel=params['multi_label']))\n        test_vectors_W = get_vectors_from_matrix(model_ir.W, dataset.batches('test_docnade', 1, num_epochs=1, shuffle=False, multilabel=params['multi_label']))\n\n        if not params['multi_label']:\n            train_data = (training_vectors, np.array(y_train, dtype=np.int32))\n            validation_data = (validation_vectors, np.array(y_val, dtype=np.int32))\n            test_data = (test_vectors, np.array(y_test, dtype=np.int32))\n\n            # Performing classification\n            test_acc, test_f1, val_acc, val_f1 = eval.perform_classification(train_data, validation_data, test_data, c_list)\n            #test_acc, test_f1 = eval.perform_classification_test(train_data, test_data, c_list)\n\n            # logging information\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation accuracy with h vector IR: %s\" % (val_acc))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest accuracy with h vector IR: %s\" % (test_acc))\n\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation F1 score with h vector IR: %s\" % (val_f1))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest F1 score with h vector IR: %s\" % (test_f1))\n\n\n            train_data_W = (training_vectors_W, np.array(y_train, dtype=np.int32))\n            validation_data_W = (validation_vectors_W, np.array(y_val, dtype=np.int32))\n            test_data_W = (test_vectors_W, np.array(y_test, dtype=np.int32))\n\n            # Performing classification\n            test_acc_W, test_f1_W, val_acc_W, val_f1_W = eval.perform_classification(train_data_W, validation_data_W, test_data_W, c_list)\n            #test_acc_W, test_f1_W = eval.perform_classification_test(train_data_W, test_data_W, c_list)\n\n            # logging information\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation accuracy with W matrix IR: %s\" % (val_acc_W))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest accuracy with W matrix IR: %s\" % (test_acc_W))\n\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation F1 score with W matrix IR: %s\" % (val_f1_W))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest F1 score with W matrix IR: %s\" % (test_f1_W))\n            \n        else:\n            total_labels = []\n\n            # Creating one-hot labels for multi label dataset\n            y_train_new = [label.strip().split(':') for label in y_train]\n            y_val_new = [label.strip().split(':') for label in y_val]\n            y_test_new = [label.strip().split(':') for label in y_test]\n\n            total_labels.extend(y_train_new)\n\n            from sklearn.preprocessing import MultiLabelBinarizer\n            mlb = MultiLabelBinarizer()\n            mlb.fit(total_labels)\n            y_train_one_hot = mlb.transform(y_train_new)\n            y_val_one_hot = mlb.transform(y_val_new)\n            y_test_one_hot = mlb.transform(y_test_new)\n\n            train_data = (training_vectors, y_train_one_hot)\n            validation_data = (validation_vectors, y_val_one_hot)\n            test_data = (test_vectors, y_test_one_hot)\n\n            # Performing multi label classification\n            test_acc, test_f1, val_acc, val_f1 = eval.perform_classification_multi(train_data, validation_data, test_data, c_list)\n            #test_acc, test_f1 = eval.perform_classification_test_multi(train_data, test_data, c_list)\n\n            # logging information\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation accuracy with h vector IR: %s\" % (val_acc))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest accuracy with h vector IR: %s\" % (test_acc))\n\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation F1 score with h vector IR: %s\" % (val_f1))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest F1 score with h vector IR: %s\" % (test_f1))\n\n            train_data_W = (training_vectors_W, y_train_one_hot)\n            validation_data_W = (validation_vectors_W, y_val_one_hot)\n            test_data_W = (test_vectors_W, y_test_one_hot)\n\n            # Performing multi label classification\n            test_acc_W, test_f1_W, val_acc_W, val_f1_W = eval.perform_classification_multi(train_data_W, validation_data_W, test_data_W, c_list)\n            #test_acc_W, test_f1_W = eval.perform_classification_test_multi(train_data_W, test_data_W, c_list)\n\n            # logging information\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation accuracy with W matrix IR: %s\" % (val_acc_W))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest accuracy with W matrix IR: %s\" % (test_acc_W))\n\n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nValidation F1 score with W matrix IR: %s\" % (val_f1_W))\n            \n            with open(os.path.join(log_dir, \"reload_info_ir.txt\"), \"a\") as f:\n                f.write(\"\\n\\nTest F1 score with W matrix IR: %s\" % (test_f1_W))\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.reload_evaluation_ppl": [[1013, 1343], ["tensorflow.Session", "os.path.join", "numpy.array", "numpy.array", "numpy.array", "tensorflow.local_variables_initializer().run", "tensorflow.global_variables_initializer().run", "print", "print", "dataset.batches_bidirectional", "dataset.batches", "numpy.mean", "numpy.exp", "open", "f.write", "dataset.batches_bidirectional", "dataset.batches", "numpy.mean", "numpy.exp", "open", "f.write", "tensorflow.ConfigProto", "tensorflow.local_variables_initializer", "tensorflow.global_variables_initializer", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "this_val_nll_bw.append", "this_val_loss_normed_bw.append", "session.run", "this_val_nll.append", "this_val_loss_normed.append", "numpy.mean", "os.path.join", "session.run", "this_test_nll.append", "this_test_loss_normed.append", "this_test_nll_bw.append", "this_test_loss_normed_bw.append", "session.run", "this_test_nll.append", "this_test_loss_normed.append", "numpy.mean", "os.path.join", "dataset.rows", "dataset.rows", "dataset.rows", "numpy.mean", "numpy.mean", "numpy.exp", "numpy.exp", "numpy.mean", "numpy.mean", "numpy.exp", "numpy.exp", "tensorflow.GPUOptions", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows"], ["", "", "def", "reload_evaluation_ppl", "(", "model_ppl", ",", "dataset", ",", "params", ")", ":", "\n", "    ", "'''\n    Main function which does evaluation on reloaded model.\n\n    model_ppl: iDocNADE/DocNADE model instance with weights/biases from best PPL model\n    datset:    an instance of data object\n    params:    parameters saved during training of the reloaded model\n    '''", "\n", "with", "tf", ".", "Session", "(", "config", "=", "tf", ".", "ConfigProto", "(", "\n", "inter_op_parallelism_threads", "=", "params", "[", "'num_cores'", "]", ",", "\n", "intra_op_parallelism_threads", "=", "params", "[", "'num_cores'", "]", ",", "\n", "gpu_options", "=", "tf", ".", "GPUOptions", "(", "allow_growth", "=", "True", ")", "\n", ")", ")", "as", "session", ":", "\n", "        ", "log_dir", "=", "os", ".", "path", ".", "join", "(", "params", "[", "'model'", "]", ",", "'logs'", ")", "\n", "\n", "# List of ratios on which IR has to be calculated", "\n", "ir_ratio_list", "=", "[", "0.0001", ",", "0.0005", ",", "0.001", ",", "0.002", ",", "0.005", ",", "0.01", ",", "0.02", ",", "0.05", ",", "0.1", ",", "0.2", ",", "0.3", ",", "0.5", ",", "0.8", ",", "1.0", "]", "\n", "\n", "# List of \"reciprocal of regularization strength\" to be used ", "\n", "# in classification using LogiticRegression module of sklearn", "\n", "c_list", "=", "[", "0.0001", ",", "0.001", ",", "0.01", ",", "0.1", ",", "0.5", ",", "1.0", ",", "3.0", ",", "5.0", ",", "10.0", ",", "100.0", ",", "500.0", ",", "1000.0", ",", "10000.0", "]", "\n", "\n", "# Loading labels for IR and Classification", "\n", "training_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'training_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "validation_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'validation_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "test_labels", "=", "np", ".", "array", "(", "\n", "[", "[", "y", "]", "for", "y", ",", "_", "in", "dataset", ".", "rows", "(", "'test_docnade'", ",", "num_epochs", "=", "1", ")", "]", "\n", ")", "\n", "\n", "tf", ".", "local_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "tf", ".", "global_variables_initializer", "(", ")", ".", "run", "(", ")", "\n", "\n", "\n", "# Calculating PPL for validation set", "\n", "\n", "# val_loss_unnormed is for Negative Log Likelihood (NLL)", "\n", "# val_loss_normed is for Perplexity (PPL)", "\n", "\n", "this_val_nll", "=", "[", "]", "\n", "this_val_loss_normed", "=", "[", "]", "\n", "this_val_nll_bw", "=", "[", "]", "\n", "this_val_loss_normed_bw", "=", "[", "]", "\n", "\n", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "# Getting iDocNADE validation set", "\n", "            ", "for", "val_y", ",", "val_x", ",", "val_x_bw", ",", "val_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'validation_docnade'", ",", "params", "[", "'validation_bs'", "]", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", "[", "'multi_label'", "]", ")", ":", "\n", "                ", "val_loss_normed", ",", "val_loss_unnormed", ",", "val_loss_normed_bw", ",", "val_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model_ppl", ".", "loss_normed", ",", "model_ppl", ".", "loss_unnormed", ",", "\n", "model_ppl", ".", "loss_normed_bw", ",", "model_ppl", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model_ppl", ".", "x", ":", "val_x", ",", "\n", "model_ppl", ".", "x_bw", ":", "val_x_bw", ",", "\n", "model_ppl", ".", "y", ":", "val_y", ",", "\n", "model_ppl", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "this_val_nll_bw", ".", "append", "(", "val_loss_unnormed_bw", ")", "\n", "this_val_loss_normed_bw", ".", "append", "(", "val_loss_normed_bw", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE validation set", "\n", "            ", "for", "val_y", ",", "val_x", ",", "val_seq_lengths", "in", "dataset", ".", "batches", "(", "'validation_docnade'", ",", "params", "[", "'validation_bs'", "]", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", "[", "'multi_label'", "]", ")", ":", "\n", "                ", "val_loss_normed", ",", "val_loss_unnormed", "=", "session", ".", "run", "(", "[", "model_ppl", ".", "loss_normed", ",", "model_ppl", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model_ppl", ".", "x", ":", "val_x", ",", "\n", "model_ppl", ".", "y", ":", "val_y", ",", "\n", "model_ppl", ".", "seq_lengths", ":", "val_seq_lengths", "\n", "}", ")", "\n", "this_val_nll", ".", "append", "(", "val_loss_unnormed", ")", "\n", "this_val_loss_normed", ".", "append", "(", "val_loss_normed", ")", "\n", "\n", "# Calculating PPl and NLL on validation set", "\n", "", "", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "            ", "total_val_nll", "=", "0.5", "*", "(", "np", ".", "mean", "(", "this_val_nll", ")", "+", "np", ".", "mean", "(", "this_val_nll_bw", ")", ")", "\n", "total_val_ppl", "=", "0.5", "*", "(", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "+", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed_bw", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "total_val_nll", "=", "np", ".", "mean", "(", "this_val_nll", ")", "\n", "total_val_ppl", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_val_loss_normed", ")", ")", "\n", "\n", "", "print", "(", "'Val PPL: {:.3f},    Val loss: {:.3f}\\n'", ".", "format", "(", "\n", "total_val_ppl", ",", "\n", "total_val_nll", "\n", ")", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"reload_info_ppl.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"Val PPL: %s,    Val loss: %s\"", "%", "\n", "(", "total_val_ppl", ",", "total_val_nll", ")", ")", "\n", "\n", "\n", "# Calculating PPL for test set", "\n", "\n", "# test_loss_unnormed is for Negative Log Likelihood (NLL)", "\n", "# test_loss_normed is for Perplexity (PPL)", "\n", "\n", "", "this_test_nll", "=", "[", "]", "\n", "this_test_loss_normed", "=", "[", "]", "\n", "this_test_nll_bw", "=", "[", "]", "\n", "this_test_loss_normed_bw", "=", "[", "]", "\n", "\n", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "# Getting iDocNADE test set", "\n", "            ", "for", "test_y", ",", "test_x", ",", "test_x_bw", ",", "test_seq_lengths", "in", "dataset", ".", "batches_bidirectional", "(", "'test_docnade'", ",", "params", "[", "'test_bs'", "]", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", "[", "'multi_label'", "]", ")", ":", "\n", "                ", "test_loss_normed", ",", "test_loss_unnormed", ",", "test_loss_normed_bw", ",", "test_loss_unnormed_bw", "=", "session", ".", "run", "(", "[", "model_ppl", ".", "loss_normed", ",", "model_ppl", ".", "loss_unnormed", ",", "\n", "model_ppl", ".", "loss_normed_bw", ",", "model_ppl", ".", "loss_unnormed_bw", "]", ",", "feed_dict", "=", "{", "\n", "model_ppl", ".", "x", ":", "test_x", ",", "\n", "model_ppl", ".", "x_bw", ":", "test_x_bw", ",", "\n", "model_ppl", ".", "y", ":", "test_y", ",", "\n", "model_ppl", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "this_test_nll", ".", "append", "(", "test_loss_unnormed", ")", "\n", "this_test_loss_normed", ".", "append", "(", "test_loss_normed", ")", "\n", "this_test_nll_bw", ".", "append", "(", "test_loss_unnormed_bw", ")", "\n", "this_test_loss_normed_bw", ".", "append", "(", "test_loss_normed_bw", ")", "\n", "", "", "else", ":", "\n", "# Getting DocNADE test set", "\n", "            ", "for", "test_y", ",", "test_x", ",", "test_seq_lengths", "in", "dataset", ".", "batches", "(", "'test_docnade'", ",", "params", "[", "'test_bs'", "]", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ",", "multilabel", "=", "params", "[", "'multi_label'", "]", ")", ":", "\n", "                ", "test_loss_normed", ",", "test_loss_unnormed", "=", "session", ".", "run", "(", "[", "model_ppl", ".", "loss_normed", ",", "model_ppl", ".", "loss_unnormed", "]", ",", "feed_dict", "=", "{", "\n", "model_ppl", ".", "x", ":", "test_x", ",", "\n", "model_ppl", ".", "y", ":", "test_y", ",", "\n", "model_ppl", ".", "seq_lengths", ":", "test_seq_lengths", "\n", "}", ")", "\n", "this_test_nll", ".", "append", "(", "test_loss_unnormed", ")", "\n", "this_test_loss_normed", ".", "append", "(", "test_loss_normed", ")", "\n", "\n", "# Calculating PPl and NLL on full test set", "\n", "", "", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "            ", "total_test_nll", "=", "0.5", "*", "(", "np", ".", "mean", "(", "this_test_nll", ")", "+", "np", ".", "mean", "(", "this_test_nll_bw", ")", ")", "\n", "total_test_ppl", "=", "0.5", "*", "(", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed", ")", ")", "+", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed_bw", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "total_test_nll", "=", "np", ".", "mean", "(", "this_test_nll", ")", "\n", "total_test_ppl", "=", "np", ".", "exp", "(", "np", ".", "mean", "(", "this_test_loss_normed", ")", ")", "\n", "\n", "", "print", "(", "'Test PPL: {:.3f},    Test loss: {:.3f}\\n'", ".", "format", "(", "\n", "total_test_ppl", ",", "\n", "total_test_nll", "\n", ")", ")", "\n", "\n", "# logging information", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "\"reload_info_ppl.txt\"", ")", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\\n\\nTest PPL: %s,    Test loss: %s\"", "%", "\n", "(", "total_test_ppl", ",", "total_test_nll", ")", ")", "\n", "\n", "\n", "", "\"\"\"\n        # Topics calculation with W matrix\n        top_n_topic_words = 20\n        w_h_top_words_indices = []\n        W_topics = model_ppl.W\n        topics_list_W = []\n\n        for h_num in range(np.array(W_topics).shape[1]):\n            w_h_top_words_indices.append(np.argsort(W_topics[:, h_num])[::-1][:top_n_topic_words])\n\n        with open(params['docnadeVocab'], 'r') as f:\n            vocab_docnade = [w.strip() for w in f.readlines()]\n\n        with open(os.path.join(log_dir, \"topics_ppl_W.txt\"), \"w\") as f:\n            for w_h_top_words_indx, h_num in zip(w_h_top_words_indices, range(len(w_h_top_words_indices))):\n                w_h_top_words = [vocab_docnade[w_indx] for w_indx in w_h_top_words_indx]\n                topics_list_W.append(w_h_top_words)\n\n                print('h_num: %s' % h_num)\n                print('w_h_top_words_indx: %s' % w_h_top_words_indx)\n                print('w_h_top_words:%s' % w_h_top_words)\n                print('----------------------------------------------------------------------')\n\n                f.write('h_num: %s\\n' % h_num)\n                f.write('w_h_top_words_indx: %s\\n' % w_h_top_words_indx)\n                f.write('w_h_top_words:%s\\n' % w_h_top_words)\n                f.write('----------------------------------------------------------------------\\n')\n\n        # Topics calculation with V matrix\n        top_n_topic_words = 20\n        w_h_top_words_indices = []\n        W_topics = model_ppl.V.T\n        topics_list_V = []\n\n        for h_num in range(np.array(W_topics).shape[1]):\n            w_h_top_words_indices.append(np.argsort(W_topics[:, h_num])[::-1][:top_n_topic_words])\n\n        with open(params['docnadeVocab'], 'r') as f:\n            vocab_docnade = [w.strip() for w in f.readlines()]\n\n        with open(os.path.join(log_dir, \"topics_ppl_V.txt\"), \"w\") as f:\n            for w_h_top_words_indx, h_num in zip(w_h_top_words_indices, range(len(w_h_top_words_indices))):\n                w_h_top_words = [vocab_docnade[w_indx] for w_indx in w_h_top_words_indx]\n                topics_list_V.append(w_h_top_words)\n\n                print('h_num: %s' % h_num)\n                print('w_h_top_words_indx: %s' % w_h_top_words_indx)\n                print('w_h_top_words:%s' % w_h_top_words)\n                print('----------------------------------------------------------------------')\n\n                f.write('h_num: %s\\n' % h_num)\n                f.write('w_h_top_words_indx: %s\\n' % w_h_top_words_indx)\n                f.write('w_h_top_words:%s\\n' % w_h_top_words)\n                f.write('----------------------------------------------------------------------\\n')\n        \n        # Calculating topic coherence of the topics calculated above\n        top_n_word_in_each_topic_list = [10, 20]\n\n        text_filenames = [\n            params['trainfile'],\n            params['valfile'],\n            params['testfile']\n        ]\n\n        # read original text documents as list of words\n        texts = []\n\n        for file in text_filenames:\n            print('filename:%s', file)\n            for line in open(file, 'r').readlines():\n                document = str(line).split('\\t')[1]\n                document = document.decode('utf-8',errors='ignore')\n                texts.append(document.split())\n\n        compute_coherence(texts, topics_list_W, top_n_word_in_each_topic_list, os.path.join(log_dir, \"topics_coherence_W.txt\"))\n        compute_coherence(texts, topics_list_V, top_n_word_in_each_topic_list, os.path.join(log_dir, \"topics_coherence_V.txt\"))\n\n        \n        # Calculating nearest neighbors using DocNADE/iDocNADE W embedding matrix\n        W = model_ppl.W\n        vocab = vocab_docnade\n        sim_mat = dict()\n\n        for i in range(W.shape[0]):\n            for j in range(W.shape[0]):\n                sim_mat[str(vocab[i]) + \"_\" + str(vocab[j])] = np.around(cosine_similarity(W[i][:], W[j][:]), decimals=3)\n                \n        sorted_sim_mat = sim_mat\n\n        print('shape sorted_sim_mat:', len(sorted_sim_mat))\n\n        top_5_words_for_each_word = dict()\n        for key in sorted_sim_mat.keys():\n            value = sorted_sim_mat[key]\n            target_word = str(key).split('_')[0]\n\n            if not target_word in top_5_words_for_each_word.keys():\n                top_5_words_for_each_word[target_word] = []\n\n            second_word = str(key).split('_')[1]\n\n            if target_word != second_word:\n                top_5_words_for_each_word[target_word].append((second_word, value))\n\n        top_n_similar_words_for_each_word = {}\n        top_n = 20\n\n        with open(os.path.join(log_dir, \"nearest_neighbors_ppl_W.txt\"), \"w\") as f:\n            for target_word in top_5_words_for_each_word.keys():\n\n                if not len(target_word) > 2: continue\n\n                sample_word_tupls = top_5_words_for_each_word[target_word]\n                sorted_by_sim = sorted(sample_word_tupls, key=lambda tup: tup[1])[::-1][:top_n * top_n]\n                \n                sorted_by_sim = [(x, np.around(y, decimals=2)) for (x, y)\n                                in sorted_by_sim if wordnet.synsets(x)\n                                and len(x) > 2][:top_n]  # check if the word is in english dict\n                \n                top_n_similar_words_for_each_word[target_word] = sorted_by_sim\n                top_words = [word for word, sim in sorted_by_sim]\n                print('target word:%s  Top_words:%s' % (target_word, top_words))\n                print('======================================================================================================')\n\n                f.write('target word:%s  Top_words:%s\\n' % (target_word, top_words))\n                f.write('======================================================================================================\\n')\n\n            print('len top_5_words_for_each_word:%s' % len(top_5_words_for_each_word))\n            f.write('len top_5_words_for_each_word:%s' % len(top_5_words_for_each_word))\n\n\n        # Calculating nearest neighbors using DocNADE/iDocNADE V matrix\n        W = model_ppl.V.T\n        vocab = vocab_docnade\n        sim_mat = dict()\n\n        for i in range(W.shape[0]):\n            for j in range(W.shape[0]):\n                sim_mat[str(vocab[i]) + \"_\" + str(vocab[j])] = np.around(cosine_similarity(W[i][:], W[j][:]), decimals=3)\n                \n        sorted_sim_mat = sim_mat\n\n        print('shape sorted_sim_mat:', len(sorted_sim_mat))\n\n        top_5_words_for_each_word = dict()\n        for key in sorted_sim_mat.keys():\n            value = sorted_sim_mat[key]\n            target_word = str(key).split('_')[0]\n\n            if not target_word in top_5_words_for_each_word.keys():\n                top_5_words_for_each_word[target_word] = []\n\n            second_word = str(key).split('_')[1]\n\n            if target_word != second_word:\n                top_5_words_for_each_word[target_word].append((second_word, value))\n\n        top_n_similar_words_for_each_word = {}\n        top_n = 20\n\n        with open(os.path.join(log_dir, \"nearest_neighbors_ppl_V.txt\"), \"w\") as f:\n            for target_word in top_5_words_for_each_word.keys():\n\n                if not len(target_word) > 2: continue\n\n                sample_word_tupls = top_5_words_for_each_word[target_word]\n                sorted_by_sim = sorted(sample_word_tupls, key=lambda tup: tup[1])[::-1][:top_n * top_n]\n                \n                sorted_by_sim = [(x, np.around(y, decimals=2)) for (x, y)\n                                in sorted_by_sim if wordnet.synsets(x)\n                                and len(x) > 2][:top_n]  # check if the word is in english dict\n                \n                top_n_similar_words_for_each_word[target_word] = sorted_by_sim\n                top_words = [word for word, sim in sorted_by_sim]\n                print('target word:%s  Top_words:%s' % (target_word, top_words))\n                print('======================================================================================================')\n\n                f.write('target word:%s  Top_words:%s\\n' % (target_word, top_words))\n                f.write('======================================================================================================\\n')\n\n            print('len top_5_words_for_each_word:%s' % len(top_5_words_for_each_word))\n            f.write('len top_5_words_for_each_word:%s' % len(top_5_words_for_each_word))\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool": [[1345, 1355], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "'''\n    Function to convert string parameters to boolean\n    '''", "\n", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.main": [[1357, 1630], ["train_model.str2bool", "train_model.str2bool", "train_model.str2bool", "train_model.str2bool", "train_model.str2bool", "train_model.str2bool", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "os.path.isdir", "os.path.isdir", "datetime.datetime.now", "model.Dataset", "train_model.train", "open", "json.load", "json.load", "tensorflow.Session", "tensorflow.train.import_meta_graph", "tf.train.import_meta_graph.restore", "model.Dataset", "train_model.reload_evaluation_ppl", "tensorflow.Session", "tensorflow.train.import_meta_graph", "tf.train.import_meta_graph.restore", "model.Dataset", "train_model.reload_evaluation_ir", "str", "os.path.isdir", "os.mkdir", "open", "f.write", "train_model.loadGloveModel", "open", "numpy.zeros", "enumerate", "tensorflow.convert_to_tensor", "print", "print", "model.iDocNADE", "print", "model.DocNADE", "print", "json.load.keys", "tensorflow.train.latest_checkpoint", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "model.iDocNADE_reload", "print", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "model.DocNADE_reload", "print", "tensorflow.train.latest_checkpoint", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "model.iDocNADE_reload", "print", "tf.Session.run", "tf.Session.run", "tf.Session.run", "tf.Session.run", "model.DocNADE_reload", "print", "os.path.join", "json.dumps", "json.dumps", "w.strip", "open", "pickle.load", "enumerate", "enumerate", "enumerate", "enumerate", "str", "vars", "f.readlines", "len", "str().lower", "loadGloveModel.keys", "numpy.zeros", "tf.Session.run", "W_list_ppl.append", "tf.Session.run", "bias_list_ppl.append", "tf.Session.run", "bias_bw_list_ppl.append", "tf.Session.run", "W_list_ppl.append", "tf.Session.run", "bias_list_ppl.append", "tf.Session.run", "W_list_ir.append", "tf.Session.run", "bias_list_ir.append", "tf.Session.run", "bias_bw_list_ir.append", "tf.Session.run", "W_list_ir.append", "tf.Session.run", "bias_list_ir.append", "len", "numpy.zeros", "numpy.array", "len", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str().lower", "str().lower", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.str2bool", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.train", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.reload_evaluation_ppl", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.reload_evaluation_ir", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.loadGloveModel"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "# Converting string parameters parsed from configuration file to boolean type", "\n", "    ", "args", ".", "reload", "=", "str2bool", "(", "args", ".", "reload", ")", "\n", "args", ".", "initialize_docnade", "=", "str2bool", "(", "args", ".", "initialize_docnade", ")", "\n", "args", ".", "bidirectional", "=", "str2bool", "(", "args", ".", "bidirectional", ")", "\n", "args", ".", "projection", "=", "str2bool", "(", "args", ".", "projection", ")", "\n", "args", ".", "deep", "=", "str2bool", "(", "args", ".", "deep", ")", "\n", "args", ".", "multi_label", "=", "str2bool", "(", "args", ".", "multi_label", ")", "\n", "\n", "# Creating placeholders for tensorflow tensors", "\n", "x", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", "None", ")", ",", "name", "=", "'x'", ")", "\n", "x_bw", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", "None", ")", ",", "name", "=", "'x_bw'", ")", "\n", "if", "args", ".", "multi_label", ":", "\n", "        ", "y", "=", "tf", ".", "placeholder", "(", "tf", ".", "string", ",", "shape", "=", "(", "None", ")", ",", "name", "=", "'y'", ")", "\n", "", "else", ":", "\n", "        ", "y", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ")", ",", "name", "=", "'y'", ")", "\n", "", "seq_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ")", ",", "name", "=", "'seq_lengths'", ")", "\n", "\n", "\n", "if", "args", ".", "reload", ":", "\n", "# Model reload", "\n", "        ", "with", "open", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"params.json\"", ")", "as", "f", ":", "\n", "            ", "params", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "not", "'multi_label'", "in", "params", ".", "keys", "(", ")", ":", "\n", "            ", "params", "[", "'multi_label'", "]", "=", "False", "\n", "\n", "", "params", "[", "'trainfile'", "]", "=", "args", ".", "trainfile", "\n", "params", "[", "'valfile'", "]", "=", "args", ".", "valfile", "\n", "params", "[", "'testfile'", "]", "=", "args", ".", "testfile", "\n", "\n", "params", "[", "'reload_model_dir'", "]", "=", "args", ".", "reload_model_dir", "\n", "\n", "reload_ir", "=", "False", "\n", "if", "os", ".", "path", ".", "isdir", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ir\"", ")", ":", "\n", "            ", "reload_ir", "=", "True", "\n", "\n", "", "reload_ppl", "=", "False", "\n", "if", "os", ".", "path", ".", "isdir", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ppl\"", ")", ":", "\n", "            ", "reload_ppl", "=", "True", "\n", "\n", "", "if", "reload_ppl", ":", "\n", "            ", "sess_ppl", "=", "tf", ".", "Session", "(", ")", "\n", "\n", "saver_ppl", "=", "tf", ".", "train", ".", "import_meta_graph", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ppl/model_ppl-1.meta\"", ")", "\n", "saver_ppl", ".", "restore", "(", "sess_ppl", ",", "tf", ".", "train", ".", "latest_checkpoint", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ppl/\"", ")", ")", "\n", "\n", "# Loading best saved IR/PPL weights and biases", "\n", "if", "params", "[", "'projection'", "]", ":", "\n", "                ", "embedding_prior_projection_ppl", "=", "sess_ppl", ".", "run", "(", "\"embedding_prior_projection:0\"", ")", "\n", "", "else", ":", "\n", "                ", "embedding_prior_projection_ppl", "=", "None", "\n", "\n", "", "if", "params", "[", "'initialize_docnade'", "]", ":", "\n", "                ", "embedding_prior_ppl", "=", "sess_ppl", ".", "run", "(", "\"embedding_prior:0\"", ")", "\n", "", "else", ":", "\n", "                ", "embedding_prior_ppl", "=", "None", "\n", "\n", "", "W_list_ppl", "=", "[", "]", "\n", "bias_list_ppl", "=", "[", "]", "\n", "bias_bw_list_ppl", "=", "[", "]", "\n", "\n", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "\n", "                ", "if", "params", "[", "'deep'", "]", ":", "\n", "                    ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                        ", "embedding_name_ppl", "=", "\"embedding_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "embedding_temp_ppl", "=", "sess_ppl", ".", "run", "(", "embedding_name_ppl", ")", "\n", "W_list_ppl", ".", "append", "(", "embedding_temp_ppl", ")", "\n", "\n", "bias_name_ppl", "=", "\"bias_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_temp_ppl", "=", "sess_ppl", ".", "run", "(", "bias_name_ppl", ")", "\n", "bias_list_ppl", ".", "append", "(", "bias_temp_ppl", ")", "\n", "\n", "bias_bw_name_ppl", "=", "\"bias_bw_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_bw_temp_ppl", "=", "sess_ppl", ".", "run", "(", "bias_bw_name_ppl", ")", "\n", "bias_bw_list_ppl", ".", "append", "(", "bias_bw_temp_ppl", ")", "\n", "\n", "", "", "embedding_ppl", "=", "sess_ppl", ".", "run", "(", "\"embedding:0\"", ")", "\n", "bias_ppl", "=", "sess_ppl", ".", "run", "(", "\"bias:0\"", ")", "\n", "bias_bw_ppl", "=", "sess_ppl", ".", "run", "(", "\"bias_bw:0\"", ")", "\n", "V_ppl", "=", "sess_ppl", ".", "run", "(", "\"softmax/w:0\"", ")", "\n", "bias_V_ppl", "=", "sess_ppl", ".", "run", "(", "\"b:0\"", ")", "\n", "bias_bw_V_ppl", "=", "sess_ppl", ".", "run", "(", "\"b_bw:0\"", ")", "\n", "\n", "# Creating iDocNADE model based on best saved PPL model", "\n", "model_ppl", "=", "m", ".", "iDocNADE_reload", "(", "x", ",", "x_bw", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "embedding_prior_ppl", ",", "W_reload", "=", "embedding_ppl", ",", "\n", "W_prior_reload", "=", "embedding_prior_ppl", ",", "W_prior_proj_reload", "=", "embedding_prior_projection_ppl", ",", "\n", "bias_reload", "=", "bias_ppl", ",", "bias_bw_reload", "=", "bias_bw_ppl", ",", "V_reload", "=", "V_ppl", ",", "b_reload", "=", "bias_V_ppl", ",", "b_bw_reload", "=", "bias_bw_V_ppl", ",", "\n", "W_list_reload", "=", "W_list_ppl", ",", "bias_list_reload", "=", "bias_list_ppl", ",", "bias_bw_list_reload", "=", "bias_bw_list_ppl", ",", "\n", "lambda_embeddings", "=", "params", "[", "'lambda_embeddings'", "]", ")", "\n", "\n", "print", "(", "\"iDocNADE PPL created\"", ")", "\n", "\n", "", "else", ":", "\n", "\n", "                ", "if", "params", "[", "'deep'", "]", ":", "\n", "                    ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                        ", "embedding_name_ppl", "=", "\"embedding_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "embedding_temp_ppl", "=", "sess_ppl", ".", "run", "(", "embedding_name_ppl", ")", "\n", "W_list_ppl", ".", "append", "(", "embedding_temp_ppl", ")", "\n", "\n", "bias_name_ppl", "=", "\"bias_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_temp_ppl", "=", "sess_ppl", ".", "run", "(", "bias_name_ppl", ")", "\n", "bias_list_ppl", ".", "append", "(", "bias_temp_ppl", ")", "\n", "\n", "", "", "embedding_ppl", "=", "sess_ppl", ".", "run", "(", "\"embedding:0\"", ")", "\n", "bias_ppl", "=", "sess_ppl", ".", "run", "(", "\"bias:0\"", ")", "\n", "V_ppl", "=", "sess_ppl", ".", "run", "(", "\"softmax/w:0\"", ")", "\n", "bias_V_ppl", "=", "sess_ppl", ".", "run", "(", "\"b:0\"", ")", "\n", "\n", "# Creating DocNADE model based on best saved PPL model", "\n", "model_ppl", "=", "m", ".", "DocNADE_reload", "(", "x", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "embedding_prior_ppl", ",", "W_reload", "=", "embedding_ppl", ",", "\n", "W_prior_reload", "=", "embedding_prior_ppl", ",", "W_prior_proj_reload", "=", "embedding_prior_projection_ppl", ",", "\n", "bias_reload", "=", "bias_ppl", ",", "bias_bw_reload", "=", "None", ",", "V_reload", "=", "V_ppl", ",", "b_reload", "=", "bias_V_ppl", ",", "b_bw_reload", "=", "None", ",", "\n", "W_list_reload", "=", "W_list_ppl", ",", "bias_list_reload", "=", "bias_list_ppl", ",", "lambda_embeddings", "=", "params", "[", "'lambda_embeddings'", "]", ")", "\n", "print", "(", "\"DocNADE PPL created\"", ")", "\n", "\n", "", "dataset", "=", "data", ".", "Dataset", "(", "params", "[", "'dataset'", "]", ")", "\n", "reload_evaluation_ppl", "(", "model_ppl", ",", "dataset", ",", "params", ")", "\n", "\n", "", "if", "reload_ir", ":", "\n", "            ", "sess_ir", "=", "tf", ".", "Session", "(", ")", "\n", "\n", "saver_ir", "=", "tf", ".", "train", ".", "import_meta_graph", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ir/model_ir-1.meta\"", ")", "\n", "saver_ir", ".", "restore", "(", "sess_ir", ",", "tf", ".", "train", ".", "latest_checkpoint", "(", "\"model/\"", "+", "args", ".", "reload_model_dir", "+", "\"/model_ir/\"", ")", ")", "\n", "\n", "# Loading best saved IR/PPL weights and biases", "\n", "if", "params", "[", "'projection'", "]", ":", "\n", "                ", "embedding_prior_projection_ir", "=", "sess_ir", ".", "run", "(", "\"embedding_prior_projection:0\"", ")", "\n", "", "else", ":", "\n", "                ", "embedding_prior_projection_ir", "=", "None", "\n", "\n", "", "if", "params", "[", "'initialize_docnade'", "]", ":", "\n", "                ", "embedding_prior_ir", "=", "sess_ir", ".", "run", "(", "\"embedding_prior:0\"", ")", "\n", "", "else", ":", "\n", "                ", "embedding_prior_ir", "=", "None", "\n", "\n", "", "W_list_ir", "=", "[", "]", "\n", "bias_list_ir", "=", "[", "]", "\n", "bias_bw_list_ir", "=", "[", "]", "\n", "\n", "if", "params", "[", "'bidirectional'", "]", ":", "\n", "\n", "                ", "if", "params", "[", "'deep'", "]", ":", "\n", "                    ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                        ", "embedding_name_ir", "=", "\"embedding_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "embedding_temp_ir", "=", "sess_ir", ".", "run", "(", "embedding_name_ir", ")", "\n", "W_list_ir", ".", "append", "(", "embedding_temp_ir", ")", "\n", "\n", "bias_name_ir", "=", "\"bias_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_temp_ir", "=", "sess_ir", ".", "run", "(", "bias_name_ir", ")", "\n", "bias_list_ir", ".", "append", "(", "bias_temp_ir", ")", "\n", "\n", "bias_bw_name_ir", "=", "\"bias_bw_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_bw_temp_ir", "=", "sess_ir", ".", "run", "(", "bias_bw_name_ir", ")", "\n", "bias_bw_list_ir", ".", "append", "(", "bias_bw_temp_ir", ")", "\n", "\n", "", "", "embedding_ir", "=", "sess_ir", ".", "run", "(", "\"embedding:0\"", ")", "\n", "bias_ir", "=", "sess_ir", ".", "run", "(", "\"bias:0\"", ")", "\n", "bias_bw_ir", "=", "sess_ir", ".", "run", "(", "\"bias_bw:0\"", ")", "\n", "V_ir", "=", "sess_ir", ".", "run", "(", "\"softmax/w:0\"", ")", "\n", "bias_V_ir", "=", "sess_ir", ".", "run", "(", "\"b:0\"", ")", "\n", "bias_bw_V_ir", "=", "sess_ir", ".", "run", "(", "\"b_bw:0\"", ")", "\n", "\n", "# Creating iDocNADE model based on best saved IR model", "\n", "model_ir", "=", "m", ".", "iDocNADE_reload", "(", "x", ",", "x_bw", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "embedding_prior_ir", ",", "W_reload", "=", "embedding_ir", ",", "\n", "W_prior_reload", "=", "embedding_prior_ir", ",", "W_prior_proj_reload", "=", "embedding_prior_projection_ir", ",", "\n", "bias_reload", "=", "bias_ir", ",", "bias_bw_reload", "=", "bias_bw_ir", ",", "V_reload", "=", "V_ir", ",", "b_reload", "=", "bias_V_ir", ",", "b_bw_reload", "=", "bias_bw_V_ir", ",", "\n", "W_list_reload", "=", "W_list_ir", ",", "bias_list_reload", "=", "bias_list_ir", ",", "bias_bw_list_reload", "=", "bias_bw_list_ir", ",", "\n", "lambda_embeddings", "=", "params", "[", "'lambda_embeddings'", "]", ")", "\n", "\n", "print", "(", "\"iDocNADE IR created\"", ")", "\n", "\n", "", "else", ":", "\n", "\n", "                ", "if", "params", "[", "'deep'", "]", ":", "\n", "                    ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                        ", "embedding_name_ir", "=", "\"embedding_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "embedding_temp_ir", "=", "sess_ir", ".", "run", "(", "embedding_name_ir", ")", "\n", "W_list_ir", ".", "append", "(", "embedding_temp_ir", ")", "\n", "\n", "bias_name_ir", "=", "\"bias_\"", "+", "str", "(", "index", ")", "+", "\":0\"", "\n", "bias_temp_ir", "=", "sess_ir", ".", "run", "(", "bias_name_ir", ")", "\n", "bias_list_ir", ".", "append", "(", "bias_temp_ir", ")", "\n", "\n", "", "", "embedding_ir", "=", "sess_ir", ".", "run", "(", "\"embedding:0\"", ")", "\n", "bias_ir", "=", "sess_ir", ".", "run", "(", "\"bias:0\"", ")", "\n", "V_ir", "=", "sess_ir", ".", "run", "(", "\"softmax/w:0\"", ")", "\n", "bias_V_ir", "=", "sess_ir", ".", "run", "(", "\"b:0\"", ")", "\n", "\n", "# Creating DocNADE model based on best saved IR model", "\n", "model_ir", "=", "m", ".", "DocNADE_reload", "(", "x", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "embedding_prior_ir", ",", "W_reload", "=", "embedding_ir", ",", "\n", "W_prior_reload", "=", "embedding_prior_ir", ",", "W_prior_proj_reload", "=", "embedding_prior_projection_ir", ",", "\n", "bias_reload", "=", "bias_ir", ",", "bias_bw_reload", "=", "None", ",", "V_reload", "=", "V_ir", ",", "b_reload", "=", "bias_V_ir", ",", "b_bw_reload", "=", "None", ",", "\n", "W_list_reload", "=", "W_list_ir", ",", "bias_list_reload", "=", "bias_list_ir", ",", "lambda_embeddings", "=", "params", "[", "'lambda_embeddings'", "]", ")", "\n", "print", "(", "\"DocNADE IR created\"", ")", "\n", "\n", "", "dataset", "=", "data", ".", "Dataset", "(", "params", "[", "'dataset'", "]", ")", "\n", "reload_evaluation_ir", "(", "model_ir", ",", "dataset", ",", "params", ")", "\n", "", "", "else", ":", "\n", "# Model training", "\n", "        ", "now", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "if", "args", ".", "bidirectional", ":", "\n", "            ", "args", ".", "model", "+=", "\"_iDocNADE\"", "\n", "", "else", ":", "\n", "            ", "args", ".", "model", "+=", "\"_DocNADE\"", "\n", "\n", "", "if", "args", ".", "initialize_docnade", ":", "\n", "            ", "args", ".", "model", "+=", "\"_embprior\"", "\n", "\n", "", "if", "args", ".", "pretrained_embeddings_path", ":", "\n", "            ", "args", ".", "model", "+=", "\"_pretrained\"", "\n", "\n", "", "args", ".", "model", "+=", "\"_act_\"", "+", "str", "(", "args", ".", "activation", ")", "+", "\"_hidden_\"", "+", "str", "(", "args", ".", "hidden_size", ")", "+", "\"_vocab_\"", "+", "str", "(", "args", ".", "vocab_size", ")", "+", "\"_lr_\"", "+", "str", "(", "args", ".", "learning_rate", ")", "+", "\"_proj_\"", "+", "str", "(", "args", ".", "projection", ")", "+", "\"_deep_\"", "+", "str", "(", "args", ".", "deep", ")", "+", "\"_lambda_\"", "+", "str", "(", "args", ".", "lambda_embeddings", ")", "+", "\"_\"", "+", "str", "(", "now", ".", "day", ")", "+", "\"_\"", "+", "str", "(", "now", ".", "month", ")", "+", "\"_\"", "+", "str", "(", "now", ".", "year", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "model", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "args", ".", "model", ")", "\n", "\n", "", "docnade_vocab", "=", "args", ".", "docnadeVocab", "\n", "\n", "#if args.bidirectional or args.initialize_docnade:", "\n", "#    args.patience = 30", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model", ",", "'params.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "vars", "(", "args", ")", ")", ")", "\n", "\n", "", "dataset", "=", "data", ".", "Dataset", "(", "args", ".", "dataset", ")", "\n", "\n", "if", "args", ".", "initialize_docnade", ":", "\n", "            ", "glove_embeddings", "=", "loadGloveModel", "(", "params", "=", "args", ")", "\n", "\n", "", "with", "open", "(", "docnade_vocab", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "vocab_docnade", "=", "[", "w", ".", "strip", "(", ")", "for", "w", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "# Creating embedding prior matrix using glove embeddings", "\n", "", "docnade_embedding_matrix", "=", "None", "\n", "if", "args", ".", "initialize_docnade", ":", "\n", "            ", "missing_words", "=", "0", "\n", "docnade_embedding_matrix", "=", "np", ".", "zeros", "(", "(", "len", "(", "vocab_docnade", ")", ",", "args", ".", "hidden_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "vocab_docnade", ")", ":", "\n", "                ", "if", "str", "(", "word", ")", ".", "lower", "(", ")", "in", "glove_embeddings", ".", "keys", "(", ")", ":", "\n", "                    ", "if", "len", "(", "glove_embeddings", "[", "str", "(", "word", ")", ".", "lower", "(", ")", "]", ")", "==", "0", ":", "\n", "                        ", "docnade_embedding_matrix", "[", "i", ",", ":", "]", "=", "np", ".", "zeros", "(", "(", "args", ".", "hidden_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "missing_words", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "docnade_embedding_matrix", "[", "i", ",", ":", "]", "=", "np", ".", "array", "(", "glove_embeddings", "[", "str", "(", "word", ")", ".", "lower", "(", ")", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "", "else", ":", "\n", "                    ", "docnade_embedding_matrix", "[", "i", ",", ":", "]", "=", "np", ".", "zeros", "(", "(", "args", ".", "hidden_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "missing_words", "+=", "1", "\n", "\n", "", "", "docnade_embedding_matrix", "=", "tf", ".", "convert_to_tensor", "(", "docnade_embedding_matrix", ")", "\n", "print", "(", "\"Total missing words:%d out of %d\"", "%", "(", "missing_words", ",", "len", "(", "vocab_docnade", ")", ")", ")", "\n", "\n", "# Load pretrained DocNADE embeddings for iDocNADE setting", "\n", "", "docnade_pretrained_matrix", "=", "None", "\n", "if", "args", ".", "pretrained_embeddings_path", ":", "\n", "            ", "with", "open", "(", "args", ".", "pretrained_embeddings_path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "docnade_pretrained_matrix", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "print", "(", "\"pretrained embeddings loaded.\"", ")", "\n", "\n", "", "if", "args", ".", "bidirectional", ":", "\n", "            ", "model", "=", "m", ".", "iDocNADE", "(", "x", ",", "x_bw", ",", "y", ",", "seq_lengths", ",", "args", ",", "W_initializer", "=", "docnade_embedding_matrix", ",", "lambda_embeddings", "=", "args", ".", "lambda_embeddings", ",", "W_pretrained", "=", "docnade_pretrained_matrix", ")", "\n", "print", "(", "\"iDocNADE created\"", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "m", ".", "DocNADE", "(", "x", ",", "y", ",", "seq_lengths", ",", "args", ",", "W_initializer", "=", "docnade_embedding_matrix", ",", "lambda_embeddings", "=", "args", ".", "lambda_embeddings", ",", "W_pretrained", "=", "docnade_pretrained_matrix", ")", "\n", "print", "(", "\"DocNADE created\"", ")", "\n", "\n", "# Train function", "\n", "", "train", "(", "model", ",", "dataset", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.parse_args": [[1632, 1705], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'path to model output directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'path to the input dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--vocab-size'", ",", "type", "=", "int", ",", "default", "=", "2000", ",", "\n", "help", "=", "'the vocab size'", ")", "\n", "parser", ".", "add_argument", "(", "'--hidden-size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'size of the hidden layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation'", ",", "type", "=", "str", ",", "default", "=", "'tanh'", ",", "\n", "help", "=", "'which activation to use: sigmoid|tanh|relu'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-rate'", ",", "type", "=", "float", ",", "default", "=", "0.0004", ",", "\n", "help", "=", "'learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-steps'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "\n", "help", "=", "'the number of steps to train for'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'batch size for training data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-samples'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'softmax samples (default: full softmax)'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-cores'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'the number of CPU cores to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-every'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'print training loss after this many steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--validation-ppl-freq'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'print validation PPL and NLL after this many steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--validation-ir-freq'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "\n", "help", "=", "'print loss after this many steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'number of classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--initialize-docnade'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether to include glove embedding prior or not'", ")", "\n", "parser", ".", "add_argument", "(", "'--docnadeVocab'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'path to vocabulary file used by DocNADE'", ")", "\n", "parser", ".", "add_argument", "(", "'--test-ppl-freq'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'print and log test PPL and NLL after this many steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--test-ir-freq'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'print and log test IR after this many steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'patience for early stopping criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--validation-bs'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'the batch size for validation evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--test-bs'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'the batch size for test evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--bidirectional'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether to use iDocNADE model or not'", ")", "\n", "parser", ".", "add_argument", "(", "'--combination-type'", ",", "type", "=", "str", ",", "default", "=", "\"concat\"", ",", "\n", "help", "=", "'combination type for iDocNADE forward and backward hidden document representation'", ")", "\n", "parser", ".", "add_argument", "(", "'--projection'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether to project prior embeddings or not'", ")", "\n", "parser", ".", "add_argument", "(", "'--reload'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether to reload model or not'", ")", "\n", "parser", ".", "add_argument", "(", "'--reload-model-dir'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'path for model to be reloaded'", ")", "\n", "parser", ".", "add_argument", "(", "'--deep-hidden-sizes'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "\n", "help", "=", "'sizes of the deep hidden layers for deepDocNADE'", ")", "\n", "parser", ".", "add_argument", "(", "'--deep'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether to maked model deep (deepDocNADE) or not (docNADE/iDocNADE)'", ")", "\n", "parser", ".", "add_argument", "(", "'--multi-label'", ",", "type", "=", "str", ",", "default", "=", "\"False\"", ",", "\n", "help", "=", "'whether dataset is multi-label or not'", ")", "\n", "parser", ".", "add_argument", "(", "'--trainfile'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'path to train text file'", ")", "\n", "parser", ".", "add_argument", "(", "'--valfile'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'path to validation text file'", ")", "\n", "parser", ".", "add_argument", "(", "'--testfile'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'path to test text file'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda-embeddings'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'combination weight for prior embeddings into docnade'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-embeddings-path'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "'path for pretrained embeddings'", ")", "\n", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.get_one_hot_multi_labels": [[9, 18], ["numpy.zeros", "enumerate", "len", "l.split", "int"], "function", ["None"], ["def", "get_one_hot_multi_labels", "(", "labels", ",", "num_classes", ")", ":", "\n", "    ", "one_hot_labels", "=", "np", ".", "zeros", "(", "(", "len", "(", "labels", ")", ",", "num_classes", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "index", ",", "label_list", "in", "enumerate", "(", "labels", ")", ":", "\n", "        ", "for", "l", "in", "label_list", ":", "\n", "#labels_list = l.split(':')", "\n", "            ", "labels_list", "=", "[", "temp", "for", "temp", "in", "l", ".", "split", "(", "':'", ")", "if", "temp", "!=", "''", "]", "\n", "for", "new_labels", "in", "labels_list", ":", "\n", "                ", "one_hot_labels", "[", "index", ",", "int", "(", "new_labels", ")", "]", "=", "1.0", "\n", "", "", "", "return", "one_hot_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.compare_labels": [[20, 58], ["numpy.asarray", "numpy.array", "print", "exit", "numpy.ones", "numpy.asarray", "numpy.dot", "print", "isinstance", "isinstance", "print", "exit", "numpy.array", "len", "len", "numpy.array", "print", "numpy.ones", "numpy.sum"], "function", ["None"], ["", "def", "compare_labels", "(", "train_labels", ",", "test_label", ",", "label_type", "=", "\"\"", ",", "evaluation_type", "=", "\"\"", ",", "labels_to_count", "=", "[", "]", ")", ":", "\n", "#train_labels = train_labels[:, labels_to_count]", "\n", "#test_label = test_label[labels_to_count]", "\n", "\n", "    ", "vec_goodLabel", "=", "[", "]", "\n", "\n", "train_labels", "=", "np", ".", "asarray", "(", "train_labels", ")", "\n", "\n", "if", "label_type", "==", "\"single\"", ":", "\n", "        ", "if", "not", "(", "isinstance", "(", "test_label", ",", "int", ")", "or", "isinstance", "(", "train_labels", "[", "0", "]", ",", "int", ")", ")", ":", "\n", "            ", "print", "(", "\"Labels are not instances of int\"", ")", "\n", "exit", "(", ")", "\n", "\n", "", "test_labels", "=", "np", ".", "ones", "(", "train_labels", ".", "shape", "[", "0", "]", ",", "dtype", "=", "np", ".", "float32", ")", "*", "test_label", "\n", "\n", "vec_goodLabel", "=", "np", ".", "array", "(", "(", "train_labels", "==", "test_labels", ")", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "", "elif", "label_type", "==", "\"multi\"", ":", "\n", "        ", "if", "not", "len", "(", "train_labels", "[", "0", "]", ")", "==", "len", "(", "test_label", ")", ":", "\n", "            ", "print", "(", "\"Mismatched label vector length\"", ")", "\n", "exit", "(", ")", "\n", "\n", "", "test_labels", "=", "np", ".", "asarray", "(", "test_label", ")", "\n", "labels_comparison_vec", "=", "np", ".", "dot", "(", "train_labels", ",", "test_labels", ")", "\n", "\n", "if", "evaluation_type", "==", "\"relaxed\"", ":", "\n", "            ", "vec_goodLabel", "=", "np", ".", "array", "(", "(", "labels_comparison_vec", "!=", "0", ")", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "\n", "", "elif", "evaluation_type", "==", "\"strict\"", ":", "\n", "            ", "test_label_vec", "=", "np", ".", "ones", "(", "train_labels", ".", "shape", "[", "0", "]", ")", "*", "np", ".", "sum", "(", "test_label", ")", "\n", "vec_goodLabel", "=", "np", ".", "array", "(", "(", "labels_comparison_vec", "==", "test_label_vec", ")", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Invalid evaluation_type value.\"", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Invalid label_type value.\"", ")", "\n", "\n", "", "return", "vec_goodLabel", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_IR_prec": [[59, 143], ["numpy.mean", "print", "exit", "numpy.floor", "enumerate", "enumerate", "len", "len", "numpy.argsort", "numpy.zeros", "evaluate.compare_labels", "numpy.argsort", "numpy.zeros", "evaluate.compare_labels", "enumerate", "len", "numpy.sum", "float", "len", "list_totalRetrievalCount.append", "int", "len", "numpy.floor", "numpy.sum", "numpy.sum", "float", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.compare_labels", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.compare_labels"], ["", "def", "perform_IR_prec", "(", "kernel_matrix_test", ",", "train_labels", ",", "test_labels", ",", "list_percRetrieval", "=", "None", ",", "single_precision", "=", "False", ",", "label_type", "=", "\"\"", ",", "evaluation", "=", "\"\"", ",", "index2label_dict", "=", "None", ",", "labels_to_not_count", "=", "[", "]", ",", "corpus_docs", "=", "None", ",", "query_docs", "=", "None", ",", "IR_filename", "=", "\"\"", ")", ":", "\n", "    ", "'''\n    :param kernel_matrix_test: shape: size = |test_samples| x |train_samples|\n    :param train_labels:              size = |train_samples| or |train_samples| x num_labels\n    :param test_labels:               size = |test_samples| or |test_samples| x num_labels\n    :param list_percRetrieval:        list of fractions at which IR has to be calculated\n    :param single_precision:          True, if only one fraction is used\n    :param label_type:                \"single\" or \"multi\"\n    :param evaluation:                \"strict\" or \"relaxed\", only for \n    :return:\n    '''", "\n", "#print('Computing IR prec......')", "\n", "\n", "if", "not", "len", "(", "test_labels", ")", "==", "len", "(", "kernel_matrix_test", ")", ":", "\n", "        ", "print", "(", "'mismatched samples in test_labels and kernel_matrix_test'", ")", "\n", "exit", "(", ")", "\n", "\n", "", "labels_to_count", "=", "[", "]", "\n", "#if labels_to_not_count:", "\n", "#    for index, label in index2label_dict.iteritems():", "\n", "#        if not label in labels_to_not_count:", "\n", "#            labels_to_count.append(int(index))", "\n", "\n", "prec", "=", "[", "]", "\n", "\n", "if", "single_precision", ":", "\n", "        ", "vec_simIndexSorted", "=", "np", ".", "argsort", "(", "kernel_matrix_test", ",", "axis", "=", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "prec_num_docs", "=", "np", ".", "floor", "(", "list_percRetrieval", "[", "0", "]", "*", "kernel_matrix_test", ".", "shape", "[", "1", "]", ")", "\n", "vec_simIndexSorted_prec", "=", "vec_simIndexSorted", "[", ":", ",", ":", "int", "(", "prec_num_docs", ")", "]", "\n", "\n", "for", "counter", ",", "indices", "in", "enumerate", "(", "vec_simIndexSorted_prec", ")", ":", "\n", "            ", "if", "label_type", "==", "\"multi\"", ":", "\n", "                ", "classQuery", "=", "test_labels", "[", "counter", ",", ":", "]", "\n", "tr_labels", "=", "train_labels", "[", "indices", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "classQuery", "=", "test_labels", "[", "counter", "]", "\n", "tr_labels", "=", "train_labels", "[", "indices", "]", "\n", "", "list_percPrecision", "=", "np", ".", "zeros", "(", "len", "(", "list_percRetrieval", ")", ")", "\n", "\n", "vec_goodLabel", "=", "compare_labels", "(", "tr_labels", ",", "classQuery", ",", "label_type", "=", "label_type", ",", "evaluation_type", "=", "evaluation", ",", "labels_to_count", "=", "labels_to_count", ")", "\n", "\n", "list_percPrecision", "[", "0", "]", "=", "np", ".", "sum", "(", "vec_goodLabel", ")", "/", "float", "(", "len", "(", "vec_goodLabel", ")", ")", "\n", "\n", "prec", "+=", "[", "list_percPrecision", "]", "\n", "", "", "else", ":", "\n", "        ", "vec_simIndexSorted", "=", "np", ".", "argsort", "(", "kernel_matrix_test", ",", "axis", "=", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "for", "counter", ",", "indices", "in", "enumerate", "(", "vec_simIndexSorted", ")", ":", "\n", "            ", "if", "label_type", "==", "\"multi\"", ":", "\n", "                ", "classQuery", "=", "test_labels", "[", "counter", ",", ":", "]", "\n", "tr_labels", "=", "train_labels", "[", "indices", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "classQuery", "=", "test_labels", "[", "counter", "]", "\n", "tr_labels", "=", "train_labels", "[", "indices", "]", "\n", "", "list_percPrecision", "=", "np", ".", "zeros", "(", "len", "(", "list_percRetrieval", ")", ")", "\n", "\n", "vec_goodLabel", "=", "compare_labels", "(", "tr_labels", ",", "classQuery", ",", "label_type", "=", "label_type", ",", "evaluation_type", "=", "evaluation", ",", "labels_to_count", "=", "labels_to_count", ")", "\n", "\n", "list_totalRetrievalCount", "=", "[", "]", "\n", "for", "frac", "in", "list_percRetrieval", ":", "\n", "                ", "list_totalRetrievalCount", ".", "append", "(", "np", ".", "floor", "(", "frac", "*", "kernel_matrix_test", ".", "shape", "[", "1", "]", ")", ")", "\n", "\n", "", "countGoodLabel", "=", "0", "\n", "for", "indexRetrieval", ",", "totalRetrievalCount", "in", "enumerate", "(", "list_totalRetrievalCount", ")", ":", "\n", "                ", "if", "indexRetrieval", "==", "0", ":", "\n", "                    ", "countGoodLabel", "+=", "np", ".", "sum", "(", "vec_goodLabel", "[", ":", "int", "(", "totalRetrievalCount", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "countGoodLabel", "+=", "np", ".", "sum", "(", "vec_goodLabel", "[", "int", "(", "lastTotalRetrievalCount", ")", ":", "int", "(", "totalRetrievalCount", ")", "]", ")", "\n", "\n", "", "list_percPrecision", "[", "indexRetrieval", "]", "=", "countGoodLabel", "/", "float", "(", "totalRetrievalCount", ")", "\n", "lastTotalRetrievalCount", "=", "totalRetrievalCount", "\n", "\n", "", "prec", "+=", "[", "list_percPrecision", "]", "# vec_simIndexSorted[:int(list_totalRetrievalCount[0])]", "\n", "\"\"\"\n            with open(IR_filename, \"a\") as f:\n                f.write(\"Query\\t::\\t\" + query_docs[counter] + \"\\n\\n\")\n                for index in indices[:int(kernel_matrix_test.shape[1] * 0.02)]:\n                    f.write(str(kernel_matrix_test[counter, index]) + \"\\t::\\t\" + corpus_docs[index] + \"\\n\")\n                f.write(\"\\n\\n\")\n            \"\"\"", "\n", "\n", "\n", "", "", "prec", "=", "np", ".", "mean", "(", "prec", ",", "axis", "=", "0", ")", "\n", "# print('prec:', prec)", "\n", "return", "prec", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_classification": [[144, 184], ["numpy.mean", "numpy.std", "sklearn.svm.SVC.fit", "sklearn.svm.SVC.predict", "sklearn.svm.SVC.predict", "sklearn.metrics.accuracy_score", "sklearn.metrics.accuracy_score", "test_acc.append", "val_acc.append", "test_f1.append", "val_f1.append", "numpy.vstack", "numpy.vstack", "sklearn.linear_model.LogisticRegression", "sklearn.metrics.precision_recall_fscore_support", "sklearn.metrics.precision_recall_fscore_support", "sklearn.svm.SVC"], "function", ["None"], ["", "def", "perform_classification", "(", "train_data", ",", "val_data", ",", "test_data", ",", "c_list", ",", "classification_model", "=", "\"logistic\"", ",", "norm_before_classification", "=", "False", ")", ":", "\n", "    ", "docVectors_train", ",", "train_labels", "=", "train_data", "\n", "docVectors_val", ",", "val_labels", "=", "val_data", "\n", "docVectors_test", ",", "test_labels", "=", "test_data", "\n", "\n", "if", "norm_before_classification", ":", "\n", "        ", "mean", "=", "np", ".", "mean", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_val", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_val", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "\n", "docVectors_train", "=", "(", "docVectors_train", "-", "mean", ")", "/", "std", "\n", "docVectors_val", "=", "(", "docVectors_val", "-", "mean", ")", "/", "std", "\n", "docVectors_test", "=", "(", "docVectors_test", "-", "mean", ")", "/", "std", "\n", "\n", "## Classification Accuracy", "\n", "", "test_acc", "=", "[", "]", "\n", "val_acc", "=", "[", "]", "\n", "test_f1", "=", "[", "]", "\n", "val_f1", "=", "[", "]", "\n", "\n", "for", "c", "in", "c_list", ":", "\n", "        ", "if", "classification_model", "==", "\"logistic\"", ":", "\n", "            ", "clf", "=", "LogisticRegression", "(", "C", "=", "c", ")", "\n", "", "elif", "classification_model", "==", "\"svm\"", ":", "\n", "            ", "clf", "=", "SVC", "(", "C", "=", "c", ",", "kernel", "=", "'precomputed'", ")", "\n", "\n", "", "clf", ".", "fit", "(", "docVectors_train", ",", "train_labels", ")", "\n", "pred_test_labels", "=", "clf", ".", "predict", "(", "docVectors_test", ")", "\n", "pred_val_labels", "=", "clf", ".", "predict", "(", "docVectors_val", ")", "\n", "\n", "acc_test", "=", "accuracy_score", "(", "test_labels", ",", "pred_test_labels", ")", "\n", "acc_val", "=", "accuracy_score", "(", "val_labels", ",", "pred_val_labels", ")", "\n", "f1_test", "=", "precision_recall_fscore_support", "(", "test_labels", ",", "pred_test_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "f1_val", "=", "precision_recall_fscore_support", "(", "val_labels", ",", "pred_val_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "\n", "test_acc", ".", "append", "(", "acc_test", ")", "\n", "val_acc", ".", "append", "(", "acc_val", ")", "\n", "test_f1", ".", "append", "(", "f1_test", ")", "\n", "val_f1", ".", "append", "(", "f1_val", ")", "\n", "\n", "", "return", "test_acc", ",", "test_f1", ",", "val_acc", ",", "val_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_classification_test": [[185, 216], ["numpy.mean", "numpy.std", "sklearn.svm.SVC.fit", "sklearn.svm.SVC.predict", "sklearn.metrics.accuracy_score", "test_acc.append", "test_f1.append", "numpy.vstack", "numpy.vstack", "sklearn.linear_model.LogisticRegression", "sklearn.metrics.precision_recall_fscore_support", "sklearn.svm.SVC"], "function", ["None"], ["", "def", "perform_classification_test", "(", "train_data", ",", "test_data", ",", "c_list", ",", "classification_model", "=", "\"logistic\"", ",", "norm_before_classification", "=", "False", ")", ":", "\n", "    ", "docVectors_train", ",", "train_labels", "=", "train_data", "\n", "docVectors_test", ",", "test_labels", "=", "test_data", "\n", "\n", "if", "norm_before_classification", ":", "\n", "        ", "mean", "=", "np", ".", "mean", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "\n", "docVectors_train", "=", "(", "docVectors_train", "-", "mean", ")", "/", "std", "\n", "docVectors_test", "=", "(", "docVectors_test", "-", "mean", ")", "/", "std", "\n", "\n", "## Classification Accuracy", "\n", "", "test_acc", "=", "[", "]", "\n", "test_f1", "=", "[", "]", "\n", "\n", "for", "c", "in", "c_list", ":", "\n", "        ", "if", "classification_model", "==", "\"logistic\"", ":", "\n", "            ", "clf", "=", "LogisticRegression", "(", "C", "=", "c", ")", "\n", "", "elif", "classification_model", "==", "\"svm\"", ":", "\n", "            ", "clf", "=", "SVC", "(", "C", "=", "c", ",", "kernel", "=", "'precomputed'", ")", "\n", "\n", "", "clf", ".", "fit", "(", "docVectors_train", ",", "train_labels", ")", "\n", "pred_test_labels", "=", "clf", ".", "predict", "(", "docVectors_test", ")", "\n", "\n", "acc_test", "=", "accuracy_score", "(", "test_labels", ",", "pred_test_labels", ")", "\n", "f1_test", "=", "precision_recall_fscore_support", "(", "test_labels", ",", "pred_test_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "\n", "test_acc", ".", "append", "(", "acc_test", ")", "\n", "test_f1", ".", "append", "(", "f1_test", ")", "\n", "\n", "", "return", "test_acc", ",", "test_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate_accuracy": [[217, 225], ["zip", "numpy.mean", "set().intersection", "accuracy.append", "accuracy.append", "set"], "function", ["None"], ["", "def", "evaluate_accuracy", "(", "true_labels", ",", "predicted_labels", ")", ":", "\n", "    ", "accuracy", "=", "[", "]", "\n", "for", "true", ",", "pred", "in", "zip", "(", "true_labels", ",", "predicted_labels", ")", ":", "\n", "        ", "if", "set", "(", "true", ")", ".", "intersection", "(", "pred", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "1.0", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "0.0", ")", "\n", "", "", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_classification_multi": [[226, 266], ["numpy.mean", "numpy.std", "sklearn.multiclass.OneVsRestClassifier.fit", "sklearn.multiclass.OneVsRestClassifier.predict", "sklearn.multiclass.OneVsRestClassifier.predict", "sklearn.metrics.accuracy_score", "sklearn.metrics.accuracy_score", "test_acc.append", "val_acc.append", "test_f1.append", "val_f1.append", "numpy.vstack", "numpy.vstack", "sklearn.multiclass.OneVsRestClassifier", "sklearn.metrics.precision_recall_fscore_support", "sklearn.metrics.precision_recall_fscore_support", "sklearn.linear_model.LogisticRegression", "sklearn.multiclass.OneVsRestClassifier", "sklearn.svm.SVC"], "function", ["None"], ["", "def", "perform_classification_multi", "(", "train_data", ",", "val_data", ",", "test_data", ",", "c_list", ",", "classification_model", "=", "\"logistic\"", ",", "norm_before_classification", "=", "False", ")", ":", "\n", "    ", "docVectors_train", ",", "train_labels", "=", "train_data", "\n", "docVectors_val", ",", "val_labels", "=", "val_data", "\n", "docVectors_test", ",", "test_labels", "=", "test_data", "\n", "\n", "if", "norm_before_classification", ":", "\n", "        ", "mean", "=", "np", ".", "mean", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_val", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_val", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "\n", "docVectors_train", "=", "(", "docVectors_train", "-", "mean", ")", "/", "std", "\n", "docVectors_val", "=", "(", "docVectors_val", "-", "mean", ")", "/", "std", "\n", "docVectors_test", "=", "(", "docVectors_test", "-", "mean", ")", "/", "std", "\n", "\n", "## Classification Accuracy", "\n", "", "test_acc", "=", "[", "]", "\n", "val_acc", "=", "[", "]", "\n", "test_f1", "=", "[", "]", "\n", "val_f1", "=", "[", "]", "\n", "\n", "for", "c", "in", "c_list", ":", "\n", "        ", "if", "classification_model", "==", "\"logistic\"", ":", "\n", "            ", "clf", "=", "OneVsRestClassifier", "(", "LogisticRegression", "(", "C", "=", "c", ")", ",", "n_jobs", "=", "5", ")", "\n", "", "elif", "classification_model", "==", "\"svm\"", ":", "\n", "            ", "clf", "=", "OneVsRestClassifier", "(", "SVC", "(", "C", "=", "c", ",", "kernel", "=", "'precomputed'", ")", ")", "\n", "\n", "", "clf", ".", "fit", "(", "docVectors_train", ",", "train_labels", ")", "\n", "pred_test_labels", "=", "clf", ".", "predict", "(", "docVectors_test", ")", "\n", "pred_val_labels", "=", "clf", ".", "predict", "(", "docVectors_val", ")", "\n", "\n", "acc_test", "=", "accuracy_score", "(", "test_labels", ",", "pred_test_labels", ")", "\n", "acc_val", "=", "accuracy_score", "(", "val_labels", ",", "pred_val_labels", ")", "\n", "f1_test", "=", "precision_recall_fscore_support", "(", "test_labels", ",", "pred_test_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "f1_val", "=", "precision_recall_fscore_support", "(", "val_labels", ",", "pred_val_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "\n", "test_acc", ".", "append", "(", "acc_test", ")", "\n", "val_acc", ".", "append", "(", "acc_val", ")", "\n", "test_f1", ".", "append", "(", "f1_test", ")", "\n", "val_f1", ".", "append", "(", "f1_val", ")", "\n", "\n", "", "return", "test_acc", ",", "test_f1", ",", "val_acc", ",", "val_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_classification_test_multi": [[267, 298], ["numpy.mean", "numpy.std", "sklearn.multiclass.OneVsRestClassifier.fit", "sklearn.multiclass.OneVsRestClassifier.predict", "sklearn.metrics.accuracy_score", "test_acc.append", "test_f1.append", "numpy.vstack", "numpy.vstack", "sklearn.multiclass.OneVsRestClassifier", "sklearn.metrics.precision_recall_fscore_support", "sklearn.linear_model.LogisticRegression", "sklearn.multiclass.OneVsRestClassifier", "sklearn.svm.SVC"], "function", ["None"], ["", "def", "perform_classification_test_multi", "(", "train_data", ",", "test_data", ",", "c_list", ",", "classification_model", "=", "\"logistic\"", ",", "norm_before_classification", "=", "False", ")", ":", "\n", "    ", "docVectors_train", ",", "train_labels", "=", "train_data", "\n", "docVectors_test", ",", "test_labels", "=", "test_data", "\n", "\n", "if", "norm_before_classification", ":", "\n", "        ", "mean", "=", "np", ".", "mean", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np", ".", "vstack", "(", "(", "docVectors_train", ",", "docVectors_test", ")", ")", ",", "axis", "=", "0", ")", "\n", "\n", "docVectors_train", "=", "(", "docVectors_train", "-", "mean", ")", "/", "std", "\n", "docVectors_test", "=", "(", "docVectors_test", "-", "mean", ")", "/", "std", "\n", "\n", "## Classification Accuracy", "\n", "", "test_acc", "=", "[", "]", "\n", "test_f1", "=", "[", "]", "\n", "\n", "for", "c", "in", "c_list", ":", "\n", "        ", "if", "classification_model", "==", "\"logistic\"", ":", "\n", "            ", "clf", "=", "OneVsRestClassifier", "(", "LogisticRegression", "(", "C", "=", "c", ")", ",", "n_jobs", "=", "5", ")", "\n", "", "elif", "classification_model", "==", "\"svm\"", ":", "\n", "            ", "clf", "=", "OneVsRestClassifier", "(", "SVC", "(", "C", "=", "c", ",", "kernel", "=", "'precomputed'", ")", ")", "\n", "\n", "", "clf", ".", "fit", "(", "docVectors_train", ",", "train_labels", ")", "\n", "pred_test_labels", "=", "clf", ".", "predict", "(", "docVectors_test", ")", "\n", "\n", "acc_test", "=", "accuracy_score", "(", "test_labels", ",", "pred_test_labels", ")", "\n", "f1_test", "=", "precision_recall_fscore_support", "(", "test_labels", ",", "pred_test_labels", ",", "pos_label", "=", "None", ",", "average", "=", "'macro'", ")", "[", "2", "]", "\n", "\n", "test_acc", ".", "append", "(", "acc_test", ")", "\n", "test_f1", ".", "append", "(", "f1_test", ")", "\n", "\n", "", "return", "test_acc", ",", "test_f1", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.closest_docs_by_index": [[300, 307], ["sklearn.cosine_similarity", "range", "numpy.array", "numpy.argsort", "len", "docs.append"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.cosine_similarity"], ["", "def", "closest_docs_by_index", "(", "corpus_vectors", ",", "query_vectors", ",", "n_docs", ")", ":", "\n", "    ", "docs", "=", "[", "]", "\n", "sim", "=", "pw", ".", "cosine_similarity", "(", "corpus_vectors", ",", "query_vectors", ")", "\n", "order", "=", "np", ".", "argsort", "(", "sim", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "query_vectors", ")", ")", ":", "\n", "        ", "docs", ".", "append", "(", "order", "[", ":", ",", "i", "]", "[", "0", ":", "n_docs", "]", ")", "\n", "", "return", "np", ".", "array", "(", "docs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.precision": [[309, 316], ["len", "float", "len", "len"], "function", ["None"], ["", "def", "precision", "(", "label", ",", "predictions", ")", ":", "\n", "    ", "if", "len", "(", "predictions", ")", ":", "\n", "        ", "return", "float", "(", "\n", "len", "(", "[", "x", "for", "x", "in", "predictions", "if", "label", "in", "x", "]", ")", "\n", ")", "/", "len", "(", "predictions", ")", "\n", "", "else", ":", "\n", "        ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate": [[318, 363], ["evaluate.get_one_hot_multi_labels", "evaluate.get_one_hot_multi_labels", "evaluate.perform_IR_prec", "len", "len", "sklearn.cosine_similarity", "len", "int", "evaluate.closest_docs_by_index", "range", "perform_IR_prec.append", "perform_IR_prec.append", "len", "evaluate.precision"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.get_one_hot_multi_labels", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.get_one_hot_multi_labels", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_IR_prec", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.cosine_similarity", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.closest_docs_by_index", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.precision"], ["", "", "def", "evaluate", "(", "\n", "corpus_vectors", ",", "\n", "query_vectors", ",", "\n", "corpus_labels", ",", "\n", "query_labels", ",", "\n", "recall", "=", "[", "0.02", "]", ",", "\n", "num_classes", "=", "None", ",", "\n", "multi_label", "=", "False", ",", "\n", "query_docs", "=", "None", ",", "\n", "corpus_docs", "=", "None", ",", "\n", "IR_filename", "=", "\"\"", "\n", ")", ":", "\n", "    ", "if", "multi_label", ":", "\n", "        ", "query_one_hot_labels", "=", "get_one_hot_multi_labels", "(", "query_labels", ",", "num_classes", ")", "\n", "corpus_one_hot_labels", "=", "get_one_hot_multi_labels", "(", "corpus_labels", ",", "num_classes", ")", "\n", "similarity_matrix", "=", "pw", ".", "cosine_similarity", "(", "corpus_vectors", ",", "query_vectors", ")", ".", "T", "\n", "if", "len", "(", "recall", ")", "==", "1", ":", "\n", "            ", "single_precision", "=", "True", "\n", "", "else", ":", "\n", "            ", "single_precision", "=", "False", "\n", "", "results", "=", "perform_IR_prec", "(", "similarity_matrix", ",", "corpus_one_hot_labels", ",", "query_one_hot_labels", ",", "list_percRetrieval", "=", "recall", ",", "single_precision", "=", "single_precision", ",", "label_type", "=", "\"multi\"", ",", "evaluation", "=", "\"relaxed\"", ",", "corpus_docs", "=", "corpus_docs", ",", "query_docs", "=", "query_docs", ",", "IR_filename", "=", "IR_filename", ")", "\n", "", "else", ":", "\n", "        ", "corpus_size", "=", "len", "(", "corpus_labels", ")", "\n", "query_size", "=", "len", "(", "query_labels", ")", "\n", "\n", "results", "=", "[", "]", "\n", "for", "r", "in", "recall", ":", "\n", "            ", "n_docs", "=", "int", "(", "(", "corpus_size", "*", "r", ")", "+", "0.5", ")", "\n", "if", "not", "n_docs", ":", "\n", "                ", "results", ".", "append", "(", "0.0", ")", "\n", "continue", "\n", "\n", "", "closest", "=", "closest_docs_by_index", "(", "corpus_vectors", ",", "query_vectors", ",", "n_docs", ")", "\n", "\n", "avg", "=", "0.0", "\n", "for", "i", "in", "range", "(", "query_size", ")", ":", "\n", "                ", "doc_labels", "=", "query_labels", "[", "i", "]", "\n", "doc_avg", "=", "0.0", "\n", "for", "label", "in", "doc_labels", ":", "\n", "                    ", "doc_avg", "+=", "precision", "(", "label", ",", "corpus_labels", "[", "closest", "[", "i", "]", "]", ")", "\n", "", "doc_avg", "/=", "len", "(", "doc_labels", ")", "\n", "avg", "+=", "doc_avg", "\n", "", "avg", "/=", "query_size", "\n", "results", ".", "append", "(", "avg", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.evaluate_write": [[366, 410], ["evaluate.get_one_hot_multi_labels", "evaluate.get_one_hot_multi_labels", "evaluate.perform_IR_prec", "len", "len", "evaluate.closest_docs_by_index", "sklearn.cosine_similarity", "len", "open", "range", "evaluate.precision", "f.write", "enumerate", "f.write", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.get_one_hot_multi_labels", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.get_one_hot_multi_labels", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.perform_IR_prec", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.closest_docs_by_index", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.None.train_model.cosine_similarity", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.evaluate.precision"], ["", "def", "evaluate_write", "(", "\n", "corpus_vectors", ",", "\n", "query_vectors", ",", "\n", "corpus_labels", ",", "\n", "query_labels", ",", "\n", "corpus_docs", ",", "\n", "query_docs", ",", "\n", "recall", "=", "3", ",", "\n", "num_classes", "=", "None", ",", "\n", "multi_label", "=", "False", "\n", ")", ":", "\n", "    ", "if", "multi_label", ":", "\n", "        ", "query_one_hot_labels", "=", "get_one_hot_multi_labels", "(", "query_labels", ",", "num_classes", ")", "\n", "corpus_one_hot_labels", "=", "get_one_hot_multi_labels", "(", "corpus_labels", ",", "num_classes", ")", "\n", "similarity_matrix", "=", "pw", ".", "cosine_similarity", "(", "corpus_vectors", ",", "query_vectors", ")", ".", "T", "\n", "if", "len", "(", "recall", ")", "==", "1", ":", "\n", "            ", "single_precision", "=", "True", "\n", "", "else", ":", "\n", "            ", "single_precision", "=", "False", "\n", "", "results", "=", "perform_IR_prec", "(", "similarity_matrix", ",", "corpus_one_hot_labels", ",", "query_one_hot_labels", ",", "list_percRetrieval", "=", "recall", ",", "single_precision", "=", "single_precision", ",", "label_type", "=", "\"multi\"", ",", "evaluation", "=", "\"relaxed\"", ")", "\n", "", "else", ":", "\n", "        ", "corpus_size", "=", "len", "(", "corpus_labels", ")", "\n", "query_size", "=", "len", "(", "query_labels", ")", "\n", "\n", "results", "=", "[", "]", "\n", "#n_docs = int((corpus_size * r) + 0.5)", "\n", "\n", "closest", "=", "closest_docs_by_index", "(", "corpus_vectors", ",", "query_vectors", ",", "recall", ")", "\n", "\n", "with", "open", "(", "\"query_IR_top_3.txt\"", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "i", "in", "range", "(", "query_size", ")", ":", "\n", "                ", "doc_labels", "=", "query_labels", "[", "i", "]", "\n", "doc_prec", "=", "precision", "(", "doc_labels", "[", "0", "]", ",", "corpus_labels", "[", "closest", "[", "i", "]", "]", ")", "\n", "closest_docs", "=", "[", "corpus_docs", "[", "i", "]", "for", "i", "in", "closest", "[", "i", "]", "]", "\n", "closest_docs_labels", "=", "[", "corpus_labels", "[", "i", "]", "for", "i", "in", "closest", "[", "i", "]", "]", "\n", "\n", "f", ".", "write", "(", "\"\\n\\nPrecision : \"", "+", "str", "(", "doc_prec", ")", "+", "\" <==> \"", "+", "str", "(", "doc_labels", "[", "0", "]", ")", "+", "\" :: \"", "+", "query_docs", "[", "i", "]", ")", "\n", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "closest_docs", ")", ":", "\n", "                    ", "label", "=", "closest_docs_labels", "[", "i", "]", "\n", "f", ".", "write", "(", "\"\\n\"", "+", "str", "(", "label", ")", "+", "\" :: \"", "+", "doc", ")", "\n", "\n", "\n", "", "", "", "", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.__init__": [[51, 55], ["glob.glob", "data.file_name"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.file_name"], ["def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "path", "=", "path", "\n", "files", "=", "glob", ".", "glob", "(", "path", "+", "'/*.csv'", ")", "\n", "self", ".", "collections", "=", "{", "file_name", "(", "file", ")", ":", "file", "for", "file", "in", "files", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows": [[56, 70], ["ValueError", "open", "csv.reader"], "methods", ["None"], ["", "def", "rows", "(", "self", ",", "collection_name", ",", "num_epochs", "=", "None", ")", ":", "\n", "        ", "if", "collection_name", "not", "in", "self", ".", "collections", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Collection not found: {}'", ".", "format", "(", "collection_name", ")", "\n", ")", "\n", "", "epoch", "=", "0", "\n", "while", "True", ":", "\n", "            ", "with", "open", "(", "self", ".", "collections", "[", "collection_name", "]", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "r", "=", "csv", ".", "reader", "(", "f", ")", "\n", "for", "row", "in", "r", ":", "\n", "                    ", "yield", "row", "\n", "", "", "epoch", "+=", "1", "\n", "if", "num_epochs", "and", "(", "epoch", ">=", "num_epochs", ")", ":", "\n", "                ", "raise", "StopIteration", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset._batch_iter": [[71, 74], ["itertools.zip_longest", "data.Dataset.rows"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.rows"], ["", "", "", "def", "_batch_iter", "(", "self", ",", "collection_name", ",", "batch_size", ",", "num_epochs", ")", ":", "\n", "        ", "gen", "=", "[", "self", ".", "rows", "(", "collection_name", ",", "num_epochs", ")", "]", "*", "batch_size", "\n", "return", "itertools", ".", "zip_longest", "(", "fillvalue", "=", "None", ",", "*", "gen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches": [[76, 85], ["data.Dataset._batch_iter", "zip", "data.format_row", "keras.pad_sequences", "keras.pad_sequences", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset._batch_iter", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.format_row"], ["", "def", "batches", "(", "self", ",", "collection_name", ",", "batch_size", ",", "num_epochs", "=", "None", ",", "shuffle", "=", "True", ",", "max_len", "=", "None", ",", "multilabel", "=", "False", ")", ":", "\n", "        ", "for", "batch", "in", "self", ".", "_batch_iter", "(", "collection_name", ",", "batch_size", ",", "num_epochs", ")", ":", "\n", "            ", "data", "=", "[", "format_row", "(", "row", ",", "shuffle", ",", "multilabel", ")", "for", "row", "in", "batch", "if", "row", "]", "\n", "y", ",", "x", ",", "seq_lengths", "=", "zip", "(", "*", "data", ")", "\n", "if", "not", "max_len", "is", "None", ":", "\n", "                ", "x", "=", "pp", ".", "pad_sequences", "(", "x", ",", "maxlen", "=", "max_len", ",", "padding", "=", "'post'", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "pp", ".", "pad_sequences", "(", "x", ",", "padding", "=", "'post'", ")", "\n", "", "yield", "np", ".", "array", "(", "y", ")", ",", "x", ",", "np", ".", "array", "(", "seq_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset.batches_bidirectional": [[87, 98], ["data.Dataset._batch_iter", "zip", "data.format_row_bidirectional", "keras.pad_sequences", "keras.pad_sequences", "keras.pad_sequences", "keras.pad_sequences", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.Dataset._batch_iter", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.format_row_bidirectional"], ["", "", "def", "batches_bidirectional", "(", "self", ",", "collection_name", ",", "batch_size", ",", "num_epochs", "=", "None", ",", "shuffle", "=", "True", ",", "max_len", "=", "None", ",", "multilabel", "=", "False", ")", ":", "\n", "        ", "for", "batch", "in", "self", ".", "_batch_iter", "(", "collection_name", ",", "batch_size", ",", "num_epochs", ")", ":", "\n", "            ", "data", "=", "[", "format_row_bidirectional", "(", "row", ",", "shuffle", ",", "multilabel", ")", "for", "row", "in", "batch", "if", "row", "]", "\n", "y", ",", "x", ",", "x_rev", ",", "seq_lengths", "=", "zip", "(", "*", "data", ")", "\n", "if", "not", "max_len", "is", "None", ":", "\n", "                ", "x", "=", "pp", ".", "pad_sequences", "(", "x", ",", "maxlen", "=", "max_len", ",", "padding", "=", "'post'", ")", "\n", "x_rev", "=", "pp", ".", "pad_sequences", "(", "x_rev", ",", "maxlen", "=", "max_len", ",", "padding", "=", "'post'", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "pp", ".", "pad_sequences", "(", "x", ",", "padding", "=", "'post'", ")", "\n", "x_rev", "=", "pp", ".", "pad_sequences", "(", "x_rev", ",", "padding", "=", "'post'", ")", "\n", "", "yield", "np", ".", "array", "(", "y", ")", ",", "x", ",", "x_rev", ",", "np", ".", "array", "(", "seq_lengths", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.file_name": [[18, 20], ["os.path.basename().replace", "os.path.basename"], "function", ["None"], ["def", "file_name", "(", "abs_path", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "basename", "(", "abs_path", ")", ".", "replace", "(", "'.csv'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.format_row": [[22, 32], ["numpy.array", "numpy.random.shuffle", "int", "len", "int", "len", "raw_x.split"], "function", ["None"], ["", "def", "format_row", "(", "row", ",", "shuffle", "=", "True", ",", "multilabel", "=", "False", ")", ":", "\n", "    ", "y", ",", "raw_x", "=", "row", "\n", "x", "=", "np", ".", "array", "(", "[", "int", "(", "v", ")", "for", "v", "in", "raw_x", ".", "split", "(", ")", "]", ")", "\n", "if", "shuffle", ":", "\n", "        ", "np", ".", "random", ".", "shuffle", "(", "x", ")", "\n", "\n", "", "if", "multilabel", ":", "\n", "        ", "return", "[", "y", ",", "x", ",", "len", "(", "x", ")", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "int", "(", "y", ")", ",", "x", ",", "len", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.data.format_row_bidirectional": [[34, 45], ["numpy.array", "numpy.random.shuffle", "int", "len", "int", "len", "raw_x.split"], "function", ["None"], ["", "", "def", "format_row_bidirectional", "(", "row", ",", "shuffle", "=", "True", ",", "multilabel", "=", "False", ")", ":", "\n", "    ", "y", ",", "raw_x", "=", "row", "\n", "x", "=", "np", ".", "array", "(", "[", "int", "(", "v", ")", "for", "v", "in", "raw_x", ".", "split", "(", ")", "]", ")", "\n", "if", "shuffle", ":", "\n", "        ", "np", ".", "random", ".", "shuffle", "(", "x", ")", "\n", "", "x_rev", "=", "x", "[", ":", ":", "-", "1", "]", "\n", "\n", "if", "multilabel", ":", "\n", "        ", "return", "[", "y", ",", "x", ",", "x_rev", ",", "len", "(", "x", ")", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "int", "(", "y", ")", ",", "x", ",", "x_rev", ",", "len", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.DocNADE.__init__": [[241, 482], ["tensorflow.shape", "tensorflow.get_variable", "tensorflow.scan", "tensorflow.transpose", "tensorflow.concat", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.reshape", "model.masked_sequence_cross_entropy_loss", "tensorflow.Variable", "model.gradients", "tensorflow.shape", "tensorflow.device", "tensorflow.nn.embedding_lookup", "enumerate", "tensorflow.transpose", "tensorflow.sigmoid", "range", "model.linear", "model.linear", "tensorflow.get_variable", "tensorflow.transpose", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.add", "tensorflow.constant_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "W_list.append", "bias_list.append", "tensorflow.zeros", "tensorflow.tanh", "tensorflow.range", "tensorflow.to_int32", "len", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.nn.sampled_softmax_loss", "tensorflow.train.AdamOptimizer", "tensorflow.trainable_variables", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.scalar_mul", "tensorflow.nn.relu", "print", "exit", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.random_uniform_initializer", "str", "tensorflow.random_uniform_initializer", "str", "tensorflow.constant_initializer", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.gradients", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear"], ["def", "__init__", "(", "self", ",", "x", ",", "y", ",", "seq_lengths", ",", "params", ",", "\n", "W_initializer", "=", "None", ",", "lambda_embeddings", "=", "-", "1.0", ",", "\n", "W_pretrained", "=", "None", ")", ":", "\n", "        ", "'''\n        x:           input of shape [batch_size X max document length] contains word indices\n        y:           document labels\n        seq_lengths: actual sequence lengths of documents in the padded matrix input x\n        params:      training comfiguration parameters\n        '''", "\n", "self", ".", "x", "=", "x", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "seq_lengths", "=", "seq_lengths", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "self", ".", "b_s", "=", "tf", ".", "shape", "(", "x", ")", "\n", "self", ".", "initializer_embeddings", "=", "W_initializer", "\n", "self", ".", "lambda_embeddings", "=", "lambda_embeddings", "\n", "\n", "# Create W (embedding matrix) of DocNADE", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "            ", "if", "W_pretrained", "is", "None", ":", "\n", "                ", "max_embed_init", "=", "1.0", "/", "(", "params", ".", "vocab_size", "*", "params", ".", "hidden_size", ")", "\n", "W", "=", "tf", ".", "get_variable", "(", "\n", "'embedding'", ",", "\n", "[", "params", ".", "vocab_size", ",", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "W", "=", "tf", ".", "get_variable", "(", "\n", "'embedding'", ",", "\n", "initializer", "=", "W_pretrained", "\n", ")", "\n", "\n", "# Do an embedding lookup for each word in each sequence", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x", ")", "\n", "\n", "# Create embedding prior matrix", "\n", "if", "not", "W_initializer", "is", "None", ":", "\n", "                ", "W_prior", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_prior'", ",", "\n", "initializer", "=", "W_initializer", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x", ")", "\n", "W_prior_shape", "=", "W_initializer", ".", "shape", "\n", "\n", "# Create embedding prior projection matrix", "\n", "if", "params", ".", "projection", ":", "\n", "                    ", "max_embed_init", "=", "1.0", "/", "(", "W_prior_shape", "[", "1", "]", ".", "value", "*", "params", ".", "hidden_size", ")", "\n", "W_prior_proj", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_prior_projection'", ",", "\n", "[", "W_prior_shape", "[", "1", "]", ".", "value", ",", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "embeddings_prior_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "reshape", "(", "embeddings_prior_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", ".", "hidden_size", "]", ")", "\n", "\n", "# Lambda multiplication", "\n", "", "if", "not", "self", ".", "lambda_embeddings", "<", "0.0", ":", "\n", "                    ", "self", ".", "embeddings_prior", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "add", "(", "self", ".", "embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "\n", "# Create bias vector of DocNADE", "\n", "", "", "bias", "=", "tf", ".", "get_variable", "(", "\n", "'bias'", ",", "\n", "[", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "\n", "################################## Deep DocNADE Parameters ##################################", "\n", "\n", "# List of deep projection matrices", "\n", "W_list", "=", "[", "]", "\n", "\n", "# List of deep bias vectors", "\n", "bias_list", "=", "[", "]", "\n", "\n", "if", "params", ".", "deep", ":", "\n", "            ", "in_size", "=", "params", ".", "hidden_size", "\n", "for", "index", ",", "size", "in", "enumerate", "(", "params", ".", "deep_hidden_sizes", ")", ":", "\n", "                ", "out_size", "=", "size", "\n", "max_embed_init", "=", "1.0", "/", "(", "in_size", "*", "out_size", ")", "\n", "W_temp", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_'", "+", "str", "(", "index", ")", ",", "\n", "[", "in_size", ",", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "bias_temp", "=", "tf", ".", "get_variable", "(", "\n", "'bias_'", "+", "str", "(", "index", ")", ",", "\n", "[", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "W_list", ".", "append", "(", "W_temp", ")", "\n", "bias_list", ".", "append", "(", "bias_temp", ")", "\n", "in_size", "=", "out_size", "\n", "\n", "#############################################################################################", "\n", "\n", "# Compute the hidden layer inputs: each gets summed embeddings of", "\n", "# previous words in an autoregressive manner", "\n", "", "", "def", "sum_embeddings", "(", "previous", ",", "current", ")", ":", "\n", "            ", "return", "previous", "+", "current", "\n", "\n", "", "h", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "h", "=", "tf", ".", "transpose", "(", "h", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "# add initial zero vector to each sequence, will then generate the", "\n", "# first element using just the bias term", "\n", "h", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", ".", "hidden_size", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "pre_act", "=", "h", "\n", "\n", "# Apply activation", "\n", "if", "params", ".", "activation", "==", "'sigmoid'", ":", "\n", "            ", "h", "=", "tf", ".", "sigmoid", "(", "h", "+", "bias", ")", "\n", "", "elif", "params", ".", "activation", "==", "'tanh'", ":", "\n", "            ", "h", "=", "tf", ".", "tanh", "(", "h", "+", "bias", ")", "\n", "", "elif", "params", ".", "activation", "==", "'relu'", ":", "\n", "            ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", "+", "bias", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Invalid value for activation: %s'", "%", "(", "params", ".", "activation", ")", ")", "\n", "exit", "(", ")", "\n", "", "self", ".", "aft_act", "=", "h", "\n", "\n", "# Extract final state for each sequence (document representation) in the batch", "\n", "indices", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "indices", "=", "indices", "\n", "self", ".", "h", "=", "tf", ".", "gather_nd", "(", "h", ",", "indices", ")", "\n", "\n", "h", "=", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "h", "=", "tf", ".", "reshape", "(", "h", ",", "[", "-", "1", ",", "params", ".", "hidden_size", "]", ")", "\n", "\n", "################ Deep DocNADE forward propagation ###################", "\n", "\n", "if", "params", ".", "deep", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "params", ".", "deep_hidden_sizes", ")", ")", ":", "\n", "                ", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "if", "params", ".", "activation", "==", "'sigmoid'", ":", "\n", "                    ", "h", "=", "tf", ".", "sigmoid", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "sigmoid", "(", "self", ".", "h", ")", "\n", "", "elif", "params", ".", "activation", "==", "'tanh'", ":", "\n", "                    ", "h", "=", "tf", ".", "tanh", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "tanh", "(", "self", ".", "h", ")", "\n", "", "else", ":", "\n", "                    ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h", ")", "\n", "\n", "###################### Softmax logits ###############################", "\n", "\n", "", "", "", "if", "not", "params", ".", "num_samples", ":", "\n", "# Applying full softmax projection of word hidden vectors", "\n", "            ", "self", ".", "logits", ",", "_", "=", "linear", "(", "h", ",", "params", ".", "vocab_size", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ")", "\n", "loss_function", "=", "None", "\n", "", "else", ":", "\n", "# Applying sampled softmax projection of word hidden vectors", "\n", "            ", "self", ".", "logits", ",", "_", "=", "linear", "(", "h", ",", "params", ".", "num_samples", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ")", "\n", "if", "W_initializer", "is", "None", ":", "\n", "                ", "max_embed_init", "=", "1.0", "/", "(", "params", ".", "vocab_size", "*", "params", ".", "hidden_size", ")", "\n", "w_t", "=", "tf", ".", "get_variable", "(", "\n", "\"proj_w_t\"", ",", "\n", "[", "params", ".", "vocab_size", ",", "params", ".", "num_samples", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "w_t", "=", "tf", ".", "get_variable", "(", "\n", "\"proj_w_t\"", ",", "\n", "initializer", "=", "W_initializer", "\n", ")", "\n", "", "b", "=", "tf", ".", "get_variable", "(", "\"proj_b\"", ",", "[", "params", ".", "vocab_size", "]", ")", "\n", "self", ".", "proj_w", "=", "tf", ".", "transpose", "(", "w_t", ")", "\n", "self", ".", "proj_b", "=", "b", "\n", "\n", "def", "sampled_loss", "(", "logits", ",", "labels", ")", ":", "\n", "                ", "labels", "=", "tf", ".", "reshape", "(", "labels", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "local_w_t", "=", "tf", ".", "cast", "(", "w_t", ",", "tf", ".", "float32", ")", "\n", "local_b", "=", "tf", ".", "cast", "(", "b", ",", "tf", ".", "float32", ")", "\n", "local_inputs", "=", "tf", ".", "cast", "(", "logits", ",", "tf", ".", "float32", ")", "\n", "return", "tf", ".", "nn", ".", "sampled_softmax_loss", "(", "\n", "weights", "=", "local_w_t", ",", "\n", "biases", "=", "local_b", ",", "\n", "labels", "=", "labels", ",", "\n", "inputs", "=", "local_inputs", ",", "\n", "num_sampled", "=", "params", ".", "num_samples", ",", "\n", "num_classes", "=", "params", ".", "vocab_size", ",", "\n", "partition_strategy", "=", "'div'", "\n", ")", "\n", "", "loss_function", "=", "sampled_loss", "\n", "\n", "", "\"\"\"\n        # Compute the loss. If using sampled softmax for training, use full\n        # softmax for evaluation and validation\n        if not params.num_samples:\n            self.loss = masked_sequence_cross_entropy_loss(\n                x,\n                seq_lengths,\n                self.logits\n            )\n        else:\n            projected_logits = \\\n                tf.matmul(self.logits, self.proj_w) + self.proj_b\n            self.loss = masked_sequence_cross_entropy_loss(\n                x,\n                seq_lengths,\n                projected_logits\n            )\n        \"\"\"", "\n", "\n", "# Calculates masked cross entropy loss", "\n", "self", ".", "loss_normed", ",", "self", ".", "labels", ",", "self", ".", "mask", ",", "self", ".", "loss_unnormed", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", "\n", "\n", "#Total DocNADE loss", "\n", "self", ".", "total_loss", "=", "self", ".", "loss_unnormed", "\n", "\n", "# Optimiser object", "\n", "step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "self", ".", "opt", "=", "gradients", "(", "\n", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "params", ".", "learning_rate", ")", ",", "\n", "loss", "=", "self", ".", "total_loss", ",", "\n", "vars", "=", "tf", ".", "trainable_variables", "(", ")", ",", "\n", "step", "=", "step", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.iDocNADE.__init__": [[489, 804], ["tensorflow.shape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.scan", "tensorflow.scan", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.concat", "tensorflow.concat", "tensorflow.stack", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.concat", "tensorflow.add", "model.masked_sequence_cross_entropy_loss", "model.masked_sequence_cross_entropy_loss", "tensorflow.Variable", "model.gradients", "tensorflow.shape", "tensorflow.device", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "enumerate", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.sigmoid", "tensorflow.sigmoid", "range", "model.linear", "model.linear", "tensorflow.get_variable", "tensorflow.transpose", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.add", "tensorflow.add", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "W_list.append", "bias_list.append", "bias_bw_list.append", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.range", "tensorflow.to_int32", "tensorflow.range", "tensorflow.to_int32", "len", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.nn.sampled_softmax_loss", "tensorflow.train.AdamOptimizer", "tensorflow.trainable_variables", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.scalar_mul", "tensorflow.scalar_mul", "tensorflow.nn.relu", "tensorflow.nn.relu", "print", "exit", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.random_uniform_initializer", "str", "tensorflow.random_uniform_initializer", "str", "tensorflow.constant_initializer", "str", "tensorflow.constant_initializer", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.gradients", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear"], ["def", "__init__", "(", "self", ",", "x", ",", "x_bw", ",", "y", ",", "seq_lengths", ",", "params", ",", "\n", "W_initializer", "=", "None", ",", "lambda_embeddings", "=", "-", "1.0", ",", "\n", "W_pretrained", "=", "None", ")", ":", "\n", "        ", "'''\n        x:           input of shape [batch_size X max document length] contains word indices\n        x_bw:        input of shape [batch_size X max document length] contains word indices in reverse order of x\n        y:           document labels\n        seq_lengths: actual sequence lengths of documents in the padded matrix input x\n        params:      training comfiguration parameters\n        '''", "\n", "self", ".", "x", "=", "x", "\n", "self", ".", "x_bw", "=", "x_bw", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "seq_lengths", "=", "seq_lengths", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "self", ".", "b_s", "=", "tf", ".", "shape", "(", "x", ")", "\n", "self", ".", "initializer_embeddings", "=", "W_initializer", "\n", "self", ".", "lambda_embeddings", "=", "lambda_embeddings", "\n", "\n", "# Create W (embedding matrix) of iDocNADE", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "            ", "if", "W_pretrained", "is", "None", ":", "\n", "                ", "max_embed_init", "=", "1.0", "/", "(", "params", ".", "vocab_size", "*", "params", ".", "hidden_size", ")", "\n", "W", "=", "tf", ".", "get_variable", "(", "\n", "'embedding'", ",", "\n", "[", "params", ".", "vocab_size", ",", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "W", "=", "tf", ".", "get_variable", "(", "\n", "'embedding'", ",", "\n", "initializer", "=", "W_pretrained", "\n", ")", "\n", "\n", "# Do an embedding lookup for each word in each forward and backward sequence", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x", ")", "\n", "self", ".", "embeddings_bw", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x_bw", ")", "\n", "\n", "# Create embedding prior matrix", "\n", "if", "not", "W_initializer", "is", "None", ":", "\n", "                ", "W_prior", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_prior'", ",", "\n", "initializer", "=", "W_initializer", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x_bw", ")", "\n", "W_prior_shape", "=", "W_initializer", ".", "shape", "\n", "\n", "# Create embedding prior projection matrix", "\n", "if", "params", ".", "projection", ":", "\n", "                    ", "max_embed_init", "=", "1.0", "/", "(", "W_prior_shape", "[", "1", "]", ".", "value", "*", "params", ".", "hidden_size", ")", "\n", "W_prior_proj", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_prior_projection'", ",", "\n", "[", "W_prior_shape", "[", "1", "]", ".", "value", ",", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "embeddings_prior_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "reshape", "(", "embeddings_prior_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", ".", "hidden_size", "]", ")", "\n", "\n", "embeddings_prior_bw_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior_bw", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_bw_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_bw_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "reshape", "(", "embeddings_prior_bw_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", ".", "hidden_size", "]", ")", "\n", "\n", "# Lambda multiplication", "\n", "", "if", "not", "self", ".", "lambda_embeddings", "<", "0.0", ":", "\n", "                    ", "self", ".", "embeddings_prior", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior_bw", ")", "\n", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "add", "(", "self", ".", "embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "self", ".", "embeddings_bw", "=", "tf", ".", "add", "(", "self", ".", "embeddings_bw", ",", "self", ".", "embeddings_prior_bw", ")", "\n", "\n", "# Create forward bias vector of iDocNADE", "\n", "", "", "bias", "=", "tf", ".", "get_variable", "(", "\n", "'bias'", ",", "\n", "[", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "\n", "# Create backward bias vector of iDocNADE", "\n", "bias_bw", "=", "tf", ".", "get_variable", "(", "\n", "'bias_bw'", ",", "\n", "[", "params", ".", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "\n", "################################## Deep DocNADE Parameters ##################################", "\n", "\n", "# List of deep projection matrices", "\n", "W_list", "=", "[", "]", "\n", "\n", "# List of deep forward bias vectors", "\n", "bias_list", "=", "[", "]", "\n", "\n", "# List of deep backward bias vectors", "\n", "bias_bw_list", "=", "[", "]", "\n", "\n", "if", "params", ".", "deep", ":", "\n", "            ", "in_size", "=", "params", ".", "hidden_size", "\n", "for", "index", ",", "size", "in", "enumerate", "(", "params", ".", "deep_hidden_sizes", ")", ":", "\n", "                ", "out_size", "=", "size", "\n", "max_embed_init", "=", "1.0", "/", "(", "in_size", "*", "out_size", ")", "\n", "W_temp", "=", "tf", ".", "get_variable", "(", "\n", "'embedding_'", "+", "str", "(", "index", ")", ",", "\n", "[", "in_size", ",", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", "\n", ")", "\n", ")", "\n", "bias_temp", "=", "tf", ".", "get_variable", "(", "\n", "'bias_'", "+", "str", "(", "index", ")", ",", "\n", "[", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "bias_bw_temp", "=", "tf", ".", "get_variable", "(", "\n", "'bias_bw_'", "+", "str", "(", "index", ")", ",", "\n", "[", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0", ")", "\n", ")", "\n", "W_list", ".", "append", "(", "W_temp", ")", "\n", "bias_list", ".", "append", "(", "bias_temp", ")", "\n", "bias_bw_list", ".", "append", "(", "bias_bw_temp", ")", "\n", "in_size", "=", "out_size", "\n", "\n", "#############################################################################################", "\n", "\n", "# Compute the hidden layer inputs: each gets summed embeddings of", "\n", "# previous words", "\n", "", "", "def", "sum_embeddings", "(", "previous", ",", "current", ")", ":", "\n", "            ", "return", "previous", "+", "current", "\n", "\n", "", "h", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "h_bw", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings_bw", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "\n", "h", "=", "tf", ".", "transpose", "(", "h", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "h_bw", "=", "tf", ".", "transpose", "(", "h_bw", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "# add initial zero vector to each sequence, will then generate the", "\n", "# first element using just the bias term", "\n", "h", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", ".", "hidden_size", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "h_bw", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", ".", "hidden_size", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h_bw", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "pre_act", "=", "h", "\n", "self", ".", "pre_act_bw", "=", "h_bw", "\n", "\n", "# Apply activation", "\n", "if", "params", ".", "activation", "==", "'sigmoid'", ":", "\n", "            ", "h", "=", "tf", ".", "sigmoid", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "sigmoid", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "elif", "params", ".", "activation", "==", "'tanh'", ":", "\n", "            ", "h", "=", "tf", ".", "tanh", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "tanh", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "elif", "params", ".", "activation", "==", "'relu'", ":", "\n", "            ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Invalid value for activation: %s'", "%", "(", "params", ".", "activation", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "self", ".", "aft_act", "=", "h", "\n", "self", ".", "aft_act_bw", "=", "h_bw", "\n", "\n", "# Extract final state for each forward sequence (document representation) in the batch", "\n", "indices", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Extract final state for each backward sequence (document representation) in the batch", "\n", "indices_bw", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "indices", "=", "indices", "\n", "self", ".", "indices_bw", "=", "indices_bw", "\n", "\n", "self", ".", "h", "=", "tf", ".", "gather_nd", "(", "h", ",", "indices", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "gather_nd", "(", "h_bw", ",", "indices_bw", ")", "\n", "\n", "h", "=", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "h_bw", "=", "h_bw", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "h", "=", "tf", ".", "reshape", "(", "h", ",", "[", "-", "1", ",", "params", ".", "hidden_size", "]", ")", "\n", "h_bw", "=", "tf", ".", "reshape", "(", "h_bw", ",", "[", "-", "1", ",", "params", ".", "hidden_size", "]", ")", "\n", "\n", "self", ".", "h_comb_concat", "=", "tf", ".", "concat", "(", "[", "\n", "self", ".", "h", ",", "self", ".", "h_bw", "\n", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "h_comb_sum", "=", "tf", ".", "add", "(", "self", ".", "h", ",", "self", ".", "h_bw", ")", "\n", "\n", "################ Deep network forward propagation ###################", "\n", "\n", "if", "params", ".", "deep", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "params", ".", "deep_hidden_sizes", ")", ")", ":", "\n", "                ", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h_bw", ",", "W_list", "[", "i", "]", ",", "bias_bw_list", "[", "i", "]", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h_bw", ",", "W_list", "[", "i", "]", ",", "bias_bw_list", "[", "i", "]", ")", "\n", "if", "params", ".", "activation", "==", "'sigmoid'", ":", "\n", "                    ", "h", "=", "tf", ".", "sigmoid", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "sigmoid", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "sigmoid", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "sigmoid", "(", "self", ".", "h_bw", ")", "\n", "", "elif", "params", ".", "activation", "==", "'tanh'", ":", "\n", "                    ", "h", "=", "tf", ".", "tanh", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "tanh", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "tanh", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "tanh", "(", "self", ".", "h_bw", ")", "\n", "", "else", ":", "\n", "                    ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h_bw", ")", "\n", "\n", "###################### Softmax logits ###############################", "\n", "\n", "", "", "", "if", "not", "params", ".", "num_samples", ":", "\n", "# Applying full softmax projection of forward and backward word hidden vectors", "\n", "            ", "self", ".", "logits", ",", "self", ".", "logits_bw", "=", "linear", "(", "h", ",", "params", ".", "vocab_size", ",", "input_bw", "=", "h_bw", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ")", "\n", "loss_function", "=", "None", "\n", "", "else", ":", "\n", "# Applying sampled softmax projection of forward and backward word hidden vectors", "\n", "            ", "self", ".", "logits", ",", "self", ".", "logits_bw", "=", "linear", "(", "h", ",", "params", ".", "num_samples", ",", "input_bw", "=", "h_bw", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ")", "\n", "\n", "if", "W_initializer", "is", "None", ":", "\n", "                ", "max_embed_init", "=", "1.0", "/", "(", "params", ".", "vocab_size", "*", "params", ".", "hidden_size", ")", "\n", "w_t", "=", "tf", ".", "get_variable", "(", "\n", "\"proj_w_t\"", ",", "\n", "[", "params", ".", "vocab_size", ",", "params", ".", "num_samples", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "maxval", "=", "max_embed_init", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "w_t", "=", "tf", ".", "get_variable", "(", "\n", "\"proj_w_t\"", ",", "\n", "initializer", "=", "W_initializer", "\n", ")", "\n", "", "b", "=", "tf", ".", "get_variable", "(", "\"proj_b\"", ",", "[", "params", ".", "vocab_size", "]", ")", "\n", "self", ".", "proj_w", "=", "tf", ".", "transpose", "(", "w_t", ")", "\n", "self", ".", "proj_b", "=", "b", "\n", "\n", "def", "sampled_loss", "(", "logits", ",", "labels", ")", ":", "\n", "                ", "labels", "=", "tf", ".", "reshape", "(", "labels", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "local_w_t", "=", "tf", ".", "cast", "(", "w_t", ",", "tf", ".", "float32", ")", "\n", "local_b", "=", "tf", ".", "cast", "(", "b", ",", "tf", ".", "float32", ")", "\n", "local_inputs", "=", "tf", ".", "cast", "(", "logits", ",", "tf", ".", "float32", ")", "\n", "return", "tf", ".", "nn", ".", "sampled_softmax_loss", "(", "\n", "weights", "=", "local_w_t", ",", "\n", "biases", "=", "local_b", ",", "\n", "labels", "=", "labels", ",", "\n", "inputs", "=", "local_inputs", ",", "\n", "num_sampled", "=", "params", ".", "num_samples", ",", "\n", "num_classes", "=", "params", ".", "vocab_size", ",", "\n", "partition_strategy", "=", "'div'", "\n", ")", "\n", "", "loss_function", "=", "sampled_loss", "\n", "", "\"\"\"\n        # Compute the loss. If using sampled softmax for training, use full\n        # softmax for evaluation and validation\n        if not params.num_samples:\n            self.loss = masked_sequence_cross_entropy_loss(\n                x,\n                seq_lengths,\n                self.logits\n            )\n        else:\n            projected_logits = \\\n                tf.matmul(self.logits, self.proj_w) + self.proj_b\n            self.loss = masked_sequence_cross_entropy_loss(\n                x,\n                seq_lengths,\n                projected_logits\n            )\n        \"\"\"", "\n", "\n", "# Calculates masked cross entropy loss for forward network", "\n", "self", ".", "loss_normed", ",", "self", ".", "labels", ",", "self", ".", "mask", ",", "self", ".", "loss_unnormed", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", "\n", "\n", "# Calculates masked cross entropy loss for backward network", "\n", "self", ".", "loss_normed_bw", ",", "_", ",", "_", ",", "self", ".", "loss_unnormed_bw", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x_bw", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits_bw", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", "\n", "\n", "# Total iDocNADE (bidirectional DocNADE) loss", "\n", "self", ".", "total_loss", "=", "0.5", "*", "(", "self", ".", "loss_unnormed", "+", "self", ".", "loss_unnormed_bw", ")", "\n", "\n", "# Optimiser object", "\n", "step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "self", ".", "opt", "=", "gradients", "(", "\n", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "params", ".", "learning_rate", ")", ",", "\n", "loss", "=", "self", ".", "total_loss", ",", "\n", "vars", "=", "tf", ".", "trainable_variables", "(", ")", ",", "\n", "step", "=", "step", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.DocNADE_reload.__init__": [[812, 956], ["tensorflow.shape", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.Variable", "tensorflow.scan", "tensorflow.transpose", "tensorflow.concat", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.reshape", "model.linear_reload", "model.masked_sequence_cross_entropy_loss", "tensorflow.shape", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.add", "enumerate", "tensorflow.transpose", "tensorflow.sigmoid", "range", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.scalar_mul", "tensorflow.Variable", "tensorflow.Variable", "W_list.append", "bias_list.append", "tensorflow.zeros", "tensorflow.tanh", "tensorflow.range", "tensorflow.to_int32", "len", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.relu", "print", "exit", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.relu", "tensorflow.nn.relu"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear_reload", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss"], ["def", "__init__", "(", "self", ",", "x", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "None", ",", "\n", "W_reload", "=", "None", ",", "W_prior_reload", "=", "None", ",", "W_prior_proj_reload", "=", "None", ",", "bias_reload", "=", "None", ",", "\n", "bias_bw_reload", "=", "None", ",", "V_reload", "=", "None", ",", "b_reload", "=", "None", ",", "b_bw_reload", "=", "None", ",", "\n", "W_list_reload", "=", "None", ",", "bias_list_reload", "=", "None", ",", "lambda_embeddings", "=", "None", ")", ":", "\n", "        ", "self", ".", "x", "=", "x", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "seq_lengths", "=", "seq_lengths", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "self", ".", "b_s", "=", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "self", ".", "V", "=", "V_reload", "\n", "self", ".", "b", "=", "b_reload", "\n", "self", ".", "W", "=", "W_reload", "\n", "\n", "self", ".", "lambda_embeddings", "=", "lambda_embeddings", "\n", "\n", "# Reload W (embedding matrix) of DocNADE", "\n", "W", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "# Do an embedding lookup for each word in each sequence", "\n", "self", ".", "embeddings", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x", ")", "\n", "\n", "# Reload embedding prior matrix", "\n", "if", "not", "W_initializer", "is", "None", ":", "\n", "            ", "W_prior", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_prior_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x", ")", "\n", "W_prior_shape", "=", "W_initializer", ".", "shape", "\n", "\n", "# Reload embedding prior projection matrix", "\n", "if", "params", "[", "'projection'", "]", ":", "\n", "                ", "W_prior_proj", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_prior_proj_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "embeddings_prior_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "reshape", "(", "embeddings_prior_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "\n", "# Lambda multiplication", "\n", "", "if", "not", "self", ".", "lambda_embeddings", "<", "0.0", ":", "\n", "                ", "self", ".", "embeddings_prior", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "add", "(", "self", ".", "embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "\n", "# Reload bias vector of DocNADE", "\n", "", "bias", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "########################## Deep DocNADE Parameters ##########################", "\n", "\n", "W_list", "=", "[", "]", "\n", "bias_list", "=", "[", "]", "\n", "\n", "if", "params", "[", "'deep'", "]", ":", "\n", "            ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                ", "W_temp", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_list_reload", "[", "index", "]", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "bias_temp", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_list_reload", "[", "index", "]", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "W_list", ".", "append", "(", "W_temp", ")", "\n", "bias_list", ".", "append", "(", "bias_temp", ")", "\n", "\n", "#############################################################################", "\n", "\n", "# Compute the hidden layer inputs: each gets summed embeddings of", "\n", "# previous words", "\n", "", "", "def", "sum_embeddings", "(", "previous", ",", "current", ")", ":", "\n", "            ", "return", "previous", "+", "current", "\n", "\n", "", "h", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "h", "=", "tf", ".", "transpose", "(", "h", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "# add initial zero vector to each sequence, will then generate the", "\n", "# first element using just the bias term", "\n", "h", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", "[", "'hidden_size'", "]", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h", "\n", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pre_act", "=", "h", "\n", "\n", "# Apply activation", "\n", "if", "params", "[", "'activation'", "]", "==", "'sigmoid'", ":", "\n", "            ", "h", "=", "tf", ".", "sigmoid", "(", "h", "+", "bias", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "            ", "h", "=", "tf", ".", "tanh", "(", "h", "+", "bias", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'relu'", ":", "\n", "            ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", "+", "bias", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Invalid value for activation: %s'", "%", "(", "params", "[", "'activation'", "]", ")", ")", "\n", "exit", "(", ")", "\n", "", "self", ".", "aft_act", "=", "h", "\n", "\n", "# Extract final state for each sequence in the batch", "\n", "indices", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "indices", "=", "indices", "\n", "self", ".", "h", "=", "tf", ".", "gather_nd", "(", "h", ",", "indices", ")", "\n", "\n", "h", "=", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "h", "=", "tf", ".", "reshape", "(", "h", ",", "[", "-", "1", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "\n", "################ Deep network forward propagation ###################", "\n", "\n", "if", "params", "[", "'deep'", "]", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ")", ":", "\n", "                ", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "if", "params", "[", "'activation'", "]", "==", "'sigmoid'", ":", "\n", "                    ", "h", "=", "tf", ".", "sigmoid", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "sigmoid", "(", "self", ".", "h", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "                    ", "h", "=", "tf", ".", "tanh", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "tanh", "(", "self", ".", "h", ")", "\n", "", "else", ":", "\n", "                    ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h", ")", "\n", "\n", "###################### Softmax logits ###############################", "\n", "\n", "", "", "", "self", ".", "logits", ",", "_", "=", "linear_reload", "(", "h", ",", "params", "[", "'vocab_size'", "]", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ",", "\n", "V_reload", "=", "V_reload", ",", "b_reload", "=", "b_reload", ")", "\n", "loss_function", "=", "None", "\n", "\n", "# Calculates masked cross entropy loss", "\n", "self", ".", "loss_normed", ",", "self", ".", "labels", ",", "self", ".", "mask", ",", "self", ".", "loss_unnormed", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.iDocNADE_reload.__init__": [[963, 1176], ["tensorflow.shape", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.scan", "tensorflow.scan", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.concat", "tensorflow.concat", "tensorflow.stack", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.concat", "tensorflow.add", "model.linear_reload", "model.masked_sequence_cross_entropy_loss", "model.masked_sequence_cross_entropy_loss", "tensorflow.shape", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.add", "tensorflow.add", "enumerate", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.sigmoid", "tensorflow.sigmoid", "range", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.scalar_mul", "tensorflow.scalar_mul", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.Variable", "W_list.append", "bias_list.append", "bias_bw_list.append", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.range", "tensorflow.to_int32", "tensorflow.range", "tensorflow.to_int32", "len", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.relu", "tensorflow.nn.relu", "print", "exit", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.nn.relu", "tensorflow.nn.relu"], "methods", ["home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear_reload", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss", "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss"], ["def", "__init__", "(", "self", ",", "x", ",", "x_bw", ",", "y", ",", "seq_lengths", ",", "params", ",", "W_initializer", "=", "None", ",", "\n", "W_reload", "=", "None", ",", "W_prior_reload", "=", "None", ",", "W_prior_proj_reload", "=", "None", ",", "bias_reload", "=", "None", ",", "\n", "bias_bw_reload", "=", "None", ",", "V_reload", "=", "None", ",", "b_reload", "=", "None", ",", "b_bw_reload", "=", "None", ",", "\n", "W_list_reload", "=", "[", "]", ",", "bias_list_reload", "=", "[", "]", ",", "bias_bw_list_reload", "=", "[", "]", ",", "lambda_embeddings", "=", "None", ")", ":", "\n", "        ", "self", ".", "x", "=", "x", "\n", "self", ".", "x_bw", "=", "x_bw", "\n", "self", ".", "y", "=", "y", "\n", "self", ".", "seq_lengths", "=", "seq_lengths", "\n", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "self", ".", "b_s", "=", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "self", ".", "V", "=", "V_reload", "\n", "self", ".", "b", "=", "b_reload", "\n", "self", ".", "W", "=", "W_reload", "\n", "\n", "self", ".", "lambda_embeddings", "=", "lambda_embeddings", "\n", "\n", "# Reload W (embedding matrix) of iDocNADE", "\n", "W", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "# Do an embedding lookup for each word in each forward and backward sequence", "\n", "self", ".", "embeddings", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x", ")", "\n", "self", ".", "embeddings_bw", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "x_bw", ")", "\n", "\n", "# Reload embedding prior matrix", "\n", "if", "not", "W_initializer", "is", "None", ":", "\n", "            ", "W_prior", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_prior_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_prior", ",", "x_bw", ")", "\n", "W_prior_shape", "=", "W_initializer", ".", "shape", "\n", "\n", "# Reload embedding prior projection matrix", "\n", "if", "params", "[", "'projection'", "]", ":", "\n", "                ", "W_prior_proj", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_prior_proj_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "embeddings_prior_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior", "=", "tf", ".", "reshape", "(", "embeddings_prior_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "\n", "embeddings_prior_bw_reshape", "=", "tf", ".", "reshape", "(", "self", ".", "embeddings_prior_bw", ",", "[", "-", "1", ",", "W_prior_shape", "[", "1", "]", ".", "value", "]", ")", "\n", "embeddings_prior_bw_projected", "=", "tf", ".", "matmul", "(", "embeddings_prior_bw_reshape", ",", "W_prior_proj", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "reshape", "(", "embeddings_prior_bw_projected", ",", "[", "self", ".", "b_s", "[", "0", "]", ",", "self", ".", "b_s", "[", "1", "]", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "\n", "# Lambda multiplication", "\n", "", "if", "not", "self", ".", "lambda_embeddings", "<", "0.0", ":", "\n", "                ", "self", ".", "embeddings_prior", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "self", ".", "embeddings_prior_bw", "=", "tf", ".", "scalar_mul", "(", "self", ".", "lambda_embeddings", ",", "self", ".", "embeddings_prior_bw", ")", "\n", "\n", "", "self", ".", "embeddings", "=", "tf", ".", "add", "(", "self", ".", "embeddings", ",", "self", ".", "embeddings_prior", ")", "\n", "self", ".", "embeddings_bw", "=", "tf", ".", "add", "(", "self", ".", "embeddings_bw", ",", "self", ".", "embeddings_prior_bw", ")", "\n", "\n", "# Reload forward bias vector of iDocNADE", "\n", "", "bias", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "# Reload backward bias vector of iDocNADE", "\n", "bias_bw", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_bw_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "######################### Deep DocNADE Parameters ######################", "\n", "W_list", "=", "[", "]", "\n", "bias_list", "=", "[", "]", "\n", "bias_bw_list", "=", "[", "]", "\n", "\n", "if", "params", "[", "'deep'", "]", ":", "\n", "            ", "for", "index", ",", "size", "in", "enumerate", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ":", "\n", "                ", "W_temp", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "W_list_reload", "[", "index", "]", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "bias_temp", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_list_reload", "[", "index", "]", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "bias_bw_temp", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "bias_bw_list_reload", "[", "index", "]", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "W_list", ".", "append", "(", "W_temp", ")", "\n", "bias_list", ".", "append", "(", "bias_temp", ")", "\n", "bias_bw_list", ".", "append", "(", "bias_bw_temp", ")", "\n", "\n", "#########################################################################", "\n", "\n", "# Compute the hidden layer inputs: each gets summed embeddings of", "\n", "# previous words", "\n", "", "", "def", "sum_embeddings", "(", "previous", ",", "current", ")", ":", "\n", "            ", "return", "previous", "+", "current", "\n", "\n", "", "h", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "h_bw", "=", "tf", ".", "scan", "(", "sum_embeddings", ",", "tf", ".", "transpose", "(", "self", ".", "embeddings_bw", ",", "[", "1", ",", "2", ",", "0", "]", ")", ")", "\n", "\n", "h", "=", "tf", ".", "transpose", "(", "h", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "h_bw", "=", "tf", ".", "transpose", "(", "h_bw", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "# add initial zero vector to each sequence, will then generate the", "\n", "# first element using just the bias term", "\n", "h", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", "[", "'hidden_size'", "]", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "h_bw", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", ",", "params", "[", "'hidden_size'", "]", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "h_bw", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "pre_act", "=", "h", "\n", "self", ".", "pre_act_bw", "=", "h_bw", "\n", "\n", "# Apply activation", "\n", "if", "params", "[", "'activation'", "]", "==", "'sigmoid'", ":", "\n", "            ", "h", "=", "tf", ".", "sigmoid", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "sigmoid", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "            ", "h", "=", "tf", ".", "tanh", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "tanh", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'relu'", ":", "\n", "            ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", "+", "bias", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "h_bw", "+", "bias_bw", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Invalid value for activation: %s'", "%", "(", "params", "[", "'activation'", "]", ")", ")", "\n", "exit", "(", ")", "\n", "\n", "", "self", ".", "aft_act", "=", "h", "\n", "self", ".", "aft_act_bw", "=", "h_bw", "\n", "\n", "# Extract final state for each forward sequence (document representation) in the batch", "\n", "indices", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Extract final state for each backward sequence (document representation) in the batch", "\n", "indices_bw", "=", "tf", ".", "stack", "(", "[", "\n", "tf", ".", "range", "(", "batch_size", ")", ",", "\n", "tf", ".", "to_int32", "(", "seq_lengths", ")", "\n", "]", ",", "axis", "=", "1", ")", "\n", "\n", "self", ".", "indices", "=", "indices", "\n", "self", ".", "indices_bw", "=", "indices_bw", "\n", "\n", "self", ".", "h", "=", "tf", ".", "gather_nd", "(", "h", ",", "indices", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "gather_nd", "(", "h_bw", ",", "indices_bw", ")", "\n", "\n", "h", "=", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "h_bw", "=", "h_bw", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "\n", "h", "=", "tf", ".", "reshape", "(", "h", ",", "[", "-", "1", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "h_bw", "=", "tf", ".", "reshape", "(", "h_bw", ",", "[", "-", "1", ",", "params", "[", "'hidden_size'", "]", "]", ")", "\n", "\n", "self", ".", "h_comb_concat", "=", "tf", ".", "concat", "(", "[", "\n", "self", ".", "h", ",", "self", ".", "h_bw", "\n", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "h_comb_sum", "=", "tf", ".", "add", "(", "self", ".", "h", ",", "self", ".", "h_bw", ")", "\n", "\n", "################ Deep network forward propagation ###################", "\n", "\n", "if", "params", "[", "'deep'", "]", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "params", "[", "'deep_hidden_sizes'", "]", ")", ")", ":", "\n", "                ", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "h_bw", ",", "W_list", "[", "i", "]", ",", "bias_bw_list", "[", "i", "]", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h", ",", "W_list", "[", "i", "]", ",", "bias_list", "[", "i", "]", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "h_bw", ",", "W_list", "[", "i", "]", ",", "bias_bw_list", "[", "i", "]", ")", "\n", "if", "params", "[", "'activation'", "]", "==", "'sigmoid'", ":", "\n", "                    ", "h", "=", "tf", ".", "sigmoid", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "sigmoid", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "sigmoid", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "sigmoid", "(", "self", ".", "h_bw", ")", "\n", "", "elif", "params", "[", "'activation'", "]", "==", "'tanh'", ":", "\n", "                    ", "h", "=", "tf", ".", "tanh", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "tanh", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "tanh", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "tanh", "(", "self", ".", "h_bw", ")", "\n", "", "else", ":", "\n", "                    ", "h", "=", "tf", ".", "nn", ".", "relu", "(", "h", ")", "\n", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "h_bw", ")", "\n", "self", ".", "h", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h", ")", "\n", "self", ".", "h_bw", "=", "tf", ".", "nn", ".", "relu", "(", "self", ".", "h_bw", ")", "\n", "\n", "###################### Softmax logits ###############################", "\n", "\n", "", "", "", "self", ".", "logits", ",", "self", ".", "logits_bw", "=", "linear_reload", "(", "h", ",", "params", "[", "'vocab_size'", "]", ",", "input_bw", "=", "h_bw", ",", "scope", "=", "'softmax'", ",", "W_initializer", "=", "None", ",", "\n", "V_reload", "=", "V_reload", ",", "b_reload", "=", "b_reload", ",", "b_bw_reload", "=", "b_bw_reload", ")", "\n", "loss_function", "=", "None", "\n", "\n", "# Calculates masked cross entropy loss for forward network", "\n", "self", ".", "loss_normed", ",", "self", ".", "labels", ",", "self", ".", "mask", ",", "self", ".", "loss_unnormed", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", "\n", "\n", "# Calculates masked cross entropy loss for backward network", "\n", "self", ".", "loss_normed_bw", ",", "_", ",", "_", ",", "self", ".", "loss_unnormed_bw", "=", "masked_sequence_cross_entropy_loss", "(", "\n", "x_bw", ",", "\n", "seq_lengths", ",", "\n", "self", ".", "logits_bw", ",", "\n", "loss_function", "=", "loss_function", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", ""]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors": [[11, 28], ["numpy.array", "vecs.extend", "session.run"], "function", ["None"], ["def", "vectors", "(", "model", ",", "data", ",", "session", ")", ":", "\n", "    ", "'''\n    Function for getting document hidden representation for DocNADE\n\n    model:   DocNADE model instance\n    data:    data object instance\n    session: tensorflow session object\n    '''", "\n", "vecs", "=", "[", "]", "\n", "for", "_", ",", "x", ",", "seq_lengths", "in", "data", ":", "\n", "        ", "vecs", ".", "extend", "(", "\n", "session", ".", "run", "(", "[", "model", ".", "h", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "[", "0", "]", "\n", ")", "\n", "", "return", "np", ".", "array", "(", "vecs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.vectors_bidirectional": [[29, 60], ["numpy.array", "vecs.extend", "print", "vecs.extend", "session.run", "session.run"], "function", ["None"], ["", "def", "vectors_bidirectional", "(", "model", ",", "data", ",", "session", ",", "combination_type", ")", ":", "\n", "    ", "'''\n    Function for getting document hidden representation for iDocNADE\n\n    model:            DocNADE model instance\n    data:             data object instance\n    session:          tensorflow session object\n    combination_type: parameter for combining forward and backward hidden representation for a document\n    '''", "\n", "vecs", "=", "[", "]", "\n", "if", "combination_type", "==", "\"concat\"", ":", "\n", "        ", "for", "_", ",", "x", ",", "x_bw", ",", "seq_lengths", "in", "data", ":", "\n", "            ", "vecs", ".", "extend", "(", "\n", "session", ".", "run", "(", "[", "model", ".", "h_comb_concat", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "x_bw", ":", "x_bw", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "[", "0", "]", "\n", ")", "\n", "", "", "elif", "combination_type", "==", "\"sum\"", ":", "\n", "        ", "for", "_", ",", "x", ",", "x_bw", ",", "seq_lengths", "in", "data", ":", "\n", "            ", "vecs", ".", "extend", "(", "\n", "session", ".", "run", "(", "[", "model", ".", "h_comb_sum", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "x_bw", ":", "x_bw", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "[", "0", "]", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"vectors function: Invalid value for combination_type.\"", ")", "\n", "", "return", "np", ".", "array", "(", "vecs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.loss": [[62, 72], ["loss.append", "sum", "len", "session.run"], "function", ["None"], ["", "def", "loss", "(", "model", ",", "data", ",", "session", ")", ":", "\n", "    ", "loss", "=", "[", "]", "\n", "for", "_", ",", "x", ",", "seq_lengths", "in", "data", ":", "\n", "        ", "loss", ".", "append", "(", "\n", "session", ".", "run", "(", "[", "model", ".", "loss", "]", ",", "feed_dict", "=", "{", "\n", "model", ".", "x", ":", "x", ",", "\n", "model", ".", "seq_lengths", ":", "seq_lengths", "\n", "}", ")", "[", "0", "]", "\n", ")", "\n", "", "return", "sum", "(", "loss", ")", "/", "len", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.gradients": [[74, 106], ["opt.compute_gradients", "opt.apply_gradients", "zip", "tensorflow.python.ops.clip_ops.clip_by_global_norm", "isinstance", "list", "print", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "zip", "tensorflow.python.ops.clip_ops.global_norm"], "function", ["None"], ["", "def", "gradients", "(", "opt", ",", "loss", ",", "vars", ",", "step", ",", "max_gradient_norm", "=", "None", ",", "dont_clip", "=", "[", "]", ")", ":", "\n", "    ", "'''\n    Function for calculating and applying gradients on all trainable parameters\n    '''", "\n", "gradients", "=", "opt", ".", "compute_gradients", "(", "loss", ",", "vars", ")", "\n", "if", "max_gradient_norm", "is", "not", "None", ":", "\n", "        ", "to_clip", "=", "[", "(", "g", ",", "v", ")", "for", "g", ",", "v", "in", "gradients", "if", "v", ".", "name", "not", "in", "dont_clip", "]", "\n", "not_clipped", "=", "[", "(", "g", ",", "v", ")", "for", "g", ",", "v", "in", "gradients", "if", "v", ".", "name", "in", "dont_clip", "]", "\n", "gradients", ",", "variables", "=", "zip", "(", "*", "to_clip", ")", "\n", "clipped_gradients", ",", "_", "=", "clip_ops", ".", "clip_by_global_norm", "(", "\n", "gradients", ",", "\n", "max_gradient_norm", "\n", ")", "\n", "gradients", "=", "list", "(", "zip", "(", "clipped_gradients", ",", "variables", ")", ")", "+", "not_clipped", "\n", "\n", "# Add histograms for variables, gradients and gradient norms", "\n", "", "for", "gradient", ",", "variable", "in", "gradients", ":", "\n", "        ", "if", "isinstance", "(", "gradient", ",", "ops", ".", "IndexedSlices", ")", ":", "\n", "            ", "grad_values", "=", "gradient", ".", "values", "\n", "", "else", ":", "\n", "            ", "grad_values", "=", "gradient", "\n", "", "if", "grad_values", "is", "None", ":", "\n", "            ", "print", "(", "'warning: missing gradient: {}'", ".", "format", "(", "variable", ".", "name", ")", ")", "\n", "", "if", "grad_values", "is", "not", "None", ":", "\n", "            ", "tf", ".", "summary", ".", "histogram", "(", "variable", ".", "name", ",", "variable", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "variable", ".", "name", "+", "'/gradients'", ",", "grad_values", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\n", "variable", ".", "name", "+", "'/gradient_norm'", ",", "\n", "clip_ops", ".", "global_norm", "(", "[", "grad_values", "]", ")", "\n", ")", "\n", "\n", "", "", "return", "opt", ".", "apply_gradients", "(", "gradients", ",", "global_step", "=", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear": [[108, 150], ["tensorflow.constant_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.random_normal_initializer", "tensorflow.random_normal_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.transpose", "numpy.sqrt", "input.get_shape", "input.get_shape"], "function", ["None"], ["", "def", "linear", "(", "input", ",", "output_dim", ",", "input_bw", "=", "None", ",", "scope", "=", "None", ",", "stddev", "=", "None", ",", "W_initializer", "=", "None", ")", ":", "\n", "    ", "'''\n    Function to generate softmax distribution over vocabulary for a document during training.\n\n    input:         input batch of documents\n    output_dim:    vocabulary size\n    input_bw:      input batch of documents for backward direction\n    W_initializer: initializer for softmax projection matrix V\n    '''", "\n", "const", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", "\n", "\n", "if", "W_initializer", "is", "None", ":", "\n", "        ", "if", "stddev", ":", "\n", "            ", "norm", "=", "tf", ".", "random_normal_initializer", "(", "stddev", "=", "stddev", ")", "\n", "", "else", ":", "\n", "            ", "norm", "=", "tf", ".", "random_normal_initializer", "(", "\n", "stddev", "=", "np", ".", "sqrt", "(", "2.0", "/", "input", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ")", "\n", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", "or", "'linear'", ")", ":", "\n", "            ", "w", "=", "tf", ".", "get_variable", "(", "\n", "'w'", ",", "\n", "[", "input", ".", "get_shape", "(", ")", "[", "1", "]", ",", "output_dim", "]", ",", "\n", "initializer", "=", "norm", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "w", "=", "tf", ".", "get_variable", "(", "\n", "'w'", ",", "\n", "initializer", "=", "tf", ".", "transpose", "(", "W_initializer", ")", "\n", ")", "\n", "\n", "", "b", "=", "tf", ".", "get_variable", "(", "'b'", ",", "[", "output_dim", "]", ",", "initializer", "=", "const", ")", "\n", "b_bw", "=", "tf", ".", "get_variable", "(", "'b_bw'", ",", "[", "output_dim", "]", ",", "initializer", "=", "const", ")", "\n", "\n", "input_bw_logits", "=", "None", "\n", "if", "input_bw", "is", "None", ":", "\n", "        ", "input_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input", ",", "w", ",", "b", ")", "\n", "", "else", ":", "\n", "        ", "input_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input", ",", "w", ",", "b", ")", "\n", "input_bw_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input_bw", ",", "w", ",", "b_bw", ")", "\n", "\n", "", "return", "input_logits", ",", "input_bw_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.linear_reload": [[152, 188], ["tensorflow.Variable", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b", "tensorflow.nn.xw_plus_b"], "function", ["None"], ["", "def", "linear_reload", "(", "input", ",", "output_dim", ",", "input_bw", "=", "None", ",", "scope", "=", "None", ",", "stddev", "=", "None", ",", "W_initializer", "=", "None", ",", "\n", "V_reload", "=", "None", ",", "b_reload", "=", "None", ",", "b_bw_reload", "=", "None", ")", ":", "\n", "    ", "'''\n    Function to generate softmax distribution over vocabulary for a document during model reload.\n\n    input:         input batch of documents\n    output_dim:    vocabulary size\n    input_bw:      input batch of documents for backward direction\n    V_reload:      parameter to initialize the V matrix (softmax logits)\n    b_reload:      parameter to initialize the b bias (softmax logits)\n    '''", "\n", "w", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "V_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "b", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "b_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "if", "input_bw", "is", "None", ":", "\n", "        ", "b_bw", "=", "None", "\n", "", "else", ":", "\n", "        ", "b_bw", "=", "tf", ".", "Variable", "(", "\n", "initial_value", "=", "b_bw_reload", ",", "\n", "trainable", "=", "False", "\n", ")", "\n", "\n", "", "input_bw_logits", "=", "None", "\n", "if", "input_bw", "is", "None", ":", "\n", "        ", "input_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input", ",", "w", ",", "b", ")", "\n", "", "else", ":", "\n", "        ", "input_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input", ",", "w", ",", "b", ")", "\n", "input_bw_logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "input_bw", ",", "w", ",", "b_bw", ")", "\n", "\n", "", "return", "input_logits", ",", "input_bw_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.pgcool_iDocNADEe.model.model.masked_sequence_cross_entropy_loss": [[190, 235], ["tensorflow.reshape", "tensorflow.reduce_max", "tensorflow.less", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.range", "tensorflow.reshape", "tensorflow.where", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "loss_function", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.to_float"], "function", ["None"], ["", "def", "masked_sequence_cross_entropy_loss", "(", "\n", "x", ",", "\n", "seq_lengths", ",", "\n", "logits", ",", "\n", "loss_function", "=", "None", ",", "\n", "norm_by_seq_lengths", "=", "True", "\n", ")", ":", "\n", "    ", "'''\n    Compute the cross-entropy loss between all elements in x and logits.\n    Masks out the loss for all positions greater than the sequence\n    length (as we expect that sequences may be padded).\n\n    Optionally, also either use a different loss function (eg: sampled\n    softmax), and/or normalise the loss for each sequence by the\n    sequence length.\n    '''", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "labels", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", "]", ")", "\n", "\n", "max_doc_length", "=", "tf", ".", "reduce_max", "(", "seq_lengths", ")", "\n", "mask", "=", "tf", ".", "less", "(", "\n", "tf", ".", "range", "(", "0", ",", "max_doc_length", ",", "1", ")", ",", "\n", "tf", ".", "reshape", "(", "seq_lengths", ",", "[", "batch_size", ",", "1", "]", ")", "\n", ")", "\n", "mask", "=", "tf", ".", "reshape", "(", "mask", ",", "[", "-", "1", "]", ")", "\n", "mask", "=", "tf", ".", "to_float", "(", "tf", ".", "where", "(", "\n", "mask", ",", "\n", "tf", ".", "ones_like", "(", "labels", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "\n", "tf", ".", "zeros_like", "(", "labels", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", ")", ")", "\n", "\n", "if", "loss_function", "is", "None", ":", "\n", "        ", "loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "logits", ",", "\n", "labels", "=", "labels", "\n", ")", "\n", "", "else", ":", "\n", "        ", "loss", "=", "loss_function", "(", "logits", ",", "labels", ")", "\n", "", "loss", "*=", "mask", "\n", "loss", "=", "tf", ".", "reshape", "(", "loss", ",", "[", "batch_size", ",", "-", "1", "]", ")", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "loss", ",", "axis", "=", "1", ")", "\n", "loss_unnormed", "=", "loss", "\n", "if", "norm_by_seq_lengths", ":", "\n", "        ", "loss", "=", "loss", "/", "tf", ".", "to_float", "(", "seq_lengths", ")", "\n", "", "return", "tf", ".", "reduce_mean", "(", "loss", ")", ",", "labels", ",", "mask", ",", "tf", ".", "reduce_mean", "(", "loss_unnormed", ")", "\n", "\n"]]}