{"home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.mypath.Path.db_dir": [[2, 22], ["print"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "db_dir", "(", "database", ")", ":", "\n", "        ", "if", "database", "==", "'ucf101'", ":", "\n", "# folder that contains class labels", "\n", "            ", "root_dir", "=", "'/Path/to/UCF-101'", "\n", "\n", "# Save preprocess data into output_dir", "\n", "output_dir", "=", "'/path/to/VAR/ucf101'", "\n", "\n", "return", "root_dir", ",", "output_dir", "\n", "", "elif", "database", "==", "'hmdb51'", ":", "\n", "# folder that contains class labels", "\n", "            ", "root_dir", "=", "'/Path/to/hmdb-51'", "\n", "\n", "output_dir", "=", "'/path/to/VAR/hmdb51'", "\n", "\n", "return", "root_dir", ",", "output_dir", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Database {} not available.'", ".", "format", "(", "database", ")", ")", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.mypath.Path.model_dir": [[23, 26], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "model_dir", "(", ")", ":", "\n", "        ", "return", "'/path/to/Models/c3d-pretrained.pth'", "", "", "", ""]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.inference.CenterCrop": [[7, 15], ["int", "int", "numpy.array().astype", "numpy.shape", "round", "round", "numpy.array"], "function", ["None"], ["def", "CenterCrop", "(", "frame", ",", "size", ")", ":", "\n", "    ", "h", ",", "w", "=", "np", ".", "shape", "(", "frame", ")", "[", "0", ":", "2", "]", "\n", "th", ",", "tw", "=", "size", "\n", "x1", "=", "int", "(", "round", "(", "(", "w", "-", "tw", ")", "/", "2.", ")", ")", "\n", "y1", "=", "int", "(", "round", "(", "(", "h", "-", "th", ")", "/", "2.", ")", ")", "\n", "\n", "frame", "=", "frame", "[", "y1", ":", "y1", "+", "th", ",", "x1", ":", "x1", "+", "tw", ",", ":", "]", "\n", "return", "np", ".", "array", "(", "frame", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.inference.center_crop": [[17, 20], ["numpy.array().astype", "numpy.array"], "function", ["None"], ["", "def", "center_crop", "(", "frame", ")", ":", "\n", "    ", "frame", "=", "frame", "[", "8", ":", "120", ",", "30", ":", "142", ",", ":", "]", "\n", "return", "np", ".", "array", "(", "frame", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.inference.main": [[22, 74], ["torch.device", "print", "network.C3D_model.C3D", "torch.load", "C3D_model.C3D.load_state_dict", "C3D_model.C3D.to", "C3D_model.C3D.eval", "cv2.VideoCapture", "cv2.VideoCapture.release", "cv2.destroyAllWindows", "open", "f.readlines", "f.close", "cv2.VideoCapture.read", "inference.center_crop", "clip.append", "cv2.imshow", "cv2.waitKey", "torch.cuda.is_available", "cv2.resize", "numpy.array", "len", "numpy.array().astype", "numpy.expand_dims", "numpy.transpose", "torch.from_numpy", "torch.autograd.Variable().to", "cv2.putText", "cv2.putText", "clip.pop", "torch.no_grad", "C3D_model.C3D.forward", "torch.nn.Softmax", "[].detach().cpu().numpy", "[].strip", "numpy.array", "torch.autograd.Variable", "[].detach().cpu", "class_names[].split", "[].detach", "torch.max"], "function", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.inference.center_crop", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.forward"], ["", "def", "main", "(", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "print", "(", "\"Device being used:\"", ",", "device", ")", "\n", "\n", "with", "open", "(", "'./dataloaders/ucf_labels.txt'", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "class_names", "=", "f", ".", "readlines", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "# init model", "\n", "", "model", "=", "C3D_model", ".", "C3D", "(", "num_classes", "=", "101", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "'run/run_1/models/C3D_ucf101_epoch-39.pth.tar'", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# read video", "\n", "video", "=", "'/Path/to/UCF-101/ApplyLipstick/v_ApplyLipstick_g04_c02.avi'", "\n", "cap", "=", "cv2", ".", "VideoCapture", "(", "video", ")", "\n", "retaining", "=", "True", "\n", "\n", "clip", "=", "[", "]", "\n", "while", "retaining", ":", "\n", "        ", "retaining", ",", "frame", "=", "cap", ".", "read", "(", ")", "\n", "if", "not", "retaining", "and", "frame", "is", "None", ":", "\n", "            ", "continue", "\n", "", "tmp_", "=", "center_crop", "(", "cv2", ".", "resize", "(", "frame", ",", "(", "171", ",", "128", ")", ")", ")", "\n", "tmp", "=", "tmp_", "-", "np", ".", "array", "(", "[", "[", "[", "90.0", ",", "98.0", ",", "102.0", "]", "]", "]", ")", "\n", "clip", ".", "append", "(", "tmp", ")", "\n", "if", "len", "(", "clip", ")", "==", "16", ":", "\n", "            ", "inputs", "=", "np", ".", "array", "(", "clip", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "inputs", "=", "np", ".", "expand_dims", "(", "inputs", ",", "axis", "=", "0", ")", "\n", "inputs", "=", "np", ".", "transpose", "(", "inputs", ",", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", ")", "\n", "inputs", "=", "torch", ".", "from_numpy", "(", "inputs", ")", "\n", "inputs", "=", "torch", ".", "autograd", ".", "Variable", "(", "inputs", ",", "requires_grad", "=", "False", ")", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "model", ".", "forward", "(", "inputs", ")", "\n", "\n", "", "probs", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "(", "outputs", ")", "\n", "label", "=", "torch", ".", "max", "(", "probs", ",", "1", ")", "[", "1", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "\n", "cv2", ".", "putText", "(", "frame", ",", "class_names", "[", "label", "]", ".", "split", "(", "' '", ")", "[", "-", "1", "]", ".", "strip", "(", ")", ",", "(", "20", ",", "20", ")", ",", "\n", "cv2", ".", "FONT_HERSHEY_SIMPLEX", ",", "0.6", ",", "\n", "(", "0", ",", "0", ",", "255", ")", ",", "1", ")", "\n", "cv2", ".", "putText", "(", "frame", ",", "\"prob: %.4f\"", "%", "probs", "[", "0", "]", "[", "label", "]", ",", "(", "20", ",", "40", ")", ",", "\n", "cv2", ".", "FONT_HERSHEY_SIMPLEX", ",", "0.6", ",", "\n", "(", "0", ",", "0", ",", "255", ")", ",", "1", ")", "\n", "clip", ".", "pop", "(", "0", ")", "\n", "\n", "", "cv2", ".", "imshow", "(", "'result'", ",", "frame", ")", "\n", "cv2", ".", "waitKey", "(", "30", ")", "\n", "\n", "", "cap", ".", "release", "(", ")", "\n", "cv2", ".", "destroyAllWindows", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.train.train_model": [[52, 199], ["torch.nn.CrossEntropyLoss", "torch.optim.SGD", "torch.optim.lr_scheduler.StepLR", "print", "R3D_model.R3DClassifier.to", "nn.CrossEntropyLoss.to", "os.path.join", "tensorboardX.SummaryWriter", "print", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "range", "tensorboardX.SummaryWriter.close", "network.C3D_model.C3D", "print", "torch.load", "print", "R3D_model.R3DClassifier.load_state_dict", "optim.SGD.load_state_dict", "dataloaders.dataset.VideoDataset", "dataloaders.dataset.VideoDataset", "dataloaders.dataset.VideoDataset", "len", "network.R2Plus1D_model.R2Plus1DClassifier", "os.path.join", "socket.gethostname", "timeit.default_timer", "tqdm.tqdm", "print", "timeit.default_timer", "print", "torch.save", "print", "R3D_model.R3DClassifier.eval", "timeit.default_timer", "tqdm.tqdm", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "print", "timeit.default_timer", "print", "network.C3D_model.get_1x_lr_params", "network.C3D_model.get_10x_lr_params", "network.R3D_model.R3DClassifier", "R3D_model.R3DClassifier.parameters", "print", "os.path.join", "sum", "datetime.datetime.now().strftime", "optim.lr_scheduler.StepLR.step", "R3D_model.R3DClassifier.train", "R3D_model.R3DClassifier.eval", "torch.autograd.Variable().to", "torch.autograd.Variable().to", "optim.SGD.zero_grad", "nn.CrossEntropyLoss.", "torch.sum", "running_corrects.double", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "os.path.join", "inputs.to.to", "labels.to.to", "nn.CrossEntropyLoss.", "torch.sum", "running_corrects.double", "network.R2Plus1D_model.get_1x_lr_params", "network.R2Plus1D_model.get_10x_lr_params", "R3D_model.R3DClassifier.", "torch.nn.Softmax", "torch.max", "criterion.backward", "optim.SGD.step", "criterion.item", "inputs.to.size", "R3D_model.R3DClassifier.state_dict", "optim.SGD.state_dict", "os.path.join", "torch.no_grad", "R3D_model.R3DClassifier.", "torch.nn.Softmax", "torch.max", "criterion.item", "inputs.to.size", "str", "p.numel", "datetime.datetime.now", "torch.autograd.Variable", "torch.autograd.Variable", "torch.no_grad", "R3D_model.R3DClassifier.", "str", "str", "str", "R3D_model.R3DClassifier.parameters", "str", "str"], "function", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_1x_lr_params", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_10x_lr_params", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_1x_lr_params", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_10x_lr_params"], ["def", "train_model", "(", "dataset", "=", "dataset", ",", "save_dir", "=", "save_dir", ",", "num_classes", "=", "num_classes", ",", "lr", "=", "lr", ",", "\n", "num_epochs", "=", "nEpochs", ",", "save_epoch", "=", "snapshot", ",", "useTest", "=", "useTest", ",", "test_interval", "=", "nTestInterval", ")", ":", "\n", "    ", "\"\"\"\n        Args:\n            num_classes (int): Number of classes in the data\n            num_epochs (int, optional): Number of epochs to train for.\n    \"\"\"", "\n", "\n", "if", "modelName", "==", "'C3D'", ":", "\n", "        ", "model", "=", "C3D_model", ".", "C3D", "(", "num_classes", "=", "num_classes", ",", "pretrained", "=", "True", ")", "\n", "train_params", "=", "[", "{", "'params'", ":", "C3D_model", ".", "get_1x_lr_params", "(", "model", ")", ",", "'lr'", ":", "lr", "}", ",", "\n", "{", "'params'", ":", "C3D_model", ".", "get_10x_lr_params", "(", "model", ")", ",", "'lr'", ":", "lr", "*", "10", "}", "]", "\n", "", "elif", "modelName", "==", "'R2Plus1D'", ":", "\n", "        ", "model", "=", "R2Plus1D_model", ".", "R2Plus1DClassifier", "(", "num_classes", "=", "num_classes", ",", "layer_sizes", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ")", "\n", "train_params", "=", "[", "{", "'params'", ":", "R2Plus1D_model", ".", "get_1x_lr_params", "(", "model", ")", ",", "'lr'", ":", "lr", "}", ",", "\n", "{", "'params'", ":", "R2Plus1D_model", ".", "get_10x_lr_params", "(", "model", ")", ",", "'lr'", ":", "lr", "*", "10", "}", "]", "\n", "", "elif", "modelName", "==", "'R3D'", ":", "\n", "        ", "model", "=", "R3D_model", ".", "R3DClassifier", "(", "num_classes", "=", "num_classes", ",", "layer_sizes", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ")", "\n", "train_params", "=", "model", ".", "parameters", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'We only implemented C3D and R2Plus1D models.'", ")", "\n", "raise", "NotImplementedError", "\n", "", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "# standard crossentropy loss for classification", "\n", "optimizer", "=", "optim", ".", "SGD", "(", "train_params", ",", "lr", "=", "lr", ",", "momentum", "=", "0.9", ",", "weight_decay", "=", "5e-4", ")", "\n", "scheduler", "=", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "step_size", "=", "10", ",", "\n", "gamma", "=", "0.1", ")", "# the scheduler divides the lr by 10 every 10 epochs", "\n", "\n", "if", "resume_epoch", "==", "0", ":", "\n", "        ", "print", "(", "\"Training {} from scratch...\"", ".", "format", "(", "modelName", ")", ")", "\n", "", "else", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'models'", ",", "saveName", "+", "'_epoch-'", "+", "str", "(", "resume_epoch", "-", "1", ")", "+", "'.pth.tar'", ")", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "# Load all tensors onto the CPU", "\n", "print", "(", "\"Initializing weights from: {}...\"", ".", "format", "(", "\n", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'models'", ",", "saveName", "+", "'_epoch-'", "+", "str", "(", "resume_epoch", "-", "1", ")", "+", "'.pth.tar'", ")", ")", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'opt_dict'", "]", ")", "\n", "\n", "", "print", "(", "'Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "criterion", ".", "to", "(", "device", ")", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'models'", ",", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%b%d_%H-%M-%S'", ")", "+", "'_'", "+", "socket", ".", "gethostname", "(", ")", ")", "\n", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "log_dir", ")", "\n", "\n", "print", "(", "'Training model on {} dataset...'", ".", "format", "(", "dataset", ")", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "VideoDataset", "(", "dataset", "=", "dataset", ",", "split", "=", "'train'", ",", "clip_len", "=", "16", ")", ",", "batch_size", "=", "20", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "val_dataloader", "=", "DataLoader", "(", "VideoDataset", "(", "dataset", "=", "dataset", ",", "split", "=", "'val'", ",", "clip_len", "=", "16", ")", ",", "batch_size", "=", "20", ",", "num_workers", "=", "4", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "VideoDataset", "(", "dataset", "=", "dataset", ",", "split", "=", "'test'", ",", "clip_len", "=", "16", ")", ",", "batch_size", "=", "20", ",", "num_workers", "=", "4", ")", "\n", "\n", "trainval_loaders", "=", "{", "'train'", ":", "train_dataloader", ",", "'val'", ":", "val_dataloader", "}", "\n", "trainval_sizes", "=", "{", "x", ":", "len", "(", "trainval_loaders", "[", "x", "]", ".", "dataset", ")", "for", "x", "in", "[", "'train'", ",", "'val'", "]", "}", "\n", "test_size", "=", "len", "(", "test_dataloader", ".", "dataset", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "resume_epoch", ",", "num_epochs", ")", ":", "\n", "# each epoch has a training and validation step", "\n", "        ", "for", "phase", "in", "[", "'train'", ",", "'val'", "]", ":", "\n", "            ", "start_time", "=", "timeit", ".", "default_timer", "(", ")", "\n", "\n", "# reset the running loss and corrects", "\n", "running_loss", "=", "0.0", "\n", "running_corrects", "=", "0.0", "\n", "\n", "# set model to train() or eval() mode depending on whether it is trained", "\n", "# or being validated. Primarily affects layers such as BatchNorm or Dropout.", "\n", "if", "phase", "==", "'train'", ":", "\n", "# scheduler.step() is to be called once every epoch during training", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "", "for", "inputs", ",", "labels", "in", "tqdm", "(", "trainval_loaders", "[", "phase", "]", ")", ":", "\n", "# move inputs and labels to the device the training is taking place on", "\n", "                ", "inputs", "=", "Variable", "(", "inputs", ",", "requires_grad", "=", "True", ")", ".", "to", "(", "device", ")", "\n", "labels", "=", "Variable", "(", "labels", ")", ".", "to", "(", "device", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "phase", "==", "'train'", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ")", "\n", "", "else", ":", "\n", "                    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "outputs", "=", "model", "(", "inputs", ")", "\n", "\n", "", "", "probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "(", "outputs", ")", "\n", "preds", "=", "torch", ".", "max", "(", "probs", ",", "1", ")", "[", "1", "]", "\n", "loss", "=", "criterion", "(", "outputs", ",", "labels", ")", "\n", "\n", "if", "phase", "==", "'train'", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "running_loss", "+=", "loss", ".", "item", "(", ")", "*", "inputs", ".", "size", "(", "0", ")", "\n", "running_corrects", "+=", "torch", ".", "sum", "(", "preds", "==", "labels", ".", "data", ")", "\n", "\n", "", "epoch_loss", "=", "running_loss", "/", "trainval_sizes", "[", "phase", "]", "\n", "epoch_acc", "=", "running_corrects", ".", "double", "(", ")", "/", "trainval_sizes", "[", "phase", "]", "\n", "\n", "if", "phase", "==", "'train'", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'data/train_loss_epoch'", ",", "epoch_loss", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'data/train_acc_epoch'", ",", "epoch_acc", ",", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'data/val_loss_epoch'", ",", "epoch_loss", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'data/val_acc_epoch'", ",", "epoch_acc", ",", "epoch", ")", "\n", "\n", "", "print", "(", "\"[{}] Epoch: {}/{} Loss: {} Acc: {}\"", ".", "format", "(", "phase", ",", "epoch", "+", "1", ",", "nEpochs", ",", "epoch_loss", ",", "epoch_acc", ")", ")", "\n", "stop_time", "=", "timeit", ".", "default_timer", "(", ")", "\n", "print", "(", "\"Execution time: \"", "+", "str", "(", "stop_time", "-", "start_time", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "epoch", "%", "save_epoch", "==", "(", "save_epoch", "-", "1", ")", ":", "\n", "            ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "epoch", "+", "1", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'opt_dict'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'models'", ",", "saveName", "+", "'_epoch-'", "+", "str", "(", "epoch", ")", "+", "'.pth.tar'", ")", ")", "\n", "print", "(", "\"Save model at {}\\n\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'models'", ",", "saveName", "+", "'_epoch-'", "+", "str", "(", "epoch", ")", "+", "'.pth.tar'", ")", ")", ")", "\n", "\n", "", "if", "useTest", "and", "epoch", "%", "test_interval", "==", "(", "test_interval", "-", "1", ")", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "start_time", "=", "timeit", ".", "default_timer", "(", ")", "\n", "\n", "running_loss", "=", "0.0", "\n", "running_corrects", "=", "0.0", "\n", "\n", "for", "inputs", ",", "labels", "in", "tqdm", "(", "test_dataloader", ")", ":", "\n", "                ", "inputs", "=", "inputs", ".", "to", "(", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "outputs", "=", "model", "(", "inputs", ")", "\n", "", "probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "(", "outputs", ")", "\n", "preds", "=", "torch", ".", "max", "(", "probs", ",", "1", ")", "[", "1", "]", "\n", "loss", "=", "criterion", "(", "outputs", ",", "labels", ")", "\n", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "*", "inputs", ".", "size", "(", "0", ")", "\n", "running_corrects", "+=", "torch", ".", "sum", "(", "preds", "==", "labels", ".", "data", ")", "\n", "\n", "", "epoch_loss", "=", "running_loss", "/", "test_size", "\n", "epoch_acc", "=", "running_corrects", ".", "double", "(", ")", "/", "test_size", "\n", "\n", "writer", ".", "add_scalar", "(", "'data/test_loss_epoch'", ",", "epoch_loss", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'data/test_acc_epoch'", ",", "epoch_acc", ",", "epoch", ")", "\n", "\n", "print", "(", "\"[test] Epoch: {}/{} Loss: {} Acc: {}\"", ".", "format", "(", "epoch", "+", "1", ",", "nEpochs", ",", "epoch_loss", ",", "epoch_acc", ")", ")", "\n", "stop_time", "=", "timeit", ".", "default_timer", "(", ")", "\n", "print", "(", "\"Execution time: \"", "+", "str", "(", "stop_time", "-", "start_time", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalConv.__init__": [[20, 91], ["torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.Conv3d", "torch.BatchNorm3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.ReLU", "int", "torch.Conv3d", "torch.BatchNorm3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.ReLU", "math.floor"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ",", "first_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", "SpatioTemporalConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# if ints are entered, convert them to iterables, 1 -> [1, 1, 1]", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "\n", "if", "first_conv", ":", "\n", "# decomposing the parameters into spatial and temporal components by", "\n", "# masking out the values with the defaults on the axis that", "\n", "# won't be convolved over. This is necessary to avoid unintentional", "\n", "# behavior such as padding being added twice", "\n", "            ", "spatial_kernel_size", "=", "kernel_size", "\n", "spatial_stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", "\n", "spatial_padding", "=", "padding", "\n", "\n", "temporal_kernel_size", "=", "(", "3", ",", "1", ",", "1", ")", "\n", "temporal_stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "temporal_padding", "=", "(", "1", ",", "0", ",", "0", ")", "\n", "\n", "# from the official code, first conv's intermed_channels = 45", "\n", "intermed_channels", "=", "45", "\n", "\n", "# the spatial conv is effectively a 2D conv due to the", "\n", "# spatial_kernel_size, followed by batch_norm and ReLU", "\n", "self", ".", "spatial_conv", "=", "nn", ".", "Conv3d", "(", "in_channels", ",", "intermed_channels", ",", "spatial_kernel_size", ",", "\n", "stride", "=", "spatial_stride", ",", "padding", "=", "spatial_padding", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "intermed_channels", ")", "\n", "# the temporal conv is effectively a 1D conv, but has batch norm", "\n", "# and ReLU added inside the model constructor, not here. This is an", "\n", "# intentional design choice, to allow this module to externally act", "\n", "# identical to a standard Conv3D, so it can be reused easily in any", "\n", "# other codebase", "\n", "self", ".", "temporal_conv", "=", "nn", ".", "Conv3d", "(", "intermed_channels", ",", "out_channels", ",", "temporal_kernel_size", ",", "\n", "stride", "=", "temporal_stride", ",", "padding", "=", "temporal_padding", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "else", ":", "\n", "# decomposing the parameters into spatial and temporal components by", "\n", "# masking out the values with the defaults on the axis that", "\n", "# won't be convolved over. This is necessary to avoid unintentional", "\n", "# behavior such as padding being added twice", "\n", "            ", "spatial_kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", "\n", "spatial_stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", "\n", "spatial_padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", "\n", "\n", "temporal_kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "temporal_stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "temporal_padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", "\n", "\n", "# compute the number of intermediary channels (M) using formula", "\n", "# from the paper section 3.5", "\n", "intermed_channels", "=", "int", "(", "math", ".", "floor", "(", "(", "kernel_size", "[", "0", "]", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "*", "in_channels", "*", "out_channels", ")", "/", "(", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "*", "in_channels", "+", "kernel_size", "[", "0", "]", "*", "out_channels", ")", ")", ")", "\n", "\n", "# the spatial conv is effectively a 2D conv due to the", "\n", "# spatial_kernel_size, followed by batch_norm and ReLU", "\n", "self", ".", "spatial_conv", "=", "nn", ".", "Conv3d", "(", "in_channels", ",", "intermed_channels", ",", "spatial_kernel_size", ",", "\n", "stride", "=", "spatial_stride", ",", "padding", "=", "spatial_padding", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "intermed_channels", ")", "\n", "\n", "# the temporal conv is effectively a 1D conv, but has batch norm", "\n", "# and ReLU added inside the model constructor, not here. This is an", "\n", "# intentional design choice, to allow this module to externally act", "\n", "# identical to a standard Conv3D, so it can be reused easily in any", "\n", "# other codebase", "\n", "self", ".", "temporal_conv", "=", "nn", ".", "Conv3d", "(", "intermed_channels", ",", "out_channels", ",", "temporal_kernel_size", ",", "\n", "stride", "=", "temporal_stride", ",", "padding", "=", "temporal_padding", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalConv.forward": [[94, 98], ["R2Plus1D_model.SpatioTemporalConv.relu", "R2Plus1D_model.SpatioTemporalConv.relu", "R2Plus1D_model.SpatioTemporalConv.bn1", "R2Plus1D_model.SpatioTemporalConv.bn2", "R2Plus1D_model.SpatioTemporalConv.spatial_conv", "R2Plus1D_model.SpatioTemporalConv.temporal_conv"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "relu", "(", "self", ".", "bn1", "(", "self", ".", "spatial_conv", "(", "x", ")", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "bn2", "(", "self", ".", "temporal_conv", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalResBlock.__init__": [[111, 140], ["torch.Module.__init__", "torch.BatchNorm3d", "torch.ReLU", "R2Plus1D_model.SpatioTemporalConv", "torch.BatchNorm3d", "R2Plus1D_model.SpatioTemporalConv", "torch.BatchNorm3d", "R2Plus1D_model.SpatioTemporalConv", "R2Plus1D_model.SpatioTemporalConv"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "downsample", "=", "False", ")", ":", "\n", "        ", "super", "(", "SpatioTemporalResBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# If downsample == True, the first conv of the layer has stride = 2", "\n", "# to halve the residual output size, and the input x is passed", "\n", "# through a seperate 1x1x1 conv with stride = 2 to also halve it.", "\n", "\n", "# no pooling layers are used inside ResNet", "\n", "self", ".", "downsample", "=", "downsample", "\n", "\n", "# to allow for SAME padding", "\n", "padding", "=", "kernel_size", "//", "2", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "# downsample with stride =2 the input x", "\n", "            ", "self", ".", "downsampleconv", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "1", ",", "stride", "=", "2", ")", "\n", "self", ".", "downsamplebn", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "\n", "# downsample with stride = 2when producing the residual", "\n", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "\n", "", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "# standard conv->batchnorm->ReLU", "\n", "self", ".", "conv2", "=", "SpatioTemporalConv", "(", "out_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalResBlock.forward": [[141, 149], ["R2Plus1D_model.SpatioTemporalResBlock.relu", "R2Plus1D_model.SpatioTemporalResBlock.bn2", "R2Plus1D_model.SpatioTemporalResBlock.relu", "R2Plus1D_model.SpatioTemporalResBlock.bn1", "R2Plus1D_model.SpatioTemporalResBlock.conv2", "R2Plus1D_model.SpatioTemporalResBlock.downsamplebn", "R2Plus1D_model.SpatioTemporalResBlock.conv1", "R2Plus1D_model.SpatioTemporalResBlock.downsampleconv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "res", "=", "self", ".", "relu", "(", "self", ".", "bn1", "(", "self", ".", "conv1", "(", "x", ")", ")", ")", "\n", "res", "=", "self", ".", "bn2", "(", "self", ".", "conv2", "(", "res", ")", ")", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "x", "=", "self", ".", "downsamplebn", "(", "self", ".", "downsampleconv", "(", "x", ")", ")", "\n", "\n", "", "return", "self", ".", "relu", "(", "x", "+", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalResLayer.__init__": [[164, 177], ["torch.Module.__init__", "block_type", "torch.ModuleList", "range", "block_type"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "layer_size", ",", "block_type", "=", "SpatioTemporalResBlock", ",", "\n", "downsample", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", "SpatioTemporalResLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# implement the first block", "\n", "self", ".", "block1", "=", "block_type", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "downsample", ")", "\n", "\n", "# prepare module list to hold all (layer_size - 1) blocks", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "for", "i", "in", "range", "(", "layer_size", "-", "1", ")", ":", "\n", "# all these blocks are identical, and have downsample = False by default", "\n", "            ", "self", ".", "blocks", "+=", "[", "block_type", "(", "out_channels", ",", "out_channels", ",", "kernel_size", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.SpatioTemporalResLayer.forward": [[178, 184], ["R2Plus1D_model.SpatioTemporalResLayer.block1", "block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "block1", "(", "x", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "x", "=", "block", "(", "x", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DNet.__init__": [[196, 211], ["torch.Module.__init__", "R2Plus1D_model.SpatioTemporalConv", "R2Plus1D_model.SpatioTemporalResLayer", "R2Plus1D_model.SpatioTemporalResLayer", "R2Plus1D_model.SpatioTemporalResLayer", "R2Plus1D_model.SpatioTemporalResLayer", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "layer_sizes", ",", "block_type", "=", "SpatioTemporalResBlock", ")", ":", "\n", "        ", "super", "(", "R2Plus1DNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# first conv, with stride 1x2x2 and kernel size 1x7x7", "\n", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "3", ",", "64", ",", "(", "1", ",", "7", ",", "7", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "3", ",", "3", ")", ",", "first_conv", "=", "True", ")", "\n", "# output of conv2 is same size as of conv1, no downsampling needed. kernel_size 3x3x3", "\n", "self", ".", "conv2", "=", "SpatioTemporalResLayer", "(", "64", ",", "64", ",", "3", ",", "layer_sizes", "[", "0", "]", ",", "block_type", "=", "block_type", ")", "\n", "# each of the final three layers doubles num_channels, while performing downsampling", "\n", "# inside the first block", "\n", "self", ".", "conv3", "=", "SpatioTemporalResLayer", "(", "64", ",", "128", ",", "3", ",", "layer_sizes", "[", "1", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "self", ".", "conv4", "=", "SpatioTemporalResLayer", "(", "128", ",", "256", ",", "3", ",", "layer_sizes", "[", "2", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "self", ".", "conv5", "=", "SpatioTemporalResLayer", "(", "256", ",", "512", ",", "3", ",", "layer_sizes", "[", "3", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "\n", "# global average pooling of the output", "\n", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DNet.forward": [[212, 222], ["R2Plus1D_model.R2Plus1DNet.conv1", "R2Plus1D_model.R2Plus1DNet.conv2", "R2Plus1D_model.R2Plus1DNet.conv3", "R2Plus1D_model.R2Plus1DNet.conv4", "R2Plus1D_model.R2Plus1DNet.conv5", "R2Plus1D_model.R2Plus1DNet.pool", "R2Plus1D_model.R2Plus1DNet.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "\n", "return", "x", ".", "view", "(", "-", "1", ",", "512", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DClassifier.__init__": [[236, 246], ["torch.Module.__init__", "R2Plus1D_model.R2Plus1DNet", "torch.Linear", "R2Plus1D_model.R2Plus1DClassifier.__init_weight", "R2Plus1D_model.R2Plus1DClassifier.__load_pretrained_weights"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__init_weight", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__load_pretrained_weights"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "layer_sizes", ",", "block_type", "=", "SpatioTemporalResBlock", ",", "pretrained", "=", "False", ")", ":", "\n", "        ", "super", "(", "R2Plus1DClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "res2plus1d", "=", "R2Plus1DNet", "(", "layer_sizes", ",", "block_type", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "512", ",", "num_classes", ")", "\n", "\n", "self", ".", "__init_weight", "(", ")", "\n", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "__load_pretrained_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DClassifier.forward": [[247, 252], ["R2Plus1D_model.R2Plus1DClassifier.res2plus1d", "R2Plus1D_model.R2Plus1DClassifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "res2plus1d", "(", "x", ")", "\n", "logits", "=", "self", ".", "linear", "(", "x", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DClassifier.__load_pretrained_weights": [[253, 258], ["R2Plus1D_model.R2Plus1DClassifier.state_dict", "print", "print", "s_dict[].size"], "methods", ["None"], ["", "def", "__load_pretrained_weights", "(", "self", ")", ":", "\n", "        ", "s_dict", "=", "self", ".", "state_dict", "(", ")", "\n", "for", "name", "in", "s_dict", ":", "\n", "            ", "print", "(", "name", ")", "\n", "print", "(", "s_dict", "[", "name", "]", ".", "size", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.R2Plus1DClassifier.__init_weight": [[259, 268], ["R2Plus1D_model.R2Plus1DClassifier.modules", "isinstance", "torch.init.kaiming_normal_", "isinstance", "m.weight.data.fill_", "m.bias.data.zero_"], "methods", ["None"], ["", "", "def", "__init_weight", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "# n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels", "\n", "# m.weight.data.normal_(0, math.sqrt(2. / n))", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.get_1x_lr_params": [[270, 279], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_1x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for the conv layer of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "res2plus1d", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "i", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R2Plus1D_model.get_10x_lr_params": [[281, 290], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_10x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for the fc layer of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "linear", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "j", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalConv.__init__": [[21, 34], ["torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.Conv3d", "torch.BatchNorm3d", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "SpatioTemporalConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# if ints are entered, convert them to iterables, 1 -> [1, 1, 1]", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "\n", "\n", "self", ".", "temporal_spatial_conv", "=", "nn", ".", "Conv3d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "bias", "=", "bias", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalConv.forward": [[36, 40], ["R3D_model.SpatioTemporalConv.bn", "R3D_model.SpatioTemporalConv.relu", "R3D_model.SpatioTemporalConv.temporal_spatial_conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "bn", "(", "self", ".", "temporal_spatial_conv", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalResBlock.__init__": [[53, 83], ["torch.Module.__init__", "torch.BatchNorm3d", "torch.ReLU", "R3D_model.SpatioTemporalConv", "torch.BatchNorm3d", "torch.ReLU", "R3D_model.SpatioTemporalConv", "torch.BatchNorm3d", "R3D_model.SpatioTemporalConv", "R3D_model.SpatioTemporalConv"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "downsample", "=", "False", ")", ":", "\n", "        ", "super", "(", "SpatioTemporalResBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# If downsample == True, the first conv of the layer has stride = 2", "\n", "# to halve the residual output size, and the input x is passed", "\n", "# through a seperate 1x1x1 conv with stride = 2 to also halve it.", "\n", "\n", "# no pooling layers are used inside ResNet", "\n", "self", ".", "downsample", "=", "downsample", "\n", "\n", "# to allow for SAME padding", "\n", "padding", "=", "kernel_size", "//", "2", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "# downsample with stride =2 the input x", "\n", "            ", "self", ".", "downsampleconv", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "1", ",", "stride", "=", "2", ")", "\n", "self", ".", "downsamplebn", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "\n", "# downsample with stride = 2when producing the residual", "\n", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "\n", "", "self", ".", "bn1", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "# standard conv->batchnorm->ReLU", "\n", "self", ".", "conv2", "=", "SpatioTemporalConv", "(", "out_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm3d", "(", "out_channels", ")", "\n", "self", ".", "outrelu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalResBlock.forward": [[84, 92], ["R3D_model.SpatioTemporalResBlock.relu1", "R3D_model.SpatioTemporalResBlock.bn2", "R3D_model.SpatioTemporalResBlock.outrelu", "R3D_model.SpatioTemporalResBlock.bn1", "R3D_model.SpatioTemporalResBlock.conv2", "R3D_model.SpatioTemporalResBlock.downsamplebn", "R3D_model.SpatioTemporalResBlock.conv1", "R3D_model.SpatioTemporalResBlock.downsampleconv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "res", "=", "self", ".", "relu1", "(", "self", ".", "bn1", "(", "self", ".", "conv1", "(", "x", ")", ")", ")", "\n", "res", "=", "self", ".", "bn2", "(", "self", ".", "conv2", "(", "res", ")", ")", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "x", "=", "self", ".", "downsamplebn", "(", "self", ".", "downsampleconv", "(", "x", ")", ")", "\n", "\n", "", "return", "self", ".", "outrelu", "(", "x", "+", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalResLayer.__init__": [[107, 120], ["torch.Module.__init__", "block_type", "torch.ModuleList", "range", "block_type"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "layer_size", ",", "block_type", "=", "SpatioTemporalResBlock", ",", "\n", "downsample", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", "SpatioTemporalResLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# implement the first block", "\n", "self", ".", "block1", "=", "block_type", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "downsample", ")", "\n", "\n", "# prepare module list to hold all (layer_size - 1) blocks", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "for", "i", "in", "range", "(", "layer_size", "-", "1", ")", ":", "\n", "# all these blocks are identical, and have downsample = False by default", "\n", "            ", "self", ".", "blocks", "+=", "[", "block_type", "(", "out_channels", ",", "out_channels", ",", "kernel_size", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.SpatioTemporalResLayer.forward": [[121, 127], ["R3D_model.SpatioTemporalResLayer.block1", "block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "block1", "(", "x", ")", "\n", "for", "block", "in", "self", ".", "blocks", ":", "\n", "            ", "x", "=", "block", "(", "x", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DNet.__init__": [[139, 154], ["torch.Module.__init__", "R3D_model.SpatioTemporalConv", "R3D_model.SpatioTemporalResLayer", "R3D_model.SpatioTemporalResLayer", "R3D_model.SpatioTemporalResLayer", "R3D_model.SpatioTemporalResLayer", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__"], ["def", "__init__", "(", "self", ",", "layer_sizes", ",", "block_type", "=", "SpatioTemporalResBlock", ")", ":", "\n", "        ", "super", "(", "R3DNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# first conv, with stride 1x2x2 and kernel size 3x7x7", "\n", "self", ".", "conv1", "=", "SpatioTemporalConv", "(", "3", ",", "64", ",", "[", "3", ",", "7", ",", "7", "]", ",", "stride", "=", "[", "1", ",", "2", ",", "2", "]", ",", "padding", "=", "[", "1", ",", "3", ",", "3", "]", ")", "\n", "# output of conv2 is same size as of conv1, no downsampling needed. kernel_size 3x3x3", "\n", "self", ".", "conv2", "=", "SpatioTemporalResLayer", "(", "64", ",", "64", ",", "3", ",", "layer_sizes", "[", "0", "]", ",", "block_type", "=", "block_type", ")", "\n", "# each of the final three layers doubles num_channels, while performing downsampling", "\n", "# inside the first block", "\n", "self", ".", "conv3", "=", "SpatioTemporalResLayer", "(", "64", ",", "128", ",", "3", ",", "layer_sizes", "[", "1", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "self", ".", "conv4", "=", "SpatioTemporalResLayer", "(", "128", ",", "256", ",", "3", ",", "layer_sizes", "[", "2", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "self", ".", "conv5", "=", "SpatioTemporalResLayer", "(", "256", ",", "512", ",", "3", ",", "layer_sizes", "[", "3", "]", ",", "block_type", "=", "block_type", ",", "downsample", "=", "True", ")", "\n", "\n", "# global average pooling of the output", "\n", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DNet.forward": [[155, 165], ["R3D_model.R3DNet.conv1", "R3D_model.R3DNet.conv2", "R3D_model.R3DNet.conv3", "R3D_model.R3DNet.conv4", "R3D_model.R3DNet.conv5", "R3D_model.R3DNet.pool", "R3D_model.R3DNet.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "\n", "return", "x", ".", "view", "(", "-", "1", ",", "512", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DClassifier.__init__": [[179, 189], ["torch.Module.__init__", "R3D_model.R3DNet", "torch.Linear", "R3D_model.R3DClassifier.__init_weight", "R3D_model.R3DClassifier.__load_pretrained_weights"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__init_weight", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__load_pretrained_weights"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "layer_sizes", ",", "block_type", "=", "SpatioTemporalResBlock", ",", "pretrained", "=", "False", ")", ":", "\n", "        ", "super", "(", "R3DClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "res3d", "=", "R3DNet", "(", "layer_sizes", ",", "block_type", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "512", ",", "num_classes", ")", "\n", "\n", "self", ".", "__init_weight", "(", ")", "\n", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "__load_pretrained_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DClassifier.forward": [[190, 195], ["R3D_model.R3DClassifier.res3d", "R3D_model.R3DClassifier.linear"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "res3d", "(", "x", ")", "\n", "logits", "=", "self", ".", "linear", "(", "x", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DClassifier.__load_pretrained_weights": [[196, 201], ["R3D_model.R3DClassifier.state_dict", "print", "print", "s_dict[].size"], "methods", ["None"], ["", "def", "__load_pretrained_weights", "(", "self", ")", ":", "\n", "        ", "s_dict", "=", "self", ".", "state_dict", "(", ")", "\n", "for", "name", "in", "s_dict", ":", "\n", "            ", "print", "(", "name", ")", "\n", "print", "(", "s_dict", "[", "name", "]", ".", "size", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.R3DClassifier.__init_weight": [[202, 209], ["R3D_model.R3DClassifier.modules", "isinstance", "torch.init.kaiming_normal_", "isinstance", "m.weight.data.fill_", "m.bias.data.zero_"], "methods", ["None"], ["", "", "def", "__init_weight", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.get_1x_lr_params": [[211, 220], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_1x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for the conv layer of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "res3d", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "i", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.R3D_model.get_10x_lr_params": [[222, 231], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_10x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for the fc layer of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "linear", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "j", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__init__": [[10, 43], ["torch.Module.__init__", "torch.Conv3d", "torch.Conv3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.Conv3d", "torch.Conv3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "C3D_model.C3D.__init_weight", "C3D_model.C3D.__load_pretrained_weights"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__init_weight", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__load_pretrained_weights"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "pretrained", "=", "False", ")", ":", "\n", "        ", "super", "(", "C3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv3d", "(", "3", ",", "64", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "pool1", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "1", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv3d", "(", "64", ",", "128", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv3a", "=", "nn", ".", "Conv3d", "(", "128", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "conv3b", "=", "nn", ".", "Conv3d", "(", "256", ",", "256", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "pool3", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv4a", "=", "nn", ".", "Conv3d", "(", "256", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "conv4b", "=", "nn", ".", "Conv3d", "(", "512", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "pool4", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv5a", "=", "nn", ".", "Conv3d", "(", "512", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "conv5b", "=", "nn", ".", "Conv3d", "(", "512", ",", "512", ",", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "pool5", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "fc6", "=", "nn", ".", "Linear", "(", "8192", ",", "4096", ")", "\n", "self", ".", "fc7", "=", "nn", ".", "Linear", "(", "4096", ",", "4096", ")", "\n", "self", ".", "fc8", "=", "nn", ".", "Linear", "(", "4096", ",", "num_classes", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "0.5", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "__init_weight", "(", ")", "\n", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "__load_pretrained_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.forward": [[44, 73], ["C3D_model.C3D.relu", "C3D_model.C3D.pool1", "C3D_model.C3D.relu", "C3D_model.C3D.pool2", "C3D_model.C3D.relu", "C3D_model.C3D.relu", "C3D_model.C3D.pool3", "C3D_model.C3D.relu", "C3D_model.C3D.relu", "C3D_model.C3D.pool4", "C3D_model.C3D.relu", "C3D_model.C3D.relu", "C3D_model.C3D.pool5", "C3D_model.C3D.view", "C3D_model.C3D.relu", "C3D_model.C3D.dropout", "C3D_model.C3D.relu", "C3D_model.C3D.dropout", "C3D_model.C3D.fc8", "C3D_model.C3D.conv1", "C3D_model.C3D.conv2", "C3D_model.C3D.conv3a", "C3D_model.C3D.conv3b", "C3D_model.C3D.conv4a", "C3D_model.C3D.conv4b", "C3D_model.C3D.conv5a", "C3D_model.C3D.conv5b", "C3D_model.C3D.fc6", "C3D_model.C3D.fc7"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "relu", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "pool1", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "pool2", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv3a", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv3b", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "pool3", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv4a", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv4b", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "pool4", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv5a", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "conv5b", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "pool5", "(", "x", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "8192", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "fc6", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "fc7", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "logits", "=", "self", ".", "fc8", "(", "x", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__load_pretrained_weights": [[74, 116], ["torch.load", "torch.load", "torch.load", "torch.load", "C3D_model.C3D.state_dict", "C3D_model.C3D.load_state_dict", "mypath.Path.model_dir"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.mypath.Path.model_dir"], ["", "def", "__load_pretrained_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialiaze network.\"\"\"", "\n", "corresp_name", "=", "{", "\n", "# Conv1", "\n", "\"features.0.weight\"", ":", "\"conv1.weight\"", ",", "\n", "\"features.0.bias\"", ":", "\"conv1.bias\"", ",", "\n", "# Conv2", "\n", "\"features.3.weight\"", ":", "\"conv2.weight\"", ",", "\n", "\"features.3.bias\"", ":", "\"conv2.bias\"", ",", "\n", "# Conv3a", "\n", "\"features.6.weight\"", ":", "\"conv3a.weight\"", ",", "\n", "\"features.6.bias\"", ":", "\"conv3a.bias\"", ",", "\n", "# Conv3b", "\n", "\"features.8.weight\"", ":", "\"conv3b.weight\"", ",", "\n", "\"features.8.bias\"", ":", "\"conv3b.bias\"", ",", "\n", "# Conv4a", "\n", "\"features.11.weight\"", ":", "\"conv4a.weight\"", ",", "\n", "\"features.11.bias\"", ":", "\"conv4a.bias\"", ",", "\n", "# Conv4b", "\n", "\"features.13.weight\"", ":", "\"conv4b.weight\"", ",", "\n", "\"features.13.bias\"", ":", "\"conv4b.bias\"", ",", "\n", "# Conv5a", "\n", "\"features.16.weight\"", ":", "\"conv5a.weight\"", ",", "\n", "\"features.16.bias\"", ":", "\"conv5a.bias\"", ",", "\n", "# Conv5b", "\n", "\"features.18.weight\"", ":", "\"conv5b.weight\"", ",", "\n", "\"features.18.bias\"", ":", "\"conv5b.bias\"", ",", "\n", "# fc6", "\n", "\"classifier.0.weight\"", ":", "\"fc6.weight\"", ",", "\n", "\"classifier.0.bias\"", ":", "\"fc6.bias\"", ",", "\n", "# fc7", "\n", "\"classifier.3.weight\"", ":", "\"fc7.weight\"", ",", "\n", "\"classifier.3.bias\"", ":", "\"fc7.bias\"", ",", "\n", "}", "\n", "\n", "p_dict", "=", "torch", ".", "load", "(", "Path", ".", "model_dir", "(", ")", ")", "\n", "s_dict", "=", "self", ".", "state_dict", "(", ")", "\n", "for", "name", "in", "p_dict", ":", "\n", "            ", "if", "name", "not", "in", "corresp_name", ":", "\n", "                ", "continue", "\n", "", "s_dict", "[", "corresp_name", "[", "name", "]", "]", "=", "p_dict", "[", "name", "]", "\n", "", "self", ".", "load_state_dict", "(", "s_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.C3D.__init_weight": [[117, 126], ["C3D_model.C3D.modules", "isinstance", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "isinstance", "m.weight.data.fill_", "m.bias.data.zero_"], "methods", ["None"], ["", "def", "__init_weight", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "# n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels", "\n", "# m.weight.data.normal_(0, math.sqrt(2. / n))", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_1x_lr_params": [[127, 137], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_1x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for conv and two fc layers of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "conv1", ",", "model", ".", "conv2", ",", "model", ".", "conv3a", ",", "model", ".", "conv3b", ",", "model", ".", "conv4a", ",", "model", ".", "conv4b", ",", "\n", "model", ".", "conv5a", ",", "model", ".", "conv5b", ",", "model", ".", "fc6", ",", "model", ".", "fc7", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "i", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.network.C3D_model.get_10x_lr_params": [[138, 147], ["range", "len", "b[].parameters"], "function", ["None"], ["", "", "", "", "def", "get_10x_lr_params", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    This generator returns all the parameters for the last fc layer of the net.\n    \"\"\"", "\n", "b", "=", "[", "model", ".", "fc8", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "b", ")", ")", ":", "\n", "        ", "for", "k", "in", "b", "[", "j", "]", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "k", ".", "requires_grad", ":", "\n", "                ", "yield", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__init__": [[24, 70], ["mypath.Path.db_dir", "os.path.join", "sorted", "print", "numpy.array", "dataset.VideoDataset.check_integrity", "RuntimeError", "print", "dataset.VideoDataset.preprocess", "os.listdir", "os.listdir", "len", "len", "dataset.VideoDataset.check_preprocess", "os.path.join", "dataset.VideoDataset.fnames.append", "labels.append", "len", "enumerate", "os.path.exists", "os.path.join", "sorted", "open", "enumerate", "os.path.exists", "set", "sorted", "f.writelines", "open", "enumerate", "sorted", "f.writelines", "str", "str"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.None.mypath.Path.db_dir", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.check_integrity", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.preprocess", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.check_preprocess"], ["def", "__init__", "(", "self", ",", "dataset", "=", "'ucf101'", ",", "split", "=", "'train'", ",", "clip_len", "=", "16", ",", "preprocess", "=", "False", ")", ":", "\n", "        ", "self", ".", "root_dir", ",", "self", ".", "output_dir", "=", "Path", ".", "db_dir", "(", "dataset", ")", "\n", "folder", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "split", ")", "\n", "self", ".", "clip_len", "=", "clip_len", "\n", "self", ".", "split", "=", "split", "\n", "\n", "# The following three parameters are chosen as described in the paper section 4.1", "\n", "self", ".", "resize_height", "=", "128", "\n", "self", ".", "resize_width", "=", "171", "\n", "self", ".", "crop_size", "=", "112", "\n", "\n", "if", "not", "self", ".", "check_integrity", "(", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Dataset not found or corrupted.'", "+", "\n", "' You need to download it from official website.'", ")", "\n", "\n", "", "if", "(", "not", "self", ".", "check_preprocess", "(", ")", ")", "or", "preprocess", ":", "\n", "            ", "print", "(", "'Preprocessing of {} dataset, this will take long, but it will be done only once.'", ".", "format", "(", "dataset", ")", ")", "\n", "self", ".", "preprocess", "(", ")", "\n", "\n", "# Obtain all the filenames of files inside all the class folders", "\n", "# Going through each class folder one at a time", "\n", "", "self", ".", "fnames", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "for", "label", "in", "sorted", "(", "os", ".", "listdir", "(", "folder", ")", ")", ":", "\n", "            ", "for", "fname", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "label", ")", ")", ":", "\n", "                ", "self", ".", "fnames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "label", ",", "fname", ")", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "\n", "", "", "assert", "len", "(", "labels", ")", "==", "len", "(", "self", ".", "fnames", ")", "\n", "print", "(", "'Number of {} videos: {:d}'", ".", "format", "(", "split", ",", "len", "(", "self", ".", "fnames", ")", ")", ")", "\n", "\n", "# Prepare a mapping between the label names (strings) and indices (ints)", "\n", "self", ".", "label2index", "=", "{", "label", ":", "index", "for", "index", ",", "label", "in", "enumerate", "(", "sorted", "(", "set", "(", "labels", ")", ")", ")", "}", "\n", "# Convert the list of label names into an array of label indices", "\n", "self", ".", "label_array", "=", "np", ".", "array", "(", "[", "self", ".", "label2index", "[", "label", "]", "for", "label", "in", "labels", "]", ",", "dtype", "=", "int", ")", "\n", "\n", "if", "dataset", "==", "\"ucf101\"", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "'dataloaders/ucf_labels.txt'", ")", ":", "\n", "                ", "with", "open", "(", "'dataloaders/ucf_labels.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "for", "id", ",", "label", "in", "enumerate", "(", "sorted", "(", "self", ".", "label2index", ")", ")", ":", "\n", "                        ", "f", ".", "writelines", "(", "str", "(", "id", "+", "1", ")", "+", "' '", "+", "label", "+", "'\\n'", ")", "\n", "\n", "", "", "", "", "elif", "dataset", "==", "'hmdb51'", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "'dataloaders/hmdb_labels.txt'", ")", ":", "\n", "                ", "with", "open", "(", "'dataloaders/hmdb_labels.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "                    ", "for", "id", ",", "label", "in", "enumerate", "(", "sorted", "(", "self", ".", "label2index", ")", ")", ":", "\n", "                        ", "f", ".", "writelines", "(", "str", "(", "id", "+", "1", ")", "+", "' '", "+", "label", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__len__": [[72, 74], ["len"], "methods", ["None"], ["", "", "", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "fnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.__getitem__": [[75, 87], ["dataset.VideoDataset.load_frames", "dataset.VideoDataset.crop", "numpy.array", "dataset.VideoDataset.normalize", "dataset.VideoDataset.to_tensor", "dataset.VideoDataset.randomflip", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.load_frames", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.crop", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.normalize", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.to_tensor", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.randomflip"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "# Loading and preprocessing.", "\n", "        ", "buffer", "=", "self", ".", "load_frames", "(", "self", ".", "fnames", "[", "index", "]", ")", "\n", "buffer", "=", "self", ".", "crop", "(", "buffer", ",", "self", ".", "clip_len", ",", "self", ".", "crop_size", ")", "\n", "labels", "=", "np", ".", "array", "(", "self", ".", "label_array", "[", "index", "]", ")", "\n", "\n", "if", "self", ".", "split", "==", "'test'", ":", "\n", "# Perform data augmentation", "\n", "            ", "buffer", "=", "self", ".", "randomflip", "(", "buffer", ")", "\n", "", "buffer", "=", "self", ".", "normalize", "(", "buffer", ")", "\n", "buffer", "=", "self", ".", "to_tensor", "(", "buffer", ")", "\n", "return", "torch", ".", "from_numpy", "(", "buffer", ")", ",", "torch", ".", "from_numpy", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.check_integrity": [[88, 93], ["os.path.exists"], "methods", ["None"], ["", "def", "check_integrity", "(", "self", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "root_dir", ")", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.check_preprocess": [[94, 115], ["enumerate", "os.path.exists", "os.listdir", "os.listdir", "os.path.exists", "os.path.join", "os.path.join", "os.path.join", "cv2.imread", "os.path.join", "os.path.join", "sorted", "os.listdir", "numpy.shape", "numpy.shape", "os.path.join"], "methods", ["None"], ["", "", "def", "check_preprocess", "(", "self", ")", ":", "\n", "# TODO: Check image size in output_dir", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "output_dir", ")", ":", "\n", "            ", "return", "False", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ")", ")", ":", "\n", "            ", "return", "False", "\n", "\n", "", "for", "ii", ",", "video_class", "in", "enumerate", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ")", ")", ")", ":", "\n", "            ", "for", "video", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ",", "video_class", ")", ")", ":", "\n", "                ", "video_name", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ",", "video_class", ",", "video", ")", ",", "\n", "sorted", "(", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ",", "video_class", ",", "video", ")", ")", ")", "[", "0", "]", ")", "\n", "image", "=", "cv2", ".", "imread", "(", "video_name", ")", "\n", "if", "np", ".", "shape", "(", "image", ")", "[", "0", "]", "!=", "128", "or", "np", ".", "shape", "(", "image", ")", "[", "1", "]", "!=", "171", ":", "\n", "                    ", "return", "False", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n", "", "", "if", "ii", "==", "10", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.preprocess": [[116, 152], ["os.listdir", "print", "os.path.exists", "os.mkdir", "os.mkdir", "os.mkdir", "os.mkdir", "os.path.join", "sklearn.model_selection.train_test_split", "sklearn.model_selection.train_test_split", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "dataset.VideoDataset.process_video", "dataset.VideoDataset.process_video", "dataset.VideoDataset.process_video", "os.listdir"], "methods", ["home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.process_video", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.process_video", "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.process_video"], ["", "def", "preprocess", "(", "self", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "output_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "self", ".", "output_dir", ")", "\n", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ")", ")", "\n", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'val'", ")", ")", "\n", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'test'", ")", ")", "\n", "\n", "# Split train/val/test sets", "\n", "", "for", "file", "in", "os", ".", "listdir", "(", "self", ".", "root_dir", ")", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "file", ")", "\n", "video_files", "=", "[", "name", "for", "name", "in", "os", ".", "listdir", "(", "file_path", ")", "]", "\n", "\n", "train_and_valid", ",", "test", "=", "train_test_split", "(", "video_files", ",", "test_size", "=", "0.2", ",", "random_state", "=", "42", ")", "\n", "train", ",", "val", "=", "train_test_split", "(", "train_and_valid", ",", "test_size", "=", "0.2", ",", "random_state", "=", "42", ")", "\n", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'train'", ",", "file", ")", "\n", "val_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'val'", ",", "file", ")", "\n", "test_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "'test'", ",", "file", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "train_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "val_dir", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "val_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "test_dir", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "test_dir", ")", "\n", "\n", "", "for", "video", "in", "train", ":", "\n", "                ", "self", ".", "process_video", "(", "video", ",", "file", ",", "train_dir", ")", "\n", "\n", "", "for", "video", "in", "val", ":", "\n", "                ", "self", ".", "process_video", "(", "video", ",", "file", ",", "val_dir", ")", "\n", "\n", "", "for", "video", "in", "test", ":", "\n", "                ", "self", ".", "process_video", "(", "video", ",", "file", ",", "test_dir", ")", "\n", "\n", "", "", "print", "(", "'Preprocessing finished.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.process_video": [[153, 192], ["cv2.VideoCapture", "int", "int", "int", "cv2.VideoCapture.release", "video.split", "os.path.exists", "os.mkdir", "os.path.join", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "cv2.VideoCapture.read", "os.path.join", "os.path.join", "cv2.imwrite", "cv2.resize", "os.path.join", "str"], "methods", ["None"], ["", "def", "process_video", "(", "self", ",", "video", ",", "action_name", ",", "save_dir", ")", ":", "\n", "# Initialize a VideoCapture object to read video data into a numpy array", "\n", "        ", "video_filename", "=", "video", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "video_filename", ")", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "video_filename", ")", ")", "\n", "\n", "", "capture", "=", "cv2", ".", "VideoCapture", "(", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "action_name", ",", "video", ")", ")", "\n", "\n", "frame_count", "=", "int", "(", "capture", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_COUNT", ")", ")", "\n", "frame_width", "=", "int", "(", "capture", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ")", ")", "\n", "frame_height", "=", "int", "(", "capture", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ")", ")", "\n", "\n", "# Make sure splited video has at least 16 frames", "\n", "EXTRACT_FREQUENCY", "=", "4", "\n", "if", "frame_count", "//", "EXTRACT_FREQUENCY", "<=", "16", ":", "\n", "            ", "EXTRACT_FREQUENCY", "-=", "1", "\n", "if", "frame_count", "//", "EXTRACT_FREQUENCY", "<=", "16", ":", "\n", "                ", "EXTRACT_FREQUENCY", "-=", "1", "\n", "if", "frame_count", "//", "EXTRACT_FREQUENCY", "<=", "16", ":", "\n", "                    ", "EXTRACT_FREQUENCY", "-=", "1", "\n", "\n", "", "", "", "count", "=", "0", "\n", "i", "=", "0", "\n", "retaining", "=", "True", "\n", "\n", "while", "(", "count", "<", "frame_count", "and", "retaining", ")", ":", "\n", "            ", "retaining", ",", "frame", "=", "capture", ".", "read", "(", ")", "\n", "if", "frame", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "count", "%", "EXTRACT_FREQUENCY", "==", "0", ":", "\n", "                ", "if", "(", "frame_height", "!=", "self", ".", "resize_height", ")", "or", "(", "frame_width", "!=", "self", ".", "resize_width", ")", ":", "\n", "                    ", "frame", "=", "cv2", ".", "resize", "(", "frame", ",", "(", "self", ".", "resize_width", ",", "self", ".", "resize_height", ")", ")", "\n", "", "cv2", ".", "imwrite", "(", "filename", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "video_filename", ",", "'0000{}.jpg'", ".", "format", "(", "str", "(", "i", ")", ")", ")", ",", "img", "=", "frame", ")", "\n", "i", "+=", "1", "\n", "", "count", "+=", "1", "\n", "\n", "# Release the VideoCapture once it is no longer needed", "\n", "", "capture", ".", "release", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.randomflip": [[193, 202], ["numpy.random.random", "enumerate", "cv2.flip", "cv2.flip"], "methods", ["None"], ["", "def", "randomflip", "(", "self", ",", "buffer", ")", ":", "\n", "        ", "\"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"", "\n", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "for", "i", ",", "frame", "in", "enumerate", "(", "buffer", ")", ":", "\n", "                ", "frame", "=", "cv2", ".", "flip", "(", "buffer", "[", "i", "]", ",", "flipCode", "=", "1", ")", "\n", "buffer", "[", "i", "]", "=", "cv2", ".", "flip", "(", "frame", ",", "flipCode", "=", "1", ")", "\n", "\n", "", "", "return", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.normalize": [[204, 210], ["enumerate", "numpy.array"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "buffer", ")", ":", "\n", "        ", "for", "i", ",", "frame", "in", "enumerate", "(", "buffer", ")", ":", "\n", "            ", "frame", "-=", "np", ".", "array", "(", "[", "[", "[", "90.0", ",", "98.0", ",", "102.0", "]", "]", "]", ")", "\n", "buffer", "[", "i", "]", "=", "frame", "\n", "\n", "", "return", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.to_tensor": [[211, 213], ["buffer.transpose"], "methods", ["None"], ["", "def", "to_tensor", "(", "self", ",", "buffer", ")", ":", "\n", "        ", "return", "buffer", ".", "transpose", "(", "(", "3", ",", "0", ",", "1", ",", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.load_frames": [[214, 223], ["sorted", "len", "numpy.empty", "enumerate", "numpy.dtype", "numpy.array().astype", "os.path.join", "os.listdir", "numpy.array", "cv2.imread"], "methods", ["None"], ["", "def", "load_frames", "(", "self", ",", "file_dir", ")", ":", "\n", "        ", "frames", "=", "sorted", "(", "[", "os", ".", "path", ".", "join", "(", "file_dir", ",", "img", ")", "for", "img", "in", "os", ".", "listdir", "(", "file_dir", ")", "]", ")", "\n", "frame_count", "=", "len", "(", "frames", ")", "\n", "buffer", "=", "np", ".", "empty", "(", "(", "frame_count", ",", "self", ".", "resize_height", ",", "self", ".", "resize_width", ",", "3", ")", ",", "np", ".", "dtype", "(", "'float32'", ")", ")", "\n", "for", "i", ",", "frame_name", "in", "enumerate", "(", "frames", ")", ":", "\n", "            ", "frame", "=", "np", ".", "array", "(", "cv2", ".", "imread", "(", "frame_name", ")", ")", ".", "astype", "(", "np", ".", "float64", ")", "\n", "buffer", "[", "i", "]", "=", "frame", "\n", "\n", "", "return", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.jfzhang95_pytorch-video-recognition.dataloaders.dataset.VideoDataset.crop": [[224, 240], ["numpy.random.randint", "numpy.random.randint", "numpy.random.randint"], "methods", ["None"], ["", "def", "crop", "(", "self", ",", "buffer", ",", "clip_len", ",", "crop_size", ")", ":", "\n", "# randomly select time index for temporal jittering", "\n", "        ", "time_index", "=", "np", ".", "random", ".", "randint", "(", "buffer", ".", "shape", "[", "0", "]", "-", "clip_len", ")", "\n", "\n", "# Randomly select start indices in order to crop the video", "\n", "height_index", "=", "np", ".", "random", ".", "randint", "(", "buffer", ".", "shape", "[", "1", "]", "-", "crop_size", ")", "\n", "width_index", "=", "np", ".", "random", ".", "randint", "(", "buffer", ".", "shape", "[", "2", "]", "-", "crop_size", ")", "\n", "\n", "# Crop and jitter the video using indexing. The spatial crop is performed on", "\n", "# the entire array, so each frame is cropped in the same location. The temporal", "\n", "# jitter takes place via the selection of consecutive frames", "\n", "buffer", "=", "buffer", "[", "time_index", ":", "time_index", "+", "clip_len", ",", "\n", "height_index", ":", "height_index", "+", "crop_size", ",", "\n", "width_index", ":", "width_index", "+", "crop_size", ",", ":", "]", "\n", "\n", "return", "buffer", "\n", "\n"]]}