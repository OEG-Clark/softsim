{"home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.DenseRetriever.__init__": [[107, 114], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "question_encoder", ":", "nn", ".", "Module", ",", "batch_size", ":", "int", ",", "tensorizer", ":", "Tensorizer", "\n", ")", ":", "\n", "        ", "self", ".", "question_encoder", "=", "question_encoder", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "selector", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.DenseRetriever.generate_question_vectors": [[116, 129], ["caption_dense_retriever.DenseRetriever.question_encoder.eval", "caption_dense_retriever.DenseRetriever.generate_question_vectors"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.generate_question_vectors"], ["", "def", "generate_question_vectors", "(", "\n", "self", ",", "questions", ":", "List", "[", "str", "]", ",", "query_token", ":", "str", "=", "None", ",", "img_id", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "T", ":", "\n", "\n", "            ", "bsz", "=", "self", ".", "batch_size", "\n", "self", ".", "question_encoder", ".", "eval", "(", ")", "\n", "return", "generate_question_vectors", "(", "\n", "self", ".", "question_encoder", ",", "\n", "self", ".", "tensorizer", ",", "\n", "questions", ",", "\n", "bsz", ",", "\n", "query_token", "=", "query_token", ",", "\n", "selector", "=", "self", ".", "selector", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.LocalFaissRetriever.__init__": [[137, 146], ["caption_dense_retriever.DenseRetriever.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "question_encoder", ":", "nn", ".", "Module", ",", "\n", "batch_size", ":", "int", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "index", ":", "DenseIndexer", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "question_encoder", ",", "batch_size", ",", "tensorizer", ")", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.LocalFaissRetriever.index_encoded_data": [[147, 169], ["enumerate", "caption_dense_retriever.LocalFaissRetriever.index.index_data", "logger.info", "caption_dense_retriever.iterate_encoded_files", "buffer.append", "len", "caption_dense_retriever.LocalFaissRetriever.index.index_data"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.index_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.iterate_encoded_files", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.index_data"], ["", "def", "index_encoded_data", "(", "\n", "self", ",", "\n", "vector_files", ":", "List", "[", "str", "]", ",", "\n", "buffer_size", ":", "int", ",", "\n", "path_id_prefixes", ":", "List", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Indexes encoded passages takes form a list of files\n        :param vector_files: file names to get passages vectors from\n        :param buffer_size: size of a buffer (amount of passages) to send for the indexing at once\n        :return:\n        \"\"\"", "\n", "buffer", "=", "[", "]", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "\n", "iterate_encoded_files", "(", "vector_files", ",", "path_id_prefixes", "=", "path_id_prefixes", ")", "\n", ")", ":", "\n", "            ", "buffer", ".", "append", "(", "item", ")", "\n", "if", "0", "<", "buffer_size", "==", "len", "(", "buffer", ")", ":", "\n", "                ", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "buffer", "=", "[", "]", "\n", "", "", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "logger", ".", "info", "(", "\"Data indexing completed.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.LocalFaissRetriever.get_top_docs": [[170, 184], ["time.time", "caption_dense_retriever.LocalFaissRetriever.index.search_knn", "logger.info", "time.time"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.search_knn"], ["", "def", "get_top_docs", "(", "\n", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "=", "100", "\n", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Does the retrieval of the best matching passages given the query vectors batch\n        :param query_vectors:\n        :param top_docs:\n        :return:\n        \"\"\"", "\n", "time0", "=", "time", ".", "time", "(", ")", "\n", "results", "=", "self", ".", "index", ".", "search_knn", "(", "query_vectors", ",", "top_docs", ")", "\n", "logger", ".", "info", "(", "\"index search time: %f sec.\"", ",", "time", ".", "time", "(", ")", "-", "time0", ")", "\n", "self", ".", "index", "=", "None", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.generate_question_vectors": [[47, 105], ["len", "torch.cat", "logger.info", "torch.no_grad", "enumerate", "torch.cat.size", "torch.cat.size", "len", "range", "torch.stack().cuda", "torch.zeros_like().cuda", "tensorizer.get_attn_mask", "query_vectors.extend", "selector.get_positions", "dpr.models.biencoder.BiEncoder.get_representation", "question_encoder", "out.cpu().split", "logger.info", "tensorizer.text_to_tensor", "torch.stack", "torch.zeros_like", "len", "len", "dpr.models.biencoder._select_span_with_token", "tensorizer.text_to_tensor", "out.cpu"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder._select_span_with_token", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["def", "generate_question_vectors", "(", "\n", "question_encoder", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "questions", ":", "List", "[", "str", "]", ",", "\n", "bsz", ":", "int", ",", "\n", "query_token", ":", "str", "=", "None", ",", "\n", "selector", ":", "RepTokenSelector", "=", "None", ",", "\n", ")", "->", "T", ":", "\n", "    ", "n", "=", "len", "(", "questions", ")", "\n", "query_vectors", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "n", ",", "bsz", ")", ")", ":", "\n", "            ", "batch_questions", "=", "questions", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "\n", "\n", "if", "query_token", ":", "\n", "# TODO: tmp workaround for EL, remove or revise", "\n", "                ", "if", "query_token", "==", "\"[START_ENT]\"", ":", "\n", "                    ", "batch_token_tensors", "=", "[", "\n", "_select_span_with_token", "(", "q", ",", "tensorizer", ",", "token_str", "=", "query_token", ")", "\n", "for", "q", "in", "batch_questions", "\n", "]", "\n", "", "else", ":", "\n", "                    ", "batch_token_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\" \"", ".", "join", "(", "[", "query_token", ",", "q", "]", ")", ")", "\n", "for", "q", "in", "batch_questions", "\n", "]", "\n", "", "", "else", ":", "\n", "                ", "batch_token_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "q", ")", "for", "q", "in", "batch_questions", "\n", "]", "\n", "\n", "", "q_ids_batch", "=", "torch", ".", "stack", "(", "batch_token_tensors", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "q_seg_batch", "=", "torch", ".", "zeros_like", "(", "q_ids_batch", ")", ".", "cuda", "(", ")", "\n", "q_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "q_ids_batch", ")", "\n", "\n", "if", "selector", ":", "\n", "                ", "rep_positions", "=", "selector", ".", "get_positions", "(", "q_ids_batch", ",", "tensorizer", ")", "\n", "\n", "_", ",", "out", ",", "_", "=", "BiEncoder", ".", "get_representation", "(", "\n", "question_encoder", ",", "\n", "q_ids_batch", ",", "\n", "q_seg_batch", ",", "\n", "q_attn_mask", ",", "\n", "representation_token_pos", "=", "rep_positions", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "_", ",", "out", ",", "_", "=", "question_encoder", "(", "q_ids_batch", ",", "q_seg_batch", ",", "q_attn_mask", ")", "\n", "\n", "", "query_vectors", ".", "extend", "(", "out", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "if", "len", "(", "query_vectors", ")", "%", "100", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Encoded queries %d\"", ",", "len", "(", "query_vectors", ")", ")", "\n", "\n", "", "", "", "query_tensor", "=", "torch", ".", "cat", "(", "query_vectors", ",", "dim", "=", "0", ")", "\n", "logger", ".", "info", "(", "\"Total encoded queries tensor %s\"", ",", "query_tensor", ".", "size", "(", ")", ")", "\n", "assert", "query_tensor", ".", "size", "(", "0", ")", "==", "len", "(", "questions", ")", "\n", "return", "query_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.validate": [[186, 202], ["dpr.data.qa_validation.calculate_matches", "logger.info", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_matches"], ["", "", "def", "validate", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "result_ctx_ids", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "List", "[", "List", "[", "bool", "]", "]", ":", "\n", "    ", "match_stats", "=", "calculate_matches", "(", "\n", "passages", ",", "answers", ",", "result_ctx_ids", ",", "workers_num", ",", "match_type", "\n", ")", "\n", "top_k_hits", "=", "match_stats", ".", "top_k_hits", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits %s\"", ",", "top_k_hits", ")", "\n", "top_k_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits accuracy %s\"", ",", "top_k_hits", ")", "\n", "return", "match_stats", ".", "questions_doc_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.save_results": [[204, 243], ["enumerate", "logger.info", "len", "merged_data.append", "open", "writer.write", "str", "json.dumps", "range"], "function", ["None"], ["", "def", "save_results", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "questions", ":", "List", "[", "str", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "top_passages_and_scores", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "per_question_hits", ":", "List", "[", "List", "[", "bool", "]", "]", ",", "\n", "out_file", ":", "str", ",", "\n", ")", ":", "\n", "# join passages text with the result ids, their questions and assigning has|no answer labels", "\n", "    ", "merged_data", "=", "[", "]", "\n", "# assert len(per_question_hits) == len(questions) == len(answers)", "\n", "for", "i", ",", "q", "in", "enumerate", "(", "questions", ")", ":", "\n", "        ", "q_answers", "=", "answers", "[", "i", "]", "\n", "results_and_scores", "=", "top_passages_and_scores", "[", "i", "]", "\n", "hits", "=", "per_question_hits", "[", "i", "]", "\n", "docs", "=", "[", "passages", "[", "doc_id", "]", "for", "doc_id", "in", "results_and_scores", "[", "0", "]", "]", "\n", "scores", "=", "[", "str", "(", "score", ")", "for", "score", "in", "results_and_scores", "[", "1", "]", "]", "\n", "ctxs_num", "=", "len", "(", "hits", ")", "\n", "\n", "merged_data", ".", "append", "(", "\n", "{", "\n", "\"question\"", ":", "q", ",", "\n", "\"answers\"", ":", "q_answers", ",", "\n", "\"ctxs\"", ":", "[", "\n", "{", "\n", "\"id\"", ":", "results_and_scores", "[", "0", "]", "[", "c", "]", ",", "\n", "\"title\"", ":", "docs", "[", "c", "]", "[", "1", "]", ",", "\n", "\"text\"", ":", "docs", "[", "c", "]", "[", "0", "]", ",", "\n", "\"score\"", ":", "scores", "[", "c", "]", ",", "\n", "\"has_answer\"", ":", "hits", "[", "c", "]", ",", "\n", "}", "\n", "for", "c", "in", "range", "(", "ctxs_num", ")", "\n", "]", ",", "\n", "}", "\n", ")", "\n", "\n", "", "with", "open", "(", "out_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "merged_data", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "", "logger", ".", "info", "(", "\"Saved results * scores  to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.iterate_encoded_files": [[245, 260], ["enumerate", "logger.info", "open", "pickle.load", "list", "str().startswith", "str", "str"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "iterate_encoded_files", "(", "\n", "vector_files", ":", "list", ",", "path_id_prefixes", ":", "List", "=", "None", "\n", ")", "->", "Iterator", "[", "Tuple", "]", ":", "\n", "    ", "for", "i", ",", "file", "in", "enumerate", "(", "vector_files", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading file %s\"", ",", "file", ")", "\n", "id_prefix", "=", "None", "\n", "if", "path_id_prefixes", ":", "\n", "            ", "id_prefix", "=", "path_id_prefixes", "[", "i", "]", "\n", "", "with", "open", "(", "file", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "doc_vectors", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "for", "doc", "in", "doc_vectors", ":", "\n", "                ", "doc", "=", "list", "(", "doc", ")", "\n", "if", "id_prefix", "and", "not", "str", "(", "doc", "[", "0", "]", ")", ".", "startswith", "(", "id_prefix", ")", ":", "\n", "                    ", "doc", "[", "0", "]", "=", "id_prefix", "+", "str", "(", "doc", "[", "0", "]", ")", "\n", "", "yield", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.validate_tables": [[262, 284], ["dpr.data.qa_validation.calculate_chunked_matches", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_chunked_matches"], ["", "", "", "", "def", "validate_tables", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "TableChunk", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "result_ctx_ids", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "List", "[", "List", "[", "bool", "]", "]", ":", "\n", "    ", "match_stats", "=", "calculate_chunked_matches", "(", "\n", "passages", ",", "answers", ",", "result_ctx_ids", ",", "workers_num", ",", "match_type", "\n", ")", "\n", "top_k_chunk_hits", "=", "match_stats", ".", "top_k_chunk_hits", "\n", "top_k_table_hits", "=", "match_stats", ".", "top_k_table_hits", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits %s\"", ",", "top_k_chunk_hits", ")", "\n", "top_k_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_chunk_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k table chunk hits accuracy %s\"", ",", "top_k_hits", ")", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k tables hits %s\"", ",", "top_k_table_hits", ")", "\n", "top_k_table_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_table_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k tables accuracy %s\"", ",", "top_k_table_hits", ")", "\n", "\n", "return", "match_stats", ".", "top_k_chunk_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.caption_dense_retriever.main": [[286, 460], ["hydra.main", "dpr.options.setup_cfg_gpu", "logger.info", "logger.info", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_cfg_params_from_state", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "getattr.eval", "dpr.utils.model_utils.get_model_obj", "logger.info", "len", "logger.info", "dpr.utils.model_utils.get_model_obj.load_state_dict", "dpr.utils.model_utils.get_model_obj.get_out_size", "logger.info", "logger.info", "hydra.utils.instantiate", "hydra.utils.instantiate.load_data", "hydra.utils.instantiate", "logger.info", "hydra.utils.instantiate.init_index", "caption_dense_retriever.LocalFaissRetriever", "logger.info", "caption_dense_retriever.DenseRetriever.generate_question_vectors", "logger.info", "logger.info", "enumerate", "logger.info", "caption_dense_retriever.LocalFaissRetriever.get_top_docs", "omegaconf.OmegaConf.to_yaml", "logger.info", "getattr", "logger.info", "logger.warning", "questions.append", "question_answers.append", "img_id.append", "type", "logger.info", "hydra.utils.instantiate", "id_prefixes.append", "ctx_sources.append", "glob.glob", "input_paths.extend", "path_id_prefixes.extend", "hydra.utils.instantiate.index_exists", "logger.info", "object.index", "logger.info", "caption_dense_retriever.LocalFaissRetriever.index_encoded_data", "retriever.generate_question_vectors.numpy", "hydra.utils.instantiate.load_data_to", "len", "RuntimeError", "caption_dense_retriever.validate_tables", "caption_dense_retriever.validate", "caption_dense_retriever.save_results", "next", "hasattr", "next.convert_to_kilt", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.items", "key.startswith", "len", "len", "len", "len", "object.index", "iter", "RuntimeError", "len", "isinstance"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.main", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_cfg_gpu", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_cfg_params_from_state", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.get_out_size", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.init_index", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.generate_question_vectors", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.get_top_docs", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.index_exists", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.index_encoded_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlTablesCtxSrc.load_data_to", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate_tables", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.save_results", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltCsvCtxSrc.convert_to_kilt"], ["", "@", "hydra", ".", "main", "(", "config_path", "=", "\"conf\"", ",", "config_name", "=", "\"caption_dense_retriever\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "cfg", "=", "setup_cfg_gpu", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"CFG (after gpu  configuration):\"", ")", "\n", "logger", ".", "info", "(", "\"%s\"", ",", "OmegaConf", ".", "to_yaml", "(", "cfg", ")", ")", "\n", "\n", "saved_state", "=", "load_states_from_checkpoint", "(", "cfg", ".", "model_file", ")", "\n", "set_cfg_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "cfg", ")", "\n", "\n", "tensorizer", ",", "encoder", ",", "_", "=", "init_biencoder_components", "(", "\n", "cfg", ".", "encoder", ".", "encoder_model_type", ",", "cfg", ",", "inference_only", "=", "True", ",", "load_images", "=", "False", "\n", ")", "\n", "\n", "encoder_path", "=", "cfg", ".", "encoder_path", "\n", "if", "encoder_path", ":", "\n", "        ", "logger", ".", "info", "(", "\"Selecting encoder: %s\"", ",", "encoder_path", ")", "\n", "encoder", "=", "getattr", "(", "encoder", ",", "encoder_path", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Selecting standard question encoder\"", ")", "\n", "encoder", "=", "encoder", ".", "question_model", "\n", "\n", "", "encoder", ",", "_", "=", "setup_for_distributed_mode", "(", "\n", "encoder", ",", "None", ",", "cfg", ".", "device", ",", "cfg", ".", "n_gpu", ",", "cfg", ".", "local_rank", ",", "cfg", ".", "fp16", "\n", ")", "\n", "encoder", ".", "eval", "(", ")", "\n", "\n", "# load weights from the model file", "\n", "model_to_load", "=", "get_model_obj", "(", "encoder", ")", "\n", "logger", ".", "info", "(", "\"Loading saved model state ...\"", ")", "\n", "\n", "encoder_prefix", "=", "(", "encoder_path", "if", "encoder_path", "else", "\"question_model\"", ")", "+", "\".\"", "\n", "prefix_len", "=", "len", "(", "encoder_prefix", ")", "\n", "\n", "logger", ".", "info", "(", "\"Encoder state prefix %s\"", ",", "encoder_prefix", ")", "\n", "question_encoder_state", "=", "{", "\n", "key", "[", "prefix_len", ":", "]", ":", "value", "\n", "for", "(", "key", ",", "value", ")", "in", "saved_state", ".", "model_dict", ".", "items", "(", ")", "\n", "if", "key", ".", "startswith", "(", "encoder_prefix", ")", "\n", "}", "\n", "model_to_load", ".", "load_state_dict", "(", "question_encoder_state", ")", "\n", "vector_size", "=", "model_to_load", ".", "get_out_size", "(", ")", "\n", "logger", ".", "info", "(", "\"Encoder vector_size=%d\"", ",", "vector_size", ")", "\n", "\n", "# get questions & answers", "\n", "questions", "=", "[", "]", "\n", "question_answers", "=", "[", "]", "\n", "img_id", "=", "[", "]", "\n", "\n", "if", "not", "cfg", ".", "qa_dataset", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Please specify qa_dataset to use\"", ")", "\n", "return", "\n", "\n", "", "ds_key", "=", "cfg", ".", "qa_dataset", "\n", "logger", ".", "info", "(", "\"qa_dataset: %s\"", ",", "ds_key", ")", "\n", "\n", "qa_src", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "datasets", "[", "ds_key", "]", ")", "\n", "qa_src", ".", "load_data", "(", ")", "\n", "\n", "for", "ds_item", "in", "qa_src", ".", "data", ":", "\n", "\n", "        ", "question", ",", "answers", "=", "ds_item", "[", "'question'", "]", ",", "ds_item", "[", "'answers'", "]", "\n", "questions", ".", "append", "(", "question", ")", "\n", "question_answers", ".", "append", "(", "answers", ")", "\n", "img_id", ".", "append", "(", "ds_item", "[", "'img_id'", "]", ")", "\n", "\n", "", "index", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "indexers", "[", "cfg", ".", "indexer", "]", ")", "\n", "logger", ".", "info", "(", "\"Index class %s \"", ",", "type", "(", "index", ")", ")", "\n", "index_buffer_sz", "=", "index", ".", "buffer_size", "\n", "index", ".", "init_index", "(", "vector_size", ")", "\n", "retriever", "=", "LocalFaissRetriever", "(", "encoder", ",", "cfg", ".", "batch_size", ",", "tensorizer", ",", "index", ")", "\n", "\n", "logger", ".", "info", "(", "\"Using special token %s\"", ",", "qa_src", ".", "special_token", ")", "\n", "questions_tensor", "=", "retriever", ".", "generate_question_vectors", "(", "\n", "questions", ",", "query_token", "=", "qa_src", ".", "special_token", ",", "img_id", "=", "img_id", "\n", ")", "\n", "\n", "if", "qa_src", ".", "selector", ":", "\n", "        ", "logger", ".", "info", "(", "\"Using custom representation token selector\"", ")", "\n", "retriever", ".", "selector", "=", "qa_src", ".", "selector", "\n", "\n", "", "id_prefixes", "=", "[", "]", "\n", "ctx_sources", "=", "[", "]", "\n", "for", "ctx_src", "in", "cfg", ".", "ctx_datatsets", ":", "\n", "        ", "ctx_src", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "ctx_sources", "[", "ctx_src", "]", ")", "\n", "id_prefixes", ".", "append", "(", "ctx_src", ".", "id_prefix", ")", "\n", "ctx_sources", ".", "append", "(", "ctx_src", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"id_prefixes per dataset: %s\"", ",", "id_prefixes", ")", "\n", "\n", "# index all passages", "\n", "ctx_files_patterns", "=", "cfg", ".", "encoded_ctx_files", "\n", "index_path", "=", "cfg", ".", "index_path", "\n", "\n", "logger", ".", "info", "(", "\"ctx_files_patterns: %s\"", ",", "ctx_files_patterns", ")", "\n", "if", "ctx_files_patterns", ":", "\n", "        ", "assert", "len", "(", "ctx_files_patterns", ")", "==", "len", "(", "\n", "id_prefixes", "\n", ")", ",", "\"ctx len={} pref leb={}\"", ".", "format", "(", "len", "(", "ctx_files_patterns", ")", ",", "len", "(", "id_prefixes", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "(", "\n", "index_path", "\n", ")", ",", "\"Either encoded_ctx_files or index_path parameter should be set.\"", "\n", "\n", "", "input_paths", "=", "[", "]", "\n", "path_id_prefixes", "=", "[", "]", "\n", "for", "i", ",", "pattern", "in", "enumerate", "(", "ctx_files_patterns", ")", ":", "\n", "        ", "pattern_files", "=", "glob", ".", "glob", "(", "pattern", ")", "\n", "pattern_id_prefix", "=", "id_prefixes", "[", "i", "]", "\n", "input_paths", ".", "extend", "(", "pattern_files", ")", "\n", "path_id_prefixes", ".", "extend", "(", "[", "pattern_id_prefix", "]", "*", "len", "(", "pattern_files", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Embeddings files id prefixes: %s\"", ",", "path_id_prefixes", ")", "\n", "\n", "if", "index_path", "and", "index", ".", "index_exists", "(", "index_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Index path: %s\"", ",", "index_path", ")", "\n", "retriever", ".", "index", ".", "deserialize", "(", "index_path", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading all passages data from files: %s\"", ",", "input_paths", ")", "\n", "retriever", ".", "index_encoded_data", "(", "\n", "input_paths", ",", "index_buffer_sz", ",", "path_id_prefixes", "=", "path_id_prefixes", "\n", ")", "\n", "if", "index_path", ":", "\n", "            ", "retriever", ".", "index", ".", "serialize", "(", "index_path", ")", "\n", "\n", "# get top k results", "\n", "", "", "top_ids_and_scores", "=", "retriever", ".", "get_top_docs", "(", "questions_tensor", ".", "numpy", "(", ")", ",", "cfg", ".", "n_docs", ")", "\n", "\n", "# we no longer need the index", "\n", "retriever", "=", "None", "\n", "\n", "all_passages", "=", "{", "}", "\n", "for", "ctx_src", "in", "ctx_sources", ":", "\n", "        ", "ctx_src", ".", "load_data_to", "(", "all_passages", ")", "\n", "\n", "", "if", "len", "(", "all_passages", ")", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"No passages data found. Please specify ctx_file param properly.\"", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "validate_as_tables", ":", "\n", "        ", "questions_doc_hits", "=", "validate_tables", "(", "\n", "all_passages", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "cfg", ".", "validation_workers", ",", "\n", "cfg", ".", "match", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "questions_doc_hits", "=", "validate", "(", "\n", "all_passages", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "cfg", ".", "validation_workers", ",", "\n", "cfg", ".", "match", ",", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "out_file", ":", "\n", "        ", "save_results", "(", "\n", "all_passages", ",", "\n", "questions", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "questions_doc_hits", ",", "\n", "cfg", ".", "out_file", ",", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "kilt_out_file", ":", "\n", "        ", "kilt_ctx", "=", "next", "(", "\n", "iter", "(", "[", "ctx", "for", "ctx", "in", "ctx_sources", "if", "isinstance", "(", "ctx", ",", "KiltCsvCtxSrc", ")", "]", ")", ",", "None", "\n", ")", "\n", "if", "not", "kilt_ctx", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"No Kilt compatible context file provided\"", ")", "\n", "", "assert", "hasattr", "(", "cfg", ",", "\"kilt_out_file\"", ")", "\n", "kilt_ctx", ".", "convert_to_kilt", "(", "qa_src", ".", "kilt_gold_file", ",", "cfg", ".", "out_file", ",", "cfg", ".", "kilt_out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.DenseRetriever.__init__": [[229, 236], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "question_encoder", ":", "nn", ".", "Module", ",", "batch_size", ":", "int", ",", "tensorizer", ":", "Tensorizer", "\n", ")", ":", "\n", "        ", "self", ".", "question_encoder", "=", "question_encoder", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "selector", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.DenseRetriever.generate_question_vectors": [[252, 265], ["image_dense_retriever.DenseRetriever.question_encoder.eval", "image_dense_retriever.DenseRetriever.generate_question_vectors"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.generate_question_vectors"], ["", "def", "generate_question_vectors", "(", "\n", "self", ",", "questions", ":", "List", "[", "str", "]", ",", "query_token", ":", "str", "=", "None", ",", "img_id", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "T", ":", "\n", "\n", "            ", "bsz", "=", "self", ".", "batch_size", "\n", "self", ".", "question_encoder", ".", "eval", "(", ")", "\n", "return", "generate_question_vectors", "(", "\n", "self", ".", "question_encoder", ",", "\n", "self", ".", "tensorizer", ",", "\n", "questions", ",", "\n", "bsz", ",", "\n", "query_token", "=", "query_token", ",", "\n", "selector", "=", "self", ".", "selector", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.__init__": [[273, 282], ["image_dense_retriever.DenseRetriever.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "question_encoder", ":", "nn", ".", "Module", ",", "\n", "batch_size", ":", "int", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "index", ":", "DenseIndexer", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "question_encoder", ",", "batch_size", ",", "tensorizer", ")", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.index_encoded_data": [[283, 305], ["enumerate", "image_dense_retriever.LocalFaissRetriever.index.index_data", "logger.info", "image_dense_retriever.iterate_encoded_files", "buffer.append", "len", "image_dense_retriever.LocalFaissRetriever.index.index_data"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.index_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.iterate_encoded_files", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.index_data"], ["", "def", "index_encoded_data", "(", "\n", "self", ",", "\n", "vector_files", ":", "List", "[", "str", "]", ",", "\n", "buffer_size", ":", "int", ",", "\n", "path_id_prefixes", ":", "List", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Indexes encoded passages takes form a list of files\n        :param vector_files: file names to get passages vectors from\n        :param buffer_size: size of a buffer (amount of passages) to send for the indexing at once\n        :return:\n        \"\"\"", "\n", "buffer", "=", "[", "]", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "\n", "iterate_encoded_files", "(", "vector_files", ",", "path_id_prefixes", "=", "path_id_prefixes", ")", "\n", ")", ":", "\n", "            ", "buffer", ".", "append", "(", "item", ")", "\n", "if", "0", "<", "buffer_size", "==", "len", "(", "buffer", ")", ":", "\n", "                ", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "buffer", "=", "[", "]", "\n", "", "", "self", ".", "index", ".", "index_data", "(", "buffer", ")", "\n", "logger", ".", "info", "(", "\"Data indexing completed.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.get_top_docs": [[306, 320], ["time.time", "image_dense_retriever.LocalFaissRetriever.index.search_knn", "logger.info", "time.time"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.search_knn"], ["", "def", "get_top_docs", "(", "\n", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "=", "100", "\n", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Does the retrieval of the best matching passages given the query vectors batch\n        :param query_vectors:\n        :param top_docs:\n        :return:\n        \"\"\"", "\n", "time0", "=", "time", ".", "time", "(", ")", "\n", "results", "=", "self", ".", "index", ".", "search_knn", "(", "query_vectors", ",", "top_docs", ")", "\n", "logger", ".", "info", "(", "\"index search time: %f sec.\"", ",", "time", ".", "time", "(", ")", "-", "time0", ")", "\n", "self", ".", "index", "=", "None", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.load_images": [[49, 85], ["print", "os.listdir", "print", "numpy.load", "d.tolist.tolist", "float", "float", "torch.tensor", "torch.tensor.unsqueeze", "torch.tensor().unsqueeze", "os.path.join", "torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["def", "load_images", "(", "dir", "=", "load_images_path", ",", "dummy", "=", "False", ")", ":", "\n", "    ", "visual_feats", "=", "{", "}", "\n", "visual_pos", "=", "{", "}", "\n", "print", "(", "\"start to load image features\"", ")", "\n", "files", "=", "os", ".", "listdir", "(", "dir", ")", "\n", "if", "dummy", ":", "\n", "        ", "files", "=", "files", "[", ":", "10", "]", "\n", "", "for", "file", "in", "files", ":", "\n", "        ", "d", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "file", ")", ",", "allow_pickle", "=", "True", ")", "\n", "d", "=", "d", ".", "tolist", "(", ")", "\n", "imgid", "=", "d", "[", "'image_id'", "]", "\n", "img_h", "=", "float", "(", "d", "[", "'image_height'", "]", ")", "\n", "img_w", "=", "float", "(", "d", "[", "'image_width'", "]", ")", "\n", "feats", "=", "torch", ".", "tensor", "(", "d", "[", "'features'", "]", ")", "\n", "visual_feats", "[", "imgid", "]", "=", "feats", ".", "unsqueeze", "(", "0", ")", "\n", "boxes", "=", "d", "[", "'bbox'", "]", "\n", "boxes", "[", ":", ",", "(", "0", ",", "2", ")", "]", "/=", "img_w", "\n", "boxes", "[", ":", ",", "(", "1", ",", "3", ")", "]", "/=", "img_h", "\n", "for", "bd_box", "in", "boxes", ":", "\n", "            ", "if", "bd_box", "[", "0", "]", "<", "-", "0.0001", ":", "\n", "                ", "bd_box", "[", "0", "]", "=", "0", "\n", "", "if", "bd_box", "[", "0", "]", ">", "1", ":", "\n", "                ", "bd_box", "[", "0", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "2", "]", ">", "1", ":", "\n", "                ", "bd_box", "[", "2", "]", "=", "1", "\n", "", "if", "bd_box", "[", "1", "]", "<", "-", "0.0001", ":", "\n", "                ", "bd_box", "[", "1", "]", "=", "0", "\n", "", "if", "bd_box", "[", "1", "]", ">", "1", ":", "\n", "                ", "bd_box", "[", "1", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "3", "]", ">", "1", ":", "\n", "                ", "bd_box", "[", "3", "]", "=", "1", "\n", "", "", "boxes", "=", "torch", ".", "tensor", "(", "boxes", ")", ".", "unsqueeze", "(", "0", ")", "\n", "visual_pos", "[", "imgid", "]", "=", "boxes", "\n", "\n", "", "print", "(", "f\"loaded {len(visual_feats)} image features from {dir}\"", ")", "\n", "return", "visual_feats", ",", "visual_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.generate_question_vectors": [[169, 227], ["len", "torch.cat", "logger.info", "torch.no_grad", "enumerate", "torch.cat.size", "torch.cat.size", "len", "range", "torch.stack().cuda", "torch.zeros_like().cuda", "tensorizer.get_attn_mask", "query_vectors.extend", "selector.get_positions", "dpr.models.biencoder.BiEncoder.get_representation", "question_encoder", "out.cpu().split", "logger.info", "tensorizer.text_to_tensor", "torch.stack", "torch.zeros_like", "len", "len", "dpr.models.biencoder._select_span_with_token", "tensorizer.text_to_tensor", "out.cpu"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder._select_span_with_token", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["", "def", "generate_question_vectors", "(", "\n", "question_encoder", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "questions", ":", "List", "[", "str", "]", ",", "\n", "bsz", ":", "int", ",", "\n", "query_token", ":", "str", "=", "None", ",", "\n", "selector", ":", "RepTokenSelector", "=", "None", ",", "\n", ")", "->", "T", ":", "\n", "    ", "n", "=", "len", "(", "questions", ")", "\n", "query_vectors", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "n", ",", "bsz", ")", ")", ":", "\n", "            ", "batch_questions", "=", "questions", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "\n", "\n", "if", "query_token", ":", "\n", "# TODO: tmp workaround for EL, remove or revise", "\n", "                ", "if", "query_token", "==", "\"[START_ENT]\"", ":", "\n", "                    ", "batch_token_tensors", "=", "[", "\n", "_select_span_with_token", "(", "q", ",", "tensorizer", ",", "token_str", "=", "query_token", ")", "\n", "for", "q", "in", "batch_questions", "\n", "]", "\n", "", "else", ":", "\n", "                    ", "batch_token_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\" \"", ".", "join", "(", "[", "query_token", ",", "q", "]", ")", ")", "\n", "for", "q", "in", "batch_questions", "\n", "]", "\n", "", "", "else", ":", "\n", "                ", "batch_token_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "q", ")", "for", "q", "in", "batch_questions", "\n", "]", "\n", "\n", "", "q_ids_batch", "=", "torch", ".", "stack", "(", "batch_token_tensors", ",", "dim", "=", "0", ")", ".", "cuda", "(", ")", "\n", "q_seg_batch", "=", "torch", ".", "zeros_like", "(", "q_ids_batch", ")", ".", "cuda", "(", ")", "\n", "q_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "q_ids_batch", ")", "\n", "\n", "if", "selector", ":", "\n", "                ", "rep_positions", "=", "selector", ".", "get_positions", "(", "q_ids_batch", ",", "tensorizer", ")", "\n", "\n", "_", ",", "out", ",", "_", "=", "BiEncoder", ".", "get_representation", "(", "\n", "question_encoder", ",", "\n", "q_ids_batch", ",", "\n", "q_seg_batch", ",", "\n", "q_attn_mask", ",", "\n", "representation_token_pos", "=", "rep_positions", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "_", ",", "out", ",", "_", "=", "question_encoder", "(", "q_ids_batch", ",", "q_seg_batch", ",", "q_attn_mask", ")", "\n", "\n", "", "query_vectors", ".", "extend", "(", "out", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "if", "len", "(", "query_vectors", ")", "%", "100", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Encoded queries %d\"", ",", "len", "(", "query_vectors", ")", ")", "\n", "\n", "", "", "", "query_tensor", "=", "torch", ".", "cat", "(", "query_vectors", ",", "dim", "=", "0", ")", "\n", "logger", ".", "info", "(", "\"Total encoded queries tensor %s\"", ",", "query_tensor", ".", "size", "(", ")", ")", "\n", "assert", "query_tensor", ".", "size", "(", "0", ")", "==", "len", "(", "questions", ")", "\n", "return", "query_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate": [[322, 338], ["dpr.data.qa_validation.calculate_matches", "logger.info", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_matches"], ["", "", "def", "validate", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "result_ctx_ids", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "List", "[", "List", "[", "bool", "]", "]", ":", "\n", "    ", "match_stats", "=", "calculate_matches", "(", "\n", "passages", ",", "answers", ",", "result_ctx_ids", ",", "workers_num", ",", "match_type", "\n", ")", "\n", "top_k_hits", "=", "match_stats", ".", "top_k_hits", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits %s\"", ",", "top_k_hits", ")", "\n", "top_k_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits accuracy %s\"", ",", "top_k_hits", ")", "\n", "return", "match_stats", ".", "questions_doc_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.save_results": [[340, 379], ["enumerate", "logger.info", "len", "merged_data.append", "open", "writer.write", "str", "json.dumps", "range"], "function", ["None"], ["", "def", "save_results", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "questions", ":", "List", "[", "str", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "top_passages_and_scores", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "per_question_hits", ":", "List", "[", "List", "[", "bool", "]", "]", ",", "\n", "out_file", ":", "str", ",", "\n", ")", ":", "\n", "# join passages text with the result ids, their questions and assigning has|no answer labels", "\n", "    ", "merged_data", "=", "[", "]", "\n", "# assert len(per_question_hits) == len(questions) == len(answers)", "\n", "for", "i", ",", "q", "in", "enumerate", "(", "questions", ")", ":", "\n", "        ", "q_answers", "=", "answers", "[", "i", "]", "\n", "results_and_scores", "=", "top_passages_and_scores", "[", "i", "]", "\n", "hits", "=", "per_question_hits", "[", "i", "]", "\n", "docs", "=", "[", "passages", "[", "doc_id", "]", "for", "doc_id", "in", "results_and_scores", "[", "0", "]", "]", "\n", "scores", "=", "[", "str", "(", "score", ")", "for", "score", "in", "results_and_scores", "[", "1", "]", "]", "\n", "ctxs_num", "=", "len", "(", "hits", ")", "\n", "\n", "merged_data", ".", "append", "(", "\n", "{", "\n", "\"question\"", ":", "q", ",", "\n", "\"answers\"", ":", "q_answers", ",", "\n", "\"ctxs\"", ":", "[", "\n", "{", "\n", "\"id\"", ":", "results_and_scores", "[", "0", "]", "[", "c", "]", ",", "\n", "\"title\"", ":", "docs", "[", "c", "]", "[", "1", "]", ",", "\n", "\"text\"", ":", "docs", "[", "c", "]", "[", "0", "]", ",", "\n", "\"score\"", ":", "scores", "[", "c", "]", ",", "\n", "\"has_answer\"", ":", "hits", "[", "c", "]", ",", "\n", "}", "\n", "for", "c", "in", "range", "(", "ctxs_num", ")", "\n", "]", ",", "\n", "}", "\n", ")", "\n", "\n", "", "with", "open", "(", "out_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "        ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "merged_data", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "", "logger", ".", "info", "(", "\"Saved results * scores  to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.iterate_encoded_files": [[381, 396], ["enumerate", "logger.info", "open", "pickle.load", "list", "str().startswith", "str", "str"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "iterate_encoded_files", "(", "\n", "vector_files", ":", "list", ",", "path_id_prefixes", ":", "List", "=", "None", "\n", ")", "->", "Iterator", "[", "Tuple", "]", ":", "\n", "    ", "for", "i", ",", "file", "in", "enumerate", "(", "vector_files", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading file %s\"", ",", "file", ")", "\n", "id_prefix", "=", "None", "\n", "if", "path_id_prefixes", ":", "\n", "            ", "id_prefix", "=", "path_id_prefixes", "[", "i", "]", "\n", "", "with", "open", "(", "file", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "doc_vectors", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "for", "doc", "in", "doc_vectors", ":", "\n", "                ", "doc", "=", "list", "(", "doc", ")", "\n", "if", "id_prefix", "and", "not", "str", "(", "doc", "[", "0", "]", ")", ".", "startswith", "(", "id_prefix", ")", ":", "\n", "                    ", "doc", "[", "0", "]", "=", "id_prefix", "+", "str", "(", "doc", "[", "0", "]", ")", "\n", "", "yield", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate_tables": [[398, 420], ["dpr.data.qa_validation.calculate_chunked_matches", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_chunked_matches"], ["", "", "", "", "def", "validate_tables", "(", "\n", "passages", ":", "Dict", "[", "object", ",", "TableChunk", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "result_ctx_ids", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "List", "[", "List", "[", "bool", "]", "]", ":", "\n", "    ", "match_stats", "=", "calculate_chunked_matches", "(", "\n", "passages", ",", "answers", ",", "result_ctx_ids", ",", "workers_num", ",", "match_type", "\n", ")", "\n", "top_k_chunk_hits", "=", "match_stats", ".", "top_k_chunk_hits", "\n", "top_k_table_hits", "=", "match_stats", ".", "top_k_table_hits", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k documents hits %s\"", ",", "top_k_chunk_hits", ")", "\n", "top_k_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_chunk_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k table chunk hits accuracy %s\"", ",", "top_k_hits", ")", "\n", "\n", "logger", ".", "info", "(", "\"Validation results: top k tables hits %s\"", ",", "top_k_table_hits", ")", "\n", "top_k_table_hits", "=", "[", "v", "/", "len", "(", "result_ctx_ids", ")", "for", "v", "in", "top_k_table_hits", "]", "\n", "logger", ".", "info", "(", "\"Validation results: top k tables accuracy %s\"", ",", "top_k_table_hits", ")", "\n", "\n", "return", "match_stats", ".", "top_k_chunk_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.main": [[422, 596], ["hydra.main", "dpr.options.setup_cfg_gpu", "logger.info", "logger.info", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_cfg_params_from_state", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "getattr.eval", "dpr.utils.model_utils.get_model_obj", "logger.info", "len", "logger.info", "dpr.utils.model_utils.get_model_obj.load_state_dict", "dpr.utils.model_utils.get_model_obj.get_out_size", "logger.info", "logger.info", "hydra.utils.instantiate", "hydra.utils.instantiate.load_data", "hydra.utils.instantiate", "logger.info", "hydra.utils.instantiate.init_index", "image_dense_retriever.LocalFaissRetriever", "logger.info", "image_dense_retriever.DenseRetriever.generate_question_vectors", "logger.info", "logger.info", "enumerate", "logger.info", "image_dense_retriever.LocalFaissRetriever.get_top_docs", "omegaconf.OmegaConf.to_yaml", "logger.info", "getattr", "logger.info", "logger.warning", "questions.append", "question_answers.append", "img_id.append", "type", "logger.info", "hydra.utils.instantiate", "id_prefixes.append", "ctx_sources.append", "glob.glob", "input_paths.extend", "path_id_prefixes.extend", "hydra.utils.instantiate.index_exists", "logger.info", "object.index", "logger.info", "image_dense_retriever.LocalFaissRetriever.index_encoded_data", "retriever.generate_question_vectors.numpy", "hydra.utils.instantiate.load_data_to", "len", "RuntimeError", "image_dense_retriever.validate_tables", "image_dense_retriever.validate", "image_dense_retriever.save_results", "next", "hasattr", "next.convert_to_kilt", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.items", "key.startswith", "len", "len", "len", "len", "object.index", "iter", "RuntimeError", "len", "isinstance"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.main", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_cfg_gpu", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_cfg_params_from_state", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.get_out_size", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.init_index", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.generate_question_vectors", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.get_top_docs", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.index_exists", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.LocalFaissRetriever.index_encoded_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlTablesCtxSrc.load_data_to", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate_tables", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.validate", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.image_dense_retriever.save_results", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltCsvCtxSrc.convert_to_kilt"], ["", "@", "hydra", ".", "main", "(", "config_path", "=", "\"conf\"", ",", "config_name", "=", "\"dense_retriever\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "cfg", "=", "setup_cfg_gpu", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"CFG (after gpu  configuration):\"", ")", "\n", "logger", ".", "info", "(", "\"%s\"", ",", "OmegaConf", ".", "to_yaml", "(", "cfg", ")", ")", "\n", "\n", "saved_state", "=", "load_states_from_checkpoint", "(", "cfg", ".", "model_file", ")", "\n", "set_cfg_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "cfg", ")", "\n", "\n", "tensorizer", ",", "encoder", ",", "_", "=", "init_biencoder_components", "(", "\n", "cfg", ".", "encoder", ".", "encoder_model_type", ",", "cfg", ",", "inference_only", "=", "True", ",", "load_images", "=", "False", "\n", ")", "\n", "\n", "encoder_path", "=", "cfg", ".", "encoder_path", "\n", "if", "encoder_path", ":", "\n", "        ", "logger", ".", "info", "(", "\"Selecting encoder: %s\"", ",", "encoder_path", ")", "\n", "encoder", "=", "getattr", "(", "encoder", ",", "encoder_path", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Selecting standard question encoder\"", ")", "\n", "encoder", "=", "encoder", ".", "question_model", "\n", "\n", "", "encoder", ",", "_", "=", "setup_for_distributed_mode", "(", "\n", "encoder", ",", "None", ",", "cfg", ".", "device", ",", "cfg", ".", "n_gpu", ",", "cfg", ".", "local_rank", ",", "cfg", ".", "fp16", "\n", ")", "\n", "encoder", ".", "eval", "(", ")", "\n", "\n", "# load weights from the model file", "\n", "model_to_load", "=", "get_model_obj", "(", "encoder", ")", "\n", "logger", ".", "info", "(", "\"Loading saved model state ...\"", ")", "\n", "\n", "encoder_prefix", "=", "(", "encoder_path", "if", "encoder_path", "else", "\"question_model\"", ")", "+", "\".\"", "\n", "prefix_len", "=", "len", "(", "encoder_prefix", ")", "\n", "\n", "logger", ".", "info", "(", "\"Encoder state prefix %s\"", ",", "encoder_prefix", ")", "\n", "question_encoder_state", "=", "{", "\n", "key", "[", "prefix_len", ":", "]", ":", "value", "\n", "for", "(", "key", ",", "value", ")", "in", "saved_state", ".", "model_dict", ".", "items", "(", ")", "\n", "if", "key", ".", "startswith", "(", "encoder_prefix", ")", "\n", "}", "\n", "model_to_load", ".", "load_state_dict", "(", "question_encoder_state", ")", "\n", "vector_size", "=", "model_to_load", ".", "get_out_size", "(", ")", "\n", "logger", ".", "info", "(", "\"Encoder vector_size=%d\"", ",", "vector_size", ")", "\n", "\n", "# get questions & answers", "\n", "questions", "=", "[", "]", "\n", "question_answers", "=", "[", "]", "\n", "img_id", "=", "[", "]", "\n", "\n", "if", "not", "cfg", ".", "qa_dataset", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Please specify qa_dataset to use\"", ")", "\n", "return", "\n", "\n", "", "ds_key", "=", "cfg", ".", "qa_dataset", "\n", "logger", ".", "info", "(", "\"qa_dataset: %s\"", ",", "ds_key", ")", "\n", "\n", "qa_src", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "datasets", "[", "ds_key", "]", ")", "\n", "qa_src", ".", "load_data", "(", ")", "\n", "\n", "for", "ds_item", "in", "qa_src", ".", "data", ":", "\n", "\n", "        ", "question", ",", "answers", "=", "ds_item", "[", "'question'", "]", ",", "ds_item", "[", "'answers'", "]", "\n", "questions", ".", "append", "(", "question", ")", "\n", "question_answers", ".", "append", "(", "answers", ")", "\n", "img_id", ".", "append", "(", "ds_item", "[", "'img_id'", "]", ")", "\n", "\n", "", "index", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "indexers", "[", "cfg", ".", "indexer", "]", ")", "\n", "logger", ".", "info", "(", "\"Index class %s \"", ",", "type", "(", "index", ")", ")", "\n", "index_buffer_sz", "=", "index", ".", "buffer_size", "\n", "index", ".", "init_index", "(", "vector_size", ")", "\n", "retriever", "=", "LocalFaissRetriever", "(", "encoder", ",", "cfg", ".", "batch_size", ",", "tensorizer", ",", "index", ")", "\n", "\n", "logger", ".", "info", "(", "\"Using special token %s\"", ",", "qa_src", ".", "special_token", ")", "\n", "questions_tensor", "=", "retriever", ".", "generate_question_vectors", "(", "\n", "questions", ",", "query_token", "=", "qa_src", ".", "special_token", ",", "img_id", "=", "img_id", "\n", ")", "\n", "\n", "if", "qa_src", ".", "selector", ":", "\n", "        ", "logger", ".", "info", "(", "\"Using custom representation token selector\"", ")", "\n", "retriever", ".", "selector", "=", "qa_src", ".", "selector", "\n", "\n", "", "id_prefixes", "=", "[", "]", "\n", "ctx_sources", "=", "[", "]", "\n", "for", "ctx_src", "in", "cfg", ".", "ctx_datatsets", ":", "\n", "        ", "ctx_src", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "ctx_sources", "[", "ctx_src", "]", ")", "\n", "id_prefixes", ".", "append", "(", "ctx_src", ".", "id_prefix", ")", "\n", "ctx_sources", ".", "append", "(", "ctx_src", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"id_prefixes per dataset: %s\"", ",", "id_prefixes", ")", "\n", "\n", "# index all passages", "\n", "ctx_files_patterns", "=", "cfg", ".", "encoded_ctx_files", "\n", "index_path", "=", "cfg", ".", "index_path", "\n", "\n", "logger", ".", "info", "(", "\"ctx_files_patterns: %s\"", ",", "ctx_files_patterns", ")", "\n", "if", "ctx_files_patterns", ":", "\n", "        ", "assert", "len", "(", "ctx_files_patterns", ")", "==", "len", "(", "\n", "id_prefixes", "\n", ")", ",", "\"ctx len={} pref leb={}\"", ".", "format", "(", "len", "(", "ctx_files_patterns", ")", ",", "len", "(", "id_prefixes", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "(", "\n", "index_path", "\n", ")", ",", "\"Either encoded_ctx_files or index_path parameter should be set.\"", "\n", "\n", "", "input_paths", "=", "[", "]", "\n", "path_id_prefixes", "=", "[", "]", "\n", "for", "i", ",", "pattern", "in", "enumerate", "(", "ctx_files_patterns", ")", ":", "\n", "        ", "pattern_files", "=", "glob", ".", "glob", "(", "pattern", ")", "\n", "pattern_id_prefix", "=", "id_prefixes", "[", "i", "]", "\n", "input_paths", ".", "extend", "(", "pattern_files", ")", "\n", "path_id_prefixes", ".", "extend", "(", "[", "pattern_id_prefix", "]", "*", "len", "(", "pattern_files", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Embeddings files id prefixes: %s\"", ",", "path_id_prefixes", ")", "\n", "\n", "if", "index_path", "and", "index", ".", "index_exists", "(", "index_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Index path: %s\"", ",", "index_path", ")", "\n", "retriever", ".", "index", ".", "deserialize", "(", "index_path", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading all passages data from files: %s\"", ",", "input_paths", ")", "\n", "retriever", ".", "index_encoded_data", "(", "\n", "input_paths", ",", "index_buffer_sz", ",", "path_id_prefixes", "=", "path_id_prefixes", "\n", ")", "\n", "if", "index_path", ":", "\n", "            ", "retriever", ".", "index", ".", "serialize", "(", "index_path", ")", "\n", "\n", "# get top k results", "\n", "", "", "top_ids_and_scores", "=", "retriever", ".", "get_top_docs", "(", "questions_tensor", ".", "numpy", "(", ")", ",", "cfg", ".", "n_docs", ")", "\n", "\n", "# we no longer need the index", "\n", "retriever", "=", "None", "\n", "\n", "all_passages", "=", "{", "}", "\n", "for", "ctx_src", "in", "ctx_sources", ":", "\n", "        ", "ctx_src", ".", "load_data_to", "(", "all_passages", ")", "\n", "\n", "", "if", "len", "(", "all_passages", ")", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"No passages data found. Please specify ctx_file param properly.\"", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "validate_as_tables", ":", "\n", "        ", "questions_doc_hits", "=", "validate_tables", "(", "\n", "all_passages", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "cfg", ".", "validation_workers", ",", "\n", "cfg", ".", "match", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "questions_doc_hits", "=", "validate", "(", "\n", "all_passages", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "cfg", ".", "validation_workers", ",", "\n", "cfg", ".", "match", ",", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "out_file", ":", "\n", "        ", "save_results", "(", "\n", "all_passages", ",", "\n", "questions", ",", "\n", "question_answers", ",", "\n", "top_ids_and_scores", ",", "\n", "questions_doc_hits", ",", "\n", "cfg", ".", "out_file", ",", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "kilt_out_file", ":", "\n", "        ", "kilt_ctx", "=", "next", "(", "\n", "iter", "(", "[", "ctx", "for", "ctx", "in", "ctx_sources", "if", "isinstance", "(", "ctx", ",", "KiltCsvCtxSrc", ")", "]", ")", ",", "None", "\n", ")", "\n", "if", "not", "kilt_ctx", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"No Kilt compatible context file provided\"", ")", "\n", "", "assert", "hasattr", "(", "cfg", ",", "\"kilt_out_file\"", ")", "\n", "kilt_ctx", ".", "convert_to_kilt", "(", "qa_src", ".", "kilt_gold_file", ",", "cfg", ".", "out_file", ",", "cfg", ".", "kilt_out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.generate_dense_embeddings.gen_ctx_vectors": [[40, 95], ["len", "enumerate", "range", "dpr.utils.model_utils.move_to_device", "dpr.utils.model_utils.move_to_device", "dpr.utils.model_utils.move_to_device", "out.cpu.cpu", "len", "tensorizer.text_to_tensor", "torch.stack", "torch.zeros_like", "tensorizer.get_attn_mask", "torch.no_grad", "model", "len", "len", "out.cpu.size", "results.extend", "results.extend", "logger.info", "out[].view().numpy", "range", "out[].view().numpy", "range", "out.cpu.size", "out.cpu.size", "out[].view", "out[].view"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_device", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask"], ["def", "gen_ctx_vectors", "(", "\n", "cfg", ":", "DictConfig", ",", "\n", "ctx_rows", ":", "List", "[", "Tuple", "[", "object", ",", "BiEncoderPassage", "]", "]", ",", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", "=", "True", ",", "\n", ")", "->", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ":", "\n", "    ", "n", "=", "len", "(", "ctx_rows", ")", "\n", "bsz", "=", "cfg", ".", "batch_size", "\n", "total", "=", "0", "\n", "results", "=", "[", "]", "\n", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "n", ",", "bsz", ")", ")", ":", "\n", "        ", "batch", "=", "ctx_rows", "[", "batch_start", ":", "batch_start", "+", "bsz", "]", "\n", "batch_token_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", "[", "1", "]", ".", "text", ",", "title", "=", "ctx", "[", "1", "]", ".", "title", "if", "insert_title", "else", "None", "\n", ")", "\n", "for", "ctx", "in", "batch", "\n", "]", "\n", "\n", "ctx_ids_batch", "=", "move_to_device", "(", "\n", "torch", ".", "stack", "(", "batch_token_tensors", ",", "dim", "=", "0", ")", ",", "cfg", ".", "device", "\n", ")", "\n", "ctx_seg_batch", "=", "move_to_device", "(", "torch", ".", "zeros_like", "(", "ctx_ids_batch", ")", ",", "cfg", ".", "device", ")", "\n", "ctx_attn_mask", "=", "move_to_device", "(", "\n", "tensorizer", ".", "get_attn_mask", "(", "ctx_ids_batch", ")", ",", "cfg", ".", "device", "\n", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "out", ",", "_", "=", "model", "(", "ctx_ids_batch", ",", "ctx_seg_batch", ",", "ctx_attn_mask", ")", "\n", "", "out", "=", "out", ".", "cpu", "(", ")", "\n", "\n", "ctx_ids", "=", "[", "r", "[", "0", "]", "for", "r", "in", "batch", "]", "\n", "extra_info", "=", "[", "]", "\n", "if", "len", "(", "batch", "[", "0", "]", ")", ">", "3", ":", "\n", "            ", "extra_info", "=", "[", "r", "[", "3", ":", "]", "for", "r", "in", "batch", "]", "\n", "\n", "", "assert", "len", "(", "ctx_ids", ")", "==", "out", ".", "size", "(", "0", ")", "\n", "total", "+=", "len", "(", "ctx_ids", ")", "\n", "\n", "# TODO: refactor to avoid 'if'", "\n", "if", "extra_info", ":", "\n", "            ", "results", ".", "extend", "(", "\n", "[", "\n", "(", "ctx_ids", "[", "i", "]", ",", "out", "[", "i", "]", ".", "view", "(", "-", "1", ")", ".", "numpy", "(", ")", ",", "*", "extra_info", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "out", ".", "size", "(", "0", ")", ")", "\n", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "results", ".", "extend", "(", "\n", "[", "(", "ctx_ids", "[", "i", "]", ",", "out", "[", "i", "]", ".", "view", "(", "-", "1", ")", ".", "numpy", "(", ")", ")", "for", "i", "in", "range", "(", "out", ".", "size", "(", "0", ")", ")", "]", "\n", ")", "\n", "\n", "", "if", "total", "%", "10", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Encoded passages %d\"", ",", "total", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.generate_dense_embeddings.main": [[97, 175], ["hydra.main", "dpr.options.setup_cfg_gpu", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_cfg_params_from_state", "logger.info", "logger.info", "init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "encoder.eval", "dpr.utils.model_utils.get_model_obj", "logger.info", "logger.debug", "len", "dpr.utils.model_utils.get_model_obj.load_state_dict", "logger.info", "hydra.utils.instantiate", "hydra.utils.instantiate.load_data_to", "math.ceil", "logger.info", "generate_dense_embeddings.gen_ctx_vectors", "pathlib.Path().mkdir", "logger.info", "logger.info", "omegaconf.OmegaConf.to_yaml", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.keys", "len", "str", "open", "pickle.dump", "len", "dpr.utils.model_utils.load_states_from_checkpoint.model_dict.items", "key.startswith", "all_passages_dict.items", "len", "pathlib.Path", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.main", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_cfg_gpu", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_cfg_params_from_state", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlTablesCtxSrc.load_data_to", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.generate_dense_embeddings.gen_ctx_vectors", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["", "@", "hydra", ".", "main", "(", "config_path", "=", "\"conf\"", ",", "config_name", "=", "\"gen_embs\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "\n", "    ", "assert", "cfg", ".", "model_file", ",", "\"Please specify encoder checkpoint as model_file param\"", "\n", "assert", "cfg", ".", "ctx_src", ",", "\"Please specify passages source as ctx_src param\"", "\n", "\n", "cfg", "=", "setup_cfg_gpu", "(", "cfg", ")", "\n", "\n", "saved_state", "=", "load_states_from_checkpoint", "(", "cfg", ".", "model_file", ")", "\n", "set_cfg_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "cfg", ")", "\n", "\n", "logger", ".", "info", "(", "\"CFG:\"", ")", "\n", "logger", ".", "info", "(", "\"%s\"", ",", "OmegaConf", ".", "to_yaml", "(", "cfg", ")", ")", "\n", "\n", "if", "cfg", ".", "encoder", ".", "encoder_model_type", "==", "'hf_bert'", ":", "\n", "        ", "from", "dpr", ".", "models", "import", "init_biencoder_components", "\n", "\n", "", "else", ":", "\n", "        ", "from", "dpr", ".", "model_lxmert", "import", "init_biencoder_components", "\n", "\n", "", "tensorizer", ",", "encoder", ",", "_", "=", "init_biencoder_components", "(", "\n", "cfg", ".", "encoder", ".", "encoder_model_type", ",", "cfg", ",", "inference_only", "=", "True", ",", "load_images", "=", "False", "\n", ")", "\n", "\n", "encoder", "=", "encoder", ".", "ctx_model", "if", "cfg", ".", "encoder_type", "==", "\"ctx\"", "else", "encoder", ".", "question_model", "\n", "\n", "encoder", ",", "_", "=", "setup_for_distributed_mode", "(", "\n", "encoder", ",", "\n", "None", ",", "\n", "cfg", ".", "device", ",", "\n", "cfg", ".", "n_gpu", ",", "\n", "cfg", ".", "local_rank", ",", "\n", "cfg", ".", "fp16", ",", "\n", "cfg", ".", "fp16_opt_level", ",", "\n", ")", "\n", "encoder", ".", "eval", "(", ")", "\n", "\n", "# load weights from the model file", "\n", "model_to_load", "=", "get_model_obj", "(", "encoder", ")", "\n", "logger", ".", "info", "(", "\"Loading saved model state ...\"", ")", "\n", "logger", ".", "debug", "(", "\"saved model keys =%s\"", ",", "saved_state", ".", "model_dict", ".", "keys", "(", ")", ")", "\n", "\n", "prefix_len", "=", "len", "(", "\"ctx_model.\"", ")", "\n", "ctx_state", "=", "{", "\n", "key", "[", "prefix_len", ":", "]", ":", "value", "\n", "for", "(", "key", ",", "value", ")", "in", "saved_state", ".", "model_dict", ".", "items", "(", ")", "\n", "if", "key", ".", "startswith", "(", "\"ctx_model.\"", ")", "\n", "}", "\n", "model_to_load", ".", "load_state_dict", "(", "ctx_state", ")", "\n", "\n", "logger", ".", "info", "(", "\"reading data source: %s\"", ",", "cfg", ".", "ctx_src", ")", "\n", "\n", "ctx_src", "=", "hydra", ".", "utils", ".", "instantiate", "(", "cfg", ".", "ctx_sources", "[", "cfg", ".", "ctx_src", "]", ")", "\n", "all_passages_dict", "=", "{", "}", "\n", "ctx_src", ".", "load_data_to", "(", "all_passages_dict", ")", "\n", "all_passages", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "all_passages_dict", ".", "items", "(", ")", "]", "\n", "\n", "shard_size", "=", "math", ".", "ceil", "(", "len", "(", "all_passages", ")", "/", "cfg", ".", "num_shards", ")", "\n", "start_idx", "=", "cfg", ".", "shard_id", "*", "shard_size", "\n", "end_idx", "=", "start_idx", "+", "shard_size", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Producing encodings for passages range: %d to %d (out of total %d)\"", ",", "\n", "start_idx", ",", "\n", "end_idx", ",", "\n", "len", "(", "all_passages", ")", ",", "\n", ")", "\n", "shard_passages", "=", "all_passages", "[", "start_idx", ":", "end_idx", "]", "\n", "\n", "data", "=", "gen_ctx_vectors", "(", "cfg", ",", "shard_passages", ",", "encoder", ",", "tensorizer", ",", "True", ")", "\n", "\n", "file", "=", "cfg", ".", "out_file", "+", "\"_\"", "+", "str", "(", "cfg", ".", "shard_id", ")", "\n", "pathlib", ".", "Path", "(", "os", ".", "path", ".", "dirname", "(", "file", ")", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"Writing results to %s\"", "%", "file", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Total passages processed %d. Written to %s\"", ",", "len", "(", "data", ")", ",", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_reader.prepare_train_features": [[16, 92], ["tokenizer", "tokenizer.pop", "tokenizer.pop", "enumerate", "input_ids.index", "tokenizer.sequence_ids", "context.lower().index", "answer.lower", "len", "len", "tokenized_examples[].append", "tokenized_examples[].append", "tokenized_examples[].append", "tokenized_examples[].append", "tokenized_examples[].append", "tokenized_examples[].append", "context.lower", "len"], "function", ["None"], ["def", "prepare_train_features", "(", "examples", ",", "args", ")", ":", "\n", "# Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results", "\n", "# in one example possible giving several features when a context is long, each of those features having a", "\n", "# context that overlaps a bit the context of the previous feature.", "\n", "    ", "tokenized_examples", "=", "tokenizer", "(", "\n", "examples", "[", "\"title\"", "if", "args", ".", "pad_on_right", "else", "\"text\"", "]", ",", "\n", "examples", "[", "\"text\"", "if", "args", ".", "pad_on_right", "else", "\"title\"", "]", ",", "\n", "truncation", "=", "\"only_second\"", "if", "args", ".", "pad_on_right", "else", "\"only_first\"", ",", "\n", "max_length", "=", "args", ".", "max_length", ",", "\n", "stride", "=", "args", ".", "doc_stride", ",", "\n", "return_overflowing_tokens", "=", "True", ",", "\n", "return_offsets_mapping", "=", "True", ",", "\n", "padding", "=", "\"max_length\"", "\n", ")", "\n", "\n", "# Since one example might give us several features if it has a long context, we need a map from a feature to", "\n", "# its corresponding example. This key gives us just that.", "\n", "sample_mapping", "=", "tokenized_examples", ".", "pop", "(", "\"overflow_to_sample_mapping\"", ")", "\n", "# The offset mappings will give us a map from token to character position in the original context. This will", "\n", "# help us compute the start_positions and end_positions.", "\n", "offset_mapping", "=", "tokenized_examples", ".", "pop", "(", "\"offset_mapping\"", ")", "\n", "\n", "# Let's label those examples!", "\n", "tokenized_examples", "[", "\"start_positions\"", "]", "=", "[", "]", "\n", "tokenized_examples", "[", "\"end_positions\"", "]", "=", "[", "]", "\n", "\n", "for", "i", ",", "offsets", "in", "enumerate", "(", "offset_mapping", ")", ":", "\n", "# We will label impossible answers with the index of the CLS token.", "\n", "        ", "input_ids", "=", "tokenized_examples", "[", "\"input_ids\"", "]", "[", "i", "]", "\n", "\n", "cls_index", "=", "input_ids", ".", "index", "(", "tokenizer", ".", "cls_token_id", ")", "\n", "\n", "# Grab the sequence corresponding to that example (to know what is the context and what is the question).", "\n", "sequence_ids", "=", "tokenized_examples", ".", "sequence_ids", "(", "i", ")", "\n", "\n", "# One example can give several spans, this is the index of the example containing this span of text.", "\n", "sample_index", "=", "sample_mapping", "[", "i", "]", "\n", "answer", "=", "examples", "[", "\"answer\"", "]", "[", "sample_index", "]", "\n", "# If no answers are given, set the cls_index as answer.", "\n", "try", ":", "\n", "# Start/end character index of the answer in the text.", "\n", "            ", "context", "=", "examples", "[", "'text'", "]", "[", "sample_index", "]", "\n", "start_char", "=", "context", ".", "lower", "(", ")", ".", "index", "(", "answer", ".", "lower", "(", ")", ")", "\n", "end_char", "=", "start_char", "+", "len", "(", "answer", ")", "\n", "\n", "# Start token index of the current span in the text.", "\n", "token_start_index", "=", "0", "\n", "while", "sequence_ids", "[", "token_start_index", "]", "!=", "(", "1", "if", "pad_on_right", "else", "0", ")", ":", "\n", "                ", "token_start_index", "+=", "1", "\n", "\n", "# End token index of the current span in the text.", "\n", "", "token_end_index", "=", "len", "(", "input_ids", ")", "-", "1", "\n", "while", "sequence_ids", "[", "token_end_index", "]", "!=", "(", "1", "if", "pad_on_right", "else", "0", ")", ":", "\n", "                ", "token_end_index", "-=", "1", "\n", "\n", "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).", "\n", "", "if", "not", "(", "offsets", "[", "token_start_index", "]", "[", "0", "]", "<=", "start_char", "and", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ")", ":", "\n", "                ", "tokenized_examples", "[", "\"start_positions\"", "]", ".", "append", "(", "cls_index", ")", "\n", "tokenized_examples", "[", "\"end_positions\"", "]", ".", "append", "(", "cls_index", ")", "\n", "", "else", ":", "\n", "# Otherwise move the token_start_index and token_end_index to the two ends of the answer.", "\n", "# Note: we could go after the last offset if the answer is the last word (edge case).", "\n", "                ", "while", "token_start_index", "<", "len", "(", "offsets", ")", "and", "offsets", "[", "token_start_index", "]", "[", "0", "]", "<=", "start_char", ":", "\n", "                    ", "token_start_index", "+=", "1", "\n", "", "tokenized_examples", "[", "\"start_positions\"", "]", ".", "append", "(", "token_start_index", "-", "1", ")", "\n", "while", "offsets", "[", "token_end_index", "]", "[", "1", "]", ">=", "end_char", ":", "\n", "                    ", "token_end_index", "-=", "1", "\n", "", "tokenized_examples", "[", "\"end_positions\"", "]", ".", "append", "(", "token_end_index", "+", "1", ")", "\n", "\n", "", "", "except", ":", "\n", "\n", "            ", "tokenized_examples", "[", "\"start_positions\"", "]", ".", "append", "(", "cls_index", ")", "\n", "tokenized_examples", "[", "\"end_positions\"", "]", ".", "append", "(", "cls_index", ")", "\n", "\n", "\n", "", "", "return", "tokenized_examples", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.__init__": [[65, 107], ["logger.info", "dpr.utils.model_utils.get_model_file", "dpr.models.init_biencoder_components", "dpr.utils.model_utils.setup_for_distributed_mode", "dpr.utils.conf_utils.BiencoderDatasetsCfg", "dpr.utils.model_utils.load_states_from_checkpoint", "dpr.options.set_cfg_params_from_state", "train_dense_encoder.BiEncoderTrainer._load_saved_state"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_biencoder_components", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.setup_for_distributed_mode", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.load_states_from_checkpoint", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_cfg_params_from_state", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._load_saved_state"], ["def", "__init__", "(", "self", ",", "cfg", ":", "DictConfig", ")", ":", "\n", "        ", "self", ".", "shard_id", "=", "cfg", ".", "local_rank", "if", "cfg", ".", "local_rank", "!=", "-", "1", "else", "0", "\n", "self", ".", "distributed_factor", "=", "cfg", ".", "distributed_world_size", "or", "1", "\n", "load_images", "=", "True", "\n", "\n", "logger", ".", "info", "(", "\"***** Initializing components for training *****\"", ")", "\n", "\n", "# if model file is specified, encoder parameters from saved state should be used for initialization", "\n", "model_file", "=", "get_model_file", "(", "cfg", ",", "cfg", ".", "checkpoint_file_name", ")", "\n", "saved_state", "=", "None", "\n", "if", "model_file", ":", "\n", "            ", "saved_state", "=", "load_states_from_checkpoint", "(", "model_file", ")", "\n", "set_cfg_params_from_state", "(", "saved_state", ".", "encoder_params", ",", "cfg", ")", "\n", "\n", "", "tensorizer", ",", "model", ",", "optimizer", "=", "init_biencoder_components", "(", "\n", "cfg", ".", "encoder", ".", "encoder_model_type", ",", "cfg", ",", "load_images", "=", "load_images", "\n", ")", "\n", "#         ", "\n", "model", ",", "optimizer", "=", "setup_for_distributed_mode", "(", "\n", "model", ",", "\n", "optimizer", ",", "\n", "cfg", ".", "device", ",", "\n", "cfg", ".", "n_gpu", ",", "\n", "cfg", ".", "local_rank", ",", "\n", "cfg", ".", "fp16", ",", "\n", "cfg", ".", "fp16_opt_level", ",", "\n", ")", "\n", "self", ".", "biencoder", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "start_epoch", "=", "0", "\n", "self", ".", "start_batch", "=", "0", "\n", "self", ".", "scheduler_state", "=", "None", "\n", "self", ".", "best_validation_result", "=", "None", "\n", "self", ".", "best_cp_name", "=", "None", "\n", "self", ".", "cfg", "=", "cfg", "\n", "self", ".", "ds_cfg", "=", "BiencoderDatasetsCfg", "(", "cfg", ")", "\n", "\n", "if", "saved_state", ":", "\n", "            ", "self", ".", "_load_saved_state", "(", "saved_state", ")", "\n", "\n", "", "self", ".", "dev_iterator", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator": [[108, 155], ["logger.info", "random.Random", "random.Random.shuffle", "dpr.utils.data_utils.MultiSetDataIterator", "ds.load_data", "dpr.utils.data_utils.ShardedDataIterator"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data"], ["", "def", "get_data_iterator", "(", "\n", "self", ",", "\n", "batch_size", ":", "int", ",", "\n", "is_train_set", ":", "bool", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "\n", "offset", ":", "int", "=", "0", ",", "\n", "rank", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "\n", "        ", "hydra_datasets", "=", "(", "\n", "self", ".", "ds_cfg", ".", "train_datasets", "if", "is_train_set", "else", "self", ".", "ds_cfg", ".", "dev_datasets", "\n", ")", "\n", "sampling_rates", "=", "self", ".", "ds_cfg", ".", "sampling_rates", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Initializing task/set data %s\"", ",", "\n", "self", ".", "ds_cfg", ".", "train_datasets_names", "\n", "if", "is_train_set", "\n", "else", "self", ".", "ds_cfg", ".", "dev_datasets_names", ",", "\n", ")", "\n", "\n", "# randomized data loading to avoid file system congestion", "\n", "datasets_list", "=", "[", "ds", "for", "ds", "in", "hydra_datasets", "]", "\n", "rnd", "=", "random", ".", "Random", "(", "rank", ")", "\n", "rnd", ".", "shuffle", "(", "datasets_list", ")", "\n", "[", "ds", ".", "load_data", "(", ")", "for", "ds", "in", "datasets_list", "]", "\n", "\n", "sharded_iterators", "=", "[", "\n", "ShardedDataIterator", "(", "\n", "ds", ",", "\n", "shard_id", "=", "self", ".", "shard_id", ",", "\n", "num_shards", "=", "self", ".", "distributed_factor", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "shuffle_seed", "=", "shuffle_seed", ",", "\n", "offset", "=", "offset", ",", "\n", ")", "\n", "for", "ds", "in", "hydra_datasets", "\n", "]", "\n", "\n", "return", "MultiSetDataIterator", "(", "\n", "sharded_iterators", ",", "\n", "shuffle_seed", ",", "\n", "shuffle", ",", "\n", "sampling_rates", "=", "sampling_rates", "if", "is_train_set", "else", "[", "1", "]", ",", "\n", "rank", "=", "rank", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.run_train": [[157, 212], ["train_dense_encoder.BiEncoderTrainer.get_data_iterator", "train_dense_encoder.BiEncoderTrainer.get_max_iterations", "logger.info", "logger.info", "math.ceil", "logger.info", "logger.info", "range", "logger.warning", "logger.info", "int", "logger.info", "dpr.utils.model_utils.get_schedule_linear", "dpr.utils.model_utils.get_schedule_linear", "int", "logger.info", "train_dense_encoder.BiEncoderTrainer._train_epoch", "logger.info"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_max_iterations", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_schedule_linear", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_schedule_linear", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._train_epoch"], ["", "def", "run_train", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "self", ".", "cfg", "\n", "\n", "train_iterator", "=", "self", ".", "get_data_iterator", "(", "\n", "cfg", ".", "train", ".", "batch_size", ",", "\n", "True", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_seed", "=", "cfg", ".", "seed", ",", "\n", "offset", "=", "self", ".", "start_batch", ",", "\n", "rank", "=", "cfg", ".", "local_rank", ",", "\n", ")", "\n", "max_iterations", "=", "train_iterator", ".", "get_max_iterations", "(", ")", "\n", "logger", ".", "info", "(", "\"  Total iterations per epoch=%d\"", ",", "max_iterations", ")", "\n", "if", "max_iterations", "==", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "\"No data found for training.\"", ")", "\n", "return", "\n", "\n", "", "updates_per_epoch", "=", "(", "\n", "train_iterator", ".", "max_iterations", "//", "cfg", ".", "train", ".", "gradient_accumulation_steps", "\n", ")", "\n", "\n", "total_updates", "=", "updates_per_epoch", "*", "cfg", ".", "train", ".", "num_train_epochs", "\n", "logger", ".", "info", "(", "\" Total updates=%d\"", ",", "total_updates", ")", "\n", "warmup_steps", "=", "cfg", ".", "train", ".", "warmup_steps", "\n", "\n", "if", "self", ".", "scheduler_state", ":", "\n", "# TODO: ideally we'd want to just call", "\n", "# scheduler.load_state_dict(self.scheduler_state)", "\n", "# but it doesn't work properly as of now", "\n", "\n", "            ", "logger", ".", "info", "(", "\"Loading scheduler state %s\"", ",", "self", ".", "scheduler_state", ")", "\n", "shift", "=", "int", "(", "self", ".", "scheduler_state", "[", "\"last_epoch\"", "]", ")", "\n", "logger", ".", "info", "(", "\"Steps shift %d\"", ",", "shift", ")", "\n", "scheduler", "=", "get_schedule_linear", "(", "\n", "self", ".", "optimizer", ",", "\n", "warmup_steps", ",", "\n", "total_updates", ",", "\n", "steps_shift", "=", "shift", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "scheduler", "=", "get_schedule_linear", "(", "\n", "self", ".", "optimizer", ",", "warmup_steps", ",", "total_updates", "\n", ")", "\n", "\n", "", "eval_step", "=", "math", ".", "ceil", "(", "updates_per_epoch", "/", "cfg", ".", "train", ".", "eval_per_epoch", ")", "\n", "logger", ".", "info", "(", "\"  Eval step = %d\"", ",", "eval_step", ")", "\n", "logger", ".", "info", "(", "\"***** Training *****\"", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "start_epoch", ",", "int", "(", "cfg", ".", "train", ".", "num_train_epochs", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Epoch %d *****\"", ",", "epoch", ")", "\n", "self", ".", "_train_epoch", "(", "scheduler", ",", "epoch", ",", "eval_step", ",", "train_iterator", ")", "\n", "\n", "", "if", "cfg", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Training finished. Best validation checkpoint %s\"", ",", "self", ".", "best_cp_name", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save": [[214, 238], ["train_dense_encoder.BiEncoderTrainer._save_checkpoint", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_average_rank", "train_dense_encoder.BiEncoderTrainer.validate_nll", "logger.info"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._save_checkpoint", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll"], ["", "", "def", "validate_and_save", "(", "self", ",", "epoch", ":", "int", ",", "iteration", ":", "int", ",", "scheduler", ")", ":", "\n", "        ", "cfg", "=", "self", ".", "cfg", "\n", "# for distributed mode, save checkpoint for only one process", "\n", "save_cp", "=", "cfg", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "\n", "\n", "if", "epoch", "==", "cfg", ".", "val_av_rank_start_epoch", ":", "\n", "            ", "self", ".", "best_validation_result", "=", "None", "\n", "\n", "", "if", "not", "cfg", ".", "dev_datasets", ":", "\n", "            ", "validation_loss", "=", "0", "\n", "", "else", ":", "\n", "            ", "if", "epoch", ">=", "cfg", ".", "val_av_rank_start_epoch", ":", "\n", "                ", "validation_loss", "=", "self", ".", "validate_average_rank", "(", ")", "\n", "", "else", ":", "\n", "                ", "validation_loss", "=", "self", ".", "validate_nll", "(", ")", "\n", "\n", "", "", "if", "save_cp", ":", "\n", "            ", "cp_name", "=", "self", ".", "_save_checkpoint", "(", "scheduler", ",", "epoch", ",", "iteration", ")", "\n", "logger", ".", "info", "(", "\"Saved checkpoint to %s\"", ",", "cp_name", ")", "\n", "\n", "if", "validation_loss", "<", "(", "self", ".", "best_validation_result", "or", "validation_loss", "+", "1", ")", ":", "\n", "                ", "self", ".", "best_validation_result", "=", "validation_loss", "\n", "self", ".", "best_cp_name", "=", "cp_name", "\n", "logger", ".", "info", "(", "\"New Best validation checkpoint %s\"", ",", "cp_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll": [[239, 309], ["logger.info", "train_dense_encoder.BiEncoderTrainer.biencoder.eval", "time.time", "enumerate", "float", "logger.info", "train_dense_encoder.BiEncoderTrainer.get_data_iterator", "data_iterator.iterate_ds_data", "isinstance", "logger.info", "dpr.models.biencoder.BiEncoder.create_biencoder_input2", "ds_cfg.selector.get_positions", "train_dense_encoder._do_biencoder_fwd_pass", "loss.item", "logger.info", "loss.item", "time.time"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.iterate_ds_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.create_biencoder_input2", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder._do_biencoder_fwd_pass"], ["", "", "", "def", "validate_nll", "(", "self", ")", "->", "float", ":", "\n", "        ", "logger", ".", "info", "(", "\"NLL validation ...\"", ")", "\n", "cfg", "=", "self", ".", "cfg", "\n", "self", ".", "biencoder", ".", "eval", "(", ")", "\n", "\n", "if", "not", "self", ".", "dev_iterator", ":", "\n", "            ", "self", ".", "dev_iterator", "=", "self", ".", "get_data_iterator", "(", "\n", "cfg", ".", "train", ".", "dev_batch_size", ",", "False", ",", "shuffle", "=", "False", ",", "rank", "=", "cfg", ".", "local_rank", "\n", ")", "\n", "", "data_iterator", "=", "self", ".", "dev_iterator", "\n", "\n", "total_loss", "=", "0.0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "total_correct_predictions", "=", "0", "\n", "num_hard_negatives", "=", "cfg", ".", "train", ".", "hard_negatives", "\n", "num_other_negatives", "=", "cfg", ".", "train", ".", "other_negatives", "\n", "log_result_step", "=", "cfg", ".", "train", ".", "log_batch_step", "\n", "batches", "=", "0", "\n", "dataset", "=", "0", "\n", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "data_iterator", ".", "iterate_ds_data", "(", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "samples_batch", ",", "Tuple", ")", ":", "\n", "                ", "samples_batch", ",", "dataset", "=", "samples_batch", "\n", "", "logger", ".", "info", "(", "\"Eval step: %d ,rnk=%s\"", ",", "i", ",", "cfg", ".", "local_rank", ")", "\n", "biencoder_input", "=", "BiEncoder", ".", "create_biencoder_input2", "(", "\n", "samples_batch", ",", "\n", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "\n", "num_other_negatives", ",", "\n", "shuffle", "=", "False", ",", "\n", ")", "\n", "\n", "# get the token to be used for representation selection", "\n", "ds_cfg", "=", "self", ".", "ds_cfg", ".", "dev_datasets", "[", "dataset", "]", "\n", "rep_positions", "=", "ds_cfg", ".", "selector", ".", "get_positions", "(", "\n", "biencoder_input", ".", "question_ids", ",", "self", ".", "tensorizer", "\n", ")", "\n", "encoder_type", "=", "ds_cfg", ".", "encoder_type", "\n", "\n", "loss", ",", "correct_cnt", "=", "_do_biencoder_fwd_pass", "(", "\n", "self", ".", "biencoder", ",", "\n", "biencoder_input", ",", "\n", "self", ".", "tensorizer", ",", "\n", "cfg", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "rep_positions", "=", "rep_positions", ",", "\n", ")", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_correct_predictions", "+=", "correct_cnt", "\n", "batches", "+=", "1", "\n", "if", "(", "i", "+", "1", ")", "%", "log_result_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Eval step: %d , used_time=%f sec., loss=%f \"", ",", "\n", "i", ",", "\n", "time", ".", "time", "(", ")", "-", "start_time", ",", "\n", "loss", ".", "item", "(", ")", ",", "\n", ")", "\n", "\n", "", "", "total_loss", "=", "total_loss", "/", "batches", "\n", "total_samples", "=", "batches", "*", "cfg", ".", "train", ".", "dev_batch_size", "*", "self", ".", "distributed_factor", "\n", "correct_ratio", "=", "float", "(", "total_correct_predictions", "/", "total_samples", ")", "\n", "logger", ".", "info", "(", "\n", "\"NLL Validation: loss = %f. correct prediction ratio  %d/%d ~  %f\"", ",", "\n", "total_loss", ",", "\n", "total_correct_predictions", ",", "\n", "total_samples", ",", "\n", "correct_ratio", ",", "\n", ")", "\n", "return", "total_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank": [[310, 462], ["logger.info", "train_dense_encoder.BiEncoderTrainer.biencoder.eval", "dpr.models.biencoder.BiEncoderNllLoss.get_similarity_function", "enumerate", "torch.cat", "torch.cat", "logger.info", "logger.info", "torch.cat.size", "dpr.models.biencoder.BiEncoderNllLoss.get_similarity_function.", "torch.sort", "enumerate", "float", "logger.info", "train_dense_encoder.BiEncoderTrainer.get_data_iterator", "data_iterator.iterate_ds_data", "isinstance", "dpr.models.biencoder.BiEncoder.create_biencoder_input2", "len", "ctxs_ids.size", "ds_cfg.selector.get_positions", "enumerate", "positive_idx_per_question.extend", "torch.cat.size", "torch.cat.size", "len", "gold_idx.item", "dpr.utils.dist_utils.all_gather_list", "enumerate", "len", "range", "train_dense_encoder.BiEncoderTrainer.tensorizer.get_attn_mask", "train_dense_encoder.BiEncoderTrainer.tensorizer.get_attn_mask", "torch.cat.extend", "logger.info", "torch.no_grad", "train_dense_encoder.BiEncoderTrainer.biencoder", "torch.cat.extend", "ctx_dense.cpu().split", "len", "len", "q_ids.size", "q_dense.cpu().split", "ctx_dense.cpu", "q_dense.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.get_data_iterator", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.iterate_ds_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.create_biencoder_input2", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_gather_list", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask"], ["", "def", "validate_average_rank", "(", "self", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        Validates biencoder model using each question's gold passage's rank across the set of passages from the dataset.\n        It generates vectors for specified amount of negative passages from each question (see --val_av_rank_xxx params)\n        and stores them in RAM as well as question vectors.\n        Then the similarity scores are calculted for the entire\n        num_questions x (num_questions x num_passages_per_question) matrix and sorted per quesrtion.\n        Each question's gold passage rank in that  sorted list of scores is averaged across all the questions.\n        :return: averaged rank number\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Average rank validation ...\"", ")", "\n", "\n", "cfg", "=", "self", ".", "cfg", "\n", "self", ".", "biencoder", ".", "eval", "(", ")", "\n", "distributed_factor", "=", "self", ".", "distributed_factor", "\n", "\n", "if", "not", "self", ".", "dev_iterator", ":", "\n", "            ", "self", ".", "dev_iterator", "=", "self", ".", "get_data_iterator", "(", "\n", "cfg", ".", "train", ".", "dev_batch_size", ",", "False", ",", "shuffle", "=", "False", ",", "rank", "=", "cfg", ".", "local_rank", "\n", ")", "\n", "", "data_iterator", "=", "self", ".", "dev_iterator", "\n", "\n", "sub_batch_size", "=", "cfg", ".", "train", ".", "val_av_rank_bsz", "\n", "sim_score_f", "=", "BiEncoderNllLoss", ".", "get_similarity_function", "(", ")", "\n", "q_represenations", "=", "[", "]", "\n", "ctx_represenations", "=", "[", "]", "\n", "positive_idx_per_question", "=", "[", "]", "\n", "\n", "num_hard_negatives", "=", "cfg", ".", "train", ".", "val_av_rank_hard_neg", "\n", "num_other_negatives", "=", "cfg", ".", "train", ".", "val_av_rank_other_neg", "\n", "\n", "log_result_step", "=", "cfg", ".", "train", ".", "log_batch_step", "\n", "dataset", "=", "0", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "data_iterator", ".", "iterate_ds_data", "(", ")", ")", ":", "\n", "# samples += 1", "\n", "            ", "if", "(", "\n", "len", "(", "q_represenations", ")", "\n", ">", "cfg", ".", "train", ".", "val_av_rank_max_qs", "/", "distributed_factor", "\n", ")", ":", "\n", "                ", "break", "\n", "\n", "", "if", "isinstance", "(", "samples_batch", ",", "Tuple", ")", ":", "\n", "                ", "samples_batch", ",", "dataset", "=", "samples_batch", "\n", "\n", "", "biencoder_input", "=", "BiEncoder", ".", "create_biencoder_input2", "(", "\n", "samples_batch", ",", "\n", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "\n", "num_other_negatives", ",", "\n", "shuffle", "=", "False", ",", "\n", ")", "\n", "total_ctxs", "=", "len", "(", "ctx_represenations", ")", "\n", "ctxs_ids", "=", "biencoder_input", ".", "context_ids", "\n", "ctxs_segments", "=", "biencoder_input", ".", "ctx_segments", "\n", "bsz", "=", "ctxs_ids", ".", "size", "(", "0", ")", "\n", "\n", "# get the token to be used for representation selection", "\n", "ds_cfg", "=", "self", ".", "ds_cfg", ".", "dev_datasets", "[", "dataset", "]", "\n", "encoder_type", "=", "ds_cfg", ".", "encoder_type", "\n", "rep_positions", "=", "ds_cfg", ".", "selector", ".", "get_positions", "(", "\n", "biencoder_input", ".", "question_ids", ",", "self", ".", "tensorizer", "\n", ")", "\n", "\n", "# split contexts batch into sub batches since it is supposed to be too large to be processed in one batch", "\n", "for", "j", ",", "batch_start", "in", "enumerate", "(", "range", "(", "0", ",", "bsz", ",", "sub_batch_size", ")", ")", ":", "\n", "\n", "                ", "q_ids", ",", "q_segments", "=", "(", "\n", "(", "biencoder_input", ".", "question_ids", ",", "biencoder_input", ".", "question_segments", ")", "\n", "if", "j", "==", "0", "\n", "else", "(", "None", ",", "None", ")", "\n", ")", "\n", "\n", "if", "j", "==", "0", "and", "cfg", ".", "n_gpu", ">", "1", "and", "q_ids", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "# if we are in DP (but not in DDP) mode, all model input tensors should have batch size >1 or 0,", "\n", "# otherwise the other input tensors will be split but only the first split will be called", "\n", "                    ", "continue", "\n", "\n", "", "ctx_ids_batch", "=", "ctxs_ids", "[", "batch_start", ":", "batch_start", "+", "sub_batch_size", "]", "\n", "ctx_seg_batch", "=", "ctxs_segments", "[", "\n", "batch_start", ":", "batch_start", "+", "sub_batch_size", "\n", "]", "\n", "\n", "q_attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "q_ids", ")", "\n", "ctx_attn_mask", "=", "self", ".", "tensorizer", ".", "get_attn_mask", "(", "ctx_ids_batch", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "q_dense", ",", "ctx_dense", "=", "self", ".", "biencoder", "(", "\n", "q_ids", ",", "\n", "q_segments", ",", "\n", "q_attn_mask", ",", "\n", "ctx_ids_batch", ",", "\n", "ctx_seg_batch", ",", "\n", "ctx_attn_mask", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "representation_token_pos", "=", "rep_positions", ",", "\n", ")", "\n", "\n", "", "if", "q_dense", "is", "not", "None", ":", "\n", "                    ", "q_represenations", ".", "extend", "(", "q_dense", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "", "ctx_represenations", ".", "extend", "(", "ctx_dense", ".", "cpu", "(", ")", ".", "split", "(", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n", "", "batch_positive_idxs", "=", "biencoder_input", ".", "is_positive", "\n", "positive_idx_per_question", ".", "extend", "(", "\n", "[", "total_ctxs", "+", "v", "for", "v", "in", "batch_positive_idxs", "]", "\n", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "log_result_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Av.rank validation: step %d, computed ctx_vectors %d, q_vectors %d\"", ",", "\n", "i", ",", "\n", "len", "(", "ctx_represenations", ")", ",", "\n", "len", "(", "q_represenations", ")", ",", "\n", ")", "\n", "\n", "", "", "ctx_represenations", "=", "torch", ".", "cat", "(", "ctx_represenations", ",", "dim", "=", "0", ")", "\n", "q_represenations", "=", "torch", ".", "cat", "(", "q_represenations", ",", "dim", "=", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Av.rank validation: total q_vectors size=%s\"", ",", "q_represenations", ".", "size", "(", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Av.rank validation: total ctx_vectors size=%s\"", ",", "ctx_represenations", ".", "size", "(", ")", "\n", ")", "\n", "\n", "q_num", "=", "q_represenations", ".", "size", "(", "0", ")", "\n", "assert", "q_num", "==", "len", "(", "positive_idx_per_question", ")", "\n", "\n", "scores", "=", "sim_score_f", "(", "q_represenations", ",", "ctx_represenations", ")", "\n", "values", ",", "indices", "=", "torch", ".", "sort", "(", "scores", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "\n", "rank", "=", "0", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "positive_idx_per_question", ")", ":", "\n", "# aggregate the rank of the known gold passage in the sorted results for each question", "\n", "            ", "gold_idx", "=", "(", "indices", "[", "i", "]", "==", "idx", ")", ".", "nonzero", "(", ")", "\n", "rank", "+=", "gold_idx", ".", "item", "(", ")", "\n", "\n", "", "if", "distributed_factor", ">", "1", ":", "\n", "# each node calcuated its own rank, exchange the information between node and calculate the \"global\" average rank", "\n", "# NOTE: the set of passages is still unique for every node", "\n", "            ", "eval_stats", "=", "all_gather_list", "(", "[", "rank", ",", "q_num", "]", ",", "max_size", "=", "100", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "eval_stats", ")", ":", "\n", "                ", "remote_rank", ",", "remote_q_num", "=", "item", "\n", "if", "i", "!=", "cfg", ".", "local_rank", ":", "\n", "                    ", "rank", "+=", "remote_rank", "\n", "q_num", "+=", "remote_q_num", "\n", "\n", "", "", "", "av_rank", "=", "float", "(", "rank", "/", "q_num", ")", "\n", "logger", ".", "info", "(", "\n", "\"Av.rank validation: average rank %s, total questions=%d\"", ",", "av_rank", ",", "q_num", "\n", ")", "\n", "return", "av_rank", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._train_epoch": [[463, 599], ["train_dense_encoder.BiEncoderTrainer.biencoder.train", "enumerate", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_and_save", "logger.info", "logger.info", "train_data_iterator.iterate_ds_data", "isinstance", "train_data_iterator.get_iteration", "random.seed", "dpr.models.biencoder.BiEncoder.create_biencoder_input2", "selector.get_positions", "train_dense_encoder._do_biencoder_fwd_pass", "loss.item", "loss.item", "loss.backward", "train_dense_encoder.BiEncoderTrainer.optimizer.step", "scheduler.step", "train_dense_encoder.BiEncoderTrainer.biencoder.zero_grad", "logger.info", "logger.info", "logger.info", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_and_save", "train_dense_encoder.BiEncoderTrainer.biencoder.train", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "loss.item", "train_data_iterator.get_iteration", "amp.master_params", "train_dense_encoder.BiEncoderTrainer.biencoder.parameters"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.iterate_ds_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_iteration", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.create_biencoder_input2", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder._do_biencoder_fwd_pass", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.step", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.step", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_and_save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_iteration"], ["", "def", "_train_epoch", "(", "\n", "self", ",", "\n", "scheduler", ",", "\n", "epoch", ":", "int", ",", "\n", "eval_step", ":", "int", ",", "\n", "train_data_iterator", ":", "MultiSetDataIterator", ",", "\n", ")", ":", "\n", "\n", "        ", "cfg", "=", "self", ".", "cfg", "\n", "rolling_train_loss", "=", "0.0", "\n", "epoch_loss", "=", "0", "\n", "epoch_correct_predictions", "=", "0", "\n", "\n", "log_result_step", "=", "cfg", ".", "train", ".", "log_batch_step", "\n", "rolling_loss_step", "=", "cfg", ".", "train", ".", "train_rolling_loss_step", "\n", "num_hard_negatives", "=", "cfg", ".", "train", ".", "hard_negatives", "\n", "num_other_negatives", "=", "cfg", ".", "train", ".", "other_negatives", "\n", "seed", "=", "cfg", ".", "seed", "\n", "self", ".", "biencoder", ".", "train", "(", ")", "\n", "epoch_batches", "=", "train_data_iterator", ".", "max_iterations", "\n", "data_iteration", "=", "0", "\n", "\n", "dataset", "=", "0", "\n", "for", "i", ",", "samples_batch", "in", "enumerate", "(", "\n", "train_data_iterator", ".", "iterate_ds_data", "(", "epoch", "=", "epoch", ")", "\n", ")", ":", "\n", "            ", "if", "isinstance", "(", "samples_batch", ",", "Tuple", ")", ":", "\n", "                ", "samples_batch", ",", "dataset", "=", "samples_batch", "\n", "\n", "", "ds_cfg", "=", "self", ".", "ds_cfg", ".", "train_datasets", "[", "dataset", "]", "\n", "special_token", "=", "ds_cfg", ".", "special_token", "\n", "encoder_type", "=", "ds_cfg", ".", "encoder_type", "\n", "shuffle_positives", "=", "ds_cfg", ".", "shuffle_positives", "\n", "\n", "# to be able to resume shuffled ctx- pools", "\n", "data_iteration", "=", "train_data_iterator", ".", "get_iteration", "(", ")", "\n", "random", ".", "seed", "(", "seed", "+", "epoch", "+", "data_iteration", ")", "\n", "\n", "biencoder_batch", "=", "BiEncoder", ".", "create_biencoder_input2", "(", "\n", "samples_batch", ",", "\n", "self", ".", "tensorizer", ",", "\n", "True", ",", "\n", "num_hard_negatives", ",", "\n", "num_other_negatives", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_positives", "=", "shuffle_positives", ",", "\n", "query_token", "=", "special_token", ",", "\n", ")", "\n", "\n", "# get the token to be used for representation selection", "\n", "from", "dpr", ".", "data", ".", "biencoder_data", "import", "DEFAULT_SELECTOR", "\n", "\n", "selector", "=", "ds_cfg", ".", "selector", "if", "ds_cfg", "else", "DEFAULT_SELECTOR", "\n", "\n", "rep_positions", "=", "selector", ".", "get_positions", "(", "\n", "biencoder_batch", ".", "question_ids", ",", "self", ".", "tensorizer", "\n", ")", "\n", "\n", "loss_scale", "=", "(", "\n", "cfg", ".", "loss_scale_factors", "[", "dataset", "]", "if", "cfg", ".", "loss_scale_factors", "else", "None", "\n", ")", "\n", "loss", ",", "correct_cnt", "=", "_do_biencoder_fwd_pass", "(", "\n", "self", ".", "biencoder", ",", "\n", "biencoder_batch", ",", "\n", "self", ".", "tensorizer", ",", "\n", "cfg", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "rep_positions", "=", "rep_positions", ",", "\n", "loss_scale", "=", "loss_scale", ",", "\n", ")", "\n", "\n", "epoch_correct_predictions", "+=", "correct_cnt", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "rolling_train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "cfg", ".", "fp16", ":", "\n", "                ", "from", "apex", "import", "amp", "\n", "\n", "with", "amp", ".", "scale_loss", "(", "loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "if", "cfg", ".", "train", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "amp", ".", "master_params", "(", "self", ".", "optimizer", ")", ",", "cfg", ".", "train", ".", "max_grad_norm", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "if", "cfg", ".", "train", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "biencoder", ".", "parameters", "(", ")", ",", "cfg", ".", "train", ".", "max_grad_norm", "\n", ")", "\n", "\n", "", "", "if", "(", "i", "+", "1", ")", "%", "cfg", ".", "train", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "biencoder", ".", "zero_grad", "(", ")", "\n", "\n", "", "if", "i", "%", "log_result_step", "==", "0", ":", "\n", "                ", "lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", "\n", "logger", ".", "info", "(", "\n", "\"Epoch: %d: Step: %d/%d, loss=%f, lr=%f\"", ",", "\n", "epoch", ",", "\n", "data_iteration", ",", "\n", "epoch_batches", ",", "\n", "loss", ".", "item", "(", ")", ",", "\n", "lr", ",", "\n", ")", "\n", "\n", "", "if", "(", "i", "+", "1", ")", "%", "rolling_loss_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Train batch %d\"", ",", "data_iteration", ")", "\n", "latest_rolling_train_av_loss", "=", "rolling_train_loss", "/", "rolling_loss_step", "\n", "logger", ".", "info", "(", "\n", "\"Avg. loss per last %d batches: %f\"", ",", "\n", "rolling_loss_step", ",", "\n", "latest_rolling_train_av_loss", ",", "\n", ")", "\n", "rolling_train_loss", "=", "0.0", "\n", "\n", "", "if", "data_iteration", "%", "eval_step", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"rank=%d, Validation: Epoch: %d Step: %d/%d\"", ",", "\n", "cfg", ".", "local_rank", ",", "\n", "epoch", ",", "\n", "data_iteration", ",", "\n", "epoch_batches", ",", "\n", ")", "\n", "self", ".", "validate_and_save", "(", "\n", "epoch", ",", "train_data_iterator", ".", "get_iteration", "(", ")", ",", "scheduler", "\n", ")", "\n", "self", ".", "biencoder", ".", "train", "(", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Epoch finished on %d\"", ",", "cfg", ".", "local_rank", ")", "\n", "self", ".", "validate_and_save", "(", "epoch", ",", "data_iteration", ",", "scheduler", ")", "\n", "\n", "epoch_loss", "=", "(", "epoch_loss", "/", "epoch_batches", ")", "if", "epoch_batches", ">", "0", "else", "0", "\n", "logger", ".", "info", "(", "\"Av Loss per epoch=%f\"", ",", "epoch_loss", ")", "\n", "logger", ".", "info", "(", "\"epoch total correct predictions=%d\"", ",", "epoch_correct_predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._save_checkpoint": [[600, 616], ["dpr.utils.model_utils.get_model_obj", "os.path.join", "dpr.options.get_encoder_params_state_from_cfg", "dpr.utils.model_utils.CheckpointState", "torch.save", "logger.info", "dpr.utils.model_utils.get_model_obj.get_state_dict", "train_dense_encoder.BiEncoderTrainer.optimizer.state_dict", "scheduler.state_dict", "dpr.utils.model_utils.CheckpointState._asdict", "str"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.get_encoder_params_state_from_cfg", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_state_dict"], ["", "def", "_save_checkpoint", "(", "self", ",", "scheduler", ",", "epoch", ":", "int", ",", "offset", ":", "int", ")", "->", "str", ":", "\n", "        ", "cfg", "=", "self", ".", "cfg", "\n", "model_to_save", "=", "get_model_obj", "(", "self", ".", "biencoder", ")", "\n", "cp", "=", "os", ".", "path", ".", "join", "(", "cfg", ".", "output_dir", ",", "cfg", ".", "checkpoint_file_name", "+", "\".\"", "+", "str", "(", "epoch", ")", ")", "\n", "meta_params", "=", "get_encoder_params_state_from_cfg", "(", "cfg", ")", "\n", "state", "=", "CheckpointState", "(", "\n", "model_to_save", ".", "get_state_dict", "(", ")", ",", "\n", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "offset", ",", "\n", "epoch", ",", "\n", "meta_params", ",", "\n", ")", "\n", "torch", ".", "save", "(", "state", ".", "_asdict", "(", ")", ",", "cp", ")", "\n", "logger", ".", "info", "(", "\"Saved checkpoint at %s\"", ",", "cp", ")", "\n", "return", "cp", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer._load_saved_state": [[617, 645], ["logger.info", "dpr.utils.model_utils.get_model_obj", "logger.info", "dpr.utils.model_utils.get_model_obj.load_state", "logger.info", "train_dense_encoder.BiEncoderTrainer.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.load_state"], ["", "def", "_load_saved_state", "(", "self", ",", "saved_state", ":", "CheckpointState", ")", ":", "\n", "        ", "epoch", "=", "saved_state", ".", "epoch", "\n", "# offset is currently ignored since all checkpoints are made after full epochs", "\n", "offset", "=", "saved_state", ".", "offset", "\n", "if", "offset", "==", "0", ":", "# epoch has been completed", "\n", "            ", "epoch", "+=", "1", "\n", "", "logger", ".", "info", "(", "\"Loading checkpoint @ batch=%s and epoch=%s\"", ",", "offset", ",", "epoch", ")", "\n", "\n", "if", "self", ".", "cfg", ".", "ignore_checkpoint_offset", ":", "\n", "            ", "self", ".", "start_epoch", "=", "0", "\n", "self", ".", "start_batch", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "start_epoch", "=", "epoch", "\n", "# TODO: offset doesn't work for multiset currently", "\n", "self", ".", "start_batch", "=", "0", "# offset", "\n", "\n", "", "model_to_load", "=", "get_model_obj", "(", "self", ".", "biencoder", ")", "\n", "logger", ".", "info", "(", "\"Loading saved model state ...\"", ")", "\n", "\n", "model_to_load", ".", "load_state", "(", "saved_state", ")", "\n", "\n", "if", "not", "self", ".", "cfg", ".", "ignore_checkpoint_optimizer", ":", "\n", "            ", "if", "saved_state", ".", "optimizer_dict", ":", "\n", "                ", "logger", ".", "info", "(", "\"Loading saved optimizer state ...\"", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "saved_state", ".", "optimizer_dict", ")", "\n", "\n", "", "if", "saved_state", ".", "scheduler_dict", ":", "\n", "                ", "self", ".", "scheduler_state", "=", "saved_state", ".", "scheduler_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder._calc_loss": [[647, 726], ["loss_function.calc", "torch.empty_like().cpu().copy_().detach_", "torch.empty_like().cpu().copy_().detach_", "dpr.utils.dist_utils.all_gather_list", "enumerate", "torch.cat", "torch.cat", "ctx_vectors.size", "torch.empty_like().cpu().copy_", "torch.empty_like().cpu().copy_", "torch.cat.append", "torch.cat.append", "positive_idx_per_question.extend", "hard_negatives_per_question.extend", "torch.cat.append", "torch.cat.append", "positive_idx_per_question.extend", "hard_negatives_per_question.extend", "q_vector.to", "ctx_vectors.to", "torch.empty_like().cpu", "torch.empty_like().cpu", "torch.empty_like", "torch.empty_like"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.calc", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_gather_list"], ["", "", "", "", "def", "_calc_loss", "(", "\n", "cfg", ",", "\n", "loss_function", ",", "\n", "local_q_vector", ",", "\n", "local_ctx_vectors", ",", "\n", "local_positive_idxs", ",", "\n", "local_hard_negatives_idxs", ":", "list", "=", "None", ",", "\n", "loss_scale", ":", "float", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "bool", "]", ":", "\n", "    ", "\"\"\"\n    Calculates In-batch negatives schema loss and supports to run it in DDP mode by exchanging the representations\n    across all the nodes.\n    \"\"\"", "\n", "distributed_world_size", "=", "cfg", ".", "distributed_world_size", "or", "1", "\n", "if", "distributed_world_size", ">", "1", ":", "\n", "        ", "q_vector_to_send", "=", "(", "\n", "torch", ".", "empty_like", "(", "local_q_vector", ")", ".", "cpu", "(", ")", ".", "copy_", "(", "local_q_vector", ")", ".", "detach_", "(", ")", "\n", ")", "\n", "ctx_vector_to_send", "=", "(", "\n", "torch", ".", "empty_like", "(", "local_ctx_vectors", ")", ".", "cpu", "(", ")", ".", "copy_", "(", "local_ctx_vectors", ")", ".", "detach_", "(", ")", "\n", ")", "\n", "\n", "global_question_ctx_vectors", "=", "all_gather_list", "(", "\n", "[", "\n", "q_vector_to_send", ",", "\n", "ctx_vector_to_send", ",", "\n", "local_positive_idxs", ",", "\n", "local_hard_negatives_idxs", ",", "\n", "]", ",", "\n", "max_size", "=", "cfg", ".", "global_loss_buf_sz", ",", "\n", ")", "\n", "\n", "global_q_vector", "=", "[", "]", "\n", "global_ctxs_vector", "=", "[", "]", "\n", "\n", "# ctxs_per_question = local_ctx_vectors.size(0)", "\n", "positive_idx_per_question", "=", "[", "]", "\n", "hard_negatives_per_question", "=", "[", "]", "\n", "\n", "total_ctxs", "=", "0", "\n", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "global_question_ctx_vectors", ")", ":", "\n", "            ", "q_vector", ",", "ctx_vectors", ",", "positive_idx", ",", "hard_negatives_idxs", "=", "item", "\n", "\n", "if", "i", "!=", "cfg", ".", "local_rank", ":", "\n", "                ", "global_q_vector", ".", "append", "(", "q_vector", ".", "to", "(", "local_q_vector", ".", "device", ")", ")", "\n", "global_ctxs_vector", ".", "append", "(", "ctx_vectors", ".", "to", "(", "local_q_vector", ".", "device", ")", ")", "\n", "positive_idx_per_question", ".", "extend", "(", "[", "v", "+", "total_ctxs", "for", "v", "in", "positive_idx", "]", ")", "\n", "hard_negatives_per_question", ".", "extend", "(", "\n", "[", "[", "v", "+", "total_ctxs", "for", "v", "in", "l", "]", "for", "l", "in", "hard_negatives_idxs", "]", "\n", ")", "\n", "", "else", ":", "\n", "                ", "global_q_vector", ".", "append", "(", "local_q_vector", ")", "\n", "global_ctxs_vector", ".", "append", "(", "local_ctx_vectors", ")", "\n", "positive_idx_per_question", ".", "extend", "(", "\n", "[", "v", "+", "total_ctxs", "for", "v", "in", "local_positive_idxs", "]", "\n", ")", "\n", "hard_negatives_per_question", ".", "extend", "(", "\n", "[", "[", "v", "+", "total_ctxs", "for", "v", "in", "l", "]", "for", "l", "in", "local_hard_negatives_idxs", "]", "\n", ")", "\n", "", "total_ctxs", "+=", "ctx_vectors", ".", "size", "(", "0", ")", "\n", "", "global_q_vector", "=", "torch", ".", "cat", "(", "global_q_vector", ",", "dim", "=", "0", ")", "\n", "global_ctxs_vector", "=", "torch", ".", "cat", "(", "global_ctxs_vector", ",", "dim", "=", "0", ")", "\n", "\n", "", "else", ":", "\n", "        ", "global_q_vector", "=", "local_q_vector", "\n", "global_ctxs_vector", "=", "local_ctx_vectors", "\n", "positive_idx_per_question", "=", "local_positive_idxs", "\n", "hard_negatives_per_question", "=", "local_hard_negatives_idxs", "\n", "\n", "", "loss", ",", "is_correct", "=", "loss_function", ".", "calc", "(", "\n", "global_q_vector", ",", "\n", "global_ctxs_vector", ",", "\n", "positive_idx_per_question", ",", "\n", "hard_negatives_per_question", ",", "\n", "loss_scale", "=", "loss_scale", ",", "\n", ")", "\n", "\n", "return", "loss", ",", "is_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder._do_biencoder_fwd_pass": [[728, 814], ["dpr.models.biencoder.BiEncoderBatch", "tensorizer.get_attn_mask", "tensorizer.get_attn_mask", "dpr.models.biencoder.BiEncoderNllLoss", "train_dense_encoder._calc_loss", "is_correct.sum().item.sum().item", "loss.mean.mean", "dpr.utils.model_utils.move_to_device", "model", "model", "torch.no_grad", "is_correct.sum().item.sum", "dpr.models.biencoder.BiEncoderBatch._asdict", "model", "model"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder._calc_loss", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_device"], ["", "def", "_do_biencoder_fwd_pass", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "input", ":", "BiEncoderBatch", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "cfg", ",", "\n", "encoder_type", ":", "str", ",", "\n", "rep_positions", "=", "0", ",", "\n", "loss_scale", ":", "float", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "int", "]", ":", "\n", "\n", "    ", "input", "=", "BiEncoderBatch", "(", "**", "move_to_device", "(", "input", ".", "_asdict", "(", ")", ",", "cfg", ".", "device", ")", ")", "\n", "\n", "q_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "question_ids", ")", "\n", "ctx_attn_mask", "=", "tensorizer", ".", "get_attn_mask", "(", "input", ".", "context_ids", ")", "\n", "\n", "if", "model", ".", "training", ":", "\n", "        ", "if", "\"img_ids\"", "in", "input", ":", "\n", "            ", "model_out", "=", "model", "(", "\n", "input", ".", "question_ids", ",", "\n", "input", ".", "question_segments", ",", "\n", "q_attn_mask", ",", "\n", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "\n", "ctx_attn_mask", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "representation_token_pos", "=", "rep_positions", ",", "\n", "img_ids", "=", "input", ".", "img_ids", ",", "\n", "\n", ")", "\n", "", "else", ":", "\n", "            ", "model_out", "=", "model", "(", "\n", "input", ".", "question_ids", ",", "\n", "input", ".", "question_segments", ",", "\n", "q_attn_mask", ",", "\n", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "\n", "ctx_attn_mask", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "representation_token_pos", "=", "rep_positions", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "\"img_ids\"", "in", "input", ":", "\n", "                ", "model_out", "=", "model", "(", "\n", "input", ".", "question_ids", ",", "\n", "input", ".", "question_segments", ",", "\n", "q_attn_mask", ",", "\n", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "\n", "ctx_attn_mask", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "representation_token_pos", "=", "rep_positions", ",", "\n", "img_ids", "=", "input", ".", "img_ids", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "model_out", "=", "model", "(", "\n", "input", ".", "question_ids", ",", "\n", "input", ".", "question_segments", ",", "\n", "q_attn_mask", ",", "\n", "input", ".", "context_ids", ",", "\n", "input", ".", "ctx_segments", ",", "\n", "ctx_attn_mask", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "representation_token_pos", "=", "rep_positions", ")", "\n", "\n", "", "", "", "local_q_vector", ",", "local_ctx_vectors", "=", "model_out", "\n", "\n", "loss_function", "=", "BiEncoderNllLoss", "(", ")", "\n", "\n", "loss", ",", "is_correct", "=", "_calc_loss", "(", "\n", "cfg", ",", "\n", "loss_function", ",", "\n", "local_q_vector", ",", "\n", "local_ctx_vectors", ",", "\n", "input", ".", "is_positive", ",", "\n", "input", ".", "hard_negatives", ",", "\n", "loss_scale", "=", "loss_scale", ",", "\n", ")", "\n", "\n", "is_correct", "=", "is_correct", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "cfg", ".", "n_gpu", ">", "1", ":", "\n", "        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "cfg", ".", "train", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "        ", "loss", "=", "loss", "/", "cfg", ".", "train", ".", "gradient_accumulation_steps", "\n", "", "return", "loss", ",", "is_correct", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.main": [[816, 848], ["hydra.main", "dpr.options.setup_cfg_gpu", "dpr.options.set_seed", "train_dense_encoder.BiEncoderTrainer", "ValueError", "os.makedirs", "logger.info", "logger.info", "train_dense_encoder.BiEncoderTrainer.run_train", "omegaconf.OmegaConf.to_yaml", "len", "logger.info", "train_dense_encoder.BiEncoderTrainer.validate_nll", "train_dense_encoder.BiEncoderTrainer.validate_average_rank", "logger.warning"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.main", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_cfg_gpu", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_seed", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.run_train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_nll", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.DPR.train_dense_encoder.BiEncoderTrainer.validate_average_rank"], ["", "@", "hydra", ".", "main", "(", "config_path", "=", "\"conf\"", ",", "config_name", "=", "\"biencoder_train_cfg\"", ")", "\n", "def", "main", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "if", "cfg", ".", "train", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "cfg", ".", "train", ".", "gradient_accumulation_steps", "\n", ")", "\n", ")", "\n", "\n", "", "if", "cfg", ".", "output_dir", "is", "not", "None", ":", "\n", "        ", "os", ".", "makedirs", "(", "cfg", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "cfg", "=", "setup_cfg_gpu", "(", "cfg", ")", "\n", "set_seed", "(", "cfg", ")", "\n", "\n", "if", "cfg", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "logger", ".", "info", "(", "\"CFG (after gpu  configuration):\"", ")", "\n", "logger", ".", "info", "(", "\"%s\"", ",", "OmegaConf", ".", "to_yaml", "(", "cfg", ")", ")", "\n", "\n", "", "trainer", "=", "BiEncoderTrainer", "(", "cfg", ")", "\n", "\n", "if", "cfg", ".", "train_datasets", "and", "len", "(", "cfg", ".", "train_datasets", ")", ">", "0", ":", "\n", "        ", "trainer", ".", "run_train", "(", ")", "\n", "", "elif", "cfg", ".", "model_file", "and", "cfg", ".", "dev_datasets", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"No train files are specified. Run 2 types of validation for specified model file\"", "\n", ")", "\n", "trainer", ".", "validate_nll", "(", ")", "\n", "trainer", ".", "validate_average_rank", "(", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Neither train_file or (model_file & dev_file) parameters are specified. Nothing to do.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_cfg_params_from_state": [[56, 71], ["None"], "function", ["None"], ["def", "set_cfg_params_from_state", "(", "state", ":", "dict", ",", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "\"\"\"\n    Overrides some of the encoder config parameters from a give state object\n    \"\"\"", "\n", "if", "not", "state", ":", "\n", "        ", "return", "\n", "", "cfg", ".", "do_lower_case", "=", "state", "[", "\"do_lower_case\"", "]", "\n", "cfg", ".", "encoder", ".", "question_pretrained_model_cfg", "=", "state", "[", "\"question_pretrained_model_cfg\"", "]", "\n", "cfg", ".", "encoder", ".", "ctx_pretrained_model_cfg", "=", "state", "[", "\"ctx_pretrained_model_cfg\"", "]", "\n", "cfg", ".", "encoder", ".", "question_encoder_model_type", "=", "state", "[", "\"question_encoder_model_type\"", "]", "\n", "cfg", ".", "encoder", ".", "ctx_encoder_model_type", "=", "state", "[", "\"ctx_encoder_model_type\"", "]", "\n", "cfg", ".", "encoder", ".", "pretrained_file", "=", "state", "[", "\"pretrained_file\"", "]", "\n", "cfg", ".", "encoder", ".", "projection_dim", "=", "state", "[", "\"projection_dim\"", "]", "\n", "cfg", ".", "encoder", ".", "question_sequence_length", "=", "state", "[", "\"question_sequence_length\"", "]", "\n", "cfg", ".", "encoder", ".", "ctx_sequence_length", "=", "state", "[", "\"ctx_sequence_length\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.get_encoder_params_state_from_cfg": [[73, 90], ["None"], "function", ["None"], ["", "def", "get_encoder_params_state_from_cfg", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "\"\"\"\n    Selects the param values to be saved in a checkpoint, so that a trained model can be used for downstream\n    tasks without the need to specify these parameter again\n    :return: Dict of params to memorize in a checkpoint\n    \"\"\"", "\n", "\n", "return", "{", "\n", "\"do_lower_case\"", ":", "cfg", ".", "do_lower_case", ",", "\n", "\"question_pretrained_model_cfg\"", ":", "cfg", ".", "encoder", ".", "question_pretrained_model_cfg", ",", "\n", "\"ctx_pretrained_model_cfg\"", ":", "cfg", ".", "encoder", ".", "ctx_pretrained_model_cfg", ",", "\n", "\"question_encoder_model_type\"", ":", "cfg", ".", "encoder", ".", "question_encoder_model_type", ",", "\n", "\"ctx_encoder_model_type\"", ":", "cfg", ".", "encoder", ".", "ctx_encoder_model_type", ",", "\n", "\"pretrained_file\"", ":", "cfg", ".", "encoder", ".", "pretrained_file", ",", "\n", "\"projection_dim\"", ":", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "\"question_sequence_length\"", ":", "cfg", ".", "encoder", ".", "question_sequence_length", ",", "\n", "\"ctx_sequence_length\"", ":", "cfg", ".", "encoder", ".", "ctx_sequence_length", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.set_seed": [[94, 101], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "def", "set_seed", "(", "args", ")", ":", "\n", "    ", "seed", "=", "args", ".", "seed", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_cfg_gpu": [[103, 136], ["logger.info", "os.environ.get", "logger.info", "logger.info", "logger.info", "int", "str", "torch.cuda.device_count", "torch.cuda.set_device", "str", "torch.distributed.init_process_group", "socket.gethostname", "torch.device", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["", "", "def", "setup_cfg_gpu", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Setup params for CUDA, GPU & distributed training\n    \"\"\"", "\n", "logger", ".", "info", "(", "\"args.local_rank %s\"", ",", "cfg", ".", "local_rank", ")", "\n", "ws", "=", "os", ".", "environ", ".", "get", "(", "\"WORLD_SIZE\"", ")", "\n", "cfg", ".", "distributed_world_size", "=", "int", "(", "ws", ")", "if", "ws", "else", "1", "\n", "logger", ".", "info", "(", "\"WORLD_SIZE %s\"", ",", "ws", ")", "\n", "if", "cfg", ".", "local_rank", "==", "-", "1", "or", "cfg", ".", "no_cuda", ":", "# single-node multi-gpu (or cpu) mode", "\n", "        ", "device", "=", "str", "(", "\n", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "cfg", ".", "no_cuda", "else", "\"cpu\"", "\n", ")", "\n", ")", "\n", "cfg", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "# distributed mode", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "cfg", ".", "local_rank", ")", "\n", "device", "=", "str", "(", "torch", ".", "device", "(", "\"cuda\"", ",", "cfg", ".", "local_rank", ")", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "\"nccl\"", ")", "\n", "cfg", ".", "n_gpu", "=", "1", "\n", "\n", "", "cfg", ".", "device", "=", "device", "\n", "\n", "logger", ".", "info", "(", "\n", "\"Initialized host %s as d.rank %d on device=%s, n_gpu=%d, world size=%d\"", ",", "\n", "socket", ".", "gethostname", "(", ")", ",", "\n", "cfg", ".", "local_rank", ",", "\n", "cfg", ".", "device", ",", "\n", "cfg", ".", "n_gpu", ",", "\n", "cfg", ".", "distributed_world_size", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"16-bits training: %s \"", ",", "cfg", ".", "fp16", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.dpr.options.setup_logger": [[138, 148], ["logger.setLevel", "logger.hasHandlers", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logger.addHandler", "logger.handlers.clear"], "function", ["None"], ["", "def", "setup_logger", "(", "logger", ")", ":", "\n", "    ", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "if", "logger", ".", "hasHandlers", "(", ")", ":", "\n", "        ", "logger", ".", "handlers", ".", "clear", "(", ")", "\n", "", "log_formatter", "=", "logging", ".", "Formatter", "(", "\n", "\"[%(thread)s] %(asctime)s [%(levelname)s] %(name)s: %(message)s\"", "\n", ")", "\n", "console", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console", ".", "setFormatter", "(", "log_formatter", ")", "\n", "logger", ".", "addHandler", "(", "console", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_rank": [[18, 20], ["torch.get_rank"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_rank"], ["def", "get_rank", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_world_size": [[22, 24], ["torch.get_world_size"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_default_group": [[26, 28], ["None"], "function", ["None"], ["", "def", "get_default_group", "(", ")", ":", "\n", "    ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_reduce": [[30, 34], ["torch.all_reduce", "dist_utils.get_default_group"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_reduce", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_default_group"], ["", "def", "all_reduce", "(", "tensor", ",", "group", "=", "None", ")", ":", "\n", "    ", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "get_default_group", "(", ")", "\n", "", "return", "dist", ".", "all_reduce", "(", "tensor", ",", "group", "=", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_gather_list": [[36, 91], ["pickle.dumps", "len", "dist_utils.get_rank", "dist_utils.get_world_size", "buffer.zero_", "len.to_bytes", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "torch.ByteTensor", "buffer[].copy_", "dist_utils.all_reduce", "ValueError", "torch.cuda.ByteTensor", "torch.cuda.ByteTensor", "torch.ByteTensor().pin_memory", "torch.ByteTensor().pin_memory", "list", "list", "range", "hasattr", "all_gather_list._buffer.numel", "int.from_bytes", "Exception", "torch.ByteTensor", "torch.ByteTensor", "result.append", "pickle.loads", "bytes", "out_buffer[].tolist"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_rank", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.get_world_size", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.dist_utils.all_reduce"], ["", "def", "all_gather_list", "(", "data", ",", "group", "=", "None", ",", "max_size", "=", "16384", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable.\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group (optional): group of the collective\n    \"\"\"", "\n", "SIZE_STORAGE_BYTES", "=", "4", "# int32 to encode the payload size", "\n", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "enc_size", "=", "len", "(", "enc", ")", "\n", "\n", "if", "enc_size", "+", "SIZE_STORAGE_BYTES", ">", "max_size", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'encoded data exceeds max_size, this can be fixed by increasing buffer size: {}'", ".", "format", "(", "enc_size", ")", ")", "\n", "\n", "", "rank", "=", "get_rank", "(", ")", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "buffer_size", "=", "max_size", "*", "world_size", "\n", "\n", "if", "not", "hasattr", "(", "all_gather_list", ",", "'_buffer'", ")", "or", "all_gather_list", ".", "_buffer", ".", "numel", "(", ")", "<", "buffer_size", ":", "\n", "        ", "all_gather_list", ".", "_buffer", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "buffer_size", ")", "\n", "all_gather_list", ".", "_cpu_buffer", "=", "torch", ".", "ByteTensor", "(", "max_size", ")", ".", "pin_memory", "(", ")", "\n", "\n", "", "buffer", "=", "all_gather_list", ".", "_buffer", "\n", "buffer", ".", "zero_", "(", ")", "\n", "cpu_buffer", "=", "all_gather_list", ".", "_cpu_buffer", "\n", "\n", "assert", "enc_size", "<", "256", "**", "SIZE_STORAGE_BYTES", ",", "'Encoded object size should be less than {} bytes'", ".", "format", "(", "\n", "256", "**", "SIZE_STORAGE_BYTES", ")", "\n", "\n", "size_bytes", "=", "enc_size", ".", "to_bytes", "(", "SIZE_STORAGE_BYTES", ",", "byteorder", "=", "'big'", ")", "\n", "\n", "cpu_buffer", "[", "0", ":", "SIZE_STORAGE_BYTES", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "size_bytes", ")", ")", "\n", "cpu_buffer", "[", "SIZE_STORAGE_BYTES", ":", "enc_size", "+", "SIZE_STORAGE_BYTES", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "\n", "start", "=", "rank", "*", "max_size", "\n", "size", "=", "enc_size", "+", "SIZE_STORAGE_BYTES", "\n", "buffer", "[", "start", ":", "start", "+", "size", "]", ".", "copy_", "(", "cpu_buffer", "[", ":", "size", "]", ")", "\n", "\n", "all_reduce", "(", "buffer", ",", "group", "=", "group", ")", "\n", "\n", "try", ":", "\n", "        ", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "out_buffer", "=", "buffer", "[", "i", "*", "max_size", ":", "(", "i", "+", "1", ")", "*", "max_size", "]", "\n", "size", "=", "int", ".", "from_bytes", "(", "out_buffer", "[", "0", ":", "SIZE_STORAGE_BYTES", "]", ",", "byteorder", "=", "'big'", ")", "\n", "if", "size", ">", "0", ":", "\n", "                ", "result", ".", "append", "(", "pickle", ".", "loads", "(", "bytes", "(", "out_buffer", "[", "SIZE_STORAGE_BYTES", ":", "size", "+", "SIZE_STORAGE_BYTES", "]", ".", "tolist", "(", ")", ")", ")", ")", "\n", "", "", "return", "result", "\n", "", "except", "pickle", ".", "UnpicklingError", ":", "\n", "        ", "raise", "Exception", "(", "\n", "'Unable to unpickle data from other workers. all_gather_list requires all '", "\n", "'workers to enter the function together, so this error usually indicates '", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.setup_for_distributed_mode": [[36, 70], ["torch.nn.parallel.DistributedDataParallel.to", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "apex.amp.register_half_function", "ImportError"], "function", ["None"], ["def", "setup_for_distributed_mode", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "device", ":", "object", ",", "\n", "n_gpu", ":", "int", "=", "1", ",", "\n", "local_rank", ":", "int", "=", "-", "1", ",", "\n", "fp16", ":", "bool", "=", "False", ",", "\n", "fp16_opt_level", ":", "str", "=", "\"O1\"", ",", "\n", ")", "->", "(", "nn", ".", "Module", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "    ", "model", ".", "to", "(", "device", ")", "\n", "if", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "apex", "\n", "from", "apex", "import", "amp", "\n", "\n", "apex", ".", "amp", ".", "register_half_function", "(", "torch", ",", "\"einsum\"", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", "\n", ")", "\n", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "fp16_opt_level", ")", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model", ",", "\n", "device_ids", "=", "[", "device", "if", "device", "else", "local_rank", "]", ",", "\n", "output_device", "=", "local_rank", ",", "\n", "find_unused_parameters", "=", "True", ",", "\n", ")", "\n", "", "return", "model", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_cuda": [[72, 89], ["model_utils.move_to_cuda._move_to_cuda"], "function", ["None"], ["", "def", "move_to_cuda", "(", "sample", ")", ":", "\n", "    ", "if", "len", "(", "sample", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "_move_to_cuda", "(", "maybe_tensor", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "maybe_tensor", ")", ":", "\n", "            ", "return", "maybe_tensor", ".", "cuda", "(", ")", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "dict", ")", ":", "\n", "            ", "return", "{", "key", ":", "_move_to_cuda", "(", "value", ")", "for", "key", ",", "value", "in", "maybe_tensor", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "list", ")", ":", "\n", "            ", "return", "[", "_move_to_cuda", "(", "x", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "tuple", ")", ":", "\n", "            ", "return", "[", "_move_to_cuda", "(", "x", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "else", ":", "\n", "            ", "return", "maybe_tensor", "\n", "\n", "", "", "return", "_move_to_cuda", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.move_to_device": [[91, 111], ["model_utils.move_to_device._move_to_device"], "function", ["None"], ["", "def", "move_to_device", "(", "sample", ",", "device", ")", ":", "\n", "    ", "if", "len", "(", "sample", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "_move_to_device", "(", "maybe_tensor", ",", "device", ")", ":", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "maybe_tensor", ")", ":", "\n", "            ", "return", "maybe_tensor", ".", "to", "(", "device", ")", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "dict", ")", ":", "\n", "            ", "return", "{", "\n", "key", ":", "_move_to_device", "(", "value", ",", "device", ")", "\n", "for", "key", ",", "value", "in", "maybe_tensor", ".", "items", "(", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "list", ")", ":", "\n", "            ", "return", "[", "_move_to_device", "(", "x", ",", "device", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "elif", "isinstance", "(", "maybe_tensor", ",", "tuple", ")", ":", "\n", "            ", "return", "[", "_move_to_device", "(", "x", ",", "device", ")", "for", "x", "in", "maybe_tensor", "]", "\n", "", "else", ":", "\n", "            ", "return", "maybe_tensor", "\n", "\n", "", "", "return", "_move_to_device", "(", "sample", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_schedule_linear": [[113, 136], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max"], "function", ["None"], ["", "def", "get_schedule_linear", "(", "\n", "optimizer", ",", "\n", "warmup_steps", ",", "\n", "total_training_steps", ",", "\n", "steps_shift", "=", "0", ",", "\n", "last_epoch", "=", "-", "1", ",", "\n", ")", ":", "\n", "\n", "    ", "\"\"\"Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "current_step", "+=", "steps_shift", "\n", "if", "current_step", "<", "warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "warmup_steps", ")", ")", "\n", "", "return", "max", "(", "\n", "1e-7", ",", "\n", "float", "(", "total_training_steps", "-", "current_step", ")", "\n", "/", "float", "(", "max", "(", "1", ",", "total_training_steps", "-", "warmup_steps", ")", ")", ",", "\n", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights": [[138, 147], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "function", ["None"], ["", "def", "init_weights", "(", "modules", ":", "List", ")", ":", "\n", "    ", "for", "module", "in", "modules", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_obj": [[149, 151], ["hasattr"], "function", ["None"], ["", "", "", "def", "get_model_obj", "(", "model", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "return", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.get_model_file": [[153, 168], ["logger.info", "os.path.exists", "glob.glob", "len", "max", "os.path.join"], "function", ["None"], ["", "def", "get_model_file", "(", "args", ",", "file_prefix", ")", "->", "str", ":", "\n", "    ", "if", "args", ".", "model_file", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "model_file", ")", ":", "\n", "        ", "return", "args", ".", "model_file", "\n", "\n", "", "out_cp_files", "=", "(", "\n", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "file_prefix", "+", "\"*\"", ")", ")", "\n", "if", "args", ".", "output_dir", "\n", "else", "[", "]", "\n", ")", "\n", "logger", ".", "info", "(", "\"Checkpoint files %s\"", ",", "out_cp_files", ")", "\n", "model_file", "=", "None", "\n", "\n", "if", "len", "(", "out_cp_files", ")", ">", "0", ":", "\n", "        ", "model_file", "=", "max", "(", "out_cp_files", ",", "key", "=", "os", ".", "path", ".", "getctime", ")", "\n", "", "return", "model_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.load_states_from_checkpoint": [[170, 178], ["logger.info", "torch.load", "logger.info", "CheckpointState", "os.path.exists", "os.path.join", "torch.load.keys", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "load_states_from_checkpoint", "(", "model_file", ":", "str", ")", "->", "CheckpointState", ":", "\n", "    ", "logger", ".", "info", "(", "\"Reading saved model from %s\"", ",", "model_file", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "model_file", ")", ":", "\n", "        ", "model_file", "=", "os", ".", "path", ".", "join", "(", "PROJ_PATH", ",", "model_file", ")", "\n", "\n", "", "state_dict", "=", "torch", ".", "load", "(", "model_file", ",", "map_location", "=", "lambda", "s", ",", "l", ":", "default_restore_location", "(", "s", ",", "\"cpu\"", ")", ")", "\n", "logger", ".", "info", "(", "\"model_state_dict keys %s\"", ",", "state_dict", ".", "keys", "(", ")", ")", "\n", "return", "CheckpointState", "(", "**", "state_dict", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.__init__": [[59, 101], ["len", "max", "max", "math.ceil", "min", "logger.info", "math.ceil", "int"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "data", ":", "torch", ".", "utils", ".", "data", ".", "Dataset", ",", "\n", "shard_id", ":", "int", "=", "0", ",", "\n", "num_shards", ":", "int", "=", "1", ",", "\n", "batch_size", ":", "int", "=", "1", ",", "\n", "shuffle", "=", "True", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "\n", "offset", ":", "int", "=", "0", ",", "\n", "strict_batch_size", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "data", "=", "data", "\n", "total_size", "=", "len", "(", "data", ")", "\n", "\n", "self", ".", "shards_num", "=", "max", "(", "num_shards", ",", "1", ")", "\n", "self", ".", "shard_id", "=", "max", "(", "shard_id", ",", "0", ")", "\n", "\n", "samples_per_shard", "=", "math", ".", "ceil", "(", "total_size", "/", "self", ".", "shards_num", ")", "\n", "\n", "self", ".", "shard_start_idx", "=", "self", ".", "shard_id", "*", "samples_per_shard", "\n", "\n", "self", ".", "shard_end_idx", "=", "min", "(", "self", ".", "shard_start_idx", "+", "samples_per_shard", ",", "total_size", ")", "\n", "\n", "if", "strict_batch_size", ":", "\n", "            ", "self", ".", "max_iterations", "=", "math", ".", "ceil", "(", "samples_per_shard", "/", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "max_iterations", "=", "int", "(", "samples_per_shard", "/", "batch_size", ")", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"samples_per_shard=%d, shard_start_idx=%d, shard_end_idx=%d, max_iterations=%d\"", ",", "\n", "samples_per_shard", ",", "\n", "self", ".", "shard_start_idx", ",", "\n", "self", ".", "shard_end_idx", ",", "\n", "self", ".", "max_iterations", ",", "\n", ")", "\n", "\n", "self", ".", "iteration", "=", "offset", "# to track in-shard iteration status", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "shuffle_seed", "=", "shuffle_seed", "\n", "self", ".", "strict_batch_size", "=", "strict_batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.total_data_len": [[102, 104], ["len"], "methods", ["None"], ["", "def", "total_data_len", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.iterations_num": [[105, 107], ["None"], "methods", ["None"], ["", "def", "iterations_num", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "max_iterations", "-", "self", ".", "iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.max_iterations_num": [[108, 110], ["None"], "methods", ["None"], ["", "def", "max_iterations_num", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "max_iterations", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.get_iteration": [[111, 113], ["None"], "methods", ["None"], ["", "def", "get_iteration", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply": [[114, 117], ["visitor_func"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "visitor_func", ":", "Callable", ")", ":", "\n", "        ", "for", "sample", "in", "self", ".", "data", ":", "\n", "            ", "visitor_func", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.get_shard_indices": [[118, 126], ["list", "range", "random.Random", "random.Random.shuffle", "len"], "methods", ["None"], ["", "", "def", "get_shard_indices", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "# to be able to resume, same shuffling should be used when starting from a failed/stopped iteration", "\n", "            ", "epoch_rnd", "=", "random", ".", "Random", "(", "self", ".", "shuffle_seed", "+", "epoch", ")", "\n", "epoch_rnd", ".", "shuffle", "(", "indices", ")", "\n", "", "shard_indices", "=", "indices", "[", "self", ".", "shard_start_idx", ":", "self", ".", "shard_end_idx", "]", "\n", "return", "shard_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.iterate_ds_data": [[128, 159], ["data_utils.ShardedDataIterator.get_shard_indices", "range", "logger.info", "len", "logger.debug", "logger.debug", "items_idxs.extend", "len", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.get_shard_indices"], ["", "def", "iterate_ds_data", "(", "self", ",", "epoch", ":", "int", "=", "0", ")", "->", "Iterator", "[", "List", "]", ":", "\n", "# if resuming iteration somewhere in the middle of epoch, one needs to adjust max_iterations", "\n", "        ", "max_iterations", "=", "self", ".", "max_iterations", "-", "self", ".", "iteration", "\n", "shard_indices", "=", "self", ".", "get_shard_indices", "(", "epoch", ")", "\n", "\n", "for", "i", "in", "range", "(", "\n", "self", ".", "iteration", "*", "self", ".", "batch_size", ",", "len", "(", "shard_indices", ")", ",", "self", ".", "batch_size", "\n", ")", ":", "\n", "            ", "items_idxs", "=", "shard_indices", "[", "i", ":", "i", "+", "self", ".", "batch_size", "]", "\n", "if", "self", ".", "strict_batch_size", "and", "len", "(", "items_idxs", ")", "<", "self", ".", "batch_size", ":", "\n", "                ", "logger", ".", "debug", "(", "\"Extending batch to max size\"", ")", "\n", "items_idxs", ".", "extend", "(", "shard_indices", "[", "0", ":", "self", ".", "batch_size", "-", "len", "(", "items", ")", "]", ")", "\n", "", "self", ".", "iteration", "+=", "1", "\n", "items", "=", "[", "self", ".", "data", "[", "idx", "]", "for", "idx", "in", "items_idxs", "]", "\n", "yield", "items", "\n", "\n", "# some shards may done iterating while the others are at the last batch. Just return the first batch", "\n", "", "while", "self", ".", "iteration", "<", "max_iterations", ":", "\n", "            ", "logger", ".", "debug", "(", "\"Fulfilling non complete shard=\"", ".", "format", "(", "self", ".", "shard_id", ")", ")", "\n", "self", ".", "iteration", "+=", "1", "\n", "items_idxs", "=", "shard_indices", "[", "0", ":", "self", ".", "batch_size", "]", "\n", "items", "=", "[", "self", ".", "data", "[", "idx", "]", "for", "idx", "in", "items_idxs", "]", "\n", "yield", "items", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Finished iterating, iteration={}, shard={}\"", ".", "format", "(", "\n", "self", ".", "iteration", ",", "self", ".", "shard_id", "\n", ")", "\n", ")", "\n", "# reset the iteration status", "\n", "self", ".", "iteration", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.iterate_ds_sampled_data": [[160, 179], ["data_utils.ShardedDataIterator.get_shard_indices", "itertools.cycle", "range", "logger.info", "next", "range"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.get_shard_indices"], ["", "def", "iterate_ds_sampled_data", "(", "\n", "self", ",", "num_iterations", ":", "int", ",", "epoch", ":", "int", "=", "0", "\n", ")", "->", "Iterator", "[", "List", "]", ":", "\n", "        ", "self", ".", "iteration", "=", "0", "\n", "shard_indices", "=", "self", ".", "get_shard_indices", "(", "epoch", ")", "\n", "cycle_it", "=", "itertools", ".", "cycle", "(", "shard_indices", ")", "\n", "for", "i", "in", "range", "(", "num_iterations", ")", ":", "\n", "            ", "items_idxs", "=", "[", "next", "(", "cycle_it", ")", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "self", ".", "iteration", "+=", "1", "\n", "items", "=", "[", "self", ".", "data", "[", "idx", "]", "for", "idx", "in", "items_idxs", "]", "\n", "yield", "items", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Finished iterating, iteration={}, shard={}\"", ".", "format", "(", "\n", "self", ".", "iteration", ",", "self", ".", "shard_id", "\n", ")", "\n", ")", "\n", "# TODO: reset the iteration status?", "\n", "self", ".", "iteration", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.get_dataset": [[180, 182], ["None"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ")", "->", "torch", ".", "utils", ".", "data", ".", "Dataset", ":", "\n", "        ", "return", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.__init__": [[189, 221], ["sum", "logger.info", "logger.info", "logger.info", "sum", "logger.info", "logger.info", "it.total_data_len", "int", "ds.max_iterations_num", "enumerate", "ds.max_iterations_num"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.total_data_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.max_iterations_num", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.max_iterations_num"], ["def", "__init__", "(", "\n", "self", ",", "\n", "datasets", ":", "List", "[", "ShardedDataIterator", "]", ",", "\n", "shuffle_seed", ":", "int", "=", "0", ",", "\n", "shuffle", "=", "True", ",", "\n", "sampling_rates", ":", "List", "=", "[", "]", ",", "\n", "rank", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "        ", "self", ".", "iterables", "=", "datasets", "\n", "data_lengths", "=", "[", "it", ".", "total_data_len", "(", ")", "for", "it", "in", "datasets", "]", "\n", "self", ".", "total_data", "=", "sum", "(", "data_lengths", ")", "\n", "logger", ".", "info", "(", "\"rank=%d; Multi set data sizes %s\"", ",", "rank", ",", "data_lengths", ")", "\n", "logger", ".", "info", "(", "\"rank=%d; Multi set total data %s\"", ",", "rank", ",", "self", ".", "total_data", ")", "\n", "logger", ".", "info", "(", "\"rank=%d; Multi set sampling_rates %s\"", ",", "rank", ",", "sampling_rates", ")", "\n", "self", ".", "shuffle_seed", "=", "shuffle_seed", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "iteration", "=", "0", "\n", "self", ".", "rank", "=", "rank", "\n", "\n", "if", "sampling_rates", ":", "\n", "            ", "self", ".", "max_its_pr_ds", "=", "[", "\n", "int", "(", "ds", ".", "max_iterations_num", "(", ")", "*", "sampling_rates", "[", "i", "]", ")", "\n", "for", "i", ",", "ds", "in", "enumerate", "(", "datasets", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "max_its_pr_ds", "=", "[", "ds", ".", "max_iterations_num", "(", ")", "for", "ds", "in", "datasets", "]", "\n", "\n", "", "self", ".", "max_iterations", "=", "sum", "(", "self", ".", "max_its_pr_ds", ")", "\n", "logger", ".", "info", "(", "\n", "\"rank=%d; Multi set max_iterations per dataset %s\"", ",", "rank", ",", "self", ".", "max_its_pr_ds", "\n", ")", "\n", "logger", ".", "info", "(", "\"rank=%d; Multi set max_iterations %d\"", ",", "rank", ",", "self", ".", "max_iterations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.total_data_len": [[222, 224], ["None"], "methods", ["None"], ["", "def", "total_data_len", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "total_data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_max_iterations": [[225, 227], ["None"], "methods", ["None"], ["", "def", "get_max_iterations", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "max_iterations", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.iterate_ds_data": [[228, 290], ["logger.info", "logger.info", "enumerate", "logger.info", "enumerate", "logger.info", "logger.info", "logger.info", "logger.info", "data_src_indices.extend", "iterators.append", "random.Random", "random.Random.shuffle", "len", "next", "next", "it.get_iteration", "data_utils.MultiSetDataIterator.iterables[].iterate_ds_sampled_data", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_iteration", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.iterate_ds_sampled_data"], ["", "def", "iterate_ds_data", "(", "self", ",", "epoch", ":", "int", "=", "0", ")", "->", "Iterator", "[", "Tuple", "[", "List", ",", "int", "]", "]", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"rank=%d; Iteration start\"", ",", "self", ".", "rank", ")", "\n", "logger", ".", "info", "(", "\n", "\"rank=%d; Multi set iteration: iteration ptr per set: %s\"", ",", "\n", "self", ".", "rank", ",", "\n", "[", "it", ".", "get_iteration", "(", ")", "for", "it", "in", "self", ".", "iterables", "]", ",", "\n", ")", "\n", "\n", "data_src_indices", "=", "[", "]", "\n", "iterators", "=", "[", "]", "\n", "for", "source", ",", "src_its", "in", "enumerate", "(", "self", ".", "max_its_pr_ds", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"rank=%d; Multi set iteration: source %d, batches to be taken: %s\"", ",", "\n", "self", ".", "rank", ",", "\n", "source", ",", "\n", "src_its", ",", "\n", ")", "\n", "data_src_indices", ".", "extend", "(", "[", "source", "]", "*", "src_its", ")", "\n", "\n", "iterators", ".", "append", "(", "\n", "self", ".", "iterables", "[", "source", "]", ".", "iterate_ds_sampled_data", "(", "src_its", ",", "epoch", "=", "epoch", ")", "\n", ")", "\n", "\n", "", "if", "self", ".", "shuffle", ":", "\n", "# to be able to resume, same shuffling should be used when starting from a failed/stopped iteration", "\n", "            ", "epoch_rnd", "=", "random", ".", "Random", "(", "self", ".", "shuffle_seed", "+", "epoch", ")", "\n", "epoch_rnd", ".", "shuffle", "(", "data_src_indices", ")", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"rank=%d; data_src_indices len=%d\"", ",", "self", ".", "rank", ",", "len", "(", "data_src_indices", ")", "\n", ")", "\n", "for", "i", ",", "source_idx", "in", "enumerate", "(", "data_src_indices", ")", ":", "\n", "            ", "it", "=", "iterators", "[", "source_idx", "]", "\n", "next_item", "=", "next", "(", "it", ",", "None", ")", "\n", "if", "next_item", "is", "not", "None", ":", "\n", "                ", "self", ".", "iteration", "+=", "1", "\n", "yield", "(", "next_item", ",", "source_idx", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"rank=%d; Next item in the source %s is None\"", ",", "self", ".", "rank", ",", "source_idx", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"rank=%d; last iteration %d\"", ",", "self", ".", "rank", ",", "self", ".", "iteration", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "\"rank=%d; Multi set iteration finished: iteration per set: %s\"", ",", "\n", "self", ".", "rank", ",", "\n", "[", "it", ".", "iteration", "for", "it", "in", "self", ".", "iterables", "]", ",", "\n", ")", "\n", "[", "next", "(", "it", ",", "None", ")", "for", "it", "in", "iterators", "]", "\n", "\n", "# TODO: clear iterators in some non-hacky way", "\n", "for", "it", "in", "self", ".", "iterables", ":", "\n", "            ", "it", ".", "iteration", "=", "0", "\n", "", "logger", ".", "info", "(", "\n", "\"rank=%d; Multi set iteration finished after next: iteration per set: %s\"", ",", "\n", "self", ".", "rank", ",", "\n", "[", "it", ".", "iteration", "for", "it", "in", "self", ".", "iterables", "]", ",", "\n", ")", "\n", "# reset the iteration status", "\n", "self", ".", "iteration", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_iteration": [[291, 293], ["None"], "methods", ["None"], ["", "def", "get_iteration", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_dataset": [[294, 296], ["data_utils.MultiSetDataIterator.iterables[].get_dataset"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_dataset"], ["", "def", "get_dataset", "(", "self", ",", "ds_id", ":", "int", ")", "->", "torch", ".", "utils", ".", "data", ".", "Dataset", ":", "\n", "        ", "return", "self", ".", "iterables", "[", "ds_id", "]", ".", "get_dataset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_datasets": [[297, 299], ["it.get_dataset"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.MultiSetDataIterator.get_dataset"], ["", "def", "get_datasets", "(", "self", ")", "->", "List", "[", "torch", ".", "utils", ".", "data", ".", "Dataset", "]", ":", "\n", "        ", "return", "[", "it", ".", "get_dataset", "(", ")", "for", "it", "in", "self", ".", "iterables", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.text_to_tensor": [[307, 315], ["None"], "methods", ["None"], ["def", "text_to_tensor", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "title", ":", "str", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "apply_max_len", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.get_pair_separator_ids": [[316, 318], ["None"], "methods", ["None"], ["", "def", "get_pair_separator_ids", "(", "self", ")", "->", "T", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.get_pad_id": [[319, 321], ["None"], "methods", ["None"], ["", "def", "get_pad_id", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.get_attn_mask": [[322, 324], ["None"], "methods", ["None"], ["", "def", "get_attn_mask", "(", "self", ",", "tokens_tensor", ":", "T", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.is_sub_word_id": [[325, 327], ["None"], "methods", ["None"], ["", "def", "is_sub_word_id", "(", "self", ",", "token_id", ":", "int", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.to_string": [[328, 330], ["None"], "methods", ["None"], ["", "def", "to_string", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "True", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.set_pad_to_max": [[331, 333], ["None"], "methods", ["None"], ["", "def", "set_pad_to_max", "(", "self", ",", "pad", ":", "bool", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.Tensorizer.get_token_id": [[334, 336], ["None"], "methods", ["None"], ["", "def", "get_token_id", "(", "self", ",", "token", ":", "str", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.read_serialized_data_from_files": [[26, 36], ["enumerate", "logger.info", "open", "logger.info", "pickle.load", "results.extend", "logger.info", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["def", "read_serialized_data_from_files", "(", "paths", ":", "List", "[", "str", "]", ")", "->", "List", ":", "\n", "    ", "results", "=", "[", "]", "\n", "for", "i", ",", "path", "in", "enumerate", "(", "paths", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading file %s\"", ",", "path", ")", "\n", "data", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "results", ".", "extend", "(", "data", ")", "\n", "logger", ".", "info", "(", "\"Aggregated data size: {}\"", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "", "", "logger", ".", "info", "(", "\"Total data size: {}\"", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.read_data_from_json_files": [[40, 48], ["open", "logger.info", "json.load", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "read_data_from_json_files", "(", "path", ":", "str", ")", "->", "List", ":", "\n", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading file %s\"", "%", "path", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "results", "=", "data", "\n", "logger", ".", "info", "(", "\"Aggregated data size: {}\"", ".", "format", "(", "len", "(", "results", ")", ")", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.__init__": [[31, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "data", ",", "annotators", ",", "opts", "=", "None", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "annotators", "=", "annotators", "\n", "self", ".", "opts", "=", "opts", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.__len__": [[36, 39], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of tokens.\"\"\"", "\n", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.slice": [[40, 45], ["copy.copy"], "methods", ["None"], ["", "def", "slice", "(", "self", ",", "i", "=", "None", ",", "j", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return a view of the list of tokens from [i, j).\"\"\"", "\n", "new_tokens", "=", "copy", ".", "copy", "(", "self", ")", "\n", "new_tokens", ".", "data", "=", "self", ".", "data", "[", "i", ":", "j", "]", "\n", "return", "new_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.untokenize": [[46, 49], ["None"], "methods", ["None"], ["", "def", "untokenize", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the original text (with whitespace reinserted).\"\"\"", "\n", "return", "''", ".", "join", "(", "[", "t", "[", "self", ".", "TEXT_WS", "]", "for", "t", "in", "self", ".", "data", "]", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.words": [[50, 60], ["t[].lower"], "methods", ["None"], ["", "def", "words", "(", "self", ",", "uncased", "=", "False", ")", ":", "\n", "        ", "\"\"\"Returns a list of the text of each token\n\n        Args:\n            uncased: lower cases text\n        \"\"\"", "\n", "if", "uncased", ":", "\n", "            ", "return", "[", "t", "[", "self", ".", "TEXT", "]", ".", "lower", "(", ")", "for", "t", "in", "self", ".", "data", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "t", "[", "self", ".", "TEXT", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.offsets": [[61, 64], ["None"], "methods", ["None"], ["", "", "def", "offsets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"", "\n", "return", "[", "t", "[", "self", ".", "SPAN", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.pos": [[65, 72], ["None"], "methods", ["None"], ["", "def", "pos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of part-of-speech tags of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'pos'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "POS", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.lemmas": [[73, 80], ["None"], "methods", ["None"], ["", "def", "lemmas", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of the lemmatized text of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'lemma'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "LEMMA", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.entities": [[81, 88], ["None"], "methods", ["None"], ["", "def", "entities", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a list of named-entity-recognition tags of each token.\n        Returns None if this annotation was not included.\n        \"\"\"", "\n", "if", "'ner'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "t", "[", "self", ".", "NER", "]", "for", "t", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.ngrams": [[89, 116], ["tokenizers.Tokens.words", "filter_fn", "range", "range", "len", "min", "tokenizers.Tokens.ngrams._skip"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.words"], ["", "def", "ngrams", "(", "self", ",", "n", "=", "1", ",", "uncased", "=", "False", ",", "filter_fn", "=", "None", ",", "as_strings", "=", "True", ")", ":", "\n", "        ", "\"\"\"Returns a list of all ngrams from length 1 to n.\n\n        Args:\n            n: upper limit of ngram length\n            uncased: lower cases text\n            filter_fn: user function that takes in an ngram list and returns\n              True or False to keep or not keep the ngram\n            as_string: return the ngram as a string vs list\n        \"\"\"", "\n", "\n", "def", "_skip", "(", "gram", ")", ":", "\n", "            ", "if", "not", "filter_fn", ":", "\n", "                ", "return", "False", "\n", "", "return", "filter_fn", "(", "gram", ")", "\n", "\n", "", "words", "=", "self", ".", "words", "(", "uncased", ")", "\n", "ngrams", "=", "[", "(", "s", ",", "e", "+", "1", ")", "\n", "for", "s", "in", "range", "(", "len", "(", "words", ")", ")", "\n", "for", "e", "in", "range", "(", "s", ",", "min", "(", "s", "+", "n", ",", "len", "(", "words", ")", ")", ")", "\n", "if", "not", "_skip", "(", "words", "[", "s", ":", "e", "+", "1", "]", ")", "]", "\n", "\n", "# Concatenate into strings", "\n", "if", "as_strings", ":", "\n", "            ", "ngrams", "=", "[", "'{}'", ".", "format", "(", "' '", ".", "join", "(", "words", "[", "s", ":", "e", "]", ")", ")", "for", "(", "s", ",", "e", ")", "in", "ngrams", "]", "\n", "\n", "", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.entity_groups": [[117, 137], ["tokenizers.Tokens.entities", "tokenizers.Tokens.opts.get", "len", "groups.append", "len", "tokenizers.Tokens.slice().untokenize", "tokenizers.Tokens.slice"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.entities", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.untokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.slice"], ["", "def", "entity_groups", "(", "self", ")", ":", "\n", "        ", "\"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"", "\n", "entities", "=", "self", ".", "entities", "(", ")", "\n", "if", "not", "entities", ":", "\n", "            ", "return", "None", "\n", "", "non_ent", "=", "self", ".", "opts", ".", "get", "(", "'non_ent'", ",", "'O'", ")", "\n", "groups", "=", "[", "]", "\n", "idx", "=", "0", "\n", "while", "idx", "<", "len", "(", "entities", ")", ":", "\n", "            ", "ner_tag", "=", "entities", "[", "idx", "]", "\n", "# Check for entity tag", "\n", "if", "ner_tag", "!=", "non_ent", ":", "\n", "# Chomp the sequence", "\n", "                ", "start", "=", "idx", "\n", "while", "(", "idx", "<", "len", "(", "entities", ")", "and", "entities", "[", "idx", "]", "==", "ner_tag", ")", ":", "\n", "                    ", "idx", "+=", "1", "\n", "", "groups", ".", "append", "(", "(", "self", ".", "slice", "(", "start", ",", "idx", ")", ".", "untokenize", "(", ")", ",", "ner_tag", ")", ")", "\n", "", "else", ":", "\n", "                ", "idx", "+=", "1", "\n", "", "", "return", "groups", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokenizer.tokenize": [[144, 146], ["None"], "methods", ["None"], ["def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokenizer.shutdown": [[147, 149], ["None"], "methods", ["None"], ["", "def", "shutdown", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokenizer.__del__": [[150, 152], ["tokenizers.Tokenizer.shutdown"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokenizer.shutdown"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "shutdown", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.SimpleTokenizer.__init__": [[158, 171], ["regex.compile", "set", "len", "logger.warning", "kwargs.get", "kwargs.get", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            annotators: None or empty set (only tokenizes).\n        \"\"\"", "\n", "self", ".", "_regexp", "=", "regex", ".", "compile", "(", "\n", "'(%s)|(%s)'", "%", "(", "self", ".", "ALPHA_NUM", ",", "self", ".", "NON_WS", ")", ",", "\n", "flags", "=", "regex", ".", "IGNORECASE", "+", "regex", ".", "UNICODE", "+", "regex", ".", "MULTILINE", "\n", ")", "\n", "if", "len", "(", "kwargs", ".", "get", "(", "'annotators'", ",", "{", "}", ")", ")", ">", "0", ":", "\n", "            ", "logger", ".", "warning", "(", "'%s only tokenizes! Skipping annotators: %s'", "%", "\n", "(", "type", "(", "self", ")", ".", "__name__", ",", "kwargs", ".", "get", "(", "'annotators'", ")", ")", ")", "\n", "", "self", ".", "annotators", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.SimpleTokenizer.tokenize": [[172, 194], ["range", "tokenizers.Tokens", "len", "matches[].group", "matches[].span", "data.append", "tokenizers.SimpleTokenizer._regexp.finditer", "len", "matches[].span"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "matches", "=", "[", "m", "for", "m", "in", "self", ".", "_regexp", ".", "finditer", "(", "text", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "matches", ")", ")", ":", "\n", "# Get text", "\n", "            ", "token", "=", "matches", "[", "i", "]", ".", "group", "(", ")", "\n", "\n", "# Get whitespace", "\n", "span", "=", "matches", "[", "i", "]", ".", "span", "(", ")", "\n", "start_ws", "=", "span", "[", "0", "]", "\n", "if", "i", "+", "1", "<", "len", "(", "matches", ")", ":", "\n", "                ", "end_ws", "=", "matches", "[", "i", "+", "1", "]", ".", "span", "(", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "end_ws", "=", "span", "[", "1", "]", "\n", "\n", "# Format data", "\n", "", "data", ".", "append", "(", "(", "\n", "token", ",", "\n", "text", "[", "start_ws", ":", "end_ws", "]", ",", "\n", "span", ",", "\n", ")", ")", "\n", "", "return", "Tokens", "(", "data", ",", "self", ".", "annotators", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.SpacyTokenizer.__init__": [[198, 212], ["kwargs.get", "copy.deepcopy", "spacy.load", "kwargs.get", "any", "set"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            annotators: set that can include pos, lemma, and ner.\n            model: spaCy model to use (either path, or keyword like 'en').\n        \"\"\"", "\n", "model", "=", "kwargs", ".", "get", "(", "\"model\"", ",", "\"en_core_web_sm\"", ")", "\n", "self", ".", "annotators", "=", "copy", ".", "deepcopy", "(", "kwargs", ".", "get", "(", "'annotators'", ",", "set", "(", ")", ")", ")", "\n", "nlp_kwargs", "=", "{", "'parser'", ":", "False", "}", "\n", "if", "not", "any", "(", "[", "p", "in", "self", ".", "annotators", "for", "p", "in", "[", "'lemma'", ",", "'pos'", ",", "'ner'", "]", "]", ")", ":", "\n", "            ", "nlp_kwargs", "[", "'tagger'", "]", "=", "False", "\n", "", "if", "'ner'", "not", "in", "self", ".", "annotators", ":", "\n", "            ", "nlp_kwargs", "[", "'entity'", "]", "=", "False", "\n", "", "self", ".", "nlp", "=", "spacy", ".", "load", "(", "model", ",", "**", "nlp_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.SpacyTokenizer.tokenize": [[213, 242], ["text.replace", "tokenizers.SpacyTokenizer.nlp.tokenizer", "any", "range", "tokenizers.Tokens", "tokenizers.SpacyTokenizer.nlp.tagger", "tokenizers.SpacyTokenizer.nlp.entity", "len", "data.append", "len", "len", "len"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "# We don't treat new lines as tokens.", "\n", "        ", "clean_text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "tokens", "=", "self", ".", "nlp", ".", "tokenizer", "(", "clean_text", ")", "\n", "if", "any", "(", "[", "p", "in", "self", ".", "annotators", "for", "p", "in", "[", "'lemma'", ",", "'pos'", ",", "'ner'", "]", "]", ")", ":", "\n", "            ", "self", ".", "nlp", ".", "tagger", "(", "tokens", ")", "\n", "", "if", "'ner'", "in", "self", ".", "annotators", ":", "\n", "            ", "self", ".", "nlp", ".", "entity", "(", "tokens", ")", "\n", "\n", "", "data", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "# Get whitespace", "\n", "            ", "start_ws", "=", "tokens", "[", "i", "]", ".", "idx", "\n", "if", "i", "+", "1", "<", "len", "(", "tokens", ")", ":", "\n", "                ", "end_ws", "=", "tokens", "[", "i", "+", "1", "]", ".", "idx", "\n", "", "else", ":", "\n", "                ", "end_ws", "=", "tokens", "[", "i", "]", ".", "idx", "+", "len", "(", "tokens", "[", "i", "]", ".", "text", ")", "\n", "\n", "", "data", ".", "append", "(", "(", "\n", "tokens", "[", "i", "]", ".", "text", ",", "\n", "text", "[", "start_ws", ":", "end_ws", "]", ",", "\n", "(", "tokens", "[", "i", "]", ".", "idx", ",", "tokens", "[", "i", "]", ".", "idx", "+", "len", "(", "tokens", "[", "i", "]", ".", "text", ")", ")", ",", "\n", "tokens", "[", "i", "]", ".", "tag_", ",", "\n", "tokens", "[", "i", "]", ".", "lemma_", ",", "\n", "tokens", "[", "i", "]", ".", "ent_type_", ",", "\n", ")", ")", "\n", "\n", "# Set special option for non-entity tag: '' vs 'O' in spaCy", "\n", "", "return", "Tokens", "(", "data", ",", "self", ".", "annotators", ",", "opts", "=", "{", "'non_ent'", ":", "''", "}", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.conf_utils.BiencoderDatasetsCfg.__init__": [[10, 29], ["logger.info", "logger.info", "hydra.utils.instantiate", "hydra.utils.instantiate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ":", "DictConfig", ")", ":", "\n", "        ", "datasets", "=", "cfg", ".", "datasets", "\n", "self", ".", "train_datasets_names", "=", "cfg", ".", "train_datasets", "\n", "logger", ".", "info", "(", "\"train_datasets: %s\"", ",", "self", ".", "train_datasets_names", ")", "\n", "if", "self", ".", "train_datasets_names", ":", "\n", "            ", "self", ".", "train_datasets", "=", "[", "\n", "hydra", ".", "utils", ".", "instantiate", "(", "datasets", "[", "ds_name", "]", ")", "\n", "for", "ds_name", "in", "self", ".", "train_datasets_names", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_datasets", "=", "[", "]", "\n", "", "if", "cfg", ".", "dev_datasets", ":", "\n", "            ", "self", ".", "dev_datasets_names", "=", "cfg", ".", "dev_datasets", "\n", "logger", ".", "info", "(", "\"dev_datasets: %s\"", ",", "self", ".", "dev_datasets_names", ")", "\n", "self", ".", "dev_datasets", "=", "[", "\n", "hydra", ".", "utils", ".", "instantiate", "(", "datasets", "[", "ds_name", "]", ")", "\n", "for", "ds_name", "in", "self", ".", "dev_datasets_names", "\n", "]", "\n", "", "self", ".", "sampling_rates", "=", "cfg", ".", "train_sampling_rates", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.__init__": [[67, 82], ["torch.nn.Module.__init__", "biencoder.BiEncoder.load_images"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.load_images"], ["def", "__init__", "(", "\n", "self", ",", "\n", "question_model", ":", "nn", ".", "Module", ",", "\n", "ctx_model", ":", "nn", ".", "Module", ",", "\n", "fix_q_encoder", ":", "bool", "=", "False", ",", "\n", "fix_ctx_encoder", ":", "bool", "=", "False", ",", "\n", "load_images", ":", "bool", "=", "True", "\n", ")", ":", "\n", "        ", "super", "(", "BiEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "question_model", "=", "question_model", "\n", "self", ".", "ctx_model", "=", "ctx_model", "\n", "self", ".", "fix_q_encoder", "=", "fix_q_encoder", "\n", "self", ".", "fix_ctx_encoder", "=", "fix_ctx_encoder", "\n", "if", "load_images", ":", "\n", "            ", "self", ".", "load_images", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.load_images": [[83, 123], ["print", "print", "os.listdir", "numpy.load", "d.tolist.tolist.tolist", "float", "float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.unsqueeze", "torch.tensor.unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "os.path.join", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "", "def", "load_images", "(", "self", ",", "dirs", "=", "[", "\"/scratch/yzeng55/okvqa/img_feat/clean_train/\"", ",", "\"/scratch/yzeng55/okvqa/img_feat/clean_val/\"", "]", ",", "dummy", "=", "False", ")", ":", "\n", "        ", "visual_feats", "=", "{", "}", "\n", "visual_pos", "=", "{", "}", "\n", "print", "(", "\"start to load image features\"", ")", "\n", "for", "dir", "in", "dirs", ":", "\n", "            ", "files", "=", "os", ".", "listdir", "(", "dir", ")", "\n", "if", "dummy", ":", "\n", "                ", "files", "=", "files", "[", ":", "10", "]", "\n", "", "for", "file", "in", "files", ":", "\n", "\n", "                ", "d", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "file", ")", ",", "allow_pickle", "=", "True", ")", "\n", "d", "=", "d", ".", "tolist", "(", ")", "\n", "imgid", "=", "d", "[", "'image_id'", "]", "\n", "img_h", "=", "float", "(", "d", "[", "'image_height'", "]", ")", "\n", "img_w", "=", "float", "(", "d", "[", "'image_width'", "]", ")", "\n", "feats", "=", "torch", ".", "tensor", "(", "d", "[", "'features'", "]", ")", "\n", "visual_feats", "[", "imgid", "]", "=", "feats", ".", "unsqueeze", "(", "0", ")", "\n", "boxes", "=", "d", "[", "'bbox'", "]", "\n", "boxes", "[", ":", ",", "(", "0", ",", "2", ")", "]", "/=", "img_w", "\n", "boxes", "[", ":", ",", "(", "1", ",", "3", ")", "]", "/=", "img_h", "\n", "for", "bd_box", "in", "boxes", ":", "\n", "                    ", "if", "bd_box", "[", "0", "]", "<", "-", "0.0001", ":", "\n", "                        ", "bd_box", "[", "0", "]", "=", "0", "\n", "", "if", "bd_box", "[", "0", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "0", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "2", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "2", "]", "=", "1", "\n", "", "if", "bd_box", "[", "1", "]", "<", "-", "0.0001", ":", "\n", "                        ", "bd_box", "[", "1", "]", "=", "0", "\n", "", "if", "bd_box", "[", "1", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "1", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "3", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "3", "]", "=", "1", "\n", "", "", "boxes", "=", "torch", ".", "tensor", "(", "boxes", ")", ".", "unsqueeze", "(", "0", ")", "\n", "visual_pos", "[", "imgid", "]", "=", "boxes", "\n", "\n", "\n", "", "", "self", ".", "visual_feats", "=", "visual_feats", "\n", "self", ".", "visual_pos", "=", "visual_pos", "\n", "print", "(", "f\"loaded {len(self.visual_feats)} image features from {dirs}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.get_representation": [[124, 158], ["sub_model", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sub_model", "sequence_output.requires_grad_", "pooled_output.requires_grad_"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_representation", "(", "\n", "sub_model", ":", "nn", ".", "Module", ",", "\n", "ids", ":", "T", ",", "\n", "segments", ":", "T", ",", "\n", "attn_mask", ":", "T", ",", "\n", "fix_encoder", ":", "bool", "=", "False", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", ")", "->", "(", "T", ",", "T", ",", "T", ")", ":", "\n", "        ", "sequence_output", "=", "None", "\n", "pooled_output", "=", "None", "\n", "hidden_states", "=", "None", "\n", "if", "ids", "is", "not", "None", ":", "\n", "            ", "if", "fix_encoder", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "representation_token_pos", "=", "representation_token_pos", ",", "\n", ")", "\n", "\n", "", "if", "sub_model", ".", "training", ":", "\n", "                    ", "sequence_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "pooled_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "representation_token_pos", "=", "representation_token_pos", ",", "\n", ")", "\n", "\n", "", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.get_visual_representation": [[159, 210], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sub_model", "sequence_output.requires_grad_", "pooled_output.requires_grad_", "sub_model", "sub_model", "print"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_visual_representation", "(", "\n", "sub_model", ":", "nn", ".", "Module", ",", "\n", "ids", ":", "T", ",", "\n", "segments", ":", "T", ",", "\n", "attn_mask", ":", "T", ",", "\n", "fix_encoder", ":", "bool", "=", "False", ",", "\n", "visual_feats", "=", "None", ",", "\n", "visual_pos", "=", "None", ",", "\n", ")", "->", "(", "T", ",", "T", ",", "T", ")", ":", "\n", "        ", "sequence_output", "=", "None", "\n", "pooled_output", "=", "None", "\n", "hidden_states", "=", "None", "\n", "if", "ids", "is", "not", "None", ":", "\n", "            ", "if", "fix_encoder", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", ")", "\n", "\n", "", "if", "sub_model", ".", "training", ":", "\n", "                    ", "sequence_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "pooled_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "", "", "else", ":", "\n", "\n", "                ", "try", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "input_ids", "=", "ids", ",", "\n", "token_type_ids", "=", "segments", ",", "\n", "attention_mask", "=", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "return_dict", "=", "True", "\n", ")", "\n", "", "except", ":", "\n", "                    ", "outputs", "=", "sub_model", "(", "\n", "input_ids", "=", "ids", ",", "\n", "token_type_ids", "=", "segments", ",", "\n", "attention_mask", "=", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "return_dict", "=", "True", "\n", ")", "\n", "print", "(", "\"visual outputs\"", ",", "outputs", ")", "\n", "\n", "\n", "", "", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.forward": [[211, 265], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "visual_feats.to.to.to", "visual_pos.to.to.to", "biencoder.BiEncoder.get_visual_representation", "biencoder.BiEncoder.get_representation", "visual_feats.to.to.append", "visual_pos.to.to.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.get_visual_representation", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "question_ids", ":", "T", ",", "\n", "question_segments", ":", "T", ",", "\n", "question_attn_mask", ":", "T", ",", "\n", "context_ids", ":", "T", ",", "\n", "ctx_segments", ":", "T", ",", "\n", "ctx_attn_mask", ":", "T", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", "img_ids", "=", "None", "\n", ")", "->", "Tuple", "[", "T", ",", "T", "]", ":", "\n", "        ", "visual_feats", "=", "[", "]", "\n", "visual_pos", "=", "[", "]", "\n", "for", "ids", "in", "img_ids", ":", "\n", "#             try:", "\n", "            ", "visual_feats", ".", "append", "(", "self", ".", "visual_feats", "[", "ids", "]", ")", "\n", "visual_pos", ".", "append", "(", "self", ".", "visual_pos", "[", "ids", "]", ")", "\n", "#             except:", "\n", "#                 visual_feats.append(torch.rand([1,36,2048]))", "\n", "#                 visual_pos.append(torch.rand([1,36,4]))", "\n", "\n", "\n", "", "visual_feats", "=", "torch", ".", "cat", "(", "visual_feats", ",", "dim", "=", "0", ")", "\n", "visual_pos", "=", "torch", ".", "cat", "(", "visual_pos", ",", "dim", "=", "0", ")", "\n", "device", "=", "question_ids", ".", "device", "\n", "visual_feats", "=", "visual_feats", ".", "to", "(", "device", ")", "\n", "visual_pos", "=", "visual_pos", ".", "to", "(", "device", ")", "\n", "\n", "q_encoder", "=", "(", "\n", "self", ".", "question_model", "\n", "if", "encoder_type", "is", "None", "or", "encoder_type", "==", "\"question\"", "\n", "else", "self", ".", "ctx_model", "\n", ")", "\n", "_q_seq", ",", "q_pooled_out", ",", "_q_hidden", "=", "self", ".", "get_visual_representation", "(", "\n", "q_encoder", ",", "\n", "question_ids", ",", "\n", "question_segments", ",", "\n", "question_attn_mask", ",", "\n", "self", ".", "fix_q_encoder", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", "\n", ")", "\n", "\n", "ctx_encoder", "=", "(", "\n", "self", ".", "ctx_model", "\n", "if", "encoder_type", "is", "None", "or", "encoder_type", "==", "\"ctx\"", "\n", "else", "self", ".", "question_model", "\n", ")", "\n", "_ctx_seq", ",", "ctx_pooled_out", ",", "_ctx_hidden", "=", "self", ".", "get_representation", "(", "\n", "ctx_encoder", ",", "context_ids", ",", "ctx_segments", ",", "ctx_attn_mask", ",", "self", ".", "fix_ctx_encoder", "\n", ")", "\n", "\n", "return", "q_pooled_out", ",", "ctx_pooled_out", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.create_biencoder_input": [[267, 361], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "BiEncoderBatch", "len", "ctx_tensors.extend", "positive_ctx_indices.append", "hard_neg_ctx_indices.append", "question_tensors.append", "random.shuffle", "random.shuffle", "len", "tensorizer.text_to_tensor", "tensorizer.text_to_tensor", "ctx.view", "q.view", "len", "numpy.random.choice", "range", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["", "@", "classmethod", "\n", "def", "create_biencoder_input", "(", "\n", "cls", ",", "\n", "samples", ":", "List", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", ",", "\n", "num_hard_negatives", ":", "int", "=", "0", ",", "\n", "num_other_negatives", ":", "int", "=", "0", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "hard_neg_fallback", ":", "bool", "=", "True", ",", "\n", ")", "->", "BiEncoderBatch", ":", "\n", "        ", "\"\"\"\n        Creates a batch of the biencoder training tuple.\n        :param samples: list of data items (from json) to create the batch for\n        :param tensorizer: components to create model input tensors from a text sequence\n        :param insert_title: enables title insertion at the beginning of the context sequences\n        :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n        :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n        :param shuffle: shuffles negative passages pools\n        :param shuffle_positives: shuffles positive passages pools\n        :return: BiEncoderBatch tuple\n        \"\"\"", "\n", "question_tensors", "=", "[", "]", "\n", "ctx_tensors", "=", "[", "]", "\n", "positive_ctx_indices", "=", "[", "]", "\n", "hard_neg_ctx_indices", "=", "[", "]", "\n", "visual_feats", "=", "[", "]", "\n", "visual_pos", "=", "[", "]", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "# ctx+ & [ctx-] composition", "\n", "# as of now, take the first(gold) ctx+ only", "\n", "            ", "if", "shuffle", "and", "shuffle_positives", ":", "\n", "                ", "positive_ctxs", "=", "sample", "[", "\"positive_ctxs\"", "]", "\n", "positive_ctx", "=", "positive_ctxs", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "positive_ctxs", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "positive_ctx", "=", "sample", "[", "\"positive_ctxs\"", "]", "[", "0", "]", "\n", "\n", "", "neg_ctxs", "=", "sample", "[", "\"negative_ctxs\"", "]", "\n", "hard_neg_ctxs", "=", "sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "neg_ctxs", ")", "\n", "random", ".", "shuffle", "(", "hard_neg_ctxs", ")", "\n", "\n", "", "if", "hard_neg_fallback", "and", "len", "(", "hard_neg_ctxs", ")", "==", "0", ":", "\n", "                ", "hard_neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "", "neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_other_negatives", "]", "\n", "hard_neg_ctxs", "=", "hard_neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "all_ctxs", "=", "[", "positive_ctx", "]", "+", "neg_ctxs", "+", "hard_neg_ctxs", "\n", "hard_negatives_start_idx", "=", "1", "\n", "hard_negatives_end_idx", "=", "1", "+", "len", "(", "hard_neg_ctxs", ")", "\n", "\n", "current_ctxs_len", "=", "len", "(", "ctx_tensors", ")", "\n", "\n", "sample_ctxs_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", "[", "\"text\"", "]", ",", "\n", "title", "=", "ctx", "[", "\"title\"", "]", "if", "(", "insert_title", "and", "\"title\"", "in", "ctx", ")", "else", "None", ",", "\n", ")", "\n", "for", "ctx", "in", "all_ctxs", "\n", "]", "\n", "\n", "ctx_tensors", ".", "extend", "(", "sample_ctxs_tensors", ")", "\n", "positive_ctx_indices", ".", "append", "(", "current_ctxs_len", ")", "\n", "hard_neg_ctx_indices", ".", "append", "(", "\n", "[", "\n", "i", "\n", "for", "i", "in", "range", "(", "\n", "current_ctxs_len", "+", "hard_negatives_start_idx", ",", "\n", "current_ctxs_len", "+", "hard_negatives_end_idx", ",", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "question_tensors", ".", "append", "(", "tensorizer", ".", "text_to_tensor", "(", "question", ")", ")", "\n", "\n", "\n", "", "ctxs_tensor", "=", "torch", ".", "cat", "(", "[", "ctx", ".", "view", "(", "1", ",", "-", "1", ")", "for", "ctx", "in", "ctx_tensors", "]", ",", "dim", "=", "0", ")", "\n", "questions_tensor", "=", "torch", ".", "cat", "(", "[", "q", ".", "view", "(", "1", ",", "-", "1", ")", "for", "q", "in", "question_tensors", "]", ",", "dim", "=", "0", ")", "\n", "\n", "ctx_segments", "=", "torch", ".", "zeros_like", "(", "ctxs_tensor", ")", "\n", "question_segments", "=", "torch", ".", "zeros_like", "(", "questions_tensor", ")", "\n", "\n", "return", "BiEncoderBatch", "(", "\n", "questions_tensor", ",", "\n", "question_segments", ",", "\n", "ctxs_tensor", ",", "\n", "ctx_segments", ",", "\n", "positive_ctx_indices", ",", "\n", "hard_neg_ctx_indices", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.create_biencoder_input2": [[364, 477], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "BiEncoderBatch", "len", "ctx_tensors.extend", "positive_ctx_indices.append", "hard_neg_ctx_indices.append", "img_ids.append", "random.shuffle", "random.shuffle", "len", "tensorizer.text_to_tensor", "question_tensors.append", "ctx.view", "q.view", "len", "biencoder._select_span_with_token", "question_tensors.append", "question_tensors.append", "tensorizer.text_to_tensor", "numpy.random.choice", "range", "tensorizer.text_to_tensor", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder._select_span_with_token", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["", "@", "classmethod", "\n", "def", "create_biencoder_input2", "(", "\n", "cls", ",", "\n", "samples", ":", "List", "[", "VisBiEncoderSample", "]", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", ",", "\n", "num_hard_negatives", ":", "int", "=", "0", ",", "\n", "num_other_negatives", ":", "int", "=", "0", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "hard_neg_fallback", ":", "bool", "=", "True", ",", "\n", "query_token", ":", "str", "=", "None", ",", "\n", ")", "->", "BiEncoderBatch", ":", "\n", "        ", "\"\"\"\n        Creates a batch of the biencoder training tuple.\n        :param samples: list of BiEncoderSample-s to create the batch for\n        :param tensorizer: components to create model input tensors from a text sequence\n        :param insert_title: enables title insertion at the beginning of the context sequences\n        :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n        :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n        :param shuffle: shuffles negative passages pools\n        :param shuffle_positives: shuffles positive passages pools\n        :return: BiEncoderBatch tuple\n        \"\"\"", "\n", "question_tensors", "=", "[", "]", "\n", "ctx_tensors", "=", "[", "]", "\n", "positive_ctx_indices", "=", "[", "]", "\n", "hard_neg_ctx_indices", "=", "[", "]", "\n", "img_ids", "=", "[", "]", "\n", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "# ctx+ & [ctx-] composition", "\n", "# as of now, take the first(gold) ctx+ only", "\n", "\n", "            ", "if", "shuffle", "and", "shuffle_positives", ":", "\n", "                ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "positive_ctx", "=", "positive_ctxs", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "positive_ctxs", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "positive_ctx", "=", "sample", ".", "positive_passages", "[", "0", "]", "\n", "\n", "", "neg_ctxs", "=", "sample", ".", "negative_passages", "\n", "hard_neg_ctxs", "=", "sample", ".", "hard_negative_passages", "\n", "question", "=", "sample", ".", "query", "\n", "# question = normalize_question(sample.query)", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "neg_ctxs", ")", "\n", "random", ".", "shuffle", "(", "hard_neg_ctxs", ")", "\n", "\n", "", "if", "hard_neg_fallback", "and", "len", "(", "hard_neg_ctxs", ")", "==", "0", ":", "\n", "                ", "hard_neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "", "neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_other_negatives", "]", "\n", "hard_neg_ctxs", "=", "hard_neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "all_ctxs", "=", "[", "positive_ctx", "]", "+", "neg_ctxs", "+", "hard_neg_ctxs", "\n", "hard_negatives_start_idx", "=", "1", "\n", "hard_negatives_end_idx", "=", "1", "+", "len", "(", "hard_neg_ctxs", ")", "\n", "\n", "current_ctxs_len", "=", "len", "(", "ctx_tensors", ")", "\n", "\n", "sample_ctxs_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", ".", "text", ",", "title", "=", "ctx", ".", "title", "if", "(", "insert_title", "and", "ctx", ".", "title", ")", "else", "None", "\n", ")", "\n", "for", "ctx", "in", "all_ctxs", "\n", "]", "\n", "\n", "ctx_tensors", ".", "extend", "(", "sample_ctxs_tensors", ")", "\n", "positive_ctx_indices", ".", "append", "(", "current_ctxs_len", ")", "\n", "hard_neg_ctx_indices", ".", "append", "(", "\n", "[", "\n", "i", "\n", "for", "i", "in", "range", "(", "\n", "current_ctxs_len", "+", "hard_negatives_start_idx", ",", "\n", "current_ctxs_len", "+", "hard_negatives_end_idx", ",", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "query_token", ":", "\n", "# TODO: tmp workaround for EL, remove or revise", "\n", "                ", "if", "query_token", "==", "\"[START_ENT]\"", ":", "\n", "                    ", "query_span", "=", "_select_span_with_token", "(", "\n", "question", ",", "tensorizer", ",", "token_str", "=", "query_token", "\n", ")", "\n", "question_tensors", ".", "append", "(", "query_span", ")", "\n", "", "else", ":", "\n", "                    ", "question_tensors", ".", "append", "(", "\n", "tensorizer", ".", "text_to_tensor", "(", "\" \"", ".", "join", "(", "[", "query_token", ",", "question", "]", ")", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "question_tensors", ".", "append", "(", "tensorizer", ".", "text_to_tensor", "(", "question", ")", ")", "\n", "\n", "", "img_ids", ".", "append", "(", "sample", ".", "img_id", ")", "\n", "\n", "\n", "", "ctxs_tensor", "=", "torch", ".", "cat", "(", "[", "ctx", ".", "view", "(", "1", ",", "-", "1", ")", "for", "ctx", "in", "ctx_tensors", "]", ",", "dim", "=", "0", ")", "\n", "questions_tensor", "=", "torch", ".", "cat", "(", "[", "q", ".", "view", "(", "1", ",", "-", "1", ")", "for", "q", "in", "question_tensors", "]", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "ctx_segments", "=", "torch", ".", "zeros_like", "(", "ctxs_tensor", ")", "\n", "question_segments", "=", "torch", ".", "zeros_like", "(", "questions_tensor", ")", "\n", "\n", "return", "BiEncoderBatch", "(", "\n", "questions_tensor", ",", "\n", "question_segments", ",", "\n", "ctxs_tensor", ",", "\n", "ctx_segments", ",", "\n", "positive_ctx_indices", ",", "\n", "hard_neg_ctx_indices", ",", "\n", "img_ids", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.load_state": [[479, 485], ["biencoder.BiEncoder.load_state_dict"], "methods", ["None"], ["", "def", "load_state", "(", "self", ",", "saved_state", ":", "CheckpointState", ")", ":", "\n", "# TODO: make a long term HF compatibility fix", "\n", "        ", "if", "\"question_model.embeddings.position_ids\"", "in", "saved_state", ".", "model_dict", ":", "\n", "            ", "del", "saved_state", ".", "model_dict", "[", "\"question_model.embeddings.position_ids\"", "]", "\n", "del", "saved_state", ".", "model_dict", "[", "\"ctx_model.embeddings.position_ids\"", "]", "\n", "", "self", ".", "load_state_dict", "(", "saved_state", ".", "model_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoder.get_state_dict": [[486, 488], ["biencoder.BiEncoder.state_dict"], "methods", ["None"], ["", "def", "get_state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoderNllLoss.calc": [[491, 528], ["biencoder.BiEncoderNllLoss.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.nll_loss", "torch.nll_loss", "torch.max", "torch.max", "torch.max", "torch.max", "len", "q_vectors.size", "scores.view.view.view", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.nll_loss.mul_", "q_vectors.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_scores"], ["    ", "def", "calc", "(", "\n", "self", ",", "\n", "q_vectors", ":", "T", ",", "\n", "ctx_vectors", ":", "T", ",", "\n", "positive_idx_per_question", ":", "list", ",", "\n", "hard_negative_idx_per_question", ":", "list", "=", "None", ",", "\n", "loss_scale", ":", "float", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Computes nll loss for the given lists of question and ctx vectors.\n        Note that although hard_negative_idx_per_question in not currently in use, one can use it for the\n        loss modifications. For example - weighted NLL with different factors for hard vs regular negatives.\n        :return: a tuple of loss value and amount of correct predictions per batch\n        \"\"\"", "\n", "scores", "=", "self", ".", "get_scores", "(", "q_vectors", ",", "ctx_vectors", ")", "\n", "\n", "if", "len", "(", "q_vectors", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "            ", "q_num", "=", "q_vectors", ".", "size", "(", "0", ")", "\n", "scores", "=", "scores", ".", "view", "(", "q_num", ",", "-", "1", ")", "\n", "\n", "", "softmax_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "\n", "loss", "=", "F", ".", "nll_loss", "(", "\n", "softmax_scores", ",", "\n", "torch", ".", "tensor", "(", "positive_idx_per_question", ")", ".", "to", "(", "softmax_scores", ".", "device", ")", ",", "\n", "reduction", "=", "\"mean\"", ",", "\n", ")", "\n", "\n", "max_score", ",", "max_idxs", "=", "torch", ".", "max", "(", "softmax_scores", ",", "1", ")", "\n", "correct_predictions_count", "=", "(", "\n", "max_idxs", "==", "torch", ".", "tensor", "(", "positive_idx_per_question", ")", ".", "to", "(", "max_idxs", ".", "device", ")", "\n", ")", ".", "sum", "(", ")", "\n", "\n", "if", "loss_scale", ":", "\n", "            ", "loss", ".", "mul_", "(", "loss_scale", ")", "\n", "\n", "", "return", "loss", ",", "correct_predictions_count", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoderNllLoss.get_scores": [[529, 533], ["biencoder.BiEncoderNllLoss.get_similarity_function", "biencoder.BiEncoderNllLoss.get_similarity_function"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function"], ["", "@", "staticmethod", "\n", "def", "get_scores", "(", "q_vector", ":", "T", ",", "ctx_vectors", ":", "T", ")", "->", "T", ":", "\n", "        ", "f", "=", "BiEncoderNllLoss", ".", "get_similarity_function", "(", ")", "\n", "return", "f", "(", "q_vector", ",", "ctx_vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.BiEncoderNllLoss.get_similarity_function": [[534, 537], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_similarity_function", "(", ")", ":", "\n", "        ", "return", "dot_product_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.dot_product_scores": [[47, 57], ["torch.matmul", "torch.matmul", "torch.transpose", "torch.transpose"], "function", ["None"], ["def", "dot_product_scores", "(", "q_vectors", ":", "T", ",", "ctx_vectors", ":", "T", ")", "->", "T", ":", "\n", "    ", "\"\"\"\n    calculates q->ctx scores for every row in ctx_vector\n    :param q_vector:\n    :param ctx_vector:\n    :return:\n    \"\"\"", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n", "r", "=", "torch", ".", "matmul", "(", "q_vectors", ",", "torch", ".", "transpose", "(", "ctx_vectors", ",", "0", ",", "1", ")", ")", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder.cosine_scores": [[59, 62], ["torch.cosine_similarity"], "function", ["None"], ["", "def", "cosine_scores", "(", "q_vector", ":", "T", ",", "ctx_vectors", ":", "T", ")", ":", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n", "    ", "return", "F", ".", "cosine_similarity", "(", "q_vector", ",", "ctx_vectors", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.biencoder._select_span_with_token": [[539, 579], ["tensorizer.get_token_id", "tensorizer.text_to_tensor", "tensorizer.text_to_tensor", "token_indexes.size", "token_indexes[].item", "int", "int", "_pad_to_len", "RuntimeError", "torch.cat", "torch.cat", "tensorizer.get_pad_id", "torch.tensor", "torch.tensor", "rnd.random"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_token_id", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pad_id"], ["", "", "def", "_select_span_with_token", "(", "\n", "text", ":", "str", ",", "tensorizer", ":", "Tensorizer", ",", "token_str", ":", "str", "=", "\"[START_ENT]\"", "\n", ")", "->", "T", ":", "\n", "    ", "id", "=", "tensorizer", ".", "get_token_id", "(", "token_str", ")", "\n", "query_tensor", "=", "tensorizer", ".", "text_to_tensor", "(", "text", ")", "\n", "\n", "if", "id", "not", "in", "query_tensor", ":", "\n", "        ", "query_tensor_full", "=", "tensorizer", ".", "text_to_tensor", "(", "text", ",", "apply_max_len", "=", "False", ")", "\n", "token_indexes", "=", "(", "query_tensor_full", "==", "id", ")", ".", "nonzero", "(", ")", "\n", "if", "token_indexes", ".", "size", "(", "0", ")", ">", "0", ":", "\n", "            ", "start_pos", "=", "token_indexes", "[", "0", ",", "0", "]", ".", "item", "(", ")", "\n", "# add some randomization to avoid overfitting to a specific token position", "\n", "\n", "left_shit", "=", "int", "(", "tensorizer", ".", "max_length", "/", "2", ")", "\n", "rnd_shift", "=", "int", "(", "(", "rnd", ".", "random", "(", ")", "-", "0.5", ")", "*", "left_shit", "/", "2", ")", "\n", "left_shit", "+=", "rnd_shift", "\n", "\n", "query_tensor", "=", "query_tensor_full", "[", "start_pos", "-", "left_shit", ":", "]", "\n", "cls_id", "=", "tensorizer", ".", "tokenizer", ".", "cls_token_id", "\n", "if", "query_tensor", "[", "0", "]", "!=", "cls_id", ":", "\n", "                ", "query_tensor", "=", "torch", ".", "cat", "(", "[", "torch", ".", "tensor", "(", "[", "cls_id", "]", ")", ",", "query_tensor", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "from", "dpr", ".", "models", ".", "reader", "import", "_pad_to_len", "\n", "\n", "query_tensor", "=", "_pad_to_len", "(", "\n", "query_tensor", ",", "tensorizer", ".", "get_pad_id", "(", ")", ",", "tensorizer", ".", "max_length", "\n", ")", "\n", "query_tensor", "[", "-", "1", "]", "=", "tensorizer", ".", "tokenizer", ".", "sep_token_id", "\n", "# logger.info('aligned query_tensor %s', query_tensor)", "\n", "\n", "assert", "id", "in", "query_tensor", ",", "\"query_tensor={}\"", ".", "format", "(", "query_tensor", ")", "\n", "return", "query_tensor", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"[START_ENT] toke not found for Entity Linking sample query={}\"", ".", "format", "(", "\n", "text", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "query_tensor", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFBertEncoder.__init__": [[210, 217], ["transformers.BertModel.__init__", "hf_models.HFBertEncoder.init_weights", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "project_dim", ":", "int", "=", "0", ")", ":", "\n", "        ", "BertModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "assert", "config", ".", "hidden_size", ">", "0", ",", "\"Encoder hidden_size can't be zero\"", "\n", "self", ".", "encode_proj", "=", "(", "\n", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "project_dim", ")", "if", "project_dim", "!=", "0", "else", "None", "\n", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFBertEncoder.init_encoder": [[218, 238], ["transformers.BertConfig.from_pretrained", "cls.from_pretrained", "hf_models.HFBertEncoder"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "@", "classmethod", "\n", "def", "init_encoder", "(", "\n", "cls", ",", "\n", "cfg_name", ":", "str", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "pretrained", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "->", "BertModel", ":", "\n", "        ", "cfg", "=", "BertConfig", ".", "from_pretrained", "(", "cfg_name", "if", "cfg_name", "else", "\"bert-base-uncased\"", ")", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "attention_probs_dropout_prob", "=", "dropout", "\n", "cfg", ".", "hidden_dropout_prob", "=", "dropout", "\n", "\n", "", "if", "pretrained", ":", "\n", "            ", "return", "cls", ".", "from_pretrained", "(", "\n", "cfg_name", ",", "config", "=", "cfg", ",", "project_dim", "=", "projection_dim", ",", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "HFBertEncoder", "(", "cfg", ",", "project_dim", "=", "projection_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFBertEncoder.forward": [[239, 282], ["isinstance", "super().forward", "super().forward", "sequence_output.size", "torch.stack", "hf_models.HFBertEncoder.encode_proj", "representation_token_pos.size", "representation_token_pos.size", "range"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", ":", "T", ",", "\n", "token_type_ids", ":", "T", ",", "\n", "attention_mask", ":", "T", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "if", "self", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "None", "\n", "outputs", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", ".", "last_hidden_state", "\n", "pooled_output", "=", "outputs", ".", "pooler_output", "\n", "\n", "\n", "", "if", "isinstance", "(", "representation_token_pos", ",", "int", ")", ":", "\n", "            ", "pooled_output", "=", "sequence_output", "[", ":", ",", "representation_token_pos", ",", ":", "]", "\n", "", "else", ":", "# treat as a tensor", "\n", "            ", "bsz", "=", "sequence_output", ".", "size", "(", "0", ")", "\n", "assert", "(", "\n", "representation_token_pos", ".", "size", "(", "0", ")", "==", "bsz", "\n", ")", ",", "\"query bsz={} while representation_token_pos bsz={}\"", ".", "format", "(", "\n", "bsz", ",", "representation_token_pos", ".", "size", "(", "0", ")", "\n", ")", "\n", "pooled_output", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "sequence_output", "[", "i", ",", "representation_token_pos", "[", "i", ",", "1", "]", ",", ":", "]", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", "\n", ")", "\n", "\n", "", "if", "self", ".", "encode_proj", ":", "\n", "            ", "pooled_output", "=", "self", ".", "encode_proj", "(", "pooled_output", ")", "\n", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFBertEncoder.get_out_size": [[283, 287], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFLXMERTEncoder.__init__": [[289, 296], ["transformers.LxmertModel.__init__", "hf_models.HFLXMERTEncoder.init_weights", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "project_dim", ":", "int", "=", "0", ")", ":", "\n", "        ", "LxmertModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "assert", "config", ".", "hidden_size", ">", "0", ",", "\"Encoder hidden_size can't be zero\"", "\n", "self", ".", "encode_proj", "=", "(", "\n", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "project_dim", ")", "if", "project_dim", "!=", "0", "else", "None", "\n", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFLXMERTEncoder.init_encoder": [[297, 317], ["transformers.LxmertConfig.from_pretrained", "cls.from_pretrained", "hf_models.HFLXMERTEncoder"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "@", "classmethod", "\n", "def", "init_encoder", "(", "\n", "cls", ",", "\n", "cfg_name", ":", "str", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "pretrained", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "->", "LxmertModel", ":", "\n", "        ", "cfg", "=", "LxmertConfig", ".", "from_pretrained", "(", "cfg_name", "if", "cfg_name", "else", "\"'unc-nlp/lxmert-base-uncased'\"", ")", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "attention_probs_dropout_prob", "=", "dropout", "\n", "cfg", ".", "hidden_dropout_prob", "=", "dropout", "\n", "\n", "", "if", "pretrained", ":", "\n", "            ", "return", "cls", ".", "from_pretrained", "(", "\n", "cfg_name", ",", "config", "=", "cfg", ",", "project_dim", "=", "projection_dim", ",", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "HFLXMERTEncoder", "(", "cfg", ",", "project_dim", "=", "projection_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFLXMERTEncoder.forward": [[318, 366], ["super().forward"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "visual_feats", "=", "None", ",", "\n", "visual_pos", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "visual_attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "outputs", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "visual_attention_mask", "=", "visual_attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "None", "\n", "\n", "sequence_output", ",", "pooled_output", "=", "outputs", ".", "language_output", ",", "outputs", ".", "pooled_output", "\n", "#         if isinstance(representation_token_pos, int):", "\n", "#             pooled_output = sequence_output[:, representation_token_pos, :]", "\n", "#         else:  # treat as a tensor", "\n", "#             bsz = sequence_output.size(0)", "\n", "#             assert (", "\n", "#                 representation_token_pos.size(0) == bsz", "\n", "#             ), \"query bsz={} while representation_token_pos bsz={}\".format(", "\n", "#                 bsz, representation_token_pos.size(0)", "\n", "#             )", "\n", "#             pooled_output = torch.stack(", "\n", "#                 [", "\n", "#                     sequence_output[i, representation_token_pos[i, 1], :]", "\n", "#                     for i in range(bsz)", "\n", "#                 ]", "\n", "#             )", "\n", "\n", "#         if self.encode_proj:", "\n", "#             pooled_output = self.encode_proj(pooled_output)", "\n", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.HFLXMERTEncoder.get_out_size": [[367, 371], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.__init__": [[374, 380], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "tokenizer", ":", "BertTokenizer", ",", "max_length", ":", "int", ",", "pad_to_max", ":", "bool", "=", "True", "\n", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "pad_to_max", "=", "pad_to_max", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.text_to_tensor": [[381, 420], ["text.strip.strip.strip", "torch.tensor", "hf_models.BertTensorizer.tokenizer.encode", "hf_models.BertTensorizer.tokenizer.encode", "len", "len", "len"], "methods", ["None"], ["", "def", "text_to_tensor", "(", "\n", "self", ",", "\n", "text", ":", "str", ",", "\n", "title", ":", "str", "=", "None", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "apply_max_len", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "text", "=", "text", ".", "strip", "(", ")", "\n", "# tokenizer automatic padding is explicitly disabled since its inconsistent behavior", "\n", "# TODO: move max len to methods params?", "\n", "\n", "if", "title", ":", "\n", "            ", "token_ids", "=", "self", ".", "tokenizer", ".", "encode", "(", "\n", "title", ",", "\n", "text_pair", "=", "text", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "self", ".", "max_length", "if", "apply_max_len", "else", "10000", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "truncation", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "token_ids", "=", "self", ".", "tokenizer", ".", "encode", "(", "\n", "text", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "self", ".", "max_length", "if", "apply_max_len", "else", "10000", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "truncation", "=", "True", ",", "\n", ")", "\n", "\n", "", "seq_len", "=", "self", ".", "max_length", "\n", "if", "self", ".", "pad_to_max", "and", "len", "(", "token_ids", ")", "<", "seq_len", ":", "\n", "            ", "token_ids", "=", "token_ids", "+", "[", "self", ".", "tokenizer", ".", "pad_token_id", "]", "*", "(", "\n", "seq_len", "-", "len", "(", "token_ids", ")", "\n", ")", "\n", "", "if", "len", "(", "token_ids", ")", ">=", "seq_len", ":", "\n", "            ", "token_ids", "=", "token_ids", "[", "0", ":", "seq_len", "]", "if", "apply_max_len", "else", "token_ids", "\n", "token_ids", "[", "-", "1", "]", "=", "self", ".", "tokenizer", ".", "sep_token_id", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "token_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.get_pair_separator_ids": [[421, 423], ["torch.tensor"], "methods", ["None"], ["", "def", "get_pair_separator_ids", "(", "self", ")", "->", "T", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.get_pad_id": [[424, 426], ["None"], "methods", ["None"], ["", "def", "get_pad_id", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "pad_token_id", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.get_attn_mask": [[427, 429], ["hf_models.BertTensorizer.get_pad_id"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pad_id"], ["", "def", "get_attn_mask", "(", "self", ",", "tokens_tensor", ":", "T", ")", "->", "T", ":", "\n", "        ", "return", "tokens_tensor", "!=", "self", ".", "get_pad_id", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.is_sub_word_id": [[430, 433], ["hf_models.BertTensorizer.tokenizer.convert_ids_to_tokens", "token.startswith", "token.startswith"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_ids_to_tokens"], ["", "def", "is_sub_word_id", "(", "self", ",", "token_id", ":", "int", ")", ":", "\n", "        ", "token", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "token_id", "]", ")", "[", "0", "]", "\n", "return", "token", ".", "startswith", "(", "\"##\"", ")", "or", "token", ".", "startswith", "(", "\" ##\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.to_string": [[434, 436], ["hf_models.BertTensorizer.tokenizer.decode"], "methods", ["None"], ["", "def", "to_string", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "True", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "decode", "(", "token_ids", ",", "skip_special_tokens", "=", "skip_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.set_pad_to_max": [[437, 439], ["None"], "methods", ["None"], ["", "def", "set_pad_to_max", "(", "self", ",", "do_pad", ":", "bool", ")", ":", "\n", "        ", "self", ".", "pad_to_max", "=", "do_pad", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.BertTensorizer.get_token_id": [[440, 442], ["None"], "methods", ["None"], ["", "def", "get_token_id", "(", "self", ",", "token", ":", "str", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "vocab", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.RobertaTensorizer.__init__": [[445, 448], ["hf_models.BertTensorizer.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "max_length", ":", "int", ",", "pad_to_max", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "RobertaTensorizer", ",", "self", ")", ".", "__init__", "(", "\n", "tokenizer", ",", "max_length", ",", "pad_to_max", "=", "pad_to_max", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_bert_biencoder_components": [[36, 73], ["kwargs.pop", "hf_models.HFLXMERTEncoder.init_encoder", "hf_models.HFBertEncoder.init_encoder", "dpr.model_lxmert.biencoder.BiEncoder", "hf_models.get_lxmert_tensorizer", "hasattr", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_tensorizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["def", "get_lxmert_bert_biencoder_components", "(", "cfg", ",", "inference_only", ":", "bool", "=", "False", ",", "load_image", ":", "bool", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "cfg", ".", "encoder", ".", "dropout", "if", "hasattr", "(", "cfg", ".", "encoder", ",", "\"dropout\"", ")", "else", "0.0", "\n", "load_images", "=", "kwargs", ".", "pop", "(", "\"load_images\"", ")", "\n", "question_encoder", "=", "HFLXMERTEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "question_pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "ctx_encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "ctx_pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "fix_ctx_encoder", "=", "cfg", ".", "fix_ctx_encoder", "if", "hasattr", "(", "cfg", ",", "\"fix_ctx_encoder\"", ")", "else", "False", "\n", "\n", "biencoder", "=", "BiEncoder", "(", "\n", "question_encoder", ",", "ctx_encoder", ",", "fix_ctx_encoder", "=", "fix_ctx_encoder", ",", "load_images", "=", "load_images", "\n", ")", "\n", "\n", "optimizer", "=", "(", "\n", "get_optimizer", "(", "\n", "biencoder", ",", "\n", "learning_rate", "=", "cfg", ".", "train", ".", "learning_rate", ",", "\n", "adam_eps", "=", "cfg", ".", "train", ".", "adam_eps", ",", "\n", "weight_decay", "=", "cfg", ".", "train", ".", "weight_decay", ",", "\n", ")", "\n", "if", "not", "inference_only", "\n", "else", "None", "\n", ")", "\n", "\n", "tensorizer", "=", "get_lxmert_tensorizer", "(", "cfg", ")", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_bert_reader_components": [[75, 101], ["hf_models.HFBertEncoder.init_encoder", "reader.Reader", "hf_models.get_bert_tensorizer", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tensorizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["", "def", "get_bert_reader_components", "(", "cfg", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "cfg", ".", "encoder", ".", "dropout", "if", "hasattr", "(", "cfg", ".", "encoder", ",", "\"dropout\"", ")", "else", "0.0", "\n", "encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "hidden_size", "=", "encoder", ".", "config", ".", "hidden_size", "\n", "reader", "=", "Reader", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "optimizer", "=", "(", "\n", "get_optimizer", "(", "\n", "reader", ",", "\n", "learning_rate", "=", "cfg", ".", "train", ".", "learning_rate", ",", "\n", "adam_eps", "=", "cfg", ".", "train", ".", "adam_eps", ",", "\n", "weight_decay", "=", "cfg", ".", "train", ".", "weight_decay", ",", "\n", ")", "\n", "if", "not", "inference_only", "\n", "else", "None", "\n", ")", "\n", "\n", "tensorizer", "=", "get_bert_tensorizer", "(", "cfg", ")", "\n", "return", "tensorizer", ",", "reader", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_bert_tensorizer": [[103, 115], ["hf_models.BertTensorizer", "hf_models.get_bert_tokenizer", "hf_models._add_special_tokens"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tokenizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models._add_special_tokens"], ["", "def", "get_bert_tensorizer", "(", "cfg", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "sequence_length", "=", "cfg", ".", "encoder", ".", "sequence_length", "\n", "pretrained_model_cfg", "=", "cfg", ".", "encoder", ".", "pretrained_model_cfg", "\n", "\n", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_bert_tokenizer", "(", "\n", "pretrained_model_cfg", ",", "do_lower_case", "=", "cfg", ".", "do_lower_case", "\n", ")", "\n", "if", "cfg", ".", "special_tokens", ":", "\n", "            ", "_add_special_tokens", "(", "tokenizer", ",", "cfg", ".", "special_tokens", ")", "\n", "\n", "", "", "return", "BertTensorizer", "(", "tokenizer", ",", "sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_tensorizer": [[116, 128], ["hf_models.BertTensorizer", "hf_models.get_lxmert_tokenizer", "hf_models._add_special_tokens"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_tokenizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models._add_special_tokens"], ["", "def", "get_lxmert_tensorizer", "(", "cfg", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "sequence_length", "=", "cfg", ".", "encoder", ".", "sequence_length", "\n", "pretrained_model_cfg", "=", "cfg", ".", "encoder", ".", "pretrained_model_cfg", "\n", "\n", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_lxmert_tokenizer", "(", "\n", "pretrained_model_cfg", ",", "do_lower_case", "=", "cfg", ".", "do_lower_case", "\n", ")", "\n", "if", "cfg", ".", "special_tokens", ":", "\n", "            ", "_add_special_tokens", "(", "tokenizer", ",", "cfg", ".", "special_tokens", ")", "\n", "\n", "", "", "return", "BertTensorizer", "(", "tokenizer", ",", "sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models._add_special_tokens": [[130, 151], ["logger.info", "len", "logger.info", "enumerate", "list", "logger.info", "logger.info", "range"], "function", ["None"], ["", "def", "_add_special_tokens", "(", "tokenizer", ",", "special_tokens", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Adding special tokens %s\"", ",", "special_tokens", ")", "\n", "special_tokens_num", "=", "len", "(", "special_tokens", ")", "\n", "# TODO: this is a hack-y logic that uses some private tokenizer structure which can be changed in HF code", "\n", "assert", "special_tokens_num", "<", "50", "\n", "unused_ids", "=", "[", "\n", "tokenizer", ".", "vocab", "[", "\"[unused{}]\"", ".", "format", "(", "i", ")", "]", "for", "i", "in", "range", "(", "special_tokens_num", ")", "\n", "]", "\n", "logger", ".", "info", "(", "\"Utilizing the following unused token ids %s\"", ",", "unused_ids", ")", "\n", "\n", "for", "idx", ",", "id", "in", "enumerate", "(", "unused_ids", ")", ":", "\n", "        ", "del", "tokenizer", ".", "vocab", "[", "\"[unused{}]\"", ".", "format", "(", "idx", ")", "]", "\n", "tokenizer", ".", "vocab", "[", "special_tokens", "[", "idx", "]", "]", "=", "id", "\n", "tokenizer", ".", "ids_to_tokens", "[", "id", "]", "=", "special_tokens", "[", "idx", "]", "\n", "\n", "", "tokenizer", ".", "_additional_special_tokens", "=", "list", "(", "special_tokens", ")", "\n", "logger", ".", "info", "(", "\n", "\"Added special tokenizer.additional_special_tokens %s\"", ",", "\n", "tokenizer", ".", "additional_special_tokens", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Tokenizer's all_special_tokens %s\"", ",", "tokenizer", ".", "all_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_roberta_tensorizer": [[153, 159], ["hf_models.RobertaTensorizer", "hf_models.get_roberta_tokenizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tokenizer"], ["", "def", "get_roberta_tensorizer", "(", "args", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_roberta_tokenizer", "(", "\n", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", "\n", ")", "\n", "", "return", "RobertaTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_optimizer": [[161, 189], ["transformers.optimization.AdamW", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "get_optimizer", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "learning_rate", ":", "float", "=", "1e-5", ",", "\n", "adam_eps", ":", "float", "=", "1e-8", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", ")", "->", "torch", ".", "optim", ".", "Optimizer", ":", "\n", "    ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "learning_rate", ",", "eps", "=", "adam_eps", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_bert_tokenizer": [[191, 194], ["transformers.BertTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "def", "get_bert_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "    ", "return", "BertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_tokenizer": [[196, 200], ["transformers.LxmertTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "def", "get_lxmert_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "\n", "    ", "return", "LxmertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_roberta_tokenizer": [[202, 206], ["RobertaTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "def", "get_roberta_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "# still uses HF code for tokenizer since they are the same", "\n", "    ", "return", "RobertaTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.__init__.init_hf_lxmert_bert_biencoder": [[14, 20], ["get_lxmert_bert_biencoder_components"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.hf_models.get_lxmert_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.__init__.init_comp": [[34, 39], ["RuntimeError"], "function", ["None"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.__init__.init_biencoder_components": [[41, 44], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.__init__.init_tenzorizer": [[50, 52], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader.Reader.__init__": [[32, 38], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "dpr.utils.model_utils.init_weights"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ":", "nn", ".", "Module", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "Reader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "2", ")", "\n", "self", ".", "qa_classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", "\n", "init_weights", "(", "[", "self", ".", "qa_outputs", ",", "self", ".", "qa_classifier", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader.Reader.forward": [[39, 49], ["input_ids.size", "reader.Reader._forward", "input_ids.view", "attention_mask.view", "reader.compute_loss", "start_logits.view", "end_logits.view", "relevance_logits.view"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.Reader._forward", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.compute_loss"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "attention_mask", ":", "T", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "answer_mask", "=", "None", ")", ":", "\n", "# notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length", "\n", "        ", "N", ",", "M", ",", "L", "=", "input_ids", ".", "size", "(", ")", "\n", "start_logits", ",", "end_logits", ",", "relevance_logits", "=", "self", ".", "_forward", "(", "input_ids", ".", "view", "(", "N", "*", "M", ",", "L", ")", ",", "\n", "attention_mask", ".", "view", "(", "N", "*", "M", ",", "L", ")", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "\n", "N", ",", "M", ")", "\n", "\n", "", "return", "start_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "end_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader.Reader._forward": [[50, 59], ["reader.Reader.encoder", "reader.Reader.qa_outputs", "reader.Reader.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "reader.Reader.qa_classifier"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "# TODO: provide segment values", "\n", "        ", "sequence_output", ",", "_pooled_output", ",", "_hidden_states", "=", "self", ".", "encoder", "(", "input_ids", ",", "None", ",", "attention_mask", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "rank_logits", "=", "self", ".", "qa_classifier", "(", "sequence_output", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "return", "start_logits", ",", "end_logits", ",", "rank_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader.compute_loss": [[61, 96], ["start_positions.view.view", "end_positions.view.view", "answer_mask.type().cuda.view", "start_logits.view.view", "end_logits.view.view", "relevance_logits.view.view", "answer_mask.type().cuda.type().cuda", "start_logits.view.size", "start_positions.view.clamp_", "end_positions.view.clamp_", "torch.nn.CrossEntropyLoss", "relevance_logits.view.view", "torch.zeros().cuda", "torch.zeros().cuda", "torch.sum", "torch.sum", "reader._calc_mml", "torch.nn.CrossEntropyLoss.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "loss_tensor.view().max", "answer_mask.type().cuda.type", "torch.zeros", "torch.zeros", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "t.unsqueeze", "t.unsqueeze", "loss_tensor.view"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._calc_mml"], ["", "", "def", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "N", ",", "M", ")", ":", "\n", "    ", "start_positions", "=", "start_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_positions", "=", "end_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "answer_mask", "=", "answer_mask", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "\n", "start_logits", "=", "start_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", "*", "M", ")", "\n", "\n", "answer_mask", "=", "answer_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "cuda", "(", ")", "\n", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "reduce", "=", "False", ",", "ignore_index", "=", "ignored_index", ")", "\n", "\n", "# compute switch loss", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "switch_labels", "=", "torch", ".", "zeros", "(", "N", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "switch_loss", "=", "torch", ".", "sum", "(", "loss_fct", "(", "relevance_logits", ",", "switch_labels", ")", ")", "\n", "\n", "# compute span loss", "\n", "start_losses", "=", "[", "(", "loss_fct", "(", "start_logits", ",", "_start_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_start_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "start_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "\n", "end_losses", "=", "[", "(", "loss_fct", "(", "end_logits", ",", "_end_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_end_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "end_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "loss_tensor", "=", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "start_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "end_losses", "]", ",", "dim", "=", "1", ")", "\n", "\n", "loss_tensor", "=", "loss_tensor", ".", "view", "(", "N", ",", "M", ",", "-", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "span_loss", "=", "_calc_mml", "(", "loss_tensor", ")", "\n", "return", "span_loss", "+", "switch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader.create_reader_input": [[98, 152], ["torch.Tensor().new_full", "torch.Tensor().new_full", "torch.cat", "torch.cat", "ReaderBatch", "reader._create_question_passages_tensors", "torch.cat.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.Tensor", "torch.Tensor", "logger.warning", "torch.stack.append", "torch.stack.append", "torch.stack.append", "ids.unsqueeze"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._create_question_passages_tensors"], ["", "def", "create_reader_input", "(", "pad_token_id", ":", "int", ",", "\n", "samples", ":", "List", "[", "ReaderSample", "]", ",", "\n", "passages_per_question", ":", "int", ",", "\n", "max_length", ":", "int", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "shuffle", ":", "bool", ",", "\n", ")", "->", "ReaderBatch", ":", "\n", "    ", "\"\"\"\n    Creates a reader batch instance out of a list of ReaderSample-s\n    :param pad_token_id: id of the padding token\n    :param samples: list of samples to create the batch for\n    :param passages_per_question: amount of passages for every question in a batch\n    :param max_length: max model input sequence length\n    :param max_n_answers: max num of answers per single question\n    :param is_train: if the samples are for a train set\n    :param shuffle: should passages selection be randomized\n    :return: ReaderBatch instance\n    \"\"\"", "\n", "input_ids", "=", "[", "]", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "answers_masks", "=", "[", "]", "\n", "empty_sequence", "=", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_length", ",", ")", ",", "pad_token_id", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "        ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "negative_ctxs", "=", "sample", ".", "negative_passages", "if", "is_train", "else", "sample", ".", "passages", "\n", "\n", "sample_tensors", "=", "_create_question_passages_tensors", "(", "positive_ctxs", ",", "\n", "negative_ctxs", ",", "\n", "passages_per_question", ",", "\n", "empty_sequence", ",", "\n", "max_n_answers", ",", "\n", "pad_token_id", ",", "\n", "is_train", ",", "\n", "is_random", "=", "shuffle", ")", "\n", "if", "not", "sample_tensors", ":", "\n", "            ", "logger", ".", "warning", "(", "'No valid passages combination for question=%s '", ",", "sample", ".", "question", ")", "\n", "continue", "\n", "", "sample_input_ids", ",", "starts_tensor", ",", "ends_tensor", ",", "answer_mask", "=", "sample_tensors", "\n", "input_ids", ".", "append", "(", "sample_input_ids", ")", "\n", "if", "is_train", ":", "\n", "            ", "start_positions", ".", "append", "(", "starts_tensor", ")", "\n", "end_positions", ".", "append", "(", "ends_tensor", ")", "\n", "answers_masks", ".", "append", "(", "answer_mask", ")", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "ids", ".", "unsqueeze", "(", "0", ")", "for", "ids", "in", "input_ids", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "is_train", ":", "\n", "        ", "start_positions", "=", "torch", ".", "stack", "(", "start_positions", ",", "dim", "=", "0", ")", "\n", "end_positions", "=", "torch", ".", "stack", "(", "end_positions", ",", "dim", "=", "0", ")", "\n", "answers_masks", "=", "torch", ".", "stack", "(", "answers_masks", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "ReaderBatch", "(", "input_ids", ",", "start_positions", ",", "end_positions", ",", "answers_masks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader._calc_mml": [[154, 159], ["torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.ones().cuda", "torch.ones().cuda", "torch.ones", "torch.ones", "loss_tensor.size"], "function", ["None"], ["", "def", "_calc_mml", "(", "loss_tensor", ")", ":", "\n", "    ", "marginal_likelihood", "=", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "\n", "-", "loss_tensor", "-", "1e10", "*", "(", "loss_tensor", "==", "0", ")", ".", "float", "(", ")", ")", ",", "1", ")", "\n", "return", "-", "torch", ".", "sum", "(", "torch", ".", "log", "(", "marginal_likelihood", "+", "\n", "torch", ".", "ones", "(", "loss_tensor", ".", "size", "(", "0", ")", ")", ".", "cuda", "(", ")", "*", "(", "marginal_likelihood", "==", "0", ")", ".", "float", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader._pad_to_len": [[161, 166], ["seq.size", "torch.cat", "torch.cat", "torch.Tensor().new_full", "torch.Tensor().new_full", "torch.Tensor", "torch.Tensor"], "function", ["None"], ["", "def", "_pad_to_len", "(", "seq", ":", "T", ",", "pad_id", ":", "int", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "s_len", "=", "seq", ".", "size", "(", "0", ")", "\n", "if", "s_len", ">", "max_len", ":", "\n", "        ", "return", "seq", "[", "0", ":", "max_len", "]", "\n", "", "return", "torch", ".", "cat", "(", "[", "seq", ",", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_len", "-", "s_len", ",", ")", ",", "pad_id", ",", "dtype", "=", "torch", ".", "long", ")", "]", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader._get_answer_spans": [[168, 171], ["None"], "function", ["None"], ["", "def", "_get_answer_spans", "(", "idx", ",", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "positive_a_spans", "=", "positives", "[", "idx", "]", ".", "answers_spans", "\n", "return", "[", "span", "for", "span", "in", "positive_a_spans", "if", "(", "span", "[", "0", "]", "<", "max_len", "and", "span", "[", "1", "]", "<", "max_len", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader._get_positive_idx": [[173, 182], ["numpy.random.choice", "reader._get_answer_spans", "next", "len", "range", "reader._get_answer_spans", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans"], ["", "def", "_get_positive_idx", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ",", "is_random", ":", "bool", ")", ":", "\n", "# select just one positive", "\n", "    ", "positive_idx", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "positives", ")", ")", "if", "is_random", "else", "0", "\n", "\n", "if", "not", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", ":", "\n", "# question may be too long, find the first positive with at least one valid span", "\n", "        ", "positive_idx", "=", "next", "(", "(", "i", "for", "i", "in", "range", "(", "len", "(", "positives", ")", ")", "if", "_get_answer_spans", "(", "i", ",", "positives", ",", "max_len", ")", ")", ",", "\n", "None", ")", "\n", "", "return", "positive_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.model_lxmert.reader._create_question_passages_tensors": [[184, 237], ["empty_ids.size", "len", "torch.stack", "torch.stack", "reader._get_positive_idx", "all", "all", "reader._pad_to_len", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.tensor", "torch.tensor", "numpy.random.permutation", "range", "reader._pad_to_len", "len", "negatives_selected.append", "reader._get_answer_spans", "range", "empty_ids.clone", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len", "len", "len", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_positive_idx", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans"], ["", "def", "_create_question_passages_tensors", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "negatives", ":", "List", "[", "ReaderPassage", "]", ",", "total_size", ":", "int", ",", "\n", "empty_ids", ":", "T", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "is_random", ":", "bool", "=", "True", ")", ":", "\n", "    ", "max_len", "=", "empty_ids", ".", "size", "(", "0", ")", "\n", "if", "is_train", ":", "\n", "# select just one positive", "\n", "        ", "positive_idx", "=", "_get_positive_idx", "(", "positives", ",", "max_len", ",", "is_random", ")", "\n", "if", "positive_idx", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "positive_a_spans", "=", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", "[", "0", ":", "max_n_answers", "]", "\n", "\n", "answer_starts", "=", "[", "span", "[", "0", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "answer_ends", "=", "[", "span", "[", "1", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "\n", "assert", "all", "(", "s", "<", "max_len", "for", "s", "in", "answer_starts", ")", "\n", "assert", "all", "(", "e", "<", "max_len", "for", "e", "in", "answer_ends", ")", "\n", "\n", "positive_input_ids", "=", "_pad_to_len", "(", "positives", "[", "positive_idx", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "\n", "\n", "answer_starts_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_starts_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "answer_starts", ")", "\n", "\n", "answer_ends_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_ends_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_ends", ")", "]", "=", "torch", ".", "tensor", "(", "answer_ends", ")", "\n", "\n", "answer_mask", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "answer_mask", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "[", "1", "for", "_", "in", "range", "(", "len", "(", "answer_starts", ")", ")", "]", ")", "\n", "\n", "positives_selected", "=", "[", "positive_input_ids", "]", "\n", "\n", "", "else", ":", "\n", "        ", "positives_selected", "=", "[", "]", "\n", "answer_starts_tensor", "=", "None", "\n", "answer_ends_tensor", "=", "None", "\n", "answer_mask", "=", "None", "\n", "\n", "", "positives_num", "=", "len", "(", "positives_selected", ")", "\n", "negative_idxs", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "negatives", ")", ")", ")", "if", "is_random", "else", "range", "(", "\n", "len", "(", "negatives", ")", "-", "positives_num", ")", "\n", "\n", "negative_idxs", "=", "negative_idxs", "[", ":", "total_size", "-", "positives_num", "]", "\n", "\n", "negatives_selected", "=", "[", "_pad_to_len", "(", "negatives", "[", "i", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "for", "i", "in", "negative_idxs", "]", "\n", "\n", "while", "len", "(", "negatives_selected", ")", "<", "total_size", "-", "positives_num", ":", "\n", "        ", "negatives_selected", ".", "append", "(", "empty_ids", ".", "clone", "(", ")", ")", "\n", "\n", "", "input_ids", "=", "torch", ".", "stack", "(", "[", "t", "for", "t", "in", "positives_selected", "+", "negatives_selected", "]", ",", "dim", "=", "0", ")", "\n", "return", "input_ids", ",", "answer_starts_tensor", ",", "answer_ends_tensor", ",", "answer_mask", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.RobertaEncoder.__init__": [[45, 48], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fairseq_roberta_hub", ":", "RobertaHubInterface", ")", ":", "\n", "        ", "super", "(", "RobertaEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fairseq_roberta", "=", "fairseq_roberta_hub", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.RobertaEncoder.from_pretrained": [[49, 53], ["fairseq.models.roberta.model.RobertaModel.from_pretrained", "cls"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_dir_path", ":", "str", ")", ":", "\n", "        ", "model", "=", "FaiseqRobertaModel", ".", "from_pretrained", "(", "pretrained_dir_path", ")", "\n", "return", "cls", "(", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.RobertaEncoder.forward": [[54, 58], ["fairseq_models.RobertaEncoder.fairseq_roberta.extract_features"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "token_type_ids", ":", "T", ",", "attention_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "roberta_out", "=", "self", ".", "fairseq_roberta", ".", "extract_features", "(", "input_ids", ")", "\n", "cls_out", "=", "roberta_out", "[", ":", ",", "0", ",", ":", "]", "\n", "return", "roberta_out", ",", "cls_out", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.RobertaEncoder.get_out_size": [[59, 61], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.get_roberta_biencoder_components": [[27, 36], ["fairseq_models.RobertaEncoder.from_pretrained", "fairseq_models.RobertaEncoder.from_pretrained", "biencoder.BiEncoder", "dpr.models.hf_models.get_roberta_tensorizer", "fairseq_models.get_fairseq_adamw_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tensorizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.get_fairseq_adamw_optimizer"], ["def", "get_roberta_biencoder_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "question_encoder", "=", "RobertaEncoder", ".", "from_pretrained", "(", "args", ".", "pretrained_file", ")", "\n", "ctx_encoder", "=", "RobertaEncoder", ".", "from_pretrained", "(", "args", ".", "pretrained_file", ")", "\n", "biencoder", "=", "BiEncoder", "(", "question_encoder", ",", "ctx_encoder", ")", "\n", "optimizer", "=", "get_fairseq_adamw_optimizer", "(", "biencoder", ",", "args", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "get_roberta_tensorizer", "(", "args", ")", "\n", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.get_fairseq_adamw_optimizer": [[38, 41], ["setattr", "fairseq.optim.adam.FairseqAdam", "model.parameters"], "function", ["None"], ["", "def", "get_fairseq_adamw_optimizer", "(", "model", ":", "nn", ".", "Module", ",", "args", ")", ":", "\n", "    ", "setattr", "(", "args", ",", "'lr'", ",", "[", "args", ".", "learning_rate", "]", ")", "\n", "return", "FairseqAdam", "(", "args", ",", "model", ".", "parameters", "(", ")", ")", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.__init__": [[65, 77], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "\"\"\"Bi-Encoder model component. Encapsulates query/question and context/passage encoders.\"\"\"", "\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "question_model", ":", "nn", ".", "Module", ",", "\n", "ctx_model", ":", "nn", ".", "Module", ",", "\n", "fix_q_encoder", ":", "bool", "=", "False", ",", "\n", "fix_ctx_encoder", ":", "bool", "=", "False", ",", "\n", "load_images", ":", "bool", "=", "True", "\n", ")", ":", "\n", "        ", "super", "(", "BiEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "question_model", "=", "question_model", "\n", "self", ".", "ctx_model", "=", "ctx_model", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation": [[78, 112], ["sub_model", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sub_model", "sequence_output.requires_grad_", "pooled_output.requires_grad_"], "methods", ["None"], ["self", ".", "fix_q_encoder", "=", "fix_q_encoder", "\n", "self", ".", "fix_ctx_encoder", "=", "fix_ctx_encoder", "\n", "if", "load_images", ":", "\n", "            ", "self", ".", "load_images", "(", ")", "\n", "\n", "", "", "def", "load_images", "(", "self", ",", "dirs", "=", "[", "\"/scratch/yzeng55/okvqa/img_feat/clean_train/\"", ",", "\"/scratch/yzeng55/okvqa/img_feat/clean_val/\"", "]", ",", "dummy", "=", "False", ")", ":", "\n", "        ", "visual_feats", "=", "{", "}", "\n", "visual_pos", "=", "{", "}", "\n", "print", "(", "\"start to load image features\"", ")", "\n", "for", "dir", "in", "dirs", ":", "\n", "            ", "files", "=", "os", ".", "listdir", "(", "dir", ")", "\n", "if", "dummy", ":", "\n", "                ", "files", "=", "files", "[", ":", "10", "]", "\n", "", "for", "file", "in", "files", ":", "\n", "\n", "                ", "d", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "file", ")", ",", "allow_pickle", "=", "True", ")", "\n", "d", "=", "d", ".", "tolist", "(", ")", "\n", "imgid", "=", "d", "[", "'image_id'", "]", "\n", "img_h", "=", "float", "(", "d", "[", "'image_height'", "]", ")", "\n", "img_w", "=", "float", "(", "d", "[", "'image_width'", "]", ")", "\n", "feats", "=", "torch", ".", "tensor", "(", "d", "[", "'features'", "]", ")", "\n", "visual_feats", "[", "imgid", "]", "=", "feats", ".", "unsqueeze", "(", "0", ")", "\n", "boxes", "=", "d", "[", "'bbox'", "]", "\n", "boxes", "[", ":", ",", "(", "0", ",", "2", ")", "]", "/=", "img_w", "\n", "boxes", "[", ":", ",", "(", "1", ",", "3", ")", "]", "/=", "img_h", "\n", "for", "bd_box", "in", "boxes", ":", "\n", "                    ", "if", "bd_box", "[", "0", "]", "<", "-", "0.0001", ":", "\n", "                        ", "bd_box", "[", "0", "]", "=", "0", "\n", "", "if", "bd_box", "[", "0", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "0", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "2", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "2", "]", "=", "1", "\n", "", "if", "bd_box", "[", "1", "]", "<", "-", "0.0001", ":", "\n", "                        ", "bd_box", "[", "1", "]", "=", "0", "\n", "", "if", "bd_box", "[", "1", "]", ">", "1", ":", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.forward": [[113, 148], ["biencoder.BiEncoder.get_representation", "biencoder.BiEncoder.get_representation"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_representation"], ["                        ", "bd_box", "[", "1", "]", "=", "0.99", "\n", "", "if", "bd_box", "[", "3", "]", ">", "1", ":", "\n", "                        ", "bd_box", "[", "3", "]", "=", "1", "\n", "", "", "boxes", "=", "torch", ".", "tensor", "(", "boxes", ")", ".", "unsqueeze", "(", "0", ")", "\n", "visual_pos", "[", "imgid", "]", "=", "boxes", "\n", "\n", "\n", "", "", "self", ".", "visual_feats", "=", "visual_feats", "\n", "self", ".", "visual_pos", "=", "visual_pos", "\n", "print", "(", "f\"loaded {len(self.visual_feats)} image features from {dirs}\"", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "get_representation", "(", "\n", "sub_model", ":", "nn", ".", "Module", ",", "\n", "ids", ":", "T", ",", "\n", "segments", ":", "T", ",", "\n", "attn_mask", ":", "T", ",", "\n", "fix_encoder", ":", "bool", "=", "False", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", ")", "->", "(", "T", ",", "T", ",", "T", ")", ":", "\n", "        ", "sequence_output", "=", "None", "\n", "pooled_output", "=", "None", "\n", "hidden_states", "=", "None", "\n", "if", "ids", "is", "not", "None", ":", "\n", "            ", "if", "fix_encoder", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "representation_token_pos", "=", "representation_token_pos", ",", "\n", ")", "\n", "\n", "", "if", "sub_model", ".", "training", ":", "\n", "                    ", "sequence_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "pooled_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.create_biencoder_input": [[150, 242], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "BiEncoderBatch", "len", "ctx_tensors.extend", "positive_ctx_indices.append", "hard_neg_ctx_indices.append", "question_tensors.append", "random.shuffle", "random.shuffle", "len", "tensorizer.text_to_tensor", "tensorizer.text_to_tensor", "ctx.view", "q.view", "len", "numpy.random.choice", "range", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["                ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "representation_token_pos", "=", "representation_token_pos", ",", "\n", ")", "\n", "\n", "", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n", "", "@", "staticmethod", "\n", "def", "get_visual_representation", "(", "\n", "sub_model", ":", "nn", ".", "Module", ",", "\n", "ids", ":", "T", ",", "\n", "segments", ":", "T", ",", "\n", "attn_mask", ":", "T", ",", "\n", "fix_encoder", ":", "bool", "=", "False", ",", "\n", "visual_feats", "=", "None", ",", "\n", "visual_pos", "=", "None", ",", "\n", ")", "->", "(", "T", ",", "T", ",", "T", ")", ":", "\n", "        ", "sequence_output", "=", "None", "\n", "pooled_output", "=", "None", "\n", "hidden_states", "=", "None", "\n", "if", "ids", "is", "not", "None", ":", "\n", "            ", "if", "fix_encoder", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "ids", ",", "\n", "segments", ",", "\n", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", ")", "\n", "\n", "", "if", "sub_model", ".", "training", ":", "\n", "                    ", "sequence_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "pooled_output", ".", "requires_grad_", "(", "requires_grad", "=", "True", ")", "\n", "", "", "else", ":", "\n", "\n", "                ", "try", ":", "\n", "                    ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "sub_model", "(", "\n", "input_ids", "=", "ids", ",", "\n", "token_type_ids", "=", "segments", ",", "\n", "attention_mask", "=", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "return_dict", "=", "True", "\n", ")", "\n", "", "except", ":", "\n", "                    ", "outputs", "=", "sub_model", "(", "\n", "input_ids", "=", "ids", ",", "\n", "token_type_ids", "=", "segments", ",", "\n", "attention_mask", "=", "attn_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "return_dict", "=", "True", "\n", ")", "\n", "print", "(", "\"visual outputs\"", ",", "outputs", ")", "\n", "\n", "\n", "", "", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n", "", "def", "forward", "(", "\n", "self", ",", "\n", "question_ids", ":", "T", ",", "\n", "question_segments", ":", "T", ",", "\n", "question_attn_mask", ":", "T", ",", "\n", "context_ids", ":", "T", ",", "\n", "ctx_segments", ":", "T", ",", "\n", "ctx_attn_mask", ":", "T", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", "img_ids", "=", "None", "\n", ")", "->", "Tuple", "[", "T", ",", "T", "]", ":", "\n", "        ", "visual_feats", "=", "[", "]", "\n", "visual_pos", "=", "[", "]", "\n", "for", "ids", "in", "img_ids", ":", "\n", "#             try:", "\n", "            ", "visual_feats", ".", "append", "(", "self", ".", "visual_feats", "[", "ids", "]", ")", "\n", "visual_pos", ".", "append", "(", "self", ".", "visual_pos", "[", "ids", "]", ")", "\n", "#             except:", "\n", "#                 visual_feats.append(torch.rand([1,36,2048]))", "\n", "#                 visual_pos.append(torch.rand([1,36,4]))", "\n", "\n", "\n", "", "visual_feats", "=", "torch", ".", "cat", "(", "visual_feats", ",", "dim", "=", "0", ")", "\n", "visual_pos", "=", "torch", ".", "cat", "(", "visual_pos", ",", "dim", "=", "0", ")", "\n", "device", "=", "question_ids", ".", "device", "\n", "visual_feats", "=", "visual_feats", ".", "to", "(", "device", ")", "\n", "visual_pos", "=", "visual_pos", ".", "to", "(", "device", ")", "\n", "\n", "q_encoder", "=", "(", "\n", "self", ".", "question_model", "\n", "if", "encoder_type", "is", "None", "or", "encoder_type", "==", "\"question\"", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.create_biencoder_input2": [[244, 351], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "BiEncoderBatch", "len", "ctx_tensors.extend", "positive_ctx_indices.append", "hard_neg_ctx_indices.append", "random.shuffle", "random.shuffle", "len", "tensorizer.text_to_tensor", "question_tensors.append", "ctx.view", "q.view", "len", "biencoder._select_span_with_token", "question_tensors.append", "question_tensors.append", "tensorizer.text_to_tensor", "numpy.random.choice", "range", "tensorizer.text_to_tensor", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder._select_span_with_token", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], [")", "\n", "_q_seq", ",", "q_pooled_out", ",", "_q_hidden", "=", "self", ".", "get_visual_representation", "(", "\n", "q_encoder", ",", "\n", "question_ids", ",", "\n", "question_segments", ",", "\n", "question_attn_mask", ",", "\n", "self", ".", "fix_q_encoder", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", "\n", ")", "\n", "\n", "ctx_encoder", "=", "(", "\n", "self", ".", "ctx_model", "\n", "if", "encoder_type", "is", "None", "or", "encoder_type", "==", "\"ctx\"", "\n", "else", "self", ".", "question_model", "\n", ")", "\n", "_ctx_seq", ",", "ctx_pooled_out", ",", "_ctx_hidden", "=", "self", ".", "get_representation", "(", "\n", "ctx_encoder", ",", "context_ids", ",", "ctx_segments", ",", "ctx_attn_mask", ",", "self", ".", "fix_ctx_encoder", "\n", ")", "\n", "\n", "return", "q_pooled_out", ",", "ctx_pooled_out", "\n", "\n", "# TODO delete once moved to the new method", "\n", "", "@", "classmethod", "\n", "def", "create_biencoder_input", "(", "\n", "cls", ",", "\n", "samples", ":", "List", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", ",", "\n", "num_hard_negatives", ":", "int", "=", "0", ",", "\n", "num_other_negatives", ":", "int", "=", "0", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "hard_neg_fallback", ":", "bool", "=", "True", ",", "\n", ")", "->", "BiEncoderBatch", ":", "\n", "        ", "\"\"\"\n        Creates a batch of the biencoder training tuple.\n        :param samples: list of data items (from json) to create the batch for\n        :param tensorizer: components to create model input tensors from a text sequence\n        :param insert_title: enables title insertion at the beginning of the context sequences\n        :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n        :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n        :param shuffle: shuffles negative passages pools\n        :param shuffle_positives: shuffles positive passages pools\n        :return: BiEncoderBatch tuple\n        \"\"\"", "\n", "question_tensors", "=", "[", "]", "\n", "ctx_tensors", "=", "[", "]", "\n", "positive_ctx_indices", "=", "[", "]", "\n", "hard_neg_ctx_indices", "=", "[", "]", "\n", "visual_feats", "=", "[", "]", "\n", "visual_pos", "=", "[", "]", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "# ctx+ & [ctx-] composition", "\n", "# as of now, take the first(gold) ctx+ only", "\n", "            ", "if", "shuffle", "and", "shuffle_positives", ":", "\n", "                ", "positive_ctxs", "=", "sample", "[", "\"positive_ctxs\"", "]", "\n", "positive_ctx", "=", "positive_ctxs", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "positive_ctxs", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "positive_ctx", "=", "sample", "[", "\"positive_ctxs\"", "]", "[", "0", "]", "\n", "\n", "", "neg_ctxs", "=", "sample", "[", "\"negative_ctxs\"", "]", "\n", "hard_neg_ctxs", "=", "sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "neg_ctxs", ")", "\n", "random", ".", "shuffle", "(", "hard_neg_ctxs", ")", "\n", "\n", "", "if", "hard_neg_fallback", "and", "len", "(", "hard_neg_ctxs", ")", "==", "0", ":", "\n", "                ", "hard_neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "", "neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_other_negatives", "]", "\n", "hard_neg_ctxs", "=", "hard_neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "all_ctxs", "=", "[", "positive_ctx", "]", "+", "neg_ctxs", "+", "hard_neg_ctxs", "\n", "hard_negatives_start_idx", "=", "1", "\n", "hard_negatives_end_idx", "=", "1", "+", "len", "(", "hard_neg_ctxs", ")", "\n", "\n", "current_ctxs_len", "=", "len", "(", "ctx_tensors", ")", "\n", "\n", "sample_ctxs_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", "[", "\"text\"", "]", ",", "\n", "title", "=", "ctx", "[", "\"title\"", "]", "if", "(", "insert_title", "and", "\"title\"", "in", "ctx", ")", "else", "None", ",", "\n", ")", "\n", "for", "ctx", "in", "all_ctxs", "\n", "]", "\n", "\n", "ctx_tensors", ".", "extend", "(", "sample_ctxs_tensors", ")", "\n", "positive_ctx_indices", ".", "append", "(", "current_ctxs_len", ")", "\n", "hard_neg_ctx_indices", ".", "append", "(", "\n", "[", "\n", "i", "\n", "for", "i", "in", "range", "(", "\n", "current_ctxs_len", "+", "hard_negatives_start_idx", ",", "\n", "current_ctxs_len", "+", "hard_negatives_end_idx", ",", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "question_tensors", ".", "append", "(", "tensorizer", ".", "text_to_tensor", "(", "question", ")", ")", "\n", "\n", "\n", "", "ctxs_tensor", "=", "torch", ".", "cat", "(", "[", "ctx", ".", "view", "(", "1", ",", "-", "1", ")", "for", "ctx", "in", "ctx_tensors", "]", ",", "dim", "=", "0", ")", "\n", "questions_tensor", "=", "torch", ".", "cat", "(", "[", "q", ".", "view", "(", "1", ",", "-", "1", ")", "for", "q", "in", "question_tensors", "]", ",", "dim", "=", "0", ")", "\n", "\n", "ctx_segments", "=", "torch", ".", "zeros_like", "(", "ctxs_tensor", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.load_state": [[353, 359], ["biencoder.BiEncoder.load_state_dict"], "methods", ["None"], ["\n", "return", "BiEncoderBatch", "(", "\n", "questions_tensor", ",", "\n", "question_segments", ",", "\n", "ctxs_tensor", ",", "\n", "ctx_segments", ",", "\n", "positive_ctx_indices", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoder.get_state_dict": [[360, 362], ["biencoder.BiEncoder.state_dict"], "methods", ["None"], ["hard_neg_ctx_indices", ",", "\n", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.calc": [[365, 402], ["biencoder.BiEncoderNllLoss.get_scores", "torch.log_softmax", "torch.log_softmax", "torch.nll_loss", "torch.nll_loss", "torch.max", "torch.max", "torch.max", "torch.max", "len", "q_vectors.size", "scores.view.view.view", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.nll_loss.mul_", "q_vectors.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_scores"], ["def", "create_biencoder_input2", "(", "\n", "cls", ",", "\n", "samples", ":", "List", "[", "VisBiEncoderSample", "]", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "insert_title", ":", "bool", ",", "\n", "num_hard_negatives", ":", "int", "=", "0", ",", "\n", "num_other_negatives", ":", "int", "=", "0", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "hard_neg_fallback", ":", "bool", "=", "True", ",", "\n", "query_token", ":", "str", "=", "None", ",", "\n", ")", "->", "BiEncoderBatch", ":", "\n", "        ", "\"\"\"\n        Creates a batch of the biencoder training tuple.\n        :param samples: list of BiEncoderSample-s to create the batch for\n        :param tensorizer: components to create model input tensors from a text sequence\n        :param insert_title: enables title insertion at the beginning of the context sequences\n        :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n        :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n        :param shuffle: shuffles negative passages pools\n        :param shuffle_positives: shuffles positive passages pools\n        :return: BiEncoderBatch tuple\n        \"\"\"", "\n", "question_tensors", "=", "[", "]", "\n", "ctx_tensors", "=", "[", "]", "\n", "positive_ctx_indices", "=", "[", "]", "\n", "hard_neg_ctx_indices", "=", "[", "]", "\n", "img_ids", "=", "[", "]", "\n", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "# ctx+ & [ctx-] composition", "\n", "# as of now, take the first(gold) ctx+ only", "\n", "\n", "            ", "if", "shuffle", "and", "shuffle_positives", ":", "\n", "                ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "positive_ctx", "=", "positive_ctxs", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "positive_ctxs", ")", ")", "]", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_scores": [[403, 407], ["biencoder.BiEncoderNllLoss.get_similarity_function", "biencoder.BiEncoderNllLoss.get_similarity_function"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function"], ["                ", "positive_ctx", "=", "sample", ".", "positive_passages", "[", "0", "]", "\n", "\n", "", "neg_ctxs", "=", "sample", ".", "negative_passages", "\n", "hard_neg_ctxs", "=", "sample", ".", "hard_negative_passages", "\n", "question", "=", "sample", ".", "query", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.BiEncoderNllLoss.get_similarity_function": [[408, 411], ["None"], "methods", ["None"], ["# question = normalize_question(sample.query)", "\n", "\n", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "neg_ctxs", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.dot_product_scores": [[45, 55], ["torch.matmul", "torch.matmul", "torch.transpose", "torch.transpose"], "function", ["None"], ["\n", "\n", "def", "dot_product_scores", "(", "q_vectors", ":", "T", ",", "ctx_vectors", ":", "T", ")", "->", "T", ":", "\n", "    ", "\"\"\"\n    calculates q->ctx scores for every row in ctx_vector\n    :param q_vector:\n    :param ctx_vector:\n    :return:\n    \"\"\"", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n", "r", "=", "torch", ".", "matmul", "(", "q_vectors", ",", "torch", ".", "transpose", "(", "ctx_vectors", ",", "0", ",", "1", ")", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder.cosine_scores": [[57, 60], ["torch.cosine_similarity"], "function", ["None"], ["\n", "\n", "", "def", "cosine_scores", "(", "q_vector", ":", "T", ",", "ctx_vectors", ":", "T", ")", ":", "\n", "# q_vector: n1 x D, ctx_vectors: n2 x D, result n1 x n2", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.biencoder._select_span_with_token": [[413, 453], ["tensorizer.get_token_id", "tensorizer.text_to_tensor", "tensorizer.text_to_tensor", "token_indexes.size", "token_indexes[].item", "int", "int", "_pad_to_len", "RuntimeError", "torch.cat", "torch.cat", "tensorizer.get_pad_id", "torch.tensor", "torch.tensor", "rnd.random"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_token_id", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pad_id"], ["\n", "", "if", "hard_neg_fallback", "and", "len", "(", "hard_neg_ctxs", ")", "==", "0", ":", "\n", "                ", "hard_neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "", "neg_ctxs", "=", "neg_ctxs", "[", "0", ":", "num_other_negatives", "]", "\n", "hard_neg_ctxs", "=", "hard_neg_ctxs", "[", "0", ":", "num_hard_negatives", "]", "\n", "\n", "all_ctxs", "=", "[", "positive_ctx", "]", "+", "neg_ctxs", "+", "hard_neg_ctxs", "\n", "hard_negatives_start_idx", "=", "1", "\n", "hard_negatives_end_idx", "=", "1", "+", "len", "(", "hard_neg_ctxs", ")", "\n", "\n", "current_ctxs_len", "=", "len", "(", "ctx_tensors", ")", "\n", "\n", "sample_ctxs_tensors", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", ".", "text", ",", "title", "=", "ctx", ".", "title", "if", "(", "insert_title", "and", "ctx", ".", "title", ")", "else", "None", "\n", ")", "\n", "for", "ctx", "in", "all_ctxs", "\n", "]", "\n", "\n", "ctx_tensors", ".", "extend", "(", "sample_ctxs_tensors", ")", "\n", "positive_ctx_indices", ".", "append", "(", "current_ctxs_len", ")", "\n", "hard_neg_ctx_indices", ".", "append", "(", "\n", "[", "\n", "i", "\n", "for", "i", "in", "range", "(", "\n", "current_ctxs_len", "+", "hard_negatives_start_idx", ",", "\n", "current_ctxs_len", "+", "hard_negatives_end_idx", ",", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "query_token", ":", "\n", "# TODO: tmp workaround for EL, remove or revise", "\n", "                ", "if", "query_token", "==", "\"[START_ENT]\"", ":", "\n", "                    ", "query_span", "=", "_select_span_with_token", "(", "\n", "question", ",", "tensorizer", ",", "token_str", "=", "query_token", "\n", ")", "\n", "question_tensors", ".", "append", "(", "query_span", ")", "\n", "", "else", ":", "\n", "                    ", "question_tensors", ".", "append", "(", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.HFBertEncoder.__init__": [[188, 195], ["transformers.BertModel.__init__", "hf_models.HFBertEncoder.init_weights", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights"], ["return", "optimizer", "\n", "\n", "\n", "", "def", "get_bert_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "    ", "return", "BertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.HFBertEncoder.init_encoder": [[196, 216], ["transformers.BertConfig.from_pretrained", "cls.from_pretrained", "hf_models.HFBertEncoder"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["", "def", "get_lxmert_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "\n", "    ", "return", "LxmertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n", "\n", "", "def", "get_roberta_tokenizer", "(", "pretrained_cfg_name", ":", "str", ",", "do_lower_case", ":", "bool", "=", "True", ")", ":", "\n", "# still uses HF code for tokenizer since they are the same", "\n", "    ", "return", "RobertaTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_cfg_name", ",", "do_lower_case", "=", "do_lower_case", "\n", ")", "\n", "\n", "\n", "", "class", "HFBertEncoder", "(", "BertModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ",", "project_dim", ":", "int", "=", "0", ")", ":", "\n", "        ", "BertModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "assert", "config", ".", "hidden_size", ">", "0", ",", "\"Encoder hidden_size can't be zero\"", "\n", "self", ".", "encode_proj", "=", "(", "\n", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "project_dim", ")", "if", "project_dim", "!=", "0", "else", "None", "\n", ")", "\n", "self", ".", "init_weights", "(", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.HFBertEncoder.forward": [[217, 262], ["isinstance", "super().forward", "super().forward", "sequence_output.size", "torch.stack", "hf_models.HFBertEncoder.encode_proj", "representation_token_pos.size", "representation_token_pos.size", "range"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward"], ["\n", "", "@", "classmethod", "\n", "def", "init_encoder", "(", "\n", "cls", ",", "\n", "cfg_name", ":", "str", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "pretrained", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "->", "BertModel", ":", "\n", "        ", "cfg", "=", "BertConfig", ".", "from_pretrained", "(", "cfg_name", "if", "cfg_name", "else", "\"bert-base-uncased\"", ")", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "attention_probs_dropout_prob", "=", "dropout", "\n", "cfg", ".", "hidden_dropout_prob", "=", "dropout", "\n", "\n", "", "if", "pretrained", ":", "\n", "            ", "return", "cls", ".", "from_pretrained", "(", "\n", "cfg_name", ",", "config", "=", "cfg", ",", "project_dim", "=", "projection_dim", ",", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "HFBertEncoder", "(", "cfg", ",", "project_dim", "=", "projection_dim", ")", "\n", "\n", "", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", ":", "T", ",", "\n", "token_type_ids", ":", "T", ",", "\n", "attention_mask", ":", "T", ",", "\n", "representation_token_pos", "=", "0", ",", "\n", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "if", "self", ".", "config", ".", "output_hidden_states", ":", "\n", "            ", "sequence_output", ",", "pooled_output", ",", "hidden_states", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "None", "\n", "outputs", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", ")", "\n", "sequence_output", "=", "outputs", ".", "last_hidden_state", "\n", "pooled_output", "=", "outputs", ".", "pooler_output", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.HFBertEncoder.get_out_size": [[263, 267], ["None"], "methods", ["None"], ["", "if", "isinstance", "(", "representation_token_pos", ",", "int", ")", ":", "\n", "            ", "pooled_output", "=", "sequence_output", "[", ":", ",", "representation_token_pos", ",", ":", "]", "\n", "", "else", ":", "# treat as a tensor", "\n", "            ", "bsz", "=", "sequence_output", ".", "size", "(", "0", ")", "\n", "assert", "(", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.__init__": [[270, 276], ["None"], "methods", ["None"], ["bsz", ",", "representation_token_pos", ".", "size", "(", "0", ")", "\n", ")", "\n", "pooled_output", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "sequence_output", "[", "i", ",", "representation_token_pos", "[", "i", ",", "1", "]", ",", ":", "]", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor": [[277, 316], ["text.strip.strip.strip", "torch.tensor", "hf_models.BertTensorizer.tokenizer.encode", "hf_models.BertTensorizer.tokenizer.encode", "len", "len", "len"], "methods", ["None"], [")", "\n", "\n", "", "if", "self", ".", "encode_proj", ":", "\n", "            ", "pooled_output", "=", "self", ".", "encode_proj", "(", "pooled_output", ")", "\n", "", "return", "sequence_output", ",", "pooled_output", ",", "hidden_states", "\n", "\n", "", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "config", ".", "hidden_size", "\n", "\n", "", "", "class", "HFLXMERTEncoder", "(", "LxmertModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ",", "project_dim", ":", "int", "=", "0", ")", ":", "\n", "        ", "LxmertModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "assert", "config", ".", "hidden_size", ">", "0", ",", "\"Encoder hidden_size can't be zero\"", "\n", "self", ".", "encode_proj", "=", "(", "\n", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "project_dim", ")", "if", "project_dim", "!=", "0", "else", "None", "\n", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "init_encoder", "(", "\n", "cls", ",", "\n", "cfg_name", ":", "str", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "pretrained", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", "->", "LxmertModel", ":", "\n", "        ", "cfg", "=", "LxmertConfig", ".", "from_pretrained", "(", "cfg_name", "if", "cfg_name", "else", "\"'unc-nlp/lxmert-base-uncased'\"", ")", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "attention_probs_dropout_prob", "=", "dropout", "\n", "cfg", ".", "hidden_dropout_prob", "=", "dropout", "\n", "\n", "", "if", "pretrained", ":", "\n", "            ", "return", "cls", ".", "from_pretrained", "(", "\n", "cfg_name", ",", "config", "=", "cfg", ",", "project_dim", "=", "projection_dim", ",", "**", "kwargs", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "HFLXMERTEncoder", "(", "cfg", ",", "project_dim", "=", "projection_dim", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pair_separator_ids": [[317, 319], ["torch.tensor"], "methods", ["None"], ["\n", "", "", "def", "forward", "(", "\n", "self", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pad_id": [[320, 322], ["None"], "methods", ["None"], ["input_ids", "=", "None", ",", "\n", "visual_feats", "=", "None", ",", "\n", "visual_pos", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_attn_mask": [[323, 325], ["hf_models.BertTensorizer.get_pad_id"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pad_id"], ["attention_mask", "=", "None", ",", "\n", "visual_attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.is_sub_word_id": [[326, 329], ["hf_models.BertTensorizer.tokenizer.convert_ids_to_tokens", "token.startswith", "token.startswith"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_ids_to_tokens"], ["inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.to_string": [[330, 332], ["hf_models.BertTensorizer.tokenizer.decode"], "methods", ["None"], [")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "outputs", "=", "super", "(", ")", ".", "forward", "(", "\n", "input_ids", "=", "input_ids", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.set_pad_to_max": [[333, 335], ["None"], "methods", ["None"], ["visual_feats", "=", "visual_feats", ",", "\n", "visual_pos", "=", "visual_pos", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_token_id": [[336, 338], ["None"], "methods", ["None"], ["visual_attention_mask", "=", "visual_attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.RobertaTensorizer.__init__": [[341, 344], ["hf_models.BertTensorizer.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "hidden_states", "=", "None", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_biencoder_components": [[30, 69], ["hf_models.HFBertEncoder.init_encoder", "hf_models.HFBertEncoder.init_encoder", "dpr.models.biencoder.BiEncoder", "hf_models.get_bert_tensorizer", "hasattr", "kwargs.pop", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tensorizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["\n", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "\n", "def", "get_lxmert_bert_biencoder_components", "(", "cfg", ",", "inference_only", ":", "bool", "=", "False", ",", "load_image", ":", "bool", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "cfg", ".", "encoder", ".", "dropout", "if", "hasattr", "(", "cfg", ".", "encoder", ",", "\"dropout\"", ")", "else", "0.0", "\n", "load_images", "=", "kwargs", ".", "pop", "(", "\"load_images\"", ")", "\n", "question_encoder", "=", "HFLXMERTEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "question_pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "ctx_encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "ctx_pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "fix_ctx_encoder", "=", "cfg", ".", "fix_ctx_encoder", "if", "hasattr", "(", "cfg", ",", "\"fix_ctx_encoder\"", ")", "else", "False", "\n", "\n", "biencoder", "=", "BiEncoder", "(", "\n", "question_encoder", ",", "ctx_encoder", ",", "fix_ctx_encoder", "=", "fix_ctx_encoder", ",", "load_images", "=", "load_images", "\n", ")", "\n", "\n", "optimizer", "=", "(", "\n", "get_optimizer", "(", "\n", "biencoder", ",", "\n", "learning_rate", "=", "cfg", ".", "train", ".", "learning_rate", ",", "\n", "adam_eps", "=", "cfg", ".", "train", ".", "adam_eps", ",", "\n", "weight_decay", "=", "cfg", ".", "train", ".", "weight_decay", ",", "\n", ")", "\n", "if", "not", "inference_only", "\n", "else", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_reader_components": [[71, 97], ["hf_models.HFBertEncoder.init_encoder", "reader.Reader", "hf_models.get_bert_tensorizer", "hasattr", "hf_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tensorizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["tensorizer", "=", "get_lxmert_tensorizer", "(", "cfg", ")", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n", "\n", "", "def", "get_bert_reader_components", "(", "cfg", ",", "inference_only", ":", "bool", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "dropout", "=", "cfg", ".", "encoder", ".", "dropout", "if", "hasattr", "(", "cfg", ".", "encoder", ",", "\"dropout\"", ")", "else", "0.0", "\n", "encoder", "=", "HFBertEncoder", ".", "init_encoder", "(", "\n", "cfg", ".", "encoder", ".", "pretrained_model_cfg", ",", "\n", "projection_dim", "=", "cfg", ".", "encoder", ".", "projection_dim", ",", "\n", "dropout", "=", "dropout", ",", "\n", "pretrained", "=", "cfg", ".", "encoder", ".", "pretrained", ",", "\n", "**", "kwargs", "\n", ")", "\n", "\n", "hidden_size", "=", "encoder", ".", "config", ".", "hidden_size", "\n", "reader", "=", "Reader", "(", "encoder", ",", "hidden_size", ")", "\n", "\n", "optimizer", "=", "(", "\n", "get_optimizer", "(", "\n", "reader", ",", "\n", "learning_rate", "=", "cfg", ".", "train", ".", "learning_rate", ",", "\n", "adam_eps", "=", "cfg", ".", "train", ".", "adam_eps", ",", "\n", "weight_decay", "=", "cfg", ".", "train", ".", "weight_decay", ",", "\n", ")", "\n", "if", "not", "inference_only", "\n", "else", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tensorizer": [[99, 111], ["hf_models.BertTensorizer", "hf_models.get_bert_tokenizer", "hf_models._add_special_tokens"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tokenizer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models._add_special_tokens"], ["tensorizer", "=", "get_bert_tensorizer", "(", "cfg", ")", "\n", "return", "tensorizer", ",", "reader", ",", "optimizer", "\n", "\n", "\n", "", "def", "get_bert_tensorizer", "(", "cfg", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "sequence_length", "=", "cfg", ".", "encoder", ".", "sequence_length", "\n", "pretrained_model_cfg", "=", "cfg", ".", "encoder", ".", "pretrained_model_cfg", "\n", "\n", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_bert_tokenizer", "(", "\n", "pretrained_model_cfg", ",", "do_lower_case", "=", "cfg", ".", "do_lower_case", "\n", ")", "\n", "if", "cfg", ".", "special_tokens", ":", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models._add_special_tokens": [[113, 134], ["logger.info", "len", "logger.info", "enumerate", "list", "logger.info", "logger.info", "range"], "function", ["None"], ["\n", "", "", "return", "BertTensorizer", "(", "tokenizer", ",", "sequence_length", ")", "\n", "\n", "", "def", "get_lxmert_tensorizer", "(", "cfg", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "sequence_length", "=", "cfg", ".", "encoder", ".", "sequence_length", "\n", "pretrained_model_cfg", "=", "cfg", ".", "encoder", ".", "pretrained_model_cfg", "\n", "\n", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_lxmert_tokenizer", "(", "\n", "pretrained_model_cfg", ",", "do_lower_case", "=", "cfg", ".", "do_lower_case", "\n", ")", "\n", "if", "cfg", ".", "special_tokens", ":", "\n", "            ", "_add_special_tokens", "(", "tokenizer", ",", "cfg", ".", "special_tokens", ")", "\n", "\n", "", "", "return", "BertTensorizer", "(", "tokenizer", ",", "sequence_length", ")", "\n", "\n", "\n", "", "def", "_add_special_tokens", "(", "tokenizer", ",", "special_tokens", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Adding special tokens %s\"", ",", "special_tokens", ")", "\n", "special_tokens_num", "=", "len", "(", "special_tokens", ")", "\n", "# TODO: this is a hack-y logic that uses some private tokenizer structure which can be changed in HF code", "\n", "assert", "special_tokens_num", "<", "50", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tensorizer": [[136, 142], ["hf_models.RobertaTensorizer", "hf_models.get_roberta_tokenizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tokenizer"], ["tokenizer", ".", "vocab", "[", "\"[unused{}]\"", ".", "format", "(", "i", ")", "]", "for", "i", "in", "range", "(", "special_tokens_num", ")", "\n", "]", "\n", "logger", ".", "info", "(", "\"Utilizing the following unused token ids %s\"", ",", "unused_ids", ")", "\n", "\n", "for", "idx", ",", "id", "in", "enumerate", "(", "unused_ids", ")", ":", "\n", "        ", "del", "tokenizer", ".", "vocab", "[", "\"[unused{}]\"", ".", "format", "(", "idx", ")", "]", "\n", "tokenizer", ".", "vocab", "[", "special_tokens", "[", "idx", "]", "]", "=", "id", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_optimizer": [[144, 172], ["transformers.optimization.AdamW", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["\n", "", "tokenizer", ".", "_additional_special_tokens", "=", "list", "(", "special_tokens", ")", "\n", "logger", ".", "info", "(", "\n", "\"Added special tokenizer.additional_special_tokens %s\"", ",", "\n", "tokenizer", ".", "additional_special_tokens", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Tokenizer's all_special_tokens %s\"", ",", "tokenizer", ".", "all_special_tokens", ")", "\n", "\n", "\n", "", "def", "get_roberta_tensorizer", "(", "args", ",", "tokenizer", "=", "None", ")", ":", "\n", "    ", "if", "not", "tokenizer", ":", "\n", "        ", "tokenizer", "=", "get_roberta_tokenizer", "(", "\n", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", "\n", ")", "\n", "", "return", "RobertaTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "\n", "\n", "", "def", "get_optimizer", "(", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "learning_rate", ":", "float", "=", "1e-5", ",", "\n", "adam_eps", ":", "float", "=", "1e-8", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", ")", "->", "torch", ".", "optim", ".", "Optimizer", ":", "\n", "    ", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tokenizer": [[174, 177], ["transformers.BertTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "weight_decay", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tokenizer": [[180, 184], ["transformers.RobertaTokenizer.from_pretrained"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained"], ["p", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.__init__": [[88, 100], ["pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "TransformerSentenceEncoder", ".", "Config", ",", "\n", "padding_idx", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "projection_dim", ":", "int", "=", "0", ",", "\n", "*", "args", ",", "\n", "**", "kwarg", "\n", ")", ":", "\n", "\n", "        ", "TransformerSentenceEncoder", ".", "__init__", "(", "self", ",", "config", ",", "False", ",", "padding_idx", ",", "vocab_size", ",", "*", "args", ",", "**", "kwarg", ")", "\n", "\n", "assert", "config", ".", "embedding_dim", ">", "0", ",", "'Encoder hidden_size can\\'t be zero'", "\n", "self", ".", "encode_proj", "=", "nn", ".", "Linear", "(", "config", ".", "embedding_dim", ",", "projection_dim", ")", "if", "projection_dim", "!=", "0", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder": [[101, 119], ["pytext_models.get_pytext_bert_base_cfg", "cls", "logger.info", "torch.load", "cls.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_pytext_bert_base_cfg", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "@", "classmethod", "\n", "def", "init_encoder", "(", "cls", ",", "pretrained_file", ":", "str", "=", "None", ",", "projection_dim", ":", "int", "=", "0", ",", "dropout", ":", "float", "=", "0.1", ",", "\n", "vocab_size", ":", "int", "=", "0", ",", "\n", "padding_idx", ":", "int", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "cfg", "=", "get_pytext_bert_base_cfg", "(", ")", "\n", "\n", "if", "dropout", "!=", "0", ":", "\n", "            ", "cfg", ".", "dropout", "=", "dropout", "\n", "cfg", ".", "attention_dropout", "=", "dropout", "\n", "cfg", ".", "activation_dropout", "=", "dropout", "\n", "\n", "", "encoder", "=", "cls", "(", "cfg", ",", "padding_idx", ",", "vocab_size", ",", "projection_dim", ",", "**", "kwargs", ")", "\n", "\n", "if", "pretrained_file", ":", "\n", "            ", "logger", ".", "info", "(", "'Loading pre-trained pytext encoder state from %s'", ",", "pretrained_file", ")", "\n", "state", "=", "torch", ".", "load", "(", "pretrained_file", ")", "\n", "encoder", ".", "load_state_dict", "(", "state", ")", "\n", "", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.forward": [[120, 126], ["super().forward", "pytext_models.PytextBertEncoder.encode_proj"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "token_type_ids", ":", "T", ",", "attention_mask", ":", "T", ")", "->", "Tuple", "[", "T", ",", "...", "]", ":", "\n", "        ", "pooled_output", "=", "super", "(", ")", ".", "forward", "(", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "None", ")", ")", "[", "0", "]", "\n", "if", "self", ".", "encode_proj", ":", "\n", "            ", "pooled_output", "=", "self", ".", "encode_proj", "(", "pooled_output", ")", "\n", "\n", "", "return", "None", ",", "pooled_output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.get_out_size": [[127, 131], ["None"], "methods", ["None"], ["", "def", "get_out_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encode_proj", ":", "\n", "            ", "return", "self", ".", "encode_proj", ".", "out_features", "\n", "", "return", "self", ".", "representation_dim", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_bert_biencoder_components": [[26, 53], ["get_tokenizer", "pytext_models.PytextBertEncoder.init_encoder", "pytext_models.PytextBertEncoder.init_encoder", "biencoder.BiEncoder", "BertTensorizer", "pytext_models.get_optimizer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.PytextBertEncoder.init_encoder", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["def", "get_bert_biencoder_components", "(", "args", ",", "inference_only", ":", "bool", "=", "False", ")", ":", "\n", "# since bert tokenizer is the same in HF and pytext/fairseq, just use HF's implementation here for now", "\n", "    ", "from", ".", "hf_models", "import", "get_tokenizer", ",", "BertTensorizer", "\n", "\n", "tokenizer", "=", "get_tokenizer", "(", "args", ".", "pretrained_model_cfg", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "question_encoder", "=", "PytextBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_file", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "args", ".", "dropout", ",", "\n", "vocab_size", "=", "tokenizer", ".", "vocab_size", ",", "\n", "padding_idx", "=", "tokenizer", ".", "pad_token_type_id", "\n", ")", "\n", "\n", "ctx_encoder", "=", "PytextBertEncoder", ".", "init_encoder", "(", "args", ".", "pretrained_file", ",", "\n", "projection_dim", "=", "args", ".", "projection_dim", ",", "dropout", "=", "args", ".", "dropout", ",", "\n", "vocab_size", "=", "tokenizer", ".", "vocab_size", ",", "\n", "padding_idx", "=", "tokenizer", ".", "pad_token_type_id", "\n", ")", "\n", "\n", "biencoder", "=", "BiEncoder", "(", "question_encoder", ",", "ctx_encoder", ")", "\n", "\n", "optimizer", "=", "get_optimizer", "(", "biencoder", ",", "\n", "learning_rate", "=", "args", ".", "learning_rate", ",", "\n", "adam_eps", "=", "args", ".", "adam_eps", ",", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", ")", "if", "not", "inference_only", "else", "None", "\n", "\n", "tensorizer", "=", "BertTensorizer", "(", "tokenizer", ",", "args", ".", "sequence_length", ")", "\n", "return", "tensorizer", ",", "biencoder", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_optimizer": [[55, 63], ["pytext.optimizer.optimizers.AdamW.Config", "pytext.optimizer.optimizers.AdamW.from_config"], "function", ["None"], ["", "def", "get_optimizer", "(", "model", ":", "nn", ".", "Module", ",", "learning_rate", ":", "float", "=", "1e-5", ",", "adam_eps", ":", "float", "=", "1e-8", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ")", "->", "torch", ".", "optim", ".", "Optimizer", ":", "\n", "    ", "cfg", "=", "AdamW", ".", "Config", "(", ")", "\n", "cfg", ".", "lr", "=", "learning_rate", "\n", "cfg", ".", "weight_decay", "=", "weight_decay", "\n", "cfg", ".", "eps", "=", "adam_eps", "\n", "optimizer", "=", "AdamW", ".", "from_config", "(", "cfg", ",", "model", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_pytext_bert_base_cfg": [[65, 84], ["pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.Config"], "function", ["None"], ["", "def", "get_pytext_bert_base_cfg", "(", ")", ":", "\n", "    ", "cfg", "=", "TransformerSentenceEncoder", ".", "Config", "(", ")", "\n", "cfg", ".", "embedding_dim", "=", "768", "\n", "cfg", ".", "ffn_embedding_dim", "=", "3072", "\n", "cfg", ".", "num_encoder_layers", "=", "12", "\n", "cfg", ".", "num_attention_heads", "=", "12", "\n", "cfg", ".", "num_segments", "=", "2", "\n", "cfg", ".", "use_position_embeddings", "=", "True", "\n", "cfg", ".", "offset_positions_by_padding", "=", "True", "\n", "cfg", ".", "apply_bert_init", "=", "True", "\n", "cfg", ".", "encoder_normalize_before", "=", "True", "\n", "cfg", ".", "activation_fn", "=", "\"gelu\"", "\n", "cfg", ".", "projection_dim", "=", "0", "\n", "cfg", ".", "max_seq_len", "=", "512", "\n", "cfg", ".", "multilingual", "=", "False", "\n", "cfg", ".", "freeze_embeddings", "=", "False", "\n", "cfg", ".", "n_trans_layers_to_freeze", "=", "0", "\n", "cfg", ".", "use_torchscript", "=", "False", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_hf_lxmert_biencoder": [[14, 19], ["get_bert_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_hf_bert_biencoder": [[20, 25], ["get_bert_biencoder_components"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_hf_bert_reader": [[27, 32], ["get_bert_reader_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_reader_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_pytext_bert_biencoder": [[34, 39], ["get_bert_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.pytext_models.get_bert_biencoder_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_fairseq_roberta_biencoder": [[41, 46], ["get_roberta_biencoder_components", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.fairseq_models.get_roberta_biencoder_components"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_hf_bert_tenzorizer": [[48, 53], ["get_bert_tensorizer", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_bert_tensorizer"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_hf_roberta_tenzorizer": [[55, 60], ["get_roberta_tensorizer", "importlib.util.find_spec", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.get_roberta_tensorizer"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp": [[80, 85], ["RuntimeError"], "function", ["None"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_biencoder_components": [[87, 89], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_reader_components": [[91, 93], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_tenzorizer": [[95, 97], ["__init__.init_comp"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.__init__.init_comp"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.Reader.__init__": [[32, 38], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "dpr.utils.model_utils.init_weights"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.model_utils.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ":", "nn", ".", "Module", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "Reader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "2", ")", "\n", "self", ".", "qa_classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", "\n", "init_weights", "(", "[", "self", ".", "qa_outputs", ",", "self", ".", "qa_classifier", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.Reader.forward": [[39, 49], ["input_ids.size", "reader.Reader._forward", "input_ids.view", "attention_mask.view", "reader.compute_loss", "start_logits.view", "end_logits.view", "relevance_logits.view"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.Reader._forward", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.compute_loss"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "T", ",", "attention_mask", ":", "T", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "answer_mask", "=", "None", ")", ":", "\n", "# notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length", "\n", "        ", "N", ",", "M", ",", "L", "=", "input_ids", ".", "size", "(", ")", "\n", "start_logits", ",", "end_logits", ",", "relevance_logits", "=", "self", ".", "_forward", "(", "input_ids", ".", "view", "(", "N", "*", "M", ",", "L", ")", ",", "\n", "attention_mask", ".", "view", "(", "N", "*", "M", ",", "L", ")", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "\n", "N", ",", "M", ")", "\n", "\n", "", "return", "start_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "end_logits", ".", "view", "(", "N", ",", "M", ",", "L", ")", ",", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.Reader._forward": [[50, 59], ["reader.Reader.encoder", "reader.Reader.qa_outputs", "reader.Reader.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "reader.Reader.qa_classifier"], "methods", ["None"], ["", "def", "_forward", "(", "self", ",", "input_ids", ",", "attention_mask", ")", ":", "\n", "# TODO: provide segment values", "\n", "        ", "sequence_output", ",", "_pooled_output", ",", "_hidden_states", "=", "self", ".", "encoder", "(", "input_ids", ",", "None", ",", "attention_mask", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "rank_logits", "=", "self", ".", "qa_classifier", "(", "sequence_output", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "return", "start_logits", ",", "end_logits", ",", "rank_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.compute_loss": [[61, 96], ["start_positions.view.view", "end_positions.view.view", "answer_mask.type().cuda.view", "start_logits.view.view", "end_logits.view.view", "relevance_logits.view.view", "answer_mask.type().cuda.type().cuda", "start_logits.view.size", "start_positions.view.clamp_", "end_positions.view.clamp_", "torch.nn.CrossEntropyLoss", "relevance_logits.view.view", "torch.zeros().cuda", "torch.zeros().cuda", "torch.sum", "torch.sum", "reader._calc_mml", "torch.nn.CrossEntropyLoss.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "loss_tensor.view().max", "answer_mask.type().cuda.type", "torch.zeros", "torch.zeros", "torch.nn.CrossEntropyLoss.", "zip", "torch.nn.CrossEntropyLoss.", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "t.unsqueeze", "t.unsqueeze", "loss_tensor.view"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._calc_mml"], ["", "", "def", "compute_loss", "(", "start_positions", ",", "end_positions", ",", "answer_mask", ",", "start_logits", ",", "end_logits", ",", "relevance_logits", ",", "N", ",", "M", ")", ":", "\n", "    ", "start_positions", "=", "start_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_positions", "=", "end_positions", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "answer_mask", "=", "answer_mask", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "\n", "start_logits", "=", "start_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "view", "(", "N", "*", "M", ",", "-", "1", ")", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", "*", "M", ")", "\n", "\n", "answer_mask", "=", "answer_mask", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "cuda", "(", ")", "\n", "\n", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "reduce", "=", "False", ",", "ignore_index", "=", "ignored_index", ")", "\n", "\n", "# compute switch loss", "\n", "relevance_logits", "=", "relevance_logits", ".", "view", "(", "N", ",", "M", ")", "\n", "switch_labels", "=", "torch", ".", "zeros", "(", "N", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "switch_loss", "=", "torch", ".", "sum", "(", "loss_fct", "(", "relevance_logits", ",", "switch_labels", ")", ")", "\n", "\n", "# compute span loss", "\n", "start_losses", "=", "[", "(", "loss_fct", "(", "start_logits", ",", "_start_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_start_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "start_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "\n", "end_losses", "=", "[", "(", "loss_fct", "(", "end_logits", ",", "_end_positions", ")", "*", "_span_mask", ")", "\n", "for", "(", "_end_positions", ",", "_span_mask", ")", "\n", "in", "zip", "(", "torch", ".", "unbind", "(", "end_positions", ",", "dim", "=", "1", ")", ",", "torch", ".", "unbind", "(", "answer_mask", ",", "dim", "=", "1", ")", ")", "]", "\n", "loss_tensor", "=", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "start_losses", "]", ",", "dim", "=", "1", ")", "+", "torch", ".", "cat", "(", "[", "t", ".", "unsqueeze", "(", "1", ")", "for", "t", "in", "end_losses", "]", ",", "dim", "=", "1", ")", "\n", "\n", "loss_tensor", "=", "loss_tensor", ".", "view", "(", "N", ",", "M", ",", "-", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "span_loss", "=", "_calc_mml", "(", "loss_tensor", ")", "\n", "return", "span_loss", "+", "switch_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader.create_reader_input": [[98, 152], ["torch.Tensor().new_full", "torch.Tensor().new_full", "torch.cat", "torch.cat", "ReaderBatch", "reader._create_question_passages_tensors", "torch.cat.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.Tensor", "torch.Tensor", "logger.warning", "torch.stack.append", "torch.stack.append", "torch.stack.append", "ids.unsqueeze"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._create_question_passages_tensors"], ["", "def", "create_reader_input", "(", "pad_token_id", ":", "int", ",", "\n", "samples", ":", "List", "[", "ReaderSample", "]", ",", "\n", "passages_per_question", ":", "int", ",", "\n", "max_length", ":", "int", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "shuffle", ":", "bool", ",", "\n", ")", "->", "ReaderBatch", ":", "\n", "    ", "\"\"\"\n    Creates a reader batch instance out of a list of ReaderSample-s\n    :param pad_token_id: id of the padding token\n    :param samples: list of samples to create the batch for\n    :param passages_per_question: amount of passages for every question in a batch\n    :param max_length: max model input sequence length\n    :param max_n_answers: max num of answers per single question\n    :param is_train: if the samples are for a train set\n    :param shuffle: should passages selection be randomized\n    :return: ReaderBatch instance\n    \"\"\"", "\n", "input_ids", "=", "[", "]", "\n", "start_positions", "=", "[", "]", "\n", "end_positions", "=", "[", "]", "\n", "answers_masks", "=", "[", "]", "\n", "empty_sequence", "=", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_length", ",", ")", ",", "pad_token_id", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "sample", "in", "samples", ":", "\n", "        ", "positive_ctxs", "=", "sample", ".", "positive_passages", "\n", "negative_ctxs", "=", "sample", ".", "negative_passages", "if", "is_train", "else", "sample", ".", "passages", "\n", "\n", "sample_tensors", "=", "_create_question_passages_tensors", "(", "positive_ctxs", ",", "\n", "negative_ctxs", ",", "\n", "passages_per_question", ",", "\n", "empty_sequence", ",", "\n", "max_n_answers", ",", "\n", "pad_token_id", ",", "\n", "is_train", ",", "\n", "is_random", "=", "shuffle", ")", "\n", "if", "not", "sample_tensors", ":", "\n", "            ", "logger", ".", "warning", "(", "'No valid passages combination for question=%s '", ",", "sample", ".", "question", ")", "\n", "continue", "\n", "", "sample_input_ids", ",", "starts_tensor", ",", "ends_tensor", ",", "answer_mask", "=", "sample_tensors", "\n", "input_ids", ".", "append", "(", "sample_input_ids", ")", "\n", "if", "is_train", ":", "\n", "            ", "start_positions", ".", "append", "(", "starts_tensor", ")", "\n", "end_positions", ".", "append", "(", "ends_tensor", ")", "\n", "answers_masks", ".", "append", "(", "answer_mask", ")", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "ids", ".", "unsqueeze", "(", "0", ")", "for", "ids", "in", "input_ids", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "is_train", ":", "\n", "        ", "start_positions", "=", "torch", ".", "stack", "(", "start_positions", ",", "dim", "=", "0", ")", "\n", "end_positions", "=", "torch", ".", "stack", "(", "end_positions", ",", "dim", "=", "0", ")", "\n", "answers_masks", "=", "torch", ".", "stack", "(", "answers_masks", ",", "dim", "=", "0", ")", "\n", "\n", "", "return", "ReaderBatch", "(", "input_ids", ",", "start_positions", ",", "end_positions", ",", "answers_masks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._calc_mml": [[154, 159], ["torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.ones().cuda", "torch.ones().cuda", "torch.ones", "torch.ones", "loss_tensor.size"], "function", ["None"], ["", "def", "_calc_mml", "(", "loss_tensor", ")", ":", "\n", "    ", "marginal_likelihood", "=", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "\n", "-", "loss_tensor", "-", "1e10", "*", "(", "loss_tensor", "==", "0", ")", ".", "float", "(", ")", ")", ",", "1", ")", "\n", "return", "-", "torch", ".", "sum", "(", "torch", ".", "log", "(", "marginal_likelihood", "+", "\n", "torch", ".", "ones", "(", "loss_tensor", ".", "size", "(", "0", ")", ")", ".", "cuda", "(", ")", "*", "(", "marginal_likelihood", "==", "0", ")", ".", "float", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len": [[161, 166], ["seq.size", "torch.cat", "torch.cat", "torch.Tensor().new_full", "torch.Tensor().new_full", "torch.Tensor", "torch.Tensor"], "function", ["None"], ["", "def", "_pad_to_len", "(", "seq", ":", "T", ",", "pad_id", ":", "int", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "s_len", "=", "seq", ".", "size", "(", "0", ")", "\n", "if", "s_len", ">", "max_len", ":", "\n", "        ", "return", "seq", "[", "0", ":", "max_len", "]", "\n", "", "return", "torch", ".", "cat", "(", "[", "seq", ",", "torch", ".", "Tensor", "(", ")", ".", "new_full", "(", "(", "max_len", "-", "s_len", ",", ")", ",", "pad_id", ",", "dtype", "=", "torch", ".", "long", ")", "]", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans": [[168, 171], ["None"], "function", ["None"], ["", "def", "_get_answer_spans", "(", "idx", ",", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ")", ":", "\n", "    ", "positive_a_spans", "=", "positives", "[", "idx", "]", ".", "answers_spans", "\n", "return", "[", "span", "for", "span", "in", "positive_a_spans", "if", "(", "span", "[", "0", "]", "<", "max_len", "and", "span", "[", "1", "]", "<", "max_len", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_positive_idx": [[173, 182], ["numpy.random.choice", "reader._get_answer_spans", "next", "len", "range", "reader._get_answer_spans", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans"], ["", "def", "_get_positive_idx", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "max_len", ":", "int", ",", "is_random", ":", "bool", ")", ":", "\n", "# select just one positive", "\n", "    ", "positive_idx", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "positives", ")", ")", "if", "is_random", "else", "0", "\n", "\n", "if", "not", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", ":", "\n", "# question may be too long, find the first positive with at least one valid span", "\n", "        ", "positive_idx", "=", "next", "(", "(", "i", "for", "i", "in", "range", "(", "len", "(", "positives", ")", ")", "if", "_get_answer_spans", "(", "i", ",", "positives", ",", "max_len", ")", ")", ",", "\n", "None", ")", "\n", "", "return", "positive_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._create_question_passages_tensors": [[184, 237], ["empty_ids.size", "len", "torch.stack", "torch.stack", "reader._get_positive_idx", "all", "all", "reader._pad_to_len", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros().long", "torch.zeros().long", "torch.tensor", "torch.tensor", "torch.zeros", "torch.zeros", "torch.tensor", "torch.tensor", "numpy.random.permutation", "range", "reader._pad_to_len", "len", "negatives_selected.append", "reader._get_answer_spans", "range", "empty_ids.clone", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len", "len", "len", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_positive_idx", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._pad_to_len", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.reader._get_answer_spans"], ["", "def", "_create_question_passages_tensors", "(", "positives", ":", "List", "[", "ReaderPassage", "]", ",", "negatives", ":", "List", "[", "ReaderPassage", "]", ",", "total_size", ":", "int", ",", "\n", "empty_ids", ":", "T", ",", "\n", "max_n_answers", ":", "int", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "is_train", ":", "bool", ",", "\n", "is_random", ":", "bool", "=", "True", ")", ":", "\n", "    ", "max_len", "=", "empty_ids", ".", "size", "(", "0", ")", "\n", "if", "is_train", ":", "\n", "# select just one positive", "\n", "        ", "positive_idx", "=", "_get_positive_idx", "(", "positives", ",", "max_len", ",", "is_random", ")", "\n", "if", "positive_idx", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "positive_a_spans", "=", "_get_answer_spans", "(", "positive_idx", ",", "positives", ",", "max_len", ")", "[", "0", ":", "max_n_answers", "]", "\n", "\n", "answer_starts", "=", "[", "span", "[", "0", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "answer_ends", "=", "[", "span", "[", "1", "]", "for", "span", "in", "positive_a_spans", "]", "\n", "\n", "assert", "all", "(", "s", "<", "max_len", "for", "s", "in", "answer_starts", ")", "\n", "assert", "all", "(", "e", "<", "max_len", "for", "e", "in", "answer_ends", ")", "\n", "\n", "positive_input_ids", "=", "_pad_to_len", "(", "positives", "[", "positive_idx", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "\n", "\n", "answer_starts_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_starts_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "answer_starts", ")", "\n", "\n", "answer_ends_tensor", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ")", ".", "long", "(", ")", "\n", "answer_ends_tensor", "[", "0", ",", "0", ":", "len", "(", "answer_ends", ")", "]", "=", "torch", ".", "tensor", "(", "answer_ends", ")", "\n", "\n", "answer_mask", "=", "torch", ".", "zeros", "(", "(", "total_size", ",", "max_n_answers", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "answer_mask", "[", "0", ",", "0", ":", "len", "(", "answer_starts", ")", "]", "=", "torch", ".", "tensor", "(", "[", "1", "for", "_", "in", "range", "(", "len", "(", "answer_starts", ")", ")", "]", ")", "\n", "\n", "positives_selected", "=", "[", "positive_input_ids", "]", "\n", "\n", "", "else", ":", "\n", "        ", "positives_selected", "=", "[", "]", "\n", "answer_starts_tensor", "=", "None", "\n", "answer_ends_tensor", "=", "None", "\n", "answer_mask", "=", "None", "\n", "\n", "", "positives_num", "=", "len", "(", "positives_selected", ")", "\n", "negative_idxs", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "negatives", ")", ")", ")", "if", "is_random", "else", "range", "(", "\n", "len", "(", "negatives", ")", "-", "positives_num", ")", "\n", "\n", "negative_idxs", "=", "negative_idxs", "[", ":", "total_size", "-", "positives_num", "]", "\n", "\n", "negatives_selected", "=", "[", "_pad_to_len", "(", "negatives", "[", "i", "]", ".", "sequence_ids", ",", "pad_token_id", ",", "max_len", ")", "for", "i", "in", "negative_idxs", "]", "\n", "\n", "while", "len", "(", "negatives_selected", ")", "<", "total_size", "-", "positives_num", ":", "\n", "        ", "negatives_selected", ".", "append", "(", "empty_ids", ".", "clone", "(", ")", ")", "\n", "\n", "", "input_ids", "=", "torch", ".", "stack", "(", "[", "t", "for", "t", "in", "positives_selected", "+", "negatives_selected", "]", ",", "dim", "=", "0", ")", "\n", "return", "input_ids", ",", "answer_starts_tensor", ",", "answer_ends_tensor", ",", "answer_mask", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Cell.__init__": [[26, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "value_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "self", ".", "type", ":", "str", "=", "\"\"", "\n", "self", ".", "nested_tables", ":", "List", "[", "Table", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Cell.__str__": [[31, 33], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "self", ".", "value_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Cell.to_dpr_json": [[34, 38], ["str"], "methods", ["None"], ["", "def", "to_dpr_json", "(", "self", ",", "cell_idx", ":", "int", ")", ":", "\n", "        ", "r", "=", "{", "\"col\"", ":", "cell_idx", "}", "\n", "r", "[", "\"value\"", "]", "=", "str", "(", "self", ")", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Row.__init__": [[41, 43], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "cells", ":", "List", "[", "Cell", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Row.__str__": [[44, 46], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"| \"", ".", "join", "(", "[", "str", "(", "c", ")", "for", "c", "in", "self", ".", "cells", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Row.visit": [[47, 51], ["enumerate", "tokens_function"], "methods", ["None"], ["", "def", "visit", "(", "self", ",", "tokens_function", ",", "row_idx", ":", "int", ")", ":", "\n", "        ", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "cells", ")", ":", "\n", "            ", "if", "c", ".", "value_tokens", ":", "\n", "                ", "tokens_function", "(", "c", ".", "value_tokens", ",", "row_idx", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Row.to_dpr_json": [[52, 56], ["c.to_dpr_json", "enumerate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json"], ["", "", "", "def", "to_dpr_json", "(", "self", ",", "row_idx", ":", "int", ")", ":", "\n", "        ", "r", "=", "{", "\"row\"", ":", "row_idx", "}", "\n", "r", "[", "\"columns\"", "]", "=", "[", "c", ".", "to_dpr_json", "(", "i", ")", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "cells", ")", "]", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Table.__init__": [[59, 64], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "caption", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "caption", "=", "caption", "\n", "self", ".", "body", ":", "List", "[", "Row", "]", "=", "[", "]", "\n", "self", ".", "key", "=", "None", "\n", "self", ".", "gold_match", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Table.__str__": [[65, 72], ["enumerate", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "table_str", "=", "\"<T>: {}\\n\"", ".", "format", "(", "self", ".", "caption", ")", "\n", "table_str", "+=", "\" rows:\\n\"", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", ":", "\n", "            ", "table_str", "+=", "\" row #{}: {}\\n\"", ".", "format", "(", "i", ",", "str", "(", "r", ")", ")", "\n", "\n", "", "return", "table_str", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Table.get_key": [[73, 77], ["str"], "methods", ["None"], ["", "def", "get_key", "(", "self", ")", "->", "str", ":", "\n", "        ", "if", "not", "self", ".", "key", ":", "\n", "            ", "self", ".", "key", "=", "str", "(", "self", ")", "\n", "", "return", "self", ".", "key", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Table.visit": [[78, 83], ["enumerate", "tokens_function", "r.visit"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit"], ["", "def", "visit", "(", "self", ",", "tokens_function", ",", "include_caption", ":", "bool", "=", "False", ")", "->", "bool", ":", "\n", "        ", "if", "include_caption", ":", "\n", "            ", "tokens_function", "(", "self", ".", "caption", ",", "-", "1", ",", "-", "1", ")", "\n", "", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", ":", "\n", "            ", "r", ".", "visit", "(", "tokens_function", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.Table.to_dpr_json": [[84, 92], ["r.to_dpr_json", "enumerate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json"], ["", "", "def", "to_dpr_json", "(", "self", ")", ":", "\n", "        ", "r", "=", "{", "\n", "\"caption\"", ":", "self", ".", "caption", ",", "\n", "\"rows\"", ":", "[", "r", ".", "to_dpr_json", "(", "i", ")", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", "]", ",", "\n", "}", "\n", "if", "self", ".", "gold_match", ":", "\n", "            ", "r", "[", "\"gold_match\"", "]", "=", "1", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser.__init__": [[95, 104], ["len", "collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ",", "is_html_mask", ",", "title", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "is_html_mask", "=", "is_html_mask", "\n", "self", ".", "max_idx", "=", "len", "(", "self", ".", "tokens", ")", "\n", "self", ".", "all_tables", "=", "[", "]", "\n", "\n", "self", ".", "current_table", ":", "Table", "=", "None", "\n", "self", ".", "tables_stack", "=", "collections", ".", "deque", "(", ")", "\n", "self", ".", "title", "=", "title", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser.parse": [[105, 132], ["collections.deque", "range", "tables.NQTableParser._on_content", "tables.NQTableParser._on_table_start", "tables.NQTableParser._on_table_end", "tables.NQTableParser._onRowStart", "tables.NQTableParser._onRowEnd", "tables.NQTableParser._onCellStart", "tables.NQTableParser._on_cell_end"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_content", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_start", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_end", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowStart", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowEnd", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onCellStart", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_cell_end"], ["", "def", "parse", "(", "self", ")", "->", "List", "[", "Table", "]", ":", "\n", "        ", "self", ".", "all_tables", "=", "[", "]", "\n", "self", ".", "tables_stack", "=", "collections", ".", "deque", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "max_idx", ")", ":", "\n", "\n", "            ", "t", "=", "self", ".", "tokens", "[", "i", "]", "\n", "\n", "if", "not", "self", ".", "is_html_mask", "[", "i", "]", ":", "\n", "# cell content", "\n", "                ", "self", ".", "_on_content", "(", "t", ")", "\n", "continue", "\n", "\n", "", "if", "\"<Table\"", "in", "t", ":", "\n", "                ", "self", ".", "_on_table_start", "(", ")", "\n", "", "elif", "t", "==", "\"</Table>\"", ":", "\n", "                ", "self", ".", "_on_table_end", "(", ")", "\n", "", "elif", "\"<Tr\"", "in", "t", ":", "\n", "                ", "self", ".", "_onRowStart", "(", ")", "\n", "", "elif", "t", "==", "\"</Tr>\"", ":", "\n", "                ", "self", ".", "_onRowEnd", "(", ")", "\n", "", "elif", "\"<Td\"", "in", "t", "or", "\"<Th\"", "in", "t", ":", "\n", "                ", "self", ".", "_onCellStart", "(", ")", "\n", "", "elif", "t", "in", "[", "\"</Td>\"", ",", "\"</Th>\"", "]", ":", "\n", "                ", "self", ".", "_on_cell_end", "(", ")", "\n", "\n", "", "", "return", "self", ".", "all_tables", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._on_table_start": [[133, 148], ["tables.Table", "tables.NQTableParser.all_tables.append", "tables.NQTableParser.tables_stack.append"], "methods", ["None"], ["", "def", "_on_table_start", "(", "self", ")", ":", "\n", "        ", "caption", "=", "self", ".", "title", "\n", "parent_table", "=", "self", ".", "current_table", "\n", "if", "parent_table", ":", "\n", "            ", "self", ".", "tables_stack", ".", "append", "(", "parent_table", ")", "\n", "\n", "caption", "=", "parent_table", ".", "caption", "\n", "if", "parent_table", ".", "body", "and", "parent_table", ".", "body", "[", "-", "1", "]", ".", "cells", ":", "\n", "                ", "current_cell", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", ".", "cells", "[", "-", "1", "]", "\n", "caption", "+=", "\" | \"", "+", "\" \"", ".", "join", "(", "current_cell", ".", "value_tokens", ")", "\n", "\n", "", "", "t", "=", "Table", "(", ")", "\n", "t", ".", "caption", "=", "caption", "\n", "self", ".", "current_table", "=", "t", "\n", "self", ".", "all_tables", ".", "append", "(", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._on_table_end": [[149, 159], ["logger.error", "tables.NQTableParser.tables_stack.pop", "current_cell.nested_tables.append"], "methods", ["None"], ["", "def", "_on_table_end", "(", "self", ")", ":", "\n", "        ", "t", "=", "self", ".", "current_table", "\n", "if", "t", ":", "\n", "            ", "if", "self", ".", "tables_stack", ":", "# t is a nested table", "\n", "                ", "self", ".", "current_table", "=", "self", ".", "tables_stack", ".", "pop", "(", ")", "\n", "if", "self", ".", "current_table", ".", "body", ":", "\n", "                    ", "current_cell", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", ".", "cells", "[", "-", "1", "]", "\n", "current_cell", ".", "nested_tables", ".", "append", "(", "t", ")", "\n", "", "", "", "else", ":", "\n", "            ", "logger", ".", "error", "(", "\"table end without table object\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._onRowStart": [[160, 162], ["tables.NQTableParser.current_table.body.append", "tables.Row"], "methods", ["None"], ["", "", "def", "_onRowStart", "(", "self", ")", ":", "\n", "        ", "self", ".", "current_table", ".", "body", ".", "append", "(", "Row", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._onRowEnd": [[163, 165], ["None"], "methods", ["None"], ["", "def", "_onRowEnd", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._onCellStart": [[166, 169], ["current_row.cells.append", "tables.Cell"], "methods", ["None"], ["", "def", "_onCellStart", "(", "self", ")", ":", "\n", "        ", "current_row", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", "\n", "current_row", ".", "cells", ".", "append", "(", "Cell", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._on_cell_end": [[170, 172], ["None"], "methods", ["None"], ["", "def", "_on_cell_end", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.NQTableParser._on_content": [[173, 180], ["current_cell.value_tokens.append"], "methods", ["None"], ["", "def", "_on_content", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "self", ".", "current_table", ".", "body", ":", "\n", "            ", "current_row", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", "\n", "current_cell", "=", "current_row", ".", "cells", "[", "-", "1", "]", "\n", "current_cell", ".", "value_tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "# tokens outside of row/cells. Just append to the table caption.", "\n", "            ", "self", ".", "current_table", ".", "caption", "+=", "\" \"", "+", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.read_nq_tables_jsonl": [[182, 248], ["print", "print", "print", "print", "jsonlines.open", "tables.convert_to_csv_for_lucene", "tables.NQTableParser", "tables.NQTableParser.parse", "len", "sum", "logger.info", "len", "len", "len", "t.get_key", "any", "t.get_key"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_to_csv_for_lucene", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.parse", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.get_key", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.get_key"], ["", "", "", "def", "read_nq_tables_jsonl", "(", "path", ":", "str", ",", "out_file", ":", "str", "=", "None", ")", "->", "Dict", "[", "str", ",", "Table", "]", ":", "\n", "    ", "tables_with_issues", "=", "0", "\n", "single_row_tables", "=", "0", "\n", "nested_tables", "=", "0", "\n", "regular_tables", "=", "0", "\n", "total_tables", "=", "0", "\n", "total_rows", "=", "0", "\n", "tables_dict", "=", "{", "}", "\n", "\n", "with", "jsonlines", ".", "open", "(", "path", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "tokens", "=", "jline", "[", "\"tokens\"", "]", "\n", "\n", "if", "\"( hide ) This section has multiple issues\"", "in", "\" \"", ".", "join", "(", "tokens", ")", ":", "\n", "                ", "tables_with_issues", "+=", "1", "\n", "continue", "\n", "# if '<Table>' in tokens[1:]:", "\n", "#    nested_tables += 1", "\n", "\n", "", "mask", "=", "jline", "[", "\"html_mask\"", "]", "\n", "page_url", "=", "jline", "[", "\"doc_url\"", "]", "\n", "title", "=", "jline", "[", "\"title\"", "]", "\n", "# logger.info('Table from page %s', title)", "\n", "# logger.info('tokens len %s', len(tokens))", "\n", "# logger.info('tokens %s', tokens)", "\n", "# logger.info('page_url %s', page_url)", "\n", "p", "=", "NQTableParser", "(", "tokens", ",", "mask", ",", "title", ")", "\n", "tables", "=", "p", ".", "parse", "(", ")", "\n", "\n", "# logger.info('parsed tables %d', len(tables))", "\n", "\n", "# table = parse_table(tokens, mask)", "\n", "nested_tables", "+=", "len", "(", "tables", "[", "1", ":", "]", ")", "\n", "\n", "for", "t", "in", "tables", ":", "\n", "# logger.info('Table: %s', t)", "\n", "                ", "total_tables", "+=", "1", "\n", "\n", "# calc amount of non empty rows", "\n", "non_empty_rows", "=", "sum", "(", "\n", "[", "\n", "1", "\n", "for", "r", "in", "t", ".", "body", "\n", "if", "r", ".", "cells", "and", "any", "(", "[", "True", "for", "c", "in", "r", ".", "cells", "if", "c", ".", "value_tokens", "]", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "non_empty_rows", "<=", "1", ":", "\n", "                    ", "single_row_tables", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "regular_tables", "+=", "1", "\n", "total_rows", "+=", "len", "(", "t", ".", "body", ")", "\n", "\n", "if", "t", ".", "get_key", "(", ")", "not", "in", "tables_dict", ":", "\n", "                        ", "tables_dict", "[", "t", ".", "get_key", "(", ")", "]", "=", "t", "\n", "\n", "", "", "", "if", "len", "(", "tables_dict", ")", "%", "1000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"tables_dict %d\"", ",", "len", "(", "tables_dict", ")", ")", "\n", "\n", "", "", "", "print", "(", "\"regular tables\"", ",", "regular_tables", ")", "\n", "print", "(", "\"tables_with_issues\"", ",", "tables_with_issues", ")", "\n", "print", "(", "\"single_row_tables\"", ",", "single_row_tables", ")", "\n", "print", "(", "\"nested_tables\"", ",", "nested_tables", ")", "\n", "if", "out_file", ":", "\n", "        ", "convert_to_csv_for_lucene", "(", "tables_dict", ",", "out_file", ")", "\n", "", "return", "tables_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.get_table_string_for_answer_check": [[250, 256], ["None"], "function", ["None"], ["", "def", "get_table_string_for_answer_check", "(", "table", ":", "Table", ")", ":", "# this doesn't use caption", "\n", "    ", "table_text", "=", "\"\"", "\n", "for", "r", "in", "table", ".", "body", ":", "\n", "        ", "table_text", "+=", "\" . \"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "c", ".", "value_tokens", ")", "for", "c", "in", "r", ".", "cells", "]", ")", "\n", "", "table_text", "+=", "\" . \"", "\n", "return", "table_text", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_to_csv_for_lucene": [[258, 268], ["logger.info", "open", "csv.writer", "tables_dict.items", "tables.get_table_string_for_answer_check", "csv.writer.writerow"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.get_table_string_for_answer_check"], ["", "def", "convert_to_csv_for_lucene", "(", "tables_dict", ",", "out_file", ":", "str", ")", ":", "\n", "    ", "id", "=", "0", "\n", "with", "open", "(", "out_file", ",", "\"w\"", ",", "newline", "=", "\"\"", ")", "as", "csvfile", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "csvfile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "_", ",", "v", "in", "tables_dict", ".", "items", "(", ")", ":", "\n", "            ", "id", "+=", "1", "\n", "# strip all", "\n", "table_text", "=", "get_table_string_for_answer_check", "(", "v", ")", "\n", "writer", ".", "writerow", "(", "[", "id", ",", "table_text", ",", "v", ".", "caption", "]", ")", "\n", "", "", "logger", ".", "info", "(", "\"Saved to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_jsonl_to_qas_tsv": [[270, 287], ["logger.info", "jsonlines.open", "open", "csv.writer", "results.append", "csv.writer.writerow"], "function", ["None"], ["", "def", "convert_jsonl_to_qas_tsv", "(", "path", ",", "out", ")", ":", "\n", "    ", "results", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "path", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "q", "=", "jline", "[", "\"question\"", "]", "\n", "answers", "=", "[", "]", "\n", "if", "\"short_answers\"", "in", "jline", ":", "\n", "                ", "answers", "=", "jline", "[", "\"short_answers\"", "]", "\n", "\n", "", "results", ".", "append", "(", "(", "q", ",", "answers", ")", ")", "\n", "\n", "", "", "with", "open", "(", "out", ",", "\"w\"", ",", "newline", "=", "\"\"", ")", "as", "csvfile", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "csvfile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "r", "in", "results", ":", "\n", "            ", "writer", ".", "writerow", "(", "[", "r", "[", "0", "]", ",", "r", "[", "1", "]", "]", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Saved to %s\"", ",", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.tokenize": [[292, 295], ["nlp", "token.text.lower"], "function", ["None"], ["def", "tokenize", "(", "text", ")", ":", "\n", "    ", "doc", "=", "nlp", "(", "text", ")", "\n", "return", "[", "token", ".", "text", ".", "lower", "(", ")", "for", "token", "in", "doc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize": [[297, 300], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize"], ["", "def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.prepare_answers": [[302, 309], ["tables.normalize", "single_answer.lower().split.lower().split", "r.append", "single_answer.lower().split.lower"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize"], ["", "def", "prepare_answers", "(", "answers", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "    ", "r", "=", "[", "]", "\n", "for", "single_answer", "in", "answers", ":", "\n", "        ", "single_answer", "=", "normalize", "(", "single_answer", ")", "\n", "single_answer", "=", "single_answer", ".", "lower", "(", ")", ".", "split", "(", "\" \"", ")", "# tokenize(single_answer)", "\n", "r", ".", "append", "(", "single_answer", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_prepared_answer": [[311, 321], ["tables.normalize", "tables.tokenize", "range", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "has_prepared_answer", "(", "prep_answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "text", ")", ":", "\n", "    ", "\"\"\"Check if a document contains an answer string.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "# Answer is a list of possible strings", "\n", "text", "=", "tokenize", "(", "text", ")", "\n", "for", "single_answer", "in", "prep_answers", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "            ", "if", "single_answer", "==", "text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                ", "return", "True", "\n", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_prepared_answer2": [[323, 337], ["normalize().lower", "range", "tables.normalize", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize"], ["", "def", "has_prepared_answer2", "(", "prep_answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "text", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "text", "=", "[", "normalize", "(", "token", ")", ".", "lower", "(", ")", "for", "token", "in", "text", "]", "\n", "\n", "# text = [item for sublist in text for item in sublist]", "\n", "\n", "# text = ' '.join(text)", "\n", "# text = normalize(text)", "\n", "# text = tokenize(text)", "\n", "\n", "for", "single_answer", "in", "prep_answers", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "            ", "if", "single_answer", "==", "text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                ", "return", "True", "\n", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_answer": [[339, 360], ["tables.normalize", "tables.normalize", "tables.regex_match", "tables.tokenize", "tables.normalize", "tables.tokenize", "range", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.regex_match", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "has_answer", "(", "answers", ",", "text", ",", "regMatxh", "=", "False", ")", ":", "\n", "    ", "\"\"\"Check if a document contains an answer string.\"\"\"", "\n", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "\n", "if", "regMatxh", ":", "\n", "        ", "single_answer", "=", "normalize", "(", "answers", "[", "0", "]", ")", "\n", "if", "regex_match", "(", "text", ",", "single_answer", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "else", ":", "\n", "# Answer is a list of possible strings", "\n", "        ", "text", "=", "tokenize", "(", "text", ")", "\n", "\n", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "normalize", "(", "single_answer", ")", "\n", "single_answer", "=", "tokenize", "(", "single_answer", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "                ", "if", "single_answer", "==", "text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                    ", "return", "True", "\n", "", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_search_res_to_dpr_and_eval": [[362, 542], ["tables.read_nq_tables_jsonl", "read_nq_tables_jsonl.items", "logger.info", "torch.tensor", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "open", "csv.reader", "len", "logger.info", "jsonlines.open", "len", "jsonlines.open", "eval", "tables.prepare_answers", "qas.append", "enumerate", "open", "csv.reader", "tables.NQTableParser", "tables.NQTableParser.parse", "tables.prepare_answers", "enumerate", "out_results.append", "writer.write", "bm25result.split", "table.visit", "logger.info", "int", "t.visit", "logger.info", "len", "len", "t.to_dpr_json", "t.to_dpr_json", "tables.has_prepared_answer2", "len", "question_positives.append", "answers_table_links.append", "question_hns.append", "len", "len", "int", "table.visit", "logger.info", "question_positives.insert", "ans_links.insert", "tables.has_prepared_answer2", "len", "tables_with_answers.append", "tables_answer_locations.append", "int", "answer_locations.append", "len", "tables.has_prepared_answer2", "len", "answer_locations.append", "len", "len", "field.split", "answer_locations.append"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.read_nq_tables_jsonl", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.prepare_answers", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.parse", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.prepare_answers", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_prepared_answer2", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_prepared_answer2", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.has_prepared_answer2"], ["", "def", "convert_search_res_to_dpr_and_eval", "(", "\n", "res_file", ",", "all_tables_file_jsonl", ",", "nq_table_file", ",", "out_file", ",", "gold_res_file", ":", "str", "=", "None", "\n", ")", ":", "\n", "    ", "db", "=", "{", "}", "\n", "id", "=", "0", "\n", "tables_dict", "=", "read_nq_tables_jsonl", "(", "all_tables_file_jsonl", ")", "\n", "for", "_", ",", "v", "in", "tables_dict", ".", "items", "(", ")", ":", "\n", "        ", "id", "+=", "1", "\n", "db", "[", "id", "]", "=", "v", "\n", "\n", "", "logger", ".", "info", "(", "\"db size %s\"", ",", "len", "(", "db", ")", ")", "\n", "total", "=", "0", "\n", "dpr_results", "=", "{", "}", "\n", "import", "torch", "\n", "\n", "bm25_per_topk_hits", "=", "torch", ".", "tensor", "(", "[", "0", "]", "*", "100", ")", "\n", "qas", "=", "[", "]", "\n", "with", "open", "(", "res_file", ")", "as", "tsvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "tsvfile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "# file format: id, text", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "total", "+=", "1", "\n", "q", "=", "row", "[", "0", "]", "\n", "answers", "=", "eval", "(", "row", "[", "1", "]", ")", "\n", "\n", "prep_answers", "=", "prepare_answers", "(", "answers", ")", "\n", "qas", ".", "append", "(", "(", "q", ",", "prep_answers", ")", ")", "\n", "# logger.info('question %s', q)", "\n", "\n", "question_hns", "=", "[", "]", "\n", "question_positives", "=", "[", "]", "\n", "answers_table_links", "=", "[", "]", "\n", "\n", "for", "k", ",", "bm25result", "in", "enumerate", "(", "row", "[", "2", ":", "]", ")", ":", "\n", "                ", "score", ",", "id", "=", "bm25result", ".", "split", "(", "\",\"", ")", "\n", "table", "=", "db", "[", "int", "(", "id", ")", "]", "\n", "\n", "answer_locations", "=", "[", "]", "\n", "\n", "def", "check_answer", "(", "tokens", ",", "row_idx", ":", "int", ",", "cell_idx", ":", "int", ")", ":", "\n", "                    ", "if", "has_prepared_answer2", "(", "prep_answers", ",", "tokens", ")", ":", "\n", "                        ", "answer_locations", ".", "append", "(", "(", "row_idx", ",", "cell_idx", ")", ")", "\n", "\n", "# logger.info('table %s', table)", "\n", "\n", "# get string representation to find answer", "\n", "", "", "if", "(", "len", "(", "question_positives", ")", ">=", "10", "and", "len", "(", "question_hns", ")", ">=", "10", ")", "or", "(", "\n", "len", "(", "question_hns", ")", ">=", "30", "\n", ")", ":", "\n", "                    ", "break", "\n", "\n", "# table_str = get_table_string_for_answer_check(table)", "\n", "", "table", ".", "visit", "(", "check_answer", ")", "\n", "has_answer", "=", "len", "(", "answer_locations", ")", ">", "0", "\n", "\n", "if", "has_answer", ":", "\n", "# has_answer(answers, table.key)", "\n", "# has_answer(answers, get_table_string_for_answer_check(table))", "\n", "# bm25_per_topk_hits[k:] += 1", "\n", "\n", "                    ", "question_positives", ".", "append", "(", "table", ")", "\n", "answers_table_links", ".", "append", "(", "answer_locations", ")", "\n", "# break", "\n", "", "else", ":", "\n", "                    ", "question_hns", ".", "append", "(", "table", ")", "\n", "\n", "", "", "dpr_results", "[", "q", "]", "=", "(", "question_positives", ",", "question_hns", ",", "answers_table_links", ")", "\n", "if", "len", "(", "dpr_results", ")", "%", "100", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"dpr_results %s\"", ",", "len", "(", "dpr_results", ")", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"dpr_results size %s\"", ",", "len", "(", "dpr_results", ")", ")", "\n", "logger", ".", "info", "(", "\"total %s\"", ",", "total", ")", "\n", "logger", ".", "info", "(", "\"bm25_per_topk_hits %s\"", ",", "bm25_per_topk_hits", ")", "\n", "\n", "if", "gold_res_file", ":", "\n", "        ", "logger", ".", "info", "(", "\"Processing gold_res_file\"", ")", "\n", "with", "open", "(", "gold_res_file", ")", "as", "cFile", ":", "\n", "            ", "csvReader", "=", "csv", ".", "reader", "(", "cFile", ",", "delimiter", "=", "\",\"", ")", "\n", "for", "row", "in", "csvReader", ":", "\n", "                ", "q_id", "=", "int", "(", "row", "[", "0", "]", ")", "\n", "qas_tuple", "=", "qas", "[", "q_id", "]", "\n", "prep_answers", "=", "qas_tuple", "[", "1", "]", "\n", "question_gold_positive_match", "=", "None", "\n", "q", "=", "qas_tuple", "[", "0", "]", "\n", "\n", "# logger.info(\"q=%s q_id=%s\", q, q_id)", "\n", "answers_links", "=", "None", "\n", "for", "field", "in", "row", "[", "1", ":", "]", ":", "\n", "                    ", "psg_id", "=", "int", "(", "field", ".", "split", "(", ")", "[", "0", "]", ")", "\n", "# logger.info(\"psg_id=%s\", psg_id)", "\n", "# if psg_id >= len(db):", "\n", "#    continue", "\n", "table", "=", "db", "[", "psg_id", "]", "\n", "answer_locations", "=", "[", "]", "\n", "\n", "def", "check_answer", "(", "tokens", ",", "row_idx", ":", "int", ",", "cell_idx", ":", "int", ")", ":", "\n", "                        ", "if", "has_prepared_answer2", "(", "prep_answers", ",", "tokens", ")", ":", "\n", "                            ", "answer_locations", ".", "append", "(", "(", "row_idx", ",", "cell_idx", ")", ")", "\n", "\n", "", "", "table", ".", "visit", "(", "check_answer", ")", "\n", "has_answer", "=", "len", "(", "answer_locations", ")", ">", "0", "\n", "if", "has_answer", "and", "question_gold_positive_match", "is", "None", ":", "\n", "                        ", "question_gold_positive_match", "=", "table", "\n", "question_gold_positive_match", ".", "gold_match", "=", "True", "\n", "answers_links", "=", "answer_locations", "\n", "\n", "", "", "if", "question_gold_positive_match", "is", "None", ":", "\n", "                    ", "logger", ".", "info", "(", "\"No gold match for q=%s, q_id=%s\"", ",", "q", ",", "q_id", ")", "\n", "", "else", ":", "# inject into ctx+ at the first position", "\n", "                    ", "question_positives", ",", "hns", ",", "ans_links", "=", "dpr_results", "[", "q", "]", "\n", "question_positives", ".", "insert", "(", "0", ",", "question_gold_positive_match", ")", "\n", "ans_links", ".", "insert", "(", "0", ",", "answers_links", ")", "\n", "\n", "# return", "\n", "", "", "", "", "out_results", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "nq_table_file", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "q", "=", "jline", "[", "\"question\"", "]", "\n", "gold_positive_table", "=", "jline", "[", "\"contexts\"", "]", "[", "0", "]", "\n", "mask", "=", "gold_positive_table", "[", "\"html_mask\"", "]", "\n", "# page_url = jline['doc_url']", "\n", "title", "=", "jline", "[", "\"title\"", "]", "\n", "p", "=", "NQTableParser", "(", "gold_positive_table", "[", "\"tokens\"", "]", ",", "mask", ",", "title", ")", "\n", "tables", "=", "p", ".", "parse", "(", ")", "\n", "# select the one with the answer(s)", "\n", "prep_answers", "=", "prepare_answers", "(", "jline", "[", "\"short_answers\"", "]", ")", "\n", "\n", "tables_with_answers", "=", "[", "]", "\n", "tables_answer_locations", "=", "[", "]", "\n", "\n", "for", "t", "in", "tables", ":", "\n", "                ", "answer_locations", "=", "[", "]", "\n", "\n", "def", "check_answer", "(", "tokens", ",", "row_idx", ":", "int", ",", "cell_idx", ":", "int", ")", ":", "\n", "                    ", "if", "has_prepared_answer2", "(", "prep_answers", ",", "tokens", ")", ":", "\n", "                        ", "answer_locations", ".", "append", "(", "(", "row_idx", ",", "cell_idx", ")", ")", "\n", "\n", "", "", "t", ".", "visit", "(", "check_answer", ")", "\n", "has_answer", "=", "len", "(", "answer_locations", ")", ">", "0", "\n", "if", "has_answer", ":", "\n", "                    ", "tables_with_answers", ".", "append", "(", "t", ")", "\n", "tables_answer_locations", ".", "append", "(", "answer_locations", ")", "\n", "\n", "", "", "if", "not", "tables_with_answers", ":", "\n", "                ", "logger", ".", "info", "(", "\"No answer in gold table(s) for q=%s\"", ",", "q", ")", "\n", "# tables_with_answers.append(tables[0])", "\n", "\n", "", "positive_ctxs", ",", "hard_neg_ctxs", ",", "answers_table_links", "=", "dpr_results", "[", "q", "]", "\n", "positive_ctxs", "=", "positive_ctxs", "+", "tables_with_answers", "\n", "tables_answer_locations", "=", "answers_table_links", "+", "tables_answer_locations", "\n", "assert", "len", "(", "positive_ctxs", ")", "==", "len", "(", "tables_answer_locations", ")", "\n", "positive_ctxs", "=", "[", "t", ".", "to_dpr_json", "(", ")", "for", "t", "in", "positive_ctxs", "]", "\n", "\n", "# set has_answer attributes", "\n", "for", "i", ",", "ctx_json", "in", "enumerate", "(", "positive_ctxs", ")", ":", "\n", "                ", "answer_links", "=", "tables_answer_locations", "[", "i", "]", "\n", "ctx_json", "[", "\"answer_pos\"", "]", "=", "answer_links", "\n", "", "hard_neg_ctxs", "=", "[", "t", ".", "to_dpr_json", "(", ")", "for", "t", "in", "hard_neg_ctxs", "]", "\n", "out_results", ".", "append", "(", "\n", "{", "\n", "\"question\"", ":", "q", ",", "\n", "\"id\"", ":", "jline", "[", "\"example_id\"", "]", ",", "\n", "\"answers\"", ":", "jline", "[", "\"short_answers\"", "]", ",", "\n", "\"positive_ctxs\"", ":", "positive_ctxs", ",", "\n", "\"hard_negative_ctxs\"", ":", "hard_neg_ctxs", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"out_results size %s\"", ",", "len", "(", "out_results", ")", ")", "\n", "\n", "with", "jsonlines", ".", "open", "(", "\n", "out_file", ",", "mode", "=", "\"w\"", "\n", ")", "as", "writer", ":", "# encoding=\"utf-8\", .encode('utf-8')", "\n", "        ", "for", "r", "in", "out_results", ":", "\n", "            ", "writer", ".", "write", "(", "r", ")", "\n", "\n", "# with open(out_file, \"w\") as writer:", "\n", "#    writer.write(json.dumps(out_results, indent=4) + \"\\n\")  # indent=4", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Saved to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_long_ans_to_dpr": [[544, 581], ["logger.info", "logger.info", "jsonlines.open", "len", "jsonlines.open", "tables.NQTableParser", "tables.NQTableParser.parse", "out_results.append", "writer.write", "tables[].to_dpr_json"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.parse", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json"], ["", "def", "convert_long_ans_to_dpr", "(", "nq_table_file", ",", "out_file", ")", ":", "\n", "    ", "out_results", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "nq_table_file", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "q", "=", "jline", "[", "\"question\"", "]", "\n", "\n", "gold_positive_table", "=", "jline", "[", "\"contexts\"", "]", "\n", "\n", "mask", "=", "gold_positive_table", "[", "\"la_ans_tokens_html_mask\"", "]", "\n", "# page_url = jline['doc_url']", "\n", "title", "=", "jline", "[", "\"title\"", "]", "\n", "\n", "p", "=", "NQTableParser", "(", "gold_positive_table", "[", "\"la_ans_tokens\"", "]", ",", "mask", ",", "title", ")", "\n", "tables", "=", "p", ".", "parse", "(", ")", "\n", "# select the one with the answer(s)", "\n", "\n", "positive_ctxs", "=", "[", "tables", "[", "0", "]", ".", "to_dpr_json", "(", ")", "]", "\n", "\n", "out_results", ".", "append", "(", "\n", "{", "\n", "\"question\"", ":", "q", ",", "\n", "\"id\"", ":", "jline", "[", "\"example_id\"", "]", ",", "\n", "\"answers\"", ":", "[", "]", ",", "\n", "\"positive_ctxs\"", ":", "positive_ctxs", ",", "\n", "\"hard_negative_ctxs\"", ":", "[", "]", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"out_results size %s\"", ",", "len", "(", "out_results", ")", ")", "\n", "\n", "with", "jsonlines", ".", "open", "(", "\n", "out_file", ",", "mode", "=", "\"w\"", "\n", ")", "as", "writer", ":", "# encoding=\"utf-8\", .encode('utf-8')", "\n", "        ", "for", "r", "in", "out_results", ":", "\n", "            ", "writer", ".", "write", "(", "r", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Saved to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.parse_qa_csv_file": [[583, 592], ["open", "csv.reader", "eval", "res.append"], "function", ["None"], ["", "def", "parse_qa_csv_file", "(", "location", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "with", "open", "(", "location", ")", "as", "ifile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "question", "=", "row", "[", "0", "]", "\n", "answers", "=", "eval", "(", "row", "[", "1", "]", ")", "\n", "res", ".", "append", "(", "(", "question", ",", "answers", ")", ")", "\n", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.calc_questions_overlap": [[594, 632], ["set", "set", "logger.info", "logger.info", "logger.info", "jsonlines.open", "logger.info", "tables.parse_qa_csv_file", "len", "len", "len", "set.add", "set.add", "open", "logger.info", "json.load", "tables.parse_qa_csv_file", "set.intersection", "set.add", "set.add", "open", "logger.info", "json.load", "set.add"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.parse_qa_csv_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.parse_qa_csv_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "calc_questions_overlap", "(", "tables_file", ",", "regular_file", ",", "dev_file", ")", ":", "\n", "    ", "tab_questions", "=", "set", "(", ")", "\n", "\n", "with", "jsonlines", ".", "open", "(", "tables_file", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading file %s\"", "%", "tables_file", ")", "\n", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "q", "=", "jline", "[", "\"question\"", "]", "\n", "tab_questions", ".", "add", "(", "q", ")", "\n", "\n", "", "", "reg_questions", "=", "set", "(", ")", "\n", "\n", "if", "regular_file", "[", "-", "4", ":", "]", "==", "\".csv\"", ":", "\n", "        ", "qas", "=", "parse_qa_csv_file", "(", "regular_file", ")", "\n", "for", "qa", "in", "qas", ":", "\n", "            ", "reg_questions", ".", "add", "(", "qa", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "with", "open", "(", "regular_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading file %s\"", "%", "regular_file", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "item", "in", "data", ":", "\n", "                ", "q", "=", "item", "[", "\"question\"", "]", "\n", "reg_questions", ".", "add", "(", "q", ")", "\n", "", "", "", "if", "dev_file", ":", "\n", "        ", "if", "dev_file", "[", "-", "4", ":", "]", "==", "\".csv\"", ":", "\n", "            ", "qas", "=", "parse_qa_csv_file", "(", "dev_file", ")", "\n", "for", "qa", "in", "qas", ":", "\n", "                ", "reg_questions", ".", "add", "(", "qa", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "open", "(", "dev_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "logger", ".", "info", "(", "\"Reading file %s\"", "%", "dev_file", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "item", "in", "data", ":", "\n", "                    ", "q", "=", "item", "[", "\"question\"", "]", "\n", "reg_questions", ".", "add", "(", "q", ")", "\n", "\n", "", "", "", "", "logger", ".", "info", "(", "\"tab_questions %d\"", ",", "len", "(", "tab_questions", ")", ")", "\n", "logger", ".", "info", "(", "\"reg_questions %d\"", ",", "len", "(", "reg_questions", ")", ")", "\n", "logger", ".", "info", "(", "\"overlap %d\"", ",", "len", "(", "tab_questions", ".", "intersection", "(", "reg_questions", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.convert_train_jsonl_to_ctxmatch": [[634, 666], ["range", "jsonlines.open", "len", "tables.convert_train_jsonl_to_ctxmatch.get_table_string_for_ctx_match"], "function", ["None"], ["", "def", "convert_train_jsonl_to_ctxmatch", "(", "path", ":", "str", ",", "out_file", ":", "str", ")", ":", "\n", "    ", "def", "get_table_string_for_ctx_match", "(", "table", ":", "dict", ")", ":", "# this doesn't use caption", "\n", "        ", "table_text", "=", "table", "[", "\"caption\"", "]", "+", "\" . \"", "\n", "for", "r", "in", "table", "[", "\"rows\"", "]", ":", "\n", "            ", "table_text", "+=", "\" . \"", ".", "join", "(", "[", "c", "[", "\"value\"", "]", "for", "c", "in", "r", "[", "\"columns\"", "]", "]", ")", "\n", "", "table_text", "+=", "\" . \"", "\n", "return", "table_text", "\n", "\n", "", "results", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "path", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "if", "len", "(", "jline", "[", "\"positive_ctxs\"", "]", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "ctx_pos", "=", "jline", "[", "\"positive_ctxs\"", "]", "[", "0", "]", "\n", "table_str", "=", "get_table_string_for_ctx_match", "(", "ctx_pos", ")", "\n", "q", "=", "jline", "[", "\"question\"", "]", "\n", "results", ".", "append", "(", "(", "q", ",", "table_str", ")", ")", "\n", "\n", "if", "len", "(", "results", ")", "%", "1000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"results %d\"", ",", "len", "(", "results", ")", ")", "\n", "\n", "", "", "", "shards_sz", "=", "3000", "\n", "shard", "=", "0", "\n", "\n", "for", "s", "in", "range", "(", "0", ",", "len", "(", "results", ")", ",", "shards_sz", ")", ":", "\n", "        ", "chunk", "=", "results", "[", "s", ":", "s", "+", "shards_sz", "]", "\n", "shard_file", "=", "out_file", "+", "\".shard_{}\"", ".", "format", "(", "shard", ")", "\n", "with", "jsonlines", ".", "open", "(", "shard_file", ",", "mode", "=", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"Saving to %s\"", ",", "shard_file", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "chunk", ")", ":", "\n", "                ", "writer", ".", "write", "(", "{", "\"id\"", ":", "s", "+", "i", ",", "\"question\"", ":", "item", "[", "0", "]", ",", "\"context\"", ":", "item", "[", "1", "]", "}", ")", "\n", "", "", "shard", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.regex_match": [[668, 675], ["re.compile", "re.compile.search"], "function", ["None"], ["", "", "def", "regex_match", "(", "text", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Test if a regex pattern is contained within a text.\"\"\"", "\n", "try", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "pattern", ",", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ")", "\n", "", "except", "BaseException", ":", "\n", "        ", "return", "False", "\n", "", "return", "pattern", ".", "search", "(", "text", ")", "is", "not", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_matches": [[36, 82], ["logger.info", "dpr.utils.tokenizers.SimpleTokenizer", "multiprocessing.Pool", "logger.info", "functools.partial", "zip", "multiprocessing.Pool.map", "logger.info", "len", "QAMatchStats", "len", "len", "next", "enumerate"], "function", ["None"], ["def", "calculate_matches", "(", "\n", "all_docs", ":", "Dict", "[", "object", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "closest_docs", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "QAMatchStats", ":", "\n", "    ", "\"\"\"\n    Evaluates answers presence in the set of documents. This function is supposed to be used with a large collection of\n    documents and results. It internally forks multiple sub-processes for evaluation and then merges results\n    :param all_docs: dictionary of the entire documents database. doc_id -> (doc_text, title)\n    :param answers: list of answers's list. One list per question\n    :param closest_docs: document ids of the top results along with their scores\n    :param workers_num: amount of parallel threads to process data\n    :param match_type: type of answer matching. Refer to has_answer code for available options\n    :return: matching information tuple.\n    top_k_hits - a list where the index is the amount of top documents retrieved and the value is the total amount of\n    valid matches across an entire dataset.\n    questions_doc_hits - more detailed info with answer matches for every question and every retrieved document\n    \"\"\"", "\n", "global", "dpr_all_documents", "\n", "dpr_all_documents", "=", "all_docs", "\n", "logger", ".", "info", "(", "\"dpr_all_documents size %d\"", ",", "len", "(", "dpr_all_documents", ")", ")", "\n", "\n", "tok_opts", "=", "{", "}", "\n", "tokenizer", "=", "SimpleTokenizer", "(", "**", "tok_opts", ")", "\n", "\n", "processes", "=", "ProcessPool", "(", "processes", "=", "workers_num", ")", "\n", "logger", ".", "info", "(", "\"Matching answers in top docs...\"", ")", "\n", "get_score_partial", "=", "partial", "(", "\n", "check_answer", ",", "match_type", "=", "match_type", ",", "tokenizer", "=", "tokenizer", "\n", ")", "\n", "\n", "questions_answers_docs", "=", "zip", "(", "answers", ",", "closest_docs", ")", "\n", "scores", "=", "processes", ".", "map", "(", "get_score_partial", ",", "questions_answers_docs", ")", "\n", "\n", "logger", ".", "info", "(", "\"Per question validation results len=%d\"", ",", "len", "(", "scores", ")", ")", "\n", "\n", "n_docs", "=", "len", "(", "closest_docs", "[", "0", "]", "[", "0", "]", ")", "\n", "top_k_hits", "=", "[", "0", "]", "*", "n_docs", "\n", "for", "question_hits", "in", "scores", ":", "\n", "        ", "best_hit", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "question_hits", ")", "if", "x", ")", ",", "None", ")", "\n", "if", "best_hit", "is", "not", "None", ":", "\n", "            ", "top_k_hits", "[", "best_hit", ":", "]", "=", "[", "v", "+", "1", "for", "v", "in", "top_k_hits", "[", "best_hit", ":", "]", "]", "\n", "\n", "", "", "return", "QAMatchStats", "(", "top_k_hits", ",", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.check_answer": [[84, 105], ["enumerate", "qa_validation.has_answer", "hits.append", "logger.warning", "hits.append"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.has_answer"], ["", "def", "check_answer", "(", "questions_answers_docs", ",", "tokenizer", ",", "match_type", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "\"\"\"Search through all the top docs to see if they have any of the answers.\"\"\"", "\n", "answers", ",", "(", "doc_ids", ",", "doc_scores", ")", "=", "questions_answers_docs", "\n", "\n", "global", "dpr_all_documents", "\n", "hits", "=", "[", "]", "\n", "\n", "for", "i", ",", "doc_id", "in", "enumerate", "(", "doc_ids", ")", ":", "\n", "        ", "doc", "=", "dpr_all_documents", "[", "doc_id", "]", "\n", "text", "=", "doc", "[", "0", "]", "\n", "\n", "answer_found", "=", "False", "\n", "if", "text", "is", "None", ":", "# cannot find the document for some reason", "\n", "            ", "logger", ".", "warning", "(", "\"no doc in db\"", ")", "\n", "hits", ".", "append", "(", "False", ")", "\n", "continue", "\n", "\n", "", "if", "has_answer", "(", "answers", ",", "text", ",", "tokenizer", ",", "match_type", ")", ":", "\n", "            ", "answer_found", "=", "True", "\n", "", "hits", ".", "append", "(", "answer_found", ")", "\n", "", "return", "hits", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.has_answer": [[107, 134], ["qa_validation._normalize", "tokenizer.tokenize().words", "qa_validation._normalize", "tokenizer.tokenize", "_normalize.words", "range", "tokenizer.tokenize", "qa_validation._normalize", "qa_validation.regex_match", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.words", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.tokenizers.Tokens.words", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.regex_match"], ["", "def", "has_answer", "(", "answers", ",", "text", ",", "tokenizer", ",", "match_type", ")", "->", "bool", ":", "\n", "    ", "\"\"\"Check if a document contains an answer string.\n    If `match_type` is string, token matching is done between the text and answer.\n    If `match_type` is regex, we search the whole text with the regex.\n    \"\"\"", "\n", "text", "=", "_normalize", "(", "text", ")", "\n", "\n", "if", "match_type", "==", "\"string\"", ":", "\n", "# Answer is a list of possible strings", "\n", "        ", "text", "=", "tokenizer", ".", "tokenize", "(", "text", ")", ".", "words", "(", "uncased", "=", "True", ")", "\n", "\n", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "_normalize", "(", "single_answer", ")", "\n", "single_answer", "=", "tokenizer", ".", "tokenize", "(", "single_answer", ")", "\n", "single_answer", "=", "single_answer", ".", "words", "(", "uncased", "=", "True", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "                ", "if", "single_answer", "==", "text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                    ", "return", "True", "\n", "\n", "", "", "", "", "elif", "match_type", "==", "\"regex\"", ":", "\n", "# Answer is a regex", "\n", "        ", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "_normalize", "(", "single_answer", ")", "\n", "if", "regex_match", "(", "text", ",", "single_answer", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.regex_match": [[136, 143], ["regex.compile", "re.compile.search"], "function", ["None"], ["", "def", "regex_match", "(", "text", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Test if a regex pattern is contained within a text.\"\"\"", "\n", "try", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "pattern", ",", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ")", "\n", "", "except", "BaseException", ":", "\n", "        ", "return", "False", "\n", "", "return", "pattern", ".", "search", "(", "text", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.exact_match_score": [[146, 148], ["qa_validation._normalize_answer", "qa_validation._normalize_answer"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize_answer", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "_normalize_answer", "(", "prediction", ")", "==", "_normalize_answer", "(", "ground_truth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize_answer": [[150, 165], ["qa_validation._normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "_normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r\"\\b(a|an|the)\\b\"", ",", "\" \"", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "\"\"", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation._normalize": [[167, 169], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize"], ["", "def", "_normalize", "(", "text", ")", ":", "\n", "    ", "return", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.qa_validation.calculate_chunked_matches": [[171, 217], ["all_docs.items", "dpr.utils.tokenizers.SimpleTokenizer", "multiprocessing.Pool", "logger.info", "functools.partial", "zip", "multiprocessing.Pool.map", "logger.info", "len", "QATableMatchStats", "dpr_all_tables.get", "dpr_all_tables.get.append", "len", "next", "next", "enumerate", "enumerate"], "function", ["None"], ["", "def", "calculate_chunked_matches", "(", "\n", "all_docs", ":", "Dict", "[", "object", ",", "TableChunk", "]", ",", "\n", "answers", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "closest_docs", ":", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ",", "\n", "workers_num", ":", "int", ",", "\n", "match_type", ":", "str", ",", "\n", ")", "->", "QATableMatchStats", ":", "\n", "    ", "global", "dpr_all_documents", "\n", "dpr_all_documents", "=", "all_docs", "\n", "\n", "global", "dpr_all_tables", "\n", "dpr_all_tables", "=", "{", "}", "\n", "\n", "for", "key", ",", "table_chunk", "in", "all_docs", ".", "items", "(", ")", ":", "\n", "        ", "table_str", ",", "title", ",", "table_id", "=", "table_chunk", "\n", "table_chunks", "=", "dpr_all_tables", ".", "get", "(", "table_id", ",", "[", "]", ")", "\n", "table_chunks", ".", "append", "(", "(", "table_str", ",", "title", ")", ")", "\n", "dpr_all_tables", "[", "table_id", "]", "=", "table_chunks", "\n", "\n", "", "tok_opts", "=", "{", "}", "\n", "tokenizer", "=", "SimpleTokenizer", "(", "**", "tok_opts", ")", "\n", "\n", "processes", "=", "ProcessPool", "(", "processes", "=", "workers_num", ")", "\n", "\n", "logger", ".", "info", "(", "\"Matching answers in top docs...\"", ")", "\n", "get_score_partial", "=", "partial", "(", "\n", "check_chunked_docs_answer", ",", "match_type", "=", "match_type", ",", "tokenizer", "=", "tokenizer", "\n", ")", "\n", "questions_answers_docs", "=", "zip", "(", "answers", ",", "closest_docs", ")", "\n", "scores", "=", "processes", ".", "map", "(", "get_score_partial", ",", "questions_answers_docs", ")", "\n", "logger", ".", "info", "(", "\"Per question validation results len=%d\"", ",", "len", "(", "scores", ")", ")", "\n", "\n", "n_docs", "=", "len", "(", "closest_docs", "[", "0", "]", "[", "0", "]", ")", "\n", "top_k_hits", "=", "[", "0", "]", "*", "n_docs", "\n", "top_k_orig_hits", "=", "[", "0", "]", "*", "n_docs", "\n", "for", "s", "in", "scores", ":", "\n", "        ", "question_hits", ",", "question_orig_doc_hits", "=", "s", "\n", "best_hit", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "question_hits", ")", "if", "x", ")", ",", "None", ")", "\n", "if", "best_hit", "is", "not", "None", ":", "\n", "            ", "top_k_hits", "[", "best_hit", ":", "]", "=", "[", "v", "+", "1", "for", "v", "in", "top_k_hits", "[", "best_hit", ":", "]", "]", "\n", "\n", "", "best_hit", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "question_orig_doc_hits", ")", "if", "x", ")", ",", "None", ")", "\n", "if", "best_hit", "is", "not", "None", ":", "\n", "            ", "top_k_orig_hits", "[", "best_hit", ":", "]", "=", "[", "v", "+", "1", "for", "v", "in", "top_k_orig_hits", "[", "best_hit", ":", "]", "]", "\n", "\n", "", "", "return", "QATableMatchStats", "(", "top_k_hits", ",", "top_k_orig_hits", ",", "scores", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.__init__": [[40, 62], ["dpr.data.biencoder_data.Dataset.__init__", "logger.info"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_token", ":", "str", "=", "None", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "selector", ",", "\n", "special_token", "=", "special_token", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "shuffle_positives", "=", "shuffle_positives", ",", "\n", "query_special_suffix", "=", "query_special_suffix", ",", "\n", ")", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "normalize", "=", "normalize", "\n", "logger", ".", "info", "(", "\"Data files: %s\"", ",", "self", ".", "data_files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.load_data": [[63, 73], ["print", "dpr.utils.data_utils.read_data_from_json_files", "logger.info", "os.path.exists", "os.path.join", "len", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.read_data_from_json_files"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "self", ".", "file", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "data_files", ")", ":", "\n", "            ", "self", ".", "data_files", "=", "os", ".", "path", ".", "join", "(", "PROJ_PATH", ",", "self", ".", "data_files", ")", "\n", "", "print", "(", "\"read file\"", ",", "self", ".", "data_files", ")", "\n", "data", "=", "read_data_from_json_files", "(", "self", ".", "data_files", ")", "\n", "# filter those without positive ctx", "\n", "self", ".", "data", "=", "[", "r", "for", "r", "in", "data", "if", "len", "(", "r", "[", "\"ctxs\"", "]", ")", ">", "0", "]", "\n", "logger", ".", "info", "(", "\"Total cleaned data size: {}\"", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.__getitem__": [[74, 104], ["okvqa_data.VisBiEncoderSample", "okvqa_data.JsonQADataset._process_query", "dpr.data.biencoder_data.BiEncoderPassage", "okvqa_data.JsonQADataset.__getitem__.create_passage"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset._process_query"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "json_sample", "=", "self", ".", "data", "[", "index", "]", "\n", "r", "=", "VisBiEncoderSample", "(", ")", "\n", "r", ".", "query", "=", "self", ".", "_process_query", "(", "json_sample", "[", "\"question\"", "]", ")", "\n", "\n", "positive_ctxs", "=", "json_sample", "[", "\"ctxs\"", "]", "\n", "negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"negative_ctxs\"", "]", "if", "\"negative_ctxs\"", "in", "json_sample", "else", "[", "]", "\n", ")", "\n", "hard_negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "if", "\"hard_negative_ctxs\"", "in", "json_sample", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "for", "ctx", "in", "positive_ctxs", "+", "negative_ctxs", "+", "hard_negative_ctxs", ":", "\n", "            ", "if", "\"title\"", "not", "in", "ctx", ":", "\n", "                ", "ctx", "[", "\"title\"", "]", "=", "None", "\n", "\n", "", "", "def", "create_passage", "(", "ctx", ":", "dict", ")", ":", "\n", "            ", "return", "BiEncoderPassage", "(", "\n", "normalize_passage", "(", "ctx", "[", "\"text\"", "]", ")", "if", "self", ".", "normalize", "else", "ctx", "[", "\"text\"", "]", ",", "\n", "ctx", "[", "\"title\"", "]", ",", "\n", ")", "\n", "\n", "", "r", ".", "positive_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "positive_ctxs", "]", "\n", "r", ".", "negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "negative_ctxs", "]", "\n", "r", ".", "hard_negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "hard_negative_ctxs", "]", "\n", "r", ".", "img_id", "=", "json_sample", "[", "'img_id'", "]", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.__len__": [[105, 107], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.get_qas": [[108, 110], ["None"], "methods", ["None"], ["", "def", "get_qas", "(", "self", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "return", "[", "s", "[", "\"question\"", "]", "for", "s", "in", "self", ".", "data", "]", ",", "[", "s", "[", "\"answers\"", "]", "for", "s", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.JsonQADataset.get_qas_range": [[111, 117], ["None"], "methods", ["None"], ["", "def", "get_qas_range", "(", "\n", "self", ",", "start_idx", ":", "int", ",", "end_idx", ":", "int", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "return", "(", "\n", "[", "s", "[", "\"question\"", "]", "for", "s", "in", "self", ".", "data", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", "[", "s", "[", "\"answers\"", "]", "for", "s", "in", "self", ".", "data", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.RetrieverData.__init__": [[120, 126], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        :param file: - real file name or the resource name as they are defined in download_data.py\n        \"\"\"", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.RetrieverData.load_data": [[127, 133], ["len"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "[", "self", ".", "file", "]", "\n", "assert", "(", "\n", "len", "(", "self", ".", "data_files", ")", "==", "1", "\n", ")", ",", "\"RetrieverData source currently works with single files only. Files specified: {}\"", ".", "format", "(", "\n", "self", ".", "data_files", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.CsvCtxSrc.__init__": [[137, 152], ["okvqa_data.RetrieverData.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "id_col", ":", "int", "=", "0", ",", "\n", "text_col", ":", "int", "=", "1", ",", "\n", "title_col", ":", "int", "=", "2", ",", "\n", "id_prefix", ":", "str", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ")", "\n", "self", ".", "text_col", "=", "text_col", "\n", "self", ".", "title_col", "=", "title_col", "\n", "self", ".", "id_col", "=", "id_col", "\n", "self", ".", "id_prefix", "=", "id_prefix", "\n", "self", ".", "normalize", "=", "normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.okvqa_data.CsvCtxSrc.load_data_to": [[153, 170], ["okvqa_data.RetrieverData.load_data", "os.path.exists", "os.path.join", "open", "csv.reader", "dpr.data.biencoder_data.BiEncoderPassage", "dpr.data.biencoder_data.normalize_passage", "str"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_passage"], ["", "def", "load_data_to", "(", "self", ",", "ctxs", ":", "Dict", "[", "object", ",", "BiEncoderPassage", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "file", ")", ":", "\n", "            ", "self", ".", "file", "=", "os", ".", "path", ".", "join", "(", "PROJ_PATH", ",", "self", ".", "file", ")", "\n", "", "with", "open", "(", "self", ".", "file", ")", "as", "ifile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\",\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "row", "[", "self", ".", "id_col", "]", "==", "\"kid\"", ":", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "id_prefix", ":", "\n", "                    ", "sample_id", "=", "self", ".", "id_prefix", "+", "str", "(", "row", "[", "self", ".", "id_col", "]", ")", "\n", "", "else", ":", "\n", "                    ", "sample_id", "=", "row", "[", "self", ".", "id_col", "]", "\n", "", "passage", "=", "row", "[", "self", ".", "text_col", "]", "\n", "if", "self", ".", "normalize", ":", "\n", "                    ", "passage", "=", "normalize_passage", "(", "passage", ")", "\n", "", "ctxs", "[", "sample_id", "]", "=", "BiEncoderPassage", "(", "passage", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepTokenSelector.get_positions": [[40, 42], ["None"], "methods", ["None"], ["    ", "def", "get_positions", "(", "self", ",", "input_ids", ":", "T", ",", "tenzorizer", ":", "Tensorizer", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepStaticPosTokenSelector.__init__": [[45, 47], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "static_position", ":", "int", "=", "0", ")", ":", "\n", "        ", "self", ".", "static_position", "=", "static_position", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepStaticPosTokenSelector.get_positions": [[48, 50], ["None"], "methods", ["None"], ["", "def", "get_positions", "(", "self", ",", "input_ids", ":", "T", ",", "tenzorizer", ":", "Tensorizer", ")", ":", "\n", "        ", "return", "self", ".", "static_position", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.__init__": [[53, 56], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token", ":", "str", "=", "\"[CLS]\"", ")", ":", "\n", "        ", "self", ".", "token", "=", "token", "\n", "self", ".", "token_id", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.RepSpecificTokenSelector.get_positions": [[57, 84], ["input_ids.size", "range", "torch.stack", "tenzorizer.get_token_id", "token_indexes.size", "torch.stack.append", "logger.warning", "torch.stack.append", "token_indexes.size", "torch.tensor().to", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_token_id"], ["", "def", "get_positions", "(", "self", ",", "input_ids", ":", "T", ",", "tenzorizer", ":", "Tensorizer", ")", ":", "\n", "        ", "if", "not", "self", ".", "token_id", ":", "\n", "            ", "self", ".", "token_id", "=", "tenzorizer", ".", "get_token_id", "(", "self", ".", "token", ")", "\n", "", "token_indexes", "=", "(", "input_ids", "==", "self", ".", "token_id", ")", ".", "nonzero", "(", ")", "\n", "# check if all samples in input_ids has index presence and out a default value otherwise", "\n", "bsz", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "if", "bsz", "==", "token_indexes", ".", "size", "(", "0", ")", ":", "\n", "            ", "return", "token_indexes", "\n", "\n", "", "token_indexes_result", "=", "[", "]", "\n", "found_idx_cnt", "=", "0", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "            ", "if", "(", "\n", "found_idx_cnt", "<", "token_indexes", ".", "size", "(", "0", ")", "\n", "and", "token_indexes", "[", "found_idx_cnt", "]", "[", "0", "]", "==", "i", "\n", ")", ":", "\n", "# this samples has the special token", "\n", "                ", "token_indexes_result", ".", "append", "(", "token_indexes", "[", "found_idx_cnt", "]", ")", "\n", "found_idx_cnt", "+=", "1", "\n", "", "else", ":", "\n", "                ", "logger", ".", "warning", "(", "\"missing special token %s\"", ",", "input_ids", "[", "i", "]", ")", "\n", "\n", "token_indexes_result", ".", "append", "(", "\n", "torch", ".", "tensor", "(", "[", "i", ",", "0", "]", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", ")", "# setting 0-th token, i.e. CLS for BERT as the special one", "\n", "", "", "token_indexes_result", "=", "torch", ".", "stack", "(", "token_indexes_result", ",", "dim", "=", "0", ")", "\n", "return", "token_indexes_result", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset.__init__": [[90, 106], ["hydra.utils.instantiate"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_token", ":", "str", "=", "None", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "selector", ":", "\n", "            ", "self", ".", "selector", "=", "hydra", ".", "utils", ".", "instantiate", "(", "selector", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "selector", "=", "DEFAULT_SELECTOR", "\n", "", "self", ".", "special_token", "=", "special_token", "\n", "self", ".", "encoder_type", "=", "encoder_type", "\n", "self", ".", "shuffle_positives", "=", "shuffle_positives", "\n", "self", ".", "query_special_suffix", "=", "query_special_suffix", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset.load_data": [[107, 109], ["None"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset.__getitem__": [[110, 112], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", "->", "BiEncoderSample", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset._process_query": [[113, 120], ["biencoder_data.normalize_question", "normalize_question.endswith"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_question"], ["", "def", "_process_query", "(", "self", ",", "query", ":", "str", ")", ":", "\n", "# as of now, always normalize query", "\n", "        ", "query", "=", "normalize_question", "(", "query", ")", "\n", "if", "self", ".", "query_special_suffix", "and", "not", "query", ".", "endswith", "(", "self", ".", "query_special_suffix", ")", ":", "\n", "            ", "query", "+=", "self", ".", "query_special_suffix", "\n", "\n", "", "return", "query", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.__init__": [[133, 155], ["biencoder_data.Dataset.__init__", "logger.info"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_token", ":", "str", "=", "None", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "selector", ",", "\n", "special_token", "=", "special_token", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "shuffle_positives", "=", "shuffle_positives", ",", "\n", "query_special_suffix", "=", "query_special_suffix", ",", "\n", ")", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "normalize", "=", "normalize", "\n", "logger", ".", "info", "(", "\"Data files: %s\"", ",", "self", ".", "data_files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.load_data": [[156, 162], ["biencoder_data.get_dpr_files", "dpr.utils.data_utils.read_data_from_json_files", "logger.info", "len", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.get_dpr_files", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.read_data_from_json_files"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "get_dpr_files", "(", "self", ".", "file", ")", "\n", "data", "=", "read_data_from_json_files", "(", "self", ".", "data_files", ")", "\n", "# filter those without positive ctx", "\n", "self", ".", "data", "=", "[", "r", "for", "r", "in", "data", "if", "len", "(", "r", "[", "\"positive_ctxs\"", "]", ")", ">", "0", "]", "\n", "logger", ".", "info", "(", "\"Total cleaned data size: {}\"", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.__getitem__": [[163, 192], ["biencoder_data.BiEncoderSample", "biencoder_data.JsonQADataset._process_query", "BiEncoderPassage", "biencoder_data.JsonQADataset.__getitem__.create_passage"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Dataset._process_query"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", "->", "BiEncoderSample", ":", "\n", "        ", "json_sample", "=", "self", ".", "data", "[", "index", "]", "\n", "r", "=", "BiEncoderSample", "(", ")", "\n", "r", ".", "query", "=", "self", ".", "_process_query", "(", "json_sample", "[", "\"question\"", "]", ")", "\n", "\n", "positive_ctxs", "=", "json_sample", "[", "\"positive_ctxs\"", "]", "\n", "negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"negative_ctxs\"", "]", "if", "\"negative_ctxs\"", "in", "json_sample", "else", "[", "]", "\n", ")", "\n", "hard_negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "if", "\"hard_negative_ctxs\"", "in", "json_sample", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "for", "ctx", "in", "positive_ctxs", "+", "negative_ctxs", "+", "hard_negative_ctxs", ":", "\n", "            ", "if", "\"title\"", "not", "in", "ctx", ":", "\n", "                ", "ctx", "[", "\"title\"", "]", "=", "None", "\n", "\n", "", "", "def", "create_passage", "(", "ctx", ":", "dict", ")", ":", "\n", "            ", "return", "BiEncoderPassage", "(", "\n", "normalize_passage", "(", "ctx", "[", "\"text\"", "]", ")", "if", "self", ".", "normalize", "else", "ctx", "[", "\"text\"", "]", ",", "\n", "ctx", "[", "\"title\"", "]", ",", "\n", ")", "\n", "\n", "", "r", ".", "positive_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "positive_ctxs", "]", "\n", "r", ".", "negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "negative_ctxs", "]", "\n", "r", ".", "hard_negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "hard_negative_ctxs", "]", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.__len__": [[193, 195], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.get_qas": [[196, 198], ["None"], "methods", ["None"], ["", "def", "get_qas", "(", "self", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "return", "[", "s", "[", "\"question\"", "]", "for", "s", "in", "self", ".", "data", "]", ",", "[", "s", "[", "\"answers\"", "]", "for", "s", "in", "self", ".", "data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonQADataset.get_qas_range": [[199, 205], ["None"], "methods", ["None"], ["", "def", "get_qas_range", "(", "\n", "self", ",", "start_idx", ":", "int", ",", "end_idx", ":", "int", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "        ", "return", "(", "\n", "[", "s", "[", "\"question\"", "]", "for", "s", "in", "self", ".", "data", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", "[", "s", "[", "\"answers\"", "]", "for", "s", "in", "self", ".", "data", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Cell.__init__": [[219, 223], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "value_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "self", ".", "type", ":", "str", "=", "\"\"", "\n", "self", ".", "nested_tables", ":", "List", "[", "Table", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Cell.__str__": [[224, 226], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "self", ".", "value_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Cell.to_dpr_json": [[227, 231], ["str"], "methods", ["None"], ["", "def", "to_dpr_json", "(", "self", ",", "cell_idx", ":", "int", ")", ":", "\n", "        ", "r", "=", "{", "\"col\"", ":", "cell_idx", "}", "\n", "r", "[", "\"value\"", "]", "=", "str", "(", "self", ")", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Row.__init__": [[234, 236], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "cells", ":", "List", "[", "Cell", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Row.__str__": [[237, 239], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"| \"", ".", "join", "(", "[", "str", "(", "c", ")", "for", "c", "in", "self", ".", "cells", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Row.visit": [[240, 244], ["enumerate", "tokens_function"], "methods", ["None"], ["", "def", "visit", "(", "self", ",", "tokens_function", ",", "row_idx", ":", "int", ")", ":", "\n", "        ", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "cells", ")", ":", "\n", "            ", "if", "c", ".", "value_tokens", ":", "\n", "                ", "tokens_function", "(", "c", ".", "value_tokens", ",", "row_idx", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Row.to_dpr_json": [[245, 249], ["c.to_dpr_json", "enumerate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json"], ["", "", "", "def", "to_dpr_json", "(", "self", ",", "row_idx", ":", "int", ")", ":", "\n", "        ", "r", "=", "{", "\"row\"", ":", "row_idx", "}", "\n", "r", "[", "\"columns\"", "]", "=", "[", "c", ".", "to_dpr_json", "(", "i", ")", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "cells", ")", "]", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.__init__": [[252, 257], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "caption", "=", "\"\"", ")", ":", "\n", "        ", "self", ".", "caption", "=", "caption", "\n", "self", ".", "body", ":", "List", "[", "Row", "]", "=", "[", "]", "\n", "self", ".", "key", "=", "None", "\n", "self", ".", "gold_match", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.__str__": [[258, 265], ["enumerate", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "table_str", "=", "\"<T>: {}\\n\"", ".", "format", "(", "self", ".", "caption", ")", "\n", "table_str", "+=", "\" rows:\\n\"", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", ":", "\n", "            ", "table_str", "+=", "\" row #{}: {}\\n\"", ".", "format", "(", "i", ",", "str", "(", "r", ")", ")", "\n", "\n", "", "return", "table_str", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.get_key": [[266, 270], ["str"], "methods", ["None"], ["", "def", "get_key", "(", "self", ")", "->", "str", ":", "\n", "        ", "if", "not", "self", ".", "key", ":", "\n", "            ", "self", ".", "key", "=", "str", "(", "self", ")", "\n", "", "return", "self", ".", "key", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit": [[271, 276], ["enumerate", "tokens_function", "r.visit"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.visit"], ["", "def", "visit", "(", "self", ",", "tokens_function", ",", "include_caption", ":", "bool", "=", "False", ")", "->", "bool", ":", "\n", "        ", "if", "include_caption", ":", "\n", "            ", "tokens_function", "(", "self", ".", "caption", ",", "-", "1", ",", "-", "1", ")", "\n", "", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", ":", "\n", "            ", "r", ".", "visit", "(", "tokens_function", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json": [[277, 285], ["r.to_dpr_json", "enumerate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json"], ["", "", "def", "to_dpr_json", "(", "self", ")", ":", "\n", "        ", "r", "=", "{", "\n", "\"caption\"", ":", "self", ".", "caption", ",", "\n", "\"rows\"", ":", "[", "r", ".", "to_dpr_json", "(", "i", ")", "for", "i", ",", "r", "in", "enumerate", "(", "self", ".", "body", ")", "]", ",", "\n", "}", "\n", "if", "self", ".", "gold_match", ":", "\n", "            ", "r", "[", "\"gold_match\"", "]", "=", "1", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.__init__": [[288, 297], ["len", "collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ",", "is_html_mask", ",", "title", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "is_html_mask", "=", "is_html_mask", "\n", "self", ".", "max_idx", "=", "len", "(", "self", ".", "tokens", ")", "\n", "self", ".", "all_tables", "=", "[", "]", "\n", "\n", "self", ".", "current_table", ":", "Table", "=", "None", "\n", "self", ".", "tables_stack", "=", "collections", ".", "deque", "(", ")", "\n", "self", ".", "title", "=", "title", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.parse": [[298, 325], ["collections.deque", "range", "biencoder_data.NQTableParser._on_content", "biencoder_data.NQTableParser._on_table_start", "biencoder_data.NQTableParser._on_table_end", "biencoder_data.NQTableParser._onRowStart", "biencoder_data.NQTableParser._onRowEnd", "biencoder_data.NQTableParser._onCellStart", "biencoder_data.NQTableParser._on_cell_end"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_content", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_start", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_end", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowStart", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowEnd", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onCellStart", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_cell_end"], ["", "def", "parse", "(", "self", ")", "->", "List", "[", "Table", "]", ":", "\n", "        ", "self", ".", "all_tables", "=", "[", "]", "\n", "self", ".", "tables_stack", "=", "collections", ".", "deque", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "max_idx", ")", ":", "\n", "\n", "            ", "t", "=", "self", ".", "tokens", "[", "i", "]", "\n", "\n", "if", "not", "self", ".", "is_html_mask", "[", "i", "]", ":", "\n", "# cell content", "\n", "                ", "self", ".", "_on_content", "(", "t", ")", "\n", "continue", "\n", "\n", "", "if", "\"<Table\"", "in", "t", ":", "\n", "                ", "self", ".", "_on_table_start", "(", ")", "\n", "", "elif", "t", "==", "\"</Table>\"", ":", "\n", "                ", "self", ".", "_on_table_end", "(", ")", "\n", "", "elif", "\"<Tr\"", "in", "t", ":", "\n", "                ", "self", ".", "_onRowStart", "(", ")", "\n", "", "elif", "t", "==", "\"</Tr>\"", ":", "\n", "                ", "self", ".", "_onRowEnd", "(", ")", "\n", "", "elif", "\"<Td\"", "in", "t", "or", "\"<Th\"", "in", "t", ":", "\n", "                ", "self", ".", "_onCellStart", "(", ")", "\n", "", "elif", "t", "in", "[", "\"</Td>\"", ",", "\"</Th>\"", "]", ":", "\n", "                ", "self", ".", "_on_cell_end", "(", ")", "\n", "\n", "", "", "return", "self", ".", "all_tables", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_start": [[326, 341], ["biencoder_data.Table", "biencoder_data.NQTableParser.all_tables.append", "biencoder_data.NQTableParser.tables_stack.append"], "methods", ["None"], ["", "def", "_on_table_start", "(", "self", ")", ":", "\n", "        ", "caption", "=", "self", ".", "title", "\n", "parent_table", "=", "self", ".", "current_table", "\n", "if", "parent_table", ":", "\n", "            ", "self", ".", "tables_stack", ".", "append", "(", "parent_table", ")", "\n", "\n", "caption", "=", "parent_table", ".", "caption", "\n", "if", "parent_table", ".", "body", "and", "parent_table", ".", "body", "[", "-", "1", "]", ".", "cells", ":", "\n", "                ", "current_cell", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", ".", "cells", "[", "-", "1", "]", "\n", "caption", "+=", "\" | \"", "+", "\" \"", ".", "join", "(", "current_cell", ".", "value_tokens", ")", "\n", "\n", "", "", "t", "=", "Table", "(", ")", "\n", "t", ".", "caption", "=", "caption", "\n", "self", ".", "current_table", "=", "t", "\n", "self", ".", "all_tables", ".", "append", "(", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_table_end": [[342, 352], ["logger.error", "biencoder_data.NQTableParser.tables_stack.pop", "current_cell.nested_tables.append"], "methods", ["None"], ["", "def", "_on_table_end", "(", "self", ")", ":", "\n", "        ", "t", "=", "self", ".", "current_table", "\n", "if", "t", ":", "\n", "            ", "if", "self", ".", "tables_stack", ":", "# t is a nested table", "\n", "                ", "self", ".", "current_table", "=", "self", ".", "tables_stack", ".", "pop", "(", ")", "\n", "if", "self", ".", "current_table", ".", "body", ":", "\n", "                    ", "current_cell", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", ".", "cells", "[", "-", "1", "]", "\n", "current_cell", ".", "nested_tables", ".", "append", "(", "t", ")", "\n", "", "", "", "else", ":", "\n", "            ", "logger", ".", "error", "(", "\"table end without table object\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowStart": [[353, 355], ["biencoder_data.NQTableParser.current_table.body.append", "biencoder_data.Row"], "methods", ["None"], ["", "", "def", "_onRowStart", "(", "self", ")", ":", "\n", "        ", "self", ".", "current_table", ".", "body", ".", "append", "(", "Row", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onRowEnd": [[356, 358], ["None"], "methods", ["None"], ["", "def", "_onRowEnd", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._onCellStart": [[359, 362], ["current_row.cells.append", "biencoder_data.Cell"], "methods", ["None"], ["", "def", "_onCellStart", "(", "self", ")", ":", "\n", "        ", "current_row", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", "\n", "current_row", ".", "cells", ".", "append", "(", "Cell", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_cell_end": [[363, 365], ["None"], "methods", ["None"], ["", "def", "_on_cell_end", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser._on_content": [[366, 373], ["current_cell.value_tokens.append"], "methods", ["None"], ["", "def", "_on_content", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "self", ".", "current_table", ".", "body", ":", "\n", "            ", "current_row", "=", "self", ".", "current_table", ".", "body", "[", "-", "1", "]", "\n", "current_cell", "=", "current_row", ".", "cells", "[", "-", "1", "]", "\n", "current_cell", ".", "value_tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "# tokens outside of row/cells. Just append to the table caption.", "\n", "            ", "self", ".", "current_table", ".", "caption", "+=", "\" \"", "+", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.__init__": [[442, 461], ["biencoder_data.Dataset.__init__", "glob.glob", "random.Random", "biencoder_data.JsonLTablesQADataset.get_lin_func"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.get_lin_func"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "is_train_set", ":", "bool", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "max_negatives", ":", "int", "=", "1", ",", "\n", "seed", ":", "int", "=", "0", ",", "\n", "max_len", "=", "100", ",", "\n", "split_type", ":", "str", "=", "\"type1\"", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "selector", ",", "shuffle_positives", "=", "shuffle_positives", ")", "\n", "self", ".", "data_files", "=", "glob", ".", "glob", "(", "file", ")", "\n", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "is_train_set", "=", "is_train_set", "\n", "self", ".", "max_negatives", "=", "max_negatives", "\n", "self", ".", "rnd", "=", "random", ".", "Random", "(", "seed", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "linearize_func", "=", "JsonLTablesQADataset", ".", "get_lin_func", "(", "split_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.load_data": [[462, 471], ["logger.info", "jsonlines.open", "len", "len"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "for", "path", "in", "self", ".", "data_files", ":", "\n", "            ", "with", "jsonlines", ".", "open", "(", "path", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "                ", "data", "+=", "[", "jline", "for", "jline", "in", "jsonl_reader", "]", "\n", "\n", "# filter those without positive ctx", "\n", "", "", "self", ".", "data", "=", "[", "r", "for", "r", "in", "data", "if", "len", "(", "r", "[", "\"positive_ctxs\"", "]", ")", ">", "0", "]", "\n", "logger", ".", "info", "(", "\"Total cleaned data size: {}\"", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.__getitem__": [[472, 497], ["biencoder_data.BiEncoderSample", "biencoder_data.JsonLTablesQADataset.rnd.shuffle", "biencoder_data.JsonLTablesQADataset.rnd.shuffle", "BiEncoderPassage", "BiEncoderPassage", "biencoder_data.JsonLTablesQADataset.linearize_func", "biencoder_data.JsonLTablesQADataset.linearize_func"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", "->", "BiEncoderSample", ":", "\n", "        ", "json_sample", "=", "self", ".", "data", "[", "index", "]", "\n", "r", "=", "BiEncoderSample", "(", ")", "\n", "r", ".", "query", "=", "json_sample", "[", "\"question\"", "]", "\n", "positive_ctxs", "=", "json_sample", "[", "\"positive_ctxs\"", "]", "\n", "hard_negative_ctxs", "=", "json_sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "\n", "if", "self", ".", "shuffle_positives", ":", "\n", "            ", "self", ".", "rnd", ".", "shuffle", "(", "positive_ctxs", ")", "\n", "\n", "", "if", "self", ".", "is_train_set", ":", "\n", "            ", "self", ".", "rnd", ".", "shuffle", "(", "hard_negative_ctxs", ")", "\n", "", "positive_ctxs", "=", "positive_ctxs", "[", "0", ":", "1", "]", "\n", "hard_negative_ctxs", "=", "hard_negative_ctxs", "[", "0", ":", "self", ".", "max_negatives", "]", "\n", "\n", "r", ".", "positive_passages", "=", "[", "\n", "BiEncoderPassage", "(", "self", ".", "linearize_func", "(", "self", ",", "ctx", ",", "True", ")", ",", "ctx", "[", "\"caption\"", "]", ")", "\n", "for", "ctx", "in", "positive_ctxs", "\n", "]", "\n", "r", ".", "negative_passages", "=", "[", "]", "\n", "r", ".", "hard_negative_passages", "=", "[", "\n", "BiEncoderPassage", "(", "self", ".", "linearize_func", "(", "self", ",", "ctx", ",", "False", ")", ",", "ctx", "[", "\"caption\"", "]", ")", "\n", "for", "ctx", "in", "hard_negative_ctxs", "\n", "]", "\n", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.__len__": [[498, 500], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.get_lin_func": [[501, 507], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_lin_func", "(", "cls", ",", "split_type", ":", "str", ")", ":", "\n", "        ", "f", "=", "{", "\n", "\"type1\"", ":", "JsonLTablesQADataset", ".", "_linearize_table", ",", "\n", "}", "\n", "return", "f", "[", "split_type", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.split_table": [[508, 544], ["enumerate", "range", "biencoder_data.JsonLTablesQADataset._linearize_row", "len", "biencoder_data.JsonLTablesQADataset._linearize_row", "len", "chunks.append", "len", "len", "current_rows.append", "chunks.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row"], ["", "@", "classmethod", "\n", "def", "split_table", "(", "cls", ",", "t", ":", "dict", ",", "max_length", ":", "int", ")", ":", "\n", "        ", "rows", "=", "t", "[", "\"rows\"", "]", "\n", "header", "=", "None", "\n", "header_len", "=", "0", "\n", "start_row", "=", "0", "\n", "\n", "# get the first non empty row as the \"header\"", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "rows", ")", ":", "\n", "            ", "row_lin", ",", "row_len", "=", "JsonLTablesQADataset", ".", "_linearize_row", "(", "r", ")", "\n", "if", "len", "(", "row_lin", ")", ">", "1", ":", "# TODO: change to checking cell value tokens", "\n", "                ", "header", "=", "row_lin", "\n", "header_len", "+=", "row_len", "\n", "start_row", "=", "i", "\n", "break", "\n", "\n", "", "", "chunks", "=", "[", "]", "\n", "current_rows", "=", "[", "header", "]", "\n", "current_len", "=", "header_len", "\n", "\n", "for", "i", "in", "range", "(", "start_row", "+", "1", ",", "len", "(", "rows", ")", ")", ":", "\n", "            ", "row_lin", ",", "row_len", "=", "JsonLTablesQADataset", ".", "_linearize_row", "(", "rows", "[", "i", "]", ")", "\n", "if", "len", "(", "row_lin", ")", ">", "1", ":", "# TODO: change to checking cell value tokens", "\n", "                ", "current_rows", ".", "append", "(", "row_lin", ")", "\n", "current_len", "+=", "row_len", "\n", "", "if", "current_len", ">=", "max_length", ":", "\n", "# linearize chunk", "\n", "                ", "linearized_str", "=", "\"\\n\"", ".", "join", "(", "current_rows", ")", "+", "\"\\n\"", "\n", "chunks", ".", "append", "(", "linearized_str", ")", "\n", "current_rows", "=", "[", "header", "]", "\n", "current_len", "=", "header_len", "\n", "\n", "", "", "if", "len", "(", "current_rows", ")", ">", "1", ":", "\n", "            ", "linearized_str", "=", "\"\\n\"", ".", "join", "(", "current_rows", ")", "+", "\"\\n\"", "\n", "chunks", ".", "append", "(", "linearized_str", ")", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_table": [[545, 597], ["set", "enumerate", "biencoder_data.JsonLTablesQADataset._linearize_row", "len", "set.add", "rows_linearized.append", "biencoder_data.JsonLTablesQADataset.rnd.shuffle", "numpy.random.permutation", "biencoder_data.JsonLTablesQADataset._linearize_row", "set.add", "rows_linearized.append", "range", "biencoder_data.JsonLTablesQADataset._linearize_row", "len", "range", "len", "set.add", "rows_linearized.append", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row"], ["", "def", "_linearize_table", "(", "self", ",", "t", ":", "dict", ",", "is_positive", ":", "bool", ")", "->", "str", ":", "\n", "        ", "rows", "=", "t", "[", "\"rows\"", "]", "\n", "selected_rows", "=", "set", "(", ")", "\n", "rows_linearized", "=", "[", "]", "\n", "total_words_len", "=", "0", "\n", "\n", "# get the first non empty row as the \"header\"", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "rows", ")", ":", "\n", "            ", "row_lin", ",", "row_len", "=", "JsonLTablesQADataset", ".", "_linearize_row", "(", "r", ")", "\n", "if", "len", "(", "row_lin", ")", ">", "1", ":", "# TODO: change to checking cell value tokens", "\n", "                ", "selected_rows", ".", "add", "(", "i", ")", "\n", "rows_linearized", ".", "append", "(", "row_lin", ")", "\n", "total_words_len", "+=", "row_len", "\n", "break", "\n", "\n", "# split to chunks", "\n", "", "", "if", "is_positive", ":", "\n", "            ", "row_idx_with_answers", "=", "[", "ap", "[", "0", "]", "for", "ap", "in", "t", "[", "\"answer_pos\"", "]", "]", "\n", "\n", "if", "self", ".", "shuffle_positives", ":", "\n", "                ", "self", ".", "rnd", ".", "shuffle", "(", "row_idx_with_answers", ")", "\n", "", "for", "i", "in", "row_idx_with_answers", ":", "\n", "                ", "if", "i", "not", "in", "selected_rows", ":", "\n", "                    ", "row_lin", ",", "row_len", "=", "JsonLTablesQADataset", ".", "_linearize_row", "(", "rows", "[", "i", "]", ")", "\n", "selected_rows", ".", "add", "(", "i", ")", "\n", "rows_linearized", ".", "append", "(", "row_lin", ")", "\n", "total_words_len", "+=", "row_len", "\n", "", "if", "total_words_len", ">=", "self", ".", "max_len", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "if", "total_words_len", "<", "self", ".", "max_len", ":", "# append random rows", "\n", "\n", "            ", "if", "self", ".", "is_train_set", ":", "\n", "                ", "rows_indexes", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "len", "(", "rows", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "rows_indexes", "=", "[", "*", "range", "(", "len", "(", "rows", ")", ")", "]", "\n", "\n", "", "for", "i", "in", "rows_indexes", ":", "\n", "                ", "if", "i", "not", "in", "selected_rows", ":", "\n", "                    ", "row_lin", ",", "row_len", "=", "JsonLTablesQADataset", ".", "_linearize_row", "(", "rows", "[", "i", "]", ")", "\n", "if", "len", "(", "row_lin", ")", ">", "1", ":", "# TODO: change to checking cell value tokens", "\n", "                        ", "selected_rows", ".", "add", "(", "i", ")", "\n", "rows_linearized", ".", "append", "(", "row_lin", ")", "\n", "total_words_len", "+=", "row_len", "\n", "", "if", "total_words_len", ">=", "self", ".", "max_len", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "", "linearized_str", "=", "\"\"", "\n", "for", "r", "in", "rows_linearized", ":", "\n", "            ", "linearized_str", "+=", "r", "+", "\"\\n\"", "\n", "\n", "", "return", "linearized_str", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset._linearize_row": [[598, 603], ["sum", "len", "c.split"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_linearize_row", "(", "cls", ",", "row", ":", "dict", ")", "->", "Tuple", "[", "str", ",", "int", "]", ":", "\n", "        ", "cell_values", "=", "[", "c", "[", "\"value\"", "]", "for", "c", "in", "row", "[", "\"columns\"", "]", "]", "\n", "total_words", "=", "sum", "(", "len", "(", "c", ".", "split", "(", "\" \"", ")", ")", "for", "c", "in", "cell_values", ")", "\n", "return", "\", \"", ".", "join", "(", "[", "c", "[", "\"value\"", "]", "for", "c", "in", "row", "[", "\"columns\"", "]", "]", ")", ",", "total_words", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.get_dpr_files": [[122, 130], ["os.path.exists", "glob.glob", "glob.glob", "download"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download"], ["", "", "def", "get_dpr_files", "(", "source_name", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "source_name", ")", "or", "glob", ".", "glob", "(", "source_name", ")", ":", "\n", "        ", "return", "glob", ".", "glob", "(", "source_name", ")", "\n", "", "else", ":", "\n", "# try to use data downloader", "\n", "        ", "from", "dpr", ".", "data", ".", "download_data", "import", "download", "\n", "\n", "return", "download", "(", "source_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_passage": [[208, 211], ["ctx_text.replace().replace.replace().replace", "ctx_text.replace().replace.replace"], "function", ["None"], ["", "", "def", "normalize_passage", "(", "ctx_text", ":", "str", ")", ":", "\n", "    ", "ctx_text", "=", "ctx_text", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ".", "replace", "(", "\"\u2019\"", ",", "\"'\"", ")", "\n", "return", "ctx_text", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_question": [[213, 216], ["question.replace.replace"], "function", ["None"], ["", "def", "normalize_question", "(", "question", ":", "str", ")", "->", "str", ":", "\n", "    ", "question", "=", "question", ".", "replace", "(", "\"\u2019\"", ",", "\"'\"", ")", "\n", "return", "question", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.read_nq_tables_jsonl": [[375, 431], ["logger.info", "logger.info", "logger.info", "logger.info", "jsonlines.open", "biencoder_data.NQTableParser", "biencoder_data.NQTableParser.parse", "len", "sum", "logger.info", "len", "len", "len", "t.get_key", "any", "t.get_key"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.NQTableParser.parse", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.get_key", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.get_key"], ["", "", "", "def", "read_nq_tables_jsonl", "(", "path", ":", "str", ")", "->", "Dict", "[", "str", ",", "Table", "]", ":", "\n", "    ", "tables_with_issues", "=", "0", "\n", "single_row_tables", "=", "0", "\n", "nested_tables", "=", "0", "\n", "regular_tables", "=", "0", "\n", "total_tables", "=", "0", "\n", "total_rows", "=", "0", "\n", "tables_dict", "=", "{", "}", "\n", "\n", "with", "jsonlines", ".", "open", "(", "path", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "        ", "for", "jline", "in", "jsonl_reader", ":", "\n", "            ", "tokens", "=", "jline", "[", "\"tokens\"", "]", "\n", "\n", "if", "\"( hide ) This section has multiple issues\"", "in", "\" \"", ".", "join", "(", "tokens", ")", ":", "\n", "                ", "tables_with_issues", "+=", "1", "\n", "continue", "\n", "\n", "", "mask", "=", "jline", "[", "\"html_mask\"", "]", "\n", "# page_url = jline[\"doc_url\"]", "\n", "title", "=", "jline", "[", "\"title\"", "]", "\n", "p", "=", "NQTableParser", "(", "tokens", ",", "mask", ",", "title", ")", "\n", "tables", "=", "p", ".", "parse", "(", ")", "\n", "\n", "# table = parse_table(tokens, mask)", "\n", "\n", "nested_tables", "+=", "len", "(", "tables", "[", "1", ":", "]", ")", "\n", "\n", "for", "t", "in", "tables", ":", "\n", "                ", "total_tables", "+=", "1", "\n", "\n", "# calc amount of non empty rows", "\n", "non_empty_rows", "=", "sum", "(", "\n", "[", "\n", "1", "\n", "for", "r", "in", "t", ".", "body", "\n", "if", "r", ".", "cells", "and", "any", "(", "[", "True", "for", "c", "in", "r", ".", "cells", "if", "c", ".", "value_tokens", "]", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "non_empty_rows", "<=", "1", ":", "\n", "                    ", "single_row_tables", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "regular_tables", "+=", "1", "\n", "total_rows", "+=", "len", "(", "t", ".", "body", ")", "\n", "\n", "if", "t", ".", "get_key", "(", ")", "not", "in", "tables_dict", ":", "\n", "                        ", "tables_dict", "[", "t", ".", "get_key", "(", ")", "]", "=", "t", "\n", "\n", "", "", "", "if", "len", "(", "tables_dict", ")", "%", "1000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"tables_dict %d\"", ",", "len", "(", "tables_dict", ")", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\"regular tables %d\"", ",", "regular_tables", ")", "\n", "logger", ".", "info", "(", "\"tables_with_issues %d\"", ",", "tables_with_issues", ")", "\n", "logger", ".", "info", "(", "\"single_row_tables %d\"", ",", "single_row_tables", ")", "\n", "logger", ".", "info", "(", "\"nested_tables %d\"", ",", "nested_tables", ")", "\n", "return", "tables_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.get_table_string_for_answer_check": [[433, 439], ["None"], "function", ["None"], ["", "def", "get_table_string_for_answer_check", "(", "table", ":", "Table", ")", ":", "# this doesn't use caption", "\n", "    ", "table_text", "=", "\"\"", "\n", "for", "r", "in", "table", ".", "body", ":", "\n", "        ", "table_text", "+=", "\" . \"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "c", ".", "value_tokens", ")", "for", "c", "in", "r", ".", "cells", "]", ")", "\n", "", "table_text", "+=", "\" . \"", "\n", "return", "table_text", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.split_tables_to_chunks": [[605, 623], ["enumerate", "t.to_dpr_json", "biencoder_data.JsonLTablesQADataset.split_table", "tables_dict.items", "chunks.append", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.Table.to_dpr_json", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.JsonLTablesQADataset.split_table"], ["", "", "def", "split_tables_to_chunks", "(", "\n", "tables_dict", ":", "Dict", "[", "str", ",", "Table", "]", ",", "max_table_len", ":", "int", ",", "split_type", ":", "str", "=", "\"type1\"", "\n", ")", "->", "List", "[", "Tuple", "[", "int", ",", "str", ",", "str", ",", "int", "]", "]", ":", "\n", "    ", "tables_as_dicts", "=", "[", "t", ".", "to_dpr_json", "(", ")", "for", "k", ",", "t", "in", "tables_dict", ".", "items", "(", ")", "]", "\n", "chunks", "=", "[", "]", "\n", "chunk_id", "=", "0", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "tables_as_dicts", ")", ":", "\n", "# TODO: support other types", "\n", "        ", "assert", "split_type", "==", "\"type1\"", "\n", "table_chunks", "=", "JsonLTablesQADataset", ".", "split_table", "(", "t", ",", "max_table_len", ")", "\n", "title", "=", "t", "[", "\"caption\"", "]", "\n", "for", "c", "in", "table_chunks", ":", "\n", "# chunk id , text, title, external_id", "\n", "            ", "chunks", ".", "append", "(", "(", "chunk_id", ",", "c", ",", "title", ",", "i", ")", ")", "\n", "chunk_id", "+=", "1", "\n", "", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Splitted %d tables to %d chunks\"", ",", "i", ",", "len", "(", "chunks", ")", ")", "\n", "", "", "return", "chunks", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.RetrieverData.__init__": [[28, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        :param file: - real file name or the resource name as they are defined in download_data.py\n        \"\"\"", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.RetrieverData.load_data": [[35, 43], ["dpr.data.biencoder_data.get_dpr_files", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.get_dpr_files"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "get_dpr_files", "(", "self", ".", "file", ")", "\n", "assert", "(", "\n", "len", "(", "self", ".", "data_files", ")", "==", "1", "\n", ")", ",", "\"RetrieverData source currently works with single files only. Files specified: {}\"", ".", "format", "(", "\n", "self", ".", "data_files", "\n", ")", "\n", "self", ".", "file", "=", "self", ".", "data_files", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc.__init__": [[46, 58], ["retriever_data.RetrieverData.__init__", "hydra.utils.instantiate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_query_token", ":", "str", "=", "None", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ")", "\n", "self", ".", "data", "=", "None", "\n", "self", ".", "selector", "=", "hydra", ".", "utils", ".", "instantiate", "(", "selector", ")", "if", "selector", "else", "None", "\n", "self", ".", "special_query_token", "=", "special_query_token", "\n", "self", ".", "query_special_suffix", "=", "query_special_suffix", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc.__getitem__": [[59, 61], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", "->", "QASample", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc.__len__": [[62, 64], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc._process_question": [[65, 73], ["dpr.data.biencoder_data.normalize_question", "dpr.data.biencoder_data.normalize_question.endswith"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_question"], ["", "def", "_process_question", "(", "self", ",", "question", ":", "str", ")", ":", "\n", "# as of now, always normalize query", "\n", "        ", "question", "=", "normalize_question", "(", "question", ")", "\n", "if", "self", ".", "query_special_suffix", "and", "not", "question", ".", "endswith", "(", "\n", "self", ".", "query_special_suffix", "\n", ")", ":", "\n", "            ", "question", "+=", "self", ".", "query_special_suffix", "\n", "", "return", "question", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.CsvQASrc.__init__": [[76, 90], ["retriever_data.QASrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "question_col", ":", "int", "=", "0", ",", "\n", "answers_col", ":", "int", "=", "1", ",", "\n", "id_col", ":", "int", "=", "-", "1", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_query_token", ":", "str", "=", "None", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ",", "selector", ",", "special_query_token", ",", "query_special_suffix", ")", "\n", "self", ".", "question_col", "=", "question_col", "\n", "self", ".", "answers_col", "=", "answers_col", "\n", "self", ".", "id_col", "=", "id_col", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.CsvQASrc.load_data": [[91, 104], ["retriever_data.RetrieverData.load_data", "open", "csv.reader", "eval", "data.append", "QASample", "retriever_data.CsvQASrc._process_question"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc._process_question"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "data", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "file", ")", "as", "ifile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "question", "=", "row", "[", "self", ".", "question_col", "]", "\n", "answers", "=", "eval", "(", "row", "[", "self", ".", "answers_col", "]", ")", "\n", "id", "=", "None", "\n", "if", "self", ".", "id_col", ">=", "0", ":", "\n", "                    ", "id", "=", "row", "[", "self", ".", "id_col", "]", "\n", "", "data", ".", "append", "(", "QASample", "(", "self", ".", "_process_question", "(", "question", ")", ",", "id", ",", "answers", ")", ")", "\n", "", "", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlQASrc.__init__": [[107, 121], ["retriever_data.QASrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "question_attr", ":", "str", "=", "\"question\"", ",", "\n", "answers_attr", ":", "str", "=", "\"answers\"", ",", "\n", "id_attr", ":", "str", "=", "\"id\"", ",", "\n", "special_query_token", ":", "str", "=", "None", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ",", "selector", ",", "special_query_token", ",", "query_special_suffix", ")", "\n", "self", ".", "question_attr", "=", "question_attr", "\n", "self", ".", "answers_attr", "=", "answers_attr", "\n", "self", ".", "id_attr", "=", "id_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlQASrc.load_data": [[122, 134], ["retriever_data.RetrieverData.load_data", "jsonlines.open", "data.append", "QASample", "retriever_data.JsonlQASrc._process_question"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc._process_question"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "data", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "self", ".", "file", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "            ", "for", "jline", "in", "jsonl_reader", ":", "\n", "                ", "question", "=", "jline", "[", "self", ".", "question_attr", "]", "\n", "answers", "=", "jline", "[", "self", ".", "answers_attr", "]", "if", "self", ".", "answers_attr", "in", "jline", "else", "[", "]", "\n", "id", "=", "None", "\n", "if", "self", ".", "id_attr", "in", "jline", ":", "\n", "                    ", "id", "=", "jline", "[", "self", ".", "id_attr", "]", "\n", "", "data", ".", "append", "(", "QASample", "(", "self", ".", "_process_question", "(", "question", ")", ",", "id", ",", "answers", ")", ")", "\n", "", "", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltCsvQASrc.__init__": [[137, 158], ["retriever_data.CsvQASrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "kilt_gold_file", ":", "str", ",", "\n", "question_col", ":", "int", "=", "0", ",", "\n", "answers_col", ":", "int", "=", "1", ",", "\n", "id_col", ":", "int", "=", "-", "1", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_query_token", ":", "str", "=", "None", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "file", ",", "\n", "question_col", ",", "\n", "answers_col", ",", "\n", "id_col", ",", "\n", "selector", ",", "\n", "special_query_token", ",", "\n", "query_special_suffix", ",", "\n", ")", "\n", "self", ".", "kilt_gold_file", "=", "kilt_gold_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltJsonlQASrc.__init__": [[161, 182], ["retriever_data.JsonlQASrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "kilt_gold_file", ":", "str", ",", "\n", "question_attr", ":", "str", "=", "\"input\"", ",", "\n", "answers_attr", ":", "str", "=", "\"answer\"", ",", "\n", "id_attr", ":", "str", "=", "\"id\"", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_query_token", ":", "str", "=", "None", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "file", ",", "\n", "selector", ",", "\n", "question_attr", ",", "\n", "answers_attr", ",", "\n", "id_attr", ",", "\n", "special_query_token", ",", "\n", "query_special_suffix", ",", "\n", ")", "\n", "self", ".", "kilt_gold_file", "=", "kilt_gold_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltJsonlQASrc.load_data": [[183, 196], ["retriever_data.JsonlQASrc.load_data", "jsonlines.open", "data.append", "QASample", "retriever_data.KiltJsonlQASrc._process_question"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.QASrc._process_question"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "data", "=", "[", "]", "\n", "with", "jsonlines", ".", "open", "(", "self", ".", "file", ",", "mode", "=", "\"r\"", ")", "as", "jsonl_reader", ":", "\n", "            ", "for", "jline", "in", "jsonl_reader", ":", "\n", "                ", "question", "=", "jline", "[", "self", ".", "question_attr", "]", "\n", "out", "=", "jline", "[", "\"output\"", "]", "\n", "answers", "=", "[", "o", "[", "\"answer\"", "]", "for", "o", "in", "out", "if", "\"answer\"", "in", "o", "]", "\n", "id", "=", "None", "\n", "if", "self", ".", "id_attr", "in", "jline", ":", "\n", "                    ", "id", "=", "jline", "[", "self", ".", "id_attr", "]", "\n", "", "data", ".", "append", "(", "QASample", "(", "self", ".", "_process_question", "(", "question", ")", ",", "id", ",", "answers", ")", ")", "\n", "", "", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.TTS_ASR_QASrc.__init__": [[199, 202], ["retriever_data.QASrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "file", ":", "str", ",", "trans_file", ":", "str", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ")", "\n", "self", ".", "trans_file", "=", "trans_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.TTS_ASR_QASrc.load_data": [[203, 226], ["retriever_data.RetrieverData.load_data", "open", "csv.reader", "open", "csv.reader", "eval", "row_str.index", "int", "row_str[].strip().lower", "data.append", "QASample", "row_str[].strip", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "orig_data_dict", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "file", ",", "\"r\"", ")", "as", "ifile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "id", "=", "0", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "question", "=", "row", "[", "0", "]", "\n", "answers", "=", "eval", "(", "row", "[", "1", "]", ")", "\n", "orig_data_dict", "[", "id", "]", "=", "(", "question", ",", "answers", ")", "\n", "id", "+=", "1", "\n", "", "", "data", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "trans_file", ",", "\"r\"", ")", "as", "tfile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "tfile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "r", "in", "reader", ":", "\n", "                ", "row_str", "=", "r", "[", "0", "]", "\n", "idx", "=", "row_str", ".", "index", "(", "\"(None-\"", ")", "\n", "q_id", "=", "int", "(", "row_str", "[", "idx", "+", "len", "(", "\"(None-\"", ")", ":", "-", "1", "]", ")", "\n", "orig_data", "=", "orig_data_dict", "[", "q_id", "]", "\n", "answers", "=", "orig_data", "[", "1", "]", "\n", "q", "=", "row_str", "[", ":", "idx", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "data", ".", "append", "(", "QASample", "(", "q", ",", "idx", ",", "answers", ")", ")", "\n", "", "", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.CsvCtxSrc.__init__": [[229, 244], ["retriever_data.RetrieverData.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "id_col", ":", "int", "=", "0", ",", "\n", "text_col", ":", "int", "=", "1", ",", "\n", "title_col", ":", "int", "=", "2", ",", "\n", "id_prefix", ":", "str", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ")", "\n", "self", ".", "text_col", "=", "text_col", "\n", "self", ".", "title_col", "=", "title_col", "\n", "self", ".", "id_col", "=", "id_col", "\n", "self", ".", "id_prefix", "=", "id_prefix", "\n", "self", ".", "normalize", "=", "normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.CsvCtxSrc.load_data_to": [[245, 260], ["retriever_data.RetrieverData.load_data", "open", "csv.reader", "dpr.data.biencoder_data.BiEncoderPassage", "dpr.data.biencoder_data.normalize_passage", "str"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.normalize_passage"], ["", "def", "load_data_to", "(", "self", ",", "ctxs", ":", "Dict", "[", "object", ",", "BiEncoderPassage", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n", "with", "open", "(", "self", ".", "file", ")", "as", "ifile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "row", "[", "self", ".", "id_col", "]", "==", "\"id\"", ":", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "id_prefix", ":", "\n", "                    ", "sample_id", "=", "self", ".", "id_prefix", "+", "str", "(", "row", "[", "self", ".", "id_col", "]", ")", "\n", "", "else", ":", "\n", "                    ", "sample_id", "=", "row", "[", "self", ".", "id_col", "]", "\n", "", "passage", "=", "row", "[", "self", ".", "text_col", "]", "\n", "if", "self", ".", "normalize", ":", "\n", "                    ", "passage", "=", "normalize_passage", "(", "passage", ")", "\n", "", "ctxs", "[", "sample_id", "]", "=", "BiEncoderPassage", "(", "passage", ",", "row", "[", "self", ".", "title_col", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltCsvCtxSrc.__init__": [[263, 277], ["retriever_data.CsvCtxSrc.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "mapping_file", ":", "str", ",", "\n", "id_col", ":", "int", "=", "0", ",", "\n", "text_col", ":", "int", "=", "1", ",", "\n", "title_col", ":", "int", "=", "2", ",", "\n", "id_prefix", ":", "str", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "file", ",", "id_col", ",", "text_col", ",", "title_col", ",", "id_prefix", ",", "normalize", "=", "normalize", "\n", ")", "\n", "self", ".", "mapping_file", "=", "mapping_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.KiltCsvCtxSrc.convert_to_kilt": [[278, 311], ["logger.info", "logger.info", "open", "json.load", "jsonlines.open", "list", "len", "len", "open", "pickle.load", "jsonlines.open", "zip", "writer.write", "provenance.append", "int"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "convert_to_kilt", "(", "self", ",", "kilt_gold_file", ",", "dpr_output", ",", "kilt_out_file", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Converting to KILT format file: %s\"", ",", "dpr_output", ")", "\n", "\n", "with", "open", "(", "dpr_output", ",", "\"rt\"", ")", "as", "fin", ":", "\n", "            ", "dpr_output", "=", "json", ".", "load", "(", "fin", ")", "\n", "\n", "", "with", "jsonlines", ".", "open", "(", "kilt_gold_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "kilt_gold_file", "=", "list", "(", "reader", ")", "\n", "", "assert", "len", "(", "kilt_gold_file", ")", "==", "len", "(", "dpr_output", ")", "\n", "map_path", "=", "self", ".", "mapping_file", "\n", "with", "open", "(", "map_path", ",", "\"rb\"", ")", "as", "fin", ":", "\n", "            ", "mapping", "=", "pickle", ".", "load", "(", "fin", ")", "\n", "\n", "", "with", "jsonlines", ".", "open", "(", "kilt_out_file", ",", "mode", "=", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "dpr_entry", ",", "kilt_gold_entry", "in", "zip", "(", "dpr_output", ",", "kilt_gold_file", ")", ":", "\n", "                ", "assert", "dpr_entry", "[", "\"question\"", "]", "==", "kilt_gold_entry", "[", "\"input\"", "]", "\n", "provenance", "=", "[", "]", "\n", "for", "ctx", "in", "dpr_entry", "[", "\"ctxs\"", "]", ":", "\n", "                    ", "wikipedia_id", ",", "end_paragraph_id", "=", "mapping", "[", "int", "(", "ctx", "[", "\"id\"", "]", ")", "]", "\n", "provenance", ".", "append", "(", "\n", "{", "\n", "\"wikipedia_id\"", ":", "wikipedia_id", ",", "\n", "\"end_paragraph_id\"", ":", "end_paragraph_id", ",", "\n", "}", "\n", ")", "\n", "", "kilt_entry", "=", "{", "\n", "\"id\"", ":", "kilt_gold_entry", "[", "\"id\"", "]", ",", "\n", "\"input\"", ":", "dpr_entry", "[", "\"question\"", "]", ",", "\n", "\"output\"", ":", "[", "{", "\"provenance\"", ":", "provenance", "}", "]", ",", "\n", "}", "\n", "writer", ".", "write", "(", "kilt_entry", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Saved KILT formatted results to: %s\"", ",", "kilt_out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlTablesCtxSrc.__init__": [[314, 325], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "tables_chunk_sz", ":", "int", "=", "100", ",", "\n", "split_type", ":", "str", "=", "\"type1\"", ",", "\n", "id_prefix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "tables_chunk_sz", "=", "tables_chunk_sz", "\n", "self", ".", "split_type", "=", "split_type", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "id_prefix", "=", "id_prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.retriever_data.JsonlTablesCtxSrc.load_data_to": [[326, 338], ["logger.info", "dpr.data.biencoder_data.read_nq_tables_jsonl", "dpr.data.biencoder_data.split_tables_to_chunks", "logger.info", "ctxs.update", "TableChunk", "len", "str"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.read_nq_tables_jsonl", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.biencoder_data.split_tables_to_chunks"], ["", "def", "load_data_to", "(", "self", ",", "ctxs", ":", "Dict", ")", ":", "\n", "        ", "docs", "=", "{", "}", "\n", "logger", ".", "info", "(", "\"Parsing Tables data from: %s\"", ",", "self", ".", "file", ")", "\n", "tables_dict", "=", "read_nq_tables_jsonl", "(", "self", ".", "file", ")", "\n", "table_chunks", "=", "split_tables_to_chunks", "(", "\n", "tables_dict", ",", "self", ".", "tables_chunk_sz", ",", "split_type", "=", "self", ".", "split_type", "\n", ")", "\n", "for", "chunk", "in", "table_chunks", ":", "\n", "            ", "sample_id", "=", "self", ".", "id_prefix", "+", "str", "(", "chunk", "[", "0", "]", ")", "\n", "docs", "[", "sample_id", "]", "=", "TableChunk", "(", "chunk", "[", "1", "]", ",", "chunk", "[", "2", "]", ",", "chunk", "[", "3", "]", ")", "\n", "", "logger", ".", "info", "(", "\"Loaded %d tables chunks\"", ",", "len", "(", "docs", ")", ")", "\n", "ctxs", ".", "update", "(", "docs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.unpack": [[374, 383], ["logger.info", "gzip.GzipFile", "gzip.GzipFile.read", "gzip.GzipFile.close", "open", "open.write", "open.close", "logger.info"], "function", ["None"], ["def", "unpack", "(", "gzip_file", ":", "str", ",", "out_file", ":", "str", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Uncompressing %s\"", ",", "gzip_file", ")", "\n", "input", "=", "gzip", ".", "GzipFile", "(", "gzip_file", ",", "\"rb\"", ")", "\n", "s", "=", "input", ".", "read", "(", ")", "\n", "input", ".", "close", "(", ")", "\n", "output", "=", "open", "(", "out_file", ",", "\"wb\"", ")", "\n", "output", ".", "write", "(", "s", ")", "\n", "output", ".", "close", "(", ")", "\n", "logger", ".", "info", "(", "\" Saved to %s\"", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_resource": [[385, 433], ["logger.info", "resource_key.split", "logger.info", "os.path.join", "pathlib.Path().mkdir", "os.path.abspath", "logger.info", "os.path.exists", "os.path.abspath", "wget.download", "logger.info", "os.path.abspath", "os.path.join", "logger.info", "os.path.join", "os.path.join", "download_data.unpack", "os.remove", "pathlib.Path", "os.path.abspath.index"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.unpack"], ["", "def", "download_resource", "(", "\n", "s3_url", ":", "str", ",", "original_ext", ":", "str", ",", "compressed", ":", "bool", ",", "resource_key", ":", "str", ",", "out_dir", ":", "str", "\n", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "logger", ".", "info", "(", "\"Requested resource from %s\"", ",", "s3_url", ")", "\n", "path_names", "=", "resource_key", ".", "split", "(", "\".\"", ")", "\n", "\n", "if", "out_dir", ":", "\n", "        ", "root_dir", "=", "out_dir", "\n", "", "else", ":", "\n", "# since hydra overrides the location for the 'current dir' for every run and we don't want to duplicate", "\n", "# resources multiple times, remove the current folder's volatile part", "\n", "        ", "root_dir", "=", "os", ".", "path", ".", "abspath", "(", "\"./\"", ")", "\n", "if", "\"/outputs/\"", "in", "root_dir", ":", "\n", "            ", "root_dir", "=", "root_dir", "[", ":", "root_dir", ".", "index", "(", "\"/outputs/\"", ")", "]", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Download root_dir %s\"", ",", "root_dir", ")", "\n", "\n", "save_root", "=", "os", ".", "path", ".", "join", "(", "\n", "root_dir", ",", "\"downloads\"", ",", "*", "path_names", "[", ":", "-", "1", "]", "\n", ")", "# last segment is for file name", "\n", "\n", "pathlib", ".", "Path", "(", "save_root", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "local_file_uncompressed", "=", "os", ".", "path", ".", "abspath", "(", "\n", "os", ".", "path", ".", "join", "(", "save_root", ",", "path_names", "[", "-", "1", "]", "+", "original_ext", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"File to be downloaded as %s\"", ",", "local_file_uncompressed", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "local_file_uncompressed", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"File already exist %s\"", ",", "local_file_uncompressed", ")", "\n", "return", "save_root", ",", "local_file_uncompressed", "\n", "\n", "", "local_file", "=", "os", ".", "path", ".", "abspath", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "save_root", ",", "path_names", "[", "-", "1", "]", "+", "(", "\".tmp\"", "if", "compressed", "else", "original_ext", ")", "\n", ")", "\n", ")", "\n", "\n", "wget", ".", "download", "(", "s3_url", ",", "out", "=", "local_file", ")", "\n", "\n", "logger", ".", "info", "(", "\"Downloaded to %s\"", ",", "local_file", ")", "\n", "\n", "if", "compressed", ":", "\n", "        ", "uncompressed_file", "=", "os", ".", "path", ".", "join", "(", "save_root", ",", "path_names", "[", "-", "1", "]", "+", "original_ext", ")", "\n", "unpack", "(", "local_file", ",", "uncompressed_file", ")", "\n", "os", ".", "remove", "(", "local_file", ")", "\n", "local_file", "=", "uncompressed_file", "\n", "", "return", "save_root", ",", "local_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_file": [[435, 445], ["logger.info", "os.path.join", "os.path.exists", "wget.download", "logger.info", "logger.info"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download"], ["", "def", "download_file", "(", "s3_url", ":", "str", ",", "out_dir", ":", "str", ",", "file_name", ":", "str", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Loading from %s\"", ",", "s3_url", ")", "\n", "local_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "file_name", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "local_file", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"File already exist %s\"", ",", "local_file", ")", "\n", "return", "\n", "\n", "", "wget", ".", "download", "(", "s3_url", ",", "out", "=", "local_file", ")", "\n", "logger", ".", "info", "(", "\"Downloaded to %s\"", ",", "local_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download": [[447, 488], ["isinstance", "download_info.get", "enumerate", "download_data.download_resource", "data_files.append", "download_data.download_file", "download_data.download_file", "logger.info", "download_data.download_resource", "data_files.append", "RESOURCES_MAP.keys", "k.startswith", "download_data.download"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_resource", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download_resource", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download"], ["", "def", "download", "(", "resource_key", ":", "str", ",", "out_dir", ":", "str", "=", "None", ")", ":", "\n", "    ", "if", "resource_key", "not", "in", "RESOURCES_MAP", ":", "\n", "# match by prefix", "\n", "        ", "resources", "=", "[", "k", "for", "k", "in", "RESOURCES_MAP", ".", "keys", "(", ")", "if", "k", ".", "startswith", "(", "resource_key", ")", "]", "\n", "if", "resources", ":", "\n", "            ", "for", "key", "in", "resources", ":", "\n", "                ", "download", "(", "key", ",", "out_dir", ")", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"no resources found for specified key\"", ")", "\n", "", "return", "[", "]", "\n", "", "download_info", "=", "RESOURCES_MAP", "[", "resource_key", "]", "\n", "\n", "s3_url", "=", "download_info", "[", "\"s3_url\"", "]", "\n", "\n", "save_root_dir", "=", "None", "\n", "data_files", "=", "[", "]", "\n", "if", "isinstance", "(", "s3_url", ",", "list", ")", ":", "\n", "        ", "for", "i", ",", "url", "in", "enumerate", "(", "s3_url", ")", ":", "\n", "            ", "save_root_dir", ",", "local_file", "=", "download_resource", "(", "\n", "url", ",", "\n", "download_info", "[", "\"original_ext\"", "]", ",", "\n", "download_info", "[", "\"compressed\"", "]", ",", "\n", "\"{}_{}\"", ".", "format", "(", "resource_key", ",", "i", ")", ",", "\n", "out_dir", ",", "\n", ")", "\n", "data_files", ".", "append", "(", "local_file", ")", "\n", "", "", "else", ":", "\n", "        ", "save_root_dir", ",", "local_file", "=", "download_resource", "(", "\n", "s3_url", ",", "\n", "download_info", "[", "\"original_ext\"", "]", ",", "\n", "download_info", "[", "\"compressed\"", "]", ",", "\n", "resource_key", ",", "\n", "out_dir", ",", "\n", ")", "\n", "data_files", ".", "append", "(", "local_file", ")", "\n", "\n", "", "license_files", "=", "download_info", ".", "get", "(", "\"license_files\"", ",", "None", ")", "\n", "if", "license_files", ":", "\n", "        ", "download_file", "(", "license_files", "[", "0", "]", ",", "save_root_dir", ",", "\"LICENSE\"", ")", "\n", "download_file", "(", "license_files", "[", "1", "]", ",", "save_root_dir", ",", "\"README\"", ")", "\n", "", "return", "data_files", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.main": [[490, 511], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_data.download", "print", "RESOURCES_MAP.items", "print"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.parse_args", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.download_data.download"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output_dir\"", ",", "\n", "default", "=", "\"./\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The output directory to download file\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--resource\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Resource name. See RESOURCES_MAP for all possible values\"", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "resource", ":", "\n", "        ", "download", "(", "args", ".", "resource", ",", "args", ".", "output_dir", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Please specify resource value. Possible options are:\"", ")", "\n", "for", "k", ",", "v", "in", "RESOURCES_MAP", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "\"Resource key=%s  :  %s\"", ",", "k", ",", "v", "[", "\"desc\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderPassage.__init__": [[37, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "id", "=", "None", ",", "\n", "text", ":", "str", "=", "None", ",", "\n", "title", ":", "str", "=", "None", ",", "\n", "score", "=", "None", ",", "\n", "has_answer", ":", "bool", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "id", "=", "id", "\n", "# string passage representations", "\n", "self", ".", "passage_text", "=", "text", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "has_answer", "=", "has_answer", "\n", "self", ".", "passage_token_ids", "=", "None", "\n", "# offset of the actual passage (i.e. not a question or may be title) in the sequence_ids", "\n", "self", ".", "passage_offset", "=", "None", "\n", "self", ".", "answers_spans", "=", "None", "\n", "# passage token ids", "\n", "self", ".", "sequence_ids", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderPassage.on_serialize": [[58, 64], ["reader_data.ReaderPassage.sequence_ids.numpy"], "methods", ["None"], ["", "def", "on_serialize", "(", "self", ")", ":", "\n", "# store only final sequence_ids and the ctx offset", "\n", "        ", "self", ".", "sequence_ids", "=", "self", ".", "sequence_ids", ".", "numpy", "(", ")", "\n", "self", ".", "passage_text", "=", "None", "\n", "self", ".", "title", "=", "None", "\n", "self", ".", "passage_token_ids", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderPassage.on_deserialize": [[65, 67], ["torch.tensor"], "methods", ["None"], ["", "def", "on_deserialize", "(", "self", ")", ":", "\n", "        ", "self", ".", "sequence_ids", "=", "torch", ".", "tensor", "(", "self", ".", "sequence_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.__init__": [[74, 87], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "question", ":", "str", ",", "\n", "answers", ":", "List", ",", "\n", "positive_passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", "negative_passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", "passages", ":", "List", "[", "ReaderPassage", "]", "=", "[", "]", ",", "\n", ")", ":", "\n", "        ", "self", ".", "question", "=", "question", "\n", "self", ".", "answers", "=", "answers", "\n", "self", ".", "positive_passages", "=", "positive_passages", "\n", "self", ".", "negative_passages", "=", "negative_passages", "\n", "self", ".", "passages", "=", "passages", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.on_serialize": [[88, 91], ["passage.on_serialize"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.on_serialize"], ["", "def", "on_serialize", "(", "self", ")", ":", "\n", "        ", "for", "passage", "in", "self", ".", "passages", "+", "self", ".", "positive_passages", "+", "self", ".", "negative_passages", ":", "\n", "            ", "passage", ".", "on_serialize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.on_deserialize": [[92, 95], ["passage.on_deserialize"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.on_deserialize"], ["", "", "def", "on_deserialize", "(", "self", ")", ":", "\n", "        ", "for", "passage", "in", "self", ".", "passages", "+", "self", ".", "positive_passages", "+", "self", ".", "negative_passages", ":", "\n", "            ", "passage", ".", "on_deserialize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset.__init__": [[98, 114], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "files", ":", "str", ",", "\n", "is_train", ":", "bool", ",", "\n", "gold_passages_src", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "run_preprocessing", ":", "bool", ",", "\n", "num_workers", ":", "int", ",", "\n", ")", ":", "\n", "        ", "self", ".", "files", "=", "files", "\n", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "is_train", "=", "is_train", "\n", "self", ".", "gold_passages_src", "=", "gold_passages_src", "\n", "self", ".", "tensorizer", "=", "tensorizer", "\n", "self", ".", "run_preprocessing", "=", "run_preprocessing", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset.__getitem__": [[115, 117], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset.__len__": [[118, 120], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset.load_data": [[121, 130], ["glob.glob", "logger.info", "reader_data.ExtractiveReaderDataset._get_preprocessed_files", "dpr.utils.data_utils.read_serialized_data_from_files", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset._get_preprocessed_files", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.read_serialized_data_from_files"], ["", "def", "load_data", "(", "\n", "self", ",", "\n", ")", ":", "\n", "        ", "data_files", "=", "glob", ".", "glob", "(", "self", ".", "files", ")", "\n", "logger", ".", "info", "(", "\"Data files: %s\"", ",", "data_files", ")", "\n", "if", "not", "data_files", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"No Data files found\"", ")", "\n", "", "preprocessed_data_files", "=", "self", ".", "_get_preprocessed_files", "(", "data_files", ")", "\n", "self", ".", "data", "=", "read_serialized_data_from_files", "(", "preprocessed_data_files", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ExtractiveReaderDataset._get_preprocessed_files": [[131, 181], ["reader_data.ExtractiveReaderDataset._get_preprocessed_files._find_cached_files"], "methods", ["None"], ["", "def", "_get_preprocessed_files", "(", "\n", "self", ",", "\n", "data_files", ":", "List", ",", "\n", ")", ":", "\n", "\n", "        ", "serialized_files", "=", "[", "file", "for", "file", "in", "data_files", "if", "file", ".", "endswith", "(", "\".pkl\"", ")", "]", "\n", "if", "serialized_files", ":", "\n", "            ", "return", "serialized_files", "\n", "", "assert", "len", "(", "data_files", ")", "==", "1", ",", "\"Only 1 source file pre-processing is supported.\"", "\n", "\n", "# data may have been serialized and cached before, try to find ones from same dir", "\n", "def", "_find_cached_files", "(", "path", ":", "str", ")", ":", "\n", "            ", "dir_path", ",", "base_name", "=", "os", ".", "path", ".", "split", "(", "path", ")", "\n", "base_name", "=", "base_name", ".", "replace", "(", "\".json\"", ",", "\"\"", ")", "\n", "out_file_prefix", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "base_name", ")", "\n", "out_file_pattern", "=", "out_file_prefix", "+", "\"*.pkl\"", "\n", "return", "glob", ".", "glob", "(", "out_file_pattern", ")", ",", "out_file_prefix", "\n", "\n", "", "serialized_files", ",", "out_file_prefix", "=", "_find_cached_files", "(", "data_files", "[", "0", "]", ")", "\n", "if", "serialized_files", ":", "\n", "            ", "logger", ".", "info", "(", "\"Found preprocessed files. %s\"", ",", "serialized_files", ")", "\n", "return", "serialized_files", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Data are not preprocessed for reader training. Start pre-processing ...\"", "\n", ")", "\n", "\n", "# start pre-processing and save results", "\n", "def", "_run_preprocessing", "(", "tensorizer", ":", "Tensorizer", ")", ":", "\n", "# temporarily disable auto-padding to save disk space usage of serialized files", "\n", "            ", "tensorizer", ".", "set_pad_to_max", "(", "False", ")", "\n", "serialized_files", "=", "convert_retriever_results", "(", "\n", "self", ".", "is_train", ",", "\n", "data_files", "[", "0", "]", ",", "\n", "out_file_prefix", ",", "\n", "self", ".", "gold_passages_src", ",", "\n", "self", ".", "tensorizer", ",", "\n", "num_workers", "=", "self", ".", "num_workers", ",", "\n", ")", "\n", "tensorizer", ".", "set_pad_to_max", "(", "True", ")", "\n", "return", "serialized_files", "\n", "\n", "", "if", "self", ".", "run_preprocessing", ":", "\n", "            ", "serialized_files", "=", "_run_preprocessing", "(", "self", ".", "tensorizer", ")", "\n", "# TODO: check if pytorch process group is initialized", "\n", "# torch.distributed.barrier()", "\n", "", "else", ":", "\n", "# torch.distributed.barrier()", "\n", "            ", "serialized_files", "=", "_find_cached_files", "(", "data_files", "[", "0", "]", ")", "\n", "", "return", "serialized_files", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.preprocess_retriever_data": [[223, 319], ["tensorizer.get_pair_separator_ids", "logger.info", "logger.info", "reader_data._get_gold_ctx_dict", "tensorizer.text_to_tensor", "reader_data._concat_pair", "reader_data._select_reader_passages", "next", "tensorizer.text_to_tensor", "reader_data.preprocess_retriever_data.create_reader_sample_ids"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.get_pair_separator_ids", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._get_gold_ctx_dict", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._concat_pair", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._select_reader_passages", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["def", "preprocess_retriever_data", "(", "\n", "samples", ":", "List", "[", "Dict", "]", ",", "\n", "gold_info_file", ":", "Optional", "[", "str", "]", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "cfg", ":", "ReaderPreprocessingCfg", "=", "DEFAULT_PREPROCESSING_CFG_TRAIN", ",", "\n", "is_train_set", ":", "bool", "=", "True", ",", "\n", ")", "->", "Iterable", "[", "ReaderSample", "]", ":", "\n", "    ", "\"\"\"\n    Converts retriever results into reader training data.\n    :param samples: samples from the retriever's json file results\n    :param gold_info_file: optional path for the 'gold passages & questions' file. Required to get best results for NQ\n    :param tensorizer: Tensorizer object for text to model input tensors conversions\n    :param cfg: ReaderPreprocessingCfg object with positive and negative passage selection parameters\n    :param is_train_set: if the data should be processed as a train set\n    :return: iterable of ReaderSample objects which can be consumed by the reader model\n    \"\"\"", "\n", "sep_tensor", "=", "tensorizer", ".", "get_pair_separator_ids", "(", ")", "# separator can be a multi token", "\n", "\n", "gold_passage_map", ",", "canonical_questions", "=", "(", "\n", "_get_gold_ctx_dict", "(", "gold_info_file", ")", "if", "gold_info_file", "else", "(", "{", "}", ",", "{", "}", ")", "\n", ")", "\n", "\n", "no_positive_passages", "=", "0", "\n", "positives_from_gold", "=", "0", "\n", "\n", "def", "create_reader_sample_ids", "(", "sample", ":", "ReaderPassage", ",", "question", ":", "str", ")", ":", "\n", "        ", "question_and_title", "=", "tensorizer", ".", "text_to_tensor", "(", "\n", "sample", ".", "title", ",", "title", "=", "question", ",", "add_special_tokens", "=", "True", "\n", ")", "\n", "if", "sample", ".", "passage_token_ids", "is", "None", ":", "\n", "            ", "sample", ".", "passage_token_ids", "=", "tensorizer", ".", "text_to_tensor", "(", "\n", "sample", ".", "passage_text", ",", "add_special_tokens", "=", "False", "\n", ")", "\n", "\n", "", "all_concatenated", ",", "shift", "=", "_concat_pair", "(", "\n", "question_and_title", ",", "\n", "sample", ".", "passage_token_ids", ",", "\n", "tailing_sep", "=", "sep_tensor", "if", "cfg", ".", "use_tailing_sep", "else", "None", ",", "\n", ")", "\n", "\n", "sample", ".", "sequence_ids", "=", "all_concatenated", "\n", "sample", ".", "passage_offset", "=", "shift", "\n", "assert", "shift", ">", "1", "\n", "if", "sample", ".", "has_answer", "and", "is_train_set", ":", "\n", "            ", "sample", ".", "answers_spans", "=", "[", "\n", "(", "span", "[", "0", "]", "+", "shift", ",", "span", "[", "1", "]", "+", "shift", ")", "for", "span", "in", "sample", ".", "answers_spans", "\n", "]", "\n", "", "return", "sample", "\n", "\n", "", "for", "sample", "in", "samples", ":", "\n", "        ", "question", "=", "sample", "[", "\"question\"", "]", "\n", "\n", "if", "question", "in", "canonical_questions", ":", "\n", "            ", "question", "=", "canonical_questions", "[", "question", "]", "\n", "\n", "", "positive_passages", ",", "negative_passages", "=", "_select_reader_passages", "(", "\n", "sample", ",", "\n", "question", ",", "\n", "tensorizer", ",", "\n", "gold_passage_map", ",", "\n", "cfg", ".", "gold_page_only_positives", ",", "\n", "cfg", ".", "max_positives", ",", "\n", "cfg", ".", "max_negatives", ",", "\n", "cfg", ".", "min_negatives", ",", "\n", "cfg", ".", "max_retriever_passages", ",", "\n", "cfg", ".", "include_gold_passage", ",", "\n", "is_train_set", ",", "\n", ")", "\n", "# create concatenated sequence ids for each passage and adjust answer spans", "\n", "positive_passages", "=", "[", "\n", "create_reader_sample_ids", "(", "s", ",", "question", ")", "for", "s", "in", "positive_passages", "\n", "]", "\n", "negative_passages", "=", "[", "\n", "create_reader_sample_ids", "(", "s", ",", "question", ")", "for", "s", "in", "negative_passages", "\n", "]", "\n", "\n", "if", "is_train_set", "and", "len", "(", "positive_passages", ")", "==", "0", ":", "\n", "            ", "no_positive_passages", "+=", "1", "\n", "if", "cfg", ".", "skip_no_positves", ":", "\n", "                ", "continue", "\n", "\n", "", "", "if", "next", "(", "iter", "(", "ctx", "for", "ctx", "in", "positive_passages", "if", "ctx", ".", "score", "==", "-", "1", ")", ",", "None", ")", ":", "\n", "            ", "positives_from_gold", "+=", "1", "\n", "\n", "", "if", "is_train_set", ":", "\n", "            ", "yield", "ReaderSample", "(", "\n", "question", ",", "\n", "sample", "[", "\"answers\"", "]", ",", "\n", "positive_passages", "=", "positive_passages", ",", "\n", "negative_passages", "=", "negative_passages", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "yield", "ReaderSample", "(", "question", ",", "sample", "[", "\"answers\"", "]", ",", "passages", "=", "negative_passages", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"no positive passages samples: %d\"", ",", "no_positive_passages", ")", "\n", "logger", ".", "info", "(", "\"positive passages from gold samples: %d\"", ",", "positives_from_gold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.convert_retriever_results": [[321, 371], ["logger.info", "multiprocessing.Pool", "len", "max", "logger.info", "functools.partial", "multiprocessing.Pool.map", "logger.info", "open", "json.loads", "len", "math.ceil", "len", "serialized_files.append", "logger.info", "logger.info", "range", "range", "f.readlines", "len"], "function", ["None"], ["", "def", "convert_retriever_results", "(", "\n", "is_train_set", ":", "bool", ",", "\n", "input_file", ":", "str", ",", "\n", "out_file_prefix", ":", "str", ",", "\n", "gold_passages_file", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "num_workers", ":", "int", "=", "8", ",", "\n", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts the file with dense retriever(or any compatible file format) results into the reader input data and\n    serializes them into a set of files.\n    Conversion splits the input data into multiple chunks and processes them in parallel. Each chunk results are stored\n    in a separate file with name out_file_prefix.{number}.pkl\n    :param is_train_set: if the data should be processed for a train set (i.e. with answer span detection)\n    :param input_file: path to a json file with data to convert\n    :param out_file_prefix: output path prefix.\n    :param gold_passages_file: optional path for the 'gold passages & questions' file. Required to get best results for NQ\n    :param tensorizer: Tensorizer object for text to model input tensors conversions\n    :param num_workers: the number of parallel processes for conversion\n    :return: names of files with serialized results\n    \"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "samples", "=", "json", ".", "loads", "(", "\"\"", ".", "join", "(", "f", ".", "readlines", "(", ")", ")", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"Loaded %d questions + retrieval results from %s\"", ",", "len", "(", "samples", ")", ",", "input_file", "\n", ")", "\n", "workers", "=", "multiprocessing", ".", "Pool", "(", "num_workers", ")", "\n", "ds_size", "=", "len", "(", "samples", ")", "\n", "step", "=", "max", "(", "math", ".", "ceil", "(", "ds_size", "/", "num_workers", ")", ",", "1", ")", "\n", "chunks", "=", "[", "samples", "[", "i", ":", "i", "+", "step", "]", "for", "i", "in", "range", "(", "0", ",", "ds_size", ",", "step", ")", "]", "\n", "chunks", "=", "[", "(", "i", ",", "chunks", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "chunks", ")", ")", "]", "\n", "\n", "logger", ".", "info", "(", "\"Split data into %d chunks\"", ",", "len", "(", "chunks", ")", ")", "\n", "\n", "processed", "=", "0", "\n", "_parse_batch", "=", "partial", "(", "\n", "_preprocess_reader_samples_chunk", ",", "\n", "out_file_prefix", "=", "out_file_prefix", ",", "\n", "gold_passages_file", "=", "gold_passages_file", ",", "\n", "tensorizer", "=", "tensorizer", ",", "\n", "is_train_set", "=", "is_train_set", ",", "\n", ")", "\n", "serialized_files", "=", "[", "]", "\n", "for", "file_name", "in", "workers", ".", "map", "(", "_parse_batch", ",", "chunks", ")", ":", "\n", "        ", "processed", "+=", "1", "\n", "serialized_files", ".", "append", "(", "file_name", ")", "\n", "logger", ".", "info", "(", "\"Chunks processed %d\"", ",", "processed", ")", "\n", "logger", ".", "info", "(", "\"Data saved to %s\"", ",", "file_name", ")", "\n", "", "logger", ".", "info", "(", "\"Preprocessed data stored in %s\"", ",", "serialized_files", ")", "\n", "return", "serialized_files", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.get_best_spans": [[373, 426], ["enumerate", "sorted", "enumerate", "any", "reader_data._extend_span_to_full_words", "tensorizer.to_string", "best_spans.append", "chosen_span_intervals.append", "sorted.append", "SpanPrediction", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._extend_span_to_full_words", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.to_string"], ["", "def", "get_best_spans", "(", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "start_logits", ":", "List", ",", "\n", "end_logits", ":", "List", ",", "\n", "ctx_ids", ":", "List", ",", "\n", "max_answer_length", ":", "int", ",", "\n", "passage_idx", ":", "int", ",", "\n", "relevance_score", ":", "float", ",", "\n", "top_spans", ":", "int", "=", "1", ",", "\n", ")", "->", "List", "[", "SpanPrediction", "]", ":", "\n", "    ", "\"\"\"\n    Finds the best answer span for the extractive Q&A model\n    \"\"\"", "\n", "scores", "=", "[", "]", "\n", "for", "(", "i", ",", "s", ")", "in", "enumerate", "(", "start_logits", ")", ":", "\n", "        ", "for", "(", "j", ",", "e", ")", "in", "enumerate", "(", "end_logits", "[", "i", ":", "i", "+", "max_answer_length", "]", ")", ":", "\n", "            ", "scores", ".", "append", "(", "(", "(", "i", ",", "i", "+", "j", ")", ",", "s", "+", "e", ")", ")", "\n", "\n", "", "", "scores", "=", "sorted", "(", "scores", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "chosen_span_intervals", "=", "[", "]", "\n", "best_spans", "=", "[", "]", "\n", "\n", "for", "(", "start_index", ",", "end_index", ")", ",", "score", "in", "scores", ":", "\n", "        ", "assert", "start_index", "<=", "end_index", "\n", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "assert", "length", "<=", "max_answer_length", "\n", "\n", "if", "any", "(", "\n", "[", "\n", "start_index", "<=", "prev_start_index", "<=", "prev_end_index", "<=", "end_index", "\n", "or", "prev_start_index", "<=", "start_index", "<=", "end_index", "<=", "prev_end_index", "\n", "for", "(", "prev_start_index", ",", "prev_end_index", ")", "in", "chosen_span_intervals", "\n", "]", "\n", ")", ":", "\n", "            ", "continue", "\n", "\n", "# extend bpe subtokens to full tokens", "\n", "", "start_index", ",", "end_index", "=", "_extend_span_to_full_words", "(", "\n", "tensorizer", ",", "ctx_ids", ",", "(", "start_index", ",", "end_index", ")", "\n", ")", "\n", "\n", "predicted_answer", "=", "tensorizer", ".", "to_string", "(", "ctx_ids", "[", "start_index", ":", "end_index", "+", "1", "]", ")", "\n", "best_spans", ".", "append", "(", "\n", "SpanPrediction", "(", "\n", "predicted_answer", ",", "score", ",", "relevance_score", ",", "passage_idx", ",", "ctx_ids", "\n", ")", "\n", ")", "\n", "chosen_span_intervals", ".", "append", "(", "(", "start_index", ",", "end_index", ")", ")", "\n", "\n", "if", "len", "(", "chosen_span_intervals", ")", "==", "top_spans", ":", "\n", "            ", "break", "\n", "", "", "return", "best_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._select_reader_passages": [[428, 535], ["list", "tensorizer.text_to_tensor", "list", "list", "list", "filter", "next", "min", "reader_data.ReaderPassage", "filter", "filter", "filter", "list", "bool", "list", "iter", "reader_data._select_reader_passages.find_answer_spans"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.text_to_tensor"], ["", "def", "_select_reader_passages", "(", "\n", "sample", ":", "Dict", ",", "\n", "question", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "gold_passage_map", ":", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "\n", "gold_page_only_positives", ":", "bool", ",", "\n", "max_positives", ":", "int", ",", "\n", "max1_negatives", ":", "int", ",", "\n", "max2_negatives", ":", "int", ",", "\n", "max_retriever_passages", ":", "int", ",", "\n", "include_gold_passage", ":", "bool", ",", "\n", "is_train_set", ":", "bool", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "ReaderPassage", "]", ",", "List", "[", "ReaderPassage", "]", "]", ":", "\n", "    ", "answers", "=", "sample", "[", "\"answers\"", "]", "\n", "\n", "ctxs", "=", "[", "ReaderPassage", "(", "**", "ctx", ")", "for", "ctx", "in", "sample", "[", "\"ctxs\"", "]", "]", "[", "0", ":", "max_retriever_passages", "]", "\n", "answers_token_ids", "=", "[", "\n", "tensorizer", ".", "text_to_tensor", "(", "a", ",", "add_special_tokens", "=", "False", ")", "for", "a", "in", "answers", "\n", "]", "\n", "\n", "if", "is_train_set", ":", "\n", "        ", "positive_samples", "=", "list", "(", "filter", "(", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "ctxs", ")", ")", "\n", "negative_samples", "=", "list", "(", "filter", "(", "lambda", "ctx", ":", "not", "ctx", ".", "has_answer", ",", "ctxs", ")", ")", "\n", "", "else", ":", "\n", "        ", "positive_samples", "=", "[", "]", "\n", "negative_samples", "=", "ctxs", "\n", "\n", "", "positive_ctxs_from_gold_page", "=", "(", "\n", "list", "(", "\n", "filter", "(", "\n", "lambda", "ctx", ":", "_is_from_gold_wiki_page", "(", "\n", "gold_passage_map", ",", "ctx", ".", "title", ",", "question", "\n", ")", ",", "\n", "positive_samples", ",", "\n", ")", "\n", ")", "\n", "if", "gold_page_only_positives", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "def", "find_answer_spans", "(", "ctx", ":", "ReaderPassage", ")", ":", "\n", "        ", "if", "ctx", ".", "has_answer", ":", "\n", "            ", "if", "ctx", ".", "passage_token_ids", "is", "None", ":", "\n", "                ", "ctx", ".", "passage_token_ids", "=", "tensorizer", ".", "text_to_tensor", "(", "\n", "ctx", ".", "passage_text", ",", "add_special_tokens", "=", "False", "\n", ")", "\n", "\n", "", "answer_spans", "=", "[", "\n", "_find_answer_positions", "(", "ctx", ".", "passage_token_ids", ",", "answers_token_ids", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "answers", ")", ")", "\n", "]", "\n", "\n", "# flatten spans list", "\n", "answer_spans", "=", "[", "item", "for", "sublist", "in", "answer_spans", "for", "item", "in", "sublist", "]", "\n", "answers_spans", "=", "list", "(", "filter", "(", "None", ",", "answer_spans", ")", ")", "\n", "ctx", ".", "answers_spans", "=", "answers_spans", "\n", "\n", "if", "not", "answers_spans", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"No answer found in passage id=%s text=%s, answers=%s, question=%s\"", ",", "\n", "ctx", ".", "id", ",", "\n", "ctx", ".", "passage_text", ",", "\n", "answers", ",", "\n", "question", ",", "\n", ")", "\n", "\n", "", "ctx", ".", "has_answer", "=", "bool", "(", "answers_spans", ")", "\n", "\n", "", "return", "ctx", "\n", "\n", "# check if any of the selected ctx+ has answer spans", "\n", "", "selected_positive_ctxs", "=", "list", "(", "\n", "filter", "(", "\n", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "\n", "[", "find_answer_spans", "(", "ctx", ")", "for", "ctx", "in", "positive_ctxs_from_gold_page", "]", ",", "\n", ")", "\n", ")", "\n", "\n", "if", "not", "selected_positive_ctxs", ":", "# fallback to positive ctx not from gold pages", "\n", "        ", "selected_positive_ctxs", "=", "list", "(", "\n", "filter", "(", "\n", "lambda", "ctx", ":", "ctx", ".", "has_answer", ",", "\n", "[", "find_answer_spans", "(", "ctx", ")", "for", "ctx", "in", "positive_samples", "]", ",", "\n", ")", "\n", ")", "[", "0", ":", "max_positives", "]", "\n", "\n", "# optionally include gold passage itself if it is still not in the positives list", "\n", "", "if", "include_gold_passage", "and", "question", "in", "gold_passage_map", ":", "\n", "        ", "gold_passage", "=", "gold_passage_map", "[", "question", "]", "\n", "included_gold_passage", "=", "next", "(", "\n", "iter", "(", "ctx", "for", "ctx", "in", "selected_positive_ctxs", "if", "ctx", ".", "id", "==", "gold_passage", ".", "id", ")", ",", "\n", "None", ",", "\n", ")", "\n", "if", "not", "included_gold_passage", ":", "\n", "            ", "gold_passage", "=", "find_answer_spans", "(", "gold_passage", ")", "\n", "if", "not", "gold_passage", ".", "has_answer", ":", "\n", "                ", "logger", ".", "warning", "(", "\"No answer found in gold passage %s\"", ",", "gold_passage", ")", "\n", "", "else", ":", "\n", "                ", "selected_positive_ctxs", ".", "append", "(", "gold_passage", ")", "\n", "\n", "", "", "", "max_negatives", "=", "(", "\n", "min", "(", "max", "(", "10", "*", "len", "(", "selected_positive_ctxs", ")", ",", "max1_negatives", ")", ",", "max2_negatives", ")", "\n", "if", "is_train_set", "\n", "else", "DEFAULT_EVAL_PASSAGES", "\n", ")", "\n", "negative_samples", "=", "negative_samples", "[", "0", ":", "max_negatives", "]", "\n", "return", "selected_positive_ctxs", ",", "negative_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._find_answer_positions": [[537, 545], ["ctx_ids.size", "answer.size", "range", "answer_occurences.append"], "function", ["None"], ["", "def", "_find_answer_positions", "(", "ctx_ids", ":", "T", ",", "answer", ":", "T", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "c_len", "=", "ctx_ids", ".", "size", "(", "0", ")", "\n", "a_len", "=", "answer", ".", "size", "(", "0", ")", "\n", "answer_occurences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "c_len", "-", "a_len", "+", "1", ")", ":", "\n", "        ", "if", "(", "answer", "==", "ctx_ids", "[", "i", ":", "i", "+", "a_len", "]", ")", ".", "all", "(", ")", ":", "\n", "            ", "answer_occurences", ".", "append", "(", "(", "i", ",", "i", "+", "a_len", "-", "1", ")", ")", "\n", "", "", "return", "answer_occurences", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._concat_pair": [[547, 551], ["torch.cat", "t1.size", "len"], "function", ["None"], ["", "def", "_concat_pair", "(", "t1", ":", "T", ",", "t2", ":", "T", ",", "middle_sep", ":", "T", "=", "None", ",", "tailing_sep", ":", "T", "=", "None", ")", ":", "\n", "    ", "middle", "=", "[", "middle_sep", "]", "if", "middle_sep", "else", "[", "]", "\n", "r", "=", "[", "t1", "]", "+", "middle", "+", "[", "t2", "]", "+", "(", "[", "tailing_sep", "]", "if", "tailing_sep", "else", "[", "]", ")", "\n", "return", "torch", ".", "cat", "(", "r", ",", "dim", "=", "0", ")", ",", "t1", ".", "size", "(", "0", ")", "+", "len", "(", "middle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._get_gold_ctx_dict": [[553, 594], ["open", "logger.info", "sample[].lower", "reader_data.ReaderPassage", "json.load", "logger.info", "logger.info", "logger.info", "logger.info"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "_get_gold_ctx_dict", "(", "file", ":", "str", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "Dict", "[", "str", ",", "str", "]", "]", ":", "\n", "    ", "gold_passage_infos", "=", "(", "\n", "{", "}", "\n", ")", "# question|question_tokens -> ReaderPassage (with title and gold ctx)", "\n", "\n", "# original NQ dataset has 2 forms of same question - original, and tokenized.", "\n", "# Tokenized form is not fully consisted with the original question if tokenized by some encoder tokenizers", "\n", "# Specifically, this is the case for the BERT tokenizer.", "\n", "# Depending of which form was used for retriever training and results generation, it may be useful to convert", "\n", "# all questions to the canonical original representation.", "\n", "original_questions", "=", "{", "}", "# question from tokens -> original question (NQ only)", "\n", "\n", "with", "open", "(", "file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading file %s\"", "%", "file", ")", "\n", "data", "=", "json", ".", "load", "(", "f", ")", "[", "\"data\"", "]", "\n", "\n", "", "for", "sample", "in", "data", ":", "\n", "        ", "question", "=", "sample", "[", "\"question\"", "]", "\n", "question_from_tokens", "=", "(", "\n", "sample", "[", "\"question_tokens\"", "]", "if", "\"question_tokens\"", "in", "sample", "else", "question", "\n", ")", "\n", "original_questions", "[", "question_from_tokens", "]", "=", "question", "\n", "title", "=", "sample", "[", "\"title\"", "]", ".", "lower", "(", ")", "\n", "context", "=", "sample", "[", "\"context\"", "]", "# Note: This one is cased", "\n", "rp", "=", "ReaderPassage", "(", "sample", "[", "\"example_id\"", "]", ",", "text", "=", "context", ",", "title", "=", "title", ")", "\n", "if", "question", "in", "gold_passage_infos", ":", "\n", "            ", "logger", ".", "info", "(", "\"Duplicate question %s\"", ",", "question", ")", "\n", "rp_exist", "=", "gold_passage_infos", "[", "question", "]", "\n", "logger", ".", "info", "(", "\n", "\"Duplicate question gold info: title new =%s | old title=%s\"", ",", "\n", "title", ",", "\n", "rp_exist", ".", "title", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Duplicate question gold info: new ctx =%s \"", ",", "context", ")", "\n", "logger", ".", "info", "(", "\n", "\"Duplicate question gold info: old ctx =%s \"", ",", "rp_exist", ".", "passage_text", "\n", ")", "\n", "\n", "", "gold_passage_infos", "[", "question", "]", "=", "rp", "\n", "gold_passage_infos", "[", "question_from_tokens", "]", "=", "rp", "\n", "", "return", "gold_passage_infos", ",", "original_questions", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._is_from_gold_wiki_page": [[596, 603], ["gold_passage_map.get", "passage_title.lower", "gold_passage_map.get.title.lower"], "function", ["None"], ["", "def", "_is_from_gold_wiki_page", "(", "\n", "gold_passage_map", ":", "Dict", "[", "str", ",", "ReaderPassage", "]", ",", "passage_title", ":", "str", ",", "question", ":", "str", "\n", ")", ":", "\n", "    ", "gold_info", "=", "gold_passage_map", ".", "get", "(", "question", ",", "None", ")", "\n", "if", "gold_info", ":", "\n", "        ", "return", "passage_title", ".", "lower", "(", ")", "==", "gold_info", ".", "title", ".", "lower", "(", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._extend_span_to_full_words": [[605, 617], ["len", "tensorizer.is_sub_word_id", "tensorizer.is_sub_word_id"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.is_sub_word_id", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.models.hf_models.BertTensorizer.is_sub_word_id"], ["", "def", "_extend_span_to_full_words", "(", "\n", "tensorizer", ":", "Tensorizer", ",", "tokens", ":", "List", "[", "int", "]", ",", "span", ":", "Tuple", "[", "int", ",", "int", "]", "\n", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "    ", "start_index", ",", "end_index", "=", "span", "\n", "max_len", "=", "len", "(", "tokens", ")", "\n", "while", "start_index", ">", "0", "and", "tensorizer", ".", "is_sub_word_id", "(", "tokens", "[", "start_index", "]", ")", ":", "\n", "        ", "start_index", "-=", "1", "\n", "\n", "", "while", "end_index", "<", "max_len", "-", "1", "and", "tensorizer", ".", "is_sub_word_id", "(", "tokens", "[", "end_index", "+", "1", "]", ")", ":", "\n", "        ", "end_index", "+=", "1", "\n", "\n", "", "return", "start_index", ",", "end_index", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data._preprocess_reader_samples_chunk": [[619, 647], ["logger.info", "reader_data.preprocess_retriever_data", "tqdm.tqdm", "enumerate", "len", "r.on_serialize", "results.append", "open", "logger.info", "pickle.dump", "str", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.preprocess_retriever_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.reader_data.ReaderSample.on_serialize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["", "def", "_preprocess_reader_samples_chunk", "(", "\n", "samples", ":", "List", ",", "\n", "out_file_prefix", ":", "str", ",", "\n", "gold_passages_file", ":", "str", ",", "\n", "tensorizer", ":", "Tensorizer", ",", "\n", "is_train_set", ":", "bool", ",", "\n", ")", "->", "str", ":", "\n", "    ", "chunk_id", ",", "samples", "=", "samples", "\n", "logger", ".", "info", "(", "\"Start batch %d\"", ",", "len", "(", "samples", ")", ")", "\n", "iterator", "=", "preprocess_retriever_data", "(", "\n", "samples", ",", "\n", "gold_passages_file", ",", "\n", "tensorizer", ",", "\n", "is_train_set", "=", "is_train_set", ",", "\n", ")", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "iterator", "=", "tqdm", "(", "iterator", ")", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "iterator", ")", ":", "\n", "        ", "r", ".", "on_serialize", "(", ")", "\n", "results", ".", "append", "(", "r", ")", "\n", "\n", "", "out_file", "=", "out_file_prefix", "+", "\".\"", "+", "str", "(", "chunk_id", ")", "+", "\".pkl\"", "\n", "with", "open", "(", "out_file", ",", "mode", "=", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "logger", ".", "info", "(", "\"Serialize %d results to %s\"", ",", "len", "(", "results", ")", ",", "out_file", ")", "\n", "pickle", ".", "dump", "(", "results", ",", "f", ")", "\n", "", "return", "out_file", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.__init__": [[24, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "buffer_size", ":", "int", "=", "50000", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "index_id_to_db_id", "=", "[", "]", "\n", "self", ".", "index", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.init_index": [[29, 31], ["None"], "methods", ["None"], ["", "def", "init_index", "(", "self", ",", "vector_sz", ":", "int", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.index_data": [[32, 34], ["None"], "methods", ["None"], ["", "def", "index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.get_index_name": [[35, 37], ["None"], "methods", ["None"], ["", "def", "get_index_name", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.search_knn": [[38, 42], ["None"], "methods", ["None"], ["", "def", "search_knn", "(", "\n", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "\n", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.serialize": [[43, 56], ["logger.info", "os.path.isdir", "faiss.write_index", "os.path.join", "os.path.join", "open", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["", "def", "serialize", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Serializing index to %s\"", ",", "file", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "file", ")", ":", "\n", "            ", "index_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index.dpr\"", ")", "\n", "meta_file", "=", "os", ".", "path", ".", "join", "(", "file", ",", "\"index_meta.dpr\"", ")", "\n", "", "else", ":", "\n", "            ", "index_file", "=", "file", "+", "\".index.dpr\"", "\n", "meta_file", "=", "file", "+", "\".index_meta.dpr\"", "\n", "\n", "", "faiss", ".", "write_index", "(", "self", ".", "index", ",", "index_file", ")", "\n", "with", "open", "(", "meta_file", ",", "mode", "=", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "index_id_to_db_id", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.get_files": [[57, 65], ["os.path.isdir", "os.path.join", "os.path.join", "faiss_indexers.DenseIndexer.get_index_name", "faiss_indexers.DenseIndexer.get_index_name"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.get_index_name", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.get_index_name"], ["", "", "def", "get_files", "(", "self", ",", "path", ":", "str", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "\n", "            ", "index_file", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"index.dpr\"", ")", "\n", "meta_file", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"index_meta.dpr\"", ")", "\n", "", "else", ":", "\n", "            ", "index_file", "=", "path", "+", "\".{}.dpr\"", ".", "format", "(", "self", ".", "get_index_name", "(", ")", ")", "\n", "meta_file", "=", "path", "+", "\".{}_meta.dpr\"", ".", "format", "(", "self", ".", "get_index_name", "(", ")", ")", "\n", "", "return", "index_file", ",", "meta_file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.index_exists": [[66, 69], ["faiss_indexers.DenseIndexer.get_files", "os.path.isfile", "os.path.isfile"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.get_files"], ["", "def", "index_exists", "(", "self", ",", "path", ":", "str", ")", ":", "\n", "        ", "index_file", ",", "meta_file", "=", "self", ".", "get_files", "(", "path", ")", "\n", "return", "os", ".", "path", ".", "isfile", "(", "index_file", ")", "and", "os", ".", "path", ".", "isfile", "(", "meta_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.deserialize": [[70, 84], ["logger.info", "faiss_indexers.DenseIndexer.get_files", "faiss.read_index", "logger.info", "type", "open", "pickle.load", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer.get_files", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "deserialize", "(", "self", ",", "path", ":", "str", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading index from %s\"", ",", "path", ")", "\n", "index_file", ",", "meta_file", "=", "self", ".", "get_files", "(", "path", ")", "\n", "\n", "self", ".", "index", "=", "faiss", ".", "read_index", "(", "index_file", ")", "\n", "logger", ".", "info", "(", "\n", "\"Loaded index of type %s and size %d\"", ",", "type", "(", "self", ".", "index", ")", ",", "self", ".", "index", ".", "ntotal", "\n", ")", "\n", "\n", "with", "open", "(", "meta_file", ",", "\"rb\"", ")", "as", "reader", ":", "\n", "            ", "self", ".", "index_id_to_db_id", "=", "pickle", ".", "load", "(", "reader", ")", "\n", "", "assert", "(", "\n", "len", "(", "self", ".", "index_id_to_db_id", ")", "==", "self", ".", "index", ".", "ntotal", "\n", ")", ",", "\"Deserialized index_id_to_db_id should match faiss index size\"", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer._update_id_mapping": [[85, 88], ["faiss_indexers.DenseIndexer.index_id_to_db_id.extend", "len"], "methods", ["None"], ["", "def", "_update_id_mapping", "(", "self", ",", "db_ids", ":", "List", ")", "->", "int", ":", "\n", "        ", "self", ".", "index_id_to_db_id", ".", "extend", "(", "db_ids", ")", "\n", "return", "len", "(", "self", ".", "index_id_to_db_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.__init__": [[91, 93], ["faiss_indexers.DenseIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "buffer_size", ":", "int", "=", "50000", ")", ":", "\n", "        ", "super", "(", "DenseFlatIndexer", ",", "self", ")", ".", "__init__", "(", "buffer_size", "=", "buffer_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.init_index": [[94, 96], ["faiss.IndexFlatIP"], "methods", ["None"], ["", "def", "init_index", "(", "self", ",", "vector_sz", ":", "int", ")", ":", "\n", "        ", "self", ".", "index", "=", "faiss", ".", "IndexFlatIP", "(", "vector_sz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.index_data": [[97, 112], ["len", "range", "len", "logger.info", "numpy.concatenate", "faiss_indexers.DenseFlatIndexer._update_id_mapping", "faiss_indexers.DenseFlatIndexer.index.add", "logger.info", "numpy.reshape"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer._update_id_mapping"], ["", "def", "index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "n", "=", "len", "(", "data", ")", "\n", "# indexing in batches is beneficial for many faiss index types", "\n", "for", "i", "in", "range", "(", "0", ",", "n", ",", "self", ".", "buffer_size", ")", ":", "\n", "            ", "db_ids", "=", "[", "t", "[", "0", "]", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "]", "\n", "vectors", "=", "[", "\n", "np", ".", "reshape", "(", "t", "[", "1", "]", ",", "(", "1", ",", "-", "1", ")", ")", "for", "t", "in", "data", "[", "i", ":", "i", "+", "self", ".", "buffer_size", "]", "\n", "]", "\n", "vectors", "=", "np", ".", "concatenate", "(", "vectors", ",", "axis", "=", "0", ")", "\n", "total_data", "=", "self", ".", "_update_id_mapping", "(", "db_ids", ")", "\n", "self", ".", "index", ".", "add", "(", "vectors", ")", "\n", "logger", ".", "info", "(", "\"data indexed %d\"", ",", "total_data", ")", "\n", "\n", "", "indexed_cnt", "=", "len", "(", "self", ".", "index_id_to_db_id", ")", "\n", "logger", ".", "info", "(", "\"Total data indexed %d\"", ",", "indexed_cnt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.search_knn": [[113, 124], ["faiss_indexers.DenseFlatIndexer.index.search", "range", "len"], "methods", ["None"], ["", "def", "search_knn", "(", "\n", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "\n", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "scores", ",", "indexes", "=", "self", ".", "index", ".", "search", "(", "query_vectors", ",", "top_docs", ")", "\n", "# convert to external ids", "\n", "db_ids", "=", "[", "\n", "[", "self", ".", "index_id_to_db_id", "[", "i", "]", "for", "i", "in", "query_top_idxs", "]", "\n", "for", "query_top_idxs", "in", "indexes", "\n", "]", "\n", "result", "=", "[", "(", "db_ids", "[", "i", "]", ",", "scores", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "db_ids", ")", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseFlatIndexer.get_index_name": [[125, 127], ["None"], "methods", ["None"], ["", "def", "get_index_name", "(", "self", ")", ":", "\n", "        ", "return", "\"flat_index\"", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.__init__": [[134, 146], ["faiss_indexers.DenseIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "buffer_size", ":", "int", "=", "1e9", ",", "\n", "store_n", ":", "int", "=", "512", ",", "\n", "ef_search", ":", "int", "=", "128", ",", "\n", "ef_construction", ":", "int", "=", "200", ",", "\n", ")", ":", "\n", "        ", "super", "(", "DenseHNSWFlatIndexer", ",", "self", ")", ".", "__init__", "(", "buffer_size", "=", "buffer_size", ")", "\n", "self", ".", "store_n", "=", "store_n", "\n", "self", ".", "ef_search", "=", "ef_search", "\n", "self", ".", "ef_construction", "=", "ef_construction", "\n", "self", ".", "phi", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.init_index": [[147, 154], ["faiss.IndexHNSWFlat"], "methods", ["None"], ["", "def", "init_index", "(", "self", ",", "vector_sz", ":", "int", ")", ":", "\n", "# IndexHNSWFlat supports L2 similarity only", "\n", "# so we have to apply DOT -> L2 similairy space conversion with the help of an extra dimension", "\n", "        ", "index", "=", "faiss", ".", "IndexHNSWFlat", "(", "vector_sz", "+", "1", ",", "self", ".", "store_n", ")", "\n", "index", ".", "hnsw", ".", "efSearch", "=", "self", ".", "ef_search", "\n", "index", ".", "hnsw", ".", "efConstruction", "=", "self", ".", "ef_construction", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer._index_data": [[155, 192], ["len", "enumerate", "logger.info", "int", "range", "len", "logger.info", "RuntimeError", "max", "numpy.concatenate", "faiss_indexers.DenseHNSWFlatIndexer.train", "faiss_indexers.DenseHNSWFlatIndexer._update_id_mapping", "faiss_indexers.DenseHNSWFlatIndexer.index.add", "logger.info", "numpy.reshape", "numpy.sqrt", "numpy.hstack", "len", "enumerate", "aux_dims[].reshape"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseIndexer._update_id_mapping"], ["", "def", "_index_data", "(", "self", ",", "data", ":", "List", "[", "Tuple", "[", "object", ",", "np", ".", "array", "]", "]", ")", ":", "\n", "        ", "n", "=", "len", "(", "data", ")", "\n", "\n", "# max norm is required before putting all vectors in the index to convert inner product similarity to L2", "\n", "if", "self", ".", "phi", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"DPR HNSWF index needs to index all data at once,\"", "\n", "\"results will be unpredictable otherwise.\"", "\n", ")", "\n", "", "phi", "=", "0", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "id", ",", "doc_vector", "=", "item", "[", "0", ":", "2", "]", "\n", "norms", "=", "(", "doc_vector", "**", "2", ")", ".", "sum", "(", ")", "\n", "phi", "=", "max", "(", "phi", ",", "norms", ")", "\n", "", "logger", ".", "info", "(", "\"HNSWF DotProduct -> L2 space phi={}\"", ".", "format", "(", "phi", ")", ")", "\n", "self", ".", "phi", "=", "phi", "\n", "\n", "# indexing in batches is beneficial for many faiss index types", "\n", "bs", "=", "int", "(", "self", ".", "buffer_size", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "n", ",", "bs", ")", ":", "\n", "            ", "db_ids", "=", "[", "t", "[", "0", "]", "for", "t", "in", "data", "[", "i", ":", "i", "+", "bs", "]", "]", "\n", "vectors", "=", "[", "np", ".", "reshape", "(", "t", "[", "1", "]", ",", "(", "1", ",", "-", "1", ")", ")", "for", "t", "in", "data", "[", "i", ":", "i", "+", "bs", "]", "]", "\n", "\n", "norms", "=", "[", "(", "doc_vector", "**", "2", ")", ".", "sum", "(", ")", "for", "doc_vector", "in", "vectors", "]", "\n", "aux_dims", "=", "[", "np", ".", "sqrt", "(", "phi", "-", "norm", ")", "for", "norm", "in", "norms", "]", "\n", "hnsw_vectors", "=", "[", "\n", "np", ".", "hstack", "(", "(", "doc_vector", ",", "aux_dims", "[", "i", "]", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "for", "i", ",", "doc_vector", "in", "enumerate", "(", "vectors", ")", "\n", "]", "\n", "hnsw_vectors", "=", "np", ".", "concatenate", "(", "hnsw_vectors", ",", "axis", "=", "0", ")", "\n", "self", ".", "train", "(", "hnsw_vectors", ")", "\n", "\n", "self", ".", "_update_id_mapping", "(", "db_ids", ")", "\n", "self", ".", "index", ".", "add", "(", "hnsw_vectors", ")", "\n", "logger", ".", "info", "(", "\"data indexed %d\"", ",", "len", "(", "self", ".", "index_id_to_db_id", ")", ")", "\n", "", "indexed_cnt", "=", "len", "(", "self", ".", "index_id_to_db_id", ")", "\n", "logger", ".", "info", "(", "\"Total data indexed %d\"", ",", "indexed_cnt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.train": [[193, 195], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "vectors", ":", "np", ".", "array", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.search_knn": [[196, 211], ["numpy.zeros", "numpy.hstack", "logger.info", "faiss_indexers.DenseHNSWFlatIndexer.index.search", "len", "numpy.zeros.reshape", "range", "len"], "methods", ["None"], ["", "def", "search_knn", "(", "\n", "self", ",", "query_vectors", ":", "np", ".", "array", ",", "top_docs", ":", "int", "\n", ")", "->", "List", "[", "Tuple", "[", "List", "[", "object", "]", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "\n", "        ", "aux_dim", "=", "np", ".", "zeros", "(", "len", "(", "query_vectors", ")", ",", "dtype", "=", "\"float32\"", ")", "\n", "query_nhsw_vectors", "=", "np", ".", "hstack", "(", "(", "query_vectors", ",", "aux_dim", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"query_hnsw_vectors %s\"", ",", "query_nhsw_vectors", ".", "shape", ")", "\n", "scores", ",", "indexes", "=", "self", ".", "index", ".", "search", "(", "query_nhsw_vectors", ",", "top_docs", ")", "\n", "# convert to external ids", "\n", "db_ids", "=", "[", "\n", "[", "self", ".", "index_id_to_db_id", "[", "i", "]", "for", "i", "in", "query_top_idxs", "]", "\n", "for", "query_top_idxs", "in", "indexes", "\n", "]", "\n", "result", "=", "[", "(", "db_ids", "[", "i", "]", ",", "scores", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "db_ids", ")", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.deserialize": [[212, 216], ["faiss_indexers.DenseIndexer.deserialize"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.deserialize"], ["", "def", "deserialize", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "super", "(", "DenseHNSWFlatIndexer", ",", "self", ")", ".", "deserialize", "(", "file", ")", "\n", "# to trigger exception on subsequent indexing", "\n", "self", ".", "phi", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWFlatIndexer.get_index_name": [[217, 219], ["None"], "methods", ["None"], ["", "def", "get_index_name", "(", "self", ")", ":", "\n", "        ", "return", "\"hnsw_index\"", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.__init__": [[226, 238], ["faiss_indexers.DenseHNSWFlatIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "buffer_size", ":", "int", "=", "1e10", ",", "\n", "store_n", ":", "int", "=", "128", ",", "\n", "ef_search", ":", "int", "=", "128", ",", "\n", "ef_construction", ":", "int", "=", "200", ",", "\n", ")", ":", "\n", "        ", "super", "(", "DenseHNSWSQIndexer", ",", "self", ")", ".", "__init__", "(", "\n", "buffer_size", "=", "buffer_size", ",", "\n", "store_n", "=", "store_n", ",", "\n", "ef_search", "=", "ef_search", ",", "\n", "ef_construction", "=", "ef_construction", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.init_index": [[240, 249], ["faiss.IndexHNSWSQ"], "methods", ["None"], ["", "def", "init_index", "(", "self", ",", "vector_sz", ":", "int", ")", ":", "\n", "# IndexHNSWFlat supports L2 similarity only", "\n", "# so we have to apply DOT -> L2 similairy space conversion with the help of an extra dimension", "\n", "        ", "index", "=", "faiss", ".", "IndexHNSWSQ", "(", "\n", "vector_sz", "+", "1", ",", "faiss", ".", "ScalarQuantizer", ".", "QT_8bit", ",", "self", ".", "store_n", "\n", ")", "\n", "index", ".", "hnsw", ".", "efSearch", "=", "self", ".", "ef_search", "\n", "index", ".", "hnsw", ".", "efConstruction", "=", "self", ".", "ef_construction", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.train": [[250, 252], ["faiss_indexers.DenseHNSWSQIndexer.index.train"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train"], ["", "def", "train", "(", "self", ",", "vectors", ":", "np", ".", "array", ")", ":", "\n", "        ", "self", ".", "index", ".", "train", "(", "vectors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.indexer.faiss_indexers.DenseHNSWSQIndexer.get_index_name": [[253, 255], ["None"], "methods", ["None"], ["", "def", "get_index_name", "(", "self", ")", ":", "\n", "        ", "return", "\"hnswsq_index\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer": [[11, 31], ["print", "print", "print", "print"], "function", ["None"], ["def", "get_optimizer", "(", "optim", ")", ":", "\n", "# Bind the optimizer", "\n", "    ", "if", "optim", "==", "'rms'", ":", "\n", "        ", "print", "(", "\"Optimizer: Using RMSProp\"", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "RMSprop", "\n", "", "elif", "optim", "==", "'adam'", ":", "\n", "        ", "print", "(", "\"Optimizer: Using Adam\"", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "\n", "", "elif", "optim", "==", "'adamax'", ":", "\n", "        ", "print", "(", "\"Optimizer: Using Adamax\"", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adamax", "\n", "", "elif", "optim", "==", "'sgd'", ":", "\n", "        ", "print", "(", "\"Optimizer: sgd\"", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "\n", "", "elif", "'bert'", "in", "optim", ":", "\n", "        ", "optimizer", "=", "'bert'", "# The bert optimizer will be bind later.", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"Please add your optimizer %s in the list.\"", "%", "optim", "\n", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.parse_args": [[33, 103], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "param.get_optimizer", "torch.manual_seed", "random.seed", "numpy.random.seed"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.parse_args", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.param.get_optimizer"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Data Splits", "\n", "parser", ".", "add_argument", "(", "\"--train\"", ",", "default", "=", "'train'", ")", "\n", "parser", ".", "add_argument", "(", "\"--valid\"", ",", "default", "=", "'valid'", ")", "\n", "parser", ".", "add_argument", "(", "\"--test\"", ",", "default", "=", "None", ")", "\n", "\n", "# Training Hyper-parameters", "\n", "parser", ".", "add_argument", "(", "'--batchSize'", ",", "dest", "=", "'batch_size'", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "default", "=", "'bert'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "9595", ",", "help", "=", "'random seed'", ")", "\n", "\n", "# Debugging", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "type", "=", "str", ",", "default", "=", "'snap/test'", ")", "\n", "parser", ".", "add_argument", "(", "\"--fast\"", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--tiny\"", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--tqdm\"", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "\n", "# Model Loading", "\n", "parser", ".", "add_argument", "(", "'--load'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Load the model (usually the fine-tuned model).'", ")", "\n", "parser", ".", "add_argument", "(", "'--loadLXMERT'", ",", "dest", "=", "'load_lxmert'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Load the pre-trained LXMERT model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--loadLXMERTQA'", ",", "dest", "=", "'load_lxmert_qa'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Load the pre-trained LXMERT model with QA answer head.'", ")", "\n", "parser", ".", "add_argument", "(", "\"--fromScratch\"", ",", "dest", "=", "'from_scratch'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ",", "\n", "help", "=", "'If none of the --load, --loadLXMERT, --loadLXMERTQA is set, '", "\n", "'the model would be trained from scratch. If --fromScratch is'", "\n", "' not specified, the model would load BERT-pre-trained weights by'", "\n", "' default. '", ")", "\n", "\n", "# Optimization", "\n", "parser", ".", "add_argument", "(", "\"--mceLoss\"", ",", "dest", "=", "'mce_loss'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "\n", "# LXRT Model Config", "\n", "# Note: LXRT = L, X, R (three encoders), Transformer", "\n", "parser", ".", "add_argument", "(", "\"--llayers\"", ",", "default", "=", "9", ",", "type", "=", "int", ",", "help", "=", "'Number of Language layers'", ")", "\n", "parser", ".", "add_argument", "(", "\"--xlayers\"", ",", "default", "=", "5", ",", "type", "=", "int", ",", "help", "=", "'Number of CROSS-modality layers.'", ")", "\n", "parser", ".", "add_argument", "(", "\"--rlayers\"", ",", "default", "=", "5", ",", "type", "=", "int", ",", "help", "=", "'Number of object Relationship layers.'", ")", "\n", "\n", "# LXMERT Pre-training Config", "\n", "parser", ".", "add_argument", "(", "\"--taskMatched\"", ",", "dest", "=", "'task_matched'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--taskMaskLM\"", ",", "dest", "=", "'task_mask_lm'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--taskObjPredict\"", ",", "dest", "=", "'task_obj_predict'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--taskQA\"", ",", "dest", "=", "'task_qa'", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--visualLosses\"", ",", "dest", "=", "'visual_losses'", ",", "default", "=", "'obj,attr,feat'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--qaSets\"", ",", "dest", "=", "'qa_sets'", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--wordMaskRate\"", ",", "dest", "=", "'word_mask_rate'", ",", "default", "=", "0.15", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--objMaskRate\"", ",", "dest", "=", "'obj_mask_rate'", ",", "default", "=", "0.15", ",", "type", "=", "float", ")", "\n", "\n", "# Training configuration", "\n", "parser", ".", "add_argument", "(", "\"--multiGPU\"", ",", "action", "=", "'store_const'", ",", "default", "=", "False", ",", "const", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--numWorkers\"", ",", "dest", "=", "'num_workers'", ",", "default", "=", "0", ")", "\n", "\n", "# Parse the arguments.", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Bind optimizer class.", "\n", "args", ".", "optimizer", "=", "get_optimizer", "(", "args", ".", "optim", ")", "\n", "\n", "# Set seeds", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.utils.load_obj_tsv": [[16, 55], ["time.time", "print", "print", "open", "csv.DictReader", "enumerate", "time.time", "data.append", "int", "numpy.frombuffer", "item[].reshape", "item[].setflags", "len", "base64.b64decode", "len"], "function", ["None"], ["def", "load_obj_tsv", "(", "fname", ",", "topk", "=", "None", ")", ":", "\n", "    ", "\"\"\"Load object features from tsv file.\n\n    :param fname: The path to the tsv file.\n    :param topk: Only load features for top K images (lines) in the tsv file.\n        Will load all the features if topk is either -1 or None.\n    :return: A list of image object features where each feature is a dict.\n        See FILENAMES above for the keys in the feature dict.\n    \"\"\"", "\n", "data", "=", "[", "]", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"Start to load Faster-RCNN detected objects from %s\"", "%", "fname", ")", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "f", ",", "FIELDNAMES", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "reader", ")", ":", "\n", "\n", "            ", "for", "key", "in", "[", "'img_h'", ",", "'img_w'", ",", "'num_boxes'", "]", ":", "\n", "                ", "item", "[", "key", "]", "=", "int", "(", "item", "[", "key", "]", ")", "\n", "\n", "", "boxes", "=", "item", "[", "'num_boxes'", "]", "\n", "decode_config", "=", "[", "\n", "(", "'objects_id'", ",", "(", "boxes", ",", ")", ",", "np", ".", "int64", ")", ",", "\n", "(", "'objects_conf'", ",", "(", "boxes", ",", ")", ",", "np", ".", "float32", ")", ",", "\n", "(", "'attrs_id'", ",", "(", "boxes", ",", ")", ",", "np", ".", "int64", ")", ",", "\n", "(", "'attrs_conf'", ",", "(", "boxes", ",", ")", ",", "np", ".", "float32", ")", ",", "\n", "(", "'boxes'", ",", "(", "boxes", ",", "4", ")", ",", "np", ".", "float32", ")", ",", "\n", "(", "'features'", ",", "(", "boxes", ",", "-", "1", ")", ",", "np", ".", "float32", ")", ",", "\n", "]", "\n", "for", "key", ",", "shape", ",", "dtype", "in", "decode_config", ":", "\n", "                ", "item", "[", "key", "]", "=", "np", ".", "frombuffer", "(", "base64", ".", "b64decode", "(", "item", "[", "key", "]", ")", ",", "dtype", "=", "dtype", ")", "\n", "item", "[", "key", "]", "=", "item", "[", "key", "]", ".", "reshape", "(", "shape", ")", "\n", "item", "[", "key", "]", ".", "setflags", "(", "write", "=", "False", ")", "\n", "\n", "", "data", ".", "append", "(", "item", ")", "\n", "if", "topk", "is", "not", "None", "and", "len", "(", "data", ")", "==", "topk", ":", "\n", "                ", "break", "\n", "", "", "", "elapsed_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "print", "(", "\"Loaded %d images in file %s in %d seconds.\"", "%", "(", "len", "(", "data", ")", ",", "fname", ",", "elapsed_time", ")", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_model.OKVQAModel.__init__": [[15, 30], ["torch.Module.__init__", "lxrt.entry.LXRTEncoder", "torch.Sequential", "okvqa_model.OKVQAModel.logit_fc.apply", "torch.Linear", "lxrt.modeling.GeLU", "lxrt.modeling.BertLayerNorm", "torch.Dropout", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply"], ["    ", "def", "__init__", "(", "self", ",", "num_answers", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "lxrt_encoder", "=", "LXRTEncoder", "(", "\n", "args", ",", "\n", "max_seq_length", "=", "MAX_OKVQA_LENGTH", "\n", ")", "\n", "hid_dim", "=", "self", ".", "lxrt_encoder", ".", "dim", "\n", "self", ".", "logit_fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hid_dim", ",", "hid_dim", "*", "2", ")", ",", "\n", "GeLU", "(", ")", ",", "\n", "BertLayerNorm", "(", "hid_dim", "*", "2", ",", "eps", "=", "1e-12", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ",", "\n", "nn", ".", "Linear", "(", "hid_dim", "*", "2", ",", "num_answers", ")", "\n", ")", "\n", "self", ".", "logit_fc", ".", "apply", "(", "self", ".", "lxrt_encoder", ".", "model", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_model.OKVQAModel.forward": [[31, 45], ["okvqa_model.OKVQAModel.lxrt_encoder", "okvqa_model.OKVQAModel.logit_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "feat", ",", "pos", ",", "sent", ")", ":", "\n", "        ", "\"\"\"\n        b -- batch_size, o -- object_number, f -- visual_feature_size\n\n        :param feat: (b, o, f)\n        :param pos:  (b, o, 4)\n        :param sent: (b,) Type -- list of string\n        :param leng: (b,) Type -- int numpy array\n        :return: (b, num_answer) The logit of each answers.\n        \"\"\"", "\n", "x", "=", "self", ".", "lxrt_encoder", "(", "sent", ",", "(", "feat", ",", "pos", ")", ")", "\n", "logit", "=", "self", ".", "logit_fc", "(", "x", ")", "\n", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQADataset.__init__": [[31, 54], ["splits.split", "print", "json.load", "json.load", "okvqa_data.OKVQADataset.data.extend", "open", "open", "len", "len", "json.load", "open", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["\n", "class", "VisBiEncoderSample", "(", "object", ")", ":", "\n", "    ", "query", ":", "str", "\n", "positive_passages", ":", "List", "[", "BiEncoderPassage", "]", "\n", "negative_passages", ":", "List", "[", "BiEncoderPassage", "]", "\n", "hard_negative_passages", ":", "List", "[", "BiEncoderPassage", "]", "\n", "img_id", ":", "str", "\n", "\n", "", "class", "JsonQADataset", "(", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "selector", ":", "DictConfig", "=", "None", ",", "\n", "special_token", ":", "str", "=", "None", ",", "\n", "encoder_type", ":", "str", "=", "None", ",", "\n", "shuffle_positives", ":", "bool", "=", "False", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", "query_special_suffix", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "selector", ",", "\n", "special_token", "=", "special_token", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "shuffle_positives", "=", "shuffle_positives", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQADataset.num_answers": [[55, 58], ["len"], "methods", ["None"], ["query_special_suffix", "=", "query_special_suffix", ",", "\n", ")", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQADataset.__len__": [[59, 61], ["len"], "methods", ["None"], ["self", ".", "data", "=", "[", "]", "\n", "self", ".", "normalize", "=", "normalize", "\n", "logger", ".", "info", "(", "\"Data files: %s\"", ",", "self", ".", "data_files", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.__init__": [[63, 65], ["None"], "methods", ["None"], ["", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "self", ".", "file", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader._load_npy_files": [[66, 95], ["time.time", "print", "print", "open", "json.load", "dict", "numpy.load", "img_info.tolist.tolist.tolist", "int", "int", "data.append", "time.time", "len", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "data_files", ")", ":", "\n", "            ", "self", ".", "data_files", "=", "os", ".", "path", ".", "join", "(", "PROJ_PATH", ",", "self", ".", "data_files", ")", "\n", "", "print", "(", "\"read file\"", ",", "self", ".", "data_files", ")", "\n", "data", "=", "read_data_from_json_files", "(", "self", ".", "data_files", ")", "\n", "# filter those without positive ctx", "\n", "self", ".", "data", "=", "[", "r", "for", "r", "in", "data", "if", "len", "(", "r", "[", "\"ctxs\"", "]", ")", ">", "0", "]", "\n", "logger", ".", "info", "(", "\"Total cleaned data size: {}\"", ".", "format", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "json_sample", "=", "self", ".", "data", "[", "index", "]", "\n", "r", "=", "VisBiEncoderSample", "(", ")", "\n", "r", ".", "query", "=", "self", ".", "_process_query", "(", "json_sample", "[", "\"question\"", "]", ")", "\n", "\n", "positive_ctxs", "=", "json_sample", "[", "\"ctxs\"", "]", "\n", "negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"negative_ctxs\"", "]", "if", "\"negative_ctxs\"", "in", "json_sample", "else", "[", "]", "\n", ")", "\n", "hard_negative_ctxs", "=", "(", "\n", "json_sample", "[", "\"hard_negative_ctxs\"", "]", "\n", "if", "\"hard_negative_ctxs\"", "in", "json_sample", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "for", "ctx", "in", "positive_ctxs", "+", "negative_ctxs", "+", "hard_negative_ctxs", ":", "\n", "            ", "if", "\"title\"", "not", "in", "ctx", ":", "\n", "                ", "ctx", "[", "\"title\"", "]", "=", "None", "\n", "\n", "", "", "def", "create_passage", "(", "ctx", ":", "dict", ")", ":", "\n", "            ", "return", "BiEncoderPassage", "(", "\n", "normalize_passage", "(", "ctx", "[", "\"text\"", "]", ")", "if", "self", ".", "normalize", "else", "ctx", "[", "\"text\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data": [[96, 108], ["okvqa_data.OKVQABufferLoader._load_npy_files"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader._load_npy_files"], ["ctx", "[", "\"title\"", "]", ",", "\n", ")", "\n", "\n", "", "r", ".", "positive_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "positive_ctxs", "]", "\n", "r", ".", "negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "negative_ctxs", "]", "\n", "r", ".", "hard_negative_passages", "=", "[", "create_passage", "(", "ctx", ")", "for", "ctx", "in", "hard_negative_ctxs", "]", "\n", "r", ".", "img_id", "=", "json_sample", "[", "'img_id'", "]", "\n", "return", "r", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n", "", "def", "get_qas", "(", "self", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQATorchDataset.__init__": [[118, 151], ["torch.utils.data.Dataset.__init__", "print", "print", "img_data.extend", "img_data.extend", "okvqa_buffer_loader.load_data", "okvqa_buffer_loader.load_data", "okvqa_data.OKVQATorchDataset.data.append", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQABufferLoader.load_data"], ["\n", "", "", "class", "RetrieverData", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "file", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        :param file: - real file name or the resource name as they are defined in download_data.py\n        \"\"\"", "\n", "self", ".", "file", "=", "file", "\n", "self", ".", "data_files", "=", "[", "]", "\n", "\n", "", "def", "load_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "data_files", "=", "[", "self", ".", "file", "]", "\n", "assert", "(", "\n", "len", "(", "self", ".", "data_files", ")", "==", "1", "\n", ")", ",", "\"RetrieverData source currently works with single files only. Files specified: {}\"", ".", "format", "(", "\n", "self", ".", "data_files", "\n", ")", "\n", "\n", "\n", "", "", "class", "CsvCtxSrc", "(", "RetrieverData", ")", ":", "\n", "    ", "def", "__init__", "(", "\n", "self", ",", "\n", "file", ":", "str", ",", "\n", "id_col", ":", "int", "=", "0", ",", "\n", "text_col", ":", "int", "=", "1", ",", "\n", "title_col", ":", "int", "=", "2", ",", "\n", "id_prefix", ":", "str", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "file", ")", "\n", "self", ".", "text_col", "=", "text_col", "\n", "self", ".", "title_col", "=", "title_col", "\n", "self", ".", "id_col", "=", "id_col", "\n", "self", ".", "id_prefix", "=", "id_prefix", "\n", "self", ".", "normalize", "=", "normalize", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQATorchDataset.__len__": [[152, 154], ["len"], "methods", ["None"], ["\n", "", "def", "load_data_to", "(", "self", ",", "ctxs", ":", "Dict", "[", "object", ",", "BiEncoderPassage", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_data", "(", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQATorchDataset.__getitem__": [[155, 213], ["img_info[].copy", "img_info[].copy", "boxes.copy.copy.copy", "numpy.testing.assert_array_less", "numpy.testing.assert_array_less", "len", "len", "torch.zeros"], "methods", ["None"], ["if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "file", ")", ":", "\n", "            ", "self", ".", "file", "=", "os", ".", "path", ".", "join", "(", "PROJ_PATH", ",", "self", ".", "file", ")", "\n", "", "with", "open", "(", "self", ".", "file", ")", "as", "ifile", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "ifile", ",", "delimiter", "=", "\",\"", ")", "\n", "for", "row", "in", "reader", ":", "\n", "                ", "if", "row", "[", "self", ".", "id_col", "]", "==", "\"kid\"", ":", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "id_prefix", ":", "\n", "                    ", "sample_id", "=", "self", ".", "id_prefix", "+", "str", "(", "row", "[", "self", ".", "id_col", "]", ")", "\n", "", "else", ":", "\n", "                    ", "sample_id", "=", "row", "[", "self", ".", "id_col", "]", "\n", "", "passage", "=", "row", "[", "self", ".", "text_col", "]", "\n", "if", "self", ".", "normalize", ":", "\n", "                    ", "passage", "=", "normalize_passage", "(", "passage", ")", "\n", "", "ctxs", "[", "sample_id", "]", "=", "BiEncoderPassage", "(", "passage", ",", "''", ")", "\n", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQAEvaluator.__init__": [[216, 218], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQAEvaluator.evaluate": [[219, 261], ["quesid2ans.items", "type", "int", "anss.sort", "type", "type", "list", "range", "max", "list", "len", "len", "len", "quesid2ans.values", "len", "set", "quesid2ans.values", "len", "score.items", "len", "freq.items", "len", "top5.items"], "methods", ["None"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa_data.OKVQAEvaluator.dump_result": [[262, 283], ["open", "quesid2ans.items", "json.dump", "result.append", "ques_id.item", "type"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], []], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.__init__": [[32, 75], ["okvqa.get_tuple", "tasks.okvqa_model.OKVQAModel", "okvqa.OKVQA.model.cuda", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "os.makedirs", "okvqa.get_tuple", "okvqa.OKVQA.model.lxrt_encoder.load", "pretrain.qa_answer_table.load_lxmert_qa", "okvqa.OKVQA.model.lxrt_encoder.multi_gpu", "len", "int", "print", "BertAdam", "param.args.optimizer", "list", "list", "okvqa.OKVQA.model.parameters", "okvqa.OKVQA.model.parameters"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.get_tuple", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.get_tuple", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.load_lxmert_qa", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.multi_gpu"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "train_tuple", "=", "get_tuple", "(", "\n", "args", ".", "train", ",", "bs", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ",", "drop_last", "=", "True", "\n", ")", "\n", "if", "args", ".", "valid", "!=", "\"\"", ":", "\n", "            ", "valid_bsize", "=", "2048", "if", "args", ".", "multiGPU", "else", "16", "\n", "self", ".", "valid_tuple", "=", "get_tuple", "(", "\n", "args", ".", "valid", ",", "bs", "=", "valid_bsize", ",", "shuffle", "=", "False", ",", "drop_last", "=", "False", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "valid_tuple", "=", "None", "\n", "\n", "", "self", ".", "model", "=", "OKVQAModel", "(", "self", ".", "train_tuple", ".", "dataset", ".", "num_answers", ")", "\n", "\n", "# Load pre-trained weights", "\n", "if", "args", ".", "load_lxmert", "is", "not", "None", ":", "\n", "            ", "self", ".", "model", ".", "lxrt_encoder", ".", "load", "(", "args", ".", "load_lxmert", ")", "\n", "", "if", "args", ".", "load_lxmert_qa", "is", "not", "None", ":", "\n", "            ", "load_lxmert_qa", "(", "args", ".", "load_lxmert_qa", ",", "self", ".", "model", ",", "\n", "label2ans", "=", "self", ".", "train_tuple", ".", "dataset", ".", "label2ans", ")", "\n", "\n", "# GPU options", "\n", "", "self", ".", "model", "=", "self", ".", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "multiGPU", ":", "\n", "            ", "self", ".", "model", ".", "lxrt_encoder", ".", "multi_gpu", "(", ")", "\n", "\n", "# Losses and optimizer", "\n", "", "self", ".", "bce_loss", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "self", ".", "mce_loss", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "if", "'bert'", "in", "args", ".", "optim", ":", "\n", "            ", "batch_per_epoch", "=", "len", "(", "self", ".", "train_tuple", ".", "loader", ")", "\n", "t_total", "=", "int", "(", "batch_per_epoch", "*", "args", ".", "epochs", ")", "\n", "print", "(", "\"Total Iters: %d\"", "%", "t_total", ")", "\n", "from", "lxrt", ".", "optimization", "import", "BertAdam", "\n", "self", ".", "optim", "=", "BertAdam", "(", "list", "(", "self", ".", "model", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "warmup", "=", "0.1", ",", "\n", "t_total", "=", "t_total", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "optim", "=", "args", ".", "optimizer", "(", "list", "(", "self", ".", "model", ".", "parameters", "(", ")", ")", ",", "args", ".", "lr", ")", "\n", "\n", "", "self", ".", "output", "=", "args", ".", "output", "\n", "os", ".", "makedirs", "(", "self", ".", "output", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.train": [[76, 134], ["range", "okvqa.OKVQA.save", "iter_wrapper", "evaluator.dump_result", "print", "tqdm.tqdm.tqdm", "enumerate", "okvqa.OKVQA.model.train", "okvqa.OKVQA.optim.zero_grad", "okvqa.OKVQA.model", "okvqa.OKVQA.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "okvqa.OKVQA.optim.step", "okvqa.OKVQA.max", "zip", "os.path.join", "okvqa.OKVQA.evaluate", "open", "f.write", "f.flush", "feats.cuda", "boxes.cuda", "target.cuda", "okvqa.OKVQA.dim", "target.dim", "target.max", "okvqa.OKVQA.bce_loss", "okvqa.OKVQA.model.parameters", "label.cpu().numpy", "okvqa.OKVQA.save", "len", "okvqa.OKVQA.mce_loss", "okvqa.OKVQA.size", "okvqa.OKVQA.size", "label.cpu", "evaluator.evaluate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTEvaluator.dump_result", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.step", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.dim", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.dim", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate"], ["", "def", "train", "(", "self", ",", "train_tuple", ",", "eval_tuple", ")", ":", "\n", "        ", "dset", ",", "loader", ",", "evaluator", "=", "train_tuple", "\n", "iter_wrapper", "=", "(", "lambda", "x", ":", "tqdm", "(", "x", ",", "total", "=", "len", "(", "loader", ")", ")", ")", "if", "args", ".", "tqdm", "else", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "best_valid", "=", "[", "[", "{", "\"score\"", ":", "0", "}", "]", "]", "\n", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "            ", "quesid2ans", "=", "{", "}", "\n", "for", "i", ",", "(", "ques_id", ",", "feats", ",", "boxes", ",", "sent", ",", "target", ")", "in", "iter_wrapper", "(", "enumerate", "(", "loader", ")", ")", ":", "\n", "\n", "                ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optim", ".", "zero_grad", "(", ")", "\n", "\n", "feats", ",", "boxes", ",", "target", "=", "feats", ".", "cuda", "(", ")", ",", "boxes", ".", "cuda", "(", ")", ",", "target", ".", "cuda", "(", ")", "\n", "logit", "=", "self", ".", "model", "(", "feats", ",", "boxes", ",", "sent", ")", "\n", "assert", "logit", ".", "dim", "(", ")", "==", "target", ".", "dim", "(", ")", "==", "2", "\n", "if", "args", ".", "mce_loss", ":", "\n", "                    ", "max_value", ",", "target", "=", "target", ".", "max", "(", "1", ")", "\n", "loss", "=", "self", ".", "mce_loss", "(", "logit", ",", "target", ")", "*", "logit", ".", "size", "(", "1", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "self", ".", "bce_loss", "(", "logit", ",", "target", ")", "\n", "loss", "=", "loss", "*", "logit", ".", "size", "(", "1", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "# print('loss: ', loss)", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "1.", ")", "# 5.)", "\n", "self", ".", "optim", ".", "step", "(", ")", "\n", "\n", "score", ",", "label", "=", "logit", ".", "max", "(", "1", ")", "\n", "for", "qid", ",", "l", "in", "zip", "(", "ques_id", ",", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "                    ", "ans", "=", "dset", ".", "label2ans", "[", "l", "]", "\n", "quesid2ans", "[", "qid", "]", "=", "ans", "\n", "\n", "", "", "log_str", "=", "\"\\nEpoch %d: Train %0.2f\\n\"", "%", "(", "epoch", ",", "evaluator", ".", "evaluate", "(", "quesid2ans", ")", "[", "0", "]", "*", "100.", ")", "\n", "evaluator", ".", "dump_result", "(", "quesid2ans", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "'train_predict.json'", ")", ")", "\n", "\n", "if", "self", ".", "valid_tuple", "is", "not", "None", ":", "# Do Validation", "\n", "                ", "valid_score", "=", "self", ".", "evaluate", "(", "eval_tuple", ")", "\n", "if", "valid_score", "[", "0", "]", "[", "0", "]", ">", "best_valid", "[", "0", "]", "[", "0", "]", ":", "\n", "                    ", "best_valid", "=", "valid_score", "\n", "self", ".", "save", "(", "\"BEST\"", ")", "\n", "\n", "\n", "\n", "", "log_str", "=", "\"Epoch %d: Valid %0.2f\\n\"", "%", "(", "epoch", ",", "valid_score", "[", "0", "]", "*", "100.", ")", "+", "\"Epoch %d: Best %0.2f\\n\"", "%", "(", "epoch", ",", "best_valid", "[", "0", "]", "*", "100.", ")", "\n", "\n", "'''\n                log_str += \"Epoch %d: Valid %0.2f Freq %0.2f Top5 %0.2f\\n\" % (epoch, valid_score[0][50] * 100., valid_score[1][50] * 100., valid_score[2][50] * 100.) + \\\n                           \"Epoch %d: Best %0.2f Freq %0.2f Top5 %0.2f\\n\" % (epoch, best_valid[0][50] * 100., best_valid[1][50] * 100., best_valid[2][50] * 100.)\n                '''", "\n", "\n", "", "print", "(", "log_str", ",", "end", "=", "''", ")", "\n", "\n", "with", "open", "(", "self", ".", "output", "+", "\"/log.log\"", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "log_str", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n", "", "", "self", ".", "save", "(", "\"LAST\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.predict": [[135, 164], ["okvqa.OKVQA.model.eval", "enumerate", "evaluator.dump_result", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "okvqa.OKVQA.model", "okvqa.OKVQA.max", "zip", "feats.cuda", "boxes.cuda", "label.cpu().numpy", "score.cpu().numpy", "int", "[].append", "[].append", "[].append", "label.cpu", "score.cpu", "float", "float"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTEvaluator.dump_result"], ["", "def", "predict", "(", "self", ",", "eval_tuple", ":", "DataTuple", ",", "dump", "=", "None", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "dset", ",", "loader", ",", "evaluator", "=", "eval_tuple", "\n", "quesid2ans", "=", "{", "}", "\n", "for", "i", ",", "datum_tuple", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "ques_id", ",", "feats", ",", "boxes", ",", "sent", "=", "datum_tuple", "[", ":", "4", "]", "# avoid handling target", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "feats", ",", "boxes", "=", "feats", ".", "cuda", "(", ")", ",", "boxes", ".", "cuda", "(", ")", "\n", "logit", "=", "self", ".", "model", "(", "feats", ",", "boxes", ",", "sent", ")", "\n", "score", ",", "label", "=", "logit", ".", "max", "(", "dim", "=", "1", ")", "\n", "# score, label = logit.topk(10, dim=1)", "\n", "for", "qid", ",", "l", ",", "sc", ",", "st", "in", "zip", "(", "ques_id", ",", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "score", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "sent", ")", ":", "\n", "                    ", "ans", "=", "dset", ".", "label2ans", "[", "l", "]", "\n", "# ans = [dset.label2ans[ll] for ll in l]", "\n", "qid", "=", "int", "(", "qid", ")", "\n", "'''\n                    # quesid2ans[qid] = ans\n                    quesid2ans[qid] = {'label': ans, 'score': sc}\n                    '''", "\n", "if", "qid", "not", "in", "quesid2ans", ":", "\n", "                        ", "quesid2ans", "[", "qid", "]", "=", "{", "'label'", ":", "[", "ans", "]", ",", "'score'", ":", "[", "float", "(", "sc", ")", "]", ",", "'sent'", ":", "[", "st", "]", "}", "\n", "", "else", ":", "\n", "                        ", "quesid2ans", "[", "qid", "]", "[", "'label'", "]", ".", "append", "(", "ans", ")", "\n", "quesid2ans", "[", "qid", "]", "[", "'score'", "]", ".", "append", "(", "float", "(", "sc", ")", ")", "\n", "quesid2ans", "[", "qid", "]", "[", "'sent'", "]", ".", "append", "(", "st", ")", "\n", "\n", "", "", "", "", "if", "dump", "is", "not", "None", ":", "\n", "            ", "evaluator", ".", "dump_result", "(", "quesid2ans", ",", "dump", ")", "\n", "", "return", "quesid2ans", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.evaluate": [[165, 169], ["okvqa.OKVQA.predict", "evaluator.evaluate"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.predict", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate"], ["", "def", "evaluate", "(", "self", ",", "eval_tuple", ":", "DataTuple", ",", "dump", "=", "None", ")", ":", "\n", "        ", "dset", ",", "loader", ",", "evaluator", "=", "eval_tuple", "\n", "quesid2ans", "=", "self", ".", "predict", "(", "eval_tuple", ",", "dump", ")", "\n", "return", "evaluator", ".", "evaluate", "(", "quesid2ans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.oracle_score": [[170, 180], ["enumerate", "evaluator.evaluate", "target.max", "zip", "label.cpu().numpy", "label.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate"], ["", "@", "staticmethod", "\n", "def", "oracle_score", "(", "data_tuple", ")", ":", "\n", "        ", "dset", ",", "loader", ",", "evaluator", "=", "data_tuple", "\n", "quesid2ans", "=", "{", "}", "\n", "for", "i", ",", "(", "ques_id", ",", "feats", ",", "boxes", ",", "sent", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "_", ",", "label", "=", "target", ".", "max", "(", "1", ")", "\n", "for", "qid", ",", "l", "in", "zip", "(", "ques_id", ",", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "                ", "ans", "=", "dset", ".", "label2ans", "[", "l", "]", "\n", "quesid2ans", "[", "qid", "]", "=", "ans", "\n", "", "", "return", "evaluator", ".", "evaluate", "(", "quesid2ans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.save": [[181, 184], ["torch.save", "torch.save", "torch.save", "torch.save", "okvqa.OKVQA.model.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save"], ["", "def", "save", "(", "self", ",", "name", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "output", ",", "\"%s.pth\"", "%", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.OKVQA.load": [[185, 192], ["print", "torch.load", "torch.load", "torch.load", "torch.load", "list", "okvqa.OKVQA.model.load_state_dict", "torch.load.keys", "torch.load.keys", "torch.load.pop", "torch.load.pop", "key.replace"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "\"Load model from %s\"", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "\"%s.pth\"", "%", "path", ")", "\n", "for", "key", "in", "list", "(", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "if", "'.module'", "in", "key", ":", "\n", "                ", "state_dict", "[", "key", ".", "replace", "(", "'.module'", ",", "''", ")", "]", "=", "state_dict", ".", "pop", "(", "key", ")", "\n", "", "", "self", ".", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.tasks.okvqa.get_tuple": [[18, 29], ["tasks.okvqa_data.OKVQADataset", "tasks.okvqa_data.OKVQATorchDataset", "tasks.okvqa_data.OKVQAEvaluator", "torch.utils.data.dataloader.DataLoader", "DataTuple", "param.args.train", "param.args.valid"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train"], ["def", "get_tuple", "(", "splits", ":", "str", ",", "bs", ":", "int", ",", "shuffle", "=", "False", ",", "drop_last", "=", "False", ")", "->", "DataTuple", ":", "\n", "    ", "dset", "=", "OKVQADataset", "(", "splits", ")", "\n", "tset", "=", "OKVQATorchDataset", "(", "dset", ")", "\n", "evaluator", "=", "OKVQAEvaluator", "(", "dset", ")", "\n", "data_loader", "=", "DataLoader", "(", "\n", "tset", ",", "batch_size", "=", "bs", ",", "\n", "shuffle", "=", "shuffle", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "drop_last", "=", "drop_last", ",", "pin_memory", "=", "True", "\n", ")", "\n", "\n", "return", "DataTuple", "(", "dataset", "=", "dset", ",", "loader", "=", "data_loader", ",", "evaluator", "=", "evaluator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.GeLU.__init__": [[127, 129], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.GeLU.forward": [[130, 132], ["modeling.gelu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.gelu"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "gelu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.VisualConfig.__init__": [[143, 162], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "l_layers", "=", "12", ",", "\n", "x_layers", "=", "5", ",", "\n", "r_layers", "=", "0", ")", ":", "\n", "        ", "self", ".", "l_layers", "=", "l_layers", "\n", "self", ".", "x_layers", "=", "x_layers", "\n", "self", ".", "r_layers", "=", "r_layers", "\n", "\n", "self", ".", "visual_feat_dim", "=", "2048", "\n", "self", ".", "visual_pos_dim", "=", "4", "\n", "\n", "self", ".", "obj_id_num", "=", "1600", "\n", "self", ".", "attr_id_num", "=", "400", "\n", "\n", "self", ".", "visual_losses", "=", "self", ".", "VISUAL_LOSSES", "\n", "self", ".", "visual_loss_config", "=", "{", "\n", "'obj'", ":", "(", "self", ".", "obj_id_num", ",", "'ce'", ",", "(", "-", "1", ",", ")", ",", "1", "/", "0.15", ")", ",", "\n", "'attr'", ":", "(", "self", ".", "attr_id_num", ",", "'ce'", ",", "(", "-", "1", ",", ")", ",", "1", "/", "0.15", ")", ",", "\n", "'feat'", ":", "(", "2048", ",", "'l2'", ",", "(", "-", "1", ",", "2048", ")", ",", "1", "/", "0.15", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.VisualConfig.set_visual_dims": [[164, 167], ["None"], "methods", ["None"], ["", "def", "set_visual_dims", "(", "self", ",", "feat_dim", ",", "pos_dim", ")", ":", "\n", "        ", "self", ".", "visual_feat_dim", "=", "feat_dim", "\n", "self", ".", "visual_pos_dim", "=", "pos_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.__init__": [[175, 231], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.from_dict": [[233, 240], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.from_json_file": [[241, 247], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.__repr__": [[248, 250], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.to_dict": [[251, 255], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.to_json_string": [[256, 259], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertEmbeddings.__init__": [[267, 277], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertEmbeddings.forward": [[278, 293], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.__init__": [[296, 314], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "# visual_dim = 2048", "\n", "if", "ctx_dim", "is", "None", ":", "\n", "            ", "ctx_dim", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.transpose_for_scores": [[315, 319], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.forward": [[320, 348], ["modeling.BertAttention.query", "modeling.BertAttention.key", "modeling.BertAttention.value", "modeling.BertAttention.transpose_for_scores", "modeling.BertAttention.transpose_for_scores", "modeling.BertAttention.transpose_for_scores", "torch.matmul", "modeling.BertAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.transpose_for_scores", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.transpose_for_scores", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "context", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "context", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "context", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttOutput.__init__": [[351, 356], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertAttOutput.forward": [[357, 362], ["modeling.BertAttOutput.dense", "modeling.BertAttOutput.dropout", "modeling.BertAttOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertCrossattLayer.__init__": [[365, 369], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertAttOutput"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "att", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertAttOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertCrossattLayer.forward": [[370, 374], ["modeling.BertCrossattLayer.att", "modeling.BertCrossattLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "self", ".", "att", "(", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertSelfattLayer.__init__": [[377, 381], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertAttOutput"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfattLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertAttOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertSelfattLayer.forward": [[382, 387], ["modeling.BertSelfattLayer.self", "modeling.BertSelfattLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "# Self attention attends to itself, thus keys and querys are the same (input_tensor).", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertIntermediate.__init__": [[390, 397], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertIntermediate.forward": [[398, 402], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertOutput.__init__": [[405, 410], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertOutput.forward": [[411, 416], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertLayer.__init__": [[419, 424], ["torch.nn.Module.__init__", "modeling.BertSelfattLayer", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertSelfattLayer", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertLayer.forward": [[425, 430], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.__init__": [[440, 454], ["torch.nn.Module.__init__", "modeling.BertCrossattLayer", "modeling.BertSelfattLayer", "modeling.BertSelfattLayer", "modeling.BertIntermediate", "modeling.BertOutput", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# The cross-attention Layer", "\n", "self", ".", "visual_attention", "=", "BertCrossattLayer", "(", "config", ")", "\n", "\n", "# Self-attention Layers", "\n", "self", ".", "lang_self_att", "=", "BertSelfattLayer", "(", "config", ")", "\n", "self", ".", "visn_self_att", "=", "BertSelfattLayer", "(", "config", ")", "\n", "\n", "# Intermediate and Output Layers (FFNs)", "\n", "self", ".", "lang_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "lang_output", "=", "BertOutput", "(", "config", ")", "\n", "self", ".", "visn_inter", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "visn_output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.cross_att": [[455, 460], ["modeling.LXRTXLayer.visual_attention", "modeling.LXRTXLayer.visual_attention"], "methods", ["None"], ["", "def", "cross_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Cross Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "visual_attention", "(", "lang_input", ",", "visn_input", ",", "ctx_att_mask", "=", "visn_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visual_attention", "(", "visn_input", ",", "lang_input", ",", "ctx_att_mask", "=", "lang_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.self_att": [[461, 466], ["modeling.LXRTXLayer.lang_self_att", "modeling.LXRTXLayer.visn_self_att"], "methods", ["None"], ["", "def", "self_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Self Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "lang_self_att", "(", "lang_input", ",", "lang_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visn_self_att", "(", "visn_input", ",", "visn_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.output_fc": [[467, 476], ["modeling.LXRTXLayer.lang_inter", "modeling.LXRTXLayer.visn_inter", "modeling.LXRTXLayer.lang_output", "modeling.LXRTXLayer.visn_output"], "methods", ["None"], ["", "def", "output_fc", "(", "self", ",", "lang_input", ",", "visn_input", ")", ":", "\n", "# FC layers", "\n", "        ", "lang_inter_output", "=", "self", ".", "lang_inter", "(", "lang_input", ")", "\n", "visn_inter_output", "=", "self", ".", "visn_inter", "(", "visn_input", ")", "\n", "\n", "# Layer output", "\n", "lang_output", "=", "self", ".", "lang_output", "(", "lang_inter_output", ",", "lang_input", ")", "\n", "visn_output", "=", "self", ".", "visn_output", "(", "visn_inter_output", ",", "visn_input", ")", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.forward": [[477, 489], ["modeling.LXRTXLayer.cross_att", "modeling.LXRTXLayer.self_att", "modeling.LXRTXLayer.output_fc"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.cross_att", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.self_att", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTXLayer.output_fc"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", ":", "\n", "        ", "lang_att_output", "=", "lang_feats", "\n", "visn_att_output", "=", "visn_feats", "\n", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "cross_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "self_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_output", ",", "visn_output", "=", "self", ".", "output_fc", "(", "lang_att_output", ",", "visn_att_output", ")", "\n", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.VisualFeatEncoder.__init__": [[492, 506], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "VISUAL_CONFIG", ".", "visual_feat_dim", "\n", "pos_dim", "=", "VISUAL_CONFIG", ".", "visual_pos_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "# Box position encoding", "\n", "self", ".", "box_fc", "=", "nn", ".", "Linear", "(", "pos_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "box_layer_norm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.VisualFeatEncoder.forward": [[507, 518], ["modeling.VisualFeatEncoder.visn_fc", "modeling.VisualFeatEncoder.visn_layer_norm", "modeling.VisualFeatEncoder.box_fc", "modeling.VisualFeatEncoder.box_layer_norm", "modeling.VisualFeatEncoder.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "visn_input", ")", ":", "\n", "        ", "feats", ",", "boxes", "=", "visn_input", "\n", "\n", "x", "=", "self", ".", "visn_fc", "(", "feats", ")", "\n", "x", "=", "self", ".", "visn_layer_norm", "(", "x", ")", "\n", "y", "=", "self", ".", "box_fc", "(", "boxes", ")", "\n", "y", "=", "self", ".", "box_layer_norm", "(", "y", ")", "\n", "output", "=", "(", "x", "+", "y", ")", "/", "2", "\n", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTEncoder.__init__": [[521, 544], ["torch.nn.Module.__init__", "modeling.VisualFeatEncoder", "print", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "modeling.BertLayer", "modeling.LXRTXLayer", "modeling.BertLayer", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Obj-level image embedding layer", "\n", "self", ".", "visn_fc", "=", "VisualFeatEncoder", "(", "config", ")", "\n", "\n", "# Number of layers", "\n", "self", ".", "num_l_layers", "=", "VISUAL_CONFIG", ".", "l_layers", "\n", "self", ".", "num_x_layers", "=", "VISUAL_CONFIG", ".", "x_layers", "\n", "self", ".", "num_r_layers", "=", "VISUAL_CONFIG", ".", "r_layers", "\n", "print", "(", "\"LXRT encoder with %d l_layers, %d x_layers, and %d r_layers.\"", "%", "\n", "(", "self", ".", "num_l_layers", ",", "self", ".", "num_x_layers", ",", "self", ".", "num_r_layers", ")", ")", "\n", "\n", "# Layers", "\n", "# Using self.layer instead of self.l_layer to support loading BERT weights.", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "self", ".", "num_l_layers", ")", "]", "\n", ")", "\n", "self", ".", "x_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "LXRTXLayer", "(", "config", ")", "for", "_", "in", "range", "(", "self", ".", "num_x_layers", ")", "]", "\n", ")", "\n", "self", ".", "r_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "self", ".", "num_r_layers", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTEncoder.forward": [[546, 567], ["modeling.LXRTEncoder.visn_fc", "layer_module", "layer_module", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", "=", "None", ")", ":", "\n", "# Run visual embedding layer", "\n", "# Note: Word embedding layer was executed outside this module.", "\n", "#       Keep this design to allow loading BERT weights.", "\n", "        ", "visn_feats", "=", "self", ".", "visn_fc", "(", "visn_feats", ")", "\n", "\n", "# Run language layers", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "lang_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ")", "\n", "\n", "# Run relational layers", "\n", "", "for", "layer_module", "in", "self", ".", "r_layers", ":", "\n", "            ", "visn_feats", "=", "layer_module", "(", "visn_feats", ",", "visn_attention_mask", ")", "\n", "\n", "# Run cross-modality layers", "\n", "", "for", "layer_module", "in", "self", ".", "x_layers", ":", "\n", "            ", "lang_feats", ",", "visn_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", "\n", "\n", "", "return", "lang_feats", ",", "visn_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPooler.__init__": [[570, 574], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPooler.forward": [[575, 582], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPredictionHeadTransform.__init__": [[585, 593], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPredictionHeadTransform.forward": [[594, 599], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertLMPredictionHead.__init__": [[602, 613], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertLMPredictionHead.forward": [[614, 618], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertVisualAnswerHead.__init__": [[621, 629], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Linear", "modeling.GeLU", "BertLayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_answers", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "hid_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "logit_fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hid_dim", ",", "hid_dim", "*", "2", ")", ",", "\n", "GeLU", "(", ")", ",", "\n", "BertLayerNorm", "(", "hid_dim", "*", "2", ",", "eps", "=", "1e-12", ")", ",", "\n", "nn", ".", "Linear", "(", "hid_dim", "*", "2", ",", "num_answers", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertVisualAnswerHead.forward": [[631, 633], ["modeling.BertVisualAnswerHead.logit_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "return", "self", ".", "logit_fc", "(", "hidden_states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertVisualObjHead.__init__": [[636, 651], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "visual_losses.split.split.split", "torch.nn.ModuleDict", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "visual_losses", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# Decide the use of visual losses", "\n", "visual_losses", "=", "visual_losses", ".", "split", "(", "\",\"", ")", "\n", "for", "loss", "in", "visual_losses", ":", "\n", "            ", "assert", "loss", "in", "VISUAL_CONFIG", ".", "VISUAL_LOSSES", "\n", "", "self", ".", "visual_losses", "=", "visual_losses", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder_dict", "=", "nn", ".", "ModuleDict", "(", "{", "\n", "key", ":", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "VISUAL_CONFIG", ".", "visual_loss_config", "[", "key", "]", "[", "0", "]", ")", "\n", "for", "key", "in", "self", ".", "visual_losses", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertVisualObjHead.forward": [[653, 659], ["modeling.BertVisualObjHead.transform"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "output", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "visual_losses", ":", "\n", "            ", "output", "[", "key", "]", "=", "self", ".", "decoder_dict", "[", "key", "]", "(", "hidden_states", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPreTrainingHeads.__init__": [[662, 666], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPreTrainingHeads.forward": [[667, 671], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPreTrainedModel.__init__": [[677, 687], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPreTrainedModel.init_bert_weights": [[688, 700], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertPreTrainedModel.from_pretrained": [[701, 833], ["os.path.join", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "==", "'bert-base-uncased'", ":", "\n", "                ", "try", ":", "\n", "                    ", "print", "(", "\"The BERT-weight-downloading query to AWS was time-out;\"", "\n", "\"trying to download from UNC servers\"", ")", "\n", "archive_file", "=", "\"https://nlp.cs.unc.edu/data/bert/bert-base-uncased.tar.gz\"", "\n", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "                    ", "print", "(", "\"The weight-downloading still crashed with link: %s, \"", "\n", "\"please check your network connection\"", "%", "archive_file", ")", "\n", "return", "None", "\n", "", "", "else", ":", "\n", "                ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", "or", "from_tf", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "TF_WEIGHTS_NAME", ")", "\n", "return", "load_tf_weights_in_bert", "(", "model", ",", "weights_path", ")", "\n", "# Load from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'bert'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'bert.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'bert.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "# if len(missing_keys) > 0:", "\n", "#     logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(", "\n", "#         model.__class__.__name__, missing_keys))", "\n", "# if len(unexpected_keys) > 0:", "\n", "#     logger.info(\"Weights from pretrained model not used in {}: {}\".format(", "\n", "#         model.__class__.__name__, unexpected_keys))", "\n", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTModel.__init__": [[838, 844], ["modeling.BertPreTrainedModel.__init__", "modeling.BertEmbeddings", "modeling.LXRTEncoder", "modeling.BertPooler", "modeling.LXRTModel.apply"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "LXRTEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTModel.forward": [[845, 887], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "modeling.LXRTModel.embeddings", "modeling.LXRTModel.encoder", "modeling.LXRTModel.pooler", "torch.ones_like", "torch.zeros_like", "visual_attention_mask.unsqueeze().unsqueeze", "extended_visual_attention_mask.to.to.to", "torch.ones_like.unsqueeze", "next", "visual_attention_mask.unsqueeze", "modeling.LXRTModel.parameters", "next", "modeling.LXRTModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "visual_feats", "=", "None", ",", "visual_attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Process the visual attention mask", "\n", "if", "visual_attention_mask", "is", "not", "None", ":", "\n", "            ", "extended_visual_attention_mask", "=", "visual_attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_visual_attention_mask", "=", "extended_visual_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_visual_attention_mask", "=", "(", "1.0", "-", "extended_visual_attention_mask", ")", "*", "-", "10000.0", "\n", "", "else", ":", "\n", "            ", "extended_visual_attention_mask", "=", "None", "\n", "\n", "# Positional Word Embeddings", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "\n", "# Run LXRT backbone", "\n", "lang_feats", ",", "visn_feats", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "visn_feats", "=", "visual_feats", ",", "\n", "visn_attention_mask", "=", "extended_visual_attention_mask", ")", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "lang_feats", ")", "\n", "\n", "return", "(", "lang_feats", ",", "visn_feats", ")", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTPretraining.__init__": [[890, 921], ["modeling.BertPreTrainedModel.__init__", "modeling.LXRTModel", "modeling.BertPreTrainingHeads", "modeling.LXRTPretraining.apply", "modeling.BertVisualObjHead", "modeling.BertVisualAnswerHead"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply"], ["    ", "def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "task_mask_lm", "=", "True", ",", "\n", "task_matched", "=", "True", ",", "\n", "task_obj_predict", "=", "True", ",", "\n", "visual_losses", "=", "''", ",", "\n", "task_qa", "=", "True", ",", "\n", "num_answers", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "# Configuration", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "num_answers", "=", "num_answers", "\n", "\n", "# Use of pre-training tasks", "\n", "self", ".", "task_mask_lm", "=", "task_mask_lm", "\n", "self", ".", "task_obj_predict", "=", "task_obj_predict", "\n", "self", ".", "task_matched", "=", "task_matched", "\n", "self", ".", "task_qa", "=", "task_qa", "\n", "\n", "# LXRT backbone", "\n", "self", ".", "bert", "=", "LXRTModel", "(", "config", ")", "\n", "\n", "# Pre-training heads", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "if", "self", ".", "task_obj_predict", ":", "\n", "            ", "self", ".", "obj_predict_head", "=", "BertVisualObjHead", "(", "config", ",", "visual_losses", ")", "\n", "", "if", "self", ".", "task_qa", ":", "\n", "            ", "self", ".", "answer_head", "=", "BertVisualAnswerHead", "(", "config", ",", "self", ".", "num_answers", ")", "\n", "\n", "# Weight initialization", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTPretraining.forward": [[922, 990], ["modeling.LXRTPretraining.bert", "modeling.LXRTPretraining.cls", "torch.nn.CrossEntropyLoss", "modeling.LXRTPretraining.answer_head", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.LXRTPretraining.obj_predict_head", "torch.nn.CrossEntropyLoss.", "torch.stack().unsqueeze", "modeling.LXRTPretraining.detach", "lang_prediction_scores.view", "masked_lm_labels.view", "torch.nn.CrossEntropyLoss.detach", "cross_relationship_score.view", "matched_label.view", "torch.nn.CrossEntropyLoss.detach", "torch.nn.SmoothL1Loss", "torch.nn.CrossEntropyLoss", "visn_loss_fct", "modeling.LXRTPretraining.view", "ans.view", "torch.nn.CrossEntropyLoss.detach", "visn_prediction_scores.view", "label.view", "visn_loss.mean.mean.dim", "visn_loss.mean.mean.mean", "visn_loss.mean.mean.detach", "torch.stack", "mask_conf.view"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.dim"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "visual_feats", "=", "None", ",", "pos", "=", "None", ",", "obj_labels", "=", "None", ",", "matched_label", "=", "None", ",", "ans", "=", "None", ")", ":", "\n", "        ", "(", "lang_output", ",", "visn_output", ")", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "visual_feats", "=", "(", "visual_feats", ",", "pos", ")", ",", "\n", ")", "\n", "\n", "lang_prediction_scores", ",", "cross_relationship_score", "=", "self", ".", "cls", "(", "lang_output", ",", "pooled_output", ")", "\n", "if", "self", ".", "task_qa", ":", "\n", "            ", "answer_score", "=", "self", ".", "answer_head", "(", "pooled_output", ")", "\n", "", "else", ":", "\n", "# This answer_score would not be used anywhere,", "\n", "# just to keep a constant return function signature.", "\n", "            ", "answer_score", "=", "pooled_output", "[", "0", "]", "[", "0", "]", "\n", "\n", "", "total_loss", "=", "0.", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "losses", "=", "(", ")", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "self", ".", "task_mask_lm", ":", "\n", "            ", "masked_lm_loss", "=", "loss_fct", "(", "\n", "lang_prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "\n", "masked_lm_labels", ".", "view", "(", "-", "1", ")", "\n", ")", "\n", "total_loss", "+=", "masked_lm_loss", "\n", "losses", "+=", "(", "masked_lm_loss", ".", "detach", "(", ")", ",", ")", "\n", "", "if", "matched_label", "is", "not", "None", "and", "self", ".", "task_matched", ":", "\n", "            ", "matched_loss", "=", "loss_fct", "(", "\n", "cross_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "\n", "matched_label", ".", "view", "(", "-", "1", ")", "\n", ")", "\n", "total_loss", "+=", "matched_loss", "\n", "losses", "+=", "(", "matched_loss", ".", "detach", "(", ")", ",", ")", "\n", "", "if", "obj_labels", "is", "not", "None", "and", "self", ".", "task_obj_predict", ":", "\n", "            ", "loss_fcts", "=", "{", "\n", "'l2'", ":", "SmoothL1Loss", "(", "reduction", "=", "'none'", ")", ",", "\n", "'ce'", ":", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ",", "reduction", "=", "'none'", ")", "\n", "}", "\n", "total_visn_loss", "=", "0.", "\n", "visn_prediction_scores_dict", "=", "self", ".", "obj_predict_head", "(", "visn_output", ")", "\n", "for", "key", "in", "VISUAL_CONFIG", ".", "visual_losses", ":", "\n", "                ", "label", ",", "mask_conf", "=", "obj_labels", "[", "key", "]", "\n", "output_dim", ",", "loss_fct_name", ",", "label_shape", ",", "weight", "=", "VISUAL_CONFIG", ".", "visual_loss_config", "[", "key", "]", "\n", "visn_loss_fct", "=", "loss_fcts", "[", "loss_fct_name", "]", "\n", "visn_prediction_scores", "=", "visn_prediction_scores_dict", "[", "key", "]", "\n", "visn_loss", "=", "visn_loss_fct", "(", "\n", "visn_prediction_scores", ".", "view", "(", "-", "1", ",", "output_dim", ")", ",", "\n", "label", ".", "view", "(", "*", "label_shape", ")", ",", "\n", ")", "\n", "if", "visn_loss", ".", "dim", "(", ")", ">", "1", ":", "# Regression Losses", "\n", "                    ", "visn_loss", "=", "visn_loss", ".", "mean", "(", "1", ")", "\n", "", "visn_loss", "=", "(", "visn_loss", "*", "mask_conf", ".", "view", "(", "-", "1", ")", ")", ".", "mean", "(", ")", "*", "weight", "\n", "total_visn_loss", "+=", "visn_loss", "\n", "losses", "+=", "(", "visn_loss", ".", "detach", "(", ")", ",", ")", "\n", "", "total_loss", "+=", "total_visn_loss", "\n", "", "if", "ans", "is", "not", "None", "and", "self", ".", "task_qa", ":", "\n", "            ", "answer_loss", "=", "loss_fct", "(", "\n", "answer_score", ".", "view", "(", "-", "1", ",", "self", ".", "num_answers", ")", ",", "\n", "ans", ".", "view", "(", "-", "1", ")", "\n", ")", "\n", "# Since this Github version pre-trains with QA loss from the beginning,", "\n", "# I exclude \"*2\" here to match the effect of QA losses.", "\n", "# Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)", "\n", "# Now     : (loss *1) for 12 epochs", "\n", "#", "\n", "# * 2       # Multiply by 2 because > half of the data will not have label", "\n", "total_loss", "+=", "answer_loss", "\n", "losses", "+=", "(", "answer_loss", ".", "detach", "(", ")", ",", ")", "\n", "", "return", "total_loss", ",", "torch", ".", "stack", "(", "losses", ")", ".", "unsqueeze", "(", "0", ")", ",", "answer_score", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTFeatureExtraction.__init__": [[996, 1006], ["modeling.BertPreTrainedModel.__init__", "modeling.LXRTModel", "modeling.LXRTFeatureExtraction.apply"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply"], ["def", "__init__", "(", "self", ",", "config", ",", "mode", "=", "'lxr'", ")", ":", "\n", "        ", "\"\"\"\n\n        :param config:\n        :param mode:  Number of visual layers\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "LXRTModel", "(", "config", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.LXRTFeatureExtraction.forward": [[1007, 1018], ["modeling.LXRTFeatureExtraction.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "visual_feats", "=", "None", ",", "\n", "visual_attention_mask", "=", "None", ")", ":", "\n", "        ", "feat_seq", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "visual_feats", "=", "visual_feats", ",", "\n", "visual_attention_mask", "=", "visual_attention_mask", ")", "\n", "if", "'x'", "==", "self", ".", "mode", ":", "\n", "            ", "return", "pooled_output", "\n", "", "elif", "'x'", "in", "self", ".", "mode", "and", "(", "'l'", "in", "self", ".", "mode", "or", "'r'", "in", "self", ".", "mode", ")", ":", "\n", "            ", "return", "feat_seq", ",", "pooled_output", "\n", "", "elif", "'l'", "in", "self", ".", "mode", "or", "'r'", "in", "self", ".", "mode", ":", "\n", "            ", "return", "feat_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.load_tf_weights_in_bert": [[51, 110], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "Importtokenization", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.gelu": [[112, 119], ["torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.modeling.swish": [[134, 136], ["torch.sigmoid"], "function", ["None"], ["", "", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.__init__": [[66, 85], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.get_lr": [[86, 100], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.step": [[101, 181], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct", "logger.warning"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "warned_for_t_total", "=", "False", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# LXRT: grad is clipped outside.", "\n", "# Add grad clipping", "\n", "# if group['max_grad_norm'] > 0:", "\n", "#     clip_grad_norm_(p, group['max_grad_norm'])", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "progress", "=", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "progress", ",", "group", "[", "'warmup'", "]", ")", "\n", "# warning for exceeding t_total (only active with warmup_linear", "\n", "if", "group", "[", "'schedule'", "]", "==", "\"warmup_linear\"", "and", "progress", ">", "1.", "and", "not", "warned_for_t_total", ":", "\n", "                        ", "logger", ".", "warning", "(", "\n", "\"Training beyond specified 't_total' steps with schedule '{}'. Learning rate set to {}. \"", "\n", "\"Please set 't_total' of {} correctly.\"", ".", "format", "(", "group", "[", "'schedule'", "]", ",", "lr_scheduled", ",", "self", ".", "__class__", ".", "__name__", ")", ")", "\n", "warned_for_t_total", "=", "True", "\n", "# end warning", "\n", "", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.warmup_cosine": [[26, 30], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.warmup_constant": [[31, 37], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps.\n        Learning rate is 1. afterwards. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.warmup_linear": [[38, 44], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Specifies a triangular learning rate schedule where peak is reached at `warmup`*`t_total`-th (as provided to BertAdam) training step.\n        After `t_total`-th training step, learning rate is zero. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "max", "(", "(", "x", "-", "1.", ")", "/", "(", "warmup", "-", "1.", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.url_to_filename": [[37, 53], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.filename_to_url": [[55, 79], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.cached_path": [[81, 109], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.split_s3_path": [[111, 122], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.s3_request": [[124, 141], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.s3_etag": [[143, 150], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.s3_get": [[152, 158], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.http_get": [[160, 170], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.get_from_cache": [[172, 230], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.url_to_filename", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.s3_etag", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.s3_get", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.http_get", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.read_set_from_file": [[232, 242], ["set", "io.open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.get_file_extension": [[244, 248], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.__init__": [[75, 104], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "tokenization.BasicTokenizer", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model's\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "do_basic_tokenize", "=", "do_basic_tokenize", "\n", "if", "do_basic_tokenize", ":", "\n", "          ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ",", "\n", "never_split", "=", "never_split", ")", "\n", "", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.tokenize": [[105, 114], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "do_basic_tokenize", ":", "\n", "          ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "              ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                  ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "", "else", ":", "\n", "          ", "split_tokens", "=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_tokens_to_ids": [[115, 127], ["ids.append", "len", "logger.warning", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_ids_to_tokens": [[128, 134], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained": [[135, 172], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer.__init__": [[177, 187], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer.tokenize": [[188, 208], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._run_strip_accents": [[209, 219], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.data.tables.normalize"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._run_split_on_punc": [[220, 241], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._tokenize_chinese_chars": [[242, 254], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._is_chinese_char": [[255, 276], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BasicTokenizer._clean_text": [[277, 289], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_whitespace", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.__init__": [[294, 298], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize": [[299, 349], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.load_vocab": [[48, 61], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.whitespace_tokenize": [[63, 70], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_whitespace": [[351, 361], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_control": [[363, 373], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization._is_punctuation": [[375, 389], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.InputFeatures.__init__": [[30, 34], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.__init__": [[81, 101], ["torch.Module.__init__", "entry.set_visual_config", "lxrt.tokenization.BertTokenizer.from_pretrained", "lxrt.modeling.LXRTFeatureExtraction.from_pretrained", "print", "entry.LXRTEncoder.model.apply"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.set_visual_config", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "max_seq_length", ",", "mode", "=", "'x'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "set_visual_config", "(", "args", ")", "\n", "\n", "# Using the bert tokenizer", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "\n", "do_lower_case", "=", "True", "\n", ")", "\n", "\n", "# Build LXRT Model", "\n", "self", ".", "model", "=", "VisualBertForLXRFeature", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "\n", "mode", "=", "mode", "\n", ")", "\n", "\n", "if", "args", ".", "from_scratch", ":", "\n", "            ", "print", "(", "\"initializing all the weights\"", ")", "\n", "self", ".", "model", ".", "apply", "(", "self", ".", "model", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.multi_gpu": [[102, 104], ["torch.DataParallel", "torch.DataParallel"], "methods", ["None"], ["", "", "def", "multi_gpu", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.dim": [[105, 108], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim", "(", "self", ")", ":", "\n", "        ", "return", "768", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.forward": [[109, 121], ["entry.convert_sents_to_features", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "entry.LXRTEncoder.model", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.convert_sents_to_features"], ["", "def", "forward", "(", "self", ",", "sents", ",", "feats", ",", "visual_attention_mask", "=", "None", ")", ":", "\n", "        ", "train_features", "=", "convert_sents_to_features", "(", "\n", "sents", ",", "self", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "output", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "\n", "visual_feats", "=", "feats", ",", "\n", "visual_attention_mask", "=", "visual_attention_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.save": [[122, 125], ["torch.save", "torch.save", "torch.save", "torch.save", "entry.LXRTEncoder.model.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save"], ["", "def", "save", "(", "self", ",", "path", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "\"%s_LXRT.pth\"", "%", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.LXRTEncoder.load": [[126, 153], ["print", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load.items", "torch.load.items", "set", "set", "print", "print", "sorted", "print", "print", "sorted", "print", "entry.LXRTEncoder.model.load_state_dict", "key.startswith", "torch.load.keys", "torch.load.keys", "entry.LXRTEncoder.model.state_dict().keys", "set.difference", "print", "set.difference", "print", "entry.LXRTEncoder.model.state_dict", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "# Load state_dict from snapshot file", "\n", "        ", "print", "(", "\"Load LXMERT pre-trained model from %s\"", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "\"%s_LXRT.pth\"", "%", "path", ")", "\n", "new_state_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", ".", "startswith", "(", "\"module.\"", ")", ":", "\n", "                ", "new_state_dict", "[", "key", "[", "len", "(", "\"module.\"", ")", ":", "]", "]", "=", "value", "\n", "", "else", ":", "\n", "                ", "new_state_dict", "[", "key", "]", "=", "value", "\n", "", "", "state_dict", "=", "new_state_dict", "\n", "\n", "# Print out the differences of pre-trained and model weights.", "\n", "load_keys", "=", "set", "(", "state_dict", ".", "keys", "(", ")", ")", "\n", "model_keys", "=", "set", "(", "self", ".", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"Weights in loaded but not in model:\"", ")", "\n", "for", "key", "in", "sorted", "(", "load_keys", ".", "difference", "(", "model_keys", ")", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "print", "(", ")", "\n", "print", "(", "\"Weights in model but not in loaded:\"", ")", "\n", "for", "key", "in", "sorted", "(", "model_keys", ".", "difference", "(", "load_keys", ")", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "print", "(", ")", "\n", "\n", "# Load weights to model", "\n", "self", ".", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.convert_sents_to_features": [[36, 72], ["enumerate", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "sent.strip", "len", "len", "len", "len", "len", "len", "entry.InputFeatures", "len"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_sents_to_features", "(", "sents", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "i", ",", "sent", ")", "in", "enumerate", "(", "sents", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "sent", ".", "strip", "(", ")", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# Keep segment id which allows loading BERT-weights.", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.set_visual_config": [[74, 78], ["None"], "function", ["None"], ["", "def", "set_visual_config", "(", "args", ")", ":", "\n", "    ", "VISUAL_CONFIG", ".", "l_layers", "=", "args", ".", "llayers", "\n", "VISUAL_CONFIG", ".", "x_layers", "=", "args", ".", "xlayers", "\n", "VISUAL_CONFIG", ".", "r_layers", "=", "args", ".", "rlayers", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.InputExample.__init__": [[28, 38], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "uid", ",", "sent", ",", "visual_feats", "=", "None", ",", "\n", "obj_labels", "=", "None", ",", "attr_labels", "=", "None", ",", "\n", "is_matched", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "self", ".", "uid", "=", "uid", "\n", "self", ".", "sent", "=", "sent", "\n", "self", ".", "visual_feats", "=", "visual_feats", "\n", "self", ".", "obj_labels", "=", "obj_labels", "\n", "self", ".", "attr_labels", "=", "attr_labels", "\n", "self", ".", "is_matched", "=", "is_matched", "# whether the visual and obj matched", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTDataset.__init__": [[41, 73], ["splits.split", "print", "pretrain.qa_answer_table.AnswerTable", "print", "lxmert_data.LXMERTDataset.data.extend", "labelf.items", "json.load", "len", "open", "len", "lxmert_data.LXMERTDataset.answer_table.ans2id_map", "list", "label.keys", "lxmert_data.LXMERTDataset.answer_table.convert_ans", "lxmert_data.LXMERTDataset.answer_table.used", "label.pop", "label.pop"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.ans2id_map", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.convert_ans", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.used"], ["    ", "def", "__init__", "(", "self", ",", "splits", ":", "str", ",", "qa_sets", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param splits: The data sources to be loaded\n        :param qa_sets: if None, no action\n                        o.w., only takes the answers appearing in these dsets\n                              and remove all unlabeled data (MSCOCO captions)\n        \"\"\"", "\n", "self", ".", "name", "=", "splits", "\n", "self", ".", "sources", "=", "splits", ".", "split", "(", "','", ")", "\n", "\n", "# Loading datasets to data", "\n", "self", ".", "data", "=", "[", "]", "\n", "for", "source", "in", "self", ".", "sources", ":", "\n", "            ", "self", ".", "data", ".", "extend", "(", "json", ".", "load", "(", "open", "(", "\"data/lxmert/%s.json\"", "%", "source", ")", ")", ")", "\n", "", "print", "(", "\"Load %d data from %s\"", "%", "(", "len", "(", "self", ".", "data", ")", ",", "self", ".", "name", ")", ")", "\n", "\n", "# Create answer table according to the qa_sets", "\n", "self", ".", "answer_table", "=", "AnswerTable", "(", "qa_sets", ")", "\n", "print", "(", "\"Load an answer table of size %d.\"", "%", "(", "len", "(", "self", ".", "answer_table", ".", "ans2id_map", "(", ")", ")", ")", ")", "\n", "\n", "# Modify the answers", "\n", "for", "datum", "in", "self", ".", "data", ":", "\n", "            ", "labelf", "=", "datum", "[", "'labelf'", "]", "\n", "for", "cat", ",", "labels", "in", "labelf", ".", "items", "(", ")", ":", "\n", "                ", "for", "label", "in", "labels", ":", "\n", "                    ", "for", "ans", "in", "list", "(", "label", ".", "keys", "(", ")", ")", ":", "\n", "                        ", "new_ans", "=", "self", ".", "answer_table", ".", "convert_ans", "(", "ans", ")", "\n", "if", "self", ".", "answer_table", ".", "used", "(", "new_ans", ")", ":", "\n", "                            ", "if", "ans", "!=", "new_ans", ":", "\n", "                                ", "label", "[", "new_ans", "]", "=", "label", ".", "pop", "(", "ans", ")", "\n", "", "", "else", ":", "\n", "                            ", "label", ".", "pop", "(", "ans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTDataset.__len__": [[74, 76], ["len"], "methods", ["None"], ["", "", "", "", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTTorchDataset.__init__": [[88, 132], ["torch.utils.data.Dataset.__init__", "print", "img_data.extend", "sentf.items", "utils.load_obj_tsv", "used_data.append", "enumerate", "len", "lxmert_data.LXMERTTorchDataset.data.append", "lxmert_data.make_uid"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.src.utils.load_obj_tsv", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.make_uid"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ":", "LXMERTDataset", ",", "topk", "=", "-", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "raw_dataset", "=", "dataset", "\n", "self", ".", "task_matched", "=", "args", ".", "task_matched", "\n", "\n", "if", "args", ".", "tiny", ":", "\n", "            ", "topk", "=", "TINY_IMG_NUM", "\n", "", "elif", "args", ".", "fast", ":", "\n", "            ", "topk", "=", "FAST_IMG_NUM", "\n", "\n", "# Load the dataset", "\n", "", "img_data", "=", "[", "]", "\n", "for", "source", "in", "self", ".", "raw_dataset", ".", "sources", ":", "\n", "            ", "img_data", ".", "extend", "(", "load_obj_tsv", "(", "Split2ImgFeatPath", "[", "source", "]", ",", "topk", ")", ")", "\n", "\n", "", "self", ".", "imgid2img", "=", "{", "}", "\n", "for", "img_datum", "in", "img_data", ":", "\n", "            ", "self", ".", "imgid2img", "[", "img_datum", "[", "'img_id'", "]", "]", "=", "img_datum", "\n", "\n", "# Filter out the dataset", "\n", "", "used_data", "=", "[", "]", "\n", "for", "datum", "in", "self", ".", "raw_dataset", ".", "data", ":", "\n", "            ", "if", "datum", "[", "'img_id'", "]", "in", "self", ".", "imgid2img", ":", "\n", "                ", "used_data", ".", "append", "(", "datum", ")", "\n", "\n", "# Flatten the dataset (into one sent + one image entries)", "\n", "", "", "self", ".", "data", "=", "[", "]", "\n", "for", "datum", "in", "used_data", ":", "\n", "            ", "sentf", "=", "datum", "[", "'sentf'", "]", "\n", "for", "sents_cat", ",", "sents", "in", "sentf", ".", "items", "(", ")", ":", "\n", "                ", "if", "sents_cat", "in", "datum", "[", "'labelf'", "]", ":", "\n", "                    ", "labels", "=", "datum", "[", "'labelf'", "]", "[", "sents_cat", "]", "\n", "", "else", ":", "\n", "                    ", "labels", "=", "None", "\n", "", "for", "sent_idx", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "                    ", "new_datum", "=", "{", "\n", "'uid'", ":", "make_uid", "(", "datum", "[", "'img_id'", "]", ",", "sents_cat", ",", "sent_idx", ")", ",", "\n", "'img_id'", ":", "datum", "[", "'img_id'", "]", ",", "\n", "'sent'", ":", "sent", "\n", "}", "\n", "if", "labels", "is", "not", "None", ":", "\n", "                        ", "new_datum", "[", "'label'", "]", "=", "labels", "[", "sent_idx", "]", "\n", "", "self", ".", "data", ".", "append", "(", "new_datum", ")", "\n", "", "", "", "print", "(", "\"Use %d data in torch dataset\"", "%", "(", "len", "(", "self", ".", "data", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTTorchDataset.__len__": [[133, 135], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTTorchDataset.random_feat": [[136, 143], ["random.randint", "random.randint", "len"], "methods", ["None"], ["", "def", "random_feat", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get a random obj feat from the dataset.\"\"\"", "\n", "datum", "=", "self", ".", "data", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "data", ")", "-", "1", ")", "]", "\n", "img_id", "=", "datum", "[", "'img_id'", "]", "\n", "img_info", "=", "self", ".", "imgid2img", "[", "img_id", "]", "\n", "feat", "=", "img_info", "[", "'features'", "]", "[", "random", ".", "randint", "(", "0", ",", "35", ")", "]", "\n", "return", "feat", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTTorchDataset.__getitem__": [[144, 196], ["img_info[].copy", "img_info[].copy", "img_info[].copy", "img_info[].copy", "img_info[].copy", "img_info[].copy", "boxes.copy.copy.copy", "numpy.testing.assert_array_less", "numpy.testing.assert_array_less", "lxmert_data.InputExample", "len", "len", "datum[].copy", "list", "random.random", "datum[].copy.keys", "datum[].copy.pop", "random.randint", "lxmert_data.LXMERTTorchDataset.raw_dataset.answer_table.ans2id", "random.randint", "len", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.ans2id"], ["", "def", "__getitem__", "(", "self", ",", "item", ":", "int", ")", ":", "\n", "        ", "datum", "=", "self", ".", "data", "[", "item", "]", "\n", "\n", "uid", "=", "datum", "[", "'uid'", "]", "\n", "img_id", "=", "datum", "[", "'img_id'", "]", "\n", "\n", "# Get image info", "\n", "img_info", "=", "self", ".", "imgid2img", "[", "img_id", "]", "\n", "obj_num", "=", "img_info", "[", "'num_boxes'", "]", "\n", "feats", "=", "img_info", "[", "'features'", "]", ".", "copy", "(", ")", "\n", "boxes", "=", "img_info", "[", "'boxes'", "]", ".", "copy", "(", ")", "\n", "obj_labels", "=", "img_info", "[", "'objects_id'", "]", ".", "copy", "(", ")", "\n", "obj_confs", "=", "img_info", "[", "'objects_conf'", "]", ".", "copy", "(", ")", "\n", "attr_labels", "=", "img_info", "[", "'attrs_id'", "]", ".", "copy", "(", ")", "\n", "attr_confs", "=", "img_info", "[", "'attrs_conf'", "]", ".", "copy", "(", ")", "\n", "assert", "obj_num", "==", "len", "(", "boxes", ")", "==", "len", "(", "feats", ")", "\n", "\n", "# Normalize the boxes (to 0 ~ 1)", "\n", "img_h", ",", "img_w", "=", "img_info", "[", "'img_h'", "]", ",", "img_info", "[", "'img_w'", "]", "\n", "boxes", "=", "boxes", ".", "copy", "(", ")", "\n", "boxes", "[", ":", ",", "(", "0", ",", "2", ")", "]", "/=", "img_w", "\n", "boxes", "[", ":", ",", "(", "1", ",", "3", ")", "]", "/=", "img_h", "\n", "np", ".", "testing", ".", "assert_array_less", "(", "boxes", ",", "1", "+", "1e-5", ")", "\n", "np", ".", "testing", ".", "assert_array_less", "(", "-", "boxes", ",", "0", "+", "1e-5", ")", "\n", "\n", "# If calculating the matched loss, replace the sentence with an sentence", "\n", "# corresponding to other image.", "\n", "is_matched", "=", "1", "\n", "sent", "=", "datum", "[", "'sent'", "]", "\n", "if", "self", ".", "task_matched", ":", "\n", "            ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "is_matched", "=", "0", "\n", "other_datum", "=", "self", ".", "data", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "data", ")", "-", "1", ")", "]", "\n", "while", "other_datum", "[", "'img_id'", "]", "==", "img_id", ":", "\n", "                    ", "other_datum", "=", "self", ".", "data", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "data", ")", "-", "1", ")", "]", "\n", "", "sent", "=", "other_datum", "[", "'sent'", "]", "\n", "\n", "# Label, convert answer to id", "\n", "", "", "if", "'label'", "in", "datum", ":", "\n", "            ", "label", "=", "datum", "[", "'label'", "]", ".", "copy", "(", ")", "\n", "for", "ans", "in", "list", "(", "label", ".", "keys", "(", ")", ")", ":", "\n", "                ", "label", "[", "self", ".", "raw_dataset", ".", "answer_table", ".", "ans2id", "(", "ans", ")", "]", "=", "label", ".", "pop", "(", "ans", ")", "\n", "", "", "else", ":", "\n", "            ", "label", "=", "None", "\n", "\n", "# Create target", "\n", "", "example", "=", "InputExample", "(", "\n", "uid", ",", "sent", ",", "(", "feats", ",", "boxes", ")", ",", "\n", "(", "obj_labels", ",", "obj_confs", ")", ",", "(", "attr_labels", ",", "attr_confs", ")", ",", "\n", "is_matched", ",", "label", "\n", ")", "\n", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTEvaluator.__init__": [[199, 223], ["sentf.items", "enumerate", "lxmert_data.LXMERTEvaluator.data.append", "lxmert_data.make_uid"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.make_uid"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ":", "LXMERTDataset", ")", ":", "\n", "        ", "self", ".", "raw_dataset", "=", "dataset", "\n", "\n", "# Create QA Eval Data", "\n", "self", ".", "data", "=", "[", "]", "\n", "for", "datum", "in", "self", ".", "raw_dataset", ".", "data", ":", "\n", "            ", "sentf", "=", "datum", "[", "'sentf'", "]", "\n", "for", "sents_cat", ",", "sents", "in", "sentf", ".", "items", "(", ")", ":", "\n", "                ", "if", "sents_cat", "in", "datum", "[", "'labelf'", "]", ":", "# A labeled dataset", "\n", "                    ", "labels", "=", "datum", "[", "'labelf'", "]", "[", "sents_cat", "]", "\n", "for", "sent_idx", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "                        ", "new_datum", "=", "{", "\n", "'uid'", ":", "make_uid", "(", "datum", "[", "'img_id'", "]", ",", "sents_cat", ",", "sent_idx", ")", ",", "\n", "'img_id'", ":", "datum", "[", "'img_id'", "]", ",", "\n", "'sent'", ":", "sent", ",", "\n", "'dset'", ":", "sents_cat", ",", "\n", "'label'", ":", "labels", "[", "sent_idx", "]", "\n", "}", "\n", "self", ".", "data", ".", "append", "(", "new_datum", ")", "\n", "\n", "# uid2datum", "\n", "", "", "", "", "self", ".", "uid2datum", "=", "{", "}", "\n", "for", "datum", "in", "self", ".", "data", ":", "\n", "            ", "self", ".", "uid2datum", "[", "datum", "[", "'uid'", "]", "]", "=", "datum", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTEvaluator.evaluate": [[224, 253], ["collections.defaultdict", "collections.defaultdict", "uid2ans.items", "sorted", "print", "dset2accu.keys"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "uid2ans", ":", "dict", ",", "pprint", "=", "False", ")", ":", "\n", "        ", "score", "=", "0.", "\n", "cnt", "=", "0", "\n", "dset2score", "=", "defaultdict", "(", "lambda", ":", "0.", ")", "\n", "dset2cnt", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "for", "uid", ",", "ans", "in", "uid2ans", ".", "items", "(", ")", ":", "\n", "            ", "if", "uid", "not", "in", "self", ".", "uid2datum", ":", "# Not a labeled data", "\n", "                ", "continue", "\n", "", "datum", "=", "self", ".", "uid2datum", "[", "uid", "]", "\n", "label", "=", "datum", "[", "'label'", "]", "\n", "dset", "=", "datum", "[", "'dset'", "]", "\n", "if", "ans", "in", "label", ":", "\n", "                ", "score", "+=", "label", "[", "ans", "]", "\n", "dset2score", "[", "dset", "]", "+=", "label", "[", "ans", "]", "\n", "", "cnt", "+=", "1", "\n", "dset2cnt", "[", "dset", "]", "+=", "1", "\n", "", "accu", "=", "score", "/", "cnt", "\n", "dset2accu", "=", "{", "}", "\n", "for", "dset", "in", "dset2cnt", ":", "\n", "            ", "dset2accu", "[", "dset", "]", "=", "dset2score", "[", "dset", "]", "/", "dset2cnt", "[", "dset", "]", "\n", "\n", "", "if", "pprint", ":", "\n", "            ", "accu_str", "=", "\"Overall Accu %0.4f, \"", "%", "(", "accu", ")", "\n", "sorted_keys", "=", "sorted", "(", "dset2accu", ".", "keys", "(", ")", ")", "\n", "for", "key", "in", "sorted_keys", ":", "\n", "                ", "accu_str", "+=", "\"%s Accu %0.4f, \"", "%", "(", "key", ",", "dset2accu", "[", "key", "]", ")", "\n", "", "print", "(", "accu_str", ")", "\n", "\n", "", "return", "accu", ",", "dset2accu", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.LXMERTEvaluator.dump_result": [[254, 256], ["None"], "methods", ["None"], ["", "def", "dump_result", "(", "self", ",", "uid2ans", ":", "dict", ",", "path", ")", ":", "\n", "        ", "raise", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_data.make_uid": [[78, 80], ["None"], "function", ["None"], ["", "", "def", "make_uid", "(", "img_id", ",", "dset", ",", "sent_idx", ")", ":", "\n", "    ", "return", "\"%s_%s_%03d\"", "%", "(", "img_id", ",", "dset", ",", "sent_idx", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.__init__": [[27, 44], ["json.load", "set", "enumerate", "open", "set", "len", "len", "enumerate", "len", "set"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["def", "__init__", "(", "self", ",", "dsets", "=", "None", ")", ":", "\n", "        ", "self", ".", "all_ans", "=", "json", ".", "load", "(", "open", "(", "\"data/lxmert/all_ans.json\"", ")", ")", "\n", "if", "dsets", "is", "not", "None", ":", "\n", "            ", "dsets", "=", "set", "(", "dsets", ")", "\n", "# If the answer is used in the dsets", "\n", "self", ".", "anss", "=", "[", "ans", "[", "'ans'", "]", "for", "ans", "in", "self", ".", "all_ans", "if", "\n", "len", "(", "set", "(", "ans", "[", "'dsets'", "]", ")", "&", "dsets", ")", ">", "0", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "anss", "=", "[", "ans", "[", "'ans'", "]", "for", "ans", "in", "self", ".", "all_ans", "]", "\n", "", "self", ".", "ans_set", "=", "set", "(", "self", ".", "anss", ")", "\n", "\n", "self", ".", "_id2ans_map", "=", "self", ".", "anss", "\n", "self", ".", "_ans2id_map", "=", "{", "ans", ":", "ans_id", "for", "ans_id", ",", "ans", "in", "enumerate", "(", "self", ".", "anss", ")", "}", "\n", "\n", "assert", "len", "(", "self", ".", "_id2ans_map", ")", "==", "len", "(", "self", ".", "_ans2id_map", ")", "\n", "for", "ans_id", ",", "ans", "in", "enumerate", "(", "self", ".", "_id2ans_map", ")", ":", "\n", "            ", "assert", "self", ".", "_ans2id_map", "[", "ans", "]", "==", "ans_id", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.convert_ans": [[45, 60], ["ans[].strip.lower", "ans[].strip.startswith", "ans[].strip.startswith", "ans[].strip.startswith", "len", "ans[].strip", "ans[].strip", "ans[].strip", "ans[].strip"], "methods", ["None"], ["", "", "def", "convert_ans", "(", "self", ",", "ans", ")", ":", "\n", "        ", "if", "len", "(", "ans", ")", "==", "0", ":", "\n", "            ", "return", "\"\"", "\n", "", "ans", "=", "ans", ".", "lower", "(", ")", "\n", "if", "ans", "[", "-", "1", "]", "==", "'.'", ":", "\n", "            ", "ans", "=", "ans", "[", ":", "-", "1", "]", ".", "strip", "(", ")", "\n", "", "if", "ans", ".", "startswith", "(", "\"a \"", ")", ":", "\n", "            ", "ans", "=", "ans", "[", "2", ":", "]", ".", "strip", "(", ")", "\n", "", "if", "ans", ".", "startswith", "(", "\"an \"", ")", ":", "\n", "            ", "ans", "=", "ans", "[", "3", ":", "]", ".", "strip", "(", ")", "\n", "", "if", "ans", ".", "startswith", "(", "\"the \"", ")", ":", "\n", "            ", "ans", "=", "ans", "[", "4", ":", "]", ".", "strip", "(", ")", "\n", "", "if", "ans", "in", "self", ".", "ANS_CONVERT", ":", "\n", "            ", "ans", "=", "self", ".", "ANS_CONVERT", "[", "ans", "]", "\n", "", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.ans2id": [[61, 63], ["None"], "methods", ["None"], ["", "def", "ans2id", "(", "self", ",", "ans", ")", ":", "\n", "        ", "return", "self", ".", "_ans2id_map", "[", "ans", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.id2ans": [[64, 66], ["None"], "methods", ["None"], ["", "def", "id2ans", "(", "self", ",", "ans_id", ")", ":", "\n", "        ", "return", "self", ".", "_id2ans_map", "[", "ans_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.ans2id_map": [[67, 69], ["qa_answer_table.AnswerTable._ans2id_map.copy"], "methods", ["None"], ["", "def", "ans2id_map", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_ans2id_map", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.id2ans_map": [[70, 72], ["qa_answer_table.AnswerTable._id2ans_map.copy"], "methods", ["None"], ["", "def", "id2ans_map", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_id2ans_map", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.used": [[73, 75], ["None"], "methods", ["None"], ["", "def", "used", "(", "self", ",", "ans", ")", ":", "\n", "        ", "return", "ans", "in", "self", ".", "ans_set", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.all_answers": [[76, 78], ["qa_answer_table.AnswerTable.anss.copy"], "methods", ["None"], ["", "def", "all_answers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "anss", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.num_answers": [[79, 82], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_answers", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "anss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.load_lxmert_qa": [[84, 173], ["print", "torch.load", "model.state_dict", "list", "torch.load.items", "torch.load.items", "print", "range", "set", "set", "model.lxrt_encoder.model.load_state_dict", "answer_state_dict.pop", "answer_state_dict.pop", "answer_state_dict.pop", "answer_state_dict.pop", "answer_state_dict.pop", "answer_state_dict.pop", "set", "set", "model.load_state_dict", "torch.load.keys", "torch.load.pop", "key.startswith", "key.startswith", "answer_state_dict.keys", "copy.deepcopy", "copy.deepcopy", "qa_answer_table.AnswerTable", "label2ans[].items", "print", "print", "model.lxrt_encoder.model.state_dict().keys", "bert_state_dict.keys", "len", "model.state_dict().keys", "answer_state_dict.keys", "len", "type", "qa_answer_table.AnswerTable.convert_ans", "qa_answer_table.AnswerTable.used", "key.replace", "qa_answer_table.AnswerTable.ans2id", "model.lxrt_encoder.model.state_dict", "model.state_dict", "key.replace", "enumerate", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.convert_ans", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.used", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.ans2id"], ["", "", "def", "load_lxmert_qa", "(", "path", ",", "model", ",", "label2ans", ")", ":", "\n", "    ", "\"\"\"\n    Load model weights from LXMERT pre-training.\n    The answers in the fine-tuned QA task (indicated by label2ans)\n        would also be properly initialized with LXMERT pre-trained\n        QA heads.\n\n    :param path: Path to LXMERT snapshot.\n    :param model: LXRT model instance.\n    :param label2ans: The label2ans dict of fine-tuned QA datasets, like\n        {0: 'cat', 1: 'dog', ...}\n    :return:\n    \"\"\"", "\n", "print", "(", "\"Load QA pre-trained LXMERT from %s \"", "%", "path", ")", "\n", "loaded_state_dict", "=", "torch", ".", "load", "(", "\"%s_LXRT.pth\"", "%", "path", ")", "\n", "model_state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "\n", "# Handle Multi-GPU pre-training --> Single GPU fine-tuning", "\n", "for", "key", "in", "list", "(", "loaded_state_dict", ".", "keys", "(", ")", ")", ":", "\n", "        ", "loaded_state_dict", "[", "key", ".", "replace", "(", "\"module.\"", ",", "''", ")", "]", "=", "loaded_state_dict", ".", "pop", "(", "key", ")", "\n", "\n", "# Isolate bert model", "\n", "", "bert_state_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "loaded_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", ".", "startswith", "(", "'bert.'", ")", ":", "\n", "            ", "bert_state_dict", "[", "key", "]", "=", "value", "\n", "\n", "# Isolate answer head", "\n", "", "", "answer_state_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "loaded_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", ".", "startswith", "(", "\"answer_head.\"", ")", ":", "\n", "            ", "answer_state_dict", "[", "key", ".", "replace", "(", "'answer_head.'", ",", "''", ")", "]", "=", "value", "\n", "\n", "# Do surgery on answer state dict", "\n", "# print(model_state_dict.keys())", "\n", "", "", "ans_weight", "=", "answer_state_dict", "[", "'logit_fc.3.weight'", "]", "\n", "ans_bias", "=", "answer_state_dict", "[", "'logit_fc.3.bias'", "]", "\n", "print", "(", "answer_state_dict", ".", "keys", "(", ")", ")", "\n", "import", "copy", "\n", "for", "class_id", "in", "range", "(", "11", ")", ":", "\n", "        ", "new_answer_weight", "=", "copy", ".", "deepcopy", "(", "model_state_dict", "[", "'logit_fc_class.'", "+", "str", "(", "class_id", ")", "+", "'.3.weight'", "]", ")", "\n", "new_answer_bias", "=", "copy", ".", "deepcopy", "(", "model_state_dict", "[", "'logit_fc_class.'", "+", "str", "(", "class_id", ")", "+", "'.3.bias'", "]", ")", "\n", "answer_table", "=", "AnswerTable", "(", ")", "\n", "loaded", "=", "0", "\n", "unload", "=", "0", "\n", "if", "type", "(", "label2ans", "[", "class_id", "]", ")", "is", "list", ":", "\n", "            ", "label2ans", "[", "class_id", "]", "=", "{", "label", ":", "ans", "for", "label", ",", "ans", "in", "enumerate", "(", "label2ans", "[", "class_id", "]", ")", "}", "\n", "", "for", "label", ",", "ans", "in", "label2ans", "[", "class_id", "]", ".", "items", "(", ")", ":", "\n", "            ", "new_ans", "=", "answer_table", ".", "convert_ans", "(", "ans", ")", "\n", "if", "answer_table", ".", "used", "(", "new_ans", ")", ":", "\n", "                ", "ans_id_9500", "=", "answer_table", ".", "ans2id", "(", "new_ans", ")", "\n", "new_answer_weight", "[", "label", "]", "=", "ans_weight", "[", "ans_id_9500", "]", "\n", "new_answer_bias", "[", "label", "]", "=", "ans_bias", "[", "ans_id_9500", "]", "\n", "loaded", "+=", "1", "\n", "", "else", ":", "\n", "                ", "new_answer_weight", "[", "label", "]", "=", "0.", "\n", "new_answer_bias", "[", "label", "]", "=", "0.", "\n", "unload", "+=", "1", "\n", "", "", "print", "(", "\"Loaded %d answers from %dth LXRTQA pre-training and %d not\"", "%", "(", "loaded", ",", "class_id", ",", "unload", ")", ")", "\n", "print", "(", ")", "\n", "answer_state_dict", "[", "'logit_fc_class.'", "+", "str", "(", "class_id", ")", "+", "'.3.weight'", "]", "=", "new_answer_weight", "\n", "answer_state_dict", "[", "'logit_fc_class.'", "+", "str", "(", "class_id", ")", "+", "'.3.bias'", "]", "=", "new_answer_bias", "\n", "\n", "# Load Bert Weights", "\n", "", "bert_model_keys", "=", "set", "(", "model", ".", "lxrt_encoder", ".", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "bert_loaded_keys", "=", "set", "(", "bert_state_dict", ".", "keys", "(", ")", ")", "\n", "assert", "len", "(", "bert_model_keys", "-", "bert_loaded_keys", ")", "==", "0", "\n", "model", ".", "lxrt_encoder", ".", "model", ".", "load_state_dict", "(", "bert_state_dict", ",", "strict", "=", "False", ")", "\n", "\n", "# Load Answer Logic FC Weights", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.0.weight'", ")", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.0.bias'", ")", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.2.weight'", ")", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.2.bias'", ")", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.3.weight'", ")", "\n", "answer_state_dict", ".", "pop", "(", "'logit_fc.3.bias'", ")", "\n", "answer_state_dict", "[", "'logit_fc_type.0.weight'", "]", "=", "model_state_dict", "[", "'logit_fc_type.0.weight'", "]", "\n", "answer_state_dict", "[", "'logit_fc_type.0.bias'", "]", "=", "model_state_dict", "[", "'logit_fc_type.0.bias'", "]", "\n", "answer_state_dict", "[", "'logit_fc_type.2.weight'", "]", "=", "model_state_dict", "[", "'logit_fc_type.2.weight'", "]", "\n", "answer_state_dict", "[", "'logit_fc_type.2.bias'", "]", "=", "model_state_dict", "[", "'logit_fc_type.2.bias'", "]", "\n", "answer_state_dict", "[", "'logit_fc_type.3.weight'", "]", "=", "model_state_dict", "[", "'logit_fc_type.3.weight'", "]", "\n", "answer_state_dict", "[", "'logit_fc_type.3.bias'", "]", "=", "model_state_dict", "[", "'logit_fc_type.3.bias'", "]", "\n", "\n", "\n", "model_keys", "=", "set", "(", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "ans_loaded_keys", "=", "set", "(", "answer_state_dict", ".", "keys", "(", ")", ")", "\n", "assert", "len", "(", "ans_loaded_keys", "-", "model_keys", ")", "==", "0", "\n", "\n", "model", ".", "load_state_dict", "(", "answer_state_dict", ",", "strict", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.InputFeatures.__init__": [[54, 69], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "lm_label_ids", ",", "\n", "visual_feats", ",", "obj_labels", ",", "\n", "is_matched", ",", "ans", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "lm_label_ids", "=", "lm_label_ids", "\n", "\n", "self", ".", "visual_feats", "=", "visual_feats", "\n", "self", ".", "obj_labels", "=", "obj_labels", "\n", "\n", "self", ".", "is_matched", "=", "is_matched", "\n", "\n", "self", ".", "ans", "=", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__": [[215, 250], ["super().__init__", "lxrt.tokenization.BertTokenizer.from_pretrained", "lxrt.entry.set_visual_config", "lxrt.modeling.LXRTPretraining.from_pretrained", "lxmert_pretrain.LXMERT.model.cuda", "print", "lxmert_pretrain.LXMERT.model.apply", "lxmert_pretrain.LXMERT.load", "lxmert_pretrain.LXMERT.load_lxmert", "torch.DataParallel", "torch.DataParallel"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.__init__", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.entry.set_visual_config", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.utils.data_utils.ShardedDataIterator.apply", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.load_lxmert"], ["    ", "def", "__init__", "(", "self", ",", "max_seq_length", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "\n", "do_lower_case", "=", "True", "\n", ")", "\n", "\n", "# Build model", "\n", "set_visual_config", "(", "args", ")", "\n", "self", ".", "model", "=", "LXRTPretraining", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "\n", "task_mask_lm", "=", "args", ".", "task_mask_lm", ",", "\n", "task_obj_predict", "=", "args", ".", "task_obj_predict", ",", "\n", "task_matched", "=", "args", ".", "task_matched", ",", "\n", "task_qa", "=", "args", ".", "task_qa", ",", "\n", "visual_losses", "=", "args", ".", "visual_losses", ",", "\n", "num_answers", "=", "train_tuple", ".", "dataset", ".", "answer_table", ".", "num_answers", "\n", ")", "\n", "\n", "# Weight initialization and loading", "\n", "if", "args", ".", "from_scratch", ":", "\n", "            ", "print", "(", "\"Train from Scratch: re-initialize all BERT weights.\"", ")", "\n", "self", ".", "model", ".", "apply", "(", "self", ".", "model", ".", "init_bert_weights", ")", "\n", "", "if", "args", ".", "load", "is", "not", "None", ":", "\n", "            ", "self", ".", "load", "(", "args", ".", "load", ")", "\n", "", "if", "args", ".", "load_lxmert", "is", "not", "None", ":", "\n", "# Load lxmert would not load the answer head.", "\n", "            ", "self", ".", "load_lxmert", "(", "args", ".", "load_lxmert", ")", "\n", "\n", "# GPU Options", "\n", "", "self", ".", "model", "=", "self", ".", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "multiGPU", ":", "\n", "            ", "self", ".", "model", "=", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward": [[251, 288], ["torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "lxmert_pretrain.LXMERT.model", "lxmert_pretrain.convert_example_to_features", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "losses.detach().cpu", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.stack", "numpy.stack", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "torch.from_numpy().cuda.size", "numpy.stack", "losses.detach", "numpy.stack", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.convert_example_to_features"], ["", "", "def", "forward", "(", "self", ",", "examples", ")", ":", "\n", "        ", "train_features", "=", "[", "convert_example_to_features", "(", "example", ",", "self", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "for", "example", "in", "examples", "]", "\n", "\n", "# language Inputs", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "# Visual Inputs", "\n", "feats", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "[", "f", ".", "visual_feats", "[", "0", "]", "for", "f", "in", "train_features", "]", ")", ")", ".", "cuda", "(", ")", "\n", "pos", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "[", "f", ".", "visual_feats", "[", "1", "]", "for", "f", "in", "train_features", "]", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "# Language Prediction", "\n", "lm_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "lm_label_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "# Visual Prediction", "\n", "obj_labels", "=", "{", "}", "\n", "for", "key", "in", "(", "'obj'", ",", "'attr'", ",", "'feat'", ")", ":", "\n", "            ", "visn_labels", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "[", "f", ".", "obj_labels", "[", "key", "]", "[", "0", "]", "for", "f", "in", "train_features", "]", ")", ")", ".", "cuda", "(", ")", "\n", "visn_mask", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "[", "f", ".", "obj_labels", "[", "key", "]", "[", "1", "]", "for", "f", "in", "train_features", "]", ")", ")", ".", "cuda", "(", ")", "\n", "assert", "visn_labels", ".", "size", "(", "0", ")", "==", "visn_mask", ".", "size", "(", "0", ")", "and", "visn_labels", ".", "size", "(", "1", ")", "==", "visn_mask", ".", "size", "(", "1", ")", "\n", "obj_labels", "[", "key", "]", "=", "(", "visn_labels", ",", "visn_mask", ")", "\n", "\n", "# Joint Prediction", "\n", "", "matched_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "is_matched", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "ans", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "[", "f", ".", "ans", "for", "f", "in", "train_features", "]", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "\"\"\"\n        forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n                visual_feats=None, pos=None, obj_labels=None, matched_label=None, ans=None):\n        \"\"\"", "\n", "loss", ",", "losses", ",", "ans_logit", "=", "self", ".", "model", "(", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_labels", ",", "\n", "feats", ",", "pos", ",", "obj_labels", ",", "matched_labels", ",", "ans", "\n", ")", "\n", "return", "loss", ",", "losses", ".", "detach", "(", ")", ".", "cpu", "(", ")", ",", "ans_logit", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train_batch": [[289, 300], ["optim.zero_grad", "lxmert_pretrain.LXMERT.forward", "loss.mean.mean.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "optim.step", "loss.mean.mean.mean", "losses.mean.mean.mean", "lxmert_pretrain.LXMERT.model.parameters", "loss.mean.mean.item", "losses.mean.mean.cpu().numpy", "losses.mean.mean.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.optimization.BertAdam.step"], ["", "def", "train_batch", "(", "self", ",", "optim", ",", "batch", ")", ":", "\n", "        ", "optim", ".", "zero_grad", "(", ")", "\n", "loss", ",", "losses", ",", "ans_logit", "=", "self", ".", "forward", "(", "batch", ")", "\n", "if", "args", ".", "multiGPU", ":", "\n", "            ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "losses", "=", "losses", ".", "mean", "(", "0", ")", "\n", "", "loss", ".", "backward", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "1.", ")", "\n", "optim", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", ",", "losses", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "ans_logit", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.valid_batch": [[301, 308], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "lxmert_pretrain.LXMERT.forward", "loss.mean.mean.item", "losses.mean.mean.cpu().numpy", "loss.mean.mean.mean", "losses.mean.mean.mean", "losses.mean.mean.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.forward"], ["", "def", "valid_batch", "(", "self", ",", "batch", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", ",", "losses", ",", "ans_logit", "=", "self", ".", "forward", "(", "batch", ")", "\n", "if", "args", ".", "multiGPU", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "losses", "=", "losses", ".", "mean", "(", "0", ")", "\n", "", "", "return", "loss", ".", "item", "(", ")", ",", "losses", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "ans_logit", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train": [[309, 359], ["len", "int", "int", "print", "print", "print", "BertAdam", "range", "lxmert_pretrain.LXMERT.model.parameters", "lxmert_pretrain.LXMERT.model.train", "tqdm.tqdm.tqdm", "print", "zip", "print", "lxmert_pretrain.LXMERT.evaluate_epoch", "lxmert_pretrain.LXMERT.save", "lxmert_pretrain.LXMERT.train_batch", "train_tuple.evaluator.evaluate", "lxmert_pretrain.LXMERT.save", "len", "logit.max", "zip", "label.cpu().numpy", "train_tuple.dataset.answer_table.id2ans", "label.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.evaluate_epoch", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train_batch", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.id2ans"], ["", "def", "train", "(", "self", ",", "train_tuple", ":", "DataTuple", ",", "eval_tuple", ":", "DataTuple", ")", ":", "\n", "        ", "train_ld", "=", "train_tuple", ".", "loader", "\n", "\n", "# Optimizer", "\n", "from", "lxrt", ".", "optimization", "import", "BertAdam", "\n", "batch_per_epoch", "=", "len", "(", "train_ld", ")", "\n", "t_total", "=", "int", "(", "batch_per_epoch", "*", "args", ".", "epochs", ")", "\n", "warmup_ratio", "=", "0.05", "\n", "warmup_iters", "=", "int", "(", "t_total", "*", "warmup_ratio", ")", "\n", "print", "(", "\"Batch per epoch: %d\"", "%", "batch_per_epoch", ")", "\n", "print", "(", "\"Total Iters: %d\"", "%", "t_total", ")", "\n", "print", "(", "\"Warm up Iters: %d\"", "%", "warmup_iters", ")", "\n", "optim", "=", "BertAdam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "warmup", "=", "warmup_ratio", ",", "t_total", "=", "t_total", ")", "\n", "\n", "# Train", "\n", "best_eval_loss", "=", "9595.", "\n", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "# Train", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "total_loss", "=", "0.", "\n", "total_losses", "=", "0.", "\n", "uid2ans", "=", "{", "}", "\n", "for", "batch", "in", "tqdm", "(", "train_ld", ",", "total", "=", "len", "(", "train_ld", ")", ")", ":", "\n", "                ", "loss", ",", "losses", ",", "logit", "=", "self", ".", "train_batch", "(", "optim", ",", "batch", ")", "\n", "total_loss", "+=", "loss", "\n", "total_losses", "+=", "losses", "\n", "\n", "if", "args", ".", "task_qa", ":", "\n", "                    ", "score", ",", "label", "=", "logit", ".", "max", "(", "1", ")", "\n", "for", "datum", ",", "l", "in", "zip", "(", "batch", ",", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "                        ", "uid", "=", "datum", ".", "uid", "\n", "ans", "=", "train_tuple", ".", "dataset", ".", "answer_table", ".", "id2ans", "(", "l", ")", "\n", "uid2ans", "[", "uid", "]", "=", "ans", "\n", "\n", "", "", "", "print", "(", "\"The training loss for Epoch %d is %0.4f\"", "%", "(", "epoch", ",", "total_loss", "/", "batch_per_epoch", ")", ")", "\n", "losses_str", "=", "\"The losses are \"", "\n", "for", "name", ",", "loss", "in", "zip", "(", "LOSSES_NAME", ",", "total_losses", ")", ":", "\n", "                ", "losses_str", "+=", "\"%s: %0.4f \"", "%", "(", "name", ",", "loss", "/", "batch_per_epoch", ")", "\n", "", "print", "(", "losses_str", ")", "\n", "if", "args", ".", "task_qa", ":", "\n", "                ", "train_tuple", ".", "evaluator", ".", "evaluate", "(", "uid2ans", ",", "pprint", "=", "True", ")", "\n", "\n", "# Eval", "\n", "", "avg_eval_loss", "=", "self", ".", "evaluate_epoch", "(", "eval_tuple", ",", "iters", "=", "-", "1", ")", "\n", "\n", "# Save", "\n", "if", "avg_eval_loss", "<", "best_eval_loss", ":", "\n", "                ", "best_eval_loss", "=", "avg_eval_loss", "\n", "self", ".", "save", "(", "\"BEST_EVAL_LOSS\"", ")", "\n", "", "self", ".", "save", "(", "\"Epoch%02d\"", "%", "(", "epoch", "+", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.evaluate_epoch": [[360, 389], ["lxmert_pretrain.LXMERT.model.eval", "enumerate", "print", "zip", "print", "lxmert_pretrain.LXMERT.valid_batch", "eval_tuple.evaluator.evaluate", "len", "logit.max", "zip", "len", "label.cpu().numpy", "train_tuple.dataset.answer_table.id2ans", "len", "label.cpu"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.valid_batch", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.qa_answer_table.AnswerTable.id2ans"], ["", "", "def", "evaluate_epoch", "(", "self", ",", "eval_tuple", ":", "DataTuple", ",", "iters", ":", "int", "=", "-", "1", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "eval_ld", "=", "eval_tuple", ".", "loader", "\n", "total_loss", "=", "0.", "\n", "total_losses", "=", "0.", "\n", "uid2ans", "=", "{", "}", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "eval_ld", ")", ":", "\n", "            ", "loss", ",", "losses", ",", "logit", "=", "self", ".", "valid_batch", "(", "batch", ")", "\n", "total_loss", "+=", "loss", "\n", "total_losses", "+=", "losses", "\n", "if", "args", ".", "task_qa", ":", "\n", "                ", "score", ",", "label", "=", "logit", ".", "max", "(", "1", ")", "\n", "for", "datum", ",", "l", "in", "zip", "(", "batch", ",", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "                    ", "uid", "=", "datum", ".", "uid", "\n", "ans", "=", "train_tuple", ".", "dataset", ".", "answer_table", ".", "id2ans", "(", "l", ")", "\n", "uid2ans", "[", "uid", "]", "=", "ans", "\n", "", "", "if", "i", "==", "iters", ":", "\n", "                ", "break", "\n", "\n", "", "", "print", "(", "\"The valid loss is %0.4f\"", "%", "(", "total_loss", "/", "len", "(", "eval_ld", ")", ")", ")", "\n", "losses_str", "=", "\"The losses are \"", "\n", "for", "name", ",", "loss", "in", "zip", "(", "LOSSES_NAME", ",", "total_losses", "/", "len", "(", "eval_ld", ")", ")", ":", "\n", "            ", "losses_str", "+=", "\"%s: %0.4f \"", "%", "(", "name", ",", "loss", ")", "\n", "", "print", "(", "losses_str", ")", "\n", "\n", "if", "args", ".", "task_qa", ":", "\n", "            ", "eval_tuple", ".", "evaluator", ".", "evaluate", "(", "uid2ans", ",", "pprint", "=", "True", ")", "\n", "\n", "", "return", "total_loss", "/", "len", "(", "eval_ld", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save": [[390, 393], ["torch.save", "torch.save", "torch.save", "torch.save", "lxmert_pretrain.LXMERT.model.state_dict", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.save"], ["", "def", "save", "(", "self", ",", "name", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"%s_LXRT.pth\"", "%", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.load": [[394, 398], ["print", "torch.load", "torch.load", "torch.load", "torch.load", "lxmert_pretrain.LXMERT.model.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "\"Load BERT extractor from %s\"", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "\"%s_LXRT.pth\"", "%", "path", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.load_lxmert": [[399, 428], ["print", "torch.load", "torch.load", "torch.load", "torch.load", "list", "torch.load.items", "torch.load.items", "set", "set", "print", "print", "sorted", "print", "print", "sorted", "print", "lxmert_pretrain.LXMERT.model.load_state_dict", "torch.load.keys", "torch.load.keys", "key.startswith", "torch.load.keys", "torch.load.keys", "lxmert_pretrain.LXMERT.model.state_dict().keys", "set.difference", "print", "set.difference", "print", "torch.load.pop", "torch.load.pop", "lxmert_pretrain.LXMERT.model.state_dict", "len"], "methods", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["", "def", "load_lxmert", "(", "self", ",", "path", ")", ":", "\n", "        ", "print", "(", "\"Load LXMERT model from %s\"", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "\"%s_LXRT.pth\"", "%", "path", ")", "\n", "\n", "# Do not load any answer head", "\n", "for", "key", "in", "list", "(", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "if", "'answer'", "in", "key", ":", "\n", "                ", "state_dict", ".", "pop", "(", "key", ")", "\n", "\n", "# Change Multi GPU to single GPU", "\n", "", "", "new_state_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", ".", "startswith", "(", "\"module.\"", ")", ":", "\n", "                ", "new_state_dict", "[", "key", "[", "len", "(", "\"module.\"", ")", ":", "]", "]", "=", "value", "\n", "", "", "state_dict", "=", "new_state_dict", "\n", "\n", "load_keys", "=", "set", "(", "state_dict", ".", "keys", "(", ")", ")", "\n", "model_keys", "=", "set", "(", "self", ".", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "print", "(", ")", "\n", "print", "(", "\"Keys in loaded but not in model:\"", ")", "\n", "for", "key", "in", "sorted", "(", "load_keys", ".", "difference", "(", "model_keys", ")", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "print", "(", ")", "\n", "print", "(", "\"Keys in model but not in loaded:\"", ")", "\n", "for", "key", "in", "sorted", "(", "model_keys", ".", "difference", "(", "load_keys", ")", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "print", "(", ")", "\n", "\n", "self", ".", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.get_tuple": [[23, 44], ["pretrain.lxmert_data.LXMERTDataset", "pretrain.lxmert_data.LXMERTTorchDataset", "torch.utils.data.DataLoader", "pretrain.lxmert_data.LXMERTEvaluator", "print", "DataTuple", "set", "qa_set.lower().strip", "set.split", "qa_set.lower", "param.args.train", "param.args.batch_size", "param.args.valid"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.LXMERT.train"], ["def", "get_tuple", "(", "splits", ":", "str", ",", "bs", ":", "int", ",", "shuffle", "=", "False", ",", "drop_last", "=", "False", ",", "topk", "=", "-", "1", ")", "->", "DataTuple", ":", "\n", "# Decide which QA datasets would be used in pre-training.", "\n", "# Options: vqa, gqa, visual7w", "\n", "# Note: visual7w is a part of vgqa, we take the name here.", "\n", "    ", "qa_sets", "=", "args", ".", "qa_sets", "\n", "if", "qa_sets", "is", "not", "None", ":", "\n", "        ", "qa_sets", "=", "set", "(", "qa_set", ".", "lower", "(", ")", ".", "strip", "(", ")", "for", "qa_set", "in", "qa_sets", ".", "split", "(", "\",\"", ")", ")", "\n", "\n", "# Build dataset, data loader, and evaluator.", "\n", "", "dset", "=", "LXMERTDataset", "(", "splits", ",", "qa_sets", "=", "qa_sets", ")", "\n", "tset", "=", "LXMERTTorchDataset", "(", "dset", ",", "topk", ")", "\n", "data_loader", "=", "DataLoader", "(", "\n", "tset", ",", "batch_size", "=", "bs", ",", "\n", "shuffle", "=", "shuffle", ",", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "collate_fn", "=", "lambda", "x", ":", "x", ",", "\n", "drop_last", "=", "drop_last", ",", "pin_memory", "=", "True", "\n", ")", "\n", "evaluator", "=", "LXMERTEvaluator", "(", "dset", ")", "\n", "print", "(", ")", "\n", "\n", "return", "DataTuple", "(", "dataset", "=", "dset", ",", "torchdset", "=", "tset", ",", "loader", "=", "data_loader", ",", "evaluator", "=", "evaluator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.random_word": [[71, 108], ["enumerate", "random.random", "output_label.append", "output_label.append", "output_label.append", "random.choice", "list", "tokenizer.vocab.items"], "function", ["None"], ["", "", "def", "random_word", "(", "tokens", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"\n    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n    :param tokens: list of str, tokenized sentence.\n    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n    \"\"\"", "\n", "output_label", "=", "[", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "prob", "=", "random", ".", "random", "(", ")", "\n", "# mask token with probability", "\n", "ratio", "=", "args", ".", "word_mask_rate", "\n", "if", "prob", "<", "ratio", ":", "\n", "            ", "prob", "/=", "ratio", "\n", "\n", "# 80% randomly change token to mask token", "\n", "if", "prob", "<", "0.8", ":", "\n", "                ", "tokens", "[", "i", "]", "=", "\"[MASK]\"", "\n", "\n", "# 10% randomly change token to random token", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                ", "tokens", "[", "i", "]", "=", "random", ".", "choice", "(", "list", "(", "tokenizer", ".", "vocab", ".", "items", "(", ")", ")", ")", "[", "0", "]", "\n", "\n", "# -> rest 10% randomly keep current token", "\n", "\n", "# append current token to output (we will predict these later)", "\n", "", "try", ":", "\n", "                ", "output_label", ".", "append", "(", "tokenizer", ".", "vocab", "[", "token", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "# For unknown words (should not occur with BPE vocab)", "\n", "                ", "output_label", ".", "append", "(", "tokenizer", ".", "vocab", "[", "\"[UNK]\"", "]", ")", "\n", "", "", "else", ":", "\n", "# no masking token (will be ignored by loss function later)", "\n", "            ", "output_label", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "return", "tokens", ",", "output_label", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.random_feat": [[110, 132], ["feats.copy", "numpy.zeros", "range", "len", "len", "random.random", "train_tuple.torchdset.random_feat"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.random_feat"], ["", "def", "random_feat", "(", "feats", ")", ":", "\n", "    ", "mask_feats", "=", "feats", ".", "copy", "(", ")", "\n", "feat_mask", "=", "np", ".", "zeros", "(", "len", "(", "feats", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "feats", ")", ")", ":", "\n", "        ", "prob", "=", "random", ".", "random", "(", ")", "\n", "# mask token with probability", "\n", "if", "prob", "<", "args", ".", "obj_mask_rate", ":", "\n", "            ", "prob", "/=", "args", ".", "obj_mask_rate", "\n", "\n", "# 80% randomly change token to zero feat", "\n", "if", "prob", "<", "0.8", ":", "\n", "                ", "mask_feats", "[", "i", ",", ":", "]", "=", "0.", "\n", "\n", "# 10% randomly change token to random feat", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                ", "mask_feats", "[", "i", ",", ":", "]", "=", "train_tuple", ".", "torchdset", ".", "random_feat", "(", ")", "\n", "# -> rest 10% randomly keep current feat", "\n", "\n", "# Need to predict this feat", "\n", "", "feat_mask", "[", "i", "]", "=", "1.", "\n", "\n", "", "", "return", "mask_feats", ",", "feat_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.convert_example_to_features": [[134, 209], ["tokenizer.tokenize", "lxmert_pretrain.random_word", "tokenizer.convert_tokens_to_ids", "lxmert_pretrain.random_feat", "lxmert_pretrain.InputFeatures", "example.sent.strip", "len", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "lm_label_ids.append", "len", "len", "len", "len", "zip", "len", "len", "sum", "numpy.random.multinomial().argmax", "example.label.items", "numpy.random.multinomial"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.random_word", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.lxrt.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.pretrain.lxmert_pretrain.random_feat"], ["", "def", "convert_example_to_features", "(", "example", ":", "InputExample", ",", "max_seq_length", ",", "tokenizer", ")", "->", "InputFeatures", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n    :param example: InputExample, containing sentence input as strings and is_next label\n    :param max_seq_length: int, maximum length of sequence.\n    :param tokenizer: Tokenizer\n    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n    \"\"\"", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "sent", ".", "strip", "(", ")", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", "-", "2", ":", "\n", "        ", "tokens", "=", "tokens", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# Ge random words", "\n", "", "masked_tokens", ",", "masked_label", "=", "random_word", "(", "tokens", ",", "tokenizer", ")", "\n", "\n", "# concatenate lm labels and account for CLS, SEP, SEP", "\n", "masked_tokens", "=", "[", "'[CLS]'", "]", "+", "masked_tokens", "+", "[", "'[SEP]'", "]", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "masked_tokens", ")", "\n", "\n", "# Mask & Segment Word", "\n", "lm_label_ids", "=", "(", "[", "-", "1", "]", "+", "masked_label", "+", "[", "-", "1", "]", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "lm_label_ids", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "lm_label_ids", ")", "==", "max_seq_length", "\n", "\n", "feat", ",", "boxes", "=", "example", ".", "visual_feats", "\n", "obj_labels", ",", "obj_confs", "=", "example", ".", "obj_labels", "\n", "attr_labels", ",", "attr_confs", "=", "example", ".", "attr_labels", "\n", "\n", "# Mask Image Features:", "\n", "masked_feat", ",", "feat_mask", "=", "random_feat", "(", "feat", ")", "\n", "\n", "# QA answer label", "\n", "if", "example", ".", "label", "is", "None", "or", "len", "(", "example", ".", "label", ")", "==", "0", "or", "example", ".", "is_matched", "!=", "1", ":", "\n", "# 1. No label 2. Label is pruned 3. unmatched visual + language pair", "\n", "        ", "ans", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "keys", ",", "values", "=", "zip", "(", "*", "example", ".", "label", ".", "items", "(", ")", ")", "\n", "if", "len", "(", "keys", ")", "==", "1", ":", "\n", "            ", "ans", "=", "keys", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "value_sum", "=", "sum", "(", "values", ")", "\n", "prob", "=", "[", "value", "/", "value_sum", "for", "value", "in", "values", "]", "\n", "choice", "=", "np", ".", "random", ".", "multinomial", "(", "1", ",", "prob", ")", ".", "argmax", "(", ")", "\n", "ans", "=", "keys", "[", "choice", "]", "\n", "\n", "", "", "features", "=", "InputFeatures", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "lm_label_ids", "=", "lm_label_ids", ",", "\n", "visual_feats", "=", "(", "masked_feat", ",", "boxes", ")", ",", "\n", "obj_labels", "=", "{", "\n", "'obj'", ":", "(", "obj_labels", ",", "obj_confs", ")", ",", "\n", "'attr'", ":", "(", "attr_labels", ",", "attr_confs", ")", ",", "\n", "'feat'", ":", "(", "feat", ",", "feat_mask", ")", ",", "\n", "}", ",", "\n", "is_matched", "=", "example", ".", "is_matched", ",", "\n", "ans", "=", "ans", ",", "\n", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.okvqa_data.used_img_feats.get_img": [[4, 17], ["open", "json.load", "open", "json.dump", "img_name_list.append"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["def", "get_img", "(", "qa_path", ",", "write_path", ")", ":", "\n", "    ", "img_name_list", "=", "[", "]", "\n", "if", "'train'", "in", "qa_path", ":", "\n", "        ", "folder", "=", "'train/'", "\n", "", "else", ":", "\n", "        ", "folder", "=", "'val/'", "\n", "", "with", "open", "(", "qa_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "qas", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "item", "in", "qas", ":", "\n", "            ", "img_name_list", ".", "append", "(", "'/scratch/mluo26/yankai/output/'", "+", "folder", "+", "item", "[", "'img_id'", "]", "+", "'.npy'", ")", "\n", "\n", "", "", "with", "open", "(", "write_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "img_name_list", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.get_most_frequent_answer": [[10, 14], ["a[].lower", "collections.Counter().most_common", "collections.Counter"], "function", ["None"], ["def", "get_most_frequent_answer", "(", "answers", ":", "list", ")", ":", "\n", "    ", "answers_list", "=", "[", "a", "[", "'answer'", "]", ".", "lower", "(", ")", "for", "a", "in", "answers", "]", "\n", "ans", "=", "Counter", "(", "answers_list", ")", ".", "most_common", "(", ")", "[", "0", "]", "[", "0", "]", "\n", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.evaluate": [[15, 93], ["utils.load", "len", "tqdm.tqdm", "correct_top1.items", "correct_most_frequence.items", "d[].split", "qa_pipeline", "list", "isinstance", "predicted_answer.append", "print", "print", "utils.dump", "len", "d[].keys", "len", "get_most_frequent_answer().lower().rstrip().strip", "sorted", "[].lower().rstrip().strip", "good_answers.append", "get_most_frequent_answer().lower().rstrip", "[].lower().rstrip", "len", "len", "[].lower().rstrip().strip.lower().rstrip().strip", "get_most_frequent_answer().lower().rstrip().strip.lower().rstrip().strip", "get_most_frequent_answer().lower", "[].lower", "[].lower().rstrip().strip.lower().rstrip", "get_most_frequent_answer().lower().rstrip().strip.lower().rstrip", "predict_answer.get_most_frequent_answer", "[].lower().rstrip().strip.lower", "get_most_frequent_answer().lower().rstrip().strip.lower"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump", "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.predict_answer.get_most_frequent_answer"], ["", "def", "evaluate", "(", "args", ",", "qa_pipeline", ")", ":", "\n", "    ", "data", "=", "load", "(", "args", ".", "retrieve_kn_file", ")", "\n", "all_q", "=", "len", "(", "data", ")", "\n", "topk", "=", "100", "\n", "predicted_answer", "=", "[", "]", "\n", "correct_top1", "=", "{", "\n", "1", ":", "0", ",", "\n", "3", ":", "0", ",", "\n", "5", ":", "0", ",", "\n", "8", ":", "0", ",", "\n", "10", ":", "0", ",", "\n", "15", ":", "0", ",", "\n", "20", ":", "0", ",", "\n", "50", ":", "0", ",", "\n", "60", ":", "0", ",", "\n", "70", ":", "0", ",", "\n", "80", ":", "0", ",", "\n", "90", ":", "0", ",", "\n", "100", ":", "0", "\n", "}", "\n", "\n", "correct_most_frequence", "=", "{", "\n", "1", ":", "0", ",", "\n", "3", ":", "0", ",", "\n", "5", ":", "0", ",", "\n", "8", ":", "0", ",", "\n", "10", ":", "0", ",", "\n", "15", ":", "0", ",", "\n", "20", ":", "0", ",", "\n", "50", ":", "0", ",", "\n", "60", ":", "0", ",", "\n", "70", ":", "0", ",", "\n", "80", ":", "0", ",", "\n", "90", ":", "0", ",", "\n", "100", ":", "0", "\n", "}", "\n", "\n", "unanswerable", "=", "0", "\n", "for", "d", "in", "tqdm", ".", "tqdm", "(", "data", ")", ":", "\n", "        ", "question", "=", "d", "[", "'question'", "]", ".", "split", "(", "\"?\"", ")", "\n", "caption", "=", "question", "[", "1", "]", "\n", "question", "=", "[", "question", "[", "0", "]", "+", "\"?\"", "]", "\n", "\n", "context", "=", "[", "\"unanswerable, \"", "+", "caption", "+", "\" \"", "+", "c", "[", "'text'", "]", "for", "c", "in", "d", "[", "'ctxs'", "]", "[", ":", "topk", "]", "]", "\n", "question", "=", "question", "*", "len", "(", "context", ")", "\n", "answers", "=", "qa_pipeline", "(", "question", "=", "question", ",", "context", "=", "context", ")", "\n", "\n", "gold", "=", "list", "(", "d", "[", "'answers'", "]", ".", "keys", "(", ")", ")", "\n", "good_answers", "=", "[", "]", "\n", "if", "isinstance", "(", "answers", ",", "dict", ")", ":", "\n", "            ", "answers", "=", "[", "answers", "]", "\n", "", "for", "a", "in", "answers", ":", "\n", "            ", "if", "a", "[", "'answer'", "]", "!=", "'unanswerable'", ":", "\n", "                ", "good_answers", ".", "append", "(", "a", ")", "\n", "", "", "answers", "=", "good_answers", "\n", "predicted_answer", ".", "append", "(", "answers", ")", "\n", "if", "len", "(", "answers", ")", "==", "0", ":", "\n", "            ", "unanswerable", "+=", "1", "\n", "continue", "\n", "\n", "", "for", "size", "in", "correct_top1", ":", "\n", "\n", "            ", "common", "=", "get_most_frequent_answer", "(", "answers", "[", ":", "size", "]", ")", ".", "lower", "(", ")", ".", "rstrip", "(", ")", ".", "strip", "(", ")", "\n", "ans", "=", "sorted", "(", "answers", "[", ":", "size", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "top1", "=", "ans", "[", "0", "]", "[", "'answer'", "]", ".", "lower", "(", ")", ".", "rstrip", "(", ")", ".", "strip", "(", ")", "\n", "if", "top1", "in", "gold", ":", "\n", "                ", "correct_top1", "[", "size", "]", "+=", "d", "[", "'answers'", "]", "[", "top1", ".", "lower", "(", ")", ".", "rstrip", "(", ")", ".", "strip", "(", ")", "]", "\n", "", "if", "common", "in", "gold", ":", "\n", "                ", "correct_most_frequence", "[", "size", "]", "+=", "d", "[", "'answers'", "]", "[", "common", ".", "lower", "(", ")", ".", "rstrip", "(", ")", ".", "strip", "(", ")", "]", "\n", "", "", "d", "[", "'top1_prediction'", "]", "=", "top1", "\n", "d", "[", "'common_prediction'", "]", "=", "common", "\n", "\n", "", "for", "k", ",", "v", "in", "correct_top1", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "\"accuracy of using highest score strategy in the top{} text is {:.2f}\"", ".", "format", "(", "k", ",", "100", "*", "v", "/", "(", "len", "(", "data", ")", ")", ")", ")", "\n", "", "for", "k", ",", "v", "in", "correct_most_frequence", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "\"accuracy of using  most frequent strategy in the top{} text is {:.2f}\"", ".", "format", "(", "k", ",", "100", "*", "v", "/", "(", "len", "(", "data", ")", ")", ")", ")", "\n", "", "if", "args", ".", "prediction_save_path", ":", "\n", "        ", "dump", "(", "data", ",", "args", ".", "prediction_save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load": [[2, 6], ["open", "json.load"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.load"], ["# Copyleft 2019 Project LXRT", "\n", "\n", "import", "sys", "\n", "import", "csv", "\n", "import", "base64", "\n"]], "home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump": [[7, 11], ["open", "json.dump"], "function", ["home.repos.pwc.inspect_result.luomancs_retriever_reader_for_okvqa.evaluation.utils.dump"], ["import", "time", "\n", "\n", "import", "numpy", "as", "np", "\n", "\n", "csv", ".", "field_size_limit", "(", "sys", ".", "maxsize", ")", "\n"]]}