{"home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding.__init__": [[112, 114], ["torch.nn.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "self", ",", "num_embeddings", ":", "int", ",", "embedding_dim", ":", "int", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding.forward": [[115, 122], ["torch.arange", "torch.arange", "torch.arange", "torch.arange", "super().forward"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.forward"], ["", "def", "forward", "(", "self", ",", "input_ids_shape", ":", "torch", ".", "Size", ",", "past_key_values_length", ":", "int", "=", "0", ")", ":", "\n", "        ", "\"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"", "\n", "bsz", ",", "seq_len", "=", "input_ids_shape", "[", ":", "2", "]", "\n", "positions", "=", "torch", ".", "arange", "(", "\n", "past_key_values_length", ",", "past_key_values_length", "+", "seq_len", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "weight", ".", "device", "\n", ")", "\n", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention.__init__": [[128, 151], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ":", "int", ",", "\n", "num_heads", ":", "int", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "is_decoder", ":", "bool", "=", "False", ",", "\n", "bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "(", "\n", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", "\n", ")", ",", "f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "self", ".", "is_decoder", "=", "is_decoder", "\n", "\n", "self", ".", "k_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "v_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "q_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape": [[152, 154], ["tensor.view().transpose().contiguous", "tensor.view().transpose", "tensor.view"], "methods", ["None"], ["", "def", "_shape", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "seq_len", ":", "int", ",", "bsz", ":", "int", ")", ":", "\n", "        ", "return", "tensor", ".", "view", "(", "bsz", ",", "seq_len", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention.forward": [[155, 260], ["hidden_states.size", "modeling_blenderbot_small.BlenderbotSmallAttention._shape().view", "modeling_blenderbot_small.BlenderbotSmallAttention.view", "modeling_blenderbot_small.BlenderbotSmallAttention.view", "modeling_blenderbot_small.BlenderbotSmallAttention.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "modeling_blenderbot_small.BlenderbotSmallAttention.view", "modeling_blenderbot_small.BlenderbotSmallAttention.transpose", "modeling_blenderbot_small.BlenderbotSmallAttention.reshape", "modeling_blenderbot_small.BlenderbotSmallAttention.out_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.q_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.transpose", "attn_weights.view.view.size", "ValueError", "attn_weights.view.view.view", "attn_weights.view.view.view", "attn_weights.view.view.view", "attn_weights_reshaped.view.view.view", "modeling_blenderbot_small.BlenderbotSmallAttention.size", "ValueError", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "attention_mask.size", "ValueError", "attn_weights.view.view.view", "layer_head_mask.size", "ValueError", "layer_head_mask.view", "attn_weights.view.view.view", "modeling_blenderbot_small.BlenderbotSmallAttention.k_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.v_proj", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "modeling_blenderbot_small.BlenderbotSmallAttention._shape", "modeling_blenderbot_small.BlenderbotSmallAttention.k_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.v_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.k_proj", "modeling_blenderbot_small.BlenderbotSmallAttention.v_proj", "attn_weights.view.view.size", "modeling_blenderbot_small.BlenderbotSmallAttention.size", "attention_mask.size", "layer_head_mask.size"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallAttention._shape"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ":", "torch", ".", "Tensor", ",", "\n", "key_value_states", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "past_key_value", ":", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "attention_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "layer_head_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Optional", "[", "torch", ".", "Tensor", "]", ",", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", "]", "]", "]", ":", "\n", "        ", "\"\"\"Input shape: Batch x Time x Channel\"\"\"", "\n", "\n", "# if key_value_states are provided this layer is used as a cross-attention layer", "\n", "# for the decoder", "\n", "is_cross_attention", "=", "key_value_states", "is", "not", "None", "\n", "bsz", ",", "tgt_len", ",", "embed_dim", "=", "hidden_states", ".", "size", "(", ")", "\n", "\n", "# get query proj", "\n", "query_states", "=", "self", ".", "q_proj", "(", "hidden_states", ")", "*", "self", ".", "scaling", "\n", "# get key, value proj", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_states", "=", "past_key_value", "[", "0", "]", "\n", "value_states", "=", "past_key_value", "[", "1", "]", "\n", "", "elif", "is_cross_attention", ":", "\n", "# cross_attentions", "\n", "            ", "key_states", "=", "self", ".", "_shape", "(", "self", ".", "k_proj", "(", "key_value_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "value_states", "=", "self", ".", "_shape", "(", "self", ".", "v_proj", "(", "key_value_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k, v, self_attention", "\n", "            ", "key_states", "=", "self", ".", "_shape", "(", "self", ".", "k_proj", "(", "hidden_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "value_states", "=", "self", ".", "_shape", "(", "self", ".", "v_proj", "(", "hidden_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "key_states", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_states", "]", ",", "dim", "=", "2", ")", "\n", "value_states", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_states", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "# self_attention", "\n", "            ", "key_states", "=", "self", ".", "_shape", "(", "self", ".", "k_proj", "(", "hidden_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "value_states", "=", "self", ".", "_shape", "(", "self", ".", "v_proj", "(", "hidden_states", ")", ",", "-", "1", ",", "bsz", ")", "\n", "\n", "", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_states", ",", "value_states", ")", "\n", "\n", "", "proj_shape", "=", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "query_states", "=", "self", ".", "_shape", "(", "query_states", ",", "tgt_len", ",", "bsz", ")", ".", "view", "(", "*", "proj_shape", ")", "\n", "key_states", "=", "key_states", ".", "view", "(", "*", "proj_shape", ")", "\n", "value_states", "=", "value_states", ".", "view", "(", "*", "proj_shape", ")", "\n", "\n", "src_len", "=", "key_states", ".", "size", "(", "1", ")", "\n", "attn_weights", "=", "torch", ".", "bmm", "(", "query_states", ",", "key_states", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "if", "attn_weights", ".", "size", "(", ")", "!=", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"", "\n", ")", "\n", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "if", "attention_mask", ".", "size", "(", ")", "!=", "(", "bsz", ",", "1", ",", "tgt_len", ",", "src_len", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"", "\n", ")", "\n", "", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "+", "attention_mask", "\n", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "attn_weights", "=", "nn", ".", "functional", ".", "softmax", "(", "attn_weights", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "layer_head_mask", "is", "not", "None", ":", "\n", "            ", "if", "layer_head_mask", ".", "size", "(", ")", "!=", "(", "self", ".", "num_heads", ",", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"", "\n", ")", "\n", "", "attn_weights", "=", "layer_head_mask", ".", "view", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "*", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "if", "output_attentions", ":", "\n", "# this operation is a bit awkward, but it's required to", "\n", "# make sure that attn_weights keeps its gradient.", "\n", "# In order to do so, attn_weights have to be reshaped", "\n", "# twice and have to be reused in the following", "\n", "            ", "attn_weights_reshaped", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights_reshaped", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "", "else", ":", "\n", "            ", "attn_weights_reshaped", "=", "None", "\n", "\n", "", "attn_probs", "=", "nn", ".", "functional", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn_output", "=", "torch", ".", "bmm", "(", "attn_probs", ",", "value_states", ")", "\n", "\n", "if", "attn_output", ".", "size", "(", ")", "!=", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"", "\n", ")", "\n", "\n", "", "attn_output", "=", "attn_output", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", ")", "\n", "attn_output", "=", "attn_output", ".", "transpose", "(", "1", ",", "2", ")", "\n", "attn_output", "=", "attn_output", ".", "reshape", "(", "bsz", ",", "tgt_len", ",", "embed_dim", ")", "\n", "\n", "attn_output", "=", "self", ".", "out_proj", "(", "attn_output", ")", "\n", "\n", "return", "attn_output", ",", "attn_weights_reshaped", ",", "past_key_value", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallEncoderLayer.__init__": [[264, 279], ["torch.nn.Module.__init__", "modeling_blenderbot_small.BlenderbotSmallAttention", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "config", ".", "d_model", "\n", "self", ".", "self_attn", "=", "BlenderbotSmallAttention", "(", "\n", "embed_dim", "=", "self", ".", "embed_dim", ",", "\n", "num_heads", "=", "config", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "config", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "self_attn_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "dropout", "=", "config", ".", "dropout", "\n", "self", ".", "activation_fn", "=", "ACT2FN", "[", "config", ".", "activation_function", "]", "\n", "self", ".", "activation_dropout", "=", "config", ".", "activation_dropout", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "config", ".", "encoder_ffn_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "config", ".", "encoder_ffn_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "final_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallEncoderLayer.forward": [[280, 329], ["modeling_blenderbot_small.BlenderbotSmallEncoderLayer.self_attn", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer.self_attn_layer_norm", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer.activation_fn", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer.fc2", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer.final_layer_norm", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer.fc1", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.isinf().any", "torch.isinf().any", "torch.isinf().any", "torch.isinf().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.finfo", "torch.finfo", "torch.finfo", "torch.finfo", "torch.isinf", "torch.isinf", "torch.isinf", "torch.isinf", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ":", "torch", ".", "Tensor", ",", "\n", "attention_mask", ":", "torch", ".", "Tensor", ",", "\n", "layer_head_mask", ":", "torch", ".", "Tensor", ",", "\n", "output_attentions", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            output_attentions (:obj:`bool`, `optional`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more detail.\n        \"\"\"", "\n", "residual", "=", "hidden_states", "\n", "hidden_states", ",", "attn_weights", ",", "_", "=", "self", ".", "self_attn", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "residual", "+", "hidden_states", "\n", "hidden_states", "=", "self", ".", "self_attn_layer_norm", "(", "hidden_states", ")", "\n", "\n", "residual", "=", "hidden_states", "\n", "hidden_states", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "hidden_states", ")", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "self", ".", "fc2", "(", "hidden_states", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "residual", "+", "hidden_states", "\n", "hidden_states", "=", "self", ".", "final_layer_norm", "(", "hidden_states", ")", "\n", "\n", "if", "hidden_states", ".", "dtype", "==", "torch", ".", "float16", "and", "(", "\n", "torch", ".", "isinf", "(", "hidden_states", ")", ".", "any", "(", ")", "or", "torch", ".", "isnan", "(", "hidden_states", ")", ".", "any", "(", ")", "\n", ")", ":", "\n", "            ", "clamp_value", "=", "torch", ".", "finfo", "(", "hidden_states", ".", "dtype", ")", ".", "max", "-", "1000", "\n", "hidden_states", "=", "torch", ".", "clamp", "(", "hidden_states", ",", "min", "=", "-", "clamp_value", ",", "max", "=", "clamp_value", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "            ", "outputs", "+=", "(", "attn_weights", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoderLayer.__init__": [[333, 358], ["torch.nn.Module.__init__", "modeling_blenderbot_small.BlenderbotSmallAttention", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "modeling_blenderbot_small.BlenderbotSmallAttention", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "config", ".", "d_model", "\n", "\n", "self", ".", "self_attn", "=", "BlenderbotSmallAttention", "(", "\n", "embed_dim", "=", "self", ".", "embed_dim", ",", "\n", "num_heads", "=", "config", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "config", ".", "attention_dropout", ",", "\n", "is_decoder", "=", "True", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "config", ".", "dropout", "\n", "self", ".", "activation_fn", "=", "ACT2FN", "[", "config", ".", "activation_function", "]", "\n", "self", ".", "activation_dropout", "=", "config", ".", "activation_dropout", "\n", "\n", "self", ".", "self_attn_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "encoder_attn", "=", "BlenderbotSmallAttention", "(", "\n", "self", ".", "embed_dim", ",", "\n", "config", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "config", ".", "attention_dropout", ",", "\n", "is_decoder", "=", "True", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "config", ".", "decoder_ffn_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "config", ".", "decoder_ffn_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "final_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoderLayer.forward": [[359, 446], ["modeling_blenderbot_small.BlenderbotSmallDecoderLayer.self_attn", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.self_attn_layer_norm", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.activation_fn", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.fc2", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.final_layer_norm", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.encoder_attn", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.encoder_attn_layer_norm", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer.fc1"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ":", "torch", ".", "Tensor", ",", "\n", "attention_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "encoder_hidden_states", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "encoder_attention_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "layer_head_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "cross_attn_layer_head_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "past_key_value", ":", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "output_attentions", ":", "Optional", "[", "bool", "]", "=", "False", ",", "\n", "use_cache", ":", "Optional", "[", "bool", "]", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            encoder_hidden_states (:obj:`torch.FloatTensor`): cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask (:obj:`torch.FloatTensor`): encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            cross_attn_layer_head_mask (:obj:`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                size `(decoder_attention_heads,)`.\n            past_key_value (:obj:`Tuple(torch.FloatTensor)`): cached past key and value projection states\n            output_attentions (:obj:`bool`, `optional`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more detail.\n        \"\"\"", "\n", "residual", "=", "hidden_states", "\n", "\n", "# Self Attention", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "# add present self-attn cache to positions 1,2 of present_key_value tuple", "\n", "hidden_states", ",", "self_attn_weights", ",", "present_key_value", "=", "self", ".", "self_attn", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "layer_head_mask", "=", "layer_head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "residual", "+", "hidden_states", "\n", "hidden_states", "=", "self", ".", "self_attn_layer_norm", "(", "hidden_states", ")", "\n", "\n", "# Cross-Attention Block", "\n", "cross_attn_present_key_value", "=", "None", "\n", "cross_attn_weights", "=", "None", "\n", "if", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "residual", "=", "hidden_states", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "hidden_states", ",", "cross_attn_weights", ",", "cross_attn_present_key_value", "=", "self", ".", "encoder_attn", "(", "\n", "hidden_states", "=", "hidden_states", ",", "\n", "key_value_states", "=", "encoder_hidden_states", ",", "\n", "attention_mask", "=", "encoder_attention_mask", ",", "\n", "layer_head_mask", "=", "cross_attn_layer_head_mask", ",", "\n", "past_key_value", "=", "cross_attn_past_key_value", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "residual", "+", "hidden_states", "\n", "hidden_states", "=", "self", ".", "encoder_attn_layer_norm", "(", "hidden_states", ")", "\n", "\n", "# add cross-attn to positions 3,4 of present_key_value tuple", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "# Fully Connected", "\n", "", "residual", "=", "hidden_states", "\n", "hidden_states", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "hidden_states", ")", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "self", ".", "fc2", "(", "hidden_states", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "hidden_states", "=", "residual", "+", "hidden_states", "\n", "hidden_states", "=", "self", ".", "final_layer_norm", "(", "hidden_states", ")", "\n", "\n", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "            ", "outputs", "+=", "(", "self_attn_weights", ",", "cross_attn_weights", ")", "\n", "\n", "", "if", "use_cache", ":", "\n", "            ", "outputs", "+=", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallPreTrainedModel._init_weights": [[452, 462], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "module.weight.data[].zero_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "std", "=", "self", ".", "config", ".", "init_std", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "std", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "std", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallPreTrainedModel.dummy_inputs": [[463, 473], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.ne", "torch.tensor.ne"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "dummy_inputs", "(", "self", ")", ":", "\n", "        ", "pad_token", "=", "self", ".", "config", ".", "pad_token_id", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "[", "[", "0", ",", "6", ",", "10", ",", "4", ",", "2", "]", ",", "[", "0", ",", "8", ",", "12", ",", "2", ",", "pad_token", "]", "]", ",", "device", "=", "self", ".", "device", ")", "\n", "dummy_inputs", "=", "{", "\n", "\"attention_mask\"", ":", "input_ids", ".", "ne", "(", "pad_token", ")", ",", "\n", "\"input_ids\"", ":", "input_ids", ",", "\n", "\"decoder_input_ids\"", ":", "input_ids", ",", "\n", "}", "\n", "return", "dummy_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallEncoder.__init__": [[602, 626], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "modeling_blenderbot_small.BlenderbotSmallEncoder.init_weights", "math.sqrt", "torch.nn.Embedding", "torch.nn.Embedding", "modeling_blenderbot_small.BlenderbotSmallEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ",", "embed_tokens", ":", "Optional", "[", "nn", ".", "Embedding", "]", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "dropout", "=", "config", ".", "dropout", "\n", "self", ".", "layerdrop", "=", "config", ".", "encoder_layerdrop", "\n", "\n", "embed_dim", "=", "config", ".", "d_model", "\n", "self", ".", "padding_idx", "=", "config", ".", "pad_token_id", "\n", "self", ".", "max_source_positions", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "if", "config", ".", "scale_embedding", "else", "1.0", "\n", "\n", "if", "embed_tokens", "is", "not", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "BlenderbotSmallLearnedPositionalEmbedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "\n", "embed_dim", ",", "\n", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "BlenderbotSmallEncoderLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "encoder_layers", ")", "]", ")", "\n", "self", ".", "layernorm_embedding", "=", "nn", ".", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallEncoder.forward": [[627, 749], ["modeling_blenderbot_small.BlenderbotSmallEncoder.embed_positions", "modeling_blenderbot_small.BlenderbotSmallEncoder.layernorm_embedding", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "enumerate", "transformers.modeling_outputs.BaseModelOutput", "ValueError", "modeling_blenderbot_small._expand_mask", "random.uniform", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "modeling_blenderbot_small.BlenderbotSmallEncoder.embed_tokens", "len", "ValueError", "head_mask.size", "len", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "encoder_layer", "inputs_embeds.size", "head_mask.size", "modeling_blenderbot_small.BlenderbotSmallEncoder.forward.create_custom_forward"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._expand_mask"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Args:\n            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n                Indices can be obtained using :class:`~transformers.BlenderbotSmallTokenizer`. See\n                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n                for details.\n                `What are input IDs? <../glossary.html#input-ids>`__\n            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            head_mask (:obj:`torch.Tensor` of shape :obj:`(encoder_layers, encoder_attention_heads)`, `optional`):\n                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n                into associated vectors than the model's internal embedding lookup matrix.\n            output_attentions (:obj:`bool`, `optional`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more detail.\n            output_hidden_states (:obj:`bool`, `optional`):\n                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n                for more detail.\n            return_dict (:obj:`bool`, `optional`):\n                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# retrieve input_ids and inputs_embeds", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "embed_tokens", "(", "input_ids", ")", "*", "self", ".", "embed_scale", "\n", "\n", "", "embed_pos", "=", "self", ".", "embed_positions", "(", "input_shape", ")", "\n", "\n", "hidden_states", "=", "inputs_embeds", "+", "embed_pos", "\n", "hidden_states", "=", "self", ".", "layernorm_embedding", "(", "hidden_states", ")", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# expand attention_mask", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]", "\n", "            ", "attention_mask", "=", "_expand_mask", "(", "attention_mask", ",", "inputs_embeds", ".", "dtype", ")", "\n", "\n", "", "encoder_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "\n", "# check if head_mask has a correct number of layers specified if desired", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "assert", "head_mask", ".", "size", "(", ")", "[", "0", "]", "==", "(", "\n", "len", "(", "self", ".", "layers", ")", "\n", ")", ",", "f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"", "\n", "", "for", "idx", ",", "encoder_layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "encoder_states", "=", "encoder_states", "+", "(", "hidden_states", ",", ")", "\n", "# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)", "\n", "", "dropout_probability", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "self", ".", "training", "and", "(", "dropout_probability", "<", "self", ".", "layerdrop", ")", ":", "# skip the layer", "\n", "                ", "layer_outputs", "=", "(", "None", ",", "None", ")", "\n", "", "else", ":", "\n", "                ", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                    ", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                        ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                            ", "return", "module", "(", "*", "inputs", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "encoder_layer", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "(", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "layer_outputs", "=", "encoder_layer", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", "=", "(", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ")", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "", "if", "output_hidden_states", ":", "\n", "            ", "encoder_states", "=", "encoder_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "v", "for", "v", "in", "[", "hidden_states", ",", "encoder_states", ",", "all_attentions", "]", "if", "v", "is", "not", "None", ")", "\n", "", "return", "BaseModelOutput", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "hidden_states", "=", "encoder_states", ",", "attentions", "=", "all_attentions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder.__init__": [[761, 782], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_blenderbot_small.BlenderbotSmallLearnedPositionalEmbedding", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "modeling_blenderbot_small.BlenderbotSmallDecoder.init_weights", "math.sqrt", "torch.nn.Embedding", "torch.nn.Embedding", "modeling_blenderbot_small.BlenderbotSmallDecoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ",", "embed_tokens", ":", "Optional", "[", "nn", ".", "Embedding", "]", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "dropout", "=", "config", ".", "dropout", "\n", "self", ".", "layerdrop", "=", "config", ".", "decoder_layerdrop", "\n", "self", ".", "padding_idx", "=", "config", ".", "pad_token_id", "\n", "self", ".", "max_target_positions", "=", "config", ".", "max_position_embeddings", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "config", ".", "d_model", ")", "if", "config", ".", "scale_embedding", "else", "1.0", "\n", "\n", "if", "embed_tokens", "is", "not", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "d_model", ",", "self", ".", "padding_idx", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "BlenderbotSmallLearnedPositionalEmbedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "\n", "config", ".", "d_model", ",", "\n", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "BlenderbotSmallDecoderLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "decoder_layers", ")", "]", ")", "\n", "self", ".", "layernorm_embedding", "=", "nn", ".", "LayerNorm", "(", "config", ".", "d_model", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder.get_input_embeddings": [[783, 785], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder.set_input_embeddings": [[786, 788], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embed_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder._prepare_decoder_attention_mask": [[790, 807], ["_make_causal_mask().to", "modeling_blenderbot_small._expand_mask", "modeling_blenderbot_small._make_causal_mask"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._expand_mask", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._make_causal_mask"], ["", "def", "_prepare_decoder_attention_mask", "(", "self", ",", "attention_mask", ",", "input_shape", ",", "inputs_embeds", ",", "past_key_values_length", ")", ":", "\n", "# create causal mask", "\n", "# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]", "\n", "        ", "combined_attention_mask", "=", "None", "\n", "if", "input_shape", "[", "-", "1", "]", ">", "1", ":", "\n", "            ", "combined_attention_mask", "=", "_make_causal_mask", "(", "\n", "input_shape", ",", "inputs_embeds", ".", "dtype", ",", "past_key_values_length", "=", "past_key_values_length", "\n", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "attention_mask", "is", "not", "None", ":", "\n", "# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]", "\n", "            ", "expanded_attn_mask", "=", "_expand_mask", "(", "attention_mask", ",", "inputs_embeds", ".", "dtype", ",", "tgt_len", "=", "input_shape", "[", "-", "1", "]", ")", "\n", "combined_attention_mask", "=", "(", "\n", "expanded_attn_mask", "if", "combined_attention_mask", "is", "None", "else", "expanded_attn_mask", "+", "combined_attention_mask", "\n", ")", "\n", "\n", "", "return", "combined_attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder.forward": [[808, 1012], ["modeling_blenderbot_small.BlenderbotSmallDecoder._prepare_decoder_attention_mask", "modeling_blenderbot_small.BlenderbotSmallDecoder.embed_positions", "modeling_blenderbot_small.BlenderbotSmallDecoder.layernorm_embedding", "torch.nn.functional.dropout", "torch.nn.functional.dropout", "zip", "enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "ValueError", "modeling_blenderbot_small._expand_mask", "random.uniform", "tuple", "input_ids.view.view.size", "input_ids.view.view.view", "modeling_blenderbot_small.BlenderbotSmallDecoder.embed_tokens", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "decoder_layer", "ValueError", "len", "logger.warning", "modeling_blenderbot_small.BlenderbotSmallDecoder.forward.create_custom_forward"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoder._prepare_decoder_attention_mask", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._expand_mask"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Args:\n            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n                Indices can be obtained using :class:`~transformers.BlenderbotSmallTokenizer`. See\n                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n                for details.\n                `What are input IDs? <../glossary.html#input-ids>`__\n            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                of the decoder.\n            encoder_attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n                selected in ``[0, 1]``:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            head_mask (:obj:`torch.Tensor` of shape :obj:`(decoder_layers, decoder_attention_heads)`, `optional`):\n                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            cross_attn_head_mask (:obj:`torch.Tensor` of shape :obj:`(decoder_layers, decoder_attention_heads)`, `optional`):\n                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n                cross-attention on hidden heads. Mask values selected in ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n                Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2\n                tensors of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional\n                tensors of shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential\n                decoding.\n                If :obj:`past_key_values` are used, the user can optionally input only the last\n                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n                sequence_length)`.\n            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n                into associated vectors than the model's internal embedding lookup matrix.\n            output_attentions (:obj:`bool`, `optional`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more detail.\n            output_hidden_states (:obj:`bool`, `optional`):\n                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n                for more detail.\n            return_dict (:obj:`bool`, `optional`):\n                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# retrieve input_ids and inputs_embeds", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either decoder_input_ids or decoder_inputs_embeds\"", ")", "\n", "\n", "# past_key_values_length", "\n", "", "past_key_values_length", "=", "past_key_values", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "2", "]", "if", "past_key_values", "is", "not", "None", "else", "0", "\n", "\n", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "embed_tokens", "(", "input_ids", ")", "*", "self", ".", "embed_scale", "\n", "\n", "", "attention_mask", "=", "self", ".", "_prepare_decoder_attention_mask", "(", "\n", "attention_mask", ",", "input_shape", ",", "inputs_embeds", ",", "past_key_values_length", "\n", ")", "\n", "\n", "# expand encoder attention mask", "\n", "if", "encoder_hidden_states", "is", "not", "None", "and", "encoder_attention_mask", "is", "not", "None", ":", "\n", "# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]", "\n", "            ", "encoder_attention_mask", "=", "_expand_mask", "(", "encoder_attention_mask", ",", "inputs_embeds", ".", "dtype", ",", "tgt_len", "=", "input_shape", "[", "-", "1", "]", ")", "\n", "\n", "# embed positions", "\n", "", "positions", "=", "self", ".", "embed_positions", "(", "input_shape", ",", "past_key_values_length", ")", "\n", "\n", "# BlenderbotSmall applies layer norm on hidden_states", "\n", "inputs_embeds", "=", "self", ".", "layernorm_embedding", "(", "inputs_embeds", ")", "\n", "hidden_states", "=", "inputs_embeds", "+", "positions", "\n", "\n", "hidden_states", "=", "nn", ".", "functional", ".", "dropout", "(", "hidden_states", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# decoder layers", "\n", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attns", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "(", "output_attentions", "and", "encoder_hidden_states", "is", "not", "None", ")", "else", "None", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "\n", "# check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired", "\n", "for", "attn_mask", ",", "mask_name", "in", "zip", "(", "[", "head_mask", ",", "cross_attn_head_mask", "]", ",", "[", "\"head_mask\"", ",", "\"cross_attn_head_mask\"", "]", ")", ":", "\n", "            ", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "assert", "attn_mask", ".", "size", "(", ")", "[", "0", "]", "==", "(", "\n", "len", "(", "self", ".", "layers", ")", "\n", ")", ",", "f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"", "\n", "", "", "for", "idx", ",", "decoder_layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "+=", "(", "hidden_states", ",", ")", "\n", "", "dropout_probability", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "self", ".", "training", "and", "(", "dropout_probability", "<", "self", ".", "layerdrop", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "past_key_value", "=", "past_key_values", "[", "idx", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", "and", "self", ".", "training", ":", "\n", "\n", "                ", "if", "use_cache", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"", "\n", "\"`use_cache=False`...\"", "\n", ")", "\n", "use_cache", "=", "False", "\n", "\n", "", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "# None for past_key_value", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "output_attentions", ",", "use_cache", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "decoder_layer", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ",", "\n", "cross_attn_head_mask", "[", "idx", "]", "if", "cross_attn_head_mask", "is", "not", "None", "else", "None", ",", "\n", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "\n", "                ", "layer_outputs", "=", "decoder_layer", "(", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_attention_mask", ",", "\n", "layer_head_mask", "=", "(", "head_mask", "[", "idx", "]", "if", "head_mask", "is", "not", "None", "else", "None", ")", ",", "\n", "cross_attn_layer_head_mask", "=", "(", "\n", "cross_attn_head_mask", "[", "idx", "]", "if", "cross_attn_head_mask", "is", "not", "None", "else", "None", "\n", ")", ",", "\n", "past_key_value", "=", "past_key_value", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", ")", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "3", "if", "output_attentions", "else", "1", "]", ",", ")", "\n", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attns", "+=", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "if", "encoder_hidden_states", "is", "not", "None", ":", "\n", "                    ", "all_cross_attentions", "+=", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "# add hidden states from the last decoder layer", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "+=", "(", "hidden_states", ",", ")", "\n", "\n", "", "next_cache", "=", "next_decoder_cache", "if", "use_cache", "else", "None", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "hidden_states", ",", "next_cache", ",", "all_hidden_states", ",", "all_self_attns", ",", "all_cross_attentions", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attns", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.__init__": [[1020, 1030], ["transformers.modeling_utils.PreTrainedModel.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "modeling_blenderbot_small.BlenderbotSmallEncoder", "modeling_blenderbot_small.BlenderbotSmallDecoder", "modeling_blenderbot_small.BlenderbotSmallModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "padding_idx", ",", "vocab_size", "=", "config", ".", "pad_token_id", ",", "config", ".", "vocab_size", "\n", "self", ".", "shared", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "config", ".", "d_model", ",", "padding_idx", ")", "\n", "\n", "self", ".", "encoder", "=", "BlenderbotSmallEncoder", "(", "config", ",", "self", ".", "shared", ")", "\n", "self", ".", "decoder", "=", "BlenderbotSmallDecoder", "(", "config", ",", "self", ".", "shared", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.get_input_embeddings": [[1031, 1033], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "shared", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.set_input_embeddings": [[1034, 1038], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "shared", "=", "value", "\n", "self", ".", "encoder", ".", "embed_tokens", "=", "self", ".", "shared", "\n", "self", ".", "decoder", ".", "embed_tokens", "=", "self", ".", "shared", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.get_encoder": [[1039, 1041], ["None"], "methods", ["None"], ["", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.get_decoder": [[1042, 1044], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallModel.forward": [[1045, 1129], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "modeling_blenderbot_small.BlenderbotSmallModel.decoder", "transformers.modeling_outputs.Seq2SeqModelOutput", "modeling_blenderbot_small.BlenderbotSmallModel.encoder", "transformers.modeling_outputs.BaseModelOutput", "isinstance", "len", "len"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "BLENDERBOT_SMALL_INPUTS_DOCSTRING", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "Seq2SeqModelOutput", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Returns:\n        Example::\n            >>> from transformers import BlenderbotSmallTokenizer, BlenderbotSmallModel\n            >>> model = BlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n            >>> tokenizer = BlenderbotSmallTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n            >>> input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n            >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "use_cache", "=", "use_cache", "if", "use_cache", "is", "not", "None", "else", "self", ".", "config", ".", "use_cache", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "encoder_outputs", "is", "None", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True", "\n", "", "elif", "return_dict", "and", "not", "isinstance", "(", "encoder_outputs", ",", "BaseModelOutput", ")", ":", "\n", "            ", "encoder_outputs", "=", "BaseModelOutput", "(", "\n", "last_hidden_state", "=", "encoder_outputs", "[", "0", "]", ",", "\n", "hidden_states", "=", "encoder_outputs", "[", "1", "]", "if", "len", "(", "encoder_outputs", ")", ">", "1", "else", "None", ",", "\n", "attentions", "=", "encoder_outputs", "[", "2", "]", "if", "len", "(", "encoder_outputs", ")", ">", "2", "else", "None", ",", "\n", ")", "\n", "\n", "# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)", "\n", "", "decoder_outputs", "=", "self", ".", "decoder", "(", "\n", "input_ids", "=", "decoder_input_ids", ",", "\n", "attention_mask", "=", "decoder_attention_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_outputs", "[", "0", "]", ",", "\n", "encoder_attention_mask", "=", "attention_mask", ",", "\n", "head_mask", "=", "decoder_head_mask", ",", "\n", "cross_attn_head_mask", "=", "cross_attn_head_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "decoder_outputs", "+", "encoder_outputs", "\n", "\n", "", "return", "Seq2SeqModelOutput", "(", "\n", "last_hidden_state", "=", "decoder_outputs", ".", "last_hidden_state", ",", "\n", "past_key_values", "=", "decoder_outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "decoder_outputs", ".", "hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "decoder_outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "encoder_outputs", ".", "last_hidden_state", ",", "\n", "encoder_hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "encoder_attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.__init__": [[1145, 1152], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_blenderbot_small.BlenderbotSmallModel", "modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.register_buffer", "torch.nn.Linear", "torch.nn.Linear", "modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.init_weights", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ":", "BlenderbotSmallConfig", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model", "=", "BlenderbotSmallModel", "(", "config", ")", "\n", "self", ".", "register_buffer", "(", "\"final_logits_bias\"", ",", "torch", ".", "zeros", "(", "(", "1", ",", "self", ".", "model", ".", "shared", ".", "num_embeddings", ")", ")", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "self", ".", "model", ".", "shared", ".", "num_embeddings", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_encoder": [[1153, 1155], ["modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.model.get_encoder"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_encoder"], ["", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "get_encoder", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_decoder": [[1156, 1158], ["modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.model.get_decoder"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_decoder"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "get_decoder", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.resize_token_embeddings": [[1159, 1163], ["super().resize_token_embeddings", "modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._resize_final_logits_bias"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.resize_token_embeddings", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._resize_final_logits_bias"], ["", "def", "resize_token_embeddings", "(", "self", ",", "new_num_tokens", ":", "int", ")", "->", "nn", ".", "Embedding", ":", "\n", "        ", "new_embeddings", "=", "super", "(", ")", ".", "resize_token_embeddings", "(", "new_num_tokens", ")", "\n", "self", ".", "_resize_final_logits_bias", "(", "new_num_tokens", ")", "\n", "return", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._resize_final_logits_bias": [[1164, 1172], ["modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "_resize_final_logits_bias", "(", "self", ",", "new_num_tokens", ":", "int", ")", "->", "None", ":", "\n", "        ", "old_num_tokens", "=", "self", ".", "final_logits_bias", ".", "shape", "[", "-", "1", "]", "\n", "if", "new_num_tokens", "<=", "old_num_tokens", ":", "\n", "            ", "new_bias", "=", "self", ".", "final_logits_bias", "[", ":", ",", ":", "new_num_tokens", "]", "\n", "", "else", ":", "\n", "            ", "extra_bias", "=", "torch", ".", "zeros", "(", "(", "1", ",", "new_num_tokens", "-", "old_num_tokens", ")", ",", "device", "=", "self", ".", "final_logits_bias", ".", "device", ")", "\n", "new_bias", "=", "torch", ".", "cat", "(", "[", "self", ".", "final_logits_bias", ",", "extra_bias", "]", ",", "dim", "=", "1", ")", "\n", "", "self", ".", "register_buffer", "(", "\"final_logits_bias\"", ",", "new_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.get_output_embeddings": [[1173, 1175], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.set_output_embeddings": [[1176, 1178], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.forward": [[1179, 1254], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "transformers.file_utils.add_end_docstrings", "modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.model", "transformers.modeling_outputs.Seq2SeqLMOutput", "modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.lm_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_blenderbot_small.shift_tokens_right", "lm_logits.view", "labels.view"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.shift_tokens_right"], ["", "@", "add_start_docstrings_to_model_forward", "(", "BLENDERBOT_SMALL_INPUTS_DOCSTRING", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "Seq2SeqLMOutput", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "@", "add_end_docstrings", "(", "BLENDERBOT_SMALL_GENERATION_EXAMPLE", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n        Returns:\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "decoder_input_ids", "is", "None", ":", "\n", "                ", "decoder_input_ids", "=", "shift_tokens_right", "(", "\n", "labels", ",", "self", ".", "config", ".", "pad_token_id", ",", "self", ".", "config", ".", "decoder_start_token_id", "\n", ")", "\n", "\n", "", "", "outputs", "=", "self", ".", "model", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "decoder_input_ids", "=", "decoder_input_ids", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "decoder_attention_mask", "=", "decoder_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "decoder_head_mask", "=", "decoder_head_mask", ",", "\n", "cross_attn_head_mask", "=", "cross_attn_head_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "decoder_inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "outputs", "[", "0", "]", ")", "+", "self", ".", "final_logits_bias", "\n", "\n", "masked_lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "lm_logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "return", "(", "(", "masked_lm_loss", ",", ")", "+", "output", ")", "if", "masked_lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "Seq2SeqLMOutput", "(", "\n", "loss", "=", "masked_lm_loss", ",", "\n", "logits", "=", "lm_logits", ",", "\n", "past_key_values", "=", "outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "outputs", ".", "decoder_hidden_states", ",", "\n", "decoder_attentions", "=", "outputs", ".", "decoder_attentions", ",", "\n", "cross_attentions", "=", "outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "outputs", ".", "encoder_last_hidden_state", ",", "\n", "encoder_hidden_states", "=", "outputs", ".", "encoder_hidden_states", ",", "\n", "encoder_attentions", "=", "outputs", ".", "encoder_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.prepare_inputs_for_generation": [[1256, 1282], ["None"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "\n", "self", ",", "\n", "decoder_input_ids", ",", "\n", "past", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "# cut decoder_input_ids if past is used", "\n", "        ", "if", "past", "is", "not", "None", ":", "\n", "            ", "decoder_input_ids", "=", "decoder_input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "", "return", "{", "\n", "\"input_ids\"", ":", "None", ",", "# encoder_outputs is defined. input_ids not needed", "\n", "\"encoder_outputs\"", ":", "encoder_outputs", ",", "\n", "\"past_key_values\"", ":", "past", ",", "\n", "\"decoder_input_ids\"", ":", "decoder_input_ids", ",", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"head_mask\"", ":", "head_mask", ",", "\n", "\"decoder_head_mask\"", ":", "decoder_head_mask", ",", "\n", "\"cross_attn_head_mask\"", ":", "cross_attn_head_mask", ",", "\n", "\"use_cache\"", ":", "use_cache", ",", "# change this to avoid caching (presumably for debugging)", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration._reorder_cache": [[1284, 1293], ["tuple", "past_state.index_select"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reorder_cache", "(", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "(", ")", "\n", "for", "layer_past", "in", "past", ":", "\n", "# cached cross_attention states don't have to be reordered -> they are always the same", "\n", "            ", "reordered_past", "+=", "(", "\n", "tuple", "(", "past_state", ".", "index_select", "(", "0", ",", "beam_idx", ")", "for", "past_state", "in", "layer_past", "[", ":", "2", "]", ")", "+", "layer_past", "[", "2", ":", "]", ",", "\n", ")", "\n", "", "return", "reordered_past", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoderWrapper.__init__": [[1302, 1305], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_blenderbot_small.BlenderbotSmallDecoder"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "decoder", "=", "BlenderbotSmallDecoder", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallDecoderWrapper.forward": [[1306, 1308], ["modeling_blenderbot_small.BlenderbotSmallDecoderWrapper.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.__init__": [[1312, 1322], ["transformers.modeling_utils.PreTrainedModel.__init__", "copy.deepcopy", "modeling_blenderbot_small.BlenderbotSmallDecoderWrapper", "torch.nn.Linear", "torch.nn.Linear", "modeling_blenderbot_small.BlenderbotSmallForCausalLM.init_weights"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config", ".", "is_decoder", "=", "True", "\n", "config", ".", "is_encoder_decoder", "=", "False", "\n", "self", ".", "model", "=", "BlenderbotSmallDecoderWrapper", "(", "config", ")", "\n", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_input_embeddings": [[1323, 1325], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "decoder", ".", "embed_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_input_embeddings": [[1326, 1328], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "model", ".", "decoder", ".", "embed_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_output_embeddings": [[1329, 1331], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_output_embeddings": [[1332, 1334], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "lm_head", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.set_decoder": [[1335, 1337], ["None"], "methods", ["None"], ["", "def", "set_decoder", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "self", ".", "model", ".", "decoder", "=", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.get_decoder": [[1338, 1340], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "model", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.forward": [[1341, 1467], ["transformers.file_utils.replace_return_docstrings", "modeling_blenderbot_small.BlenderbotSmallForCausalLM.model.decoder", "modeling_blenderbot_small.BlenderbotSmallForCausalLM.lm_head", "transformers.modeling_outputs.CausalLMOutputWithCrossAttentions", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_blenderbot_small.BlenderbotSmallForCausalLM.view", "labels.view"], "methods", ["None"], ["", "@", "replace_return_docstrings", "(", "output_type", "=", "CausalLMOutputWithCrossAttentions", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "cross_attn_head_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Args:\n            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n                Indices can be obtained using :class:`~transformers.BlenderbotSmallTokenizer`. See\n                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n                for details.\n                `What are input IDs? <../glossary.html#input-ids>`__\n            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                `What are attention masks? <../glossary.html#attention-mask>`__\n            encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                if the model is configured as a decoder.\n            encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n                in the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            head_mask (:obj:`torch.Tensor` of shape :obj:`(decoder_layers, decoder_attention_heads)`, `optional`):\n                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            cross_attn_head_mask (:obj:`torch.Tensor` of shape :obj:`(decoder_layers, decoder_attention_heads)`, `optional`):\n                Mask to nullify selected heads of the cross-attention modules. Mask values selected in ``[0, 1]``:\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n                Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2\n                tensors of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional\n                tensors of shape :obj:`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two\n                additional tensors are only required when the model is used as a decoder in a Sequence to Sequence\n                model.\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see :obj:`past_key_values` input) to speed up sequential\n                decoding.\n                If :obj:`past_key_values` are used, the user can optionally input only the last ``decoder_input_ids``\n                (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n                instead of all ``decoder_input_ids`` of shape :obj:`(batch_size, sequence_length)`.\n            labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n                config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are\n                ignored (masked), the loss is only computed for the tokens with labels in ``[0, ...,\n                config.vocab_size]``.\n            use_cache (:obj:`bool`, `optional`):\n                If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n                decoding (see :obj:`past_key_values`).\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n            output_attentions (:obj:`bool`, `optional`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more detail.\n            output_hidden_states (:obj:`bool`, `optional`):\n                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n                for more detail.\n            return_dict (:obj:`bool`, `optional`):\n                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n        Returns:\n        Example::\n            >>> from transformers import BlenderbotSmallTokenizer, BlenderbotSmallForCausalLM\n            >>> tokenizer = BlenderbotSmallTokenizer.from_pretrained('facebook/bart-large')\n            >>> model = BlenderbotSmallForCausalLM.from_pretrained('facebook/bart-large', add_cross_attention=False)\n            >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> last_hidden_states = outputs.last_hidden_state\n        \"\"\"", "\n", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)", "\n", "outputs", "=", "self", ".", "model", ".", "decoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "cross_attn_head_mask", "=", "cross_attn_head_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "logits", "=", "self", ".", "lm_head", "(", "outputs", "[", "0", "]", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "return", "(", "loss", ",", ")", "+", "output", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "CausalLMOutputWithCrossAttentions", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "past_key_values", "=", "outputs", ".", "past_key_values", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "outputs", ".", "cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM.prepare_inputs_for_generation": [[1469, 1482], ["input_ids.new_ones"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "past", "=", "None", ",", "attention_mask", "=", "None", ",", "use_cache", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "# if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "input_ids", ".", "new_ones", "(", "input_ids", ".", "shape", ")", "\n", "\n", "", "if", "past", ":", "\n", "            ", "input_ids", "=", "input_ids", "[", ":", ",", "-", "1", ":", "]", "\n", "# first step, decoder_cached_states are empty", "\n", "", "return", "{", "\n", "\"input_ids\"", ":", "input_ids", ",", "# encoder_outputs is defined. input_ids not needed", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"past_key_values\"", ":", "past", ",", "\n", "\"use_cache\"", ":", "use_cache", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForCausalLM._reorder_cache": [[1484, 1490], ["tuple", "past_state.index_select"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reorder_cache", "(", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "(", ")", "\n", "for", "layer_past", "in", "past", ":", "\n", "            ", "reordered_past", "+=", "(", "tuple", "(", "past_state", ".", "index_select", "(", "0", ",", "beam_idx", ")", "for", "past_state", "in", "layer_past", ")", ",", ")", "\n", "", "return", "reordered_past", "", "", "", ""]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.shift_tokens_right": [[60, 73], ["input_ids.new_zeros", "input_ids[].clone", "input_ids.new_zeros.masked_fill_"], "function", ["None"], ["def", "shift_tokens_right", "(", "input_ids", ":", "torch", ".", "Tensor", ",", "pad_token_id", ":", "int", ",", "decoder_start_token_id", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Shift input ids one token to the right.\n    \"\"\"", "\n", "shifted_input_ids", "=", "input_ids", ".", "new_zeros", "(", "input_ids", ".", "shape", ")", "\n", "shifted_input_ids", "[", ":", ",", "1", ":", "]", "=", "input_ids", "[", ":", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "shifted_input_ids", "[", ":", ",", "0", "]", "=", "decoder_start_token_id", "\n", "\n", "assert", "pad_token_id", "is", "not", "None", ",", "\"self.model.config.pad_token_id has to be defined.\"", "\n", "# replace possible -100 values in labels by `pad_token_id`", "\n", "shifted_input_ids", ".", "masked_fill_", "(", "shifted_input_ids", "==", "-", "100", ",", "pad_token_id", ")", "\n", "\n", "return", "shifted_input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._make_causal_mask": [[76, 89], ["torch.full", "torch.full", "torch.arange", "torch.arange", "torch.cat.masked_fill_", "torch.cat.to", "mask[].expand", "float", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat.size", "torch.zeros", "torch.zeros"], "function", ["None"], ["", "def", "_make_causal_mask", "(", "input_ids_shape", ":", "torch", ".", "Size", ",", "dtype", ":", "torch", ".", "dtype", ",", "past_key_values_length", ":", "int", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"", "\n", "bsz", ",", "tgt_len", "=", "input_ids_shape", "\n", "mask", "=", "torch", ".", "full", "(", "(", "tgt_len", ",", "tgt_len", ")", ",", "float", "(", "\"-inf\"", ")", ")", "\n", "mask_cond", "=", "torch", ".", "arange", "(", "mask", ".", "size", "(", "-", "1", ")", ")", "\n", "mask", ".", "masked_fill_", "(", "mask_cond", "<", "(", "mask_cond", "+", "1", ")", ".", "view", "(", "mask", ".", "size", "(", "-", "1", ")", ",", "1", ")", ",", "0", ")", "\n", "mask", "=", "mask", ".", "to", "(", "dtype", ")", "\n", "\n", "if", "past_key_values_length", ">", "0", ":", "\n", "        ", "mask", "=", "torch", ".", "cat", "(", "[", "torch", ".", "zeros", "(", "tgt_len", ",", "past_key_values_length", ",", "dtype", "=", "dtype", ")", ",", "mask", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "mask", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "expand", "(", "bsz", ",", "1", ",", "tgt_len", ",", "tgt_len", "+", "past_key_values_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small._expand_mask": [[92, 104], ["mask.size", "mask[].expand().to", "inverted_mask.masked_fill", "inverted_mask.bool", "mask[].expand", "torch.finfo", "torch.finfo"], "function", ["None"], ["", "def", "_expand_mask", "(", "mask", ":", "torch", ".", "Tensor", ",", "dtype", ":", "torch", ".", "dtype", ",", "tgt_len", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"", "\n", "bsz", ",", "src_len", "=", "mask", ".", "size", "(", ")", "\n", "tgt_len", "=", "tgt_len", "if", "tgt_len", "is", "not", "None", "else", "src_len", "\n", "\n", "expanded_mask", "=", "mask", "[", ":", ",", "None", ",", "None", ",", ":", "]", ".", "expand", "(", "bsz", ",", "1", ",", "tgt_len", ",", "src_len", ")", ".", "to", "(", "dtype", ")", "\n", "\n", "inverted_mask", "=", "1.0", "-", "expanded_mask", "\n", "\n", "return", "inverted_mask", ".", "masked_fill", "(", "inverted_mask", ".", "bool", "(", ")", ",", "torch", ".", "finfo", "(", "dtype", ")", ".", "min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.main.setup_seed": [[10, 16], ["torch.manual_seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["None"], ["def", "setup_seed", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.main.read_wizard_json": [[20, 36], ["open", "json.load", "data.append"], "function", ["None"], ["def", "read_wizard_json", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "        ", "tmp_source", "=", "''", "\n", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "if", "tmp_source", "!=", "''", ":", "\n", "                ", "data", ".", "append", "(", "[", "tmp_source", ",", "\"__start__ \"", "+", "utt", "+", "\" __end__\"", "]", ")", "\n", "# add split '\\t' for blender", "\n", "tmp_source", "=", "tmp_source", "+", "\"\\t\"", "+", "utt", "\n", "", "else", ":", "\n", "                ", "tmp_source", "=", "utt", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.main.read_wizard_definition": [[37, 61], ["open", "json.load", "[].lower", "utt.lower", "list", "re.sub", "data.append", "j.keys", "list", "j.keys"], "function", ["None"], ["", "def", "read_wizard_definition", "(", "file_path", ")", ":", "\n", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "# line.keys() ['chosen_topic', 'persona', 'wizard_eval', 'dialog', 'chosen_topic_passage']", "\n", "# dialog.keys() dict_keys(['speaker', 'text', 'candidate_responses', 'retrieved_passages', 'retrieved_topics'])", "\n", "        ", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "external_passage", "=", "i", "[", "'retrieved_passages'", "]", "\n", "for", "j", "in", "external_passage", ":", "\n", "                ", "if", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", ".", "lower", "(", ")", "in", "utt", ".", "lower", "(", ")", ":", "\n", "                    ", "know_key", "=", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "try", ":", "\n", "# retrieved knowledge is available", "\n", "# we do not use gold knowledge", "\n", "                        ", "utt_mask", "=", "re", ".", "sub", "(", "know_key", ",", "'[MASK]'", ",", "utt", ",", "flags", "=", "re", ".", "IGNORECASE", ")", "\n", "knowledge", "=", "(", "'\\t'", ")", ".", "join", "(", "j", "[", "know_key", "]", ")", "\n", "data", ".", "append", "(", "[", "utt_mask", ",", "\"__defi__ \"", "+", "knowledge", "+", "\" __end__\"", "]", ")", "\n", "", "except", ":", "\n", "                        ", "continue", "\n", "", "", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.main.read_wizard_concat_json": [[62, 90], ["open", "json.load", "[].lower", "utt.lower", "list", "data.append", "j.keys", "list", "j.keys"], "function", ["None"], ["", "def", "read_wizard_concat_json", "(", "file_path", ")", ":", "\n", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# print(file[0].keys())", "\n", "# print(file[0]['dialog'])", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "\n", "        ", "tmp_source", "=", "''", "\n", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "\n", "external_passage", "=", "i", "[", "'retrieved_passages'", "]", "\n", "for", "j", "in", "external_passage", ":", "\n", "# print(list(j.keys()))", "\n", "                ", "if", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", ".", "lower", "(", ")", "in", "utt", ".", "lower", "(", ")", ":", "\n", "# retrieved knowledge is available", "\n", "                    ", "know_key", "=", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "knowledge", "=", "(", "\" \"", ")", ".", "join", "(", "j", "[", "know_key", "]", ")", "\n", "if", "tmp_source", "!=", "''", ":", "\n", "                        ", "data", ".", "append", "(", "[", "tmp_source", "+", "\" \"", "+", "knowledge", ",", "\"__start__ \"", "+", "utt", "+", "\" __end__\"", "]", ")", "\n", "tmp_source", "=", "tmp_source", "+", "\"\\t\"", "+", "utt", "\n", "", "else", ":", "\n", "                        ", "tmp_source", "=", "utt", "\n", "", "break", "\n", "", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.main.read_hypernym": [[91, 99], ["open", "f.readlines", "line.strip().split", "data.append", "line.strip"], "function", ["None"], ["", "def", "read_hypernym", "(", "file_path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "file", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "line", "in", "file", ":", "\n", "        ", "source", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\001'", ")", "\n", "data", ".", "append", "(", "[", "source", ",", "\"__hype__ \"", "+", "target", "+", "\" __end__\"", "]", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.inference.read_wizard_json": [[7, 25], ["open", "json.load", "data.append"], "function", ["None"], ["def", "read_wizard_json", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# print(file[0].keys())", "\n", "# print(file[0]['dialog'])", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "        ", "tmp_source", "=", "''", "\n", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "if", "tmp_source", "!=", "''", ":", "\n", "                ", "data", ".", "append", "(", "[", "tmp_source", ",", "\"__start__ \"", "+", "utt", "+", "\" __end__\"", "]", ")", "\n", "# add split '\\n' for blender", "\n", "tmp_source", "=", "tmp_source", "+", "\"\\t\"", "+", "utt", "\n", "", "else", ":", "\n", "                ", "tmp_source", "=", "utt", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.__init__": [[71, 216], ["seq2seq_model.Seq2SeqModel._load_model_args", "isinstance", "seq2seq_model.Seq2SeqModel.args.update_from_dict", "isinstance", "kwargs.pop", "seq2seq_model.Seq2SeqModel.args.update_from_dict", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.is_available", "model_class.from_pretrained", "warnings.warn", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "tokenizer_class.from_pretrained", "seq2seq_model.Seq2SeqModel.model.resize_token_embeddings", "EncoderDecoderModel.from_encoder_decoder_pretrained", "model_class.from_pretrained", "transformers.BertForMaskedLM.from_pretrained", "tokenizer_class.from_pretrained", "transformers.BertTokenizer.from_pretrained", "EncoderDecoderModel.from_encoder_decoder_pretrained", "tokenizer_class.from_pretrained", "transformers.BertTokenizer.from_pretrained", "ValueError", "kwargs.pop.as_dict().items", "torch.device", "torch.device", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "tokenizer_class.from_pretrained", "tokenizer_class.from_pretrained", "kwargs.pop.as_dict"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._load_model_args", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.modeling_blenderbot_small.BlenderbotSmallForConditionalGeneration.resize_token_embeddings"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "encoder_type", "=", "None", ",", "\n", "encoder_name", "=", "None", ",", "\n", "decoder_name", "=", "None", ",", "\n", "encoder_decoder_type", "=", "None", ",", "\n", "encoder_decoder_name", "=", "None", ",", "\n", "config", "=", "None", ",", "\n", "args", "=", "None", ",", "\n", "use_cuda", "=", "True", ",", "\n", "cuda_device", "=", "-", "1", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Initializes a Seq2SeqModel.\n\n        Args:\n            encoder_type (optional): The type of model to use as the encoder.\n            encoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n            decoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n                                    Must be the same \"size\" as the encoder model (base/base, large/large, etc.)\n            encoder_decoder_type (optional): The type of encoder-decoder model. (E.g. bart)\n            encoder_decoder_name (optional): The path to a directory containing the saved encoder and decoder of a Seq2SeqModel. (E.g. \"outputs/\") OR a valid BART or MarianMT model.\n            config (optional): A configuration file to build an EncoderDecoderModel.\n            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "\n", "if", "not", "config", ":", "\n", "# if not ((encoder_name and decoder_name) or encoder_decoder_name) and not encoder_type:", "\n", "            ", "if", "not", "(", "(", "encoder_name", "and", "decoder_name", ")", "or", "encoder_decoder_name", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"You must specify a Seq2Seq config \\t OR \\t\"", "\n", "\"encoder_type, encoder_name, and decoder_name OR \\t \\t\"", "\n", "\"encoder_type and encoder_decoder_name\"", "\n", ")", "\n", "", "elif", "not", "(", "encoder_type", "or", "encoder_decoder_type", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"You must specify a Seq2Seq config \\t OR \\t\"", "\n", "\"encoder_type, encoder_name, and decoder_name \\t OR \\t\"", "\n", "\"encoder_type and encoder_decoder_name\"", "\n", ")", "\n", "\n", "", "", "self", ".", "args", "=", "self", ".", "_load_model_args", "(", "encoder_decoder_name", ")", "\n", "\n", "if", "isinstance", "(", "args", ",", "dict", ")", ":", "\n", "            ", "self", ".", "args", ".", "update_from_dict", "(", "args", ")", "\n", "", "elif", "isinstance", "(", "args", ",", "Seq2SeqArgs", ")", ":", "\n", "            ", "self", ".", "args", "=", "args", "\n", "\n", "", "if", "\"sweep_config\"", "in", "kwargs", ":", "\n", "            ", "sweep_config", "=", "kwargs", ".", "pop", "(", "\"sweep_config\"", ")", "\n", "sweep_values", "=", "{", "key", ":", "value", "[", "\"value\"", "]", "for", "key", ",", "value", "in", "sweep_config", ".", "as_dict", "(", ")", ".", "items", "(", ")", "if", "key", "!=", "\"_wandb\"", "}", "\n", "self", ".", "args", ".", "update_from_dict", "(", "sweep_values", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "manual_seed", ":", "\n", "            ", "random", ".", "seed", "(", "self", ".", "args", ".", "manual_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "args", ".", "manual_seed", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "args", ".", "manual_seed", ")", "\n", "if", "self", ".", "args", ".", "n_gpu", ">", "0", ":", "\n", "                ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "self", ".", "args", ".", "manual_seed", ")", "\n", "\n", "", "", "if", "use_cuda", ":", "\n", "            ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "if", "cuda_device", "==", "-", "1", ":", "\n", "                    ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "device", "=", "torch", ".", "device", "(", "f\"cuda:{cuda_device}\"", ")", "\n", "", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"'use_cuda' set to True when cuda is unavailable.\"", "\n", "\"Make sure CUDA is available or set `use_cuda=False`.\"", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "\"cpu\"", "\n", "\n", "", "self", ".", "results", "=", "{", "}", "\n", "\n", "if", "not", "use_cuda", ":", "\n", "            ", "self", ".", "args", ".", "fp16", "=", "False", "\n", "\n", "# config = EncoderDecoderConfig.from_encoder_decoder_configs(config, config)", "\n", "", "if", "encoder_decoder_type", ":", "\n", "            ", "config_class", ",", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "encoder_decoder_type", "]", "\n", "", "else", ":", "\n", "            ", "config_class", ",", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "encoder_type", "]", "\n", "\n", "", "if", "encoder_decoder_type", "in", "[", "\"bart\"", ",", "\"marian\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "            ", "self", ".", "model", "=", "model_class", ".", "from_pretrained", "(", "encoder_decoder_name", ")", "\n", "if", "encoder_decoder_type", "in", "[", "\"bart\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "# self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_decoder_name)", "\n", "                ", "self", ".", "encoder_tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "encoder_decoder_name", ",", "additional_special_tokens", "=", "[", "'__defi__'", ",", "'__hype__'", ",", "'[MASK]'", "]", ")", "\n", "self", ".", "model", ".", "resize_token_embeddings", "(", "len", "(", "self", ".", "encoder_tokenizer", ")", ")", "\n", "", "elif", "encoder_decoder_type", "==", "\"marian\"", ":", "\n", "                ", "if", "self", ".", "args", ".", "base_marian_model_name", ":", "\n", "                    ", "self", ".", "encoder_tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "self", ".", "args", ".", "base_marian_model_name", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "encoder_tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "encoder_decoder_name", ")", "\n", "", "", "self", ".", "decoder_tokenizer", "=", "self", ".", "encoder_tokenizer", "\n", "self", ".", "config", "=", "self", ".", "model", ".", "config", "\n", "", "else", ":", "\n", "            ", "if", "encoder_decoder_name", ":", "\n", "# self.model = EncoderDecoderModel.from_pretrained(encoder_decoder_name)", "\n", "                ", "self", ".", "model", "=", "EncoderDecoderModel", ".", "from_encoder_decoder_pretrained", "(", "\n", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"encoder\"", ")", ",", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"decoder\"", ")", "\n", ")", "\n", "self", ".", "model", ".", "encoder", "=", "model_class", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"encoder\"", ")", ")", "\n", "self", ".", "model", ".", "decoder", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"decoder\"", ")", ")", "\n", "self", ".", "encoder_tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"encoder\"", ")", ")", "\n", "self", ".", "decoder_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "os", ".", "path", ".", "join", "(", "encoder_decoder_name", ",", "\"decoder\"", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "model", "=", "EncoderDecoderModel", ".", "from_encoder_decoder_pretrained", "(", "\n", "encoder_name", ",", "decoder_name", ",", "config", "=", "config", "\n", ")", "\n", "self", ".", "encoder_tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "encoder_name", ")", "\n", "self", ".", "decoder_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "decoder_name", ")", "\n", "", "self", ".", "encoder_config", "=", "self", ".", "model", ".", "config", ".", "encoder", "\n", "self", ".", "decoder_config", "=", "self", ".", "model", ".", "config", ".", "decoder", "\n", "\n", "", "if", "self", ".", "args", ".", "wandb_project", "and", "not", "wandb_available", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"wandb_project specified but wandb is not available. Wandb disabled.\"", ")", "\n", "self", ".", "args", ".", "wandb_project", "=", "None", "\n", "\n", "", "if", "encoder_decoder_name", ":", "\n", "            ", "self", ".", "args", ".", "model_name", "=", "encoder_decoder_name", "\n", "\n", "# # Checking if we are loading from a saved model or using a pre-trained model", "\n", "# if not saved_model_args and encoder_decoder_type == \"marian\":", "\n", "# Need to store base pre-trained model name to get the tokenizer when loading a saved model", "\n", "self", ".", "args", ".", "base_marian_model_name", "=", "encoder_decoder_name", "\n", "\n", "", "elif", "encoder_name", "and", "decoder_name", ":", "\n", "            ", "self", ".", "args", ".", "model_name", "=", "encoder_name", "+", "\"-\"", "+", "decoder_name", "\n", "", "else", ":", "\n", "            ", "self", ".", "args", ".", "model_name", "=", "\"encoder-decoder\"", "\n", "\n", "", "if", "encoder_decoder_type", ":", "\n", "            ", "self", ".", "args", ".", "model_type", "=", "encoder_decoder_type", "\n", "", "elif", "encoder_type", ":", "\n", "            ", "self", ".", "args", ".", "model_type", "=", "encoder_type", "+", "\"-bert\"", "\n", "", "else", ":", "\n", "            ", "self", ".", "args", ".", "model_type", "=", "\"encoder-decoder\"", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.train_model": [[217, 285], ["seq2seq_model.Seq2SeqModel._move_model_to_device", "seq2seq_model.Seq2SeqModel.load_and_cache_examples", "os.makedirs", "seq2seq_model.Seq2SeqModel.train", "seq2seq_model.Seq2SeqModel._save_model", "seq2seq_model.Seq2SeqModel.args.update_from_dict", "ValueError", "os.path.exists", "os.listdir", "ValueError", "logger.info"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.load_and_cache_examples", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.train", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model"], ["", "", "def", "train_model", "(", "\n", "self", ",", "train_data", ",", "output_dir", "=", "None", ",", "show_running_loss", "=", "True", ",", "args", "=", "None", ",", "eval_data", "=", "None", ",", "verbose", "=", "True", ",", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Trains the model using 'train_data'\n\n        Args:\n            train_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n                        - `input_text`: The input text sequence.\n                        - `target_text`: The target text sequence\n            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.\n\n        Returns:\n            None\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "\n", "if", "args", ":", "\n", "            ", "self", ".", "args", ".", "update_from_dict", "(", "args", ")", "\n", "\n", "# if self.args.silent:", "\n", "#     show_running_loss = False", "\n", "\n", "", "if", "self", ".", "args", ".", "evaluate_during_training", "and", "eval_data", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"evaluate_during_training is enabled but eval_data is not specified.\"", "\n", "\" Pass eval_data to model.train_model() if using evaluate_during_training.\"", "\n", ")", "\n", "\n", "", "if", "not", "output_dir", ":", "\n", "            ", "output_dir", "=", "self", ".", "args", ".", "output_dir", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "output_dir", ")", "and", "os", ".", "listdir", "(", "output_dir", ")", "and", "not", "self", ".", "args", ".", "overwrite_output_dir", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Output directory ({}) already exists and is not empty.\"", "\n", "\" Set args.overwrite_output_dir = True to overcome.\"", ".", "format", "(", "output_dir", ")", "\n", ")", "\n", "\n", "", "self", ".", "_move_model_to_device", "(", ")", "\n", "\n", "train_dataset", "=", "self", ".", "load_and_cache_examples", "(", "train_data", ",", "verbose", "=", "verbose", ")", "\n", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "global_step", ",", "tr_loss", "=", "self", ".", "train", "(", "\n", "train_dataset", ",", "\n", "output_dir", ",", "\n", "show_running_loss", "=", "show_running_loss", ",", "\n", "eval_data", "=", "eval_data", ",", "\n", "verbose", "=", "verbose", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "self", ".", "_save_model", "(", "self", ".", "args", ".", "output_dir", ",", "model", "=", "self", ".", "model", ")", "\n", "\n", "# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model", "\n", "# model_to_save.save_pretrained(output_dir)", "\n", "# self.encoder_tokenizer.save_pretrained(output_dir)", "\n", "# self.decoder_tokenizer.save_pretrained(output_dir)", "\n", "# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))", "\n", "\n", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\" Training of {} model complete. Saved to {}.\"", ".", "format", "(", "self", ".", "args", ".", "model_name", ",", "output_dir", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.train": [[286, 673], ["tensorboardX.SummaryWriter", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "set", "math.ceil", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "logger.info", "torch.nn.DataParallel.zero_grad", "tqdm.auto.tqdm.auto.trange", "torch.nn.DataParallel.train", "group.pop", "set.update", "optimizer_grouped_parameters.append", "group.pop", "torch.nn.DataParallel.named_parameters", "optimizer_grouped_parameters.append", "optimizer_grouped_parameters.append", "optimizer_grouped_parameters.extend", "os.path.isfile", "os.path.isfile", "transformers.AdamW.load_state_dict", "transformers.get_linear_schedule_with_warmup.load_state_dict", "torch.nn.DataParallel", "int", "os.path.exists", "seq2seq_model.Seq2SeqModel._create_training_progress_scores", "wandb.init", "wandb.watch", "amp.GradScaler", "tqdm.auto.tqdm.auto.trange.set_description", "tqdm.auto.tqdm.auto.tqdm", "enumerate", "os.path.join", "os.path.join", "os.path.join", "torch.load", "torch.load", "[].split", "int", "logger.info", "logger.info", "logger.info", "logger.info", "seq2seq_model.Seq2SeqModel._get_inputs_dict", "loss.mean.mean.item", "loss.mean.mean.item", "os.makedirs", "seq2seq_model.Seq2SeqModel._save_model", "seq2seq_model.Seq2SeqModel.eval_model", "training_progress_scores[].append", "training_progress_scores[].append", "pandas.DataFrame", "pandas.DataFrame.to_csv", "len", "torch.nn.DataParallel.named_parameters", "any", "set.add", "os.path.join", "os.path.join", "len", "logger.info", "torch.nn.DataParallel.", "loss.mean.mean.mean", "tqdm.auto.tqdm.auto.tqdm.set_description", "amp.GradScaler.scale().backward", "loss.mean.mean.backward", "torch.nn.utils.clip_grad_norm_", "transformers.get_linear_schedule_with_warmup.step", "torch.nn.DataParallel.zero_grad", "seq2seq_model.Seq2SeqModel._save_model", "training_progress_scores[].append", "os.path.join", "wandb.log", "len", "params_nd.append", "params_d.append", "len", "len", "dataclasses.asdict", "amp.autocast", "torch.nn.DataParallel.", "amp.GradScaler.unscale_", "torch.nn.DataParallel.parameters", "amp.GradScaler.step", "amp.GradScaler.update", "transformers.AdamW.step", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "os.path.join", "seq2seq_model.Seq2SeqModel._save_model", "seq2seq_model.Seq2SeqModel.eval_model", "seq2seq_model.Seq2SeqModel.items", "os.path.join", "training_progress_scores[].append", "training_progress_scores[].append", "pandas.DataFrame", "pandas.DataFrame.to_csv", "seq2seq_model.Seq2SeqModel._get_last_metrics", "seq2seq_model.Seq2SeqModel._save_model", "args.model_name.split", "amp.GradScaler.scale", "wandb.log", "tensorboardX.SummaryWriter.add_scalar", "seq2seq_model.Seq2SeqModel._save_model", "training_progress_scores[].append", "os.path.join", "wandb.log", "seq2seq_model.Seq2SeqModel._save_model", "seq2seq_model.Seq2SeqModel._save_model", "torch.nn.DataParallel.named_parameters", "torch.nn.DataParallel.named_parameters", "transformers.get_linear_schedule_with_warmup.get_lr", "seq2seq_model.Seq2SeqModel._get_last_metrics", "seq2seq_model.Seq2SeqModel._save_model", "any", "seq2seq_model.Seq2SeqModel._save_model", "seq2seq_model.Seq2SeqModel._save_model", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.auto.tqdm.auto.trange.close", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.auto.tqdm.auto.trange.close", "any", "transformers.get_linear_schedule_with_warmup.get_lr", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.auto.tqdm.auto.trange.close", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "tqdm.auto.tqdm.auto.trange.close"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.train", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._create_training_progress_scores", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_inputs_dict", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.eval_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.eval_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_last_metrics", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_last_metrics", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model"], ["", "", "def", "train", "(", "\n", "self", ",", "train_dataset", ",", "output_dir", ",", "show_running_loss", "=", "True", ",", "eval_data", "=", "None", ",", "verbose", "=", "True", ",", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Trains the model on train_dataset.\n\n        Utility function to be used by the train_model() method. Not intended to be used directly.\n        \"\"\"", "\n", "\n", "model", "=", "self", ".", "model", "\n", "args", "=", "self", ".", "args", "\n", "\n", "tb_writer", "=", "SummaryWriter", "(", "logdir", "=", "args", ".", "tensorboard_dir", ")", "\n", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "\n", "train_dataset", ",", "\n", "sampler", "=", "train_sampler", ",", "\n", "batch_size", "=", "args", ".", "train_batch_size", ",", "\n", "num_workers", "=", "self", ".", "args", ".", "dataloader_num_workers", ",", "\n", ")", "\n", "\n", "if", "args", ".", "max_steps", ">", "0", ":", "\n", "            ", "t_total", "=", "args", ".", "max_steps", "\n", "args", ".", "num_train_epochs", "=", "args", ".", "max_steps", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "t_total", "=", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", "\n", "\n", "", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "\n", "optimizer_grouped_parameters", "=", "[", "]", "\n", "custom_parameter_names", "=", "set", "(", ")", "\n", "for", "group", "in", "self", ".", "args", ".", "custom_parameter_groups", ":", "\n", "            ", "params", "=", "group", ".", "pop", "(", "\"params\"", ")", "\n", "custom_parameter_names", ".", "update", "(", "params", ")", "\n", "param_group", "=", "{", "**", "group", "}", "\n", "param_group", "[", "\"params\"", "]", "=", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "n", "in", "params", "]", "\n", "optimizer_grouped_parameters", ".", "append", "(", "param_group", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "args", ".", "custom_layer_parameters", ":", "\n", "            ", "layer_number", "=", "group", ".", "pop", "(", "\"layer\"", ")", "\n", "layer", "=", "f\"layer.{layer_number}.\"", "\n", "group_d", "=", "{", "**", "group", "}", "\n", "group_nd", "=", "{", "**", "group", "}", "\n", "group_nd", "[", "\"weight_decay\"", "]", "=", "0.0", "\n", "params_d", "=", "[", "]", "\n", "params_nd", "=", "[", "]", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "n", "not", "in", "custom_parameter_names", "and", "layer", "in", "n", ":", "\n", "                    ", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", ":", "\n", "                        ", "params_nd", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "                        ", "params_d", ".", "append", "(", "p", ")", "\n", "", "custom_parameter_names", ".", "add", "(", "n", ")", "\n", "", "", "group_d", "[", "\"params\"", "]", "=", "params_d", "\n", "group_nd", "[", "\"params\"", "]", "=", "params_nd", "\n", "\n", "optimizer_grouped_parameters", ".", "append", "(", "group_d", ")", "\n", "optimizer_grouped_parameters", ".", "append", "(", "group_nd", ")", "\n", "\n", "", "if", "not", "self", ".", "args", ".", "train_custom_parameters_only", ":", "\n", "            ", "optimizer_grouped_parameters", ".", "extend", "(", "\n", "[", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "n", "not", "in", "custom_parameter_names", "and", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "n", "not", "in", "custom_parameter_names", "and", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", ")", "\n", "\n", "", "warmup_steps", "=", "math", ".", "ceil", "(", "t_total", "*", "args", ".", "warmup_ratio", ")", "\n", "args", ".", "warmup_steps", "=", "warmup_steps", "if", "args", ".", "warmup_steps", "==", "0", "else", "args", ".", "warmup_steps", "\n", "\n", "# TODO: Use custom optimizer like with BertSum?", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "# decay", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "if", "(", "\n", "args", ".", "model_name", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name", ",", "\"optimizer.pt\"", ")", ")", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name", ",", "\"scheduler.pt\"", ")", ")", "\n", ")", ":", "\n", "# Load in optimizer and scheduler states", "\n", "            ", "optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name", ",", "\"optimizer.pt\"", ")", ")", ")", "\n", "scheduler", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_name", ",", "\"scheduler.pt\"", ")", ")", ")", "\n", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "logger", ".", "info", "(", "\" Training started\"", ")", "\n", "\n", "global_step", "=", "0", "\n", "tr_loss", ",", "logging_loss", "=", "0.0", ",", "0.0", "\n", "model", ".", "zero_grad", "(", ")", "\n", "train_iterator", "=", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "silent", ",", "mininterval", "=", "0", ")", "\n", "epoch_number", "=", "0", "\n", "best_eval_metric", "=", "None", "\n", "early_stopping_counter", "=", "0", "\n", "steps_trained_in_current_epoch", "=", "0", "\n", "epochs_trained", "=", "0", "\n", "\n", "if", "args", ".", "model_name", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "model_name", ")", ":", "\n", "            ", "try", ":", "\n", "# set global_step to gobal_step of last saved checkpoint from model path", "\n", "                ", "checkpoint_suffix", "=", "args", ".", "model_name", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\"-\"", ")", "\n", "if", "len", "(", "checkpoint_suffix", ")", ">", "2", ":", "\n", "                    ", "checkpoint_suffix", "=", "checkpoint_suffix", "[", "1", "]", "\n", "", "else", ":", "\n", "                    ", "checkpoint_suffix", "=", "checkpoint_suffix", "[", "-", "1", "]", "\n", "", "global_step", "=", "int", "(", "checkpoint_suffix", ")", "\n", "epochs_trained", "=", "global_step", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "\n", "steps_trained_in_current_epoch", "=", "global_step", "%", "(", "\n", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"   Continuing training from checkpoint, will skip to saved global_step\"", ")", "\n", "logger", ".", "info", "(", "\"   Continuing training from epoch %d\"", ",", "epochs_trained", ")", "\n", "logger", ".", "info", "(", "\"   Continuing training from global step %d\"", ",", "global_step", ")", "\n", "logger", ".", "info", "(", "\"   Will skip the first %d steps in the current epoch\"", ",", "steps_trained_in_current_epoch", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "logger", ".", "info", "(", "\"   Starting fine-tuning.\"", ")", "\n", "\n", "", "", "if", "args", ".", "evaluate_during_training", ":", "\n", "            ", "training_progress_scores", "=", "self", ".", "_create_training_progress_scores", "(", "**", "kwargs", ")", "\n", "\n", "", "if", "args", ".", "wandb_project", ":", "\n", "            ", "wandb", ".", "init", "(", "project", "=", "args", ".", "wandb_project", ",", "config", "=", "{", "**", "asdict", "(", "args", ")", "}", ",", "**", "args", ".", "wandb_kwargs", ")", "\n", "wandb", ".", "watch", "(", "self", ".", "model", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "            ", "from", "torch", ".", "cuda", "import", "amp", "\n", "\n", "scaler", "=", "amp", ".", "GradScaler", "(", ")", "\n", "\n", "", "model", ".", "train", "(", ")", "\n", "for", "current_epoch", "in", "train_iterator", ":", "\n", "            ", "if", "epochs_trained", ">", "0", ":", "\n", "                ", "epochs_trained", "-=", "1", "\n", "continue", "\n", "", "train_iterator", ".", "set_description", "(", "f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\"", ")", "\n", "batch_iterator", "=", "tqdm", "(", "\n", "train_dataloader", ",", "\n", "desc", "=", "f\"Running Epoch {epoch_number} of {args.num_train_epochs}\"", ",", "\n", "disable", "=", "args", ".", "silent", ",", "\n", "mininterval", "=", "0", ",", "\n", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "batch_iterator", ")", ":", "\n", "                ", "if", "steps_trained_in_current_epoch", ">", "0", ":", "\n", "                    ", "steps_trained_in_current_epoch", "-=", "1", "\n", "continue", "\n", "# batch = tuple(t.to(device) for t in batch)", "\n", "\n", "", "inputs", "=", "self", ".", "_get_inputs_dict", "(", "batch", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "                    ", "with", "amp", ".", "autocast", "(", ")", ":", "\n", "                        ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "# model outputs are always tuple in pytorch-transformers (see doc)", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "", "", "else", ":", "\n", "                    ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "# model outputs are always tuple in pytorch-transformers (see doc)", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu parallel training", "\n", "", "current_loss", "=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "show_running_loss", ":", "\n", "                    ", "batch_iterator", ".", "set_description", "(", "\n", "f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"", "\n", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "                        ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "                        ", "scaler", ".", "step", "(", "optimizer", ")", "\n", "scaler", ".", "update", "(", ")", "\n", "", "else", ":", "\n", "                        ", "optimizer", ".", "step", "(", ")", "\n", "", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ":", "\n", "# Log metrics", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "\"lr\"", ",", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "global_step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "\"loss\"", ",", "(", "tr_loss", "-", "logging_loss", ")", "/", "args", ".", "logging_steps", ",", "global_step", ")", "\n", "logging_loss", "=", "tr_loss", "\n", "if", "args", ".", "wandb_project", ":", "\n", "                            ", "wandb", ".", "log", "(", "\n", "{", "\n", "\"Training loss\"", ":", "current_loss", ",", "\n", "\"lr\"", ":", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", ",", "\n", "\"global_step\"", ":", "global_step", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "if", "args", ".", "save_steps", ">", "0", "and", "global_step", "%", "args", ".", "save_steps", "==", "0", ":", "\n", "# Save model checkpoint", "\n", "                        ", "output_dir_current", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-{}\"", ".", "format", "(", "global_step", ")", ")", "\n", "\n", "self", ".", "_save_model", "(", "output_dir_current", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ")", "\n", "\n", "", "if", "args", ".", "evaluate_during_training", "and", "(", "\n", "args", ".", "evaluate_during_training_steps", ">", "0", "\n", "and", "global_step", "%", "args", ".", "evaluate_during_training_steps", "==", "0", "\n", ")", ":", "\n", "# Only evaluate when single GPU otherwise metrics may not average well", "\n", "                        ", "results", "=", "self", ".", "eval_model", "(", "\n", "eval_data", ",", "\n", "verbose", "=", "verbose", "and", "args", ".", "evaluate_during_training_verbose", ",", "\n", "silent", "=", "args", ".", "evaluate_during_training_silent", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "for", "key", ",", "value", "in", "results", ".", "items", "(", ")", ":", "\n", "                            ", "tb_writer", ".", "add_scalar", "(", "\"eval_{}\"", ".", "format", "(", "key", ")", ",", "value", ",", "global_step", ")", "\n", "\n", "", "output_dir_current", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-{}\"", ".", "format", "(", "global_step", ")", ")", "\n", "\n", "if", "args", ".", "save_eval_checkpoints", ":", "\n", "                            ", "self", ".", "_save_model", "(", "output_dir_current", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", ")", "\n", "\n", "", "training_progress_scores", "[", "\"global_step\"", "]", ".", "append", "(", "global_step", ")", "\n", "training_progress_scores", "[", "\"train_loss\"", "]", ".", "append", "(", "current_loss", ")", "\n", "for", "key", "in", "results", ":", "\n", "                            ", "training_progress_scores", "[", "key", "]", ".", "append", "(", "results", "[", "key", "]", ")", "\n", "", "report", "=", "pd", ".", "DataFrame", "(", "training_progress_scores", ")", "\n", "report", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"training_progress_scores.csv\"", ")", ",", "index", "=", "False", ",", "\n", ")", "\n", "\n", "if", "args", ".", "wandb_project", ":", "\n", "                            ", "wandb", ".", "log", "(", "self", ".", "_get_last_metrics", "(", "training_progress_scores", ")", ")", "\n", "\n", "", "if", "not", "best_eval_metric", ":", "\n", "                            ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                                ", "self", ".", "_save_model", "(", "\n", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", "\n", ")", "\n", "", "", "if", "best_eval_metric", "and", "args", ".", "early_stopping_metric_minimize", ":", "\n", "                            ", "if", "results", "[", "args", ".", "early_stopping_metric", "]", "-", "best_eval_metric", "<", "args", ".", "early_stopping_delta", ":", "\n", "                                ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                                    ", "self", ".", "_save_model", "(", "\n", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", "\n", ")", "\n", "", "early_stopping_counter", "=", "0", "\n", "", "else", ":", "\n", "                                ", "if", "args", ".", "use_early_stopping", ":", "\n", "                                    ", "if", "early_stopping_counter", "<", "args", ".", "early_stopping_patience", ":", "\n", "                                        ", "early_stopping_counter", "+=", "1", "\n", "if", "verbose", ":", "\n", "                                            ", "logger", ".", "info", "(", "f\" No improvement in {args.early_stopping_metric}\"", ")", "\n", "logger", ".", "info", "(", "f\" Current step: {early_stopping_counter}\"", ")", "\n", "logger", ".", "info", "(", "f\" Early stopping patience: {args.early_stopping_patience}\"", ")", "\n", "", "", "else", ":", "\n", "                                        ", "if", "verbose", ":", "\n", "                                            ", "logger", ".", "info", "(", "f\" Patience of {args.early_stopping_patience} steps reached\"", ")", "\n", "logger", ".", "info", "(", "\" Training terminated.\"", ")", "\n", "train_iterator", ".", "close", "(", ")", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "", "", "", "", "else", ":", "\n", "                            ", "if", "results", "[", "args", ".", "early_stopping_metric", "]", "-", "best_eval_metric", ">", "args", ".", "early_stopping_delta", ":", "\n", "                                ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                                    ", "self", ".", "_save_model", "(", "\n", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", "\n", ")", "\n", "", "early_stopping_counter", "=", "0", "\n", "", "else", ":", "\n", "                                ", "if", "args", ".", "use_early_stopping", ":", "\n", "                                    ", "if", "early_stopping_counter", "<", "args", ".", "early_stopping_patience", ":", "\n", "                                        ", "early_stopping_counter", "+=", "1", "\n", "if", "verbose", ":", "\n", "                                            ", "logger", ".", "info", "(", "f\" No improvement in {args.early_stopping_metric}\"", ")", "\n", "logger", ".", "info", "(", "f\" Current step: {early_stopping_counter}\"", ")", "\n", "logger", ".", "info", "(", "f\" Early stopping patience: {args.early_stopping_patience}\"", ")", "\n", "", "", "else", ":", "\n", "                                        ", "if", "verbose", ":", "\n", "                                            ", "logger", ".", "info", "(", "f\" Patience of {args.early_stopping_patience} steps reached\"", ")", "\n", "logger", ".", "info", "(", "\" Training terminated.\"", ")", "\n", "train_iterator", ".", "close", "(", ")", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n", "", "", "", "", "", "", "", "epoch_number", "+=", "1", "\n", "output_dir_current", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"checkpoint-{}-epoch-{}\"", ".", "format", "(", "global_step", ",", "epoch_number", ")", ")", "\n", "\n", "if", "args", ".", "save_model_every_epoch", "or", "args", ".", "evaluate_during_training", ":", "\n", "                ", "os", ".", "makedirs", "(", "output_dir_current", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "if", "args", ".", "save_model_every_epoch", ":", "\n", "                ", "self", ".", "_save_model", "(", "output_dir_current", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ")", "\n", "\n", "", "if", "args", ".", "evaluate_during_training", ":", "\n", "                ", "results", "=", "self", ".", "eval_model", "(", "\n", "eval_data", ",", "\n", "verbose", "=", "verbose", "and", "args", ".", "evaluate_during_training_verbose", ",", "\n", "silent", "=", "args", ".", "evaluate_during_training_silent", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "if", "args", ".", "save_eval_checkpoints", ":", "\n", "                    ", "self", ".", "_save_model", "(", "output_dir_current", ",", "optimizer", ",", "scheduler", ",", "results", "=", "results", ")", "\n", "\n", "", "training_progress_scores", "[", "\"global_step\"", "]", ".", "append", "(", "global_step", ")", "\n", "training_progress_scores", "[", "\"train_loss\"", "]", ".", "append", "(", "current_loss", ")", "\n", "for", "key", "in", "results", ":", "\n", "                    ", "training_progress_scores", "[", "key", "]", ".", "append", "(", "results", "[", "key", "]", ")", "\n", "", "report", "=", "pd", ".", "DataFrame", "(", "training_progress_scores", ")", "\n", "report", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"training_progress_scores.csv\"", ")", ",", "index", "=", "False", ")", "\n", "\n", "if", "args", ".", "wandb_project", ":", "\n", "                    ", "wandb", ".", "log", "(", "self", ".", "_get_last_metrics", "(", "training_progress_scores", ")", ")", "\n", "\n", "", "if", "not", "best_eval_metric", ":", "\n", "                    ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                        ", "self", ".", "_save_model", "(", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", ")", "\n", "", "", "if", "best_eval_metric", "and", "args", ".", "early_stopping_metric_minimize", ":", "\n", "                    ", "if", "results", "[", "args", ".", "early_stopping_metric", "]", "-", "best_eval_metric", "<", "args", ".", "early_stopping_delta", ":", "\n", "                        ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                            ", "self", ".", "_save_model", "(", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", ")", "\n", "", "early_stopping_counter", "=", "0", "\n", "", "else", ":", "\n", "                        ", "if", "args", ".", "use_early_stopping", "and", "args", ".", "early_stopping_consider_epochs", ":", "\n", "                            ", "if", "early_stopping_counter", "<", "args", ".", "early_stopping_patience", ":", "\n", "                                ", "early_stopping_counter", "+=", "1", "\n", "if", "verbose", ":", "\n", "                                    ", "logger", ".", "info", "(", "f\" No improvement in {args.early_stopping_metric}\"", ")", "\n", "logger", ".", "info", "(", "f\" Current step: {early_stopping_counter}\"", ")", "\n", "logger", ".", "info", "(", "f\" Early stopping patience: {args.early_stopping_patience}\"", ")", "\n", "", "", "else", ":", "\n", "                                ", "if", "verbose", ":", "\n", "                                    ", "logger", ".", "info", "(", "f\" Patience of {args.early_stopping_patience} steps reached\"", ")", "\n", "logger", ".", "info", "(", "\" Training terminated.\"", ")", "\n", "train_iterator", ".", "close", "(", ")", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "", "", "", "", "else", ":", "\n", "                    ", "if", "results", "[", "args", ".", "early_stopping_metric", "]", "-", "best_eval_metric", ">", "args", ".", "early_stopping_delta", ":", "\n", "                        ", "best_eval_metric", "=", "results", "[", "args", ".", "early_stopping_metric", "]", "\n", "if", "args", ".", "save_best_model", ":", "\n", "                            ", "self", ".", "_save_model", "(", "args", ".", "best_model_dir", ",", "optimizer", ",", "scheduler", ",", "model", "=", "model", ",", "results", "=", "results", ")", "\n", "", "early_stopping_counter", "=", "0", "\n", "", "else", ":", "\n", "                        ", "if", "args", ".", "use_early_stopping", "and", "args", ".", "early_stopping_consider_epochs", ":", "\n", "                            ", "if", "early_stopping_counter", "<", "args", ".", "early_stopping_patience", ":", "\n", "                                ", "early_stopping_counter", "+=", "1", "\n", "if", "verbose", ":", "\n", "                                    ", "logger", ".", "info", "(", "f\" No improvement in {args.early_stopping_metric}\"", ")", "\n", "logger", ".", "info", "(", "f\" Current step: {early_stopping_counter}\"", ")", "\n", "logger", ".", "info", "(", "f\" Early stopping patience: {args.early_stopping_patience}\"", ")", "\n", "", "", "else", ":", "\n", "                                ", "if", "verbose", ":", "\n", "                                    ", "logger", ".", "info", "(", "f\" Patience of {args.early_stopping_patience} steps reached\"", ")", "\n", "logger", ".", "info", "(", "\" Training terminated.\"", ")", "\n", "train_iterator", ".", "close", "(", ")", "\n", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n", "", "", "", "", "", "", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.eval_model": [[674, 713], ["seq2seq_model.Seq2SeqModel._move_model_to_device", "seq2seq_model.Seq2SeqModel.load_and_cache_examples", "os.makedirs", "seq2seq_model.Seq2SeqModel.evaluate", "seq2seq_model.Seq2SeqModel.results.update", "seq2seq_model.Seq2SeqModel.evaluate_decode", "seq2seq_model.Seq2SeqModel.results.update", "logger.info"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.load_and_cache_examples", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.evaluate", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.evaluate_decode"], ["", "def", "eval_model", "(", "self", ",", "eval_data", ",", "output_dir", "=", "None", ",", "verbose", "=", "True", ",", "silent", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates the model on eval_data. Saves results to output_dir.\n\n        Args:\n            eval_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n                        - `input_text`: The input text sequence.\n                        - `target_text`: The target text sequence.\n            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n            verbose: If verbose, results will be printed to the console on completion of evaluation.\n            silent: If silent, tqdm progress bars will be hidden.\n            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n        Returns:\n            results: Dictionary containing evaluation results.\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "\n", "if", "not", "output_dir", ":", "\n", "            ", "output_dir", "=", "self", ".", "args", ".", "output_dir", "\n", "\n", "", "self", ".", "_move_model_to_device", "(", ")", "\n", "\n", "eval_dataset", "=", "self", ".", "load_and_cache_examples", "(", "eval_data", ",", "evaluate", "=", "True", ",", "verbose", "=", "verbose", ",", "silent", "=", "silent", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "result", "=", "self", ".", "evaluate", "(", "eval_dataset", ",", "output_dir", ",", "verbose", "=", "verbose", ",", "silent", "=", "silent", ",", "**", "kwargs", ")", "\n", "self", ".", "results", ".", "update", "(", "result", ")", "\n", "\n", "if", "self", ".", "args", ".", "evaluate_generated_text", ":", "\n", "# to_predict = eval_data[\"input_text\"].tolist()", "\n", "# preds = self.predict(to_predict)", "\n", "# result = self.compute_metrics(eval_data[\"target_text\"].tolist(), preds, **kwargs)", "\n", "            ", "result", "=", "self", ".", "evaluate_decode", "(", "eval_dataset", ",", "output_dir", ",", "verbose", "=", "verbose", ",", "silent", "=", "silent", ",", "**", "kwargs", ")", "\n", "self", ".", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "self", ".", "results", ")", "\n", "\n", "", "return", "self", ".", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.evaluate": [[714, 756], ["torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "tqdm.auto.tqdm.auto.tqdm", "os.path.join", "torch.nn.DataParallel", "seq2seq_model.Seq2SeqModel._get_inputs_dict", "open", "sorted", "torch.no_grad", "torch.nn.DataParallel.", "loss.mean().item", "results.keys", "writer.write", "loss.mean", "str"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_inputs_dict"], ["", "def", "evaluate", "(", "self", ",", "eval_dataset", ",", "output_dir", ",", "verbose", "=", "True", ",", "silent", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates the model on eval_dataset.\n\n        Utility function to be used by the eval_model() method. Not intended to be used directly.\n        \"\"\"", "\n", "\n", "model", "=", "self", ".", "model", "\n", "args", "=", "self", ".", "args", "\n", "eval_output_dir", "=", "output_dir", "\n", "\n", "results", "=", "{", "}", "\n", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "disable", "=", "args", ".", "silent", "or", "silent", ",", "desc", "=", "\"Running Evaluation\"", ")", ":", "\n", "# batch = tuple(t.to(device) for t in batch)", "\n", "\n", "            ", "inputs", "=", "self", ".", "_get_inputs_dict", "(", "batch", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "eval_loss", "+=", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "results", "[", "\"eval_loss\"", "]", "=", "eval_loss", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "eval_output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "for", "key", "in", "sorted", "(", "results", ".", "keys", "(", ")", ")", ":", "\n", "                ", "writer", ".", "write", "(", "\"{} = {}\\n\"", ".", "format", "(", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.evaluate_decode": [[757, 800], ["torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "tqdm.auto.tqdm.auto.tqdm", "torch.nn.DataParallel", "seq2seq_model.Seq2SeqModel._get_inputs_dict", "torch.no_grad", "torch.nn.DataParallel.", "loss.mean().item", "torch.argmax().view", "inputs[].view", "zip", "loss.mean", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_inputs_dict"], ["", "def", "evaluate_decode", "(", "self", ",", "eval_dataset", ",", "output_dir", ",", "verbose", "=", "True", ",", "silent", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates the model on eval_dataset.\n\n        Utility function to be used by the eval_model() method. Not intended to be used directly.\n        \"\"\"", "\n", "\n", "model", "=", "self", ".", "model", "\n", "args", "=", "self", ".", "args", "\n", "eval_output_dir", "=", "output_dir", "\n", "\n", "results", "=", "{", "}", "\n", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_dataset", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_dataset", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "eval_loss", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "correct", ",", "count", "=", "0", ",", "0", "\n", "for", "batch", "in", "tqdm", "(", "eval_dataloader", ",", "disable", "=", "args", ".", "silent", "or", "silent", ",", "desc", "=", "\"Running Evaluation\"", ")", ":", "\n", "# batch = tuple(t.to(device) for t in batch)", "\n", "            ", "inputs", "=", "self", ".", "_get_inputs_dict", "(", "batch", ")", "\n", "# print(inputs)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "eval_loss", "+=", "loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "decode_outputs", "=", "torch", ".", "argmax", "(", "outputs", "[", "1", "]", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "labels", "=", "inputs", "[", "\"labels\"", "]", ".", "view", "(", "-", "1", ")", "\n", "for", "i", ",", "j", "in", "zip", "(", "labels", ",", "decode_outputs", ")", ":", "\n", "                    ", "if", "i", "==", "j", "and", "i", "!=", "-", "100", ":", "\n", "                        ", "correct", "+=", "1", "\n", "", "if", "i", "!=", "-", "100", ":", "\n", "                        ", "count", "+=", "1", "\n", "", "", "", "nb_eval_steps", "+=", "1", "\n", "\n", "", "results", "[", "\"eval_acc\"", "]", "=", "correct", "/", "count", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.predict": [[801, 886], ["seq2seq_model.Seq2SeqModel._move_model_to_device", "input_ids.to.to.to", "all_outputs.extend", "seq2seq_model.Seq2SeqModel.model.to", "seq2seq_model.Seq2SeqModel._move_model_to_device", "range", "generate", "seq2seq_model.Seq2SeqModel.model.generate", "list.cpu().numpy", "multiprocessing.Pool", "list", "seq2seq_model.Seq2SeqModel.decoder_tokenizer.decode", "len", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.prepare_translation_batch", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.batch_encode_plus", "tqdm.auto.tqdm.auto.tqdm", "range", "list.cpu", "p.imap", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device"], ["", "def", "predict", "(", "self", ",", "to_predict", ")", ":", "\n", "        ", "\"\"\"\n        Performs predictions on a list of text.\n\n        Args:\n            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n\n        Returns:\n            preds: A python list of the generated sequences.\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "\n", "self", ".", "_move_model_to_device", "(", ")", "\n", "\n", "all_outputs", "=", "[", "]", "\n", "# Batching", "\n", "for", "batch", "in", "[", "\n", "to_predict", "[", "i", ":", "i", "+", "self", ".", "args", ".", "eval_batch_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "to_predict", ")", ",", "self", ".", "args", ".", "eval_batch_size", ")", "\n", "]", ":", "\n", "            ", "if", "self", ".", "args", ".", "model_type", "==", "\"marian\"", ":", "\n", "                ", "input_ids", "=", "self", ".", "encoder_tokenizer", ".", "prepare_translation_batch", "(", "\n", "batch", ",", "max_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "[", "\"input_ids\"", "]", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "self", ".", "encoder_tokenizer", ".", "batch_encode_plus", "(", "\n", "batch", ",", "max_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "[", "\"input_ids\"", "]", "\n", "", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "model_type", "in", "[", "\"bart\"", ",", "\"marian\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "\n", "                ", "outputs", "=", "generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "num_beams", "=", "self", ".", "args", ".", "num_beams", ",", "\n", "max_length", "=", "self", ".", "args", ".", "max_length", ",", "\n", "length_penalty", "=", "self", ".", "args", ".", "length_penalty", ",", "\n", "early_stopping", "=", "self", ".", "args", ".", "early_stopping", ",", "\n", "repetition_penalty", "=", "self", ".", "args", ".", "repetition_penalty", ",", "\n", "do_sample", "=", "self", ".", "args", ".", "do_sample", ",", "\n", "top_k", "=", "self", ".", "args", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "args", ".", "top_p", ",", "\n", "num_return_sequences", "=", "self", ".", "args", ".", "num_return_sequences", ",", "\n", "# temperature=0.7", "\n", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "self", ".", "model", ".", "generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "decoder_start_token_id", "=", "self", ".", "model", ".", "config", ".", "decoder", ".", "pad_token_id", ",", "\n", "num_beams", "=", "self", ".", "args", ".", "num_beams", ",", "\n", "max_length", "=", "self", ".", "args", ".", "max_length", ",", "\n", "length_penalty", "=", "self", ".", "args", ".", "length_penalty", ",", "\n", "early_stopping", "=", "self", ".", "args", ".", "early_stopping", ",", "\n", "repetition_penalty", "=", "self", ".", "args", ".", "repetition_penalty", ",", "\n", "do_sample", "=", "self", ".", "args", ".", "do_sample", ",", "\n", "top_k", "=", "self", ".", "args", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "args", ".", "top_p", ",", "\n", "num_return_sequences", "=", "self", ".", "args", ".", "num_return_sequences", ",", "\n", ")", "\n", "\n", "", "all_outputs", ".", "extend", "(", "outputs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "use_multiprocessed_decoding", ":", "\n", "            ", "self", ".", "model", ".", "to", "(", "\"cpu\"", ")", "\n", "with", "Pool", "(", "self", ".", "args", ".", "process_count", ")", "as", "p", ":", "\n", "                ", "outputs", "=", "list", "(", "\n", "tqdm", "(", "\n", "p", ".", "imap", "(", "self", ".", "_decode", ",", "all_outputs", ",", "chunksize", "=", "self", ".", "args", ".", "multiprocessing_chunksize", ")", ",", "\n", "total", "=", "len", "(", "all_outputs", ")", ",", "\n", "desc", "=", "\"Decoding outputs\"", ",", "\n", "disable", "=", "self", ".", "args", ".", "silent", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "_move_model_to_device", "(", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "[", "\n", "self", ".", "decoder_tokenizer", ".", "decode", "(", "output_id", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "for", "output_id", "in", "all_outputs", "\n", "]", "\n", "\n", "", "if", "self", ".", "args", ".", "num_return_sequences", ">", "1", ":", "\n", "            ", "return", "[", "\n", "outputs", "[", "i", ":", "i", "+", "self", ".", "args", ".", "num_return_sequences", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "outputs", ")", ",", "self", ".", "args", ".", "num_return_sequences", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.predict_sep": [[888, 973], ["seq2seq_model.Seq2SeqModel._move_model_to_device", "input_ids.to.to.to", "all_outputs.extend", "seq2seq_model.Seq2SeqModel.model.to", "seq2seq_model.Seq2SeqModel._move_model_to_device", "range", "seq2seq_model.Seq2SeqModel.model.generate", "seq2seq_model.Seq2SeqModel.model.generate", "list.cpu().numpy", "multiprocessing.Pool", "list", "seq2seq_model.Seq2SeqModel.decoder_tokenizer.decode", "len", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.prepare_translation_batch", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.batch_encode_plus", "tqdm.auto.tqdm.auto.tqdm", "range", "list.cpu", "p.imap", "len", "len"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device", "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device"], ["", "", "def", "predict_sep", "(", "self", ",", "to_predict", ",", "decoder_input_token_id", ")", ":", "\n", "        ", "\"\"\"\n        Performs predictions on a list of text.\n\n        Args:\n            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n\n        Returns:\n            preds: A python list of the generated sequences.\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "\n", "self", ".", "_move_model_to_device", "(", ")", "\n", "\n", "all_outputs", "=", "[", "]", "\n", "# Batching", "\n", "for", "batch", "in", "[", "\n", "to_predict", "[", "i", ":", "i", "+", "self", ".", "args", ".", "eval_batch_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "to_predict", ")", ",", "self", ".", "args", ".", "eval_batch_size", ")", "\n", "]", ":", "\n", "            ", "if", "self", ".", "args", ".", "model_type", "==", "\"marian\"", ":", "\n", "                ", "input_ids", "=", "self", ".", "encoder_tokenizer", ".", "prepare_translation_batch", "(", "\n", "batch", ",", "max_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "[", "\"input_ids\"", "]", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "self", ".", "encoder_tokenizer", ".", "batch_encode_plus", "(", "\n", "batch", ",", "max_length", "=", "self", ".", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "[", "\"input_ids\"", "]", "\n", "", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "args", ".", "model_type", "in", "[", "\"bart\"", ",", "\"marian\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "                ", "outputs", "=", "self", ".", "model", ".", "generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "num_beams", "=", "self", ".", "args", ".", "num_beams", ",", "\n", "max_length", "=", "self", ".", "args", ".", "max_length", ",", "\n", "length_penalty", "=", "self", ".", "args", ".", "length_penalty", ",", "\n", "early_stopping", "=", "self", ".", "args", ".", "early_stopping", ",", "\n", "repetition_penalty", "=", "self", ".", "args", ".", "repetition_penalty", ",", "\n", "do_sample", "=", "self", ".", "args", ".", "do_sample", ",", "\n", "top_k", "=", "self", ".", "args", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "args", ".", "top_p", ",", "\n", "num_return_sequences", "=", "self", ".", "args", ".", "num_return_sequences", ",", "\n", "decoder_start_token_id", "=", "decoder_input_token_id", "\n", "# temperature=0.7", "\n", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "self", ".", "model", ".", "generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "decoder_start_token_id", "=", "self", ".", "model", ".", "config", ".", "decoder", ".", "pad_token_id", ",", "\n", "num_beams", "=", "self", ".", "args", ".", "num_beams", ",", "\n", "max_length", "=", "self", ".", "args", ".", "max_length", ",", "\n", "length_penalty", "=", "self", ".", "args", ".", "length_penalty", ",", "\n", "early_stopping", "=", "self", ".", "args", ".", "early_stopping", ",", "\n", "repetition_penalty", "=", "self", ".", "args", ".", "repetition_penalty", ",", "\n", "do_sample", "=", "self", ".", "args", ".", "do_sample", ",", "\n", "top_k", "=", "self", ".", "args", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "args", ".", "top_p", ",", "\n", "num_return_sequences", "=", "self", ".", "args", ".", "num_return_sequences", ",", "\n", ")", "\n", "\n", "", "all_outputs", ".", "extend", "(", "outputs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "use_multiprocessed_decoding", ":", "\n", "            ", "self", ".", "model", ".", "to", "(", "\"cpu\"", ")", "\n", "with", "Pool", "(", "self", ".", "args", ".", "process_count", ")", "as", "p", ":", "\n", "                ", "outputs", "=", "list", "(", "\n", "tqdm", "(", "\n", "p", ".", "imap", "(", "self", ".", "_decode", ",", "all_outputs", ",", "chunksize", "=", "self", ".", "args", ".", "multiprocessing_chunksize", ")", ",", "\n", "total", "=", "len", "(", "all_outputs", ")", ",", "\n", "desc", "=", "\"Decoding outputs\"", ",", "\n", "disable", "=", "self", ".", "args", ".", "silent", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "_move_model_to_device", "(", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "[", "\n", "self", ".", "decoder_tokenizer", ".", "decode", "(", "output_id", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "for", "output_id", "in", "all_outputs", "\n", "]", "\n", "\n", "", "if", "self", ".", "args", ".", "num_return_sequences", ">", "1", ":", "\n", "            ", "return", "[", "\n", "outputs", "[", "i", ":", "i", "+", "self", ".", "args", ".", "num_return_sequences", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "outputs", ")", ",", "self", ".", "args", ".", "num_return_sequences", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._decode": [[974, 976], ["seq2seq_model.Seq2SeqModel.decoder_tokenizer.decode"], "methods", ["None"], ["", "", "def", "_decode", "(", "self", ",", "output_id", ")", ":", "\n", "        ", "return", "self", ".", "decoder_tokenizer", ".", "decode", "(", "output_id", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.compute_metrics": [[977, 1012], ["zip", "len", "len", "sentence_i.replace.replace.strip", "sentence_i.replace.replace.replace", "sentence_i.replace.replace.replace", "sentence_i.replace.replace.replace", "len", "print", "print", "print", "zip", "sentence_i.replace.replace.split", "sentence_i.replace.replace.split", "sentence_j.split", "sentence_i.replace.replace.split", "sentence_j.split"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "labels", ",", "preds", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Computes the evaluation metrics for the model predictions.\n\n        Args:\n            labels: List of target sequences\n            preds: List of model generated outputs\n            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n\n        Returns:\n            result: Dictionary containing evaluation results.\n        \"\"\"", "# noqa: ignore flake8\"", "\n", "assert", "len", "(", "labels", ")", "==", "len", "(", "preds", ")", "\n", "acc", "=", "0", "\n", "total_count", "=", "0", "\n", "results", "=", "{", "}", "\n", "for", "sentence_i", ",", "sentence_j", "in", "zip", "(", "labels", ",", "preds", ")", ":", "\n", "            ", "sentence_i", "=", "sentence_i", ".", "strip", "(", ")", "\n", "sentence_i", "=", "sentence_i", ".", "replace", "(", "\".\"", ",", "\" .\"", ")", "\n", "sentence_i", "=", "sentence_i", ".", "replace", "(", "\",\"", ",", "\" ,\"", ")", "\n", "sentence_i", "=", "sentence_i", ".", "replace", "(", "\"?\"", ",", "\" ?\"", ")", "\n", "total_count", "+=", "len", "(", "sentence_i", ".", "split", "(", ")", ")", "\n", "print", "(", "sentence_i", ".", "split", "(", ")", ")", "\n", "print", "(", "sentence_j", ".", "split", "(", ")", ")", "\n", "print", "(", "'-------'", ")", "\n", "for", "word_i", ",", "word_j", "in", "zip", "(", "sentence_i", ".", "split", "(", ")", ",", "sentence_j", ".", "split", "(", ")", ")", ":", "\n", "                ", "if", "word_i", "==", "word_j", ":", "\n", "                    ", "acc", "+=", "1", "\n", "", "", "", "results", "[", "'acc'", "]", "=", "acc", "/", "total_count", "\n", "# for metric, func in kwargs.items():", "\n", "#     results[metric] = func(labels, preds)", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.load_and_cache_examples": [[1013, 1040], ["os.makedirs", "CustomDataset", "seq2seq_utils.SimpleSummarizationDataset", "seq2seq_utils.Seq2SeqDataset"], "methods", ["None"], ["", "def", "load_and_cache_examples", "(", "self", ",", "data", ",", "evaluate", "=", "False", ",", "no_cache", "=", "False", ",", "verbose", "=", "True", ",", "silent", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Creates a T5Dataset from data.\n\n        Utility function for train() and eval() methods. Not intended to be used directly.\n        \"\"\"", "\n", "\n", "encoder_tokenizer", "=", "self", ".", "encoder_tokenizer", "\n", "decoder_tokenizer", "=", "self", ".", "decoder_tokenizer", "\n", "args", "=", "self", ".", "args", "\n", "\n", "if", "not", "no_cache", ":", "\n", "            ", "no_cache", "=", "args", ".", "no_cache", "\n", "\n", "", "if", "not", "no_cache", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "args", ".", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "mode", "=", "\"dev\"", "if", "evaluate", "else", "\"train\"", "\n", "\n", "if", "args", ".", "dataset_class", ":", "\n", "            ", "CustomDataset", "=", "args", ".", "dataset_class", "\n", "return", "CustomDataset", "(", "encoder_tokenizer", ",", "decoder_tokenizer", ",", "args", ",", "data", ",", "mode", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "model_type", "in", "[", "\"bart\"", ",", "\"marian\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "                ", "return", "SimpleSummarizationDataset", "(", "encoder_tokenizer", ",", "self", ".", "args", ",", "data", ",", "mode", ")", "\n", "", "else", ":", "\n", "                ", "return", "Seq2SeqDataset", "(", "encoder_tokenizer", ",", "decoder_tokenizer", ",", "self", ".", "args", ",", "data", ",", "mode", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._create_training_progress_scores": [[1041, 1052], ["None"], "methods", ["None"], ["", "", "", "def", "_create_training_progress_scores", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "extra_metrics", "=", "{", "key", ":", "[", "]", "for", "key", "in", "kwargs", "}", "\n", "training_progress_scores", "=", "{", "\n", "\"global_step\"", ":", "[", "]", ",", "\n", "\"eval_loss\"", ":", "[", "]", ",", "\n", "\"train_loss\"", ":", "[", "]", ",", "\n", "\"eval_acc\"", ":", "[", "]", ",", "\n", "**", "extra_metrics", ",", "\n", "}", "\n", "\n", "return", "training_progress_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_last_metrics": [[1053, 1055], ["metric_values.items"], "methods", ["None"], ["", "def", "_get_last_metrics", "(", "self", ",", "metric_values", ")", ":", "\n", "        ", "return", "{", "metric", ":", "values", "[", "-", "1", "]", "for", "metric", ",", "values", "in", "metric_values", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model": [[1056, 1104], ["os.makedirs", "logger.info", "seq2seq_model.Seq2SeqModel._save_model_args", "torch.save", "os.path.join", "hasattr", "os.makedirs", "model_to_save.save_pretrained", "seq2seq_model.Seq2SeqModel.config.save_pretrained", "os.makedirs", "os.makedirs", "seq2seq_model.Seq2SeqModel.encoder_config.save_pretrained", "seq2seq_model.Seq2SeqModel.decoder_config.save_pretrained", "model_to_save.save_pretrained", "model_to_save.save_pretrained", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.save_pretrained", "seq2seq_model.Seq2SeqModel.decoder_tokenizer.save_pretrained", "os.path.join", "torch.save", "torch.save", "open", "sorted", "os.path.join", "seq2seq_model.Seq2SeqModel.encoder_tokenizer.save_pretrained", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "hasattr", "os.path.join", "hasattr", "os.path.join", "os.path.join", "os.path.join", "optimizer.state_dict", "os.path.join", "scheduler.state_dict", "os.path.join", "results.keys", "writer.write", "str"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model_args"], ["", "def", "_save_model", "(", "self", ",", "output_dir", "=", "None", ",", "optimizer", "=", "None", ",", "scheduler", "=", "None", ",", "model", "=", "None", ",", "results", "=", "None", ")", ":", "\n", "        ", "if", "not", "output_dir", ":", "\n", "            ", "output_dir", "=", "self", ".", "args", ".", "output_dir", "\n", "", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "logger", ".", "info", "(", "f\"Saving model into {output_dir}\"", ")", "\n", "\n", "if", "model", "and", "not", "self", ".", "args", ".", "no_save", ":", "\n", "# Take care of distributed/parallel training", "\n", "            ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", "self", ".", "_save_model_args", "(", "output_dir", ")", "\n", "\n", "if", "self", ".", "args", ".", "model_type", "in", "[", "\"bart\"", ",", "\"marian\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "                ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "output_dir", ")", ",", "exist_ok", "=", "True", ")", "\n", "model_to_save", ".", "save_pretrained", "(", "output_dir", ")", "\n", "self", ".", "config", ".", "save_pretrained", "(", "output_dir", ")", "\n", "if", "self", ".", "args", ".", "model_type", "in", "[", "\"bart\"", ",", "\"blender\"", ",", "\"blender-large\"", "]", ":", "\n", "                    ", "self", ".", "encoder_tokenizer", ".", "save_pretrained", "(", "output_dir", ")", "\n", "", "", "else", ":", "\n", "                ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"encoder\"", ")", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"decoder\"", ")", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "encoder_config", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"encoder\"", ")", ")", "\n", "self", ".", "decoder_config", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"decoder\"", ")", ")", "\n", "\n", "model_to_save", "=", "(", "\n", "self", ".", "model", ".", "encoder", ".", "module", "if", "hasattr", "(", "self", ".", "model", ".", "encoder", ",", "\"module\"", ")", "else", "self", ".", "model", ".", "encoder", "\n", ")", "\n", "model_to_save", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"encoder\"", ")", ")", "\n", "\n", "model_to_save", "=", "(", "\n", "self", ".", "model", ".", "decoder", ".", "module", "if", "hasattr", "(", "self", ".", "model", ".", "decoder", ",", "\"module\"", ")", "else", "self", ".", "model", ".", "decoder", "\n", ")", "\n", "\n", "model_to_save", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"decoder\"", ")", ")", "\n", "\n", "self", ".", "encoder_tokenizer", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"encoder\"", ")", ")", "\n", "self", ".", "decoder_tokenizer", ".", "save_pretrained", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"decoder\"", ")", ")", "\n", "\n", "", "torch", ".", "save", "(", "self", ".", "args", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"training_args.bin\"", ")", ")", "\n", "if", "optimizer", "and", "scheduler", "and", "self", ".", "args", ".", "save_optimizer_and_scheduler", ":", "\n", "                ", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"scheduler.pt\"", ")", ")", "\n", "\n", "", "", "if", "results", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "for", "key", "in", "sorted", "(", "results", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "writer", ".", "write", "(", "\"{} = {}\\n\"", ".", "format", "(", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._move_model_to_device": [[1105, 1107], ["seq2seq_model.Seq2SeqModel.model.to"], "methods", ["None"], ["", "", "", "", "def", "_move_model_to_device", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._get_inputs_dict": [[1108, 1147], ["y[].contiguous", "y[].clone", "source_ids.to", "source_mask.to", "y[].contiguous.to", "y[].clone.to", "y[].contiguous", "y[].clone", "y[].clone.clone", "source_ids.to", "source_mask.to", "y[].contiguous.to", "y[].clone.to", "batch[].to", "y[].clone.to", "y[].clone.clone.to"], "methods", ["None"], ["", "def", "_get_inputs_dict", "(", "self", ",", "batch", ")", ":", "\n", "        ", "device", "=", "self", ".", "device", "\n", "if", "self", ".", "args", ".", "model_type", "in", "[", "\"marian\"", "]", ":", "\n", "            ", "pad_token_id", "=", "self", ".", "encoder_tokenizer", ".", "pad_token_id", "\n", "source_ids", ",", "source_mask", ",", "y", "=", "batch", "[", "\"source_ids\"", "]", ",", "batch", "[", "\"source_mask\"", "]", ",", "batch", "[", "\"target_ids\"", "]", "\n", "y_ids", "=", "y", "[", ":", ",", ":", "-", "1", "]", ".", "contiguous", "(", ")", "\n", "lm_labels", "=", "y", "[", ":", ",", "1", ":", "]", ".", "clone", "(", ")", "\n", "lm_labels", "[", "y", "[", ":", ",", "1", ":", "]", "==", "pad_token_id", "]", "=", "-", "100", "\n", "\n", "inputs", "=", "{", "\n", "\"input_ids\"", ":", "source_ids", ".", "to", "(", "device", ")", ",", "\n", "\"attention_mask\"", ":", "source_mask", ".", "to", "(", "device", ")", ",", "\n", "\"decoder_input_ids\"", ":", "y_ids", ".", "to", "(", "device", ")", ",", "\n", "\"lm_labels\"", ":", "lm_labels", ".", "to", "(", "device", ")", ",", "\n", "}", "\n", "", "elif", "self", ".", "args", ".", "model_type", "in", "[", "\"blender\"", ",", "\"bart\"", ",", "\"blender-large\"", "]", ":", "\n", "            ", "pad_token_id", "=", "self", ".", "encoder_tokenizer", ".", "pad_token_id", "\n", "source_ids", ",", "source_mask", ",", "y", "=", "batch", "[", "\"source_ids\"", "]", ",", "batch", "[", "\"source_mask\"", "]", ",", "batch", "[", "\"target_ids\"", "]", "\n", "y_ids", "=", "y", "[", ":", ",", ":", "-", "1", "]", ".", "contiguous", "(", ")", "\n", "labels", "=", "y", "[", ":", ",", "1", ":", "]", ".", "clone", "(", ")", "\n", "labels", "[", "y", "[", ":", ",", "1", ":", "]", "==", "pad_token_id", "]", "=", "-", "100", "\n", "inputs", "=", "{", "\n", "\"input_ids\"", ":", "source_ids", ".", "to", "(", "device", ")", ",", "\n", "\"attention_mask\"", ":", "source_mask", ".", "to", "(", "device", ")", ",", "\n", "\"decoder_input_ids\"", ":", "y_ids", ".", "to", "(", "device", ")", ",", "\n", "\"labels\"", ":", "labels", ".", "to", "(", "device", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "lm_labels", "=", "batch", "[", "1", "]", "\n", "lm_labels_masked", "=", "lm_labels", ".", "clone", "(", ")", "\n", "lm_labels_masked", "[", "lm_labels_masked", "==", "self", ".", "decoder_tokenizer", ".", "pad_token_id", "]", "=", "-", "100", "\n", "\n", "inputs", "=", "{", "\n", "\"input_ids\"", ":", "batch", "[", "0", "]", ".", "to", "(", "device", ")", ",", "\n", "\"decoder_input_ids\"", ":", "lm_labels", ".", "to", "(", "device", ")", ",", "\n", "\"labels\"", ":", "lm_labels_masked", ".", "to", "(", "device", ")", ",", "\n", "}", "\n", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._save_model_args": [[1148, 1151], ["os.makedirs", "seq2seq_model.Seq2SeqModel.args.save"], "methods", ["None"], ["", "def", "_save_model_args", "(", "self", ",", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "args", ".", "save", "(", "output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel._load_model_args": [[1152, 1156], ["simpletransformers.config.model_args.Seq2SeqArgs", "simpletransformers.config.model_args.Seq2SeqArgs.load"], "methods", ["None"], ["", "def", "_load_model_args", "(", "self", ",", "input_dir", ")", ":", "\n", "        ", "args", "=", "Seq2SeqArgs", "(", ")", "\n", "args", ".", "load", "(", "input_dir", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_model.Seq2SeqModel.get_named_parameters": [[1157, 1159], ["seq2seq_model.Seq2SeqModel.model.named_parameters"], "methods", ["None"], ["", "def", "get_named_parameters", "(", "self", ")", ":", "\n", "        ", "return", "[", "n", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.Seq2SeqDataset.__init__": [[32, 67], ["os.path.join", "logger.info", "os.path.exists", "logger.info", "logger.info", "open", "pickle.dump", "str", "open", "pickle.load", "zip", "multiprocessing.Pool", "list", "seq2seq_utils.preprocess_data", "str", "len", "tqdm.auto.tqdm.auto.tqdm", "tqdm.auto.tqdm.auto.tqdm", "p.imap", "len"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.preprocess_data"], ["    ", "def", "__init__", "(", "self", ",", "encoder_tokenizer", ",", "decoder_tokenizer", ",", "args", ",", "data", ",", "mode", ")", ":", "\n", "        ", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "cache_dir", ",", "args", ".", "model_name", "+", "\"_cached_\"", "+", "str", "(", "args", ".", "max_seq_length", ")", "+", "str", "(", "len", "(", "data", ")", ")", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", "and", "(", "\n", "(", "not", "args", ".", "reprocess_input_data", "and", "not", "args", ".", "no_cache", ")", "\n", "or", "(", "mode", "==", "\"dev\"", "and", "args", ".", "use_cached_eval_features", "and", "not", "args", ".", "no_cache", ")", "\n", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\" Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "                ", "self", ".", "examples", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\" Creating features from dataset file at %s\"", ",", "args", ".", "cache_dir", ")", "\n", "\n", "", "data", "=", "[", "\n", "(", "input_text", ",", "target_text", ",", "encoder_tokenizer", ",", "decoder_tokenizer", ",", "args", ")", "\n", "for", "input_text", ",", "target_text", "in", "zip", "(", "data", "[", "\"input_text\"", "]", ",", "data", "[", "\"target_text\"", "]", ")", "\n", "]", "\n", "\n", "if", "args", ".", "use_multiprocessing", ":", "\n", "            ", "with", "Pool", "(", "args", ".", "process_count", ")", "as", "p", ":", "\n", "                ", "self", ".", "examples", "=", "list", "(", "\n", "tqdm", "(", "\n", "p", ".", "imap", "(", "preprocess_data", ",", "data", ",", "chunksize", "=", "args", ".", "multiprocessing_chunksize", ")", ",", "\n", "total", "=", "len", "(", "data", ")", ",", "\n", "disable", "=", "args", ".", "silent", ",", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "examples", "=", "[", "preprocess_data", "(", "d", ")", "for", "d", "in", "tqdm", "(", "data", ",", "disable", "=", "args", ".", "silent", ")", "]", "\n", "\n", "", "logger", ".", "info", "(", "\" Saving features into cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "examples", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.Seq2SeqDataset.__len__": [[68, 70], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.Seq2SeqDataset.__getitem__": [[71, 73], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "examples", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__init__": [[94, 126], ["os.path.join", "os.path.exists", "logger.info", "logger.info", "str", "open", "pickle.load", "zip", "multiprocessing.Pool", "list", "seq2seq_utils.preprocess_data_bart", "str", "len", "tqdm.auto.tqdm.auto.tqdm", "tqdm.auto.tqdm.auto.tqdm", "p.imap", "len"], "methods", ["home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.preprocess_data_bart"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "args", ",", "data", ",", "mode", ")", ":", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "cache_dir", ",", "args", ".", "model_name", "+", "\"_cached_\"", "+", "str", "(", "args", ".", "max_seq_length", ")", "+", "str", "(", "len", "(", "data", ")", ")", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "cached_features_file", ")", "and", "(", "\n", "(", "not", "args", ".", "reprocess_input_data", "and", "not", "args", ".", "no_cache", ")", "\n", "or", "(", "mode", "==", "\"dev\"", "and", "args", ".", "use_cached_eval_features", "and", "not", "args", ".", "no_cache", ")", "\n", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\" Loading features from cached file %s\"", ",", "cached_features_file", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "                ", "self", ".", "examples", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\" Creating features from dataset file at %s\"", ",", "args", ".", "cache_dir", ")", "\n", "", "data", "=", "[", "\n", "(", "input_text", ",", "target_text", ",", "tokenizer", ",", "args", ")", "\n", "for", "input_text", ",", "target_text", "in", "zip", "(", "data", "[", "\"input_text\"", "]", ",", "data", "[", "\"target_text\"", "]", ")", "\n", "]", "\n", "\n", "if", "args", ".", "use_multiprocessing", ":", "\n", "            ", "with", "Pool", "(", "args", ".", "process_count", ")", "as", "p", ":", "\n", "                ", "self", ".", "examples", "=", "list", "(", "\n", "tqdm", "(", "\n", "p", ".", "imap", "(", "preprocess_data_bart", ",", "data", ",", "chunksize", "=", "args", ".", "multiprocessing_chunksize", ")", ",", "\n", "total", "=", "len", "(", "data", ")", ",", "\n", "disable", "=", "args", ".", "silent", ",", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "examples", "=", "[", "preprocess_data_bart", "(", "d", ")", "for", "d", "in", "tqdm", "(", "data", ",", "disable", "=", "args", ".", "silent", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__len__": [[127, 129], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.SimpleSummarizationDataset.__getitem__": [[130, 132], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "examples", "[", "index", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.preprocess_data": [[18, 29], ["encoder_tokenizer.encode", "decoder_tokenizer.encode", "torch.flatten", "torch.flatten"], "function", ["None"], ["def", "preprocess_data", "(", "data", ")", ":", "\n", "    ", "input_text", ",", "target_text", ",", "encoder_tokenizer", ",", "decoder_tokenizer", ",", "args", "=", "data", "\n", "\n", "input_text", "=", "encoder_tokenizer", ".", "encode", "(", "\n", "input_text", ",", "max_length", "=", "args", ".", "max_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "\n", "\n", "target_text", "=", "decoder_tokenizer", ".", "encode", "(", "\n", "target_text", ",", "max_length", "=", "args", ".", "max_seq_length", ",", "pad_to_max_length", "=", "True", ",", "return_tensors", "=", "\"pt\"", "\n", ")", "\n", "return", "(", "torch", ".", "flatten", "(", "input_text", ")", ",", "torch", ".", "flatten", "(", "target_text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.None.seq2seq_utils.preprocess_data_bart": [[75, 90], ["tokenizer.batch_encode_plus", "tokenizer.batch_encode_plus", "input_ids[].squeeze", "input_ids[].squeeze", "target_ids[].squeeze"], "function", ["None"], ["", "", "def", "preprocess_data_bart", "(", "data", ")", ":", "\n", "    ", "input_text", ",", "target_text", ",", "tokenizer", ",", "args", "=", "data", "\n", "\n", "input_ids", "=", "tokenizer", ".", "batch_encode_plus", "(", "\n", "[", "input_text", "]", ",", "max_length", "=", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", ",", "\n", ")", "\n", "\n", "target_ids", "=", "tokenizer", ".", "batch_encode_plus", "(", "\n", "[", "target_text", "]", ",", "max_length", "=", "args", ".", "max_seq_length", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "return_tensors", "=", "\"pt\"", "\n", ")", "\n", "\n", "return", "{", "\n", "\"source_ids\"", ":", "input_ids", "[", "\"input_ids\"", "]", ".", "squeeze", "(", ")", ",", "\n", "\"source_mask\"", ":", "input_ids", "[", "\"attention_mask\"", "]", ".", "squeeze", "(", ")", ",", "\n", "\"target_ids\"", ":", "target_ids", "[", "\"input_ids\"", "]", ".", "squeeze", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.distinct": [[15, 30], ["collections.Counter", "collections.Counter", "hypo.split", "unigram_counter.update", "bigram_counter.update", "len", "sum", "len", "sum", "nltk.ngrams", "unigram_counter.values", "bigram_counter.values"], "function", ["None"], ["def", "distinct", "(", "hypothesis", ")", ":", "\n", "    ", "'''\n    compute distinct metric\n    :param hypothesis: list of str\n    :return:\n    '''", "\n", "unigram_counter", ",", "bigram_counter", "=", "Counter", "(", ")", ",", "Counter", "(", ")", "\n", "for", "hypo", "in", "hypothesis", ":", "\n", "        ", "tokens", "=", "hypo", ".", "split", "(", ")", "\n", "unigram_counter", ".", "update", "(", "tokens", ")", "\n", "bigram_counter", ".", "update", "(", "ngrams", "(", "tokens", ",", "2", ")", ")", "\n", "\n", "", "distinct_1", "=", "len", "(", "unigram_counter", ")", "/", "sum", "(", "unigram_counter", ".", "values", "(", ")", ")", "#\u8d8a\u5927\u8d8a\u597d \u4e0d\u540c\u7684\u8bcd\u7ec4", "\n", "distinct_2", "=", "len", "(", "bigram_counter", ")", "/", "sum", "(", "bigram_counter", ".", "values", "(", ")", ")", "\n", "return", "distinct_1", ",", "distinct_2", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer": [[36, 52], ["metrics.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re_art", ".", "sub", "(", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "return", "re_punc", ".", "sub", "(", "' '", ",", "text", ")", "# convert punctuation to spaces", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._prec_recall_f1_score": [[54, 71], ["sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["None"], ["", "def", "_prec_recall_f1_score", "(", "pred_items", ",", "gold_items", ")", ":", "\n", "    ", "\"\"\"\n    Compute precision, recall and f1 given a set of gold and prediction items.\n    :param pred_items: iterable of predicted values\n    :param gold_items: iterable of gold values\n    :return: tuple (p, r, f1) for precision, recall, f1\n    \"\"\"", "\n", "common", "=", "Counter", "(", "gold_items", ")", "&", "Counter", "(", "pred_items", ")", "\n", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", ",", "0", ",", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_items", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "gold_items", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "precision", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._f1_score": [[73, 82], ["normalize_answer().split", "metrics._prec_recall_f1_score", "max", "max", "max", "metrics.normalize_answer", "normalize_answer().split", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._prec_recall_f1_score", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer"], ["", "def", "_f1_score", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Return the max F1 score between the guess and *any* answer.\"\"\"", "\n", "if", "guess", "is", "None", "or", "answers", "is", "None", ":", "\n", "        ", "return", "0", "\n", "", "g_tokens", "=", "normalize_answer", "(", "guess", ")", ".", "split", "(", ")", "\n", "scores", "=", "[", "\n", "_prec_recall_f1_score", "(", "g_tokens", ",", "normalize_answer", "(", "a", ")", ".", "split", "(", ")", ")", "for", "a", "in", "answers", "\n", "]", "\n", "return", "max", "(", "f1", "for", "_", ",", "_", ",", "f1", "in", "scores", ")", ",", "max", "(", "pre", "for", "pre", ",", "_", ",", "_", "in", "scores", ")", ",", "max", "(", "rec", "for", "_", ",", "rec", ",", "_", "in", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.f_one": [[84, 100], ["zip", "metrics._f1_score", "f1.append", "pre.append", "rec.append", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._f1_score"], ["", "def", "f_one", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "'''\n    calculate f1 metric\n    :param hypothesis: list of str\n    :param references: list of str\n    :return:\n    '''", "\n", "f1", "=", "[", "]", "\n", "pre", "=", "[", "]", "\n", "rec", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "res", "=", "_f1_score", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "f1", ".", "append", "(", "res", "[", "0", "]", ")", "\n", "pre", ".", "append", "(", "res", "[", "1", "]", ")", "\n", "rec", ".", "append", "(", "res", "[", "2", "]", ")", "\n", "", "return", "np", ".", "mean", "(", "f1", ")", ",", "np", ".", "mean", "(", "pre", ")", ",", "np", ".", "mean", "(", "rec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu1": [[102, 118], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer"], ["", "def", "_bleu1", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu2": [[120, 136], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer"], ["", "def", "_bleu2", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu3": [[138, 154], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer"], ["", "def", "_bleu3", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu4": [[156, 172], ["nltk.translate.bleu_score.sentence_bleu", "normalize_answer().split", "normalize_answer().split", "metrics.normalize_answer", "nltk.translate.bleu_score.SmoothingFunction", "metrics.normalize_answer"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.normalize_answer"], ["", "def", "_bleu4", "(", "guess", ",", "answers", ")", ":", "\n", "    ", "\"\"\"Compute approximate BLEU score between guess and a set of answers.\"\"\"", "\n", "if", "nltkbleu", "is", "None", ":", "\n", "# bleu library not installed, just return a default value", "\n", "        ", "return", "None", "\n", "# Warning: BLEU calculation *should* include proper tokenization and", "\n", "# punctuation etc. We're using the normalize_answer for everything though,", "\n", "# so we're over-estimating our BLEU scores.  Also note that NLTK's bleu is", "\n", "# going to be slower than fairseq's (which is written in C), but fairseq's", "\n", "# requires that everything be in arrays of ints (i.e. as tensors). NLTK's", "\n", "# works with strings, which is better suited for this module.", "\n", "", "return", "nltkbleu", ".", "sentence_bleu", "(", "\n", "[", "normalize_answer", "(", "a", ")", ".", "split", "(", "\" \"", ")", "for", "a", "in", "answers", "]", ",", "\n", "normalize_answer", "(", "guess", ")", ".", "split", "(", "\" \"", ")", ",", "\n", "weights", "=", "(", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ",", "1.0", "/", "4.0", ")", ",", "\n", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.bleu": [[174, 183], ["zip", "numpy.mean", "tuple", "metrics._bleu1", "metrics._bleu2", "metrics._bleu3", "np.mean.append"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu1", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu2", "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._bleu3"], ["", "def", "bleu", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "bleu_scores", "=", "[", "]", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypothesis", ",", "references", ")", ":", "\n", "        ", "b1", "=", "_bleu1", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b2", "=", "_bleu2", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "b3", "=", "_bleu3", "(", "hyp", ",", "[", "ref", "]", ")", "\n", "bleu_scores", ".", "append", "(", "[", "b1", ",", "b2", ",", "b3", "]", ")", "\n", "", "bleu_scores", "=", "np", ".", "mean", "(", "bleu_scores", ",", "axis", "=", "0", ")", "# [bleu1, bleu2, bleu3]", "\n", "return", "tuple", "(", "bleu_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.bleu_corpus": [[184, 196], ["hypothesis.copy.copy", "references.copy.copy", "corpus_bleu", "corpus_bleu", "corpus_bleu", "hyp.split", "ref.split", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["None"], ["", "def", "bleu_corpus", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "from", "nltk", ".", "translate", ".", "bleu_score", "import", "corpus_bleu", "\n", "hypothesis", "=", "hypothesis", ".", "copy", "(", ")", "\n", "references", "=", "references", ".", "copy", "(", ")", "\n", "hypothesis", "=", "[", "hyp", ".", "split", "(", ")", "for", "hyp", "in", "hypothesis", "]", "\n", "references", "=", "[", "[", "ref", ".", "split", "(", ")", "]", "for", "ref", "in", "references", "]", "\n", "# hypothesis = [normalize_answer(hyp).split(\" \") for hyp in hypothesis]", "\n", "# references = [[normalize_answer(ref).split(\" \")] for ref in references]", "\n", "b1", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "1.0", ",", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b2", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "2.0", ",", "1.0", "/", "2.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "b3", "=", "corpus_bleu", "(", "references", ",", "hypothesis", ",", "weights", "=", "(", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ",", "1.0", "/", "3.0", ")", ",", "smoothing_function", "=", "nltkbleu", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ")", "\n", "return", "(", "b1", ",", "b2", ",", "b3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.bleu_metric": [[197, 199], ["metrics.bleu_corpus"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.bleu_corpus"], ["", "def", "bleu_metric", "(", "hypothesis", ",", "references", ")", ":", "\n", "    ", "return", "bleu_corpus", "(", "hypothesis", ",", "references", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.knowledge_metric": [[201, 232], ["get_stop_words", "zip", "metrics._prec_recall_f1_score", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "hyp.split", "know.split"], "function", ["home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics._prec_recall_f1_score"], ["", "def", "knowledge_metric", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "# hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "# know = ' '.join(know)", "\n", "# know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "# if len(hyp_tokens & know_tokens) == 0:", "\n", "#     _p, _r, _f1 = .0, .0, .0", "\n", "# else:", "\n", "#     _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#     _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#     _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "\n", "# hyp_tokens = list(set([w for w in hyp.split() if w not in stop_words]))", "\n", "        ", "hyp_tokens", "=", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", "\n", "_p", ",", "_r", ",", "_f1", "=", "_prec_recall_f1_score", "(", "hyp_tokens", ",", "know_tokens", ")", "\n", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.metrics.knowledge_metric_new": [[233, 279], ["get_stop_words", "zip", "set", "set", "p_scores.append", "r_scores.append", "f_scores.append", "numpy.mean", "numpy.mean", "numpy.mean", "len", "len", "len", "len", "len", "hyp.split", "know.split"], "function", ["None"], ["", "def", "knowledge_metric_new", "(", "responses", ",", "knowledges", ")", ":", "\n", "    ", "'''\n    calculate knowledge metric\n    :param responses: list of str\n    :param knowledges: list of list of str\n    :return:\n    '''", "\n", "# stop_words = get_stop_words('en')", "\n", "# p_scores,  r_scores, f_scores = [], [], []", "\n", "# for hyp, know in zip(responses, knowledges):", "\n", "#     hyp_tokens = set([w for w in hyp.split() if w not in stop_words])", "\n", "#     know = ' '.join(know)", "\n", "#     know_tokens = set([w for w in know.split() if w not in stop_words])", "\n", "#", "\n", "#     if len(hyp_tokens & know_tokens) == 0:", "\n", "#         _p, _r, _f1 = .0, .0, .0", "\n", "#     else:", "\n", "#         _p = len(hyp_tokens & know_tokens) / len(hyp_tokens)", "\n", "#         _r = len(hyp_tokens & know_tokens) / len(know_tokens)", "\n", "#         _f1 = 2 * (_p * _r) / (_p + _r)", "\n", "#", "\n", "#     p_scores.append(_p)", "\n", "#     r_scores.append(_r)", "\n", "#     f_scores.append(_f1)", "\n", "#", "\n", "# return np.mean(r_scores), np.mean(p_scores),  np.mean(f_scores)", "\n", "\n", "stop_words", "=", "get_stop_words", "(", "'en'", ")", "\n", "p_scores", ",", "r_scores", ",", "f_scores", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "hyp", ",", "know", "in", "zip", "(", "responses", ",", "knowledges", ")", ":", "\n", "        ", "hyp_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "hyp", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "know", "=", "' '", ".", "join", "(", "know", ")", "\n", "know_tokens", "=", "set", "(", "[", "w", "for", "w", "in", "know", ".", "split", "(", ")", "if", "w", "not", "in", "stop_words", "]", ")", "\n", "\n", "if", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "==", "0", ":", "\n", "            ", "_p", ",", "_r", ",", "_f1", "=", ".0", ",", ".0", ",", ".0", "\n", "", "else", ":", "\n", "            ", "_p", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "hyp_tokens", ")", "\n", "_r", "=", "len", "(", "hyp_tokens", "&", "know_tokens", ")", "/", "len", "(", "know_tokens", ")", "\n", "_f1", "=", "2", "*", "(", "_p", "*", "_r", ")", "/", "(", "_p", "+", "_r", ")", "\n", "\n", "", "p_scores", ".", "append", "(", "_p", ")", "\n", "r_scores", ".", "append", "(", "_r", ")", "\n", "f_scores", ".", "append", "(", "_f1", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "r_scores", ")", ",", "np", ".", "mean", "(", "p_scores", ")", ",", "np", ".", "mean", "(", "f_scores", ")", "", "", ""]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.eval.read_wizard_json": [[10, 28], ["open", "json.load", "data.append"], "function", ["None"], ["def", "read_wizard_json", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# print(file[0].keys())", "\n", "# print(file[0]['dialog'])", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "        ", "tmp_source", "=", "''", "\n", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "if", "tmp_source", "!=", "''", ":", "\n", "                ", "data", ".", "append", "(", "[", "tmp_source", ",", "utt", "]", ")", "\n", "# add split '\\n' for blender", "\n", "tmp_source", "=", "tmp_source", "+", "\"\\t\"", "+", "utt", "\n", "", "else", ":", "\n", "                ", "tmp_source", "=", "utt", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.eval.move_stop_words": [[33, 36], ["str.split", "w.lower"], "function", ["None"], ["def", "move_stop_words", "(", "str", ")", ":", "\n", "    ", "item", "=", "\" \"", ".", "join", "(", "[", "w", "for", "w", "in", "str", ".", "split", "(", ")", "if", "not", "w", ".", "lower", "(", ")", "in", "stop_words", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.eval.detokenize": [[38, 47], ["tk_str.strip().split", "tk_str.strip", "tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_str", ")", ":", "\n", "    ", "tk_list", "=", "tk_str", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "\" \"", ".", "join", "(", "r_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.eval.cloze.read_wizard_json": [[4, 23], ["open", "json.load", "data.append"], "function", ["None"], ["def", "read_wizard_json", "(", "file_path", ")", ":", "\n", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# print(file[0].keys())", "\n", "# print(file[0]['dialog'])", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "        ", "tmp_source", "=", "''", "\n", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "if", "tmp_source", "!=", "''", ":", "\n", "                ", "data", ".", "append", "(", "[", "tmp_source", ",", "utt", "]", ")", "\n", "# add split '\\n' for blender", "\n", "tmp_source", "=", "tmp_source", "+", "\"\\t\"", "+", "utt", "\n", "", "else", ":", "\n", "                ", "tmp_source", "=", "utt", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.nealcly_ke-blender.preprocess.find_hyper.read_wizard_hyper_json": [[8, 45], ["open", "json.load", "[].lower", "utt.lower", "list", "data.append", "j.keys", "len", "re.sub", "re.sub", "list", "know_key.split", "know_key.split", "[].name().split", "[].name().split", "j.keys", "know_key.split", "replace_word.split", "replace_word.split", "[].name", "[].name", "[].hypernyms", "[].hypernyms", "nltk.corpus.wordnet.synsets", "nltk.corpus.wordnet.synsets", "word.lower", "word.lower"], "function", ["None"], ["def", "read_wizard_hyper_json", "(", "file_path", ")", ":", "\n", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "file", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "# print(file[0].keys())", "\n", "# print(file[0]['dialog'])", "\n", "", "data", "=", "[", "]", "\n", "for", "line", "in", "file", ":", "\n", "\n", "        ", "for", "i", "in", "line", "[", "'dialog'", "]", ":", "\n", "            ", "utt", "=", "i", "[", "'text'", "]", "\n", "external_passage", "=", "i", "[", "'retrieved_passages'", "]", "\n", "for", "j", "in", "external_passage", ":", "\n", "# print(list(j.keys()))", "\n", "                ", "if", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", ".", "lower", "(", ")", "in", "utt", ".", "lower", "(", ")", ":", "\n", "                    ", "know_key", "=", "list", "(", "j", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "# print(know_key)", "\n", "try", ":", "\n", "                        ", "if", "len", "(", "know_key", ".", "split", "(", "' '", ")", ")", ">", "1", ":", "\n", "                            ", "word", "=", "know_key", ".", "split", "(", "' '", ")", "[", "-", "1", "]", "\n", "ori", "=", "(", "' '", ")", ".", "join", "(", "know_key", ".", "split", "(", "' '", ")", "[", ":", "-", "1", "]", ")", "\n", "replace_word", "=", "wn", ".", "synsets", "(", "word", ".", "lower", "(", ")", ")", "[", "0", "]", ".", "hypernyms", "(", ")", "[", "0", "]", ".", "name", "(", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "\"_\"", "in", "replace_word", ":", "\n", "                                ", "replace_word", "=", "(", "' '", ")", ".", "join", "(", "replace_word", ".", "split", "(", "'_'", ")", ")", "\n", "", "replace_word", "=", "ori", "+", "' '", "+", "replace_word", "\n", "utt_mask", "=", "re", ".", "sub", "(", "know_key", ",", "replace_word", ",", "utt", ",", "flags", "=", "re", ".", "IGNORECASE", ")", "\n", "", "else", ":", "\n", "                            ", "word", "=", "know_key", "\n", "replace_word", "=", "wn", ".", "synsets", "(", "word", ".", "lower", "(", ")", ")", "[", "0", "]", ".", "hypernyms", "(", ")", "[", "0", "]", ".", "name", "(", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "\"_\"", "in", "replace_word", ":", "\n", "                                ", "replace_word", "=", "(", "' '", ")", ".", "join", "(", "replace_word", ".", "split", "(", "'_'", ")", ")", "\n", "", "utt_mask", "=", "re", ".", "sub", "(", "know_key", ",", "replace_word", ",", "utt", ",", "flags", "=", "re", ".", "IGNORECASE", ")", "\n", "", "data", ".", "append", "(", "[", "utt", ",", "utt_mask", "]", ")", "\n", "", "except", ":", "\n", "                        ", "continue", "\n", "", "", "", "", "", "return", "data", "\n", "\n"]]}