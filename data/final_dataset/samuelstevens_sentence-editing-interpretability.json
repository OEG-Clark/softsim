{"home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.CsvRow.__str__": [[42, 49], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "row", "=", "f\"{self.filepath},{self.epoch},{self.seen},{self.total_train_loss}\"", "\n", "\n", "if", "self", ".", "avg_val_loss", "is", "not", "None", ":", "\n", "            ", "row", "+=", "f\",{self.avg_val_loss}\"", "\n", "\n", "", "return", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.CsvRow.parse_raw": [[50, 53], ["disk.CsvRow.parse", "s.split"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.Score.parse"], ["", "@", "staticmethod", "\n", "def", "parse_raw", "(", "s", ":", "str", ")", "->", "\"Result[CsvRow]\"", ":", "\n", "        ", "return", "CsvRow", ".", "parse", "(", "s", ".", "split", "(", "\",\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.CsvRow.parse": [[54, 73], ["len", "disk.CsvRow", "len", "disk.CsvRow", "ValueError", "pathlib.Path", "int", "int", "float", "float", "pathlib.Path", "int", "int", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "parse", "(", "row", ":", "List", "[", "str", "]", ")", "->", "\"Result[CsvRow]\"", ":", "\n", "        ", "if", "len", "(", "row", ")", "==", "5", ":", "\n", "            ", "filepath", ",", "epoch", ",", "seen", ",", "train_loss", ",", "val_loss", "=", "row", "\n", "return", "CsvRow", "(", "\n", "Path", "(", "filepath", ")", ",", "\n", "int", "(", "epoch", ")", ",", "\n", "int", "(", "seen", ")", ",", "\n", "float", "(", "train_loss", ")", ",", "\n", "float", "(", "val_loss", ")", ",", "\n", ")", "\n", "", "if", "len", "(", "row", ")", "==", "4", ":", "\n", "            ", "filepath", ",", "epoch", ",", "seen", ",", "train_loss", "=", "row", "\n", "return", "CsvRow", "(", "\n", "Path", "(", "filepath", ")", ",", "int", "(", "epoch", ")", ",", "int", "(", "seen", ")", ",", "float", "(", "train_loss", ")", ",", "None", "\n", ")", "\n", "\n", "", "else", ":", "\n", "            ", "return", "ValueError", "(", "f\"row {row} does not contain 4 or 5 elements\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.read_checkpoint_file": [[75, 96], ["os.path.isfile", "FileNotFoundError", "open", "csv.reader", "disk.CsvRow.parse", "isinstance", "result.append"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.Score.parse"], ["", "", "", "def", "read_checkpoint_file", "(", "checkpoint_csv_filepath", ":", "Path", ")", "->", "Result", "[", "List", "[", "CsvRow", "]", "]", ":", "\n", "    ", "\"\"\"\n    Reads a checkpoint file and returns the data stored in the file.\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_csv_filepath", ")", ":", "\n", "        ", "return", "FileNotFoundError", "(", "\"no checkpoint file\"", ")", "\n", "\n", "", "result", ":", "List", "[", "CsvRow", "]", "=", "[", "]", "\n", "\n", "with", "open", "(", "checkpoint_csv_filepath", ",", "\"r\"", ")", "as", "file", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "file", ")", "\n", "try", ":", "\n", "            ", "for", "row", "in", "reader", ":", "\n", "                ", "parsed", "=", "CsvRow", ".", "parse", "(", "row", ")", "\n", "if", "isinstance", "(", "parsed", ",", "Exception", ")", ":", "\n", "                    ", "return", "parsed", "\n", "", "result", ".", "append", "(", "parsed", ")", "\n", "", "", "except", "Exception", "as", "err", ":", "\n", "            ", "return", "err", "\n", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.find_latest_checkpoint": [[98, 114], ["disk.read_checkpoint_file", "isinstance", "sorted", "sorted", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.read_checkpoint_file"], ["", "def", "find_latest_checkpoint", "(", "checkpoint_csv_filepath", ":", "Path", ")", "->", "Result", "[", "CsvRow", "]", ":", "\n", "    ", "\"\"\"\n    returns a tuple of checkpoint filepath, epoch, seen, train_loss, val_loss\n    \"\"\"", "\n", "rows", "=", "read_checkpoint_file", "(", "checkpoint_csv_filepath", ")", "\n", "\n", "if", "isinstance", "(", "rows", ",", "Exception", ")", ":", "\n", "        ", "return", "ValueError", "(", "f\"couldn't find latest checkpoint -> {rows}\"", ")", "\n", "\n", "", "if", "not", "rows", ":", "\n", "        ", "return", "ValueError", "(", "\"no rows in checkpoint file\"", ")", "\n", "\n", "", "sorted_by_seen", "=", "sorted", "(", "rows", ",", "key", "=", "lambda", "row", ":", "row", ".", "seen", ",", "reverse", "=", "True", ")", "\n", "sorted_by_epoch", "=", "sorted", "(", "sorted_by_seen", ",", "key", "=", "lambda", "row", ":", "row", ".", "epoch", ",", "reverse", "=", "True", ")", "\n", "\n", "return", "sorted_by_epoch", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.lookup_csvrow_by_checkpoint_path": [[116, 139], ["disk.read_checkpoint_file", "isinstance", "ValueError", "checkpoint_filepath.resolve", "ValueError", "row.filepath.resolve", "checkpoint_filepath.resolve", "checkpoint_filepath.resolve"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.read_checkpoint_file"], ["", "def", "lookup_csvrow_by_checkpoint_path", "(", "\n", "config", ":", "HyperParameters", ",", "checkpoint_filepath", ":", "Path", "\n", ")", "->", "Result", "[", "CsvRow", "]", ":", "\n", "\n", "    ", "rows", "=", "read_checkpoint_file", "(", "config", ".", "checkpoint_csv", ")", "\n", "\n", "if", "isinstance", "(", "rows", ",", "Exception", ")", ":", "\n", "        ", "return", "rows", "\n", "\n", "", "if", "not", "rows", ":", "\n", "        ", "return", "ValueError", "(", "\"no rows in checkpoint file\"", ")", "\n", "\n", "", "lookup", ":", "Dict", "[", "Path", ",", "CsvRow", "]", "=", "{", "}", "\n", "\n", "for", "row", "in", "rows", ":", "\n", "        ", "lookup", "[", "row", ".", "filepath", ".", "resolve", "(", ")", "]", "=", "row", "\n", "\n", "", "if", "checkpoint_filepath", ".", "resolve", "(", ")", "not", "in", "lookup", ":", "\n", "        ", "return", "ValueError", "(", "\n", "f\"checkpoint '{checkpoint_filepath.resolve()}' not found in {config.checkpoint_csv}\"", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "lookup", "[", "checkpoint_filepath", ".", "resolve", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.new_checkpoint": [[141, 155], ["models.get_pretrained_model", "models.get_pretrained_model.get_optimizer", "disk.Checkpoint", "models.get_pretrained_model.cuda"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.get_pretrained_model", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.SentenceClassificationModel.get_optimizer"], ["", "", "def", "new_checkpoint", "(", "config", ":", "HyperParameters", ")", "->", "Checkpoint", ":", "\n", "    ", "epoch", "=", "0", "\n", "seen", "=", "0", "\n", "loss", "=", "0.0", "\n", "\n", "model", "=", "models", ".", "get_pretrained_model", "(", "config", ")", "\n", "\n", "try", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "", "except", ":", "\n", "        ", "pass", "\n", "", "optimizer", "=", "model", ".", "get_optimizer", "(", "config", ")", "\n", "\n", "return", "Checkpoint", "(", "model", ",", "optimizer", ",", "epoch", ",", "seen", ",", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk": [[157, 179], ["models.get_pretrained_model", "models.get_pretrained_model.get_optimizer", "models.get_pretrained_model.load_state_dict", "models.get_pretrained_model.to", "model.get_optimizer.load_state_dict", "torch.cuda.is_available", "util.get_device", "torch.load", "torch.load", "torch.device"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.get_pretrained_model", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.SentenceClassificationModel.get_optimizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device"], ["", "def", "load_finetuned_model_from_disk", "(", "\n", "config", ":", "HyperParameters", ",", "filepath", ":", "Path", "\n", ")", "->", "Result", "[", "Tuple", "[", "models", ".", "SentenceClassificationModel", ",", "torch", ".", "optim", ".", "Optimizer", "]", "]", ":", "\n", "    ", "disk_model", ":", "DiskModel", "\n", "\n", "try", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "disk_model", "=", "torch", ".", "load", "(", "filepath", ")", "# type: ignore", "\n", "", "else", ":", "\n", "            ", "disk_model", "=", "torch", ".", "load", "(", "filepath", ",", "map_location", "=", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "# type: ignore", "\n", "", "", "except", "Exception", "as", "err", ":", "\n", "        ", "return", "err", "\n", "\n", "", "model", "=", "models", ".", "get_pretrained_model", "(", "config", ")", "\n", "optimizer", "=", "model", ".", "get_optimizer", "(", "config", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "disk_model", "[", "\"model_state_dict\"", "]", ")", "\n", "model", ".", "to", "(", "util", ".", "get_device", "(", ")", ")", "\n", "\n", "optimizer", ".", "load_state_dict", "(", "disk_model", "[", "\"optimizer_state_dict\"", "]", ")", "\n", "\n", "return", "model", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_checkpoint": [[181, 200], ["disk.load_finetuned_model_from_disk", "isinstance", "disk.lookup_csvrow_by_checkpoint_path", "isinstance", "disk.Checkpoint"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.lookup_csvrow_by_checkpoint_path"], ["", "def", "load_checkpoint", "(", "config", ":", "HyperParameters", ",", "filepath", ":", "Path", ")", "->", "Result", "[", "Checkpoint", "]", ":", "\n", "    ", "disk_model", "=", "load_finetuned_model_from_disk", "(", "config", ",", "filepath", ")", "\n", "if", "isinstance", "(", "disk_model", ",", "Exception", ")", ":", "\n", "        ", "return", "disk_model", "\n", "\n", "", "model", ",", "optimizer", "=", "disk_model", "\n", "\n", "checkpoint_info", "=", "lookup_csvrow_by_checkpoint_path", "(", "config", ",", "filepath", ")", "\n", "\n", "if", "isinstance", "(", "checkpoint_info", ",", "Exception", ")", ":", "\n", "        ", "return", "checkpoint_info", "\n", "\n", "", "return", "Checkpoint", "(", "\n", "model", ",", "\n", "optimizer", ",", "\n", "checkpoint_info", ".", "epoch", ",", "\n", "checkpoint_info", ".", "seen", ",", "\n", "checkpoint_info", ".", "total_train_loss", ",", "\n", "avg_val_loss", "=", "checkpoint_info", ".", "avg_val_loss", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_training_history": [[203, 213], ["disk.read_checkpoint_file", "isinstance"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.read_checkpoint_file"], ["", "def", "load_training_history", "(", "config", ":", "HyperParameters", ")", "->", "Result", "[", "List", "[", "Tuple", "[", "Path", ",", "float", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    Returns tuples of a checkpoint and validation loss\n    \"\"\"", "\n", "rows", "=", "read_checkpoint_file", "(", "config", ".", "checkpoint_csv", ")", "\n", "\n", "if", "isinstance", "(", "rows", ",", "Exception", ")", ":", "\n", "        ", "return", "rows", "\n", "", "else", ":", "\n", "        ", "return", "[", "(", "row", ".", "filepath", ",", "row", ".", "avg_val_loss", ")", "for", "row", "in", "rows", "if", "row", ".", "avg_val_loss", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_latest_checkpoint": [[215, 235], ["disk.find_latest_checkpoint", "isinstance", "ValueError", "os.path.isfile", "disk.load_checkpoint", "disk.new_checkpoint"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.find_latest_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.new_checkpoint"], ["", "", "def", "load_latest_checkpoint", "(", "config", ":", "HyperParameters", ")", "->", "Result", "[", "Checkpoint", "]", ":", "\n", "    ", "\"\"\"\n    Loads the model using:\n    * hyperparams (using params.toml)\n    * checkpoint folder\n\n    Returns the model and the optimizer and the current epoch, # of seen examples and the current loss\n    \"\"\"", "\n", "\n", "# read checkpoints.csv to see if there are any existing checkpoints", "\n", "result", "=", "find_latest_checkpoint", "(", "config", ".", "checkpoint_csv", ")", "\n", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "        ", "return", "ValueError", "(", "f\"no latest checkpoint to load -> {result}\"", ")", "\n", "", "else", ":", "\n", "        ", "if", "os", ".", "path", ".", "isfile", "(", "result", ".", "filepath", ")", ":", "\n", "# load from filepath", "\n", "            ", "return", "load_checkpoint", "(", "config", ",", "result", ".", "filepath", ")", "\n", "", "else", ":", "\n", "            ", "return", "new_checkpoint", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.save_checkpoint": [[237, 265], ["os.makedirs", "torch.save", "pathlib.Path", "checkpoint.model.state_dict", "checkpoint.optimizer.state_dict", "open", "disk.CsvRow", "file.write", "uuid.uuid4"], "function", ["None"], ["", "", "", "def", "save_checkpoint", "(", "checkpoint", ":", "Checkpoint", ",", "config", ":", "HyperParameters", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Saves a model (and related garbage) to disk and updates checkpoints.csv with the new model.\n    \"\"\"", "\n", "\n", "assert", "not", "checkpoint", ".", "filepath", ",", "\"checkpoint already has a path.\"", "\n", "\n", "os", ".", "makedirs", "(", "config", ".", "models_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "filepath", "=", "Path", "(", "config", ".", "models_dir", ")", "/", "f\"{uuid.uuid4()}.pt\"", "\n", "\n", "disk_model", ":", "DiskModel", "=", "{", "\n", "\"model_state_dict\"", ":", "checkpoint", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer_state_dict\"", ":", "checkpoint", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", "\n", "\n", "torch", ".", "save", "(", "disk_model", ",", "filepath", ")", "# type: ignore", "\n", "\n", "with", "open", "(", "config", ".", "checkpoint_csv", ",", "\"a\"", ")", "as", "file", ":", "\n", "        ", "csv_row", "=", "CsvRow", "(", "\n", "filepath", ",", "\n", "checkpoint", ".", "epoch", ",", "\n", "checkpoint", ".", "seen", ",", "\n", "checkpoint", ".", "total_train_loss", ",", "\n", "checkpoint", ".", "avg_val_loss", ",", "\n", ")", "\n", "\n", "file", ".", "write", "(", "f\"{csv_row}\\n\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.SentenceClassificationModel.__init__": [[13, 16], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_model", ":", "BertPreTrainedModel", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "# type: ignore", "\n", "self", ".", "bert", "=", "bert_model", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.SentenceClassificationModel.forward": [[17, 23], ["models.SentenceClassificationModel.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ":", "Tensor", ",", "attn_mask", ":", "Tensor", ",", "**", "kwargs", ":", "Any", ")", "->", "Tensor", ":", "\n", "        ", "if", "\"output_attentions\"", "not", "in", "kwargs", ":", "\n", "            ", "kwargs", "[", "\"output_attentions\"", "]", "=", "False", "\n", "\n", "", "logits", ":", "Tensor", "=", "self", ".", "bert", "(", "input_ids", ",", "attention_mask", "=", "attn_mask", ",", "**", "kwargs", ")", "[", "0", "]", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.SentenceClassificationModel.get_optimizer": [[24, 26], ["transformers.AdamW", "models.SentenceClassificationModel.parameters"], "methods", ["None"], ["", "def", "get_optimizer", "(", "self", ",", "config", ":", "HyperParameters", ")", "->", "Optimizer", ":", "\n", "        ", "return", "AdamW", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ",", "eps", "=", "1e-8", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.get_pretrained_model": [[28, 49], ["models.SentenceClassificationModel", "os.path.isdir", "transformers.AutoModelForSequenceClassification.from_pretrained", "AutoModelForSequenceClassification.from_pretrained.resize_token_embeddings", "transformers.AutoModelForSequenceClassification.from_pretrained", "ValueError", "len", "tokenizers.get_tokenizer"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer"], ["", "", "def", "get_pretrained_model", "(", "config", ":", "HyperParameters", ")", "->", "SentenceClassificationModel", ":", "\n", "    ", "\"\"\"\n    based on config.name, returns a vanilla pretrained model or an arxiv-specific model.\n    \"\"\"", "\n", "\n", "if", "\"arxiv\"", "in", "config", ".", "model_name", ":", "# nice pattern matching", "\n", "        ", "possible_dir", "=", "config", ".", "models_dir", "/", "config", ".", "model_name", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "possible_dir", ")", ":", "\n", "            ", "bert", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "\n", "config", ".", "root_model_name", "\n", ")", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need pretrained model on disk.\"", ")", "\n", "", "", "else", ":", "\n", "        ", "bert", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "config", ".", "model_name", ")", "\n", "\n", "bert", ".", "resize_token_embeddings", "(", "len", "(", "tokenizers", ".", "get_tokenizer", "(", "config", ")", ")", ")", "\n", "\n", "", "return", "SentenceClassificationModel", "(", "bert", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.pretrain_prep": [[51, 67], ["tokenizers.get_tokenizer", "transformers.AutoModelForSequenceClassification.from_pretrained", "AutoModelForSequenceClassification.from_pretrained.resize_token_embeddings", "AutoModelForSequenceClassification.from_pretrained.save_pretrained", "tokenizers.get_tokenizer.save_pretrained", "print", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer"], ["", "def", "pretrain_prep", "(", "config", ":", "HyperParameters", ")", "->", "None", ":", "\n", "# this code needs to create the model in the directory.", "\n", "    ", "assert", "\"arxiv\"", "in", "config", ".", "model_name", "\n", "\n", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "bert", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "config", ".", "root_model_name", ")", "\n", "\n", "bert", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "possible_dir", "=", "config", ".", "models_dir", "/", "config", ".", "model_name", "\n", "\n", "bert", ".", "save_pretrained", "(", "str", "(", "possible_dir", ")", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "str", "(", "possible_dir", ")", ")", "\n", "\n", "print", "(", "\"You probably want to run language_model.sh now.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.Example.answers": [[36, 42], ["evaluating.Answer"], "methods", ["None"], ["@", "property", "\n", "def", "answers", "(", "self", ")", "->", "Answer", ":", "\n", "        ", "return", "Answer", "(", "\n", "self", ".", "bert", "==", "self", ".", "label", ",", "\n", "self", ".", "roberta", "==", "self", ".", "label", ",", "\n", "self", ".", "scibert", "==", "self", ".", "label", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.get_examples": [[45, 91], ["range", "range", "open", "csv.reader", "open", "csv.reader", "open", "csv.reader", "len", "len", "len", "len", "len", "evaluating.Example", "examples.append", "len", "bool", "bool", "bool", "bool", "bool", "bool", "int", "int", "int", "int", "int", "int"], "function", ["None"], ["", "", "def", "get_examples", "(", ")", "->", "List", "[", "Example", "]", ":", "\n", "    ", "with", "open", "(", "BERT_FILE", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "bert", "=", "[", "\n", "(", "i", ",", "sent", ",", "bool", "(", "int", "(", "label", ")", ")", ",", "bool", "(", "int", "(", "prediction", ")", ")", ")", "\n", "for", "i", ",", "sent", ",", "label", ",", "prediction", "in", "reader", "\n", "]", "\n", "\n", "", "with", "open", "(", "ROBERTA_FILE", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "roberta", "=", "[", "\n", "(", "i", ",", "sent", ",", "bool", "(", "int", "(", "label", ")", ")", ",", "bool", "(", "int", "(", "prediction", ")", ")", ")", "\n", "for", "i", ",", "sent", ",", "label", ",", "prediction", "in", "reader", "\n", "]", "\n", "\n", "", "with", "open", "(", "SCIBERT_FILE", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "scibert", "=", "[", "\n", "(", "i", ",", "sent", ",", "bool", "(", "int", "(", "label", ")", ")", ",", "bool", "(", "int", "(", "prediction", ")", ")", ")", "\n", "for", "i", ",", "sent", ",", "label", ",", "prediction", "in", "reader", "\n", "]", "\n", "\n", "", "assert", "len", "(", "scibert", ")", "==", "len", "(", "roberta", ")", "==", "len", "(", "bert", ")", "==", "142923", ",", "f\"{len(roberta)}\"", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "scibert", ")", ")", ":", "\n", "        ", "assert", "(", "\n", "scibert", "[", "i", "]", "[", "2", "]", "==", "roberta", "[", "i", "]", "[", "2", "]", "==", "bert", "[", "i", "]", "[", "2", "]", "\n", ")", ",", "f\"'{scibert[i][2]}' '{roberta[i][2]}' '{bert[i][2]}'\"", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "roberta", ")", ")", ":", "\n", "        ", "ident", ",", "_", ",", "label", ",", "_", "=", "roberta", "[", "i", "]", "\n", "example", "=", "Example", "(", "\n", "ident", ",", "\n", "bert", "[", "i", "]", "[", "1", "]", ",", "\n", "roberta", "[", "i", "]", "[", "1", "]", ",", "\n", "scibert", "[", "i", "]", "[", "1", "]", ",", "\n", "label", ",", "\n", "bert", "[", "i", "]", "[", "3", "]", ",", "\n", "roberta", "[", "i", "]", "[", "3", "]", ",", "\n", "scibert", "[", "i", "]", "[", "3", "]", ",", "\n", ")", "\n", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.report_results": [[93, 175], ["evaluating.get_examples", "structures.ConfusionMatrix", "print", "print", "print", "print", "print", "print", "print", "structures.ConfusionMatrix", "print", "print", "print", "print", "print", "print", "print", "structures.ConfusionMatrix", "print", "print", "print", "print", "print", "print", "print", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.get_examples"], ["", "def", "report_results", "(", ")", "->", "None", ":", "\n", "    ", "examples", "=", "get_examples", "(", ")", "\n", "\n", "roberta_TP", "=", "0", "\n", "roberta_FP", "=", "0", "\n", "roberta_FN", "=", "0", "\n", "roberta_TN", "=", "0", "\n", "\n", "scibert_TP", "=", "0", "\n", "scibert_FP", "=", "0", "\n", "scibert_FN", "=", "0", "\n", "scibert_TN", "=", "0", "\n", "\n", "bert_TP", "=", "0", "\n", "bert_FP", "=", "0", "\n", "bert_FN", "=", "0", "\n", "bert_TN", "=", "0", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "if", "example", ".", "label", "and", "example", ".", "roberta", ":", "\n", "            ", "roberta_TP", "+=", "1", "\n", "", "if", "example", ".", "label", "and", "example", ".", "scibert", ":", "\n", "            ", "scibert_TP", "+=", "1", "\n", "", "if", "example", ".", "label", "and", "example", ".", "bert", ":", "\n", "            ", "bert_TP", "+=", "1", "\n", "\n", "", "if", "example", ".", "label", "and", "not", "example", ".", "roberta", ":", "\n", "            ", "roberta_FN", "+=", "1", "\n", "", "if", "example", ".", "label", "and", "not", "example", ".", "scibert", ":", "\n", "            ", "scibert_FN", "+=", "1", "\n", "", "if", "example", ".", "label", "and", "not", "example", ".", "bert", ":", "\n", "            ", "bert_FN", "+=", "1", "\n", "\n", "", "if", "not", "example", ".", "label", "and", "example", ".", "roberta", ":", "\n", "            ", "roberta_FP", "+=", "1", "\n", "", "if", "not", "example", ".", "label", "and", "example", ".", "scibert", ":", "\n", "            ", "scibert_FP", "+=", "1", "\n", "", "if", "not", "example", ".", "label", "and", "example", ".", "bert", ":", "\n", "            ", "bert_FP", "+=", "1", "\n", "\n", "", "if", "not", "example", ".", "label", "and", "not", "example", ".", "roberta", ":", "\n", "            ", "roberta_TN", "+=", "1", "\n", "", "if", "not", "example", ".", "label", "and", "not", "example", ".", "scibert", ":", "\n", "            ", "scibert_TN", "+=", "1", "\n", "", "if", "not", "example", ".", "label", "and", "not", "example", ".", "bert", ":", "\n", "            ", "bert_TN", "+=", "1", "\n", "\n", "", "", "roberta_confusion", "=", "ConfusionMatrix", "(", "roberta_TP", ",", "roberta_TN", ",", "roberta_FP", ",", "roberta_FN", ")", "\n", "assert", "roberta_confusion", ".", "total", "==", "142923", "\n", "\n", "print", "(", "\"Roberta results:\"", ")", "\n", "print", "(", "roberta_confusion", ")", "\n", "print", "(", "f\"Accuracy: {roberta_confusion.accuracy:.3f}\"", ")", "\n", "print", "(", "f\"Precision: {roberta_confusion.precision:.3f}\"", ")", "\n", "print", "(", "f\"Recall: {roberta_confusion.recall:.3f}\"", ")", "\n", "print", "(", "f\"F1: {roberta_confusion.f1:.3f}\"", ")", "\n", "print", "(", ")", "\n", "\n", "scibert_confusion", "=", "ConfusionMatrix", "(", "scibert_TP", ",", "scibert_TN", ",", "scibert_FP", ",", "scibert_FN", ")", "\n", "assert", "scibert_confusion", ".", "total", "==", "142923", "\n", "\n", "print", "(", "\"Scibert results:\"", ")", "\n", "print", "(", "scibert_confusion", ")", "\n", "print", "(", "f\"Accuracy: {scibert_confusion.accuracy:.3f}\"", ")", "\n", "print", "(", "f\"Precision: {scibert_confusion.precision:.3f}\"", ")", "\n", "print", "(", "f\"Recall: {scibert_confusion.recall:.3f}\"", ")", "\n", "print", "(", "f\"F1: {scibert_confusion.f1:.3f}\"", ")", "\n", "print", "(", ")", "\n", "\n", "bert_confusion", "=", "ConfusionMatrix", "(", "bert_TP", ",", "bert_TN", ",", "bert_FP", ",", "bert_FN", ")", "\n", "assert", "bert_confusion", ".", "total", "==", "142923", "\n", "\n", "print", "(", "\"Bert results:\"", ")", "\n", "print", "(", "bert_confusion", ")", "\n", "print", "(", "f\"Accuracy: {bert_confusion.accuracy:.3f}\"", ")", "\n", "print", "(", "f\"Precision: {bert_confusion.precision:.3f}\"", ")", "\n", "print", "(", "f\"Recall: {bert_confusion.recall:.3f}\"", ")", "\n", "print", "(", "f\"F1: {bert_confusion.f1:.3f}\"", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "\"Positive:\"", ",", "len", "(", "[", "ex", "for", "ex", "in", "examples", "if", "ex", ".", "label", "]", ")", ")", "\n", "print", "(", "\"Negative:\"", ",", "len", "(", "[", "ex", "for", "ex", "in", "examples", "if", "not", "ex", ".", "label", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.get_confusion_matrix": [[177, 192], ["torch.max", "structures.ConfusionMatrix", "len", "len"], "function", ["None"], ["", "def", "get_confusion_matrix", "(", "logits", ":", "Tensor", ",", "labels", ":", "Tensor", ")", "->", "ConfusionMatrix", ":", "\n", "    ", "_", ",", "predicted", "=", "torch", ".", "max", "(", "logits", ".", "data", ",", "1", ")", "# type: ignore", "\n", "\n", "true_pos", ":", "int", "=", "(", "(", "predicted", "*", "labels", ")", "!=", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "true_neg", ":", "int", "=", "(", "(", "(", "predicted", "-", "1", ")", "*", "(", "labels", "-", "1", ")", ")", "!=", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "false_pos", ":", "int", "=", "(", "(", "predicted", "*", "(", "labels", "-", "1", ")", ")", "!=", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "false_neg", ":", "int", "=", "(", "(", "(", "predicted", "-", "1", ")", "*", "labels", ")", "!=", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "confusion", "=", "ConfusionMatrix", "(", "true_pos", ",", "true_neg", ",", "false_pos", ",", "false_neg", ")", "\n", "\n", "assert", "confusion", ".", "total", "==", "len", "(", "\n", "predicted", "\n", ")", ",", "f\"confusion matrix {confusion} doesn't have the same number of results as {labels}: {confusion.total} != {len(labels)}\"", "\n", "\n", "return", "confusion", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.main": [[194, 196], ["evaluating.report_results"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.report_results"], ["", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "report_results", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error": [[21, 24], ["print", "print"], "function", ["None"], ["def", "show_error", "(", "err", ":", "Exception", ")", "->", "None", ":", "\n", "    ", "print", "(", "\"Error:\"", ")", "\n", "print", "(", "f\"\\t{err}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.gpu_check": [[26, 31], ["torch.cuda.is_available"], "function", ["None"], ["", "def", "gpu_check", "(", "force_cpu", ":", "bool", ")", "->", "None", ":", "\n", "    ", "if", "not", "force_cpu", ":", "\n", "        ", "assert", "(", "\n", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", ")", ",", "\"Need CUDA to run on a local machine: https://pytorch.org/get-started/locally/\"", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.script": [[33, 188], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "structures.HyperParameters", "cProfile.Profile", "cProfile.Profile.enable", "os.makedirs", "disk.find_latest_checkpoint", "isinstance", "parser.parse_args.preprocess.split", "tokenizers.get_tokenizer", "print", "data.get_val_dataloader", "util.my_tqdm", "print", "data.get_train_dataloader", "util.my_tqdm", "print", "data.get_val_dataloader", "util.my_tqdm", "print", "data.get_train_dataloader", "util.my_tqdm", "models.pretrain_prep", "local_machine_script.gpu_check", "training.sanity_check", "isinstance", "local_machine_script.gpu_check", "training.main_loop", "isinstance", "local_machine_script.gpu_check", "inference.perform_inference", "isinstance", "local_machine_script.gpu_check", "inference.perform_inference", "isinstance", "data.get_val_dataloader", "disk.load_checkpoint", "isinstance", "cProfile.Profile.disable", "cProfile.Profile.dump_stats", "pathlib.Path", "os.path.isdir", "print", "shutil.copytree", "local_machine_script.show_error", "shutil.copy", "print", "data.prepare_test_data", "print", "data.prepare_val_data", "print", "data.prepare_train_data", "local_machine_script.show_error", "bool", "local_machine_script.show_error", "local_machine_script.show_error", "local_machine_script.show_error", "pathlib.Path", "local_machine_script.show_error", "training.validate", "print"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.find_latest_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_train_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_train_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.models.pretrain_prep", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.gpu_check", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.sanity_check", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.gpu_check", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.main_loop", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.gpu_check", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.perform_inference", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.gpu_check", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.perform_inference", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_test_data", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_val_data", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_train_data", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.local_machine_script.show_error", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.validate"], ["", "", "def", "script", "(", ")", "->", "None", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--preprocess\"", ",", "help", "=", "\"preprocess data (tokenizer, pickle)\"", ",", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--sanity-check\"", ",", "help", "=", "\"run the sanity check\"", ",", "action", "=", "\"store_true\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--main-loop\"", ",", "help", "=", "\"run the main training loop\"", ",", "action", "=", "\"store_true\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--continuing\"", ",", "help", "=", "\"is training continuing?\"", ",", "action", "=", "\"store_true\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--force-cpu\"", ",", "\n", "help", "=", "\"force the script to use CPU is no GPU is present.\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--check-loader\"", ",", "\n", "help", "=", "\"iterate through the train and validation dataloaders to make sure they work.\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--profile\"", ",", "\n", "help", "=", "\"use cProfile to profile program. Saves file to PROFILE.prof\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--inference-val\"", ",", "\n", "help", "=", "\"write predictions for the validation set using a checkpoint .pt file.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--inference-test\"", ",", "\n", "help", "=", "\"write predictions for the test set using a checkpoint .pt file.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pretrain-prep\"", ",", "\n", "help", "=", "\"Do project-specific steps for additional pretraining\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--copy-models\"", ",", "help", "=", "\"Copy required models to COPY_MODELS\"", ",", "type", "=", "str", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval\"", ",", "help", "=", "\"evaluate checkpoint EVAL on the validation set\"", ",", "type", "=", "str", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"config\"", ",", "help", "=", "\"location of experiment .toml config file\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "config", "=", "HyperParameters", "(", "args", ".", "config", ")", "\n", "\n", "if", "args", ".", "profile", ":", "\n", "        ", "pr", "=", "cProfile", ".", "Profile", "(", ")", "\n", "pr", ".", "enable", "(", ")", "\n", "\n", "", "if", "args", ".", "copy_models", ":", "\n", "        ", "dst_dir", "=", "Path", "(", "args", ".", "copy_models", ")", "/", "config", ".", "models_dir", "\n", "os", ".", "makedirs", "(", "dst_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "\"arxiv\"", "in", "config", ".", "model_name", ":", "\n", "# need to copy the custom model folder AND the latest checkpoint", "\n", "            ", "model_dir", "=", "config", ".", "models_dir", "/", "config", ".", "model_name", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "model_dir", ")", ",", "f\"{model_dir} needs to exist.\"", "\n", "\n", "print", "(", "f\"Copying {model_dir} to {dst_dir / config.model_name}.\"", ")", "\n", "shutil", ".", "copytree", "(", "model_dir", ",", "dst_dir", "/", "config", ".", "model_name", ")", "\n", "\n", "# need to copy the latest checkpoint to the models folder.", "\n", "", "latest_model", "=", "disk", ".", "find_latest_checkpoint", "(", "config", ".", "checkpoint_csv", ")", "\n", "if", "isinstance", "(", "latest_model", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "latest_model", ")", "\n", "", "else", ":", "\n", "            ", "shutil", ".", "copy", "(", "latest_model", ".", "filepath", ",", "dst_dir", ")", "\n", "\n", "", "", "if", "args", ".", "preprocess", ":", "\n", "        ", "types", "=", "args", ".", "preprocess", ".", "split", "(", "\",\"", ")", "\n", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "for", "t", "in", "types", ":", "\n", "            ", "if", "t", "not", "in", "[", "\"train\"", ",", "\"val\"", ",", "\"test\"", "]", ":", "\n", "                ", "print", "(", "\"--preprocess only accepts 'train', 'val' or 'test'.\"", ")", "\n", "\n", "", "", "if", "\"test\"", "in", "types", ":", "\n", "            ", "print", "(", "\"Preprocessing test data...\"", ")", "\n", "data", ".", "prepare_test_data", "(", "config", ",", "tokenizer", ")", "\n", "", "if", "\"val\"", "in", "types", ":", "\n", "            ", "print", "(", "\"Preprocessing validation data...\"", ")", "\n", "data", ".", "prepare_val_data", "(", "config", ",", "tokenizer", ")", "\n", "", "if", "\"train\"", "in", "types", ":", "\n", "            ", "print", "(", "\"Preprocessing training data...\"", ")", "\n", "data", ".", "prepare_train_data", "(", "config", ",", "tokenizer", ")", "\n", "\n", "", "", "if", "args", ".", "check_loader", ":", "\n", "        ", "print", "(", "\"Checking small validation loader...\"", ")", "\n", "loader", "=", "data", ".", "get_val_dataloader", "(", "config", ",", "small", "=", "True", ")", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "            ", "pass", "\n", "", "print", "(", "\"Checking small training loader...\"", ")", "\n", "loader", "=", "data", ".", "get_train_dataloader", "(", "config", ",", "small", "=", "True", ")", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "            ", "pass", "\n", "", "print", "(", "\"Checking validation loader...\"", ")", "\n", "loader", "=", "data", ".", "get_val_dataloader", "(", "config", ")", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "            ", "pass", "\n", "", "print", "(", "\"Checking training loader...\"", ")", "\n", "loader", "=", "data", ".", "get_train_dataloader", "(", "config", ")", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "args", ".", "pretrain_prep", ":", "\n", "        ", "models", ".", "pretrain_prep", "(", "config", ")", "\n", "\n", "", "if", "args", ".", "sanity_check", ":", "\n", "        ", "gpu_check", "(", "args", ".", "force_cpu", ")", "\n", "result", "=", "training", ".", "sanity_check", "(", "config", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "result", ")", "\n", "\n", "", "", "if", "args", ".", "main_loop", ":", "\n", "        ", "gpu_check", "(", "args", ".", "force_cpu", ")", "\n", "result", "=", "training", ".", "main_loop", "(", "config", ",", "bool", "(", "args", ".", "continuing", ")", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "result", ")", "\n", "\n", "", "", "if", "args", ".", "inference_val", ":", "\n", "        ", "gpu_check", "(", "args", ".", "force_cpu", ")", "\n", "result", "=", "inference", ".", "perform_inference", "(", "config", ",", "args", ".", "inference_val", ",", "test", "=", "False", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "result", ")", "\n", "\n", "", "", "if", "args", ".", "inference_test", ":", "\n", "        ", "gpu_check", "(", "args", ".", "force_cpu", ")", "\n", "result", "=", "inference", ".", "perform_inference", "(", "config", ",", "args", ".", "inference_test", ",", "test", "=", "True", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "result", ")", "\n", "\n", "", "", "if", "args", ".", "eval", ":", "\n", "        ", "val_loader", "=", "data", ".", "get_val_dataloader", "(", "config", ")", "\n", "checkpoint", "=", "disk", ".", "load_checkpoint", "(", "config", ",", "Path", "(", "args", ".", "eval", ")", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "Exception", ")", ":", "\n", "            ", "show_error", "(", "checkpoint", ")", "\n", "", "else", ":", "\n", "            ", "training", ".", "validate", "(", "checkpoint", ".", "model", ",", "val_loader", ")", "\n", "\n", "", "", "if", "args", ".", "profile", ":", "\n", "        ", "pr", ".", "disable", "(", ")", "\n", "pr", ".", "dump_stats", "(", "args", ".", "profile", "+", "\".prof\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.get_filename": [[17, 21], ["os.path.split", "os.path.splitext"], "function", ["None"], ["def", "get_filename", "(", "config", ":", "HyperParameters", ",", "checkpoint_filepath", ":", "Path", ",", "ext", ":", "str", ")", "->", "str", ":", "\n", "    ", "_", ",", "checkpoint_filename", "=", "os", ".", "path", ".", "split", "(", "checkpoint_filepath", ")", "\n", "checkpoint_hash", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "checkpoint_filename", ")", "\n", "return", "f\"{config.experiment_name}-{checkpoint_hash}-{ext}-inference.csv\"", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.simple_inference": [[23, 50], ["util.get_device", "model.eval", "tokenizers.encode_batch", "torch.stack().to", "torch.stack().to", "torch.max", "result.extend", "torch.no_grad", "model", "predictions.tolist", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.encode_batch"], ["", "def", "simple_inference", "(", "\n", "sentences", ":", "List", "[", "str", "]", ",", "\n", "model", ":", "models", ".", "SentenceClassificationModel", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Given a list of sentences, just gives you a list of predictions (1 for edit, 0 for no edit). Very slow, do not use this except for one-off tasks.\n    \"\"\"", "\n", "device", "=", "util", ".", "get_device", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "result", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "cpu_ids", ",", "cpu_mask", "=", "tokenizers", ".", "encode_batch", "(", "[", "sentence", "]", ",", "tokenizer", ")", "\n", "\n", "input_ids", "=", "torch", ".", "stack", "(", "cpu_ids", ")", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "torch", ".", "stack", "(", "cpu_mask", ")", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", ")", "\n", "\n", "", "_", ",", "predictions", "=", "torch", ".", "max", "(", "logits", ".", "data", ",", "1", ")", "# type: ignore", "\n", "\n", "result", ".", "extend", "(", "predictions", ".", "tolist", "(", ")", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.perform_inference": [[52, 109], ["disk.load_finetuned_model_from_disk", "isinstance", "model.eval", "util.print_device", "tokenizers.get_tokenizer", "inference.get_filename", "util.get_device", "print", "print", "open", "csv.writer", "open", "csv.reader", "print", "util.my_tqdm", "next", "util.chunks", "tokenizers.encode_batch", "torch.stack().to", "torch.stack().to", "torch.max", "csv.writer.writerows", "int", "ids.tolist", "tokenizers.decode", "torch.no_grad", "model", "zip", "torch.stack", "torch.stack", "predictions.tolist"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.print_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.inference.get_filename", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.chunks", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.encode_batch", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode"], ["", "def", "perform_inference", "(", "\n", "config", ":", "HyperParameters", ",", "checkpoint_filepath", ":", "Path", ",", "test", ":", "bool", "=", "False", "\n", ")", "->", "Result", "[", "None", "]", ":", "\n", "\n", "    ", "disk_model", "=", "disk", ".", "load_finetuned_model_from_disk", "(", "config", ",", "checkpoint_filepath", ")", "\n", "if", "isinstance", "(", "disk_model", ",", "Exception", ")", ":", "\n", "        ", "return", "disk_model", "\n", "\n", "", "model", ",", "_", "=", "disk_model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "util", ".", "print_device", "(", ")", "\n", "\n", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "examples_filepath", "=", "config", ".", "test_file", "if", "test", "else", "config", ".", "val_file", "\n", "filename", "=", "get_filename", "(", "config", ",", "checkpoint_filepath", ",", "\"test\"", "if", "test", "else", "\"val\"", ")", "\n", "\n", "device", "=", "util", ".", "get_device", "(", ")", "\n", "\n", "print", "(", "\"\\n--- Inference ---\"", ")", "\n", "print", "(", "f\"Writing to {filename}.\"", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "outfile", ":", "\n", "        ", "csvwriter", "=", "csv", ".", "writer", "(", "outfile", ")", "\n", "\n", "with", "open", "(", "examples_filepath", ",", "\"r\"", ")", "as", "infile", ":", "\n", "            ", "csvreader", "=", "csv", ".", "reader", "(", "infile", ")", "\n", "print", "(", "next", "(", "csvreader", ")", ")", "# skip headers", "\n", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "util", ".", "chunks", "(", "csvreader", ",", "config", ".", "batch_size", ")", ")", ":", "\n", "                ", "sentences", "=", "[", "i", "for", "_", ",", "i", ",", "_", "in", "batch", "]", "\n", "cpu_labels", "=", "[", "int", "(", "i", ")", "for", "_", ",", "_", ",", "i", "in", "batch", "]", "\n", "identifiers", "=", "[", "i", "for", "i", ",", "_", ",", "_", "in", "batch", "]", "\n", "\n", "cpu_ids", ",", "cpu_mask", "=", "tokenizers", ".", "encode_batch", "(", "sentences", ",", "tokenizer", ")", "\n", "\n", "integer_ids", "=", "[", "ids", ".", "tolist", "(", ")", "for", "ids", "in", "cpu_ids", "]", "\n", "decoded_sentences", "=", "[", "\n", "tokenizers", ".", "decode", "(", "ids", ",", "tokenizer", ")", "for", "ids", "in", "integer_ids", "\n", "]", "\n", "\n", "input_ids", "=", "torch", ".", "stack", "(", "cpu_ids", ")", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "torch", ".", "stack", "(", "cpu_mask", ")", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", ")", "\n", "\n", "", "_", ",", "predictions", "=", "torch", ".", "max", "(", "logits", ".", "data", ",", "1", ")", "# type: ignore", "\n", "\n", "csvwriter", ".", "writerows", "(", "\n", "zip", "(", "\n", "identifiers", ",", "decoded_sentences", ",", "cpu_labels", ",", "predictions", ".", "tolist", "(", ")", "\n", ")", "\n", ")", "\n", "\n", "", "", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence": [[12, 23], ["str", "string_builder.append", "string_builder.append", "str", "str"], "function", ["None"], ["def", "extract_sentence", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "str", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "string_builder", "=", "[", "str", "(", "sent_elem", ".", "text", ")", "if", "sent_elem", ".", "text", "else", "\"\"", "]", "\n", "\n", "for", "del_ins", "in", "sent_elem", ":", "\n", "        ", "if", "del_ins", ".", "tag", "==", "\"del\"", "and", "del_ins", ".", "text", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "text", ")", ")", "\n", "", "if", "del_ins", ".", "tail", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "tail", ")", ")", "\n", "\n", "", "", "return", "\"\"", ".", "join", "(", "string_builder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_after_editing": [[25, 36], ["str", "string_builder.append", "string_builder.append", "str", "str"], "function", ["None"], ["", "def", "extract_sentence_after_editing", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "str", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "string_builder", "=", "[", "str", "(", "sent_elem", ".", "text", ")", "if", "sent_elem", ".", "text", "else", "\"\"", "]", "\n", "\n", "for", "del_ins", "in", "sent_elem", ":", "\n", "        ", "if", "del_ins", ".", "tag", "==", "\"ins\"", "and", "del_ins", ".", "text", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "text", ")", ")", "\n", "", "if", "del_ins", ".", "tail", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "tail", ")", ")", "\n", "\n", "", "", "return", "\"\"", ".", "join", "(", "string_builder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id": [[38, 42], ["str"], "function", ["None"], ["", "def", "extract_sentence_id", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "str", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "assert", "\"sid\"", "in", "sent_elem", ".", "attrib", "\n", "return", "str", "(", "sent_elem", ".", "attrib", "[", "\"sid\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.get_label": [[44, 49], ["len", "list"], "function", ["None"], ["", "def", "get_label", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    0 if no edit/no change, 1 if changed/edited\n    \"\"\"", "\n", "return", "1", "if", "len", "(", "list", "(", "sent_elem", ")", ")", ">", "0", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.read_xml": [[51, 71], ["os.path.isfile", "tqdm.tqdm", "os.remove", "open", "csv.writer", "csv.writer.writerow", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "aesw_to_sentences.extract_sentence", "heapq.heappush", "open", "csv.writer", "str", "heapq.heappop", "csv.writer.writerow", "len", "aesw_to_sentences.get_label"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.get_label"], ["", "def", "read_xml", "(", "xml_filepath", ":", "Path", ",", "output_filepath", ":", "Path", ")", "->", "None", ":", "\n", "    ", "if", "os", ".", "path", ".", "isfile", "(", "output_filepath", ")", ":", "\n", "        ", "os", ".", "remove", "(", "output_filepath", ")", "\n", "\n", "", "with", "open", "(", "output_filepath", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "writer", ".", "writerow", "(", "[", "\"id\"", ",", "\"sentence\"", ",", "\"label\"", "]", ")", "\n", "\n", "", "sentences", ":", "List", "[", "Tuple", "[", "int", ",", "str", ",", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "event", ",", "sent_elem", "in", "tqdm", "(", "etree", ".", "iterparse", "(", "str", "(", "xml_filepath", ")", ",", "tag", "=", "\"sentence\"", ")", ")", ":", "\n", "        ", "sent_id", "=", "extract_sentence_id", "(", "sent_elem", ")", "\n", "sent", "=", "extract_sentence", "(", "sent_elem", ")", "\n", "heapq", ".", "heappush", "(", "sentences", ",", "(", "len", "(", "sent", ")", ",", "sent_id", ",", "sent", ",", "get_label", "(", "sent_elem", ")", ")", ")", "\n", "\n", "", "with", "open", "(", "output_filepath", ",", "\"a\"", ")", "as", "file", ":", "\n", "        ", "writer", "=", "csv", ".", "writer", "(", "file", ")", "\n", "while", "sentences", ":", "\n", "            ", "_", ",", "identifier", ",", "sent", ",", "label", "=", "heapq", ".", "heappop", "(", "sentences", ")", "\n", "writer", ".", "writerow", "(", "(", "identifier", ",", "sent", ",", "label", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.main": [[73, 105], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.datasets.split", "print", "pathlib.Path", "pathlib.Path", "aesw_to_sentences.read_xml", "print", "pathlib.Path", "pathlib.Path", "aesw_to_sentences.read_xml", "print", "pathlib.Path", "pathlib.Path", "aesw_to_sentences.read_xml", "print"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.read_xml", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.read_xml", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.read_xml"], ["", "", "", "def", "main", "(", ")", "->", "None", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"datasets\"", ",", "\n", "help", "=", "\"which datasets (train, test, val) will be turned into .csv files.\"", ",", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "types", "=", "args", ".", "datasets", ".", "split", "(", "\",\"", ")", "\n", "\n", "for", "t", "in", "types", ":", "\n", "        ", "if", "t", "not", "in", "[", "\"train\"", ",", "\"val\"", ",", "\"test\"", "]", ":", "\n", "            ", "print", "(", "\"--preprocess only accepts 'train', 'val' or 'test'.\"", ")", "\n", "\n", "", "", "if", "\"test\"", "in", "types", ":", "\n", "        ", "print", "(", "\"Converting test data...\"", ")", "\n", "test_xml", "=", "Path", "(", "\"./data-unversioned/aesw/aesw2016(v1.2)_test_modified.xml\"", ")", "\n", "test_out", "=", "Path", "(", "\"./data-unversioned/aesw/aesw-test.csv\"", ")", "\n", "read_xml", "(", "test_xml", ",", "test_out", ")", "\n", "", "if", "\"val\"", "in", "types", ":", "\n", "        ", "print", "(", "\"Converting validation data...\"", ")", "\n", "test_xml", "=", "Path", "(", "\"./data-unversioned/aesw/aesw2016(v1.2)_dev.xml\"", ")", "\n", "test_out", "=", "Path", "(", "\"./data-unversioned/aesw/aesw-dev.csv\"", ")", "\n", "read_xml", "(", "test_xml", ",", "test_out", ")", "\n", "", "if", "\"train\"", "in", "types", ":", "\n", "        ", "print", "(", "\"Converting training data...\"", ")", "\n", "test_xml", "=", "Path", "(", "\"./data-unversioned/aesw/aesw2016(v1.2)_train.xml\"", ")", "\n", "test_out", "=", "Path", "(", "\"./data-unversioned/aesw/aesw-train.csv\"", ")", "\n", "read_xml", "(", "test_xml", ",", "test_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.RandomBatchSampler.__init__": [[25, 35], ["list", "random.shuffle", "range", "list", "len", "util.grouper"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.grouper"], ["def", "__init__", "(", "self", ",", "data", ":", "TensorDataset", ",", "batch_size", ":", "int", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "data", ")", ")", ")", "\n", "batches", "=", "[", "list", "(", "g", ")", "for", "g", "in", "util", ".", "grouper", "(", "indices", ",", "batch_size", ")", "]", "\n", "\n", "*", "front", ",", "last", "=", "batches", "\n", "\n", "last", "=", "[", "i", "for", "i", "in", "last", "if", "i", "]", "# filter None out", "\n", "random", ".", "shuffle", "(", "front", ")", "\n", "\n", "self", ".", "batches", "=", "front", "+", "[", "last", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.RandomBatchSampler.__iter__": [[36, 38], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "List", "[", "int", "]", "]", ":", "\n", "        ", "return", "iter", "(", "self", ".", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.RandomBatchSampler.__len__": [[39, 41], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset.__init__": [[51, 89], ["data.FileBackedSentenceDataset._save_all_datasets", "data.FileBackedSentenceDataset._get_file_len", "data.FileBackedSentenceDataset._index_starts.append", "os.path.join", "sorted", "os.listdir", "int"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._save_all_datasets", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._get_file_len"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dirname", ":", "Path", ",", "\n", "size", ":", "int", ",", "\n", "data_files", ":", "Optional", "[", "List", "[", "Path", "]", "]", "=", "None", ",", "\n", "tokenizer", ":", "Optional", "[", "PreTrainedTokenizerFast", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n        * dirname {str} -- the folder where the pickled TensorDatasets will be stored.\n        * data_files {List[str]} -- the original csv file of (sentence, label) pairs. If provided, the sentences should be ordered by length from shortest to longest to make use of smart batches per https://mccormickml.com/2020/07/29/smart-batching-tutorial. If None or empty, then the dataset assumes that the pickled TensorDatasets already exist.\n        * tokenizer {Optional[PretrainedTokenizerFast]} -- tokenizer to convert the examples in example_file\n        \"\"\"", "\n", "\n", "self", ".", "dirname", "=", "dirname", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "files", "=", "[", "]", "\n", "self", ".", "_file_lens", "=", "[", "]", "\n", "self", ".", "_index_starts", "=", "[", "]", "\n", "self", ".", "_cached", "=", "None", "\n", "\n", "if", "data_files", ":", "\n", "            ", "assert", "tokenizer", "is", "not", "None", ",", "\"Need a tokenizer with an examples file\"", "\n", "self", ".", "_save_all_datasets", "(", "data_files", ",", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "files", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "dirname", ",", "f", ")", "\n", "for", "f", "in", "sorted", "(", "os", ".", "listdir", "(", "dirname", ")", ",", "key", "=", "lambda", "f", ":", "int", "(", "f", "[", ":", "-", "5", "]", ")", ")", "\n", "]", "\n", "\n", "self", ".", "_file_lens", "=", "[", "self", ".", "size", "for", "f", "in", "self", ".", "files", "]", "\n", "# the last file length might be short.", "\n", "self", ".", "_file_lens", "[", "-", "1", "]", "=", "self", ".", "_get_file_len", "(", "self", ".", "files", "[", "-", "1", "]", ")", "\n", "\n", "", "start", "=", "0", "\n", "for", "file_len", "in", "self", ".", "_file_lens", ":", "\n", "            ", "self", ".", "_index_starts", ".", "append", "(", "start", ")", "\n", "start", "+=", "file_len", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._save_all_datasets": [[90, 124], ["shutil.rmtree", "os.makedirs", "util.my_tqdm", "csv_merge.sorted_csv_files_reader", "sentences.append", "labels.append", "data.FileBackedSentenceDataset._save_dataset", "len", "data.FileBackedSentenceDataset._save_dataset", "data.FileBackedSentenceDataset._make_dataset", "data.FileBackedSentenceDataset._make_dataset"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.sorted_csv_files_reader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._save_dataset", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._save_dataset", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._make_dataset", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._make_dataset"], ["", "", "def", "_save_all_datasets", "(", "\n", "self", ",", "data_files", ":", "List", "[", "Path", "]", ",", "tokenizer", ":", "PreTrainedTokenizerFast", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Each datafile must be sorted by length.\n        \"\"\"", "\n", "# clear any existing work", "\n", "shutil", ".", "rmtree", "(", "self", ".", "dirname", ",", "ignore_errors", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "self", ".", "dirname", ",", "exist_ok", "=", "True", ")", "\n", "\n", "sentences", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "dataset_count", "=", "0", "\n", "\n", "for", "sentence", ",", "label", "in", "util", ".", "my_tqdm", "(", "\n", "csv_merge", ".", "sorted_csv_files_reader", "(", "data_files", ")", "\n", ")", ":", "\n", "            ", "sentences", ".", "append", "(", "sentence", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "\n", "if", "len", "(", "sentences", ")", "==", "self", ".", "size", ":", "\n", "                ", "self", ".", "_save_dataset", "(", "\n", "self", ".", "_make_dataset", "(", "sentences", ",", "labels", ",", "tokenizer", ")", ",", "\n", "f\"{dataset_count}.pckl\"", ",", "\n", ")", "\n", "\n", "dataset_count", "+=", "1", "\n", "sentences", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "", "", "if", "sentences", ":", "\n", "            ", "self", ".", "_save_dataset", "(", "\n", "self", ".", "_make_dataset", "(", "sentences", ",", "labels", ",", "tokenizer", ")", ",", "\n", "f\"{dataset_count}.pckl\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._make_dataset": [[126, 153], ["tokenizers.encode_batch", "torch.stack", "torch.stack", "torch.tensor", "torch.utils.data.TensorDataset", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.encode_batch"], ["", "", "def", "_make_dataset", "(", "\n", "self", ",", "\n", "sentences", ":", "List", "[", "str", "]", ",", "\n", "labels", ":", "List", "[", "int", "]", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", ")", "->", "TensorDataset", ":", "\n", "        ", "\"\"\"\n            Arguments:\n            * sentences {List[str]} -- The list of sentences that need to be tokenized and padded. This will always be exactly one batch.\n            \"\"\"", "\n", "assert", "len", "(", "sentences", ")", "==", "len", "(", "\n", "labels", "\n", ")", ",", "\"Need the same number of sentences and labels\"", "\n", "\n", "assert", "(", "\n", "len", "(", "sentences", ")", "<=", "self", ".", "size", "\n", ")", ",", "f\"{len(sentences)} must be less than {self.size}\"", "\n", "\n", "input_ids", ",", "attention_masks", "=", "tokenizers", ".", "encode_batch", "(", "sentences", ",", "tokenizer", ")", "\n", "\n", "input_ids_t", "=", "torch", ".", "stack", "(", "input_ids", ")", "\n", "attention_masks_t", "=", "torch", ".", "stack", "(", "attention_masks", ")", "\n", "labels_t", "=", "torch", ".", "tensor", "(", "labels", ")", "# type: ignore", "\n", "\n", "dataset", "=", "TensorDataset", "(", "input_ids_t", ",", "attention_masks_t", ",", "labels_t", ")", "\n", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._save_dataset": [[154, 162], ["os.path.join", "data.FileBackedSentenceDataset.files.append", "data.FileBackedSentenceDataset._file_lens.append", "len", "open", "pickle.dump"], "methods", ["None"], ["", "def", "_save_dataset", "(", "self", ",", "dataset", ":", "TensorDataset", ",", "name", ":", "str", ")", "->", "None", ":", "\n", "        ", "filepath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dirname", ",", "name", ")", "\n", "\n", "self", ".", "files", ".", "append", "(", "filepath", ")", "\n", "self", ".", "_file_lens", ".", "append", "(", "len", "(", "dataset", ")", ")", "\n", "\n", "with", "open", "(", "filepath", ",", "\"wb\"", ")", "as", "file", ":", "\n", "            ", "pickle", ".", "dump", "(", "dataset", ",", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._find_file": [[163, 168], ["bisect.bisect"], "methods", ["None"], ["", "", "def", "_find_file", "(", "self", ",", "index", ":", "int", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Finds the i such that self._index_start[i] <= index < self._index_start[i+1]. Since the array is sorted, we use binary search to find it.\n        \"\"\"", "\n", "return", "bisect", ".", "bisect", "(", "self", ".", "_index_starts", ",", "index", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset.__getitem__": [[169, 184], ["data.FileBackedSentenceDataset._find_file", "typing.cast", "data.FileBackedSentenceDataset._set_cached_file"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._find_file", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._set_cached_file"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "Row", ":", "\n", "        ", "file_index", "=", "self", ".", "_find_file", "(", "index", ")", "\n", "filename", "=", "self", ".", "files", "[", "file_index", "]", "\n", "dataset", ":", "TensorDataset", "\n", "\n", "if", "self", ".", "_cached", "is", "None", "or", "filename", "!=", "self", ".", "_cached", "[", "0", "]", ":", "\n", "# this is called once per batch, so it's working as expected. It's just slow.", "\n", "            ", "self", ".", "_set_cached_file", "(", "filename", ")", "\n", "\n", "", "name", ",", "dataset", "=", "self", ".", "_cached", "# type: ignore", "\n", "# (OO code sucks, I know that self._cached is no longer None because we call self._set_cached_file)", "\n", "\n", "return", "cast", "(", "\n", "Tuple", "[", "Tensor", ",", "Tensor", ",", "Tensor", "]", ",", "\n", "dataset", "[", "index", "-", "self", ".", "_index_starts", "[", "file_index", "]", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._set_cached_file": [[186, 190], ["open", "pickle.load"], "methods", ["None"], ["", "def", "_set_cached_file", "(", "self", ",", "filename", ":", "str", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "dataset", "=", "pickle", ".", "load", "(", "f", ")", "\n", "self", ".", "_cached", "=", "(", "filename", ",", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset.__len__": [[191, 193], ["sum"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "sum", "(", "self", ".", "_file_lens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.FileBackedSentenceDataset._get_file_len": [[194, 199], ["len", "open", "pickle.load"], "methods", ["None"], ["", "def", "_get_file_len", "(", "self", ",", "filepath", ":", "str", ")", "->", "int", ":", "\n", "        ", "with", "open", "(", "filepath", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "contents", ":", "TensorDataset", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "return", "len", "(", "contents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_val_data": [[201, 206], ["data.FileBackedSentenceDataset"], "function", ["None"], ["", "", "def", "prepare_val_data", "(", "\n", "config", ":", "HyperParameters", ",", "tokenizer", ":", "PreTrainedTokenizerFast", "\n", ")", "->", "None", ":", "\n", "    ", "FileBackedSentenceDataset", "(", "\n", "config", ".", "val_preprocessed_folder", ",", "config", ".", "batch_size", ",", "[", "config", ".", "val_file", "]", ",", "tokenizer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_test_data": [[209, 217], ["data.FileBackedSentenceDataset"], "function", ["None"], ["", "def", "prepare_test_data", "(", "\n", "config", ":", "HyperParameters", ",", "tokenizer", ":", "PreTrainedTokenizerFast", "\n", ")", "->", "None", ":", "\n", "    ", "FileBackedSentenceDataset", "(", "\n", "config", ".", "test_preprocessed_folder", ",", "\n", "config", ".", "batch_size", ",", "\n", "[", "config", ".", "test_file", "]", ",", "\n", "tokenizer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_train_data": [[220, 234], ["data.FileBackedSentenceDataset", "data.FileBackedSentenceDataset", "os.listdir"], "function", ["None"], ["", "def", "prepare_train_data", "(", "\n", "config", ":", "HyperParameters", ",", "tokenizer", ":", "PreTrainedTokenizerFast", "\n", ")", "->", "None", ":", "\n", "    ", "if", "config", ".", "train_file", ":", "\n", "        ", "FileBackedSentenceDataset", "(", "\n", "config", ".", "train_preprocessed_folder", ",", "\n", "config", ".", "batch_size", ",", "\n", "[", "config", ".", "train_file", "]", ",", "\n", "tokenizer", ",", "\n", ")", "\n", "", "elif", "config", ".", "train_folder", ":", "\n", "        ", "train_files", "=", "[", "config", ".", "train_folder", "/", "f", "for", "f", "in", "os", ".", "listdir", "(", "config", ".", "train_folder", ")", "]", "\n", "FileBackedSentenceDataset", "(", "\n", "config", ".", "train_preprocessed_folder", ",", "config", ".", "batch_size", ",", "train_files", ",", "tokenizer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_train_dataloader": [[237, 253], ["data.FileBackedSentenceDataset", "torch.utils.data.DataLoader", "torch.utils.data.Subset", "list", "data.RandomBatchSampler", "range", "min", "len"], "function", ["None"], ["", "", "def", "get_train_dataloader", "(", "\n", "config", ":", "HyperParameters", ",", "small", ":", "bool", "=", "False", "\n", ")", "->", "DataLoader", ":", "# type: ignore", "\n", "    ", "train_dataset", "=", "FileBackedSentenceDataset", "(", "\n", "config", ".", "train_preprocessed_folder", ",", "size", "=", "config", ".", "batch_size", "\n", ")", "\n", "\n", "if", "small", ":", "\n", "        ", "train_dataset", "=", "Subset", "(", "# type: ignore", "\n", "train_dataset", ",", "list", "(", "range", "(", "min", "(", "len", "(", "train_dataset", ")", ",", "config", ".", "batch_size", "*", "8", ")", ")", ")", "\n", ")", "\n", "\n", "", "return", "DataLoader", "(", "\n", "train_dataset", ",", "\n", "pin_memory", "=", "True", ",", "\n", "batch_sampler", "=", "RandomBatchSampler", "(", "train_dataset", ",", "config", ".", "batch_size", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader": [[256, 268], ["data.FileBackedSentenceDataset", "torch.utils.data.DataLoader", "torch.utils.data.Subset", "list", "range"], "function", ["None"], ["", "def", "get_val_dataloader", "(", "\n", "config", ":", "HyperParameters", ",", "small", ":", "bool", "=", "False", "\n", ")", "->", "DataLoader", ":", "# type: ignore", "\n", "    ", "val_dataset", "=", "FileBackedSentenceDataset", "(", "\n", "config", ".", "val_preprocessed_folder", ",", "size", "=", "config", ".", "batch_size", "\n", ")", "\n", "\n", "if", "small", ":", "\n", "        ", "val_dataset", "=", "Subset", "(", "val_dataset", ",", "list", "(", "range", "(", "config", ".", "batch_size", "*", "8", ")", ")", ")", "# type: ignore", "\n", "\n", "", "return", "DataLoader", "(", "\n", "val_dataset", ",", "shuffle", "=", "False", ",", "pin_memory", "=", "True", ",", "batch_size", "=", "config", ".", "batch_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_test_dataloader": [[271, 283], ["data.FileBackedSentenceDataset", "torch.utils.data.DataLoader", "torch.utils.data.Subset", "list", "range"], "function", ["None"], ["", "def", "get_test_dataloader", "(", "\n", "config", ":", "HyperParameters", ",", "small", ":", "bool", "=", "False", "\n", ")", "->", "DataLoader", ":", "# type: ignore", "\n", "    ", "test_dataset", "=", "FileBackedSentenceDataset", "(", "\n", "config", ".", "test_preprocessed_folder", ",", "size", "=", "config", ".", "batch_size", "\n", ")", "\n", "\n", "if", "small", ":", "\n", "        ", "test_dataset", "=", "Subset", "(", "test_dataset", ",", "list", "(", "range", "(", "config", ".", "batch_size", "*", "8", ")", ")", ")", "# type: ignore", "\n", "\n", "", "return", "DataLoader", "(", "\n", "test_dataset", ",", "shuffle", "=", "False", ",", "pin_memory", "=", "True", ",", "batch_size", "=", "config", ".", "batch_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.run_language_modeling.get_dataset": [[142, 156], ["transformers.LineByLineTextDataset", "transformers.TextDataset"], "function", ["None"], ["", "def", "get_dataset", "(", "\n", "args", ":", "DataTrainingArguments", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "evaluate", "=", "False", "\n", ")", ":", "\n", "    ", "file_path", "=", "args", ".", "eval_data_file", "if", "evaluate", "else", "args", ".", "train_data_file", "\n", "if", "args", ".", "line_by_line", ":", "\n", "        ", "return", "LineByLineTextDataset", "(", "\n", "tokenizer", "=", "tokenizer", ",", "file_path", "=", "file_path", ",", "block_size", "=", "args", ".", "block_size", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "TextDataset", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "file_path", "=", "file_path", ",", "\n", "block_size", "=", "args", ".", "block_size", ",", "\n", "overwrite_cache", "=", "args", ".", "overwrite_cache", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.run_language_modeling.main": [[159, 327], ["transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "AutoModelWithLMHead.from_config.resize_token_embeddings", "transformers.DataCollatorForLanguageModeling", "transformers.Trainer", "ValueError", "os.path.exists", "os.listdir", "ValueError", "bool", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelWithLMHead.from_pretrained", "logger.info", "transformers.AutoModelWithLMHead.from_config", "len", "ValueError", "min", "run_language_modeling.get_dataset", "run_language_modeling.get_dataset", "transformers.Trainer.train", "transformers.Trainer.save_model", "transformers.Trainer.is_world_master", "logger.info", "transformers.Trainer.evaluate", "math.exp", "os.path.join", "transformers.Trainer.is_world_master", "results.update", "transformers.AutoConfig.from_pretrained", "logger.warning", "transformers.AutoTokenizer.from_pretrained", "print", "ValueError", "AutoTokenizer.from_pretrained.save_pretrained", "bool", "os.path.isdir", "open", "logger.info", "sorted", "result.keys", "logger.info", "writer.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.run_language_modeling.get_dataset", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.run_language_modeling.get_dataset", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train"], ["", "", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "\n", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", "\n", ")", "\n", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "if", "data_args", ".", "eval_data_file", "is", "None", "and", "training_args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"", "\n", "\"or remove the --do_eval argument.\"", "\n", ")", "\n", "\n", "", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "if", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", "\n", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "model_args", ")", "\n", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"", "\n", "\"and load it from here, using --tokenizer_name\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelWithLMHead", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "if", "(", "\n", "config", ".", "model_type", "in", "[", "\"bert\"", ",", "\"roberta\"", ",", "\"distilbert\"", ",", "\"camembert\"", "]", "\n", "and", "not", "data_args", ".", "mlm", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"", "\n", "\"flag (masked language modeling).\"", "\n", ")", "\n", "\n", "", "if", "data_args", ".", "block_size", "<=", "0", ":", "\n", "        ", "data_args", ".", "block_size", "=", "tokenizer", ".", "max_len", "\n", "# Our input block size will be the max possible for the model", "\n", "", "else", ":", "\n", "        ", "data_args", ".", "block_size", "=", "min", "(", "data_args", ".", "block_size", ",", "tokenizer", ".", "max_len", ")", "\n", "\n", "# Get datasets", "\n", "\n", "", "train_dataset", "=", "(", "\n", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ")", "if", "training_args", ".", "do_train", "else", "None", "\n", ")", "\n", "eval_dataset", "=", "(", "\n", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "evaluate", "=", "True", ")", "\n", "if", "training_args", ".", "do_eval", "\n", "else", "None", "\n", ")", "\n", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "mlm", "=", "data_args", ".", "mlm", ",", "\n", "mlm_probability", "=", "data_args", ".", "mlm_probability", ",", "\n", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "eval_dataset", ",", "\n", "prediction_loss_only", "=", "True", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "model_path", "=", "(", "\n", "model_args", ".", "model_name_or_path", "\n", "if", "model_args", ".", "model_name_or_path", "is", "not", "None", "\n", "and", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", "\n", "else", "None", "\n", ")", "\n", "trainer", ".", "train", "(", "model_path", "=", "model_path", ")", "\n", "trainer", ".", "save_model", "(", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "eval_output", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "eval_output", "[", "\"eval_loss\"", "]", ")", "\n", "result", "=", "{", "\"perplexity\"", ":", "perplexity", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results_lm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.deleted_character_mask": [[10, 22], ["len", "mask.extend", "mask.extend", "str", "len", "len", "str", "str"], "function", ["None"], ["def", "deleted_character_mask", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "mask", "=", "[", "False", "]", "*", "(", "len", "(", "str", "(", "sent_elem", ".", "text", ")", ")", "if", "sent_elem", ".", "text", "else", "0", ")", "\n", "\n", "for", "del_ins", "in", "sent_elem", ":", "\n", "        ", "if", "del_ins", ".", "tag", "==", "\"del\"", "and", "del_ins", ".", "text", ":", "\n", "            ", "mask", ".", "extend", "(", "[", "True", "]", "*", "len", "(", "str", "(", "del_ins", ".", "text", ")", ")", ")", "\n", "", "if", "del_ins", ".", "tail", ":", "\n", "            ", "mask", ".", "extend", "(", "[", "False", "]", "*", "len", "(", "str", "(", "del_ins", ".", "tail", ")", ")", ")", "\n", "\n", "", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.surrounding_inserted_character_mask": [[24, 49], ["len", "mask.extend", "mask.extend", "str", "len", "len", "len", "len", "str", "str", "len", "str"], "function", ["None"], ["", "def", "surrounding_inserted_character_mask", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "mask", "=", "[", "False", "]", "*", "(", "len", "(", "str", "(", "sent_elem", ".", "text", ")", ")", "if", "sent_elem", ".", "text", "else", "0", ")", "\n", "\n", "in_ins", "=", "False", "\n", "\n", "for", "del_ins", "in", "sent_elem", ":", "\n", "        ", "if", "del_ins", ".", "tag", "==", "\"del\"", ":", "\n", "            ", "in_ins", "=", "False", "\n", "mask", ".", "extend", "(", "[", "False", "]", "*", "len", "(", "str", "(", "del_ins", ".", "text", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "not", "in_ins", ",", "\"nested <ins>\"", "\n", "in_ins", "=", "True", "\n", "if", "len", "(", "mask", ")", ">", "0", ":", "\n", "                ", "mask", "[", "-", "1", "]", "=", "True", "\n", "", "if", "len", "(", "mask", ")", ">", "1", ":", "\n", "                ", "mask", "[", "-", "2", "]", "=", "True", "\n", "", "", "if", "del_ins", ".", "tail", ":", "\n", "            ", "mask", ".", "extend", "(", "[", "False", "]", "*", "len", "(", "str", "(", "del_ins", ".", "tail", ")", ")", ")", "\n", "if", "in_ins", ":", "\n", "                ", "mask", "[", "-", "len", "(", "str", "(", "del_ins", ".", "tail", ")", ")", "]", "=", "True", "\n", "in_ins", "=", "False", "\n", "\n", "", "", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.test_masks": [[51, 68], ["pathlib.Path", "tqdm.tqdm", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "important_tokens.deleted_character_mask", "str", "len", "len", "important_tokens.surrounding_inserted_character_mask", "aesw_to_sentences.extract_sentence", "len", "len", "print", "ValueError", "aesw_to_sentences.extract_sentence"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.deleted_character_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.surrounding_inserted_character_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence"], ["", "def", "test_masks", "(", ")", "->", "None", ":", "\n", "    ", "xml_filepath", "=", "Path", "(", "\"./data-unversioned/aesw/aesw2016(v1.2)_dev.xml\"", ")", "\n", "\n", "for", "event", ",", "sent_elem", "in", "tqdm", "(", "etree", ".", "iterparse", "(", "str", "(", "xml_filepath", ")", ",", "tag", "=", "\"sentence\"", ")", ")", ":", "\n", "        ", "sent_id", "=", "aesw_to_sentences", ".", "extract_sentence_id", "(", "sent_elem", ")", "\n", "mask", "=", "deleted_character_mask", "(", "sent_elem", ")", "\n", "assert", "len", "(", "mask", ")", "==", "len", "(", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", ")", ",", "sent_id", "\n", "try", ":", "\n", "            ", "mask", "=", "surrounding_inserted_character_mask", "(", "sent_elem", ")", "\n", "assert", "len", "(", "mask", ")", "==", "len", "(", "\n", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", "\n", ")", ",", "sent_id", "\n", "", "except", "AssertionError", "as", "err", ":", "\n", "            ", "print", "(", "sent_id", ",", "err", ")", "\n", "raise", "\n", "", "except", "IndexError", "as", "err", ":", "\n", "            ", "raise", "ValueError", "(", "sent_id", ",", "err", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.merge_char_masks": [[70, 73], ["len", "len", "zip"], "function", ["None"], ["", "", "", "def", "merge_char_masks", "(", "mask1", ":", "List", "[", "bool", "]", ",", "mask2", ":", "List", "[", "bool", "]", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "assert", "len", "(", "mask1", ")", "==", "len", "(", "mask2", ")", "\n", "return", "[", "a", "or", "b", "for", "a", ",", "b", "in", "zip", "(", "mask1", ",", "mask2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.get_token_mask": [[75, 98], ["enumerate", "set", "zip", "enumerate", "ch_idx_to_tok_idx.extend", "len", "len", "len", "len", "set.add", "token_mask.append", "token_mask.append", "len"], "function", ["None"], ["", "def", "get_token_mask", "(", "tokens", ":", "List", "[", "str", "]", ",", "important_char_mask", ":", "List", "[", "bool", "]", ")", "->", "List", "[", "bool", "]", ":", "\n", "    ", "ch_idx_to_tok_idx", "=", "[", "]", "\n", "\n", "for", "tok_idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "ch_idx_to_tok_idx", ".", "extend", "(", "[", "tok_idx", "]", "*", "len", "(", "token", ")", ")", "\n", "\n", "", "assert", "len", "(", "ch_idx_to_tok_idx", ")", "==", "len", "(", "\n", "important_char_mask", "\n", ")", ",", "f\"{len(ch_idx_to_tok_idx)} != {len(important_char_mask)}\"", "\n", "\n", "important_token_indices", "=", "set", "(", ")", "\n", "for", "tok_idx", ",", "ch_mask", "in", "zip", "(", "ch_idx_to_tok_idx", ",", "important_char_mask", ")", ":", "\n", "        ", "if", "ch_mask", ":", "\n", "            ", "important_token_indices", ".", "add", "(", "tok_idx", ")", "\n", "\n", "", "", "token_mask", "=", "[", "]", "\n", "for", "tok_idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "tok_idx", "in", "important_token_indices", ":", "\n", "            ", "token_mask", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "            ", "token_mask", ".", "append", "(", "False", ")", "\n", "\n", "", "", "return", "token_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.important_tokens.show_token_mask": [[100, 118], ["zip", "print", "print", "len", "len", "sent_builder.append", "len", "len", "mask_builder.append", "mask_builder.append", "len", "len", "len", "len"], "function", ["None"], ["", "def", "show_token_mask", "(", "tokens", ":", "List", "[", "str", "]", ",", "token_mask", ":", "List", "[", "bool", "]", ")", "->", "None", ":", "\n", "    ", "assert", "len", "(", "tokens", ")", "==", "len", "(", "token_mask", ")", "\n", "sent_builder", "=", "[", "]", "\n", "mask_builder", "=", "[", "]", "\n", "for", "tok", ",", "mask", "in", "zip", "(", "tokens", ",", "token_mask", ")", ":", "\n", "        ", "sent_builder", ".", "append", "(", "tok", ")", "\n", "if", "mask", ":", "\n", "# print(f\"'{tok}'\", len(tok))", "\n", "            ", "mask_builder", ".", "append", "(", "\"\"", ".", "join", "(", "[", "\"*\"", "]", "*", "len", "(", "tok", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "mask_builder", ".", "append", "(", "\"\"", ".", "join", "(", "[", "\" \"", "]", "*", "len", "(", "tok", ")", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "mask_builder", ")", "==", "len", "(", "\n", "sent_builder", "\n", ")", ",", "f\"{len(mask_builder)} != {len(sent_builder)}\"", "\n", "\n", "print", "(", "\" \"", ".", "join", "(", "sent_builder", ")", ")", "\n", "print", "(", "\" \"", ".", "join", "(", "mask_builder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.__init__": [[13, 16], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "it", ":", "Iterator", "[", "T", "]", ")", ":", "\n", "        ", "self", ".", "peeked", "=", "None", "\n", "self", ".", "it", "=", "it", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.peek": [[17, 24], ["next"], "methods", ["None"], ["", "def", "peek", "(", "self", ")", "->", "Optional", "[", "T", "]", ":", "\n", "        ", "if", "not", "self", ".", "peeked", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "peeked", "=", "next", "(", "self", ".", "it", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "return", "None", "\n", "", "", "return", "self", ".", "peeked", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.pop": [[25, 35], ["next"], "methods", ["None"], ["", "def", "pop", "(", "self", ")", "->", "Optional", "[", "T", "]", ":", "\n", "        ", "if", "self", ".", "peeked", ":", "\n", "            ", "result", "=", "self", ".", "peeked", "\n", "self", ".", "peeked", "=", "None", "\n", "return", "result", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "next", "(", "self", ".", "it", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.__iter__": [[36, 38], ["None"], "methods", ["None"], ["", "", "", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "T", "]", ":", "\n", "        ", "return", "self", ".", "it", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.sorted_csv_files_reader": [[48, 90], ["open", "csv.reader", "next", "csv_merge.Peekable", "len", "csv_merge.Option", "set", "csv_readers.items", "csv_readers[].pop", "pathlib.Path", "peekable.peek", "opened_files[].close", "set.add", "len", "csv_merge.Option", "int", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.pop", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.Peekable.peek"], ["", "def", "sorted_csv_files_reader", "(", "files", ":", "List", "[", "Path", "]", ")", "->", "Iterator", "[", "Tuple", "[", "str", ",", "int", "]", "]", ":", "\n", "    ", "opened_files", ":", "Dict", "[", "Path", ",", "IO", "[", "str", "]", "]", "=", "{", "}", "\n", "csv_readers", ":", "Dict", "[", "Path", ",", "Peekable", "[", "List", "[", "str", "]", "]", "]", "=", "{", "}", "\n", "\n", "for", "filepath", "in", "files", ":", "\n", "        ", "opened_file", "=", "open", "(", "filepath", ",", "\"r\"", ")", "\n", "opened_files", "[", "filepath", "]", "=", "opened_file", "\n", "\n", "reader", "=", "csv", ".", "reader", "(", "opened_file", ")", "\n", "next", "(", "reader", ")", "# skip header", "\n", "\n", "csv_readers", "[", "filepath", "]", "=", "Peekable", "(", "reader", ")", "\n", "\n", "", "while", "len", "(", "opened_files", ")", ">", "0", ":", "\n", "        ", "best", "=", "Option", "(", "sys", ".", "maxsize", ",", "\"\"", ",", "\"\"", ",", "\"\"", ",", "Path", "(", ")", ")", "\n", "finished_files", "=", "set", "(", ")", "\n", "\n", "for", "path", ",", "peekable", "in", "csv_readers", ".", "items", "(", ")", ":", "\n", "            ", "top", "=", "peekable", ".", "peek", "(", ")", "\n", "\n", "if", "not", "top", ":", "\n", "                ", "finished_files", ".", "add", "(", "path", ")", "\n", "continue", "# should close the file somehow", "\n", "\n", "", "identifier", ",", "sent", ",", "label", "=", "top", "\n", "\n", "if", "len", "(", "sent", ")", "<", "best", ".", "length", ":", "\n", "                ", "best", "=", "Option", "(", "len", "(", "sent", ")", ",", "identifier", ",", "sent", ",", "label", ",", "path", ")", "\n", "\n", "# close any finished files:", "\n", "", "", "for", "path", "in", "finished_files", ":", "\n", "            ", "del", "csv_readers", "[", "path", "]", "\n", "opened_files", "[", "path", "]", ".", "close", "(", ")", "\n", "del", "opened_files", "[", "path", "]", "\n", "\n", "# now return the best option and pop it off the iterator", "\n", "", "if", "best", ".", "path", "not", "in", "csv_readers", ":", "\n", "            ", "return", "\n", "\n", "", "csv_readers", "[", "best", ".", "path", "]", ".", "pop", "(", ")", "\n", "\n", "yield", "best", ".", "sentence", ",", "int", "(", "best", ".", "label", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer": [[15, 29], ["transformers.AutoTokenizer.from_pretrained", "AutoTokenizer.from_pretrained.add_special_tokens", "AutoTokenizer.from_pretrained.convert_tokens_to_ids", "set", "set"], "function", ["None"], ["def", "get_tokenizer", "(", "config", ":", "HyperParameters", ")", "->", "PreTrainedTokenizerFast", ":", "\n", "    ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "config", ".", "tokenizer_name", ",", "do_lower_case", "=", "True", ",", "use_fast", "=", "True", "\n", ")", "\n", "\n", "tokenizer", ".", "add_special_tokens", "(", "{", "\"additional_special_tokens\"", ":", "AESW_TOKENS", "}", ")", "\n", "\n", "global", "AESW_IDS", "\n", "AESW_IDS", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "AESW_TOKENS", ")", "\n", "\n", "global", "BERT_IDS", "\n", "BERT_IDS", "=", "set", "(", "tokenizer", ".", "all_special_ids", ")", "-", "set", "(", "AESW_IDS", ")", "\n", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode": [[31, 35], ["list", "typing.cast", "filter", "tokenizer.decode"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode"], ["", "def", "decode", "(", "ids", ":", "List", "[", "int", "]", ",", "tokenizer", ":", "PreTrainedTokenizerFast", ")", "->", "str", ":", "\n", "    ", "ids", "=", "list", "(", "filter", "(", "lambda", "i", ":", "i", "not", "in", "BERT_IDS", ",", "ids", ")", ")", "\n", "\n", "return", "cast", "(", "str", ",", "tokenizer", ".", "decode", "(", "ids", ",", "skip_special_tokens", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.encode_batch": [[37, 71], ["max", "tokenizer", "input_ids.append", "padded_input_ids.append", "attention_masks.append", "len", "len", "torch.tensor", "torch.tensor", "len"], "function", ["None"], ["", "def", "encode_batch", "(", "\n", "sentences", ":", "List", "[", "str", "]", ",", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Tensor", "]", ",", "List", "[", "Tensor", "]", "]", ":", "\n", "    ", "\"\"\"\n    Tokenizes and pads sentences to the max length in the list of sentences. Returns tensors for input ids and tensors for attention masking.\n    \"\"\"", "\n", "input_ids", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "text", "in", "sentences", ":", "\n", "        ", "tokenized", "=", "tokenizer", "(", "\n", "text", "=", "text", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "MAX_LEN", ",", "# Do truncate to MAX_LEN", "\n", "truncation", "=", "True", ",", "# Do truncate", "\n", "padding", "=", "False", ",", "# Don't pad", "\n", "return_attention_mask", "=", "False", ",", "\n", "return_token_type_ids", "=", "False", ",", "\n", ")", "\n", "input_ids", ".", "append", "(", "tokenized", "[", "\"input_ids\"", "]", ")", "\n", "\n", "", "max_len", "=", "max", "(", "[", "len", "(", "i", ")", "for", "i", "in", "input_ids", "]", ")", "\n", "\n", "padded_input_ids", ":", "List", "[", "Tensor", "]", "=", "[", "]", "\n", "attention_masks", ":", "List", "[", "Tensor", "]", "=", "[", "]", "\n", "\n", "for", "sent", "in", "input_ids", ":", "\n", "        ", "num_pads", "=", "max_len", "-", "len", "(", "sent", ")", "\n", "\n", "padded_input_ids", ".", "append", "(", "\n", "torch", ".", "tensor", "(", "sent", "+", "[", "tokenizer", ".", "pad_token_id", "]", "*", "num_pads", ")", "# type: ignore", "\n", ")", "\n", "attention_masks", ".", "append", "(", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "sent", ")", "+", "[", "0", "]", "*", "num_pads", ")", ")", "# type: ignore", "\n", "\n", "", "return", "padded_input_ids", ",", "attention_masks", "\n", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.make_copy_command": [[10, 12], ["None"], "function", ["None"], ["def", "make_copy_command", "(", "config_path", ":", "str", ")", "->", "str", ":", "\n", "    ", "return", "f'python -m paper {config_path} --copy-models \"$TMPDIR\"'", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_script": [[14, 66], ["open", "file.write", "make_job_scripts.make_copy_command"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.make_copy_command"], ["", "def", "write_script", "(", "\n", "name", ":", "str", ",", "copy_train_command", ":", "str", ",", "experiment_folder", ":", "str", ",", "script_filename", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "\n", "    ", "config_path", "=", "f\"./experiments/{experiment_folder}/params.toml\"", "\n", "\n", "script_text", "=", "f\"\"\"#!/bin/bash\n#SBATCH --job-name={name}\n#SBATCH --time=12:00:00\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --nodes=1 --ntasks-per-node=48 --gpus-per-node=2\n#SBATCH --account=PAS1769\n\nset -x\n\n# shellcheck disable=SC2064\ntrap \"cd $SLURM_SUBMIT_DIR;mkdir $SLURM_JOB_ID;cp -R $TMPDIR/* $SLURM_JOB_ID;exit\" TERM\n\nmodule load python/3.7-2019.10\n\ncd \"$TMPDIR\" || exit\n\npython --version\npython -m venv venv\n# shellcheck disable=SC1091\n. ./venv/bin/activate\n\npip install torch torchvision transformers==3.0.2 toml typing-extensions\n\ncd \"$SLURM_SUBMIT_DIR\" || exit\n\ncp aesw-dev.csv \"$TMPDIR\"\n{copy_train_command}\ncp -R paper \"$TMPDIR\"\ncp -R experiments \"$TMPDIR\"\nmkdir -p \"$TMPDIR/models\"\n{make_copy_command(config_path)}\n\ncd \"$TMPDIR\" || exit\n\n/usr/bin/time python -m paper {config_path} --preprocess train,val\n\n/usr/bin/time python -m paper {config_path} --main-loop\n\ncd \"$SLURM_SUBMIT_DIR\" || exit;\nmkdir \"$SLURM_JOB_ID\";\ncp -R \"$TMPDIR/models\" \"$SLURM_JOB_ID\";\ncp -R \"$TMPDIR/experiments\" \"$SLURM_JOB_ID\";\nexit\n\"\"\"", "\n", "with", "open", "(", "script_filename", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "script_text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_start_script": [[68, 81], ["os.chmod", "open", "file.write"], "function", ["None"], ["", "", "def", "write_start_script", "(", ")", "->", "None", ":", "\n", "    ", "start_script_text", "=", "f\"\"\"#!/bin/bash\n\nfor script in {JOB_FOLDER}/*main_loop*.sh\ndo\n  echo \"$script\"\n  sbatch \"$script\"\ndone\n\"\"\"", "\n", "filename", "=", "f\"{JOB_FOLDER}/start.sh\"", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "start_script_text", ")", "\n", "", "os", ".", "chmod", "(", "filename", ",", "0o755", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_inference_script": [[83, 136], ["open", "file.write", "make_job_scripts.make_copy_command"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.make_copy_command"], ["", "def", "write_inference_script", "(", "\n", "name", ":", "str", ",", "checkpoint_hash", ":", "str", ",", "experiment_folder", ":", "str", ",", "script_filename", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "config_path", "=", "f\"./experiments/{experiment_folder}/params.toml\"", "\n", "\n", "script_text", "=", "f\"\"\"#!/bin/bash\n#SBATCH --job-name={name}\n#SBATCH --time=4:00:00\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --nodes=1 --ntasks-per-node=48\n#SBATCH --account=PAS1769\n\nset -x\n\n# shellcheck disable=SC2064\ntrap \"cd $SLURM_SUBMIT_DIR;mkdir $SLURM_JOB_ID;cp -R $TMPDIR/* $SLURM_JOB_ID;exit\" TERM\n\nmodule load python/3.7-2019.10\n\ncd \"$TMPDIR\" || exit\n\npython --version\npython -m venv venv\n# shellcheck disable=SC1091\n. ./venv/bin/activate\n\npip install torch torchvision transformers==3.0.2 toml typing-extensions\n\ncd \"$SLURM_SUBMIT_DIR\" || exit\n\nCHECKPOINT=./models/{checkpoint_hash}.pt\n\ncp aesw-test.csv \"$TMPDIR\"\ncp -R paper \"$TMPDIR\"\ncp -R experiments \"$TMPDIR\"\nmkdir -p \"$TMPDIR/models\"\n{make_copy_command(config_path)}\ncp $CHECKPOINT \"$TMPDIR/models\"\n\ncd \"$TMPDIR\" || exit\n\n/usr/bin/time python -m paper {config_path} --preprocess test\n\n/usr/bin/time python -m paper {config_path} --inference-test $CHECKPOINT --force-cpu\n\ncd \"$SLURM_SUBMIT_DIR\" || exit;\nmkdir \"$SLURM_JOB_ID\";\ncp -R \"$TMPDIR\" \"$SLURM_JOB_ID\";\nexit\n\"\"\"", "\n", "\n", "with", "open", "(", "script_filename", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "script_text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.main": [[138, 146], ["make_job_scripts.write_start_script", "make_job_scripts.write_script", "make_job_scripts.write_inference_script"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_start_script", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_script", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.make_job_scripts.write_inference_script"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "for", "script", "in", "SCRIPTS", ":", "\n", "        ", "write_script", "(", "**", "script", ")", "\n", "\n", "", "for", "script", "in", "INFERENCE_SCRIPTS", ":", "\n", "        ", "write_inference_script", "(", "**", "script", ")", "\n", "\n", "", "write_start_script", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.stop_early": [[14, 30], ["min"], "function", ["None"], ["def", "stop_early", "(", "history", ":", "List", "[", "Tuple", "[", "Path", ",", "float", "]", "]", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Returns true if the last 5 epochs have all been more than the minimum validation loss\n    \"\"\"", "\n", "patience", "=", "5", "\n", "\n", "if", "not", "history", ":", "\n", "        ", "return", "False", "\n", "\n", "", "min_val_loss", "=", "min", "(", "[", "loss", "for", "_", ",", "loss", "in", "history", "]", ")", "\n", "\n", "for", "_", ",", "epoch_val_loss", "in", "history", "[", "-", "patience", ":", "]", ":", "\n", "        ", "if", "epoch_val_loss", "<=", "min_val_loss", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train": [[32, 91], ["print", "util.get_device", "model.train", "enumerate", "print", "disk.Checkpoint", "int", "util.my_tqdm", "batch[].to", "batch[].to", "batch[].to", "model.zero_grad", "model", "torch.cross_entropy", "F.cross_entropy.backward", "optimizer.step", "F.cross_entropy.detach().item", "len", "len", "range", "disk.Checkpoint", "disk.save_checkpoint", "model.view", "batch[].to.view", "int", "F.cross_entropy.detach", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.save_checkpoint"], ["", "def", "train", "(", "\n", "checkpoint", ":", "Checkpoint", ",", "\n", "config", ":", "HyperParameters", ",", "\n", "train_loader", ":", "DataLoader", ",", "# type: ignore", "\n", ")", "->", "Result", "[", "Tuple", "[", "float", ",", "Checkpoint", "]", "]", ":", "\n", "    ", "\"\"\"\n    Tries to finish the current epoch while saving checkpoints at the appropriate times. It does not save the model at the end of training.\n    \"\"\"", "\n", "\n", "print", "(", "\"\\n--- Training ---\"", ")", "\n", "\n", "device", "=", "util", ".", "get_device", "(", ")", "\n", "\n", "epoch_loss", "=", "checkpoint", ".", "total_train_loss", "\n", "\n", "model", "=", "checkpoint", ".", "model", "\n", "model", ".", "train", "(", ")", "\n", "\n", "optimizer", "=", "checkpoint", ".", "optimizer", "\n", "\n", "checkpoint_steps", ":", "List", "[", "int", "]", "=", "[", "\n", "int", "(", "len", "(", "train_loader", ")", "*", "config", ".", "checkpoint_interval", "*", "(", "i", "+", "1", ")", ")", "\n", "for", "i", "in", "range", "(", "int", "(", "1", "/", "config", ".", "checkpoint_interval", ")", ")", "\n", "]", "\n", "\n", "for", "seen", ",", "batch", "in", "enumerate", "(", "util", ".", "my_tqdm", "(", "train_loader", ")", ")", ":", "\n", "        ", "if", "seen", "<", "checkpoint", ".", "seen", ":", "\n", "            ", "continue", "# catch up to where we started", "\n", "\n", "", "if", "seen", "in", "checkpoint_steps", ":", "\n", "            ", "checkpoint", "=", "Checkpoint", "(", "\n", "model", ",", "optimizer", ",", "checkpoint", ".", "epoch", ",", "seen", ",", "epoch_loss", "\n", ")", "\n", "disk", ".", "save_checkpoint", "(", "checkpoint", ",", "config", ")", "\n", "\n", "", "input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", ")", "\n", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ".", "view", "(", "-", "1", ",", "2", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "# type: ignore", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "epoch_loss", "+=", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "avg_loss", "=", "epoch_loss", "/", "len", "(", "train_loader", ")", "\n", "print", "(", "f\"Training loss: {avg_loss:.3f}\"", ")", "\n", "\n", "checkpoint", "=", "Checkpoint", "(", "\n", "model", ",", "optimizer", ",", "checkpoint", ".", "epoch", ",", "len", "(", "train_loader", ")", ",", "epoch_loss", "\n", ")", "\n", "\n", "return", "avg_loss", ",", "checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.validate": [[93, 131], ["print", "util.get_device", "structures.ConfusionMatrix", "model.eval", "util.my_tqdm", "print", "batch[].to", "batch[].to", "batch[].to", "torch.cross_entropy", "F.cross_entropy.detach().item", "len", "torch.no_grad", "torch.no_grad", "model", "model.view", "batch[].to.view", "evaluating.get_confusion_matrix", "F.cross_entropy.detach"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.evaluating.get_confusion_matrix"], ["", "def", "validate", "(", "\n", "model", ":", "models", ".", "SentenceClassificationModel", ",", "val_loader", ":", "DataLoader", "# type: ignore", "\n", ")", "->", "Tuple", "[", "float", ",", "ConfusionMatrix", "]", ":", "\n", "    ", "print", "(", "\"\\n--- Validation ---\"", ")", "\n", "\n", "device", "=", "util", ".", "get_device", "(", ")", "\n", "\n", "total_loss", "=", "0.0", "\n", "total_confusion", "=", "ConfusionMatrix", "(", "0", ",", "0", ",", "0", ",", "0", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "val_loader", ")", ":", "\n", "        ", "input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", ")", "\n", "\n", "", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ".", "view", "(", "-", "1", ",", "2", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", "\n", "total_confusion", "=", "total_confusion", "+", "evaluating", ".", "get_confusion_matrix", "(", "\n", "logits", ",", "labels", "\n", ")", "\n", "\n", "", "avg_loss", "=", "total_loss", "/", "len", "(", "val_loader", ")", "\n", "\n", "print", "(", "\n", "f\"\"\"Validation loss: {avg_loss:.3f}\nValidation accuracy: {total_confusion.accuracy:.3f}\nValidation F1: {total_confusion.f1:.3f}\nValidation precision: {total_confusion.precision:.3f}\nValidation recall: {total_confusion.recall:.3f}\"\"\"", "\n", ")", "\n", "\n", "return", "avg_loss", ",", "total_confusion", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train_and_validate": [[133, 163], ["data.get_train_dataloader", "data.get_val_dataloader", "training.train", "isinstance", "disk.save_checkpoint", "training.validate", "isinstance", "disk.save_checkpoint"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_train_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.save_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.validate", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.save_checkpoint"], ["", "def", "train_and_validate", "(", "\n", "checkpoint", ":", "Checkpoint", ",", "config", ":", "HyperParameters", ",", "small", ":", "bool", "=", "False", "\n", ")", "->", "Result", "[", "None", "]", ":", "\n", "    ", "\"\"\"\n    Runs a single training and validation epoch, saving checkpoints where appropriate.\n    \"\"\"", "\n", "\n", "train_loader", "=", "data", ".", "get_train_dataloader", "(", "config", ",", "small", "=", "small", ")", "\n", "val_loader", "=", "data", ".", "get_val_dataloader", "(", "config", ",", "small", "=", "small", ")", "\n", "\n", "train_result", "=", "train", "(", "checkpoint", ",", "config", ",", "train_loader", ")", "\n", "if", "isinstance", "(", "train_result", ",", "Exception", ")", ":", "\n", "        ", "return", "train_result", "\n", "\n", "", "train_loss", ",", "checkpoint", "=", "train_result", "\n", "disk", ".", "save_checkpoint", "(", "checkpoint", ",", "config", ")", "\n", "\n", "val_result", "=", "validate", "(", "checkpoint", ".", "model", ",", "val_loader", ")", "\n", "\n", "if", "isinstance", "(", "val_result", ",", "Exception", ")", ":", "\n", "        ", "return", "val_result", "\n", "\n", "", "val_loss", ",", "_", "=", "val_result", "\n", "checkpoint", ".", "avg_val_loss", "=", "val_loss", "\n", "checkpoint", ".", "epoch", "+=", "1", "\n", "checkpoint", ".", "seen", "=", "0", "\n", "checkpoint", ".", "total_train_loss", "=", "0", "\n", "disk", ".", "save_checkpoint", "(", "checkpoint", ",", "config", ")", "\n", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.sanity_check": [[165, 189], ["util.print_device", "disk.new_checkpoint", "isinstance", "training.train_and_validate", "isinstance", "disk.load_latest_checkpoint", "isinstance", "training.train_and_validate", "isinstance"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.print_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.new_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train_and_validate", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_latest_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train_and_validate"], ["", "def", "sanity_check", "(", "config", ":", "HyperParameters", ")", "->", "Result", "[", "None", "]", ":", "\n", "    ", "util", ".", "print_device", "(", ")", "\n", "\n", "checkpoint", "=", "disk", ".", "new_checkpoint", "(", "config", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "Exception", ")", ":", "\n", "        ", "return", "checkpoint", "\n", "\n", "", "result", "=", "train_and_validate", "(", "checkpoint", ",", "config", ",", "small", "=", "True", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "        ", "return", "result", "\n", "\n", "", "next_checkpoint", "=", "disk", ".", "load_latest_checkpoint", "(", "config", ")", "\n", "if", "isinstance", "(", "next_checkpoint", ",", "Exception", ")", ":", "\n", "        ", "return", "next_checkpoint", "\n", "\n", "", "assert", "next_checkpoint", ".", "epoch", "==", "1", "\n", "assert", "next_checkpoint", ".", "seen", "==", "0", "\n", "\n", "# continue training", "\n", "result", "=", "train_and_validate", "(", "next_checkpoint", ",", "config", ",", "small", "=", "True", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "        ", "return", "result", "\n", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.main_loop": [[191, 225], ["util.print_device", "range", "disk.load_latest_checkpoint", "isinstance", "disk.new_checkpoint", "print", "training.train_and_validate", "isinstance", "disk.load_training_history", "isinstance", "training.stop_early", "disk.load_latest_checkpoint", "isinstance", "print"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.print_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_latest_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.new_checkpoint", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.train_and_validate", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_training_history", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.training.stop_early", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_latest_checkpoint"], ["", "def", "main_loop", "(", "config", ":", "HyperParameters", ",", "continuing", ":", "bool", ")", "->", "Result", "[", "None", "]", ":", "\n", "    ", "util", ".", "print_device", "(", ")", "\n", "\n", "if", "continuing", ":", "\n", "        ", "checkpoint", "=", "disk", ".", "load_latest_checkpoint", "(", "config", ")", "\n", "\n", "if", "isinstance", "(", "checkpoint", ",", "Exception", ")", ":", "\n", "            ", "return", "checkpoint", "\n", "", "", "else", ":", "\n", "        ", "checkpoint", "=", "disk", ".", "new_checkpoint", "(", "config", ")", "\n", "\n", "", "for", "epoch", "in", "range", "(", "config", ".", "max_epochs", ")", ":", "\n", "        ", "if", "epoch", "<", "checkpoint", ".", "epoch", ":", "\n", "            ", "continue", "\n", "\n", "", "print", "(", "f\"Training epoch {epoch}.\"", ")", "\n", "result", "=", "train_and_validate", "(", "checkpoint", ",", "config", ")", "\n", "if", "isinstance", "(", "result", ",", "Exception", ")", ":", "\n", "            ", "return", "result", "\n", "\n", "# check if early stopping should occur.", "\n", "", "history", "=", "disk", ".", "load_training_history", "(", "config", ")", "\n", "if", "isinstance", "(", "history", ",", "Exception", ")", ":", "\n", "            ", "return", "history", "\n", "\n", "", "if", "stop_early", "(", "history", ")", ":", "\n", "            ", "print", "(", "\"Stopping early\"", ")", "\n", "break", "\n", "\n", "", "checkpoint", "=", "disk", ".", "load_latest_checkpoint", "(", "config", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "Exception", ")", ":", "\n", "            ", "return", "checkpoint", "\n", "\n", "", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.__init__": [[32, 69], ["toml.load", "int", "float", "int", "pathlib.Path", "pathlib.Path", "pathlib.Path", "float", "structures.HyperParameters.get_preprocessed_folder", "structures.HyperParameters.get_preprocessed_folder", "structures.HyperParameters.get_preprocessed_folder", "os.path.split", "pathlib.Path", "pathlib.Path", "pathlib.Path", "ValueError"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.get_preprocessed_folder", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.get_preprocessed_folder", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.get_preprocessed_folder"], ["def", "__init__", "(", "self", ",", "toml_filepath", ":", "str", ")", "->", "None", ":", "\n", "        ", "parsed", "=", "toml", ".", "load", "(", "[", "toml_filepath", "]", ")", "\n", "\n", "self", ".", "batch_size", "=", "int", "(", "parsed", "[", "\"batch_size\"", "]", ")", "\n", "self", ".", "learning_rate", "=", "float", "(", "parsed", "[", "\"learning_rate\"", "]", ")", "\n", "self", ".", "max_epochs", "=", "int", "(", "parsed", "[", "\"max_epochs\"", "]", ")", "\n", "self", ".", "model_name", "=", "parsed", "[", "\"model_name\"", "]", "\n", "self", ".", "tokenizer_name", "=", "parsed", "[", "\"tokenizer_name\"", "]", "\n", "self", ".", "val_file", "=", "Path", "(", "parsed", "[", "\"val_file\"", "]", ")", "\n", "self", ".", "test_file", "=", "Path", "(", "parsed", "[", "\"test_file\"", "]", ")", "\n", "self", ".", "experiment_name", "=", "parsed", "[", "\"experiment_name\"", "]", "\n", "self", ".", "models_dir", "=", "Path", "(", "parsed", "[", "\"models_dir\"", "]", ")", "\n", "self", ".", "checkpoint_interval", "=", "float", "(", "parsed", "[", "\"checkpoint_interval\"", "]", ")", "\n", "\n", "self", ".", "train_file", "=", "None", "\n", "self", ".", "train_folder", "=", "None", "\n", "if", "\"train_file\"", "in", "parsed", ":", "\n", "            ", "self", ".", "train_file", "=", "Path", "(", "parsed", "[", "\"train_file\"", "]", ")", "\n", "", "elif", "\"train_folder\"", "in", "parsed", ":", "\n", "            ", "self", ".", "train_folder", "=", "Path", "(", "parsed", "[", "\"train_folder\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Must provide either a train_file or a train_folder in .toml file.\"", "\n", ")", "\n", "\n", "", "self", ".", "root_model_name", "=", "None", "\n", "if", "\"root_model_name\"", "in", "parsed", ":", "\n", "            ", "self", ".", "root_model_name", "=", "parsed", "[", "\"root_model_name\"", "]", "\n", "\n", "", "self", ".", "train_preprocessed_folder", "=", "self", ".", "get_preprocessed_folder", "(", "\"train\"", ")", "\n", "self", ".", "val_preprocessed_folder", "=", "self", ".", "get_preprocessed_folder", "(", "\"validate\"", ")", "\n", "self", ".", "test_preprocessed_folder", "=", "self", ".", "get_preprocessed_folder", "(", "\"test\"", ")", "\n", "\n", "# where will we store the checkpoints file?", "\n", "directory", ",", "_", "=", "os", ".", "path", ".", "split", "(", "toml_filepath", ")", "\n", "checkpoint_csv_filename", "=", "f\"{self.experiment_name}-checkpoints.csv\"", "\n", "self", ".", "checkpoint_csv", "=", "Path", "(", "directory", ")", "/", "checkpoint_csv_filename", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.HyperParameters.get_preprocessed_folder": [[70, 101], ["os.path.splitext", "os.makedirs", "pathlib.Path", "os.path.splitext", "os.makedirs", "pathlib.Path", "os.path.isfile", "structures.HyperParameters.train_folder.with_name", "ValueError", "os.path.isfile", "os.path.splitext", "os.makedirs", "pathlib.Path", "ValueError", "os.path.isfile"], "methods", ["None"], ["", "def", "get_preprocessed_folder", "(", "\n", "self", ",", "kind", ":", "Literal", "[", "\"train\"", ",", "\"validate\"", ",", "\"test\"", "]", "\n", ")", "->", "Path", ":", "\n", "        ", "suffix", "=", "\"-preprocessed\"", "\n", "if", "kind", "==", "\"train\"", ":", "\n", "            ", "if", "self", ".", "train_file", ":", "\n", "                ", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "train_file", ")", "\n", "name", "+=", "suffix", "\n", "assert", "not", "os", ".", "path", ".", "isfile", "(", "name", ")", "\n", "os", ".", "makedirs", "(", "name", ",", "exist_ok", "=", "True", ")", "\n", "return", "Path", "(", "name", ")", "\n", "", "elif", "self", ".", "train_folder", ":", "\n", "                ", "return", "self", ".", "train_folder", ".", "with_name", "(", "self", ".", "train_folder", ".", "name", "+", "suffix", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Must provide either a train_file or a train_folder in .toml file.\"", "\n", ")", "\n", "", "", "elif", "kind", "==", "\"validate\"", ":", "\n", "            ", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "val_file", ")", "\n", "name", "+=", "\"-preprocessed\"", "\n", "assert", "not", "os", ".", "path", ".", "isfile", "(", "name", ")", "\n", "os", ".", "makedirs", "(", "name", ",", "exist_ok", "=", "True", ")", "\n", "return", "Path", "(", "name", ")", "\n", "", "elif", "kind", "==", "\"test\"", ":", "\n", "            ", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "test_file", ")", "\n", "name", "+=", "\"-preprocessed\"", "\n", "assert", "not", "os", ".", "path", ".", "isfile", "(", "name", ")", "\n", "os", ".", "makedirs", "(", "name", ",", "exist_ok", "=", "True", ")", "\n", "return", "Path", "(", "name", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"{kind} is not one of 'train', 'validate' or 'test'.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.__add__": [[110, 116], ["structures.ConfusionMatrix"], "methods", ["None"], ["def", "__add__", "(", "self", ",", "other", ":", "\"ConfusionMatrix\"", ")", "->", "\"ConfusionMatrix\"", ":", "\n", "        ", "return", "ConfusionMatrix", "(", "\n", "self", ".", "TP", "+", "other", ".", "TP", ",", "\n", "self", ".", "TN", "+", "other", ".", "TN", ",", "\n", "self", ".", "FP", "+", "other", ".", "FP", ",", "\n", "self", ".", "FN", "+", "other", ".", "FN", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.accuracy": [[118, 121], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "accuracy", "(", "self", ")", "->", "float", ":", "\n", "        ", "return", "(", "self", ".", "TP", "+", "self", ".", "TN", ")", "/", "self", ".", "total", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.f1": [[122, 131], ["print"], "methods", ["None"], ["", "@", "property", "\n", "def", "f1", "(", "self", ")", "->", "float", ":", "\n", "        ", "denominator", "=", "self", ".", "precision", "+", "self", ".", "recall", "\n", "\n", "if", "denominator", "==", "0", ":", "\n", "            ", "print", "(", "\"Both precision and recall were 0; not going to divide by 0.\"", ")", "\n", "return", "0", "\n", "\n", "", "return", "(", "2", "*", "self", ".", "precision", "*", "self", ".", "recall", ")", "/", "(", "self", ".", "precision", "+", "self", ".", "recall", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.recall": [[132, 139], ["print"], "methods", ["None"], ["", "@", "property", "\n", "def", "recall", "(", "self", ")", "->", "float", ":", "\n", "        ", "if", "self", ".", "TP", "+", "self", ".", "FN", "==", "0", ":", "\n", "# means there are no negative examples (unlikely)", "\n", "            ", "print", "(", "\"No negative examples provided; not going to divide by 0.\"", ")", "\n", "return", "1", "\n", "", "return", "self", ".", "TP", "/", "(", "self", ".", "TP", "+", "self", ".", "FN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.precision": [[140, 146], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "precision", "(", "self", ")", "->", "float", ":", "\n", "        ", "if", "self", ".", "TP", "+", "self", ".", "FP", "==", "0", ":", "\n", "# means we didn't guess positive for any examples", "\n", "            ", "return", "0", "\n", "", "return", "self", ".", "TP", "/", "(", "self", ".", "TP", "+", "self", ".", "FP", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.structures.ConfusionMatrix.total": [[147, 150], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "total", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "TP", "+", "self", ".", "FP", "+", "self", ".", "TN", "+", "self", ".", "FN", "\n", "", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.grouper": [[11, 16], ["itertools.zip_longest", "iter"], "function", ["None"], ["def", "grouper", "(", "\n", "iterable", ":", "Iterable", "[", "T", "]", ",", "n", ":", "int", ",", "fillvalue", ":", "Optional", "[", "T", "]", "=", "None", "\n", ")", "->", "Iterator", "[", "List", "[", "T", "]", "]", ":", "\n", "    ", "args", "=", "[", "iter", "(", "iterable", ")", "]", "*", "n", "\n", "return", "itertools", ".", "zip_longest", "(", "*", "args", ",", "fillvalue", "=", "fillvalue", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.chunks": [[18, 33], ["range", "chunk.append", "next"], "function", ["None"], ["", "def", "chunks", "(", "elements", ":", "Iterator", "[", "T", "]", ",", "n", ":", "int", ")", "->", "Iterator", "[", "List", "[", "T", "]", "]", ":", "\n", "    ", "\"\"\"\n    Yield successive n-sized chunks from elements.\n    \"\"\"", "\n", "\n", "while", "True", ":", "\n", "        ", "chunk", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "chunk", ".", "append", "(", "next", "(", "elements", ")", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "yield", "chunk", "\n", "return", "\n", "\n", "", "", "yield", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device": [[35, 40], ["torch.cuda.is_available", "torch.device", "torch.device"], "function", ["None"], ["", "", "def", "get_device", "(", ")", "->", "torch", ".", "device", ":", "# type: ignore", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cuda\"", ")", "# type: ignore", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cpu\"", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.print_device": [[42, 47], ["torch.cuda.is_available", "print", "print", "torch.cuda.get_device_name"], "function", ["None"], ["", "", "def", "print_device", "(", ")", "->", "None", ":", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "print", "(", "\"We will use:\"", ",", "torch", ".", "cuda", ".", "get_device_name", "(", "0", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"We will use the CPU.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.is_interactive": [[49, 53], ["hasattr"], "function", ["None"], ["", "", "def", "is_interactive", "(", ")", "->", "bool", ":", "\n", "    ", "import", "__main__", "as", "main", "# type: ignore", "\n", "\n", "return", "not", "hasattr", "(", "main", ",", "\"__file__\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.is_py38": [[55, 61], ["None"], "function", ["None"], ["", "def", "is_py38", "(", ")", "->", "bool", ":", "\n", "    ", "import", "sys", "\n", "\n", "major", ",", "minor", ",", "micro", ",", "_", ",", "_", "=", "sys", ".", "version_info", "\n", "\n", "return", "major", "==", "3", "and", "minor", ">=", "8", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.import_tqdm": [[63, 68], ["util.is_interactive", "__import__", "__import__"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.is_interactive"], ["", "def", "import_tqdm", "(", ")", "->", "Any", ":", "\n", "    ", "if", "is_interactive", "(", ")", ":", "\n", "        ", "return", "__import__", "(", "\"tqdm.notebook\"", ")", ".", "tqdm", "\n", "", "else", ":", "\n", "        ", "return", "__import__", "(", "\"tqdm\"", ")", ".", "tqdm", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.fix_special_text": [[73, 80], ["text.replace().replace().replace().replace", "text.replace().replace().replace", "text.replace().replace", "text.replace"], "function", ["None"], ["def", "fix_special_text", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "\n", "    ", "return", "(", "\n", "text", ".", "replace", "(", "\"[MATH]\"", ",", "\"_MATH_\"", ")", "\n", ".", "replace", "(", "\"[REF]\"", ",", "\"_REF_\"", ")", "\n", ".", "replace", "(", "\"[EQUATION]\"", ",", "\"_MATHDISP_\"", ")", "\n", ".", "replace", "(", "\"[CITATION]\"", ",", "\"_CITE_\"", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get": [[83, 85], ["next", "iter"], "function", ["None"], ["", "def", "get", "(", "s", ":", "Set", "[", "T", "]", ")", "->", "T", ":", "\n", "    ", "return", "next", "(", "iter", "(", "s", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.baseline.baseline": [[15, 37], ["util.print_device", "tokenizers.get_tokenizer", "util.get_device", "print", "open", "csv.writer", "util.my_tqdm", "batch[].to", "csv.writer.writerows", "ids.tolist", "tokenizers.decode", "zip", "batch[].to.tolist"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.print_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode"], ["def", "baseline", "(", "\n", "config", ":", "HyperParameters", ",", "\n", "loader", ":", "\"DataLoader[Tuple[Tensor, Tensor, Tensor]]\"", ",", "\n", "filename", ":", "str", ",", "\n", ")", "->", "None", ":", "\n", "    ", "util", ".", "print_device", "(", ")", "\n", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "device", "=", "util", ".", "get_device", "(", ")", "\n", "\n", "print", "(", "\"\\n--- Baseline ---\"", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "csvfile", ":", "\n", "        ", "csvwriter", "=", "csv", ".", "writer", "(", "csvfile", ")", "\n", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "            ", "original_ids", "=", "[", "ids", ".", "tolist", "(", ")", "for", "ids", "in", "batch", "[", "0", "]", "]", "\n", "sentences", "=", "[", "tokenizers", ".", "decode", "(", "ids", ",", "tokenizer", ")", "for", "ids", "in", "original_ids", "]", "\n", "\n", "labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "\n", "csvwriter", ".", "writerows", "(", "zip", "(", "sentences", ",", "labels", ".", "tolist", "(", ")", ",", "[", "1", "for", "_", "in", "labels", "]", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.attention_merge.merge_attention_head": [[6, 70], ["numpy.zeros", "enumerate", "numpy.zeros", "enumerate", "print", "print", "print", "print", "enumerate", "print", "print", "print", "print", "enumerate", "print", "print", "print", "print", "len", "len", "len", "len", "len", "len", "word_ends.index", "word_ends.index"], "function", ["None"], ["def", "merge_attention_head", "(", "\n", "attention_head", ":", "np", ".", "ndarray", ",", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "word_ends", ":", "List", "[", "str", "]", ",", "\n", "verbose", ":", "int", "=", "0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "assert", "attention", ".", "shape", "==", "(", "len", "(", "tokens", ")", ",", "len", "(", "tokens", ")", ")", "\n", "\n", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "attention_head", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "attention_head", ")", "\n", "print", "(", ")", "\n", "\n", "# step 1: merge attention *to* split words", "\n", "\n", "", "merged_attention", "=", "np", ".", "zeros", "(", "(", "len", "(", "tokens", ")", ",", "len", "(", "words", ")", ")", ")", "\n", "\n", "for", "token_i", ",", "token_from", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "attention_sum", "=", "0", "\n", "word_j", "=", "-", "1", "\n", "for", "token_j", ",", "token_to", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "attention_sum", "+=", "attention_head", "[", "token_i", ",", "token_j", "]", "\n", "if", "token_to", "in", "word_ends", "[", "word_j", "+", "1", ":", "]", ":", "\n", "                ", "word_j", "=", "word_ends", ".", "index", "(", "token_to", ",", "word_j", "+", "1", ")", "\n", "merged_attention", "[", "token_i", ",", "word_j", "]", "=", "attention_sum", "\n", "attention_sum", "=", "0", "\n", "\n", "", "", "", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "merged_attention", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "merged_attention", ")", "\n", "print", "(", ")", "\n", "\n", "", "final_attention", "=", "np", ".", "zeros", "(", "(", "len", "(", "words", ")", ",", "len", "(", "words", ")", ")", ")", "\n", "\n", "# step 2: merge attention *from* split words", "\n", "\n", "for", "word_j", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "word_i", "=", "-", "1", "\n", "attention_to_word", "=", "0", "\n", "tokens_to_word_count", "=", "0", "\n", "for", "token_i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "attention_to_word", "+=", "merged_attention", "[", "token_i", ",", "word_j", "]", "\n", "tokens_to_word_count", "+=", "1", "\n", "\n", "if", "token", "in", "word_ends", "[", "word_i", "+", "1", ":", "]", ":", "\n", "                ", "word_i", "=", "word_ends", ".", "index", "(", "token", ",", "word_i", "+", "1", ")", "\n", "attention_from_word", "=", "attention_to_word", "/", "tokens_to_word_count", "\n", "final_attention", "[", "word_i", ",", "word_j", "]", "=", "attention_from_word", "\n", "attention_to_word", "=", "0", "\n", "tokens_to_word_count", "=", "0", "\n", "\n", "", "", "", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "final_attention", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "final_attention", ")", "\n", "print", "(", ")", "\n", "\n", "", "return", "final_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.disk.get_scores": [[26, 34], ["filename.endswith", "open", "json.load", "structures.Score.parse"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.Score.parse"], ["avg_val_loss", ":", "Optional", "[", "float", "]", "=", "None", "\n", "\n", "\n", "", "class", "DiskModel", "(", "TypedDict", ")", ":", "\n", "    ", "model_state_dict", ":", "Dict", "[", "Any", ",", "Any", "]", "\n", "optimizer_state_dict", ":", "Dict", "[", "Any", ",", "Any", "]", "\n", "\n", "\n", "", "@", "dataclass", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.model_name_to_model_and_dataset": [[14, 30], ["model_name.lower.lower", "ValueError"], "function", ["None"], ["def", "model_name_to_model_and_dataset", "(", "model_name", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "model_name", "=", "model_name", ".", "lower", "(", ")", "\n", "if", "model_name", "==", "\"bert_default\"", ":", "\n", "        ", "return", "\"BERT\"", ",", "\"n/a\"", "\n", "", "elif", "model_name", "==", "\"bert_glue\"", ":", "\n", "        ", "return", "\"BERT\"", ",", "\"GLUE\"", "\n", "", "elif", "model_name", "==", "\"bert\"", ":", "\n", "        ", "return", "\"BERT\"", ",", "\"AESW\"", "\n", "", "elif", "model_name", "==", "\"roberta\"", ":", "\n", "        ", "return", "\"RoBERTA\"", ",", "\"AESW\"", "\n", "", "elif", "model_name", "==", "\"scibert\"", ":", "\n", "        ", "return", "\"SciBERT\"", ",", "\"AESW\"", "\n", "", "elif", "model_name", "==", "\"random\"", ":", "\n", "        ", "return", "\"Random\"", ",", "\"\"", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"{model_name} is not a valid name\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.scores_to_table_row": [[32, 58], ["table.model_name_to_model_and_dataset", "model.lower"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.model_name_to_model_and_dataset"], ["", "", "def", "scores_to_table_row", "(", "\n", "spell_score_mean", ":", "Score", ",", "\n", "spell_score_max", ":", "Score", ",", "\n", "del_score_mean", ":", "Score", ",", "\n", "del_score_max", ":", "Score", ",", "\n", ")", "->", "str", ":", "\n", "    ", "assert", "(", "\n", "del_score_mean", ".", "model_name", "\n", "==", "del_score_max", ".", "model_name", "\n", "==", "spell_score_mean", ".", "model_name", "\n", "==", "spell_score_max", ".", "model_name", "\n", ")", "\n", "assert", "del_score_mean", ".", "strategy", "==", "spell_score_mean", ".", "strategy", "==", "\"sum\"", "\n", "assert", "del_score_max", ".", "strategy", "==", "spell_score_max", ".", "strategy", "==", "\"max\"", "\n", "\n", "OPEN", "=", "\"{\"", "\n", "CLOSE", "=", "\"}\"", "\n", "\n", "model", ",", "dataset", "=", "model_name_to_model_and_dataset", "(", "del_score_mean", ".", "model_name", ")", "\n", "\n", "strategies", "=", "(", "\"n/a\"", ",", "\"n/a\"", ")", "if", "model", ".", "lower", "(", ")", "==", "\"random\"", "else", "(", "\"mean\"", ",", "\"max\"", ")", "\n", "\n", "template_str", "=", "f\"\"\"\\\\multirow{OPEN}2{CLOSE}{OPEN}*{CLOSE}{OPEN}\\\\makecell[l]{OPEN}\\\\textbf{OPEN}{model}{CLOSE}\\\\\\\\{dataset}{CLOSE}{CLOSE} & {strategies[0]} & {spell_score_mean.exact_match_accuracy:.3f} & {spell_score_mean.jaccard_match_accuracy:.3f} & {spell_score_mean.avg_jaccard_similarity:.3f} & {spell_score_mean.top_3_match_accuracy:.3f} & {del_score_mean.exact_match_accuracy:.3f} & {del_score_mean.jaccard_match_accuracy:.3f} & {del_score_mean.avg_jaccard_similarity:.3f} & {del_score_mean.top_3_match_accuracy:.3f} \\\\\\\\\n& {strategies[1]} & {spell_score_max.exact_match_accuracy:.3f} & {spell_score_max.jaccard_match_accuracy:.3f} & {spell_score_max.avg_jaccard_similarity:.3f} & {spell_score_max.top_3_match_accuracy:.3f} & {del_score_max.exact_match_accuracy:.3f} & {del_score_max.jaccard_match_accuracy:.3f} & {del_score_max.avg_jaccard_similarity:.3f} & {del_score_max.top_3_match_accuracy:.3f} \\\\\\\\ \\\\hline\\n\"\"\"", "\n", "\n", "return", "template_str", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.get_relevant_scores": [[60, 88], ["next", "next", "next", "next"], "function", ["None"], ["", "def", "get_relevant_scores", "(", "\n", "model_name", ":", "str", ",", "spelling_scores", ":", "List", "[", "Score", "]", ",", "delete_scores", ":", "List", "[", "Score", "]", "\n", ")", "->", "Tuple", "[", "Score", ",", "Score", ",", "Score", ",", "Score", "]", ":", "\n", "    ", "spelling_mean", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "spelling_scores", "\n", "if", "score", ".", "model_name", "==", "model_name", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "spelling_max", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "spelling_scores", "\n", "if", "score", ".", "model_name", "==", "model_name", "and", "score", ".", "strategy", "==", "\"max\"", "\n", ")", "\n", "\n", "delete_mean", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "delete_scores", "\n", "if", "score", ".", "model_name", "==", "model_name", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "delete_max", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "delete_scores", "\n", "if", "score", ".", "model_name", "==", "model_name", "and", "score", ".", "strategy", "==", "\"max\"", "\n", ")", "\n", "\n", "return", "spelling_mean", ",", "spelling_max", ",", "delete_mean", ",", "delete_max", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.scores_to_table": [[90, 103], ["len", "len", "open", "file.write", "table.scores_to_table_row", "table.get_relevant_scores"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.scores_to_table_row", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.get_relevant_scores"], ["", "def", "scores_to_table", "(", "\n", "spelling_scores", ":", "List", "[", "Score", "]", ",", "\n", "delete_scores", ":", "List", "[", "Score", "]", ",", "\n", "models", ":", "List", "[", "str", "]", ",", "\n", "filename", ":", "Path", ",", "\n", ")", "->", "None", ":", "\n", "    ", "assert", "len", "(", "spelling_scores", ")", "==", "len", "(", "delete_scores", ")", "\n", "\n", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "for", "model", "in", "models", ":", "\n", "            ", "file", ".", "write", "(", "\n", "scores_to_table_row", "(", "\n", "*", "get_relevant_scores", "(", "model", ",", "spelling_scores", ",", "delete_scores", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.predict.get_best_choices": [[13, 34], ["indices.tolist", "best_choices.remove", "best_choices.remove", "attn.max.sum", "len", "attn.max.argsort", "max", "attn.max.max", "ValueError", "len"], "function", ["None"], ["def", "get_best_choices", "(", "attn", ":", "np", ".", "ndarray", ",", "strategy", ":", "Strategy", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "heads", ",", "words", "=", "attn", ".", "shape", "\n", "assert", "heads", "==", "12", "\n", "\n", "if", "strategy", "==", "\"sum\"", ":", "\n", "        ", "attn", "=", "attn", ".", "sum", "(", "axis", "=", "0", ")", "\n", "", "elif", "strategy", "==", "\"max\"", ":", "\n", "        ", "attn", "=", "attn", ".", "max", "(", "axis", "=", "0", ")", "# prevents picking the same word twice", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "strategy", ")", "\n", "\n", "", "assert", "len", "(", "attn", ")", "==", "words", ",", "f\"should be {words} different options; got {len(attn)}\"", "\n", "\n", "indices", "=", "attn", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "# gets indices from max attention to min attention", "\n", "best_choices", ":", "List", "[", "int", "]", "=", "indices", ".", "tolist", "(", ")", "\n", "\n", "# filter first and last word because it's [CLS] and [SEP]", "\n", "best_choices", ".", "remove", "(", "0", ")", "# [CLS]", "\n", "best_choices", ".", "remove", "(", "max", "(", "best_choices", ")", ")", "# [SEP]", "\n", "\n", "return", "best_choices", "\n", "", ""]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.Edit.to_dict": [[26, 32], ["list"], "methods", ["None"], ["def", "to_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "return", "{", "\n", "\"id\"", ":", "self", ".", "id", ",", "\n", "\"sent\"", ":", "self", ".", "sent", ",", "\n", "\"words\"", ":", "self", ".", "words", ",", "\n", "\"best_words\"", ":", "list", "(", "self", ".", "best_words", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted": [[35, 52], ["edits.is_only_deleted"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted"], ["", "", "def", "is_single_comma_deleted", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "bool", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "if", "not", "is_only_deleted", "(", "sent_elem", ")", ":", "\n", "        ", "return", "False", "\n", "\n", "", "commas_deleted", "=", "0", "\n", "is_something_else", "=", "False", "\n", "\n", "for", "edit_elem", "in", "sent_elem", ":", "\n", "        ", "if", "edit_elem", ".", "tag", "==", "\"del\"", ":", "\n", "            ", "if", "edit_elem", ".", "text", "==", "\",\"", ":", "\n", "                ", "commas_deleted", "+=", "1", "\n", "", "else", ":", "\n", "                ", "is_something_else", "=", "True", "\n", "\n", "", "", "", "return", "commas_deleted", "==", "1", "and", "not", "is_something_else", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted": [[54, 65], ["len"], "function", ["None"], ["", "def", "is_only_deleted", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "bool", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "if", "len", "(", "sent_elem", ")", "==", "0", ":", "\n", "        ", "return", "False", "\n", "\n", "", "for", "edit_elem", "in", "sent_elem", ":", "\n", "        ", "if", "edit_elem", ".", "tag", "!=", "\"del\"", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_spelling_error": [[67, 101], ["list", "spell.unknown", "util.get", "spell.unknown", "len", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get"], ["", "def", "is_only_spelling_error", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "bool", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "if", "len", "(", "sent_elem", ")", "!=", "2", ":", "# should be one delete, one insert", "\n", "        ", "return", "False", "\n", "\n", "", "del_elem", ",", "ins_elem", "=", "list", "(", "sent_elem", ")", "\n", "\n", "if", "(", "\n", "del_elem", ".", "tail", "\n", ")", ":", "# should be no text between the deleted text and the inserted text", "\n", "        ", "return", "False", "\n", "\n", "", "if", "del_elem", ".", "tag", "!=", "\"del\"", "or", "ins_elem", ".", "tag", "!=", "\"ins\"", ":", "\n", "        ", "return", "False", "\n", "\n", "", "misspelled_words", "=", "spell", ".", "unknown", "(", "[", "del_elem", ".", "text", "]", ")", "\n", "\n", "if", "not", "misspelled_words", ":", "\n", "        ", "return", "False", "\n", "\n", "", "misspelled_word", "=", "base_util", ".", "get", "(", "misspelled_words", ")", "\n", "\n", "if", "misspelled_word", "in", "string", ".", "whitespace", "or", "\" \"", "in", "misspelled_word", ":", "\n", "        ", "return", "False", "\n", "\n", "", "corrections", "=", "spell", ".", "unknown", "(", "[", "ins_elem", ".", "text", "]", ")", "\n", "\n", "if", "(", "\n", "len", "(", "corrections", ")", ">", "0", "\n", ")", ":", "# still misspelled, but not always correct. Might want to use edit distance", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices": [[103, 143], ["edits.extract_sentence_with_mask", "util.tokenize_transformer_sentences", "util.tokenize_transformer_sentences", "set", "util.tokenize_transformer_sentences.insert", "util.tokenize_transformer_sentences.append", "set", "any", "set", "len", "len", "set.add", "len", "len", "aesw_to_sentences.extract_sentence_id", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id"], ["", "def", "get_deleted_word_indices", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "Set", "[", "int", "]", "]", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "\n", "sent_text", ",", "mask", "=", "extract_sentence_with_mask", "(", "sent_elem", ")", "\n", "words", "=", "util", ".", "tokenize_transformer_sentences", "(", "sent_text", ",", "add_special_tokens", "=", "False", ")", "\n", "\n", "if", "not", "any", "(", "[", "elem", ".", "tag", "==", "\"del\"", "for", "elem", "in", "sent_elem", "]", ")", ":", "\n", "        ", "return", "words", ",", "set", "(", ")", "\n", "\n", "", "word_i", "=", "0", "\n", "word_ch_i", "=", "0", "\n", "sent_i", "=", "0", "\n", "\n", "best_words", "=", "set", "(", ")", "\n", "\n", "while", "word_i", "<", "len", "(", "words", ")", "and", "sent_i", "<", "len", "(", "sent_text", ")", ":", "\n", "        ", "if", "mask", "[", "sent_i", "]", "!=", "\" \"", ":", "\n", "            ", "best_words", ".", "add", "(", "word_i", ")", "\n", "", "word_ch_i", "+=", "1", "\n", "sent_i", "+=", "1", "\n", "\n", "if", "word_ch_i", ">=", "len", "(", "words", "[", "word_i", "]", ")", ":", "\n", "            ", "word_i", "+=", "1", "\n", "word_ch_i", "=", "0", "\n", "# skip whitespace in sent", "\n", "while", "sent_i", "<", "len", "(", "sent_text", ")", "and", "sent_text", "[", "sent_i", "]", "in", "string", ".", "whitespace", ":", "\n", "                ", "sent_i", "+=", "1", "\n", "\n", "", "", "if", "sent_i", ">=", "len", "(", "sent_text", ")", ":", "\n", "            ", "break", "\n", "\n", "", "assert", "(", "\n", "words", "[", "word_i", "]", "[", "word_ch_i", "]", "==", "sent_text", "[", "sent_i", "]", "\n", ")", ",", "f\"'{words[word_i][word_ch_i]}' != '{sent_text[sent_i]}' ({aesw_to_sentences.extract_sentence_id(sent_elem)})\"", "\n", "\n", "", "words", ".", "insert", "(", "0", ",", "\"[CLS]\"", ")", "\n", "words", ".", "append", "(", "\"[SEP]\"", ")", "\n", "best_words", "=", "set", "(", "[", "b", "+", "1", "for", "b", "in", "best_words", "]", ")", "# all incremented because of [CLS]", "\n", "\n", "return", "words", ",", "best_words", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask": [[145, 166], ["len", "len", "len", "len", "str", "string_builder.append", "mask_builder.append", "string_builder.append", "mask_builder.append", "len", "str", "str", "str", "len", "len"], "function", ["None"], ["", "def", "extract_sentence_with_mask", "(", "sent_elem", ":", "etree", ".", "_Element", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "assert", "sent_elem", ".", "tag", "==", "\"sentence\"", "\n", "string_builder", "=", "[", "str", "(", "sent_elem", ".", "text", ")", "if", "sent_elem", ".", "text", "else", "\"\"", "]", "\n", "mask_builder", "=", "[", "\" \"", "*", "len", "(", "str", "(", "sent_elem", ".", "text", ")", ")", "if", "sent_elem", ".", "text", "else", "\"\"", "]", "\n", "\n", "for", "del_ins", "in", "sent_elem", ":", "\n", "        ", "if", "del_ins", ".", "tag", "==", "\"del\"", "and", "del_ins", ".", "text", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "text", ")", ")", "\n", "mask_builder", ".", "append", "(", "\"*\"", "*", "len", "(", "string_builder", "[", "-", "1", "]", ")", ")", "\n", "", "if", "del_ins", ".", "tail", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "del_ins", ".", "tail", ")", ")", "\n", "mask_builder", ".", "append", "(", "\" \"", "*", "len", "(", "string_builder", "[", "-", "1", "]", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "mask_builder", ")", "==", "len", "(", "string_builder", ")", "\n", "\n", "sent", "=", "\"\"", ".", "join", "(", "string_builder", ")", "\n", "mask", "=", "\"\"", ".", "join", "(", "mask_builder", ")", "\n", "\n", "assert", "len", "(", "sent", ")", "==", "len", "(", "mask", ")", "\n", "\n", "return", "sent", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.save_edits": [[168, 186], ["pathlib.Path", "open", "pickle.dump", "open", "ValueError", "json_file.write", "json.dumps", "edit.to_dict"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.Edit.to_dict"], ["", "def", "save_edits", "(", "edits", ":", "List", "[", "Edit", "]", ",", "filename", ":", "str", ")", "->", "None", ":", "\n", "    ", "filepath", "=", "pathlib", ".", "Path", "(", "filename", ")", "\n", "\n", "if", "filepath", ".", "suffix", "==", "\".pckl\"", ":", "\n", "        ", "pickle_name", "=", "filename", "\n", "json_name", "=", "filepath", ".", "stem", "+", "\".json\"", "\n", "", "elif", "filepath", ".", "suffix", "==", "\".json\"", ":", "\n", "        ", "json_name", "=", "filename", "\n", "pickle_name", "=", "filepath", ".", "stem", "+", "\".pckl\"", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Filename '{filename}' must end in .json or .pckl\"", ")", "\n", "\n", "", "with", "open", "(", "disk", ".", "INTERPRETABILITY_FOLDER", "/", "pickle_name", ",", "\"wb\"", ")", "as", "pickle_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "edits", ",", "pickle_file", ")", "\n", "\n", "", "with", "open", "(", "disk", ".", "INTERPRETABILITY_FOLDER", "/", "json_name", ",", "\"w\"", ")", "as", "json_file", ":", "\n", "        ", "for", "edit", "in", "edits", ":", "\n", "            ", "json_file", ".", "write", "(", "json", ".", "dumps", "(", "edit", ".", "to_dict", "(", ")", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.load_edits": [[188, 191], ["open", "pickle.load"], "function", ["None"], ["", "", "", "def", "load_edits", "(", "filename", ":", "str", ")", "->", "List", "[", "Edit", "]", ":", "\n", "    ", "with", "open", "(", "disk", ".", "INTERPRETABILITY_FOLDER", "/", "filename", ",", "\"rb\"", ")", "as", "file", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "file", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_comma_edits": [[193, 221], ["util.my_tqdm", "edits.save_edits", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "aesw_to_sentences.extract_sentence", "edits.get_deleted_word_indices", "edits.Edit", "results.append", "edits.load_edits", "edits.is_single_comma_deleted"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.save_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.load_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted"], ["", "", "def", "get_comma_edits", "(", "again", ":", "bool", "=", "False", ")", "->", "List", "[", "Edit", "]", ":", "\n", "    ", "filename", "=", "\"comma.pckl\"", "\n", "try", ":", "\n", "        ", "if", "not", "again", ":", "\n", "            ", "return", "load_edits", "(", "filename", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "        ", "pass", "\n", "\n", "", "results", "=", "[", "]", "\n", "\n", "for", "_", ",", "sent_elem", "in", "base_util", ".", "my_tqdm", "(", "\n", "etree", ".", "iterparse", "(", "disk", ".", "DEV_XML", ",", "tag", "=", "\"sentence\"", ")", "\n", ")", ":", "\n", "        ", "sent_id", "=", "aesw_to_sentences", ".", "extract_sentence_id", "(", "sent_elem", ")", "\n", "sent", "=", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", "\n", "\n", "if", "not", "is_single_comma_deleted", "(", "sent_elem", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "sent_elem", ")", "\n", "\n", "edit", "=", "Edit", "(", "sent_id", ",", "sent", ",", "words", ",", "best_words", ")", "\n", "\n", "results", ".", "append", "(", "edit", ")", "\n", "\n", "", "save_edits", "(", "results", ",", "filename", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_deleted_words": [[223, 251], ["util.my_tqdm", "edits.save_edits", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "aesw_to_sentences.extract_sentence", "edits.get_deleted_word_indices", "edits.Edit", "results.append", "edits.load_edits", "edits.is_only_deleted"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.save_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.load_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted"], ["", "def", "get_edits_with_only_deleted_words", "(", "again", ":", "bool", "=", "False", ")", "->", "List", "[", "Edit", "]", ":", "\n", "    ", "filename", "=", "\"deleted.pckl\"", "\n", "try", ":", "\n", "        ", "if", "not", "again", ":", "\n", "            ", "return", "load_edits", "(", "filename", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "        ", "pass", "\n", "\n", "", "results", "=", "[", "]", "\n", "\n", "for", "_", ",", "sent_elem", "in", "base_util", ".", "my_tqdm", "(", "\n", "etree", ".", "iterparse", "(", "disk", ".", "DEV_XML", ",", "tag", "=", "\"sentence\"", ")", "\n", ")", ":", "\n", "        ", "sent_id", "=", "aesw_to_sentences", ".", "extract_sentence_id", "(", "sent_elem", ")", "\n", "sent", "=", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", "\n", "\n", "if", "not", "is_only_deleted", "(", "sent_elem", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "sent_elem", ")", "\n", "\n", "edit", "=", "Edit", "(", "sent_id", ",", "sent", ",", "words", ",", "best_words", ")", "\n", "\n", "results", ".", "append", "(", "edit", ")", "\n", "\n", "", "save_edits", "(", "results", ",", "filename", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_spelling_errors": [[253, 281], ["util.my_tqdm", "edits.save_edits", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "aesw_to_sentences.extract_sentence", "edits.get_deleted_word_indices", "edits.Edit", "results.append", "edits.load_edits", "edits.is_only_spelling_error"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.save_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.load_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_spelling_error"], ["", "def", "get_edits_with_only_spelling_errors", "(", "again", ":", "bool", "=", "False", ")", "->", "List", "[", "Edit", "]", ":", "\n", "    ", "filename", "=", "\"spelling.pckl\"", "\n", "try", ":", "\n", "        ", "if", "not", "again", ":", "\n", "            ", "return", "load_edits", "(", "filename", ")", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "        ", "pass", "\n", "\n", "", "results", "=", "[", "]", "\n", "\n", "for", "_", ",", "sent_elem", "in", "base_util", ".", "my_tqdm", "(", "\n", "etree", ".", "iterparse", "(", "disk", ".", "DEV_XML", ",", "tag", "=", "\"sentence\"", ")", "\n", ")", ":", "\n", "        ", "sent_id", "=", "aesw_to_sentences", ".", "extract_sentence_id", "(", "sent_elem", ")", "\n", "sent", "=", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", "\n", "\n", "if", "not", "is_only_spelling_error", "(", "sent_elem", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "sent_elem", ")", "\n", "\n", "edit", "=", "Edit", "(", "sent_id", ",", "sent", ",", "words", ",", "best_words", ")", "\n", "\n", "results", ".", "append", "(", "edit", ")", "\n", "\n", "", "save_edits", "(", "results", ",", "filename", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.test_comma_edits": [[286, 391], ["lxml.etree.fromstring", "edits.get_deleted_word_indices", "edits.is_single_comma_deleted", "lxml.etree.fromstring", "edits.get_deleted_word_indices", "util.get", "edits.is_single_comma_deleted", "lxml.etree.fromstring", "edits.get_deleted_word_indices", "util.get", "lxml.etree.fromstring", "edits.get_deleted_word_indices", "util.get", "lxml.etree.fromstring", "edits.get_deleted_word_indices", "util.get", "edits.is_single_comma_deleted", "lxml.etree.fromstring", "edits.extract_sentence_with_mask", "edits.get_deleted_word_indices", "lxml.etree.fromstring", "edits.extract_sentence_with_mask", "edits.get_deleted_word_indices", "util.get", "lxml.etree.fromstring", "edits.extract_sentence_with_mask", "edits.get_deleted_word_indices", "util.get", "lxml.etree.fromstring", "edits.extract_sentence_with_mask", "edits.get_deleted_word_indices", "util.get", "set", "edits.is_single_comma_deleted", "edits.is_single_comma_deleted"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.extract_sentence_with_mask", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_single_comma_deleted"], ["", "def", "test_comma_edits", "(", ")", "->", "None", ":", "\n", "    ", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"250.9\">This, which<del>,</del> is difficult, can be identified trivially</sentence>'", "\n", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "\n", "assert", "words", "==", "[", "\n", "\"[CLS]\"", ",", "\n", "\"This\"", ",", "\n", "\",\"", ",", "\n", "\"which\"", ",", "\n", "\",\"", ",", "\n", "\"is\"", ",", "\n", "\"difficult\"", ",", "\n", "\",\"", ",", "\n", "\"can\"", ",", "\n", "\"be\"", ",", "\n", "\"identified\"", ",", "\n", "\"trivially\"", ",", "\n", "\"[SEP]\"", ",", "\n", "]", ",", "words", "\n", "assert", "best_words", "==", "{", "4", "}", ",", "best_words", "\n", "\n", "assert", "is_single_comma_deleted", "(", "e", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"250.9\">Then<del>,</del> the SVM responses can be computed respectively for the new positive test samples (dots) and the negative ones (stars). </sentence>'", "\n", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "assert", "words", "[", "best_word", "]", "==", "\",\"", ",", "words", "[", "best_word", "]", "\n", "\n", "assert", "is_single_comma_deleted", "(", "e", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"250.9\">Then the SVM <del>responses</del> can be computed respectively for the new positive test samples (dots) and the negative ones (stars). </sentence>'", "\n", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "assert", "words", "[", "best_word", "]", "==", "\"responses\"", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"250.9\">Then the SVM can be computed respectively for <del>the</del> new positive test samples (dots) and the negative ones (stars). </sentence>'", "\n", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "assert", "words", "[", "best_word", "]", "==", "\"the\"", "\n", "assert", "best_word", "==", "9", ",", "best_word", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"136.1\">The same reasoning as above yields a probability measure _MATH_, in particular _MATH_ on _MATH_, and an optional measure _MATH_ such that _MATH_ _MATH_-a.s. for all _MATH_, i.e.<del>,</del> _MATH_ for _MATH_, and _REF_ holds.</sentence>'", "\n", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "assert", "words", "[", "best_word", "]", "==", "\",\"", "\n", "\n", "assert", "is_single_comma_deleted", "(", "e", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"2.0\">Neville elimination is an alternative procedure to that of Gauss to transform a square matrix _MATH_ into an upper<del> </del><ins>-</ins>triangular one.</sentence>'", "\n", ")", "\n", "\n", "sent", ",", "mask", "=", "extract_sentence_with_mask", "(", "e", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "\n", "assert", "best_words", "==", "set", "(", ")", "# best words is empty because a space was deleted", "\n", "assert", "not", "is_single_comma_deleted", "(", "e", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"195.0\">Now it is a routine matter to check that indeed _MATH_, where _MATH_, and where _MATH_ is the expansion of _MATH_ with all Skolem functions _MATH_ (including the \"Skolem constants\", i.e. witnesses). (i) follows immediately from (ii): <del>Let</del><ins>let</ins> _MATH_ (with _MATH_), then there is an _MATH_ such that _MATH_. </sentence>'", "\n", ")", "\n", "\n", "sent", ",", "mask", "=", "extract_sentence_with_mask", "(", "e", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"2178.3\">Among these parameters are the vertical gas velocity<del>,</del> v<del>,</del> and the line width w. </sentence>'", "\n", ")", "\n", "\n", "sent", ",", "mask", "=", "extract_sentence_with_mask", "(", "e", ")", "\n", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"32.2\">Elements at locations<del>,</del> _MATH_, with fluxes<del>,</del> _MATH_, will create a vertical field _MATHDISP_<del>,</del> in the photospheric plane. </sentence>'", "\n", ")", "\n", "\n", "sent", ",", "mask", "=", "extract_sentence_with_mask", "(", "e", ")", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "best_word", "=", "base_util", ".", "get", "(", "best_words", ")", "\n", "\n", "assert", "not", "is_single_comma_deleted", "(", "e", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.test_deleted_word_edits": [[393, 420], ["lxml.etree.fromstring", "edits.is_only_deleted", "edits.get_deleted_word_indices", "lxml.etree.fromstring", "edits.is_only_deleted", "edits.get_deleted_word_indices", "lxml.etree.fromstring", "edits.is_only_deleted", "edits.get_deleted_word_indices", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.is_only_deleted", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_deleted_word_indices"], ["", "def", "test_deleted_word_edits", "(", ")", "->", "None", ":", "\n", "    ", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"26755.3\">Thus as _MATH_, we get <del>that </del>_MATH_, which means that _MATH_. </sentence>'", "\n", ")", "\n", "\n", "assert", "is_only_deleted", "(", "e", ")", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "assert", "len", "(", "words", ")", "==", "16", "# 14 + 2 for [CLS] and [SEP]", "\n", "assert", "best_words", "==", "{", "7", "}", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"5.1\">To do this, we consider a full-order Luenberger state observer _CITE_, _CITE_ given by<del>:</del> _MATHDISP_ where _MATH_ is the estimate of the state and _MATH_ is the observer gain and defines the estimation error dynamics. </sentence>'", "\n", ")", "\n", "\n", "assert", "is_only_deleted", "(", "e", ")", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "assert", "len", "(", "words", ")", "==", "41", ",", "words", "\n", "assert", "best_words", "==", "{", "17", "}", ",", "f\"{words} {best_words}\"", "\n", "\n", "e", "=", "etree", ".", "fromstring", "(", "\n", "'<sentence sid=\"32.2\">Elements at locations<del>,</del> _MATH_, with fluxes<del>,</del> _MATH_, will create a vertical field _MATHDISP_<del>,</del> in the photospheric plane. </sentence>'", "\n", ")", "\n", "\n", "assert", "is_only_deleted", "(", "e", ")", "\n", "words", ",", "best_words", "=", "get_deleted_word_indices", "(", "e", ")", "\n", "assert", "len", "(", "words", ")", "==", "25", ",", "f\"{words} {len(words)}\"", "\n", "assert", "best_words", "==", "{", "4", ",", "9", ",", "18", "}", ",", "f\"{words} {best_words}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_cls_attention": [[14, 43], ["list", "plt.gca.imshow", "plt.gca.set_xticks", "plt.gca.set_xticklabels", "plt.gca.set_xlabel", "plt.gca.spines.items", "plt.gca.set_title", "range", "matplotlib.pyplot.gca", "numpy.arange", "plt.gca.set_yticks", "plt.gca.set_yticklabels", "plt.gca.set_yticks", "plt.gca.set_yticklabels", "spine.set_visible", "matplotlib.colors.Normalize", "len", "numpy.arange", "len"], "function", ["None"], ["def", "plot_cls_attention", "(", "\n", "tokens", ":", "List", "[", "str", "]", ",", "attn", ":", "np", ".", "ndarray", ",", "title", ":", "str", ",", "word_ticks", ":", "bool", ",", "ax", ":", "Any", "=", "None", "\n", ")", "->", "Any", ":", "\n", "# https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html", "\n", "    ", "heads", "=", "list", "(", "range", "(", "12", ")", ")", "\n", "\n", "if", "not", "ax", ":", "\n", "        ", "ax", "=", "plt", ".", "gca", "(", ")", "\n", "\n", "", "im", "=", "ax", ".", "imshow", "(", "\n", "attn", ".", "T", ",", "cmap", "=", "\"Reds\"", ",", "norm", "=", "matplotlib", ".", "colors", ".", "Normalize", "(", "vmin", "=", "0.0", ",", "vmax", "=", "1.0", ")", "\n", ")", "\n", "\n", "ax", ".", "set_xticks", "(", "np", ".", "arange", "(", "len", "(", "heads", ")", ")", ")", "\n", "ax", ".", "set_xticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Attention Heads\"", ")", "\n", "\n", "if", "word_ticks", ":", "\n", "        ", "ax", ".", "set_yticks", "(", "np", ".", "arange", "(", "len", "(", "tokens", ")", ")", ")", "\n", "ax", ".", "set_yticklabels", "(", "tokens", ",", "size", "=", "12", ")", "\n", "", "else", ":", "\n", "        ", "ax", ".", "set_yticks", "(", "[", "]", ")", "\n", "ax", ".", "set_yticklabels", "(", "[", "]", ")", "\n", "\n", "", "for", "edge", ",", "spine", "in", "ax", ".", "spines", ".", "items", "(", ")", ":", "\n", "        ", "spine", ".", "set_visible", "(", "False", ")", "\n", "\n", "", "ax", ".", "set_title", "(", "title", ")", "\n", "return", "im", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention": [[45, 62], ["compare.plot_cls_attention", "attention.get_words_and_attention_and_prediction", "run.cls_attn"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_cls_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_words_and_attention_and_prediction", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.cls_attn"], ["", "def", "plot_word_level_attention", "(", "\n", "sent", ":", "str", ",", "\n", "cls_model", ":", "models", ".", "SentenceClassificationModel", ",", "\n", "model", ":", "PreTrainedModel", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", "title", ":", "str", ",", "\n", "word_ticks", ":", "bool", ",", "\n", "**", "plotkwargs", ":", "Any", ",", "\n", ")", "->", "Any", ":", "\n", "    ", "output", "=", "attention", ".", "get_words_and_attention_and_prediction", "(", "\n", "[", "sent", "]", ",", "cls_model", ",", "model", ",", "tokenizer", "\n", ")", "[", "0", "]", "\n", "\n", "title", "+=", "f\" ({'needs edit' if output.prediction else 'no edit'})\"", "\n", "\n", "return", "plot_cls_attention", "(", "\n", "output", ".", "words", ",", "run", ".", "cls_attn", "(", "output", ".", "attention", ")", ",", "title", ",", "word_ticks", ",", "**", "plotkwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_finetuned_models_comparison": [[65, 105], ["matplotlib.pyplot.subplots", "run.load_bert_default", "compare.plot_word_level_attention", "run.load_bert", "compare.plot_word_level_attention", "fig.colorbar", "fig.colorbar.ax.set_xlabel", "len", "axes.ravel().tolist", "util.tokenize_transformer_sentences", "axes.ravel"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert_default", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences"], ["", "def", "plot_finetuned_models_comparison", "(", "sentence", ":", "str", ")", "->", "None", ":", "\n", "    ", "height", "=", "2", "+", "0.2", "*", "len", "(", "util", ".", "tokenize_transformer_sentences", "(", "sentence", ")", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "1", ",", "2", ",", "figsize", "=", "(", "12", ",", "height", ")", ",", "constrained_layout", "=", "True", ")", "\n", "\n", "(", "(", "ax1", ",", "ax2", ")", ")", "=", "axes", "\n", "\n", "bert_default_cls", ",", "bert_default", ",", "bert_default_tokenizer", "=", "run", ".", "load_bert_default", "(", ")", "\n", "\n", "im", "=", "plot_word_level_attention", "(", "\n", "sentence", ",", "\n", "bert_default_cls", ",", "\n", "bert_default", ",", "\n", "bert_default_tokenizer", ",", "\n", "title", "=", "\"Before\"", ",", "\n", "word_ticks", "=", "True", ",", "\n", "ax", "=", "ax1", ",", "\n", ")", "\n", "\n", "bert_cls", ",", "bert", ",", "bert_tokenizer", "=", "run", ".", "load_bert", "(", ")", "\n", "\n", "im", "=", "plot_word_level_attention", "(", "\n", "sentence", ",", "\n", "bert_cls", ",", "\n", "bert", ",", "\n", "bert_tokenizer", ",", "\n", "word_ticks", "=", "False", ",", "\n", "title", "=", "\"After\"", ",", "\n", "ax", "=", "ax2", ",", "\n", ")", "\n", "\n", "# fig.tight_layout()", "\n", "\n", "cbar", "=", "fig", ".", "colorbar", "(", "\n", "im", ",", "\n", "ax", "=", "axes", ".", "ravel", "(", ")", ".", "tolist", "(", ")", ",", "\n", "orientation", "=", "\"horizontal\"", ",", "\n", "aspect", "=", "30", ",", "\n", "use_gridspec", "=", "True", ",", "\n", ")", "\n", "cbar", ".", "ax", ".", "set_xlabel", "(", "\"Attention\"", ",", "va", "=", "\"center\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_all_word_models": [[107, 153], ["matplotlib.pyplot.subplots", "run.load_bert", "compare.plot_word_level_attention", "run.load_scibert", "compare.plot_word_level_attention", "run.load_roberta", "compare.plot_word_level_attention", "fig.colorbar", "fig.colorbar.ax.set_xlabel", "len", "axes.ravel().tolist", "util.tokenize_transformer_sentences", "axes.ravel"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_scibert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_roberta", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.compare.plot_word_level_attention", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences"], ["", "def", "plot_all_word_models", "(", "sentence", ":", "str", ")", "->", "None", ":", "\n", "    ", "height", "=", "2", "+", "0.2", "*", "len", "(", "util", ".", "tokenize_transformer_sentences", "(", "sentence", ")", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "1", ",", "3", ",", "figsize", "=", "(", "12", ",", "height", ")", ",", "constrained_layout", "=", "True", ")", "\n", "\n", "(", "(", "ax1", ",", "ax2", ",", "ax3", ")", ")", "=", "axes", "\n", "\n", "bert_cls", ",", "bert", ",", "bert_tokenizer", "=", "run", ".", "load_bert", "(", ")", "\n", "\n", "im", "=", "plot_word_level_attention", "(", "\n", "sentence", ",", "bert_cls", ",", "bert", ",", "bert_tokenizer", ",", "title", "=", "\"BERT\"", ",", "word_ticks", "=", "True", ",", "ax", "=", "ax1", "\n", ")", "\n", "\n", "scibert_cls", ",", "scibert", ",", "scibert_tokenizer", "=", "run", ".", "load_scibert", "(", ")", "\n", "\n", "im", "=", "plot_word_level_attention", "(", "\n", "sentence", ",", "\n", "scibert_cls", ",", "\n", "scibert", ",", "\n", "scibert_tokenizer", ",", "\n", "title", "=", "\"SciBERT\"", ",", "\n", "word_ticks", "=", "False", ",", "\n", "ax", "=", "ax2", ",", "\n", ")", "\n", "\n", "roberta_cls", ",", "roberta", ",", "roberta_tokenizer", "=", "run", ".", "load_roberta", "(", ")", "\n", "\n", "im", "=", "plot_word_level_attention", "(", "\n", "sentence", ",", "\n", "roberta_cls", ",", "\n", "roberta", ",", "\n", "roberta_tokenizer", ",", "\n", "title", "=", "\"RoBERTa\"", ",", "\n", "word_ticks", "=", "False", ",", "\n", "ax", "=", "ax3", ",", "\n", ")", "\n", "\n", "# fig.tight_layout()", "\n", "\n", "cbar", "=", "fig", ".", "colorbar", "(", "\n", "im", ",", "\n", "ax", "=", "axes", ".", "ravel", "(", ")", ".", "tolist", "(", ")", ",", "\n", "orientation", "=", "\"horizontal\"", ",", "\n", "aspect", "=", "30", ",", "\n", "use_gridspec", "=", "True", ",", "\n", ")", "\n", "cbar", ".", "ax", ".", "set_xlabel", "(", "\"Attention\"", ",", "va", "=", "\"center\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.format_special_chars": [[17, 21], ["t.replace().replace().replace().replace", "t.replace().replace().replace", "t.replace().replace", "t.replace"], "function", ["None"], ["def", "format_special_chars", "(", "tokens", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "return", "[", "\n", "t", ".", "replace", "(", "\"\u0120\"", ",", "\" \"", ")", ".", "replace", "(", "\"\u2581\"", ",", "\" \"", ")", ".", "replace", "(", "\"</w>\"", ",", "\"\"", ")", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "for", "t", "in", "tokens", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.merge_attention_head": [[24, 88], ["str", "numpy.zeros", "enumerate", "numpy.zeros", "enumerate", "print", "print", "print", "print", "enumerate", "print", "print", "print", "print", "enumerate", "print", "print", "print", "print", "len", "len", "len", "len", "len", "len", "word_ends.index", "word_ends.index"], "function", ["None"], ["", "def", "merge_attention_head", "(", "\n", "attention_head", ":", "np", ".", "ndarray", ",", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "word_ends", ":", "List", "[", "str", "]", ",", "\n", "verbose", ":", "int", "=", "0", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "assert", "attention_head", ".", "shape", "==", "(", "len", "(", "tokens", ")", ",", "len", "(", "tokens", ")", ")", ",", "str", "(", "attention_head", ".", "shape", ")", "\n", "\n", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "attention_head", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "attention_head", ")", "\n", "print", "(", ")", "\n", "\n", "# step 1: merge attention *to* split words", "\n", "\n", "", "merged_attention", "=", "np", ".", "zeros", "(", "(", "len", "(", "tokens", ")", ",", "len", "(", "words", ")", ")", ")", "\n", "\n", "for", "token_i", ",", "token_from", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "attention_sum", "=", "0", "\n", "word_j", "=", "-", "1", "\n", "for", "token_j", ",", "token_to", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "attention_sum", "+=", "attention_head", "[", "token_i", ",", "token_j", "]", "\n", "if", "token_to", "in", "word_ends", "[", "word_j", "+", "1", ":", "]", ":", "\n", "                ", "word_j", "=", "word_ends", ".", "index", "(", "token_to", ",", "word_j", "+", "1", ")", "\n", "merged_attention", "[", "token_i", ",", "word_j", "]", "=", "attention_sum", "\n", "attention_sum", "=", "0", "\n", "\n", "", "", "", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "merged_attention", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "merged_attention", ")", "\n", "print", "(", ")", "\n", "\n", "", "final_attention", "=", "np", ".", "zeros", "(", "(", "len", "(", "words", ")", ",", "len", "(", "words", ")", ")", ")", "\n", "\n", "# step 2: merge attention *from* split words", "\n", "\n", "for", "word_j", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "word_i", "=", "-", "1", "\n", "attention_to_word", "=", "0", "\n", "tokens_to_word_count", "=", "0", "\n", "for", "token_i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "attention_to_word", "+=", "merged_attention", "[", "token_i", ",", "word_j", "]", "\n", "tokens_to_word_count", "+=", "1", "\n", "\n", "if", "token", "in", "word_ends", "[", "word_i", "+", "1", ":", "]", ":", "\n", "                ", "word_i", "=", "word_ends", ".", "index", "(", "token", ",", "word_i", "+", "1", ")", "\n", "attention_from_word", "=", "attention_to_word", "/", "tokens_to_word_count", "\n", "final_attention", "[", "word_i", ",", "word_j", "]", "=", "attention_from_word", "\n", "attention_to_word", "=", "0", "\n", "tokens_to_word_count", "=", "0", "\n", "\n", "", "", "", "if", "verbose", "==", "1", ":", "\n", "        ", "print", "(", "final_attention", ".", "shape", ")", "\n", "print", "(", ")", "\n", "", "if", "verbose", "==", "2", ":", "\n", "        ", "print", "(", "final_attention", ")", "\n", "print", "(", ")", "\n", "\n", "", "return", "final_attention", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_word_ends_from_tokens": [[90, 106], ["words[].lower().endswith", "len", "len", "tokens[].lower().strip", "word_ends.append", "words[].lower", "tokens[].lower"], "function", ["None"], ["", "def", "get_word_ends_from_tokens", "(", "tokens", ":", "List", "[", "str", "]", ",", "words", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "assert", "tokens", "[", "0", "]", "==", "words", "[", "0", "]", ",", "f\"{tokens[0]} != {words[0]}\"", "\n", "assert", "tokens", "[", "-", "1", "]", "==", "words", "[", "-", "1", "]", "\n", "\n", "word_ends", "=", "[", "]", "\n", "\n", "word_i", "=", "0", "\n", "token_i", "=", "0", "\n", "while", "word_i", "<", "len", "(", "words", ")", "and", "token_i", "<", "len", "(", "tokens", ")", ":", "\n", "        ", "if", "words", "[", "word_i", "]", ".", "lower", "(", ")", ".", "endswith", "(", "tokens", "[", "token_i", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", ")", ":", "\n", "            ", "word_ends", ".", "append", "(", "tokens", "[", "token_i", "]", ")", "\n", "word_i", "+=", "1", "\n", "\n", "", "token_i", "+=", "1", "\n", "\n", "", "return", "word_ends", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_words_and_attention_and_prediction": [[114, 177], ["tokenizer.batch_encode_plus", "util.get_device", "inputs[].to", "inputs[].to", "range", "util.tokenize_transformer_sentences", "util.tokenize_transformer_sentences", "torch.no_grad", "cls_model", "torch.max", "raw_predictions.tolist", "len", "input_ids[].tolist", "tokenizer.convert_ids_to_tokens", "format_special_chars.index", "attention.format_special_chars", "attention.get_word_ends_from_tokens", "numpy.stack", "results.append", "model", "attention.merge_attention_head", "word_attention.append", "attention.ModelOutput", "attention_head.numpy", "bool"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.get_device", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.format_special_chars", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_word_ends_from_tokens", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.merge_attention_head"], ["", "def", "get_words_and_attention_and_prediction", "(", "\n", "sents", ":", "List", "[", "str", "]", ",", "\n", "cls_model", ":", "models", ".", "SentenceClassificationModel", ",", "\n", "model", ":", "PreTrainedModel", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", ")", "->", "List", "[", "ModelOutput", "]", ":", "\n", "    ", "if", "not", "sents", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "words", "=", "[", "util", ".", "tokenize_transformer_sentences", "(", "sent", ")", "for", "sent", "in", "sents", "]", "\n", "\n", "inputs", "=", "tokenizer", ".", "batch_encode_plus", "(", "\n", "sents", ",", "return_tensors", "=", "\"pt\"", ",", "add_special_tokens", "=", "True", ",", "padding", "=", "True", "\n", ")", "\n", "\n", "device", "=", "base_util", ".", "get_device", "(", ")", "\n", "\n", "input_ids", "=", "inputs", "[", "\"input_ids\"", "]", ".", "to", "(", "device", ")", "\n", "attn_mask", "=", "inputs", "[", "\"attention_mask\"", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "attention", "=", "model", "(", "input_ids", ",", "output_attentions", "=", "True", ")", "[", "-", "1", "]", "\n", "\n", "logits", "=", "cls_model", "(", "input_ids", ",", "attn_mask", "=", "attn_mask", ")", "\n", "_", ",", "raw_predictions", "=", "torch", ".", "max", "(", "logits", ".", "data", ",", "1", ")", "# type: ignore", "\n", "\n", "predictions", ":", "List", "[", "int", "]", "=", "raw_predictions", ".", "tolist", "(", ")", "\n", "\n", "# attention is a tuple of length 12", "\n", "# each element in attention is a tensor len(sents) x 12 x max_seq_len x max_seq_len", "\n", "\n", "", "results", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", ")", ")", ":", "\n", "# get tokens", "\n", "        ", "input_id_list", "=", "input_ids", "[", "i", "]", ".", "tolist", "(", ")", "\n", "tokens", ":", "List", "[", "str", "]", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_id_list", ")", "\n", "\n", "tokens", "[", "0", "]", "=", "\"[CLS]\"", "\n", "\n", "# last token isn't always [SEP] so we normalize", "\n", "last_token", "=", "tokens", ".", "index", "(", "tokenizer", ".", "sep_token", ")", "\n", "tokens", "[", "last_token", "]", "=", "\"[SEP]\"", "\n", "\n", "# only care about non-padding tokens", "\n", "tokens", "=", "tokens", "[", ":", "last_token", "+", "1", "]", "\n", "tokens", "=", "format_special_chars", "(", "tokens", ")", "\n", "\n", "word_ends", "=", "get_word_ends_from_tokens", "(", "tokens", ",", "words", "[", "i", "]", ")", "\n", "\n", "# get attention and remove the values that should be masked", "\n", "word_attention", "=", "[", "]", "\n", "\n", "for", "attention_head", "in", "attention", "[", "11", "]", "[", "i", "]", ":", "\n", "            ", "relevant_attn", "=", "attention_head", ".", "numpy", "(", ")", "[", ":", "last_token", "+", "1", ",", ":", "last_token", "+", "1", "]", "\n", "word_attn", "=", "merge_attention_head", "(", "relevant_attn", ",", "tokens", ",", "words", "[", "i", "]", ",", "word_ends", ")", "\n", "word_attention", ".", "append", "(", "word_attn", ")", "\n", "\n", "", "word_attention_arr", ":", "np", ".", "ndarray", "=", "np", ".", "stack", "(", "word_attention", ",", "axis", "=", "0", ")", "\n", "\n", "results", ".", "append", "(", "ModelOutput", "(", "words", "[", "i", "]", ",", "word_attention_arr", ",", "bool", "(", "predictions", "[", "i", "]", ")", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.get_relevant_scores": [[24, 70], ["next", "next", "next", "next", "next", "next"], "function", ["None"], ["def", "get_relevant_scores", "(", "\n", "scores", ":", "List", "[", "Score", "]", ",", "\n", ")", "->", "Tuple", "[", "Score", ",", "Score", ",", "Score", ",", "Score", ",", "Score", ",", "Score", "]", ":", "\n", "    ", "\"\"\"\n    returns a list of scores for\n    * Random\n    * BERT_default-sum\n    * BERT_GLUE-sum\n    * BERT-sum\n    * SciBERT-sum\n    * RoBERTA-sum\n    \"\"\"", "\n", "\n", "random", "=", "next", "(", "score", "for", "score", "in", "scores", "if", "score", ".", "model_name", "==", "\"random\"", ")", "\n", "\n", "bert_default_sum", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "scores", "\n", "if", "score", ".", "model_name", "==", "\"bert_default\"", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "bert_glue_sum", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "scores", "\n", "if", "score", ".", "model_name", "==", "\"bert_glue\"", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "bert_sum", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "scores", "\n", "if", "score", ".", "model_name", "==", "\"bert\"", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "scibert_sum", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "scores", "\n", "if", "score", ".", "model_name", "==", "\"scibert\"", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "roberta_sum", "=", "next", "(", "\n", "score", "\n", "for", "score", "in", "scores", "\n", "if", "score", ".", "model_name", "==", "\"roberta\"", "and", "score", ".", "strategy", "==", "\"sum\"", "\n", ")", "\n", "\n", "return", "(", "random", ",", "bert_default_sum", ",", "bert_glue_sum", ",", "bert_sum", ",", "scibert_sum", ",", "roberta_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.format_x_label": [[72, 77], ["None"], "function", ["None"], ["", "def", "format_x_label", "(", "name", ":", "str", ",", "data", ":", "str", ")", "->", "str", ":", "\n", "    ", "if", "data", ":", "\n", "        ", "return", "f\"{name}\\n({data})\"", "\n", "", "else", ":", "\n", "        ", "return", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.plot_n_bars": [[79, 99], ["numpy.arange", "matplotlib.subplots", "enumerate", "ax.set_ylabel", "ax.set_xticks", "ax.set_xticklabels", "ax.legend", "fig.tight_layout", "len", "len", "zip", "rects.append", "ax.bar", "len"], "function", ["None"], ["", "", "def", "plot_n_bars", "(", "\n", "x", ":", "Collection", "[", "str", "]", ",", "y_points", ":", "Collection", "[", "Iterable", "[", "float", "]", "]", ",", "y_labels", ":", "Collection", "[", "str", "]", "\n", ")", "->", "None", ":", "\n", "    ", "width", "=", "0.7", "/", "len", "(", "y_points", ")", "# the width of the bars", "\n", "\n", "x_pos", "=", "np", ".", "arange", "(", "len", "(", "x", ")", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "\n", "colors", "=", "[", "SEA", ",", "ORANGE", ",", "BLUE", "]", "\n", "rects", "=", "[", "]", "\n", "for", "i", ",", "(", "y", ",", "c", ")", "in", "enumerate", "(", "zip", "(", "y_points", ",", "colors", ")", ")", ":", "\n", "        ", "rects", ".", "append", "(", "ax", ".", "bar", "(", "x_pos", "+", "i", "*", "width", ",", "y", ",", "width", ",", "color", "=", "c", ")", "[", "0", "]", ")", "\n", "\n", "", "ax", ".", "set_ylabel", "(", "\"Top 3 Match Accuracy\"", ")", "\n", "ax", ".", "set_xticks", "(", "x_pos", "+", "(", "len", "(", "y_points", ")", "/", "2", "-", "0.5", ")", "*", "width", ")", "\n", "ax", ".", "set_xticklabels", "(", "x", ")", "\n", "ax", ".", "legend", "(", "rects", ",", "y_labels", ")", "\n", "\n", "fig", ".", "tight_layout", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.main": [[101, 131], ["plot.get_relevant_scores", "plot.get_relevant_scores", "zip", "plot.plot_n_bars", "disk.get_scores", "disk.get_scores", "matplotlib.show", "matplotlib.savefig", "matplotlib.savefig", "plot.format_x_label", "zip", "table.model_name_to_model_and_dataset"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.get_relevant_scores", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.get_relevant_scores", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.plot_n_bars", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.disk.get_scores", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.disk.get_scores", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.plot.format_x_label", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.table.model_name_to_model_and_dataset"], ["", "def", "main", "(", "interactive", ":", "bool", ")", "->", "None", ":", "\n", "    ", "spell_scores", "=", "get_relevant_scores", "(", "disk", ".", "get_scores", "(", "disk", ".", "SPELLING_SCORES_FILE", ")", ")", "\n", "\n", "# comma_scores = disk.get_scores(disk.COMMA_SCORES_FILE)", "\n", "# comma_scores = get_relevant_scores(comma_scores)", "\n", "\n", "del_scores", "=", "get_relevant_scores", "(", "disk", ".", "get_scores", "(", "disk", ".", "DELETE_SCORES_FILE", ")", ")", "\n", "\n", "points", "=", "[", "\n", "(", "\n", "format_x_label", "(", "\n", "*", "table", ".", "model_name_to_model_and_dataset", "(", "spell_score", ".", "model_name", ")", "\n", ")", ",", "\n", "spell_score", ".", "top_3_match_accuracy", ",", "\n", "del_score", ".", "top_3_match_accuracy", ",", "\n", ")", "\n", "for", "spell_score", ",", "del_score", "in", "zip", "(", "spell_scores", ",", "del_scores", ")", "\n", "]", "\n", "\n", "xlabels", ",", "y_spelling", ",", "y_delete", "=", "zip", "(", "*", "points", ")", "\n", "\n", "plot_n_bars", "(", "\n", "xlabels", ",", "(", "y_spelling", ",", "y_delete", ")", ",", "(", "\"spelling error\"", ",", "\"deleted words\"", ")", ",", "\n", ")", "\n", "\n", "if", "interactive", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "savefig", "(", "\"./docs/images/interpretability/bar.pdf\"", ")", "\n", "plt", ".", "savefig", "(", "\"./docs/images/interpretability/bar.png\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.__main__.clean_types": [[13, 20], ["typing.cast", "print", "sys.exit"], "function", ["None"], []], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n": [[158, 166], ["set", "len"], "methods", ["None"], ["def", "top_n", "(", "self", ",", "n", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "Set", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        if n is not provided, use len(self.target)\n        \"\"\"", "\n", "if", "n", "is", "None", ":", "\n", "            ", "n", "=", "len", "(", "self", ".", "target", ")", "\n", "\n", "", "return", "set", "(", "self", ".", "predictions", "[", ":", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity": [[167, 175], ["len", "len", "len", "run.Prediction.top_n", "run.Prediction.top_n"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "jaccard_similarity", "(", "self", ",", "n", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        if n is not provided, use len(self.target)\n        \"\"\"", "\n", "if", "n", "is", "None", ":", "\n", "            ", "n", "=", "len", "(", "self", ".", "target", ")", "\n", "\n", "", "return", "len", "(", "self", ".", "target", "&", "self", ".", "top_n", "(", "n", ")", ")", "/", "len", "(", "self", ".", "target", "|", "self", ".", "top_n", "(", "n", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert_default": [[32, 43], ["transformers.BertForSequenceClassification.from_pretrained", "torch.cuda.is_available", "models.SentenceClassificationModel", "transformers.BertTokenizerFast.from_pretrained", "BertForSequenceClassification.from_pretrained.cuda"], "function", ["None"], ["def", "load_bert_default", "(", ")", "->", "Model", ":", "\n", "    ", "default_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "default_model", ".", "cuda", "(", ")", "\n", "\n", "", "cls_model", "=", "models", ".", "SentenceClassificationModel", "(", "default_model", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizerFast", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "\n", "return", "cls_model", ",", "default_model", ".", "bert", ",", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert_glue": [[45, 58], ["transformers.BertForSequenceClassification.from_pretrained", "torch.cuda.is_available", "models.SentenceClassificationModel", "transformers.BertTokenizerFast.from_pretrained", "BertForSequenceClassification.from_pretrained.cuda"], "function", ["None"], ["", "def", "load_bert_glue", "(", ")", "->", "Model", ":", "\n", "    ", "glue_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\n", "\"./models-versioned/bert_glue\"", "\n", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "glue_model", ".", "cuda", "(", ")", "\n", "\n", "", "cls_model", "=", "models", ".", "SentenceClassificationModel", "(", "glue_model", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizerFast", ".", "from_pretrained", "(", "\"./models-versioned/bert_glue\"", ")", "\n", "\n", "return", "cls_model", ",", "glue_model", ".", "bert", ",", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert": [[60, 73], ["structures.HyperParameters", "disk.load_finetuned_model_from_disk", "isinstance", "tokenizers.get_tokenizer", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer"], ["", "def", "load_bert", "(", ")", "->", "Model", ":", "\n", "    ", "bert_config", "=", "structures", ".", "HyperParameters", "(", "\n", "\"./experiments/bert_base_aesw_32_1e6/params.toml\"", "\n", ")", "\n", "disk_model", "=", "base_disk", ".", "load_finetuned_model_from_disk", "(", "\n", "bert_config", ",", "Path", "(", "\"./models-versioned/8ac1b935-200a-41fc-94a4-5d185aab1269.pt\"", ")", ",", "\n", ")", "\n", "if", "isinstance", "(", "disk_model", ",", "Exception", ")", ":", "\n", "        ", "raise", "disk_model", "\n", "\n", "", "bert", ",", "_", "=", "disk_model", "\n", "bert_tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "bert_config", ")", "\n", "return", "bert", ",", "bert", ".", "bert", ".", "bert", ",", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_scibert": [[75, 89], ["structures.HyperParameters", "disk.load_finetuned_model_from_disk", "isinstance", "tokenizers.get_tokenizer", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer"], ["", "def", "load_scibert", "(", ")", "->", "Model", ":", "\n", "    ", "scibert_config", "=", "structures", ".", "HyperParameters", "(", "\n", "\"./experiments/scibert_32_1e6/params.toml\"", "\n", ")", "\n", "disk_model", "=", "base_disk", ".", "load_finetuned_model_from_disk", "(", "\n", "scibert_config", ",", "\n", "Path", "(", "\"./models-versioned/8128a162-5efb-4920-b63f-87a2be6fd503.pt\"", ")", ",", "\n", ")", "\n", "if", "isinstance", "(", "disk_model", ",", "Exception", ")", ":", "\n", "        ", "raise", "disk_model", "\n", "\n", "", "scibert", ",", "_", "=", "disk_model", "\n", "scibert_tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "scibert_config", ")", "\n", "return", "scibert", ",", "scibert", ".", "bert", ".", "bert", ",", "scibert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_roberta": [[91, 105], ["structures.HyperParameters", "disk.load_finetuned_model_from_disk", "isinstance", "tokenizers.get_tokenizer", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.disk.load_finetuned_model_from_disk", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer"], ["", "def", "load_roberta", "(", ")", "->", "Model", ":", "\n", "    ", "roberta_config", "=", "structures", ".", "HyperParameters", "(", "\n", "\"./experiments/roberta_base_aesw_32_1e6/params.toml\"", "\n", ")", "\n", "disk_model", "=", "base_disk", ".", "load_finetuned_model_from_disk", "(", "\n", "roberta_config", ",", "\n", "Path", "(", "\"./models-versioned/503176a5-8a15-4d3b-afa0-5e093934a611.pt\"", ")", ",", "\n", ")", "\n", "if", "isinstance", "(", "disk_model", ",", "Exception", ")", ":", "\n", "        ", "raise", "disk_model", "\n", "\n", "", "roberta", ",", "_", "=", "disk_model", "\n", "roberta_tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "roberta_config", ")", "\n", "return", "roberta", ",", "roberta", ".", "bert", ".", "roberta", ",", "roberta_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_all_models": [[107, 121], ["run.load_bert", "run.load_scibert", "run.load_roberta", "run.load_bert", "run.load_scibert", "run.load_roberta", "run.load_bert_glue", "run.load_bert_default"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_scibert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_roberta", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_scibert", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_roberta", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert_glue", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_bert_default"], ["", "def", "get_all_models", "(", "finetuned_only", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "Model", "]", ":", "\n", "    ", "if", "finetuned_only", ":", "\n", "        ", "return", "{", "\n", "\"bert\"", ":", "load_bert", "(", ")", ",", "\n", "\"scibert\"", ":", "load_scibert", "(", ")", ",", "\n", "\"roberta\"", ":", "load_roberta", "(", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "        ", "return", "{", "\n", "\"bert\"", ":", "load_bert", "(", ")", ",", "\n", "\"scibert\"", ":", "load_scibert", "(", ")", ",", "\n", "\"roberta\"", ":", "load_roberta", "(", ")", ",", "\n", "\"bert_glue\"", ":", "load_bert_glue", "(", ")", ",", "\n", "\"bert_default\"", ":", "load_bert_default", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_model_cache": [[136, 139], ["open", "pickle.dump"], "function", ["None"], ["def", "save_model_cache", "(", "model_name", ":", "str", ",", "filename", ":", "str", ",", "cache", ":", "Cache", ")", "->", "None", ":", "\n", "    ", "with", "open", "(", "PICKLE_FOLDER", "/", "model_name", "/", "filename", ",", "\"wb\"", ")", "as", "file", ":", "\n", "        ", "pickle", ".", "dump", "(", "cache", ",", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache": [[141, 144], ["open", "pickle.load", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME"], "function", ["None"], ["", "", "def", "load_model_cache", "(", "model_name", ":", "str", ",", "filename", ":", "str", ")", "->", "Cache", ":", "\n", "    ", "with", "open", "(", "PICKLE_FOLDER", "/", "model_name", "/", "filename", ",", "\"rb\"", ")", "as", "file", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "file", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.cls_attn": [[177, 184], ["None"], "function", ["None"], ["", "", "def", "cls_attn", "(", "last_layer_attn", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "heads", ",", "words_from", ",", "words_to", "=", "last_layer_attn", ".", "shape", "\n", "assert", "heads", "==", "12", ",", "heads", "\n", "assert", "words_from", "==", "words_to", "\n", "\n", "# [:, 0, :] picks the [CLS] token's attention to all words for all heads", "\n", "return", "last_layer_attn", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.use_strategy": [[186, 211], ["predict.get_best_choices", "util.multi_index", "util.multi_index", "util.multi_index", "util.multi_index", "results.append", "run.cls_attn", "run.Prediction"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.predict.get_best_choices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.cls_attn"], ["", "def", "use_strategy", "(", "\n", "edits", ":", "List", "[", "edits", ".", "Edit", "]", ",", "strategy", ":", "predict", ".", "Strategy", ",", "attn_cache", ":", "Cache", ",", "\n", ")", "->", "List", "[", "Prediction", "]", ":", "\n", "    ", "results", "=", "[", "]", "\n", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "if", "edit", ".", "id", "not", "in", "attn_cache", ":", "\n", "            ", "continue", "# tokenization error", "\n", "", "words", ",", "attn", ",", "prediction", "=", "attn_cache", "[", "edit", ".", "id", "]", "\n", "\n", "assert", "words", "==", "edit", ".", "words", ",", "f\"{words} != {edit.words}\"", "\n", "\n", "best_word_predictions", "=", "predict", ".", "get_best_choices", "(", "cls_attn", "(", "attn", ")", ",", "strategy", ")", "\n", "\n", "target_words", "=", "util", ".", "multi_index", "(", "words", ",", "edit", ".", "best_words", ")", "\n", "assert", "\"[CLS]\"", "not", "in", "target_words", ",", "edit", "\n", "assert", "\"[SEP]\"", "not", "in", "target_words", ",", "edit", "\n", "\n", "predicted_words", "=", "util", ".", "multi_index", "(", "words", ",", "best_word_predictions", ")", "\n", "assert", "\"[CLS]\"", "not", "in", "predicted_words", ",", "best_word_predictions", "\n", "assert", "\"[SEP]\"", "not", "in", "predicted_words", ",", "best_word_predictions", "\n", "\n", "results", ".", "append", "(", "Prediction", "(", "edit", ".", "id", ",", "edit", ".", "best_words", ",", "best_word_predictions", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_random_predictions": [[213, 242], ["random.seed", "list", "random.shuffle", "util.multi_index", "util.multi_index", "util.multi_index", "util.multi_index", "results.append", "range", "run.Prediction", "len", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index"], ["", "def", "get_random_predictions", "(", "edits", ":", "List", "[", "edits", ".", "Edit", "]", ")", "->", "List", "[", "Prediction", "]", ":", "\n", "    ", "random", ".", "seed", "(", "42", ")", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "for", "edit", "in", "edits", ":", "\n", "\n", "        ", "possible_indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "edit", ".", "words", ")", "-", "1", ")", "\n", ")", "# 1 to skip [CLS], len(edit.words)-1 to skip [SEP]", "\n", "random", ".", "shuffle", "(", "possible_indices", ")", "\n", "assert", "0", "not", "in", "possible_indices", "\n", "assert", "len", "(", "edit", ".", "words", ")", "-", "1", "not", "in", "possible_indices", "\n", "best_word_predictions", "=", "possible_indices", "\n", "\n", "target_words", "=", "util", ".", "multi_index", "(", "edit", ".", "words", ",", "edit", ".", "best_words", ")", "\n", "assert", "\"[CLS]\"", "not", "in", "target_words", ",", "edit", "\n", "assert", "\"[SEP]\"", "not", "in", "target_words", ",", "edit", "\n", "\n", "if", "not", "target_words", ":", "\n", "            ", "continue", "\n", "\n", "", "predicted_words", "=", "util", ".", "multi_index", "(", "edit", ".", "words", ",", "best_word_predictions", ")", "\n", "assert", "\"[CLS]\"", "not", "in", "predicted_words", ",", "best_word_predictions", "\n", "assert", "\"[SEP]\"", "not", "in", "predicted_words", ",", "best_word_predictions", "\n", "\n", "results", ".", "append", "(", "Prediction", "(", "edit", ".", "id", ",", "edit", ".", "best_words", ",", "best_word_predictions", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy": [[252, 281], ["len", "run.EvalStrategyResult", "run.EvalStrategyResult", "jaccard_sims.append", "len", "statistics.mean", "prediction.top_n", "prediction.top_n", "prediction.jaccard_similarity", "len"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity"], ["", "def", "evaluate_strategy", "(", "predictions", ":", "List", "[", "Prediction", "]", ")", "->", "EvalStrategyResult", ":", "\n", "    ", "total", "=", "len", "(", "predictions", ")", "\n", "if", "not", "total", ":", "\n", "        ", "return", "EvalStrategyResult", "(", ")", "\n", "\n", "", "exact_matches", "=", "0", "\n", "jaccard_sims", "=", "[", "]", "\n", "top_3_matches", "=", "0", "\n", "\n", "for", "prediction", "in", "predictions", ":", "\n", "        ", "if", "prediction", ".", "target", "==", "prediction", ".", "top_n", "(", ")", ":", "\n", "            ", "exact_matches", "+=", "1", "\n", "\n", "", "if", "prediction", ".", "target", "<=", "prediction", ".", "top_n", "(", "3", ")", ":", "\n", "            ", "assert", "len", "(", "prediction", ".", "target", ")", ">", "0", "\n", "top_3_matches", "+=", "1", "\n", "\n", "", "jaccard_sims", ".", "append", "(", "prediction", ".", "jaccard_similarity", "(", ")", ")", "\n", "\n", "", "exact_match_accuracy", "=", "exact_matches", "/", "total", "\n", "top_3_accuracy", "=", "top_3_matches", "/", "total", "\n", "\n", "jaccard_match_accuracy", "=", "len", "(", "[", "s", "for", "s", "in", "jaccard_sims", "if", "s", ">=", "0.5", "]", ")", "/", "total", "\n", "\n", "return", "EvalStrategyResult", "(", "\n", "exact_match_accuracy", ",", "\n", "jaccard_match_accuracy", ",", "\n", "statistics", ".", "mean", "(", "jaccard_sims", ")", ",", "\n", "top_3_accuracy", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_attention_weights": [[290, 318], ["util.my_tqdm", "util.chunks", "attention.get_words_and_attention_and_prediction", "zip", "iter", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.util.chunks", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_words_and_attention_and_prediction"], ["", "def", "get_attention_weights", "(", "\n", "edits", ":", "Iterable", "[", "edits", ".", "Edit", "]", ",", "\n", "cls_model", ":", "models", ".", "SentenceClassificationModel", ",", "\n", "model", ":", "PreTrainedModel", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", "skip_false_predictions", ":", "bool", "=", "False", ",", "\n", ")", "->", "Cache", ":", "\n", "    ", "saved", "=", "{", "}", "\n", "\n", "for", "edit_batch", "in", "base_util", ".", "my_tqdm", "(", "base_util", ".", "chunks", "(", "iter", "(", "edits", ")", ",", "8", ")", ")", ":", "\n", "        ", "sents", "=", "[", "edit", ".", "sent", "for", "edit", "in", "edit_batch", "]", "\n", "result", "=", "attention", ".", "get_words_and_attention_and_prediction", "(", "\n", "sents", ",", "cls_model", ",", "model", ",", "tokenizer", "\n", ")", "\n", "\n", "for", "edit", ",", "(", "words", ",", "attn", ",", "prediction", ")", "in", "zip", "(", "edit_batch", ",", "result", ")", ":", "\n", "            ", "if", "skip_false_predictions", "and", "not", "prediction", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "edit", ".", "words", "!=", "words", ":", "\n", "                ", "print", "(", "edit", ".", "id", ")", "\n", "print", "(", "edit", ".", "words", ")", "\n", "print", "(", "words", ")", "\n", "continue", "\n", "\n", "", "saved", "[", "edit", ".", "id", "]", "=", "(", "words", ",", "attn", ",", "prediction", ")", "\n", "\n", "", "", "return", "saved", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_edit_weights": [[320, 327], ["run.get_all_models", "get_all_models.items", "run.get_attention_weights", "run.save_model_cache", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_all_models", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_attention_weights", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_model_cache"], ["", "def", "save_edit_weights", "(", "all_edits", ":", "Iterable", "[", "edits", ".", "Edit", "]", ",", "filename", ":", "str", ")", "->", "None", ":", "\n", "    ", "all_models", "=", "get_all_models", "(", ")", "\n", "\n", "# save all the weights", "\n", "for", "model_name", ",", "(", "cls_model", ",", "model", ",", "tokenizer", ")", "in", "all_models", ".", "items", "(", ")", ":", "\n", "        ", "cache", "=", "get_attention_weights", "(", "all_edits", ",", "cls_model", ",", "model", ",", "tokenizer", ")", "\n", "save_model_cache", "(", "model_name", ",", "filename", ",", "cache", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_comma_edit_weights": [[329, 332], ["edits.get_comma_edits", "run.save_edit_weights"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_comma_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_edit_weights"], ["", "", "def", "save_comma_edit_weights", "(", ")", "->", "None", ":", "\n", "    ", "all_edits", "=", "edits", ".", "get_comma_edits", "(", ")", "\n", "save_edit_weights", "(", "all_edits", ",", "disk", ".", "COMMA_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_delete_edit_weights": [[334, 337], ["edits.get_edits_with_only_deleted_words", "run.save_edit_weights"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_deleted_words", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_edit_weights"], ["", "def", "save_delete_edit_weights", "(", ")", "->", "None", ":", "\n", "    ", "all_edits", "=", "edits", ".", "get_edits_with_only_deleted_words", "(", ")", "\n", "save_edit_weights", "(", "all_edits", ",", "disk", ".", "DELETE_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_spelling_edit_weights": [[339, 342], ["edits.get_edits_with_only_spelling_errors", "run.save_edit_weights"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_spelling_errors", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.save_edit_weights"], ["", "def", "save_spelling_edit_weights", "(", ")", "->", "None", ":", "\n", "    ", "all_edits", "=", "edits", ".", "get_edits_with_only_spelling_errors", "(", "again", "=", "True", ")", "\n", "save_edit_weights", "(", "all_edits", ",", "disk", ".", "SPELLING_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_universal_correct_predictions": [[350, 383], ["set", "run.load_model_cache", "load_model_cache.items", "set", "run.load_model_cache", "load_model_cache.items", "set", "run.load_model_cache", "load_model_cache.items", "len", "len", "len", "len", "len", "len", "set.add", "set.add", "set.add", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.COMMA_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME", "disk.SPELLING_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME", "disk.DELETE_EDIT_WEIGHTS_FILENAME"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache"], ["", "def", "get_universal_correct_predictions", "(", "weights_filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "bert_correct_predictions", "=", "set", "(", ")", "\n", "\n", "bert_cache", "=", "load_model_cache", "(", "\"bert\"", ",", "weights_filename", ")", "\n", "for", "sent_id", ",", "(", "_", ",", "_", ",", "prediction", ")", "in", "bert_cache", ".", "items", "(", ")", ":", "\n", "        ", "if", "prediction", ":", "\n", "            ", "bert_correct_predictions", ".", "add", "(", "sent_id", ")", "\n", "\n", "", "", "roberta_correct_predictions", "=", "set", "(", ")", "\n", "\n", "roberta_cache", "=", "load_model_cache", "(", "\"roberta\"", ",", "weights_filename", ")", "\n", "for", "sent_id", ",", "(", "_", ",", "_", ",", "prediction", ")", "in", "roberta_cache", ".", "items", "(", ")", ":", "\n", "        ", "if", "prediction", ":", "\n", "            ", "roberta_correct_predictions", ".", "add", "(", "sent_id", ")", "\n", "\n", "", "", "scibert_correct_predictions", "=", "set", "(", ")", "\n", "\n", "scibert_cache", "=", "load_model_cache", "(", "\"scibert\"", ",", "weights_filename", ")", "\n", "for", "sent_id", ",", "(", "_", ",", "_", ",", "prediction", ")", "in", "scibert_cache", ".", "items", "(", ")", ":", "\n", "        ", "if", "prediction", ":", "\n", "            ", "scibert_correct_predictions", ".", "add", "(", "sent_id", ")", "\n", "\n", "", "", "universal", "=", "(", "\n", "bert_correct_predictions", "\n", "&", "scibert_correct_predictions", "\n", "&", "roberta_correct_predictions", "\n", ")", "\n", "\n", "assert", "len", "(", "universal", ")", "<=", "len", "(", "bert_correct_predictions", ")", "\n", "assert", "len", "(", "universal", ")", "<=", "len", "(", "scibert_correct_predictions", ")", "\n", "assert", "len", "(", "universal", ")", "<=", "len", "(", "roberta_correct_predictions", ")", "\n", "\n", "return", "universal", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_all_strategies_on_comma_edits": [[385, 460], ["edits.get_comma_edits", "run.get_universal_correct_predictions", "run.get_random_predictions", "run.evaluate_strategy", "scores.append", "scores.append", "run.load_model_cache", "structures.Score", "structures.Score", "open", "json.dump", "run.use_strategy", "run.evaluate_strategy", "scores.append", "load_model_cache.items", "structures.Score"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_comma_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_universal_correct_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_random_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.use_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy"], ["", "def", "evaluate_all_strategies_on_comma_edits", "(", ")", "->", "None", ":", "\n", "    ", "comma_edits", "=", "edits", ".", "get_comma_edits", "(", ")", "\n", "\n", "all_strategies", ":", "List", "[", "predict", ".", "Strategy", "]", "=", "[", "\"sum\"", ",", "\"max\"", "]", "\n", "\n", "skip_false_predictions_models", "=", "[", "\n", "\"bert\"", ",", "\n", "\"scibert\"", ",", "\n", "\"roberta\"", ",", "\n", "\"bert_default\"", ",", "\n", "\"bert_glue\"", ",", "\n", "]", "\n", "\n", "scores", ":", "List", "[", "Score", "]", "=", "[", "]", "\n", "\n", "universal", "=", "get_universal_correct_predictions", "(", "disk", ".", "COMMA_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "for", "model_name", "in", "skip_false_predictions_models", ":", "\n", "# load the weights for a model", "\n", "        ", "cache", "=", "load_model_cache", "(", "model_name", ",", "disk", ".", "COMMA_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "# only do predictions that all models get", "\n", "cache", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "cache", ".", "items", "(", ")", "if", "key", "in", "universal", "}", "\n", "\n", "# for each strategy and type of edit, make a bunch of guesses", "\n", "for", "strategy", "in", "all_strategies", ":", "\n", "\n", "            ", "predictions", "=", "use_strategy", "(", "comma_edits", ",", "strategy", ",", "cache", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "model_name", ",", "\n", "strategy", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "", "", "random_predictions", "=", "get_random_predictions", "(", "comma_edits", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "random_predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"sum\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"max\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "with", "open", "(", "\n", "\"./data-unversioned/attention-weights/scores/comma-edit.json\"", ",", "\"w\"", "\n", ")", "as", "file", ":", "\n", "        ", "json", ".", "dump", "(", "scores", ",", "file", ",", "indent", "=", "4", ",", "cls", "=", "ScoreEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_all_strategies_on_spelling_edits": [[462, 537], ["edits.get_edits_with_only_spelling_errors", "run.get_universal_correct_predictions", "run.get_random_predictions", "run.evaluate_strategy", "scores.append", "scores.append", "run.load_model_cache", "structures.Score", "structures.Score", "open", "json.dump", "run.use_strategy", "run.evaluate_strategy", "scores.append", "load_model_cache.items", "structures.Score"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_spelling_errors", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_universal_correct_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_random_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.use_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy"], ["", "", "def", "evaluate_all_strategies_on_spelling_edits", "(", ")", "->", "None", ":", "\n", "    ", "spelling_edits", "=", "edits", ".", "get_edits_with_only_spelling_errors", "(", ")", "\n", "\n", "all_strategies", ":", "List", "[", "predict", ".", "Strategy", "]", "=", "[", "\"sum\"", ",", "\"max\"", "]", "\n", "\n", "skip_false_predictions_models", "=", "[", "\n", "\"bert\"", ",", "\n", "\"scibert\"", ",", "\n", "\"roberta\"", ",", "\n", "\"bert_default\"", ",", "\n", "\"bert_glue\"", ",", "\n", "]", "\n", "\n", "scores", ":", "List", "[", "Score", "]", "=", "[", "]", "\n", "\n", "universal", "=", "get_universal_correct_predictions", "(", "disk", ".", "SPELLING_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "for", "model_name", "in", "skip_false_predictions_models", ":", "\n", "# load the weights for a model", "\n", "        ", "cache", "=", "load_model_cache", "(", "model_name", ",", "disk", ".", "SPELLING_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "# only do predictions that all models get", "\n", "cache", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "cache", ".", "items", "(", ")", "if", "key", "in", "universal", "}", "\n", "\n", "# for each strategy and type of edit, make a bunch of guesses", "\n", "for", "strategy", "in", "all_strategies", ":", "\n", "\n", "            ", "predictions", "=", "use_strategy", "(", "spelling_edits", ",", "strategy", ",", "cache", ",", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "model_name", ",", "\n", "strategy", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "", "", "random_predictions", "=", "get_random_predictions", "(", "spelling_edits", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "random_predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"sum\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"max\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "with", "open", "(", "\n", "\"./data-unversioned/attention-weights/scores/spelling-edit.json\"", ",", "\"w\"", "\n", ")", "as", "file", ":", "\n", "        ", "json", ".", "dump", "(", "scores", ",", "file", ",", "indent", "=", "4", ",", "cls", "=", "ScoreEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_all_strategies_on_delete_edits": [[539, 614], ["edits.get_edits_with_only_deleted_words", "run.get_universal_correct_predictions", "run.get_random_predictions", "run.evaluate_strategy", "scores.append", "scores.append", "run.load_model_cache", "structures.Score", "structures.Score", "open", "json.dump", "run.use_strategy", "run.evaluate_strategy", "scores.append", "load_model_cache.items", "structures.Score"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.edits.get_edits_with_only_deleted_words", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_universal_correct_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_random_predictions", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.load_model_cache", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.use_strategy", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.evaluate_strategy"], ["", "", "def", "evaluate_all_strategies_on_delete_edits", "(", ")", "->", "None", ":", "\n", "    ", "delete_edits", "=", "edits", ".", "get_edits_with_only_deleted_words", "(", ")", "\n", "\n", "all_strategies", ":", "List", "[", "predict", ".", "Strategy", "]", "=", "[", "\"sum\"", ",", "\"max\"", "]", "\n", "\n", "skip_false_predictions_models", "=", "[", "\n", "\"bert\"", ",", "\n", "\"scibert\"", ",", "\n", "\"roberta\"", ",", "\n", "\"bert_default\"", ",", "\n", "\"bert_glue\"", ",", "\n", "]", "\n", "\n", "scores", ":", "List", "[", "Score", "]", "=", "[", "]", "\n", "\n", "universal", "=", "get_universal_correct_predictions", "(", "disk", ".", "DELETE_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "for", "model_name", "in", "skip_false_predictions_models", ":", "\n", "# load the weights for a model", "\n", "        ", "cache", "=", "load_model_cache", "(", "model_name", ",", "disk", ".", "DELETE_EDIT_WEIGHTS_FILENAME", ")", "\n", "\n", "# only do predictions that all models get", "\n", "cache", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "cache", ".", "items", "(", ")", "if", "key", "in", "universal", "}", "\n", "\n", "# for each strategy and type of edit, make a bunch of guesses", "\n", "for", "strategy", "in", "all_strategies", ":", "\n", "\n", "            ", "predictions", "=", "use_strategy", "(", "delete_edits", ",", "strategy", ",", "cache", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "model_name", ",", "\n", "strategy", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "", "", "random_predictions", "=", "get_random_predictions", "(", "delete_edits", ")", "\n", "\n", "result", "=", "evaluate_strategy", "(", "random_predictions", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"sum\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "scores", ".", "append", "(", "\n", "Score", "(", "\n", "\"random\"", ",", "\n", "\"max\"", ",", "\n", "result", ".", "exact_match_accuracy", ",", "\n", "result", ".", "jaccard_match_accuracy", ",", "\n", "result", ".", "mean_jaccard_similarity", ",", "\n", "result", ".", "top_3_match_accuracy", ",", "\n", "False", ",", "\n", ")", "\n", ")", "\n", "\n", "with", "open", "(", "\n", "\"./data-unversioned/attention-weights/scores/delete-edit.json\"", ",", "\"w\"", "\n", ")", "as", "file", ":", "\n", "        ", "json", ".", "dump", "(", "scores", ",", "file", ",", "indent", "=", "4", ",", "cls", "=", "ScoreEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.Score.parse": [[18, 33], ["structures.Score"], "methods", ["None"], ["train_preprocessed_folder", ":", "Path", "\n", "val_file", ":", "Path", "\n", "val_preprocessed_folder", ":", "Path", "\n", "test_file", ":", "Path", "\n", "test_preprocessed_folder", ":", "Path", "\n", "model_name", ":", "str", "\n", "root_model_name", ":", "Optional", "[", "str", "]", "\n", "tokenizer_name", ":", "str", "\n", "models_dir", ":", "Path", "\n", "experiment_name", ":", "str", "\n", "checkpoint_interval", ":", "float", "\n", "vocab_size", ":", "int", "\n", "checkpoint_csv", ":", "Path", "\n", "\n", "def", "__init__", "(", "self", ",", "toml_filepath", ":", "str", ")", "->", "None", ":", "\n", "        ", "parsed", "=", "toml", ".", "load", "(", "[", "toml_filepath", "]", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.ScoreEncoder.default": [[36, 49], ["isinstance", "super().default"], "methods", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.structures.ScoreEncoder.default"], ["self", ".", "learning_rate", "=", "float", "(", "parsed", "[", "\"learning_rate\"", "]", ")", "\n", "self", ".", "max_epochs", "=", "int", "(", "parsed", "[", "\"max_epochs\"", "]", ")", "\n", "self", ".", "model_name", "=", "parsed", "[", "\"model_name\"", "]", "\n", "self", ".", "tokenizer_name", "=", "parsed", "[", "\"tokenizer_name\"", "]", "\n", "self", ".", "val_file", "=", "Path", "(", "parsed", "[", "\"val_file\"", "]", ")", "\n", "self", ".", "test_file", "=", "Path", "(", "parsed", "[", "\"test_file\"", "]", ")", "\n", "self", ".", "experiment_name", "=", "parsed", "[", "\"experiment_name\"", "]", "\n", "self", ".", "models_dir", "=", "Path", "(", "parsed", "[", "\"models_dir\"", "]", ")", "\n", "self", ".", "checkpoint_interval", "=", "float", "(", "parsed", "[", "\"checkpoint_interval\"", "]", ")", "\n", "\n", "self", ".", "train_file", "=", "None", "\n", "self", ".", "train_folder", "=", "None", "\n", "if", "\"train_file\"", "in", "parsed", ":", "\n", "            ", "self", ".", "train_file", "=", "Path", "(", "parsed", "[", "\"train_file\"", "]", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences": [[8, 31], ["nltk.word_tokenize", "arbitrary_token.join().replace().replace().split", "util.tokenize_transformer_sentences.fix_quotes"], "function", ["None"], ["from", ".", "types", "import", "T", "\n", "\n", "\n", "def", "grouper", "(", "\n", "iterable", ":", "Iterable", "[", "T", "]", ",", "n", ":", "int", ",", "fillvalue", ":", "Optional", "[", "T", "]", "=", "None", "\n", ")", "->", "Iterator", "[", "List", "[", "T", "]", "]", ":", "\n", "    ", "args", "=", "[", "iter", "(", "iterable", ")", "]", "*", "n", "\n", "return", "itertools", ".", "zip_longest", "(", "*", "args", ",", "fillvalue", "=", "fillvalue", ")", "\n", "\n", "\n", "", "def", "chunks", "(", "elements", ":", "Iterator", "[", "T", "]", ",", "n", ":", "int", ")", "->", "Iterator", "[", "List", "[", "T", "]", "]", ":", "\n", "    ", "\"\"\"\n    Yield successive n-sized chunks from elements.\n    \"\"\"", "\n", "\n", "while", "True", ":", "\n", "        ", "chunk", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "chunk", ".", "append", "(", "next", "(", "elements", ")", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "yield", "chunk", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.flatten": [[35, 40], ["None"], "function", ["None"], ["", "", "def", "get_device", "(", ")", "->", "torch", ".", "device", ":", "# type: ignore", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cuda\"", ")", "# type: ignore", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cpu\"", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.multi_index": [[42, 47], ["result.append"], "function", ["None"], ["", "", "def", "print_device", "(", ")", "->", "None", ":", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "print", "(", "\"We will use:\"", ",", "torch", ".", "cuda", ".", "get_device_name", "(", "0", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"We will use the CPU.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.get_edits": [[79, 100], ["set", "util.my_tqdm", "random.seed", "random.shuffle", "lxml.etree.iterparse", "aesw_to_sentences.extract_sentence_id", "util.tokenize_transformer_sentences", "util.tokenize_transformer_sentences", "results.append", "aesw_to_sentences.extract_sentence", "qualitative_study.QualitativeEdit"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence_id", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.util.tokenize_transformer_sentences", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence"], ["", "def", "get_edits", "(", "ids", ":", "List", "[", "str", "]", ")", "->", "List", "[", "QualitativeEdit", "]", ":", "\n", "    ", "results", "=", "[", "]", "\n", "\n", "id_set", "=", "set", "(", "ids", ")", "\n", "\n", "for", "_", ",", "sent_elem", "in", "base_util", ".", "my_tqdm", "(", "\n", "etree", ".", "iterparse", "(", "disk", ".", "DEV_XML", ",", "tag", "=", "\"sentence\"", ")", "\n", ")", ":", "\n", "        ", "sent_id", "=", "aesw_to_sentences", ".", "extract_sentence_id", "(", "sent_elem", ")", "\n", "if", "sent_id", "not", "in", "id_set", ":", "\n", "            ", "continue", "\n", "\n", "", "words", "=", "util", ".", "tokenize_transformer_sentences", "(", "\n", "aesw_to_sentences", ".", "extract_sentence", "(", "sent_elem", ")", ",", "add_special_tokens", "=", "True", "\n", ")", "\n", "results", ".", "append", "(", "QualitativeEdit", "(", "sent_id", ",", "sent_elem", ",", "words", ",", "{", "}", ",", "{", "}", ")", ")", "\n", "\n", "", "random", ".", "seed", "(", "42", ")", "\n", "random", ".", "shuffle", "(", "results", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.escape_special_tokens": [[102, 108], ["dirty.replace().replace().replace().replace", "dirty.replace().replace().replace", "dirty.replace().replace", "dirty.replace"], "function", ["None"], ["", "def", "escape_special_tokens", "(", "dirty", ":", "str", ")", "->", "str", ":", "\n", "    ", "return", "(", "\n", "dirty", ".", "replace", "(", "\"_MATH_\"", ",", "r\"\\_MATH\\_\"", ")", "\n", ".", "replace", "(", "\"_CITE_\"", ",", "r\"\\_CITE\\_\"", ")", "\n", ".", "replace", "(", "\"_MATHDISP_\"", ",", "r\"\\_MATHDISP\\_\"", ")", "\n", ".", "replace", "(", "\"_REF_\"", ",", "r\"\\_REF\\_\"", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.to_markdown_row": [[111, 149], ["string_builder.append", "string_builder.append", "string_builder.append", "string_builder.append", "range", "string_builder.append", "qualitative_study.escape_special_tokens", "string_builder.append", "string_builder.append", "str", "string_builder.append", "string_builder.append", "string_builder.append", "string_builder.append", "ValueError", "str", "str", "string_builder.append", "str", "str().strip", "str", "str().strip", "str", "str"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.escape_special_tokens"], ["", "def", "to_markdown_row", "(", "edit", ":", "QualitativeEdit", ")", "->", "str", ":", "\n", "    ", "string_builder", "=", "[", "]", "\n", "\n", "string_builder", ".", "append", "(", "f\"({edit.sent_id}) \"", ")", "\n", "\n", "if", "edit", ".", "sent_elem", ".", "text", ":", "\n", "        ", "string_builder", ".", "append", "(", "str", "(", "edit", ".", "sent_elem", ".", "text", ")", ")", "\n", "\n", "", "for", "edit_elem", "in", "edit", ".", "sent_elem", ":", "\n", "        ", "if", "edit_elem", ".", "tag", "==", "\"del\"", ":", "\n", "            ", "string_builder", ".", "append", "(", "f\"~~{str(edit_elem.text).strip()}~~\"", ")", "\n", "if", "str", "(", "edit_elem", ".", "text", ")", "[", "-", "1", "]", "==", "\" \"", ":", "\n", "                ", "string_builder", ".", "append", "(", "\" \"", ")", "\n", "", "", "elif", "edit_elem", ".", "tag", "==", "\"ins\"", ":", "\n", "            ", "string_builder", ".", "append", "(", "f\"**{str(edit_elem.text).strip()}**\"", ")", "\n", "if", "str", "(", "edit_elem", ".", "text", ")", "[", "-", "1", "]", "==", "\" \"", ":", "\n", "                ", "string_builder", ".", "append", "(", "\" \"", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "str", "(", "edit_elem", ")", ")", "\n", "\n", "", "if", "edit_elem", ".", "tail", ":", "\n", "            ", "string_builder", ".", "append", "(", "str", "(", "edit_elem", ".", "tail", ")", ")", "\n", "\n", "", "", "string_builder", ".", "append", "(", "\"\\n\\n\"", ")", "\n", "\n", "string_builder", ".", "append", "(", "\"|   | bert | scibert | roberta |\\n\"", ")", "\n", "string_builder", ".", "append", "(", "\"|---|---|---|---|\\n\"", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "string_builder", ".", "append", "(", "\n", "f\"|{i+1}. | {edit.top_3_words['bert'][i]} | {edit.top_3_words['scibert'][i]} | {edit.top_3_words['roberta'][i]} |\\n\"", "\n", ")", "\n", "\n", "# string_builder.append(", "\n", "#     f\"| Correct prediction | {edit.predictions['bert'] == needs_edit} | {edit.predictions['scibert']== needs_edit} | {edit.predictions['roberta']== needs_edit} |\\n\"", "\n", "# )", "\n", "", "string_builder", ".", "append", "(", "\"|Relevant?| | | |\\n\\n\"", ")", "\n", "\n", "return", "escape_special_tokens", "(", "\"\"", ".", "join", "(", "string_builder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.set_top_3_words": [[151, 189], ["iter", "aesw_to_sentences.extract_sentence", "attention.get_words_and_attention_and_prediction", "print", "print", "print", "predict.get_best_choices", "len", "len", "run.cls_attn"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.aesw_to_sentences.extract_sentence", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.attention.get_words_and_attention_and_prediction", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.predict.get_best_choices", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.cls_attn"], ["", "def", "set_top_3_words", "(", "\n", "edits", ":", "List", "[", "QualitativeEdit", "]", ",", "\n", "model_name", ":", "str", ",", "\n", "cls_model", ":", "models", ".", "SentenceClassificationModel", ",", "\n", "model", ":", "PreTrainedModel", ",", "\n", "tokenizer", ":", "PreTrainedTokenizerFast", ",", "\n", ")", "->", "None", ":", "\n", "    ", "for", "edit", "in", "iter", "(", "edits", ")", ":", "\n", "        ", "sent", "=", "aesw_to_sentences", ".", "extract_sentence", "(", "edit", ".", "sent_elem", ")", "\n", "\n", "words", ",", "attn", ",", "prediction", "=", "attention", ".", "get_words_and_attention_and_prediction", "(", "\n", "[", "sent", "]", ",", "cls_model", ",", "model", ",", "tokenizer", "\n", ")", "[", "0", "]", "\n", "\n", "if", "edit", ".", "words", "!=", "words", ":", "\n", "            ", "print", "(", "edit", ".", "sent_id", ")", "\n", "print", "(", "edit", ".", "words", ")", "\n", "print", "(", "words", ")", "\n", "continue", "\n", "\n", "", "top_3_indices", "=", "predict", ".", "get_best_choices", "(", "run", ".", "cls_attn", "(", "attn", ")", ",", "\"sum\"", ")", "[", ":", "3", "]", "\n", "\n", "top_3_words", "=", "[", "edit", ".", "words", "[", "i", "]", "for", "i", "in", "top_3_indices", "]", "\n", "\n", "# print(edit.sent_id, model_name, top_3_indices, top_3_words)", "\n", "\n", "assert", "len", "(", "top_3_words", ")", "==", "3", "\n", "\n", "edit", ".", "top_3_words", "[", "model_name", "]", "=", "(", "\n", "top_3_words", "[", "0", "]", ",", "\n", "top_3_words", "[", "1", "]", ",", "\n", "top_3_words", "[", "2", "]", ",", "\n", ")", "\n", "\n", "edit", ".", "predictions", "[", "model_name", "]", "=", "prediction", "\n", "edit", ".", "target", "=", "len", "(", "edit", ".", "sent_elem", ")", ">", "0", "\n", "\n", "assert", "prediction", "==", "edit", ".", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.main": [[191, 215], ["qualitative_study.get_edits", "run.get_all_models", "run.get_all_models.items", "print", "qualitative_study.set_top_3_words", "open", "file.write", "file.write", "file.writelines", "file.write", "file.writelines", "qualitative_study.to_markdown_row", "qualitative_study.to_markdown_row", "model_name.capitalize"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.get_edits", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.get_all_models", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.set_top_3_words", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.to_markdown_row", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.qualitative_study.to_markdown_row"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "edit_list", "=", "get_edits", "(", "DELETED_ERRORS", "+", "SPELLING_ERRORS", ")", "\n", "\n", "models", "=", "run", ".", "get_all_models", "(", "finetuned_only", "=", "True", ")", "\n", "\n", "for", "model_name", ",", "(", "cls_model", ",", "model", ",", "tokenizer", ")", "in", "models", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "f\"Working on {model_name.capitalize()}\"", ")", "\n", "set_top_3_words", "(", "edit_list", ",", "model_name", ",", "cls_model", ",", "model", ",", "tokenizer", ")", "\n", "\n", "", "spelling_edits", "=", "[", "e", "for", "e", "in", "edit_list", "if", "e", ".", "sent_id", "in", "SPELLING_ERRORS", "]", "\n", "deleted_edits", "=", "[", "e", "for", "e", "in", "edit_list", "if", "e", ".", "sent_id", "in", "DELETED_ERRORS", "]", "\n", "\n", "with", "open", "(", "disk", ".", "QUALITATIVE_MD_FILE", ",", "\"w\"", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "\n", "f\"<!--\\npandoc --out qualititative-study.pdf {disk.QUALITATIVE_MD_FILE} && open qualititative-study.pdf\\n-->\\n\\n\"", "\n", ")", "\n", "\n", "file", ".", "write", "(", "\"# Spelling Errors\\n\\n\"", ")", "\n", "\n", "file", ".", "writelines", "(", "[", "to_markdown_row", "(", "edit", ")", "for", "edit", "in", "spelling_edits", "]", ")", "\n", "\n", "file", ".", "write", "(", "\"# Deleted Errors\\n\\n\"", ")", "\n", "\n", "file", ".", "writelines", "(", "[", "to_markdown_row", "(", "edit", ")", "for", "edit", "in", "deleted_edits", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.decoding.check_special_tokens": [[7, 10], ["None"], "function", ["None"], ["def", "check_special_tokens", "(", "original", ":", "str", ",", "decoded", ":", "str", ")", "->", "None", ":", "\n", "    ", "for", "tok", "in", "tokenizers", ".", "AESW_TOKENS", ":", "\n", "        ", "assert", "(", "tok", "in", "original", ")", "==", "(", "tok", "in", "decoded", ")", ",", "f\"'{original}' '{decoded}'\"", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.decoding.check_encoding_decoding": [[12, 21], ["tokenizers.get_tokenizer", "util.my_tqdm", "csv_merge.sorted_csv_files_reader", "tokenizers.encode_batch", "tokenizers.decode", "decoding.check_special_tokens", "ids_list[].tolist"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.csv_merge.sorted_csv_files_reader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.encode_batch", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.decoding.check_special_tokens"], ["", "", "def", "check_encoding_decoding", "(", "config", ":", "HyperParameters", ")", "->", "None", ":", "\n", "    ", "data_files", "=", "[", "config", ".", "val_file", ",", "config", ".", "test_file", "]", "\n", "\n", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "for", "sentence", ",", "label", "in", "util", ".", "my_tqdm", "(", "csv_merge", ".", "sorted_csv_files_reader", "(", "data_files", ")", ")", ":", "\n", "        ", "ids_list", ",", "_", "=", "tokenizers", ".", "encode_batch", "(", "[", "sentence", "]", ",", "tokenizer", ")", "\n", "decoded", "=", "tokenizers", ".", "decode", "(", "ids_list", "[", "0", "]", ".", "tolist", "(", ")", ",", "tokenizer", ")", "\n", "check_special_tokens", "(", "sentence", ",", "decoded", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.decoding.check_decoding": [[23, 44], ["tokenizers.get_tokenizer", "print", "data.prepare_val_data", "data.get_val_dataloader", "print", "util.my_tqdm", "ids.tolist", "tokenizers.decode"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.get_tokenizer", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.prepare_val_data", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.data.get_val_dataloader", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.paper.tokenizers.decode"], ["", "", "def", "check_decoding", "(", "config", ":", "HyperParameters", ")", "->", "None", ":", "\n", "    ", "tokenizer", "=", "tokenizers", ".", "get_tokenizer", "(", "config", ")", "\n", "\n", "print", "(", "\"Preprocessing data...\"", ")", "\n", "data", ".", "prepare_val_data", "(", "config", ",", "tokenizer", ")", "\n", "\n", "loader", "=", "data", ".", "get_val_dataloader", "(", "config", ")", "\n", "\n", "print", "(", "\"Decoding data...\"", ")", "\n", "for", "batch", "in", "util", ".", "my_tqdm", "(", "loader", ")", ":", "\n", "        ", "original_ids", "=", "[", "ids", ".", "tolist", "(", ")", "for", "ids", "in", "batch", "[", "0", "]", "]", "\n", "sentences", "=", "[", "tokenizers", ".", "decode", "(", "ids", ",", "tokenizer", ")", "for", "ids", "in", "original_ids", "]", "\n", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "if", "\"_MATH_\"", "in", "sent", ":", "\n", "                ", "math_seen", "=", "True", "\n", "", "if", "\"_MATHDISP_\"", "in", "sent", ":", "\n", "                ", "mathdisp_seen", "=", "True", "\n", "\n", "", "", "", "assert", "math_seen", ",", "\"didn't see any '_MATH_'\"", "\n", "assert", "mathdisp_seen", ",", "\"didn't see any '_MATHDISP_'\"", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.data.main": [[5, 26], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "open", "csv.reader", "next", "int"], "function", ["None"], ["import", "shutil", "\n", "from", "pathlib", "import", "Path", "\n", "from", "typing", "import", "Iterator", ",", "List", ",", "Optional", ",", "Tuple", ",", "cast", "\n", "\n", "import", "torch", "\n", "from", "torch", "import", "Tensor", "\n", "from", "torch", ".", "utils", ".", "data", "import", "DataLoader", ",", "Sampler", ",", "Subset", ",", "TensorDataset", "\n", "from", "transformers", "import", "PreTrainedTokenizerFast", "\n", "\n", "from", ".", "import", "csv_merge", ",", "tokenizers", ",", "util", "\n", "from", ".", "structures", "import", "HyperParameters", "\n", "\n", "Row", "=", "Tuple", "[", "Tensor", ",", "Tensor", ",", "Tensor", "]", "\n", "\n", "\n", "class", "RandomBatchSampler", "(", "Sampler", ")", ":", "# type: ignore", "\n", "    ", "\"\"\"\n    Batches the data into size `batch_size`, then randomly shuffles those batches. Last batch is smaller, because there might not be a full batch.\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "data", ":", "TensorDataset", ",", "batch_size", ":", "int", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "data", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.check.params.main": [[6, 23], ["os.listdir", "os.path.join", "print", "print", "os.path.isfile", "structures.HyperParameters", "print", "str"], "function", ["None"], ["def", "main", "(", ")", "->", "None", ":", "\n", "    ", "count", "=", "0", "\n", "for", "foldername", "in", "os", ".", "listdir", "(", "\"experiments\"", ")", ":", "\n", "        ", "params", "=", "os", ".", "path", ".", "join", "(", "\"experiments\"", ",", "foldername", ",", "\"params.toml\"", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "params", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "try", ":", "\n", "            ", "HyperParameters", "(", "params", ")", "\n", "", "except", "KeyError", "as", "err", ":", "\n", "            ", "print", "(", "params", ",", "\"is missing\"", ",", "str", "(", "err", ")", ")", "\n", "count", "+=", "1", "\n", "", "", "if", "count", "==", "0", ":", "\n", "        ", "print", "(", "\"All param files are good! :)\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"{count} param file(s) missing keys.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_top_n": [[4, 7], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["def", "test_top_n", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "2", ",", "3", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "top_n", "(", "3", ")", "==", "{", "1", ",", "2", ",", "3", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_1": [[9, 13], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "test_jaccard_1", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "2", ",", "3", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", "3", ")", "==", "1.0", "\n", "assert", "p", ".", "target", "==", "p", ".", "top_n", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_2": [[15, 19], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "test_jaccard_2", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "2", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", "2", ")", "==", "1.0", "\n", "assert", "p", ".", "target", "==", "p", ".", "top_n", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_3": [[21, 25], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "test_jaccard_3", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "2", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", ")", "==", "1.0", "\n", "assert", "p", ".", "target", "==", "p", ".", "top_n", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_4": [[27, 31], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "test_jaccard_4", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "2", ",", "3", ",", "4", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", ")", "==", "1.0", "\n", "assert", "p", ".", "target", "==", "p", ".", "top_n", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_5": [[33, 37], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity", "paper.interpret.run.Prediction.top_n"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity", "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.top_n"], ["", "def", "test_jaccard_5", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", ")", "==", "1.0", "\n", "assert", "p", ".", "target", "==", "p", ".", "top_n", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_6": [[39, 42], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity"], ["", "def", "test_jaccard_6", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "3", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", ")", "==", "1", "/", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.tests.test_interpret_prediction.test_jaccard_7": [[44, 47], ["paper.interpret.run.Prediction", "paper.interpret.run.Prediction.jaccard_similarity"], "function", ["home.repos.pwc.inspect_result.samuelstevens_sentence-editing-interpretability.interpret.run.Prediction.jaccard_similarity"], ["", "def", "test_jaccard_7", "(", ")", "->", "None", ":", "\n", "    ", "p", "=", "Prediction", "(", "\"1.0\"", ",", "{", "1", ",", "3", ",", "5", "}", ",", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ")", "\n", "assert", "p", ".", "jaccard_similarity", "(", ")", "==", "0.5", "\n", "", ""]]}