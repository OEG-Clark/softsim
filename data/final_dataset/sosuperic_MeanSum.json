{"home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.__init__": [[52, 56], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hp", ",", "opt", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "hp", "=", "hp", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "save_dir", "=", "save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.unpack_sum_model_output": [[57, 79], ["zip", "collections.defaultdict", "stats_gpu.items", "collections.defaultdict.items"], "methods", ["None"], ["", "def", "unpack_sum_model_output", "(", "self", ",", "output", ")", ":", "\n", "        ", "\"\"\"\n        SummmarizationModel is wrapped in a DataParallelModel (nn.DataParallel without final gather step).\n        Depending on the number of GPUs being used, we may have to zip and combine the outputs.\n        When there are multiple GPUs, the outputs are only cleanly combined along the batch dimension\n        if the outputs are tensors.\n\n        Returns:\n            stats: dict (str to Tensor)\n            summ_texts: list of strs\n        \"\"\"", "\n", "if", "self", ".", "ngpus", "==", "1", ":", "\n", "            ", "stats", ",", "summ_texts", "=", "output", "\n", "", "else", ":", "\n", "            ", "stats_list", ",", "summ_texts_nested", "=", "zip", "(", "*", "output", ")", "# list of dicts; list of lists", "\n", "stats", "=", "defaultdict", "(", "int", ")", "\n", "for", "stats_gpu", "in", "stats_list", ":", "\n", "                ", "for", "k", ",", "v", "in", "stats_gpu", ".", "items", "(", ")", ":", "\n", "                    ", "stats", "[", "k", "]", "+=", "v", "\n", "", "", "stats", "=", "{", "k", ":", "v", "/", "self", ".", "ngpus", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", "}", "# mean over gpus (e.g. mean of means)", "\n", "summ_texts", "=", "[", "text", "for", "gpu_texts", "in", "summ_texts_nested", "for", "text", "in", "gpu_texts", "]", "# flatten", "\n", "", "return", "stats", ",", "summ_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.update_dict": [[80, 87], ["updates.items"], "methods", ["None"], ["", "def", "update_dict", "(", "self", ",", "orig", ",", "updates", ")", ":", "\n", "        ", "\"\"\"\n        Helper function to update / overwrite the orig dict\n        \"\"\"", "\n", "for", "k", ",", "v", "in", "updates", ".", "items", "(", ")", ":", "\n", "            ", "orig", "[", "k", "]", "=", "v", "\n", "", "return", "orig", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.prepare_individual_revs": [[88, 109], ["len", "docs_ids.view.view.view", "data_loaders.summ_dataset.SummDataset.split_docs", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "train_sum.Summarizer.dataset.prepare_batch", "train_sum.Summarizer.dataset.prepare_batch", "docs_ids.view.view.size", "range", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch"], ["", "def", "prepare_individual_revs", "(", "self", ",", "texts", ",", "append_edoc", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Split concatenated reviews into individual reviews, tokenize, and create tensor\n\n        Args:\n            texts: list of strs, each str is n_docs concatenated together with EDOC_TOK delimiter\n\n        Returns: [batch, n_docs, max_len (across all reviews)]\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "texts", ")", "\n", "docs_ids", "=", "[", "SummDataset", ".", "split_docs", "(", "text", ")", "for", "text", "in", "texts", "]", "# list of lists of strs", "\n", "docs_ids", "=", "[", "rev", "for", "batch_item", "in", "docs_ids", "for", "rev", "in", "batch_item", "]", "# flatten", "\n", "dummy_ratings", "=", "[", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", "for", "_", "in", "range", "(", "len", "(", "docs_ids", ")", ")", "]", "\n", "# We do this so that max_len is across all reviews", "\n", "if", "append_edoc", ":", "\n", "# Can use global_append_id because docs_ids is a flat [batch * n_docs]", "\n", "            ", "docs_ids", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "docs_ids", ",", "dummy_ratings", ",", "global_append_id", "=", "EDOC_ID", ")", "\n", "", "else", ":", "\n", "            ", "docs_ids", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "docs_ids", ",", "dummy_ratings", ")", "# [batch * n_docs, max_len]", "\n", "", "docs_ids", "=", "docs_ids", ".", "view", "(", "batch_size", ",", "-", "1", ",", "docs_ids", ".", "size", "(", "1", ")", ")", "# [batch, n_docs, max_len]", "\n", "return", "docs_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.run_epoch": [[110, 429], ["float", "collections.defaultdict", "evaluation.eval_utils.EvalMetrics", "enumerate", "time.time", "range", "time.time", "train_sum.Summarizer.dataset.prepare_batch", "models.nn_utils.calc_lm_nll", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "torch.tensor().sum", "train_sum.Summarizer.items", "sum_optimizer.optimizer.zero_grad", "discrim_optimizer.optimizer.zero_grad", "clf_optimizer.optimizer.zero_grad", "train_sum.Summarizer.dataset.prepare_batch", "train_sum.Summarizer.prepare_individual_revs", "models.nn_utils.move_to_cuda", "train_sum.Summarizer.dataset.prepare_batch", "isinstance", "train_sum.Summarizer.dataset.prepare_batch", "train_sum.Summarizer.sum_model", "train_sum.Summarizer.unpack_sum_model_output", "train_sum.Summarizer.update_dict", "train_sum.Summarizer.sum_model", "train_sum.Summarizer.unpack_sum_model_output", "train_sum.Summarizer.update_dict", "train_sum.Summarizer.sum_model", "train_sum.Summarizer.unpack_sum_model_output", "train_sum.Summarizer.update_dict", "train_sum.Summarizer.tau.step", "stats[].backward", "models.nn_utils.calc_grad_norm", "clf_optimizer.step", "models.nn_utils.calc_grad_norm", "sum_optimizer.step", "len", "clean_summs.append", "time.time", "utils.update_moving_avg", "evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts().items", "print", "print", "print", "print", "print", "print", "print", "train_sum.Summarizer.prepare_individual_revs", "train_sum.Summarizer.extract_sum.summarize", "extract_summs.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "stats[].backward", "models.nn_utils.calc_grad_norm", "discrim_optimizer.step", "stats[].backward", "stats[].backward", "stats[].backward", "stats[].backward", "stats[].backward", "summ.replace.replace.replace", "summaries.append", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "v.item", "evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "time.time", "batch_rouge_strs.append", "epoch_rouge_strs.append", "texts[].encode", "summ_texts[].encode", "train_sum.Summarizer.items", "tb_writer.add_scalar", "tb_writer.add_text", "tb_writer.add_text", "tb_writer.add_scalar", "numpy.mean", "tb_writer.add_scalar", "print", "models.nn_utils.save_models", "time.time", "tb_writer.add_scalar", "text.replace", "range", "data_loaders.summ_dataset.SummDataset.split_docs", "data_loaders.summ_dataset.SummDataset.split_docs", "print", "evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts", "tb_writer.add_scalar", "tb_writer.add_scalar", "rouges.items", "tb_writer.add_scalar", "time.time", "utils.sync_run_data_to_bigstore", "tb_writer.add_scalar", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "train_sum.Summarizer.run_epoch", "len", "tb_writer.add_scalar", "train_sum.Summarizer.items", "collections.defaultdict.items", "evaluation.eval_utils.EvalMetrics.to_str", "evaluation.eval_utils.EvalMetrics.to_str", "d.items", "time.time", "train_sum.Summarizer.summ_dec.context_alpha.item", "len", "int", "train_sum.Summarizer.val_subset_iter.__len__", "time.time", "train_sum.Summarizer.items", "tb_writer.add_scalar", "train_sum.Summarizer.dataset.subwordenc.encode", "time.time"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_lm_nll", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.prepare_individual_revs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.unpack_sum_model_output", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.update_dict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.unpack_sum_model_output", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.update_dict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.unpack_sum_model_output", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.update_dict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.prepare_individual_revs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_models", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.sync_run_data_to_bigstore", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_str", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_str", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "run_epoch", "(", "self", ",", "data_iter", ",", "nbatches", ",", "epoch", ",", "split", ",", "\n", "sum_optimizer", "=", "None", ",", "discrim_optimizer", "=", "None", ",", "clf_optimizer", "=", "None", ",", "\n", "cpkt_every", "=", "float", "(", "'inf'", ")", ",", "save_intermediate", "=", "True", ",", "run_val_subset", "=", "False", ",", "\n", "store_all_rouges", "=", "False", ",", "store_all_summaries", "=", "False", ",", "\n", "tb_writer", "=", "None", ",", "tb_start_step", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Iterate through data in data_iter\n\n        Args:\n            data_iter: iterable providing minibatches\n            nbatches: int (number of batches in data_iter)\n                - could be less than then number of batches in the iter (e.g. when hp.train_subset is True)\n            epoch: int\n            split: str ('train', 'val')\n            *_optimizer: Wrapped optim (e.g. OptWrapper)\n                Passed during training split, not passed for validation\n\n            cpkt_every: int (save a checkpoint and run on subset of validation set depending on subsequent two flags)\n            save_intermediate: bool (save checkpoints every cpokt_every minibatches)\n            run_val_subset: bool (run model on subset of validation set every cpokt_Every minibatches)\n\n            store_all_rouges: boolean (store all rouges in addition to taking the average\n                so we can plot the distribution)\n            store_all_summaries: boolean (return all summaries)\n\n            tb_writer: Tensorboard SummaryWriter\n            tb_start_step: int\n                - Starting step. Used when running on subset of validation set. This way the results\n                can appear on the same x-axis timesteps as the training.\n\n        Returns:\n            dict of str, floats containing losses and stats\n            dict of rouge scores\n            list of summaries\n        \"\"\"", "\n", "stats_avgs", "=", "defaultdict", "(", "int", ")", "\n", "evaluator", "=", "EvalMetrics", "(", "remove_stopwords", "=", "self", ".", "hp", ".", "remove_stopwords", ",", "\n", "use_stemmer", "=", "self", ".", "hp", ".", "use_stemmer", ",", "\n", "store_all", "=", "store_all_rouges", ")", "\n", "summaries", "=", "[", "]", "# this is only added to if store_all_summaries is True", "\n", "for", "s", ",", "(", "texts", ",", "ratings", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "# texts: list of strs, each str is n_docs concatenated together with EDOC_TOK delimiter", "\n", "            ", "if", "s", ">", "nbatches", ":", "\n", "                ", "break", "\n", "\n", "", "stats", "=", "{", "}", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "sum_optimizer", ":", "\n", "                ", "sum_optimizer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "if", "discrim_optimizer", ":", "\n", "                ", "discrim_optimizer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "if", "clf_optimizer", ":", "\n", "                ", "clf_optimizer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Get data", "\n", "", "cycle_tgt_ids", "=", "None", "\n", "if", "self", ".", "hp", ".", "concat_docs", ":", "\n", "                ", "docs_ids", ",", "_", ",", "labels", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "texts", ",", "ratings", ",", "doc_append_id", "=", "EDOC_ID", ")", "\n", "# docs_ids: [batch_size, max_len]", "\n", "if", "self", ".", "sum_cycle", "and", "(", "self", ".", "cycle_loss", "==", "'rec'", ")", ":", "\n", "                    ", "cycle_tgt_ids", "=", "self", ".", "prepare_individual_revs", "(", "texts", ")", "\n", "", "", "else", ":", "\n", "                ", "docs_ids", "=", "self", ".", "prepare_individual_revs", "(", "texts", ",", "append_edoc", "=", "True", ")", "\n", "cycle_tgt_ids", "=", "docs_ids", "\n", "labels", "=", "move_to_cuda", "(", "ratings", "-", "1", ")", "\n", "\n", "", "extract_summ_ids", "=", "None", "\n", "if", "self", ".", "hp", ".", "extract_loss", ":", "\n", "                ", "extract_summs", "=", "[", "]", "\n", "for", "text", "in", "texts", ":", "\n", "                    ", "summary", "=", "self", ".", "extract_sum", ".", "summarize", "(", "text", ".", "replace", "(", "EDOC_TOK", ",", "''", ")", ",", "\n", "limit", "=", "self", ".", "hp", ".", "yelp_extractive_max_len", ")", "\n", "extract_summs", ".", "append", "(", "summary", ")", "\n", "", "dummy_ratings", "=", "[", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", "for", "_", "in", "range", "(", "len", "(", "extract_summs", ")", ")", "]", "\n", "extract_summ_ids", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "extract_summs", ",", "dummy_ratings", ")", "\n", "\n", "", "cur_tau", "=", "self", ".", "tau", "if", "isinstance", "(", "self", ".", "tau", ",", "float", ")", "else", "self", ".", "tau", ".", "val", "\n", "\n", "# Step for tensorboard: global steps in terms of number of reviews", "\n", "# This accounts for runs with different batch sizes and n_docs", "\n", "step", "=", "tb_start_step", "\n", "# We do the following so that if run_epoch is iterating over the validation subset,", "\n", "# the step is right around when run_epoch(self.val_subset_iter) was called. If we did step +=", "\n", "# (epoch * nbatches ...) for the validation subset and the cpkt_every was small, then the next", "\n", "# time run_epoch(self.val_subset_iter) was called might have a tb_start_step that was smaller", "\n", "# than the last step used for self.tb_val_sub_writer. This would make the Tensorboard line chart", "\n", "# loop back on itself.", "\n", "if", "tb_writer", "==", "self", ".", "tb_val_sub_writer", ":", "\n", "                ", "step", "+=", "s", "\n", "", "else", ":", "\n", "                ", "step", "+=", "(", "epoch", "*", "nbatches", "*", "self", ".", "hp", ".", "batch_size", "*", "self", ".", "hp", ".", "n_docs", ")", "+", "s", "*", "self", ".", "hp", ".", "batch_size", "*", "self", ".", "hp", ".", "n_docs", "\n", "\n", "# Adversarial", "\n", "", "discrim_gn", "=", "-", "1.0", "\n", "if", "self", ".", "hp", ".", "sum_discrim", ":", "\n", "# Get batch of real reviews (but not for the original reviews) by rotating original batch", "\n", "# Note: these don't have any special tokens", "\n", "                ", "texts_rotated", "=", "[", "SummDataset", ".", "split_docs", "(", "text", ")", "[", "0", "]", "for", "text", "in", "texts", "]", "# first review", "\n", "texts_rotated", "=", "texts_rotated", "[", "1", ":", "]", "+", "[", "texts_rotated", "[", "0", "]", "]", "# rotate", "\n", "docs_ids_rot", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "texts_rotated", ",", "ratings", ")", "# [batch, max_len]", "\n", "\n", "# Train discriminator", "\n", "output", "=", "self", ".", "sum_model", "(", "docs_ids", ",", "labels", ",", "\n", "cycle_tgt_ids", "=", "cycle_tgt_ids", ",", "\n", "extract_summ_ids", "=", "extract_summ_ids", ",", "\n", "tau", "=", "cur_tau", ",", "\n", "adv_step", "=", "'discrim'", ",", "real_ids", "=", "docs_ids_rot", ",", "\n", "minibatch_idx", "=", "s", ",", "print_every_nbatches", "=", "self", ".", "opt", ".", "print_every_nbatches", ",", "\n", "tb_writer", "=", "tb_writer", ",", "tb_step", "=", "step", ",", "\n", "wass_loss", "=", "stats_avgs", "[", "'wass_loss'", "]", ",", "\n", "grad_pen_loss", "=", "stats_avgs", "[", "'grad_pen_loss'", "]", ",", "\n", "adv_gen_loss", "=", "stats_avgs", "[", "'adv_gen_loss'", "]", ",", "\n", "clf_loss", "=", "stats_avgs", "[", "'clf_loss'", "]", ",", "\n", "clf_acc", "=", "stats_avgs", "[", "'clf_acc'", "]", ",", "\n", "clf_avg_diff", "=", "stats_avgs", "[", "'clf_avg_diff'", "]", ")", "\n", "fwd_stats", ",", "summ_texts", "=", "self", ".", "unpack_sum_model_output", "(", "output", ")", "\n", "stats", "=", "self", ".", "update_dict", "(", "stats", ",", "fwd_stats", ")", "\n", "if", "discrim_optimizer", ":", "\n", "                    ", "(", "stats", "[", "'adv_loss'", "]", ")", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "discrim_gn", "=", "calc_grad_norm", "(", "self", ".", "discrim_model", ")", "\n", "discrim_optimizer", ".", "step", "(", ")", "\n", "\n", "# Train generator", "\n", "", "output", "=", "self", ".", "sum_model", "(", "docs_ids", ",", "labels", ",", "\n", "cycle_tgt_ids", "=", "cycle_tgt_ids", ",", "\n", "extract_summ_ids", "=", "extract_summ_ids", ",", "\n", "tau", "=", "cur_tau", ",", "\n", "adv_step", "=", "'gen'", ",", "\n", "minibatch_idx", "=", "s", ",", "print_every_nbatches", "=", "self", ".", "opt", ".", "print_every_nbatches", ",", "\n", "tb_writer", "=", "tb_writer", ",", "tb_step", "=", "step", ",", "\n", "wass_loss", "=", "stats_avgs", "[", "'wass_loss'", "]", ",", "\n", "grad_pen_loss", "=", "stats_avgs", "[", "'grad_pen_loss'", "]", ",", "\n", "adv_gen_loss", "=", "stats_avgs", "[", "'adv_gen_loss'", "]", ",", "\n", "clf_loss", "=", "stats_avgs", "[", "'clf_loss'", "]", ",", "\n", "clf_acc", "=", "stats_avgs", "[", "'clf_acc'", "]", ",", "\n", "clf_avg_diff", "=", "stats_avgs", "[", "'clf_avg_diff'", "]", ")", "\n", "stats", ",", "summ_texts", "=", "self", ".", "unpack_sum_model_output", "(", "output", ")", "\n", "stats", "=", "self", ".", "update_dict", "(", "stats", ",", "fwd_stats", ")", "\n", "if", "sum_optimizer", ":", "\n", "                    ", "retain_graph", "=", "(", "clf_optimizer", "is", "not", "None", ")", "or", "(", "sum_optimizer", "is", "not", "None", ")", "\n", "(", "stats", "[", "'adv_gen_loss'", "]", ")", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "", "", "else", ":", "\n", "                ", "output", "=", "self", ".", "sum_model", "(", "docs_ids", ",", "labels", ",", "\n", "cycle_tgt_ids", "=", "cycle_tgt_ids", ",", "\n", "extract_summ_ids", "=", "extract_summ_ids", ",", "\n", "tau", "=", "cur_tau", ",", "\n", "adv_step", "=", "None", ",", "\n", "minibatch_idx", "=", "s", ",", "print_every_nbatches", "=", "self", ".", "opt", ".", "print_every_nbatches", ",", "\n", "tb_writer", "=", "tb_writer", ",", "tb_step", "=", "step", ",", "\n", "wass_loss", "=", "stats_avgs", "[", "'wass_loss'", "]", ",", "\n", "grad_pen_loss", "=", "stats_avgs", "[", "'grad_pen_loss'", "]", ",", "\n", "adv_gen_loss", "=", "stats_avgs", "[", "'adv_gen_loss'", "]", ",", "\n", "clf_loss", "=", "stats_avgs", "[", "'clf_loss'", "]", ",", "\n", "clf_acc", "=", "stats_avgs", "[", "'clf_acc'", "]", ",", "\n", "clf_avg_diff", "=", "stats_avgs", "[", "'clf_avg_diff'", "]", ")", "\n", "fwd_stats", ",", "summ_texts", "=", "self", ".", "unpack_sum_model_output", "(", "output", ")", "\n", "stats", "=", "self", ".", "update_dict", "(", "stats", ",", "fwd_stats", ")", "\n", "\n", "", "if", "self", ".", "hp", ".", "decay_tau", ":", "\n", "                ", "self", ".", "tau", ".", "step", "(", ")", "\n", "\n", "# Classifier loss", "\n", "", "clf_gn", "=", "-", "1.0", "\n", "if", "clf_optimizer", ":", "\n", "                ", "retain_graph", "=", "sum_optimizer", "is", "not", "None", "\n", "stats", "[", "'clf_loss'", "]", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "clf_gn", "=", "calc_grad_norm", "(", "self", ".", "clf_model", ")", "\n", "clf_optimizer", ".", "step", "(", ")", "\n", "\n", "# Cycle loss", "\n", "", "sum_gn", "=", "-", "1.0", "\n", "if", "sum_optimizer", ":", "\n", "                ", "if", "self", ".", "hp", ".", "autoenc_docs", "and", "(", "not", "self", ".", "hp", ".", "load_ae_freeze", ")", ":", "# don't backward() if loaded pretrained autoenc (it's frozen)", "\n", "                    ", "retain_graph", "=", "self", ".", "hp", ".", "early_cycle", "or", "self", ".", "hp", ".", "sum_cycle", "or", "self", ".", "hp", ".", "extract_loss", "\n", "stats", "[", "'autoenc_loss'", "]", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "", "if", "self", ".", "hp", ".", "early_cycle", "and", "(", "not", "self", ".", "hp", ".", "autoenc_only", ")", ":", "\n", "                    ", "stats", "[", "'early_cycle_loss'", "]", ".", "backward", "(", ")", "\n", "", "if", "self", ".", "hp", ".", "sum_cycle", "and", "(", "not", "self", ".", "hp", ".", "autoenc_only", ")", ":", "\n", "                    ", "retain_graph", "=", "self", ".", "hp", ".", "extract_loss", "\n", "stats", "[", "'cycle_loss'", "]", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "", "if", "self", ".", "hp", ".", "extract_loss", "and", "(", "not", "self", ".", "hp", ".", "autoenc_only", ")", ":", "\n", "                    ", "retain_graph", "=", "clf_optimizer", "is", "not", "None", "\n", "stats", "[", "'extract_loss'", "]", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "", "sum_gn", "=", "calc_grad_norm", "(", "self", ".", "docs_enc", ")", "\n", "sum_optimizer", ".", "step", "(", ")", "\n", "\n", "# Gather summaries so we can calculate rouge", "\n", "", "clean_summs", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "summ_texts", ")", ")", ":", "\n", "                ", "summ", "=", "summ_texts", "[", "idx", "]", "\n", "for", "tok", "in", "RESERVED_TOKENS", ":", "# should just be <pad> I think", "\n", "                    ", "summ", "=", "summ", ".", "replace", "(", "tok", ",", "''", ")", "\n", "", "clean_summs", ".", "append", "(", "summ", ")", "\n", "if", "store_all_summaries", ":", "\n", "                    ", "summaries", ".", "append", "(", "summ", ")", "\n", "\n", "# Calculate log likelihood of summaries using fixed language model (the one that was used to", "\n", "# initialize the models)", "\n", "", "", "ppl_time", "=", "time", ".", "time", "(", ")", "\n", "summs_x", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "clean_summs", ",", "ratings", ")", "\n", "nll", "=", "calc_lm_nll", "(", "self", ".", "fixed_lm", ",", "summs_x", ")", "\n", "ppl_time", "=", "time", ".", "time", "(", ")", "-", "ppl_time", "\n", "stats", "[", "'nll'", "]", "=", "nll", "\n", "\n", "\n", "#", "\n", "# Stats, print, etc.", "\n", "#", "\n", "stats", "[", "'total_loss'", "]", "=", "torch", ".", "tensor", "(", "[", "v", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", "if", "'loss'", "in", "k", "]", ")", ".", "sum", "(", ")", "\n", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "                ", "stats_avgs", "[", "k", "]", "=", "update_moving_avg", "(", "stats_avgs", "[", "k", "]", ",", "v", ".", "item", "(", ")", ",", "s", "+", "1", ")", "\n", "\n", "", "if", "s", "%", "self", ".", "opt", ".", "print_every_nbatches", "==", "0", ":", "\n", "# Calculate rouge", "\n", "                ", "try", ":", "\n", "                    ", "src_docs", "=", "[", "SummDataset", ".", "split_docs", "(", "concatenated", ")", "for", "concatenated", "in", "texts", "]", "\n", "avg_rouges", ",", "min_rouges", ",", "max_rouges", ",", "std_rouges", "=", "evaluator", ".", "batch_update_avg_rouge", "(", "clean_summs", ",", "src_docs", ")", "\n", "", "except", "Exception", "as", "e", ":", "# IndexError in computing (see commit for stack trace)", "\n", "# This started occurring when I switched to Google's Rouge script", "\n", "# It's happened after many minibatches (e.g. half way through the first epoch)", "\n", "# I'm not sure if this is because the summary has degenerated into something that", "\n", "# throws an error, or just that it's a rare edge case with the data.", "\n", "# For now, print and log to tensorboard and see when and how often this occurs.", "\n", "# batch_avg_rouges = evaluator.avg_rouges.", "\n", "# Note: after some experiments, this only occurred twice in 4 epochs.", "\n", "                    ", "avg_rouges", ",", "min_rouges", ",", "max_rouges", ",", "std_rouges", "=", "evaluator", ".", "avg_avg_rouges", ",", "evaluator", ".", "avg_min_rouges", ",", "evaluator", ".", "avg_max_rouges", ",", "evaluator", ".", "avg_std_rouges", "\n", "print", "(", "'Error in calculating rouge'", ")", "\n", "if", "tb_writer", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'other/rouge_error'", ",", "1", ",", "step", ")", "\n", "\n", "# Construct print statements", "\n", "", "", "mb_time", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "main_str", "=", "'Epoch={}, batch={}/{}, split={}, time={:.4f}, tau={:.4f}'", ".", "format", "(", "\n", "epoch", ",", "s", ",", "nbatches", ",", "split", ",", "mb_time", ",", "cur_tau", ")", "\n", "stats_str", "=", "', '", ".", "join", "(", "[", "'{}={:.4f}'", ".", "format", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", "]", ")", "\n", "stats_avgs_str", "=", "', '", ".", "join", "(", "[", "'{}_curavg={:.4f}'", ".", "format", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "stats_avgs", ".", "items", "(", ")", "]", ")", "\n", "gn_str", "=", "'sum_gn={:.2f}, discrim_gn={:.2f}, clf_gn={:.2f}'", ".", "format", "(", "sum_gn", ",", "discrim_gn", ",", "clf_gn", ")", "\n", "\n", "batch_rouge_strs", "=", "[", "]", "\n", "for", "stat", ",", "rouges", "in", "{", "'avg'", ":", "avg_rouges", ",", "'min'", ":", "min_rouges", ",", "\n", "'max'", ":", "max_rouges", ",", "'std'", ":", "std_rouges", "}", ".", "items", "(", ")", ":", "\n", "                    ", "batch_rouge_strs", ".", "append", "(", "'batch avg {} rouges: '", ".", "format", "(", "stat", ")", "+", "evaluator", ".", "to_str", "(", "rouges", ")", ")", "\n", "", "epoch_rouge_strs", "=", "[", "]", "\n", "for", "stat", ",", "rouges", "in", "evaluator", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "                    ", "epoch_rouge_strs", ".", "append", "(", "'epoch avg {} rouges: '", ".", "format", "(", "stat", ")", "+", "evaluator", ".", "to_str", "(", "rouges", ")", ")", "\n", "\n", "", "print_str", "=", "' --- '", ".", "join", "(", "[", "main_str", ",", "stats_str", ",", "stats_avgs_str", ",", "gn_str", "]", "+", "\n", "batch_rouge_strs", "+", "epoch_rouge_strs", ")", "\n", "print", "(", "print_str", ")", "\n", "\n", "# Example summary to get qualitative sense", "\n", "print", "(", "'\\n'", ",", "'-'", "*", "100", ")", "\n", "print", "(", "'ORIGINAL REVIEWS: '", ",", "texts", "[", "0", "]", ".", "encode", "(", "'utf8'", ")", ")", "\n", "print", "(", "'-'", "*", "100", ")", "\n", "print", "(", "'SUMMARY: '", ",", "summ_texts", "[", "0", "]", ".", "encode", "(", "'utf8'", ")", ")", "\n", "print", "(", "'-'", "*", "100", ",", "'\\n'", ")", "\n", "\n", "\n", "print", "(", "'\\n'", ",", "'#'", "*", "100", ",", "'\\n'", ")", "\n", "\n", "# Write to tensorboard", "\n", "if", "tb_writer", ":", "\n", "                    ", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'stats/{}'", ".", "format", "(", "k", ")", ",", "v", ",", "step", ")", "\n", "", "for", "k", ",", "v", "in", "{", "'sum_gn'", ":", "sum_gn", ",", "'discrim_gn'", ":", "discrim_gn", ",", "'clf_gn'", ":", "clf_gn", "}", ".", "items", "(", ")", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'grad_norm/{}'", ".", "format", "(", "k", ")", ",", "v", ",", "step", ")", "\n", "\n", "", "for", "stat", ",", "rouges", "in", "{", "'avg'", ":", "avg_rouges", ",", "'min'", ":", "min_rouges", ",", "\n", "'max'", ":", "max_rouges", ",", "'std'", ":", "std_rouges", "}", ".", "items", "(", ")", ":", "\n", "                        ", "for", "rouge_name", ",", "d", "in", "rouges", ".", "items", "(", ")", ":", "\n", "                            ", "for", "metric_name", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "                                ", "tb_writer", ".", "add_scalar", "(", "'rouges_{}/{}/{}'", ".", "format", "(", "stat", ",", "rouge_name", ",", "metric_name", ")", ",", "v", ",", "step", ")", "\n", "\n", "", "", "", "tb_writer", ".", "add_scalar", "(", "'stats/sec_per_nll_calc'", ",", "time", ".", "time", "(", ")", "-", "ppl_time", ",", "step", ")", "\n", "\n", "tb_writer", ".", "add_text", "(", "'summary/orig_reviews'", ",", "texts", "[", "0", "]", ",", "step", ")", "\n", "tb_writer", ".", "add_text", "(", "'summary/summary'", ",", "summ_texts", "[", "0", "]", ",", "step", ")", "\n", "\n", "tb_writer", ".", "add_scalar", "(", "'stats/sec_per_batch'", ",", "mb_time", ",", "step", ")", "\n", "\n", "if", "self", ".", "hp", ".", "docs_attn", ":", "# scalar may be learnable depending on flag", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'stats/context_alpha'", ",", "self", ".", "summ_dec", ".", "context_alpha", ".", "item", "(", ")", ",", "step", ")", "\n", "\n", "", "mean_summ_len", "=", "np", ".", "mean", "(", "[", "len", "(", "self", ".", "dataset", ".", "subwordenc", ".", "encode", "(", "summ", ")", ")", "for", "summ", "in", "clean_summs", "]", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'stats/mean_summ_len'", ",", "mean_summ_len", ",", "step", ")", "\n", "\n", "if", "(", "not", "self", ".", "hp", ".", "debug", ")", "and", "(", "not", "self", ".", "opt", ".", "no_bigstore", ")", ":", "\n", "                        ", "sync_time", "=", "time", ".", "time", "(", ")", "\n", "sync_run_data_to_bigstore", "(", "self", ".", "save_dir", ",", "exp_sub_dir", "=", "self", ".", "opt", ".", "bs_dir", ",", "\n", "method", "=", "'rsync'", ",", "tb_only", "=", "True", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'stats/sec_per_bigstore_sync'", ",", "time", ".", "time", "(", ")", "-", "sync_time", ",", "step", ")", "\n", "\n", "# Periodic checkpointing", "\n", "", "", "", "if", "s", "%", "cpkt_every", "==", "0", ":", "\n", "                ", "if", "save_intermediate", ":", "\n", "                    ", "print", "(", "'Intermdediate checkpoint during training epoch'", ")", "\n", "save_model", "=", "self", ".", "sum_model", ".", "module", "if", "self", ".", "ngpus", ">", "1", "else", "self", ".", "sum_model", "\n", "save_models", "(", "self", ".", "save_dir", ",", "{", "'sum_model'", ":", "save_model", ",", "'tau'", ":", "self", ".", "tau", "}", ",", "\n", "self", ".", "optimizers", ",", "epoch", ",", "self", ".", "opt", ",", "\n", "'sub{}'", ".", "format", "(", "int", "(", "s", "/", "cpkt_every", ")", ")", ")", "\n", "\n", "", "if", "(", "s", ">", "0", ")", "and", "run_val_subset", ":", "\n", "                    ", "start", "=", "time", ".", "time", "(", ")", "\n", "start_step", "=", "(", "epoch", "*", "nbatches", "*", "self", ".", "hp", ".", "batch_size", "*", "self", ".", "hp", ".", "n_docs", ")", "+", "s", "*", "self", ".", "hp", ".", "batch_size", "*", "self", ".", "hp", ".", "n_docs", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "self", ".", "run_epoch", "(", "self", ".", "val_subset_iter", ",", "self", ".", "val_subset_iter", ".", "__len__", "(", ")", ",", "epoch", ",", "'val_subset'", ",", "\n", "save_intermediate", "=", "False", ",", "run_val_subset", "=", "False", ",", "\n", "tb_writer", "=", "self", ".", "tb_val_sub_writer", ",", "tb_start_step", "=", "start_step", ")", "\n", "", "tb_writer", ".", "add_scalar", "(", "'stats/sec_per_val_subset'", ",", "time", ".", "time", "(", ")", "-", "start", ",", "start_step", ")", "\n", "\n", "", "", "", "return", "stats_avgs", ",", "evaluator", ",", "summaries", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.train": [[430, 757], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get", "train_sum.Summarizer.dataset.get_data_loader", "train_sum.Summarizer.dataset.get_data_loader", "train_sum.Summarizer.dataset.get_data_loader", "os.path.join", "print", "os.mkdir", "os.mkdir", "os.mkdir", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "models.mlstm.StackedLSTMDecoder", "models.summarization.SummarizationModel", "models.nn_utils.OptWrapper", "train_sum.Summarizer.sum_model.parameters", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "range", "models.nn_utils.StepAnnealer", "len", "len", "print", "print", "torch.Embedding", "torch.Embedding", "torch.Embedding", "models.mlstm.StackedLSTM", "models.mlstm.StackedLSTMEncoder", "models.nn_utils.freeze", "torch.Sequential", "torch.Sequential", "torch.Sequential", "copy.deepcopy", "copy.deepcopy", "models.mlstm.StackedLSTMDecoder", "models.mlstm.StackedLSTMEncoder", "models.mlstm.StackedLSTMEncoder", "models.mlstm.StackedLSTMDecoder", "models.mlstm.StackedLSTMDecoder", "models.nn_utils.freeze", "models.nn_utils.freeze", "models.nn_utils.freeze", "models.nn_utils.freeze", "len", "Discriminator", "models.nn_utils.OptWrapper", "torch.Adam", "torch.Adam", "torch.Adam", "CentroidW2VSummarizer", "train_sum.Summarizer.sum_model.cuda", "models.custom_parallel.DataParallelModel", "train_sum.Summarizer.sum_model.eval", "models.nn_utils.save_models", "len", "train_sum.Summarizer.opt.gpus.split", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "len", "copy.deepcopy", "collections.OrderedDict", "copy.deepcopy", "torch.GRU", "torch.GRU", "torch.GRU", "models.mlstm.StackedLSTMDecoder", "models.mlstm.StackedLSTMDecoder", "models.mlstm.StackedLSTMEncoder", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "models.mlstm.StackedLSTMDecoder", "print", "models.nn_utils.OptWrapper", "print", "torch.Adam", "torch.Adam", "torch.Adam", "len", "print", "print", "torch.Sequential", "torch.Sequential", "torch.Sequential", "models.nn_utils.OptWrapper", "models.nn_utils.freeze", "train_sum.Summarizer.sum_model.named_parameters", "sum", "sum", "train_sum.Summarizer.sum_model.train", "train_sum.Summarizer.__len__", "train_sum.Summarizer.run_epoch", "stats_avgs.items", "evaluator.get_avg_stats_dicts().items", "train_sum.Summarizer.run_epoch", "stats_avgs.items", "evaluator.get_avg_stats_dicts().items", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "models.mlstm.StackedLSTMEncoder", "models.mlstm.StackedLSTMEncoder", "models.nn_utils.freeze", "torch.Adam", "torch.Adam", "torch.Adam", "pretrain_classifier.TextClassifier", "train_sum.Summarizer.discrim_model.parameters", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "len", "collections.OrderedDict", "train_sum.Summarizer.clf_model.parameters", "torch.Adam", "torch.Adam", "torch.Adam", "train_sum.Summarizer.dataset.get_data_loader", "train_sum.Summarizer.tb_tr_writer.add_scalar", "rouges.items", "print", "train_sum.Summarizer.__len__", "train_sum.Summarizer.tb_val_writer.add_scalar", "rouges.items", "copy.deepcopy", "copy.deepcopy", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "pretrain_classifier.TextClassifier.parameters", "len", "p.nelement", "p.nelement", "int", "evaluator.get_avg_stats_dicts", "d.items", "evaluator.get_avg_stats_dicts", "d.items", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "train_sum.Summarizer.tb_tr_writer.add_scalar", "train_sum.Summarizer.tb_val_writer.add_scalar", "torch.Embedding", "torch.Embedding", "torch.Embedding", "models.text_cnn.BasicTextCNN", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_models", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.train", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main train loop\n        \"\"\"", "\n", "#", "\n", "# Get data, setup", "\n", "#", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "self", ".", "opt", ".", "dataset", ")", "\n", "train_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'train'", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "sample_reviews", "=", "True", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "val_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'val'", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "sample_reviews", "=", "False", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "val_subset_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'val'", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "sample_reviews", "=", "False", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ",", "\n", "subset", "=", "0.1", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "self", ".", "val_subset_iter", "=", "val_subset_iter", "\n", "\n", "self", ".", "tau", "=", "self", ".", "hp", ".", "tau", "\n", "if", "self", ".", "hp", ".", "decay_tau", ":", "\n", "            ", "self", ".", "tau", "=", "StepAnnealer", "(", "self", ".", "hp", ".", "tau", ",", "\n", "interval_size", "=", "self", ".", "hp", ".", "decay_interval_size", ",", "\n", "# intervals=intervals, intervals_vals=intervals_vals,", "\n", "alpha", "=", "self", ".", "hp", ".", "decay_tau_alpha", ",", "method", "=", "self", ".", "hp", ".", "decay_tau_method", ",", "\n", "min_val", "=", "self", ".", "hp", ".", "min_tau", ")", "\n", "\n", "", "tb_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "'tensorboard/'", ")", "\n", "print", "(", "'Tensorboard events will be logged to: {}'", ".", "format", "(", "tb_path", ")", ")", "\n", "os", ".", "mkdir", "(", "tb_path", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'train/'", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'val/'", ")", "\n", "self", ".", "tb_tr_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'train/'", ")", "\n", "self", ".", "tb_val_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'val/'", ")", "\n", "self", ".", "tb_val_sub_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'val_sub/'", ")", "\n", "#", "\n", "# Get models, optimizers, and loss functions", "\n", "#", "\n", "self", ".", "ngpus", "=", "1", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", "==", "1", "else", "len", "(", "self", ".", "opt", ".", "gpus", ".", "split", "(", "','", ")", ")", "\n", "self", ".", "models", "=", "{", "}", "# used for saving", "\n", "self", ".", "optimizers", "=", "{", "}", "# used for saving", "\n", "\n", "#", "\n", "# Summarization model", "\n", "#", "\n", "# Encoder-decoder for documents to summary", "\n", "self", ".", "fixed_lm", "=", "None", "\n", "if", "len", "(", "self", ".", "opt", ".", "load_lm", ")", ">", "1", ":", "\n", "            ", "print", "(", "'Loading pretrained language model from: {}'", ".", "format", "(", "self", ".", "opt", ".", "load_lm", ")", ")", "\n", "self", ".", "docs_enc", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "self", ".", "docs_enc", "=", "self", ".", "docs_enc", ".", "module", "if", "isinstance", "(", "self", ".", "docs_enc", ",", "nn", ".", "DataParallel", ")", "else", "self", ".", "docs_enc", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Training model from scratch'", ")", "\n", "embed", "=", "nn", ".", "Embedding", "(", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "self", ".", "hp", ".", "emb_size", ")", "\n", "lstm", "=", "StackedLSTM", "(", "mLSTM", ",", "\n", "self", ".", "hp", ".", "lstm_layers", ",", "self", ".", "hp", ".", "emb_size", ",", "self", ".", "hp", ".", "hidden_size", ",", "\n", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "\n", "self", ".", "hp", ".", "lstm_dropout", ",", "\n", "layer_norm", "=", "self", ".", "hp", ".", "lstm_ln", ")", "\n", "self", ".", "docs_enc", "=", "StackedLSTMEncoder", "(", "embed", ",", "lstm", ")", "\n", "\n", "", "if", "self", ".", "hp", ".", "track_ppl", ":", "\n", "            ", "if", "len", "(", "self", ".", "opt", ".", "load_lm", ")", ">", "1", ":", "\n", "                ", "self", ".", "fixed_lm", "=", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ")", "\n", "", "else", ":", "\n", "# didn't pass in pretrained language model as we're training from scratch", "\n", "# load it from the default", "\n", "                ", "self", ".", "fixed_lm", "=", "torch", ".", "load", "(", "self", ".", "dataset", ".", "conf", ".", "lm_path", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "self", ".", "fixed_lm", "=", "self", ".", "fixed_lm", ".", "module", "if", "isinstance", "(", "self", ".", "fixed_lm", ",", "nn", ".", "DataParallel", ")", "else", "self", ".", "fixed_lm", "\n", "\n", "", "freeze", "(", "self", ".", "fixed_lm", ")", "\n", "\n", "# Combining document representations", "\n", "", "self", ".", "combine_encs_h_net", "=", "None", "\n", "self", ".", "combine_encs_c_net", "=", "None", "\n", "if", "self", ".", "hp", ".", "combine_encs", "==", "'ff'", ":", "\n", "            ", "self", ".", "combine_encs_h_net", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'ln1'", ",", "nn", ".", "LayerNorm", "(", "self", ".", "hp", ".", "n_docs", "*", "self", ".", "hp", ".", "hidden_size", ")", ")", ",", "\n", "(", "'fc1'", ",", "nn", ".", "Linear", "(", "self", ".", "hp", ".", "n_docs", "*", "self", ".", "hp", ".", "hidden_size", ",", "self", ".", "hp", ".", "hidden_size", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", ")", ")", ",", "\n", "(", "'ln2'", ",", "nn", ".", "LayerNorm", "(", "self", ".", "hp", ".", "hidden_size", ")", ")", ",", "\n", "(", "'fc2'", ",", "nn", ".", "Linear", "(", "self", ".", "hp", ".", "hidden_size", ",", "self", ".", "hp", ".", "hidden_size", ")", ")", "\n", "]", ")", ")", "\n", "if", "self", ".", "hp", ".", "combine_tie_hc", ":", "\n", "                ", "self", ".", "combine_encs_c_net", "=", "self", ".", "combine_encs_h_net", "\n", "", "else", ":", "\n", "                ", "self", ".", "combine_encs_c_net", "=", "copy", ".", "deepcopy", "(", "self", ".", "combine_encs_h_net", ")", "\n", "", "", "elif", "self", ".", "hp", ".", "combine_encs", "==", "'gru'", ":", "\n", "            ", "self", ".", "combine_encs_h_net", "=", "nn", ".", "GRU", "(", "self", ".", "hp", ".", "hidden_size", ",", "self", ".", "hp", ".", "hidden_size", ",", "\n", "num_layers", "=", "self", ".", "hp", ".", "combine_encs_gru_nlayers", ",", "\n", "batch_first", "=", "True", ",", "\n", "dropout", "=", "self", ".", "hp", ".", "combine_encs_gru_dropout", ",", "\n", "bidirectional", "=", "self", ".", "hp", ".", "combine_encs_gru_bi", ")", "\n", "if", "self", ".", "hp", ".", "combine_tie_hc", ":", "\n", "                ", "self", ".", "combine_encs_c_net", "=", "self", ".", "combine_encs_h_net", "\n", "", "else", ":", "\n", "                ", "self", ".", "combine_encs_c_net", "=", "copy", ".", "deepcopy", "(", "self", ".", "combine_encs_h_net", ")", "\n", "\n", "# Decoder for generating summaries", "\n", "", "", "self", ".", "summ_dec", "=", "StackedLSTMDecoder", "(", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "embed", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "rnn", ")", ",", "\n", "use_docs_attn", "=", "self", ".", "hp", ".", "docs_attn", ",", "\n", "attn_emb_size", "=", "self", ".", "hp", ".", "hidden_size", ",", "\n", "attn_hidden_size", "=", "self", ".", "hp", ".", "docs_attn_hidden_size", ",", "\n", "attn_learn_alpha", "=", "self", ".", "hp", ".", "docs_attn_learn_alpha", ")", "\n", "\n", "# Autoencoder for documents", "\n", "self", ".", "docs_autodec", "=", "None", "\n", "if", "self", ".", "hp", ".", "autoenc_docs", ":", "\n", "            ", "if", "self", ".", "hp", ".", "autoenc_docs_tie_dec", ":", "\n", "                ", "self", ".", "docs_autodec", "=", "StackedLSTMDecoder", "(", "self", ".", "summ_dec", ".", "embed", ",", "self", ".", "summ_dec", ".", "rnn", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "docs_autodec", "=", "StackedLSTMDecoder", "(", "copy", ".", "deepcopy", "(", "self", ".", "summ_dec", ".", "embed", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "summ_dec", ".", "rnn", ")", ")", "\n", "\n", "# Encoder(-decoder) for summary to documents", "\n", "", "", "self", ".", "summ_enc", "=", "None", "\n", "self", ".", "docs_dec", "=", "None", "\n", "if", "self", ".", "hp", ".", "sum_cycle", "or", "self", ".", "hp", ".", "extract_loss", ":", "\n", "            ", "if", "self", ".", "hp", ".", "concat_docs", ":", "# encoder is different: multi-reviews vs. \"canonical\" review -> representation", "\n", "                ", "self", ".", "summ_enc", "=", "StackedLSTMEncoder", "(", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "embed", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "rnn", ")", ")", "\n", "", "else", ":", "# encoder is same: one review  or \"canonical\" review (summary) -> representation", "\n", "                ", "if", "self", ".", "hp", ".", "tie_enc", ":", "\n", "                    ", "self", ".", "summ_enc", "=", "StackedLSTMEncoder", "(", "self", ".", "docs_enc", ".", "embed", ",", "self", ".", "docs_enc", ".", "rnn", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "summ_enc", "=", "StackedLSTMEncoder", "(", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "embed", ")", ",", "\n", "copy", ".", "deepcopy", "(", "self", ".", "docs_enc", ".", "rnn", ")", ")", "\n", "", "", "", "if", "self", ".", "hp", ".", "sum_cycle", "and", "self", ".", "hp", ".", "cycle_loss", "==", "'rec'", ":", "\n", "            ", "self", ".", "docs_dec", "=", "StackedLSTMDecoder", "(", "self", ".", "summ_dec", ".", "embed", ",", "self", ".", "summ_dec", ".", "rnn", ")", "\n", "\n", "# Load a pretrained model and freeze", "\n", "# 1. We may want this so that we have fixed, good representations for the documents.", "\n", "# This could be helpful, especially when we are using a FF or GRU to combine the n_docs representations", "\n", "# instead of taking the mean. Previous experiments without pretraining and freezing found these variants", "\n", "# of the model worse in terms of ROUGE and the loss decreasing. This may be simply because there's two", "\n", "# things to train at once (good document representations and how to combine them).", "\n", "# 2. Thus, we freeze everything except for the FF / GRU model", "\n", "", "if", "self", ".", "hp", ".", "load_ae_freeze", ":", "# load autoencoder and freeze", "\n", "# SummarizationModel", "\n", "            ", "trained", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_autoenc", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "[", "'sum_model'", "]", "\n", "trained", "=", "trained", ".", "module", "if", "isinstance", "(", "trained", ",", "nn", ".", "DataParallel", ")", "else", "trained", "\n", "# self.docs_enc = trained.docs_enc", "\n", "self", ".", "docs_enc", "=", "StackedLSTMEncoder", "(", "trained", ".", "docs_enc", ".", "embed", ",", "trained", ".", "docs_enc", ".", "rnn", ")", "\n", "self", ".", "summ_enc", "=", "StackedLSTMEncoder", "(", "self", ".", "docs_enc", ".", "embed", ",", "self", ".", "docs_enc", ".", "rnn", ")", "\n", "# self.sumn_enc = self.docs_enc # TODO: not sure why this is different from the above", "\n", "\n", "self", ".", "docs_autodec", "=", "StackedLSTMDecoder", "(", "trained", ".", "docs_autodec", ".", "embed", ",", "trained", ".", "docs_autodec", ".", "rnn", ")", "\n", "# self.docs_autodec = trained.docs_autodec", "\n", "self", ".", "summ_dec", "=", "StackedLSTMDecoder", "(", "self", ".", "docs_autodec", ".", "embed", ",", "\n", "self", ".", "docs_autodec", ".", "rnn", ",", "\n", "use_docs_attn", "=", "self", ".", "hp", ".", "docs_attn", ",", "\n", "attn_emb_size", "=", "self", ".", "hp", ".", "hidden_size", ",", "\n", "attn_hidden_size", "=", "self", ".", "hp", ".", "docs_attn_hidden_size", ",", "\n", "attn_learn_alpha", "=", "self", ".", "hp", ".", "docs_attn_learn_alpha", ")", "\n", "if", "self", ".", "hp", ".", "sum_cycle", "and", "self", ".", "hp", ".", "cycle_loss", "==", "'rec'", ":", "\n", "                ", "self", ".", "docs_dec", "=", "StackedLSTMDecoder", "(", "self", ".", "summ_dec", ".", "embed", ",", "self", ".", "summ_dec", ".", "rnn", ")", "\n", "\n", "", "freeze", "(", "self", ".", "docs_enc", ")", "\n", "freeze", "(", "self", ".", "docs_autodec", ")", "\n", "freeze", "(", "self", ".", "summ_dec", ")", "\n", "freeze", "(", "self", ".", "summ_enc", ")", "\n", "\n", "# TODO: I'm not sure if this is necessary or if it does anything", "\n", "# Note though that observing memory usage through nvidia-smi before and after doesn't", "\n", "# necessarily tell you, as the memory is \"freed but not returned to the device\"", "\n", "# https://discuss.pytorch.org/t/947", "\n", "del", "trained", "\n", "\n", "# Freeze embedding layers", "\n", "", "if", "self", ".", "hp", ".", "freeze_embed", ":", "\n", "            ", "for", "model", "in", "[", "self", ".", "docs_enc", ",", "self", ".", "docs_autodec", ",", "self", ".", "summ_dec", ",", "self", ".", "summ_enc", ",", "self", ".", "docs_dec", "]", ":", "\n", "                ", "if", "model", ":", "\n", "                    ", "freeze", "(", "model", ".", "embed", ")", "\n", "\n", "#", "\n", "# Discriminator", "\n", "#", "\n", "", "", "", "self", ".", "discrim_model", "=", "None", "\n", "self", ".", "discrim_optimizer", "=", "None", "\n", "if", "self", ".", "hp", ".", "sum_discrim", ":", "\n", "            ", "if", "len", "(", "self", ".", "opt", ".", "load_discrim", ")", ":", "\n", "                ", "print", "(", "'Loading pretrained discriminator from: {}'", ".", "format", "(", "self", ".", "opt", ".", "load_discrim", ")", ")", "\n", "if", "self", ".", "hp", ".", "discrim_model", "==", "'cnn'", ":", "\n", "                    ", "text_model", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_discrim", ")", "[", "'model'", "]", "\n", "", "self", ".", "discrim_optimizer", "=", "OptWrapper", "(", "self", ".", "discrim_model", ",", "self", ".", "hp", ".", "sum_clip", ",", "\n", "optim", ".", "Adam", "(", "text_model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "hp", ".", "discrim_lr", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'Path to pretrained discriminator not given: training from scratch'", ")", "\n", "if", "self", ".", "hp", ".", "discrim_model", "==", "'cnn'", ":", "\n", "                    ", "cnn_output_size", "=", "self", ".", "hp", ".", "cnn_n_feat_maps", "*", "len", "(", "self", ".", "hp", ".", "cnn_filter_sizes", ")", "\n", "text_model", "=", "TextClassifier", "(", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "self", ".", "hp", ".", "emb_size", ",", "\n", "self", ".", "hp", ".", "cnn_filter_sizes", ",", "self", ".", "hp", ".", "cnn_n_feat_maps", ",", "self", ".", "hp", ".", "cnn_dropout", ",", "\n", "cnn_output_size", ",", "self", ".", "dataset", ".", "n_ratings_labels", ",", "\n", "onehot_inputs", "=", "self", ".", "hp", ".", "discrim_onehot", ")", "\n", "", "", "self", ".", "discrim_model", "=", "Discriminator", "(", "text_model", ",", "self", ".", "hp", ".", "discrim_model", ")", "\n", "discrim_params", "=", "[", "p", "for", "p", "in", "self", ".", "discrim_model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "self", ".", "discrim_optimizer", "=", "OptWrapper", "(", "self", ".", "discrim_model", ",", "self", ".", "hp", ".", "sum_clip", ",", "\n", "optim", ".", "Adam", "(", "discrim_params", ",", "lr", "=", "self", ".", "hp", ".", "discrim_lr", ")", ")", "\n", "self", ".", "optimizers", "[", "'discrim_optimizer'", "]", "=", "self", ".", "discrim_optimizer", "\n", "\n", "#", "\n", "# Classifier", "\n", "#", "\n", "", "self", ".", "clf_model", "=", "None", "\n", "self", ".", "clf_optimizer", "=", "None", "\n", "if", "self", ".", "hp", ".", "sum_clf", ":", "\n", "            ", "if", "len", "(", "self", ".", "opt", ".", "load_clf", ")", ">", "0", ":", "\n", "                ", "print", "(", "'Loading pretrained classifier from: {}'", ".", "format", "(", "self", ".", "opt", ".", "load_clf", ")", ")", "\n", "self", ".", "clf_model", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_clf", ")", "[", "'model'", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "'Path to pretrained classifer not given: training from scratch'", ")", "\n", "cnn_output_size", "=", "self", ".", "hp", ".", "cnn_n_feat_maps", "*", "len", "(", "self", ".", "hp", ".", "cnn_filter_sizes", ")", "\n", "self", ".", "clf_model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'embed'", ",", "nn", ".", "Embedding", "(", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "self", ".", "hp", ".", "emb_size", ")", ")", ",", "\n", "(", "'cnn'", ",", "BasicTextCNN", "(", "self", ".", "hp", ".", "cnn_filter_sizes", ",", "self", ".", "hp", ".", "cnn_n_feat_maps", ",", "self", ".", "hp", ".", "emb_size", ",", "\n", "self", ".", "hp", ".", "cnn_dropout", ")", ")", ",", "\n", "(", "'fc_out'", ",", "nn", ".", "Linear", "(", "cnn_output_size", ",", "self", ".", "dataset", ".", "n_ratings_labels", ")", ")", "\n", "]", ")", ")", "\n", "", "clf_params", "=", "[", "p", "for", "p", "in", "self", ".", "clf_model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "\n", "if", "self", ".", "hp", ".", "sum_clf_lr", ">", "0", ":", "\n", "                ", "self", ".", "clf_optimizer", "=", "OptWrapper", "(", "self", ".", "clf_model", ",", "self", ".", "hp", ".", "sum_clip", ",", "\n", "optim", ".", "Adam", "(", "clf_params", ",", "lr", "=", "self", ".", "hp", ".", "sum_clf_lr", ")", ")", "\n", "self", ".", "optimizers", "[", "'clf_optimizer'", "]", "=", "self", ".", "clf_optimizer", "\n", "", "else", ":", "\n", "                ", "freeze", "(", "self", ".", "clf_model", ")", "\n", "\n", "#", "\n", "# Overall model", "\n", "#", "\n", "", "", "self", ".", "sum_model", "=", "SummarizationModel", "(", "self", ".", "docs_enc", ",", "self", ".", "docs_autodec", ",", "\n", "self", ".", "combine_encs_h_net", ",", "self", ".", "combine_encs_c_net", ",", "self", ".", "summ_dec", ",", "\n", "self", ".", "summ_enc", ",", "self", ".", "docs_dec", ",", "\n", "self", ".", "discrim_model", ",", "self", ".", "clf_model", ",", "\n", "self", ".", "fixed_lm", ",", "\n", "self", ".", "hp", ",", "self", ".", "dataset", ")", "\n", "self", ".", "models", "[", "'sum_model'", "]", "=", "self", ".", "sum_model", "\n", "\n", "# Exclude discriminator and classifier as they have their own optimizers", "\n", "sum_optim_params", "=", "[", "p", "for", "n", ",", "p", "in", "self", ".", "sum_model", ".", "named_parameters", "(", ")", "if", "(", "'discrim'", "not", "in", "n", ")", "and", "(", "'clf'", "not", "in", "n", ")", "and", "p", ".", "requires_grad", "]", "\n", "self", ".", "sum_optimizer", "=", "OptWrapper", "(", "self", ".", "sum_model", ",", "self", ".", "hp", ".", "sum_clip", ",", "\n", "optim", ".", "Adam", "(", "sum_optim_params", ",", "lr", "=", "self", ".", "hp", ".", "sum_lr", ")", ")", "\n", "self", ".", "optimizers", "[", "'sum_optimizer'", "]", "=", "self", ".", "sum_optimizer", "\n", "\n", "# Count number of params", "\n", "all_params", "=", "self", ".", "sum_model", ".", "parameters", "(", ")", "\n", "all_params", "=", "[", "p", "for", "params", "in", "all_params", "for", "p", "in", "params", "]", "# flatten", "\n", "print", "(", "'Number of parameters: {}'", ".", "format", "(", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "all_params", "]", ")", ")", ")", "\n", "all_trainable_params", "=", "[", "p", "for", "p", "in", "all_params", "if", "p", ".", "requires_grad", "]", "\n", "print", "(", "'Number of trainable parameters: {}'", ".", "format", "(", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "all_trainable_params", "]", ")", ")", ")", "\n", "\n", "#", "\n", "# Get extractive summarizer if using that loss", "\n", "#", "\n", "if", "self", ".", "hp", ".", "extract_loss", ":", "\n", "            ", "self", ".", "extract_sum", "=", "CentroidW2VSummarizer", "(", "WORD2VEC_PATH", ",", "length_limit", "=", "2", ",", "\n", "topic_threshold", "=", "0.3", ",", "sim_threshold", "=", "0.95", ",", "\n", "reordering", "=", "True", ",", "subtract_centroid", "=", "False", ",", "keep_first", "=", "False", ",", "\n", "bow_param", "=", "0", ",", "length_param", "=", "0", ",", "position_param", "=", "0", ",", "\n", "debug", "=", "False", ")", "\n", "\n", "#", "\n", "# Move to cuda and parallelize", "\n", "#", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "ngpus", ">", "1", ":", "\n", "            ", "self", ".", "sum_model", "=", "DataParallelModel", "(", "self", ".", "sum_model", ")", "\n", "\n", "#", "\n", "# Train", "\n", "#", "\n", "", "for", "epoch", "in", "range", "(", "self", ".", "hp", ".", "max_nepochs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "sum_model", ".", "train", "(", ")", "\n", "\n", "if", "(", "self", ".", "hp", ".", "n_docs_min", ">", "0", ")", "and", "(", "self", ".", "hp", ".", "n_docs_max", ">", "0", ")", ":", "\n", "# Creation of data loader shuffles and random seed will result in shuffling every epoch", "\n", "                    ", "train_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'train'", ",", "\n", "n_docs_min", "=", "self", ".", "hp", ".", "n_docs_min", ",", "\n", "n_docs_max", "=", "self", ".", "hp", ".", "n_docs_max", ",", "\n", "sample_reviews", "=", "True", ",", "\n", "seed", "=", "epoch", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ")", "\n", "\n", "", "nbatches", "=", "train_iter", ".", "__len__", "(", ")", "\n", "stats_avgs", ",", "evaluator", ",", "_", "=", "self", ".", "run_epoch", "(", "\n", "train_iter", ",", "nbatches", ",", "epoch", ",", "'train'", ",", "\n", "sum_optimizer", "=", "self", ".", "sum_optimizer", ",", "\n", "discrim_optimizer", "=", "self", ".", "discrim_optimizer", ",", "\n", "clf_optimizer", "=", "self", ".", "clf_optimizer", ",", "\n", "# cpkt_every=5, save_intermediate=True, run_val_subset=True,", "\n", "cpkt_every", "=", "int", "(", "nbatches", "/", "10", ")", ",", "save_intermediate", "=", "True", ",", "run_val_subset", "=", "True", ",", "\n", "tb_writer", "=", "self", ".", "tb_tr_writer", ")", "\n", "\n", "for", "k", ",", "v", "in", "stats_avgs", ".", "items", "(", ")", ":", "\n", "                    ", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall_stats/{}'", ".", "format", "(", "k", ")", ",", "v", ",", "epoch", ")", "\n", "", "for", "stat", ",", "rouges", "in", "evaluator", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "                    ", "for", "rouge_name", ",", "d", "in", "rouges", ".", "items", "(", ")", ":", "\n", "                        ", "for", "metric_name", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "                            ", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall_rouges_{}/{}/{}'", ".", "format", "(", "\n", "stat", ",", "rouge_name", ",", "metric_name", ")", ",", "v", ",", "epoch", ")", "\n", "", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "                ", "print", "(", "'Exiting from training early'", ")", "\n", "\n", "# Run on validation", "\n", "", "self", ".", "sum_model", ".", "eval", "(", ")", "\n", "if", "self", ".", "hp", ".", "train_subset", "==", "1.0", ":", "\n", "                ", "stats_avgs", ",", "evaluator", ",", "_", "=", "self", ".", "run_epoch", "(", "val_iter", ",", "val_iter", ".", "__len__", "(", ")", ",", "epoch", ",", "'val'", ",", "\n", "save_intermediate", "=", "False", ",", "run_val_subset", "=", "False", ",", "\n", "tb_writer", "=", "self", ".", "tb_val_writer", ")", "\n", "for", "k", ",", "v", "in", "stats_avgs", ".", "items", "(", ")", ":", "\n", "                    ", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall_stats/{}'", ".", "format", "(", "k", ")", ",", "v", ",", "epoch", ")", "\n", "", "for", "stat", ",", "rouges", "in", "evaluator", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "                    ", "for", "rouge_name", ",", "d", "in", "rouges", ".", "items", "(", ")", ":", "\n", "                        ", "for", "metric_name", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "                            ", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall_rouges_{}/{}/{}'", ".", "format", "(", "\n", "stat", ",", "rouge_name", ",", "metric_name", ")", ",", "v", ",", "epoch", ")", "\n", "", "", "", "", "save_model", "=", "self", ".", "sum_model", ".", "module", "if", "self", ".", "ngpus", ">", "1", "else", "self", ".", "sum_model", "\n", "save_models", "(", "self", ".", "save_dir", ",", "{", "'sum_model'", ":", "save_model", ",", "'tau'", ":", "self", ".", "tau", "}", ",", "self", ".", "optimizers", ",", "epoch", ",", "self", ".", "opt", ",", "\n", "'tot{:.2f}_r1f{:.2f}'", ".", "format", "(", "stats_avgs", "[", "'total_loss'", "]", ",", "\n", "evaluator", ".", "avg_avg_rouges", "[", "'rouge1'", "]", "[", "'f'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.train_sum.Summarizer.test": [[758, 936], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "sum", "print", "train_sum.Summarizer.sum_model.eval", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "enumerate", "os.path.join", "os.path.join", "utils.save_file", "os.path.join", "utils.save_file", "print", "print", "print", "print", "os.path.join", "utils.save_file", "print", "print", "evaluator.get_avg_stats_dicts().items", "os.path.join", "evaluator.plot_rouge_distributions", "train_sum.Summarizer.test.grouped_reviews_iter"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.plot_rouge_distributions"], ["", "", "def", "test", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Run trained model on test set\n        \"\"\"", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "self", ".", "opt", ".", "dataset", ")", "\n", "if", "self", ".", "opt", ".", "test_group_ratings", ":", "\n", "            ", "def", "grouped_reviews_iter", "(", "n_docs", ")", ":", "\n", "                ", "store_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset", ".", "conf", ".", "processed_path", ",", "'test'", ",", "\n", "'plHKBwA18aWeP-TG8DC96Q_reviews.json'", ")", "\n", "# 'SqxIx0KbTmCvUlOfkjamew_reviews.json')", "\n", "from", "utils", "import", "load_file", "\n", "revs", "=", "load_file", "(", "store_path", ")", "\n", "rating_to_revs", "=", "defaultdict", "(", "list", ")", "\n", "for", "rev", "in", "revs", ":", "\n", "                    ", "rating_to_revs", "[", "rev", "[", "'stars'", "]", "]", ".", "append", "(", "rev", "[", "'text'", "]", ")", "\n", "", "for", "rating", "in", "[", "1", ",", "3", ",", "5", "]", ":", "\n", "# Want to return same variables as dataloader iter", "\n", "                    ", "texts", "=", "[", "SummDataset", ".", "concat_docs", "(", "rating_to_revs", "[", "rating", "]", "[", ":", "n_docs", "]", ")", "]", "\n", "ratings", "=", "torch", ".", "LongTensor", "(", "[", "rating", "]", ")", "\n", "metadata", "=", "{", "'item'", ":", "[", "'SqxIx0KbTmCvUlOfkjamew'", "]", ",", "\n", "'categories'", ":", "[", "'Restaurants---Vegan---Thai'", "]", ",", "\n", "'city'", ":", "[", "'Las Vegas'", "]", "}", "\n", "yield", "(", "texts", ",", "ratings", ",", "metadata", ")", "\n", "", "", "self", ".", "hp", ".", "batch_size", "=", "1", "\n", "test_iter", "=", "grouped_reviews_iter", "(", "self", ".", "hp", ".", "n_docs", ")", "\n", "test_iter_len", "=", "3", "\n", "", "else", ":", "\n", "            ", "test_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'test'", ",", "sample_reviews", "=", "False", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "test_iter_len", "=", "test_iter", ".", "__len__", "(", ")", "\n", "\n", "\n", "", "self", ".", "tb_val_sub_writer", "=", "None", "\n", "\n", "#", "\n", "# Get model and loss", "\n", "#", "\n", "ckpt", "=", "torch", ".", "load", "(", "opt", ".", "load_test_sum", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "self", ".", "sum_model", "=", "ckpt", "[", "'sum_model'", "]", "\n", "# We should always be loading from the checkpoint, but I wasn't saving it earlier", "\n", "# Tau may have been decayed over the course of training, so want to use the tau at the time of checkpointing", "\n", "self", ".", "tau", "=", "self", ".", "hp", ".", "tau", "\n", "if", "'tau'", "in", "ckpt", ":", "\n", "            ", "self", ".", "tau", "=", "ckpt", "[", "'tau'", "]", "\n", "# We may want to test with a different n_docs than what was used during training", "\n", "# Update the checkpointed model", "\n", "", "self", ".", "sum_model", ".", "hp", ".", "n_docs", "=", "self", ".", "hp", ".", "n_docs", "\n", "\n", "# For tracking NLL of generated summaries", "\n", "self", ".", "fixed_lm", "=", "torch", ".", "load", "(", "self", ".", "dataset", ".", "conf", ".", "lm_path", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "self", ".", "fixed_lm", "=", "self", ".", "fixed_lm", ".", "module", "if", "isinstance", "(", "self", ".", "fixed_lm", ",", "nn", ".", "DataParallel", ")", "else", "self", ".", "fixed_lm", "\n", "\n", "\n", "# Adding this now for backwards compatability", "\n", "# Was testing with a model that didn't have early_cycle", "\n", "# Because the ckpt is the saved SummarizationModel, which contains a hp attribute, it will not have", "\n", "# self.hp.early_cycle. I do save a snapshot of the code used to train the model and could load that.", "\n", "# However, I should really just be saving the state_dict of the model.", "\n", "if", "not", "hasattr", "(", "self", ".", "sum_model", ".", "hp", ",", "'early_cycle'", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "hp", ".", "early_cycle", "=", "False", "\n", "", "if", "not", "hasattr", "(", "self", ".", "sum_model", ".", "hp", ",", "'cos_honly'", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "hp", ".", "cos_honly", "=", "False", "\n", "", "if", "not", "hasattr", "(", "self", ".", "sum_model", ".", "hp", ",", "'cos_wgt'", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "hp", ".", "cos_wgt", "=", "1.0", "\n", "", "if", "not", "hasattr", "(", "self", ".", "sum_model", ".", "hp", ",", "'tie_enc'", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "hp", ".", "tie_enc", "=", "True", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "sum_model", ".", "cuda", "(", ")", "\n", "", "self", ".", "ngpus", "=", "1", "\n", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", ":", "\n", "            ", "self", ".", "ngpus", "=", "len", "(", "self", ".", "opt", ".", "gpus", ".", "split", "(", "','", ")", ")", "\n", "self", ".", "sum_model", "=", "DataParallelModel", "(", "self", ".", "sum_model", ")", "\n", "\n", "", "n_params", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "self", ".", "sum_model", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'Number of parameters: {}'", ".", "format", "(", "n_params", ")", ")", "\n", "\n", "# Note: starting from here, this code is similar to lm_autoenc_baseline() and the", "\n", "# end of run_summarization_baseline()", "\n", "\n", "#", "\n", "# Run on test set", "\n", "#", "\n", "self", ".", "sum_model", ".", "eval", "(", ")", "\n", "\n", "# Note: in order to run a model trained on the Yelp dataset on the Amazon dataset,", "\n", "# you have to uncomment the following line. This is because the two models", "\n", "# have slightly different vocab_size's, and vocab_size is used inside run_epoch.", "\n", "# (They have slightly different vocab_size because the subword encoder for", "\n", "# both is built using a *target* size of 32000, but the actual size is slightly", "\n", "# lower or higher than 32000).", "\n", "# self.dataset = SummDatasetFactory.get('yelp')", "\n", "# TODO: handle this better", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "stats_avgs", ",", "evaluator", ",", "summaries", "=", "self", ".", "run_epoch", "(", "test_iter", ",", "test_iter_len", ",", "0", ",", "'test'", ",", "\n", "save_intermediate", "=", "False", ",", "run_val_subset", "=", "False", ",", "\n", "store_all_rouges", "=", "True", ",", "store_all_summaries", "=", "True", ")", "\n", "#", "\n", "# Pass summaries through classifier", "\n", "#", "\n", "# Note: I know that since the SummarizationModel already calculates the classification accuracy", "\n", "# if sum_clf=True. Hence, technically, I could refactor it to add everything that I'd like to compute", "\n", "# in the forward pass and add to stats(). However, I think it's cleaner /easier to just do everything", "\n", "# I want here, especially if I add more things like per rating counts and accuracy. Plus,", "\n", "# it's just one pass through the test set -- which I'll run infrequently to evaluate a trained model.", "\n", "# I think that it takes more time is fine.", "\n", "#", "\n", "", "results", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "true_rating_dist", "=", "defaultdict", "(", "int", ")", "# used to track distribution of mean ratings", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "# these are predicted ratnigs", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "clf_model", "=", "self", ".", "sum_model", ".", "module", ".", "clf_model", "if", "self", ".", "ngpus", ">", "1", "else", "self", ".", "sum_model", ".", "clf_model", "\n", "if", "self", ".", "opt", ".", "test_group_ratings", ":", "\n", "            ", "test_iter", "=", "grouped_reviews_iter", "(", "self", ".", "hp", ".", "n_docs", ")", "\n", "", "for", "i", ",", "(", "texts", ",", "ratings_batch", ",", "metadata", ")", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "            ", "summaries_batch", "=", "summaries", "[", "i", "*", "self", ".", "hp", ".", "batch_size", ":", "i", "*", "self", ".", "hp", ".", "batch_size", "+", "len", "(", "texts", ")", "]", "\n", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "summaries_batch", ",", "ratings_batch", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "\n", "for", "rating", "in", "ratings_batch", ":", "\n", "                ", "true_rating_dist", "[", "rating", ".", "item", "(", ")", "]", "+=", "1", "\n", "\n", "", "if", "acc", "is", "None", ":", "\n", "                ", "print", "(", "'Summary was too short to classify'", ")", "\n", "pred_ratings", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", "]", "\n", "pred_probs", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ".", "item", "(", ")", ",", "i", "+", "1", ")", "\n", "\n", "", "for", "j", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", ":", "\n", "                ", "dic", "=", "{", "'docs'", ":", "texts", "[", "j", "]", ",", "\n", "'summary'", ":", "summaries_batch", "[", "j", "]", ",", "\n", "'rating'", ":", "ratings_batch", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_prob'", ":", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "results", ".", "append", "(", "dic", ")", "\n", "\n", "# Save summaries, rouge scores, and rouge distributions figures", "\n", "", "", "dataset_dir", "=", "self", ".", "opt", ".", "dataset", "if", "self", ".", "opt", ".", "az_cat", "is", "None", "else", "'amazon_{}'", ".", "format", "(", "self", ".", "opt", ".", "az_cat", ")", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "OUTPUTS_EVAL_DIR", ",", "dataset_dir", ",", "'n_docs_{}'", ".", "format", "(", "self", ".", "hp", ".", "n_docs", ")", ",", "\n", "'unsup_{}'", ".", "format", "(", "self", ".", "opt", ".", "notes", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "\n", "", "summs_out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'summaries.json'", ")", "\n", "save_file", "(", "results", ",", "summs_out_fp", ")", "\n", "\n", "true_rating_dist", "=", "{", "k", ":", "v", "/", "float", "(", "sum", "(", "true_rating_dist", ".", "values", "(", ")", ")", ")", "for", "k", ",", "v", "in", "true_rating_dist", ".", "items", "(", ")", "}", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'classificaton_acc.json'", ")", "\n", "save_file", "(", "{", "'acc'", ":", "accuracy", ",", "'per_rating_acc'", ":", "per_rating_acc", ",", "'true_rating_dist'", ":", "true_rating_dist", "}", ",", "out_fp", ")", "\n", "\n", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "'Stats:'", ")", "\n", "print", "(", "'Rating accuracy: '", ",", "accuracy", ")", "\n", "print", "(", "'Per rating accuracy: '", ",", "dict", "(", "per_rating_acc", ")", ")", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'stats.json'", ")", "\n", "save_file", "(", "stats_avgs", ",", "out_fp", ")", "\n", "\n", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "'Rouges:'", ")", "\n", "for", "stat", ",", "rouge_dict", "in", "evaluator", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "stat", ".", "upper", "(", ")", ")", "\n", "print", "(", "evaluator", ".", "to_str", "(", "rouge_dict", ")", ")", "\n", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'avg_{}-rouges.json'", ".", "format", "(", "stat", ")", ")", "\n", "save_file", "(", "rouge_dict", ",", "out_fp", ")", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'avg_{}-rouges.csv'", ".", "format", "(", "stat", ")", ")", "\n", "evaluator", ".", "to_csv", "(", "rouge_dict", ",", "out_fp", ")", "\n", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'{}-rouges.pdf'", ")", "\n", "evaluator", ".", "plot_rouge_distributions", "(", "show", "=", "self", ".", "opt", ".", "show_figs", ",", "out_fp", "=", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifier.__init__": [[37, 56], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.append", "layers.append", "collections.OrderedDict", "torch.Embedding", "torch.Embedding", "torch.Embedding", "models.text_cnn.BasicTextCNN", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "emb_size", ",", "\n", "cnn_filter_sizes", ",", "cnn_n_feat_maps", ",", "cnn_dropout", ",", "\n", "cnn_output_size", ",", "n_labels", ",", "\n", "onehot_inputs", "=", "False", ",", "mse", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", "TextClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "onehot_inputs", "=", "onehot_inputs", "\n", "self", ".", "mse", "=", "mse", "# treating classification as regression problem", "\n", "\n", "layers", "=", "[", "\n", "(", "'embed'", ",", "nn", ".", "Embedding", "(", "vocab_size", ",", "emb_size", ")", ")", ",", "\n", "(", "'cnn'", ",", "BasicTextCNN", "(", "cnn_filter_sizes", ",", "cnn_n_feat_maps", ",", "emb_size", ",", "cnn_dropout", ")", ")", "\n", "]", "\n", "if", "mse", ":", "\n", "            ", "layers", ".", "append", "(", "(", "'fc_out'", ",", "nn", ".", "Linear", "(", "cnn_output_size", ",", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "layers", ".", "append", "(", "(", "'fc_out'", ",", "nn", ".", "Linear", "(", "cnn_output_size", ",", "n_labels", ")", ")", ")", "\n", "", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "layers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifier.forward": [[57, 68], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "pretrain_classifier.TextClassifier.model.cnn", "pretrain_classifier.TextClassifier.model.fc_out", "pretrain_classifier.TextClassifier.model", "models.nn_utils.convert_to_onehot.dim", "models.nn_utils.convert_to_onehot", "models.nn_utils.convert_to_onehot.float"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.convert_to_onehot"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "onehot_inputs", ":", "\n", "            ", "if", "x", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "x", "=", "convert_to_onehot", "(", "x", ",", "self", ".", "vocab_size", ")", "# [batch, seq_len] -> [batch, seq_len, vocab]", "\n", "", "inp_emb", "=", "torch", ".", "matmul", "(", "x", ".", "float", "(", ")", ",", "self", ".", "model", ".", "embed", ".", "weight", ")", "# [batch, seq_len, emb_size]", "\n", "cnn_emb", "=", "self", ".", "model", ".", "cnn", "(", "inp_emb", ")", "\n", "logits", "=", "self", ".", "model", ".", "fc_out", "(", "cnn_emb", ")", "\n", "", "else", ":", "\n", "            ", "logits", "=", "self", ".", "model", "(", "x", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifierTrainer.__init__": [[71, 75], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hp", ",", "opt", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "hp", "=", "hp", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "save_dir", "=", "save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifierTrainer.run_epoch": [[76, 201], ["collections.defaultdict", "collections.defaultdict", "enumerate", "print", "time.time", "len", "pretrain_classifier.TextClassifierTrainer.dataset.prepare_batch", "pretrain_classifier.TextClassifierTrainer.model", "pretrain_classifier.TextClassifierTrainer.item", "models.nn_utils.calc_clf_acc().item", "utils.update_moving_avg", "utils.update_moving_avg", "dict", "optimizer.optimizer.zero_grad", "logits.squeeze.squeeze.squeeze", "pretrain_classifier.TextClassifierTrainer.loss_fn", "pretrain_classifier.TextClassifierTrainer.loss_fn", "pretrain_classifier.TextClassifierTrainer.backward", "models.nn_utils.calc_grad_norm", "optimizer.step", "utils.update_moving_avg", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "models.nn_utils.calc_per_rating_acc", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "models.nn_utils.calc_per_rating_acc", "print", "print", "print", "print", "print", "print", "labels.float", "models.nn_utils.calc_clf_acc", "logits.squeeze.squeeze.round", "torch.softmax", "torch.softmax", "torch.softmax", "print_str.format", "tb_writer.add_scalar", "tb_writer.add_scalar", "tb_writer.add_scalar", "tb_writer.add_scalar", "tb_writer.add_text", "tb_writer.add_text", "collections.defaultdict.items", "models.nn_utils.save_model", "dict", "tb_writer.add_scalar", "tb_writer.add_scalar", "tb_writer.add_scalar", "time.time", "len", "logits.squeeze.squeeze.round().long", "logits.squeeze.squeeze.round"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_per_rating_acc", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_per_rating_acc", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_clf_acc", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_model"], ["", "def", "run_epoch", "(", "self", ",", "data_iter", ",", "nbatches", ",", "epoch", ",", "split", ",", "optimizer", "=", "None", ",", "tb_writer", "=", "None", ",", "save_intermediate", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            data_iter: iterable providing minibatches\n            nbatches: int (number of batches in data_iter)\n            epoch: int\n            split: str ('train', 'val')\n            optimizer: Wrapped optim (e.g. OptWrapper)\n            tb_writer: Tensorboard SummaryWriter\n            save_intermediate: boolean (save intermediate checkpoints)\n\n        Returns:\n            1D tensor containing average loss across all items in data_iter\n        \"\"\"", "\n", "\n", "loss_avg", "=", "0", "\n", "acc_avg", "=", "0", "\n", "rating_diff_avg", "=", "0", "\n", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "s", ",", "batch", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "if", "optimizer", ":", "\n", "                ", "optimizer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "texts", ",", "ratings", ",", "metadata", "=", "batch", "\n", "batch_size", "=", "len", "(", "texts", ")", "\n", "x", ",", "lengths", ",", "labels", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "texts", ",", "ratings", ")", "\n", "\n", "#", "\n", "# Forward pass", "\n", "#", "\n", "logits", "=", "self", ".", "model", "(", "x", ")", "\n", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "                ", "logits", "=", "logits", ".", "squeeze", "(", "1", ")", "# [batch, 1] -> [batch]", "\n", "loss", "=", "self", ".", "loss_fn", "(", "logits", ",", "labels", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "self", ".", "loss_fn", "(", "logits", ",", "labels", ")", "\n", "", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "acc", "=", "calc_clf_acc", "(", "logits", ",", "labels", ")", ".", "item", "(", ")", "\n", "\n", "#", "\n", "# Backward pass", "\n", "#", "\n", "gn", "=", "-", "1.0", "# dummy for val (norm can't be < 0 anyway)", "\n", "if", "optimizer", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "gn", "=", "calc_grad_norm", "(", "self", ".", "model", ")", "# not actually using this, just for printing", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "#", "\n", "# Print etc.", "\n", "#", "\n", "", "loss_avg", "=", "update_moving_avg", "(", "loss_avg", ",", "loss_value", ",", "s", "+", "1", ")", "\n", "acc_avg", "=", "update_moving_avg", "(", "acc_avg", ",", "acc", ",", "s", "+", "1", ")", "\n", "print_str", "=", "'Epoch={}, batch={}/{}, split={}, time={:.4f} --- '", "'loss={:.4f}, loss_avg_so_far={:.4f}, acc={:.4f}, acc_avg_so_far={:.4f}, grad_norm={:.4f}'", "\n", "\n", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "                ", "rating_diff", "=", "(", "labels", "-", "logits", ".", "round", "(", ")", ".", "long", "(", ")", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "rating_diff_avg", "=", "update_moving_avg", "(", "rating_diff_avg", ",", "rating_diff", ",", "s", "+", "1", ")", "\n", "print_str", "+=", "', rating_diff={:.4f}, rating_diff_avg_so_far={:.4f}'", ".", "format", "(", "rating_diff", ",", "rating_diff_avg", ")", "\n", "\n", "true_ratings", "=", "labels", "+", "1", "\n", "pred_ratings", "=", "logits", ".", "round", "(", ")", "+", "1", "\n", "probs", "=", "torch", ".", "ones", "(", "batch_size", ")", "# dummy", "\n", "per_rating_counts", ",", "per_rating_acc", "=", "calc_per_rating_acc", "(", "pred_ratings", ",", "true_ratings", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "", "else", ":", "\n", "                ", "true_ratings", "=", "labels", "+", "1", "\n", "probs", ",", "max_idxs", "=", "torch", ".", "max", "(", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", ",", "dim", "=", "1", ")", "\n", "pred_ratings", "=", "max_idxs", "+", "1", "\n", "per_rating_counts", ",", "per_rating_acc", "=", "calc_per_rating_acc", "(", "pred_ratings", ",", "true_ratings", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "\n", "", "if", "s", "%", "self", ".", "opt", ".", "print_every_nbatches", "==", "0", ":", "\n", "                ", "print", "(", "print_str", ".", "format", "(", "\n", "epoch", ",", "s", ",", "nbatches", ",", "split", ",", "time", ".", "time", "(", ")", "-", "start", ",", "\n", "loss_value", ",", "loss_avg", ",", "acc", ",", "acc_avg", ",", "gn", "\n", ")", ")", "\n", "print", "(", "'Review: {}'", ".", "format", "(", "texts", "[", "0", "]", ")", ")", "\n", "print", "(", "'True rating: {}'", ".", "format", "(", "true_ratings", "[", "0", "]", ")", ")", "\n", "print", "(", "'Predicted rating: {}'", ".", "format", "(", "pred_ratings", "[", "0", "]", ")", ")", "\n", "print", "(", "'Predicted rating probability: {:.4f}'", ".", "format", "(", "probs", "[", "0", "]", ")", ")", "\n", "print", "(", "'Per rating accuracy: {}'", ".", "format", "(", "dict", "(", "per_rating_acc", ")", ")", ")", "\n", "\n", "if", "tb_writer", ":", "\n", "# Global steps in terms of number of items", "\n", "# This accounts for runs with different batch sizes", "\n", "                    ", "step", "=", "(", "epoch", "*", "nbatches", "*", "self", ".", "hp", ".", "batch_size", ")", "+", "(", "s", "*", "self", ".", "hp", ".", "batch_size", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'loss/batch_loss'", ",", "loss_value", ",", "step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'loss/avg_loss'", ",", "loss_avg", ",", "step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'acc/batch_acc'", ",", "acc", ",", "step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'acc/avg_acc'", ",", "acc_avg", ",", "step", ")", "\n", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'rating_diff/batch_diff'", ",", "rating_diff", ",", "step", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'rating_diff/avg_diff'", ",", "rating_diff_avg", ",", "step", ")", "\n", "\n", "", "tb_writer", ".", "add_text", "(", "'predictions/review'", ",", "texts", "[", "0", "]", ",", "step", ")", "\n", "tb_writer", ".", "add_text", "(", "'predictions/true_pred_prob'", ",", "\n", "'True={}, Pred={}, Prob={:.4f}'", ".", "format", "(", "\n", "true_ratings", "[", "0", "]", ",", "pred_ratings", "[", "0", "]", ",", "probs", "[", "0", "]", ")", ",", "\n", "step", ")", "\n", "for", "r", ",", "acc", "in", "per_rating_acc", ".", "items", "(", ")", ":", "\n", "                        ", "tb_writer", ".", "add_scalar", "(", "'acc/curavg_per_rating_acc_{}'", ".", "format", "(", "r", ")", ",", "acc", ",", "step", ")", "\n", "\n", "\n", "# Save periodically so we don't have to wait for epoch to finish", "\n", "", "", "", "if", "save_intermediate", ":", "\n", "                ", "save_every", "=", "nbatches", "//", "10", "\n", "if", "save_every", "!=", "0", "and", "s", "%", "save_every", "==", "0", ":", "\n", "                    ", "model_to_save", "=", "self", ".", "model", ".", "module", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", "else", "self", ".", "model", "\n", "save_model", "(", "self", ".", "save_dir", ",", "model_to_save", ",", "self", ".", "optimizer", ",", "epoch", ",", "self", ".", "opt", ",", "'intermediate'", ")", "\n", "\n", "", "", "", "print_str", "=", "'Epoch={}, split={}, --- '", "'loss_avg={:.4f}, acc_avg={:.4f}, per_rating_acc={}'", ".", "format", "(", "\n", "epoch", ",", "split", ",", "loss_avg", ",", "acc_avg", ",", "dict", "(", "per_rating_acc", ")", ")", "\n", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "            ", "print_str", "+=", "', rating_diff_avg={:.4f}'", ".", "format", "(", "rating_diff_avg", ")", "\n", "", "print", "(", "print_str", ")", "\n", "\n", "return", "loss_avg", ",", "acc_avg", ",", "rating_diff_avg", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifierTrainer.train": [[202, 290], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get", "pretrain_classifier.TextClassifierTrainer.dataset.get_data_loader", "pretrain_classifier.TextClassifierTrainer.dataset.get_data_loader", "os.path.join", "print", "os.mkdir", "os.mkdir", "os.mkdir", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "sum", "print", "models.nn_utils.OptWrapper", "range", "len", "NotImplementedError", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "pretrain_classifier.TextClassifierTrainer.model.cuda", "len", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "pretrain_classifier.TextClassifierTrainer.model.eval", "pretrain_classifier.TextClassifierTrainer.run_epoch", "pretrain_classifier.TextClassifierTrainer.tb_val_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_val_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_val_writer.add_scalar", "per_rating_acc.items", "models.nn_utils.save_model", "pretrain_classifier.TextClassifier", "p.nelement", "pretrain_classifier.TextClassifierTrainer.model.parameters", "pretrain_classifier.TextClassifierTrainer.model.train", "pretrain_classifier.TextClassifierTrainer.run_epoch", "pretrain_classifier.TextClassifierTrainer.tb_tr_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_tr_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_tr_writer.add_scalar", "per_rating_acc.items", "pretrain_classifier.TextClassifierTrainer.__len__", "pretrain_classifier.TextClassifierTrainer.tb_val_writer.add_scalar", "len", "pretrain_classifier.TextClassifierTrainer.model.parameters", "pretrain_classifier.TextClassifierTrainer.__len__", "pretrain_classifier.TextClassifierTrainer.tb_tr_writer.add_scalar", "print", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_model", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.train", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main train loop\n        \"\"\"", "\n", "#", "\n", "# Get data, setup", "\n", "#", "\n", "\n", "# NOTE: Use n_docs=1 so we can classify one review", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "self", ".", "opt", ".", "dataset", ")", "\n", "train_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'train'", ",", "sample_reviews", "=", "True", ",", "n_docs", "=", "1", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "val_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'val'", ",", "sample_reviews", "=", "False", ",", "n_docs", "=", "1", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "self", ".", "tb_tr_writer", "=", "None", "\n", "self", ".", "tb_val_writer", "=", "None", "\n", "tb_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "'tensorboard/'", ")", "\n", "print", "(", "'Tensorboard events will be logged to: {}'", ".", "format", "(", "tb_path", ")", ")", "\n", "os", ".", "mkdir", "(", "tb_path", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'train/'", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'val/'", ")", "\n", "self", ".", "tb_tr_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'train/'", ")", "\n", "self", ".", "tb_val_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'val/'", ")", "\n", "\n", "#", "\n", "# Get model and loss", "\n", "#", "\n", "if", "len", "(", "self", ".", "opt", ".", "load_train_model", ")", ">", "0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Need to save run to same directory, handle changes in hp, etc.'", ")", "\n", "# checkpoint = torch.load(opt.load_model)", "\n", "# self.model = checkpoint['model']", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "hp", ".", "model_type", "==", "'cnn'", ":", "\n", "                ", "cnn_output_size", "=", "self", ".", "hp", ".", "cnn_n_feat_maps", "*", "len", "(", "self", ".", "hp", ".", "cnn_filter_sizes", ")", "\n", "self", ".", "model", "=", "TextClassifier", "(", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "self", ".", "hp", ".", "emb_size", ",", "\n", "self", ".", "hp", ".", "cnn_filter_sizes", ",", "self", ".", "hp", ".", "cnn_n_feat_maps", ",", "self", ".", "hp", ".", "cnn_dropout", ",", "\n", "cnn_output_size", ",", "self", ".", "dataset", ".", "n_ratings_labels", ",", "\n", "onehot_inputs", "=", "self", ".", "hp", ".", "clf_onehot", ",", "mse", "=", "self", ".", "hp", ".", "clf_mse", ")", "\n", "\n", "", "", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "            ", "self", ".", "loss_fn", "=", "nn", ".", "MSELoss", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", ":", "\n", "            ", "self", ".", "model", "=", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n", "", "n_params", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'Number of parameters: {}'", ".", "format", "(", "n_params", ")", ")", "\n", "\n", "#", "\n", "# Get optimizer", "\n", "#", "\n", "self", ".", "optimizer", "=", "OptWrapper", "(", "\n", "self", ".", "model", ",", "\n", "self", ".", "hp", ".", "clf_clip", ",", "\n", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "hp", ".", "clf_lr", ")", ")", "\n", "\n", "#", "\n", "# Train epochs", "\n", "#", "\n", "for", "e", "in", "range", "(", "hp", ".", "max_nepochs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "model", ".", "train", "(", ")", "\n", "loss_avg", ",", "acc_avg", ",", "rating_diff_avg", ",", "per_rating_acc", "=", "self", ".", "run_epoch", "(", "\n", "train_iter", ",", "train_iter", ".", "__len__", "(", ")", ",", "e", ",", "'train'", ",", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "tb_writer", "=", "self", ".", "tb_tr_writer", ")", "\n", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall/loss'", ",", "loss_avg", ",", "e", ")", "\n", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall/acc'", ",", "acc_avg", ",", "e", ")", "\n", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall/rating_diff'", ",", "rating_diff_avg", ",", "e", ")", "\n", "for", "r", ",", "acc", "in", "per_rating_acc", ".", "items", "(", ")", ":", "\n", "                    ", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall/per_rating_acc_{}_stars'", ".", "format", "(", "r", ")", ",", "acc", ",", "e", ")", "\n", "", "", "except", "KeyboardInterrupt", ":", "\n", "                ", "print", "(", "'Exiting from training early'", ")", "\n", "\n", "", "self", ".", "model", ".", "eval", "(", ")", "\n", "loss_avg", ",", "acc_avg", ",", "rating_diff_avg", ",", "per_rating_acc", "=", "self", ".", "run_epoch", "(", "\n", "val_iter", ",", "val_iter", ".", "__len__", "(", ")", ",", "e", ",", "'val'", ",", "optimizer", "=", "None", ")", "\n", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall/loss'", ",", "loss_avg", ",", "e", ")", "\n", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall/acc'", ",", "acc_avg", ",", "e", ")", "\n", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall/rating_diff'", ",", "rating_diff_avg", ",", "e", ")", "\n", "for", "r", ",", "acc", "in", "per_rating_acc", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall/per_rating_acc_{}'", ".", "format", "(", "r", ")", ",", "acc", ",", "e", ")", "\n", "", "fn_str", "=", "'l{:.4f}_a{:.4f}_d{:.4f}'", ".", "format", "(", "loss_avg", ",", "acc_avg", ",", "rating_diff_avg", ")", "\n", "model_to_save", "=", "self", ".", "model", ".", "module", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", "else", "self", ".", "model", "\n", "save_model", "(", "self", ".", "save_dir", ",", "model_to_save", ",", "self", ".", "optimizer", ",", "e", ",", "self", ".", "opt", ",", "fn_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_classifier.TextClassifierTrainer.test": [[291, 336], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get", "pretrain_classifier.TextClassifierTrainer.dataset.get_data_loader", "os.path.join", "tensorboardX.SummaryWriter", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "sum", "print", "pretrain_classifier.TextClassifierTrainer.model.eval", "pretrain_classifier.TextClassifierTrainer.tb_test_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_test_writer.add_scalar", "pretrain_classifier.TextClassifierTrainer.tb_test_writer.add_scalar", "per_rating_acc.items", "os.path.exists", "os.mkdir", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "pretrain_classifier.TextClassifierTrainer.model.cuda", "len", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "pretrain_classifier.TextClassifierTrainer.run_epoch", "pretrain_classifier.TextClassifierTrainer.tb_test_writer.add_scalar", "p.nelement", "pretrain_classifier.TextClassifierTrainer.__len__", "pretrain_classifier.TextClassifierTrainer.model.parameters"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__"], ["", "", "def", "test", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Run trained model on test set\n        \"\"\"", "\n", "#", "\n", "# Setup data, logging", "\n", "#", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "self", ".", "opt", ".", "dataset", ")", "\n", "test_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'test'", ",", "sample_reviews", "=", "False", ",", "n_docs", "=", "1", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "tb_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "'tensorboard/test/'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "tb_path", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "tb_path", ")", "\n", "", "self", ".", "tb_test_writer", "=", "SummaryWriter", "(", "tb_path", ")", "\n", "\n", "#", "\n", "# Get model and loss", "\n", "#", "\n", "self", ".", "model", "=", "torch", ".", "load", "(", "opt", ".", "load_test_model", ")", "[", "'model'", "]", "\n", "if", "self", ".", "hp", ".", "clf_mse", ":", "\n", "            ", "self", ".", "loss_fn", "=", "nn", ".", "MSELoss", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", ":", "\n", "            ", "self", ".", "model", "=", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n", "", "n_params", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'Number of parameters: {}'", ".", "format", "(", "n_params", ")", ")", "\n", "\n", "#", "\n", "# Test", "\n", "#", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss_avg", ",", "acc_avg", ",", "rating_diff_avg", ",", "per_rating_acc", "=", "self", ".", "run_epoch", "(", "\n", "test_iter", ",", "test_iter", ".", "__len__", "(", ")", ",", "0", ",", "'test'", ",", "\n", "tb_writer", "=", "self", ".", "tb_test_writer", ",", "save_intermediate", "=", "False", ")", "\n", "", "self", ".", "tb_test_writer", ".", "add_scalar", "(", "'overall/loss'", ",", "loss_avg", ",", "0", ")", "\n", "self", ".", "tb_test_writer", ".", "add_scalar", "(", "'overall/acc'", ",", "acc_avg", ",", "0", ")", "\n", "self", ".", "tb_test_writer", ".", "add_scalar", "(", "'overall/rating_diff'", ",", "rating_diff_avg", ",", "0", ")", "\n", "for", "r", ",", "acc", "in", "per_rating_acc", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "tb_test_writer", ".", "add_scalar", "(", "'overall/per_rating_acc_{}_stars'", ".", "format", "(", "r", ")", ",", "acc", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.__init__": [[71, 75], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hp", ",", "opt", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "hp", "=", "hp", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "save_dir", "=", "save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch": [[76, 197], ["enumerate", "print", "time.time", "pretrain_lm.LanguageModel.dataset.prepare_batch", "pretrain_lm.create_lm_data_iter", "enumerate", "utils.update_moving_avg", "print", "models.nn_utils.save_model", "optimizer.optimizer.zero_grad", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda.size", "pretrain_lm.LanguageModel.model", "batch_trg.transpose().contiguous.transpose().contiguous.transpose().contiguous", "pretrain_lm.LanguageModel.backward", "models.nn_utils.calc_grad_norm", "optimizer.step", "print_str.format", "tb_writer.add_scalar", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "zip", "range", "range", "pretrain_lm.LanguageModel.item", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pretrain_lm.copy_state", "pretrain_lm.copy_state", "pretrain_lm.LanguageModel.model", "pretrain_lm.LanguageModel.loss_fn", "models.nn_utils.move_to_cuda", "pretrain_lm.LanguageModel.item", "pretrain_lm.LanguageModel.model.module.rnn.state0", "pretrain_lm.LanguageModel.model.rnn.state0", "batch_trg.transpose().contiguous.transpose().contiguous.transpose", "len", "pretrain_lm.LanguageModel.loss_fn", "len", "pretrain_lm.LanguageModel.loss_fn", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "batch_obj.ntokens.float", "len", "time.time", "pretrain_lm.copy_state", "pretrain_lm.copy_state", "pretrain_lm.LanguageModel.opt.gpus.split", "range", "range", "range", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.create_lm_data_iter", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_model", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.copy_state", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.copy_state", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.copy_state", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.copy_state"], ["", "def", "run_epoch", "(", "self", ",", "data_iter", ",", "nbatches", ",", "epoch", ",", "split", ",", "optimizer", "=", "None", ",", "tb_writer", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            data_iter: Pytorch DataLoader\n            nbatches: int (number of batches in data_iter)\n            epoch: int\n            split: str ('train', 'val')\n            optimizer: Wrapped optim (e.g. OptWrapper, NoamOpt)\n            tb_writer: Tensorboard SummaryWriter\n\n        Returns:\n            1D tensor containing average loss across all items in data_iter\n        \"\"\"", "\n", "\n", "loss_avg", "=", "0", "\n", "n_fwds", "=", "0", "\n", "for", "s_idx", ",", "(", "texts", ",", "ratings", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Add special tokens to texts", "\n", "x", ",", "lengths", ",", "labels", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "texts", ",", "ratings", ",", "\n", "doc_append_id", "=", "EDOC_ID", ")", "\n", "iter", "=", "create_lm_data_iter", "(", "x", ",", "self", ".", "hp", ".", "lm_seq_len", ")", "\n", "for", "b_idx", ",", "batch_obj", "in", "enumerate", "(", "iter", ")", ":", "\n", "                ", "if", "optimizer", ":", "\n", "                    ", "optimizer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "#", "\n", "# Forward pass", "\n", "#", "\n", "", "if", "self", ".", "hp", ".", "model_type", "==", "'mlstm'", ":", "\n", "# Note: iter creates a sequence of length hp.lm_seq_len + 1, and batch_obj.trg is all about the", "\n", "# last token, while batch_obj.trg_y is all but the first token. They're named as such because", "\n", "# the Batch class was originally designed for the Encoder-Decoder version of the Transformer, and", "\n", "# the trg variables correspond to inputs to the Decoder.", "\n", "                    ", "batch", "=", "move_to_cuda", "(", "batch_obj", ".", "trg", ")", "# it's trg because doesn't include last token", "\n", "batch_trg", "=", "move_to_cuda", "(", "batch_obj", ".", "trg_y", ")", "\n", "batch_size", ",", "seq_len", "=", "batch", ".", "size", "(", ")", "\n", "\n", "if", "b_idx", "==", "0", ":", "\n", "                        ", "h_init", ",", "c_init", "=", "self", ".", "model", ".", "module", ".", "rnn", ".", "state0", "(", "batch_size", ")", "if", "self", ".", "ngpus", ">", "1", "else", "self", ".", "model", ".", "rnn", ".", "state0", "(", "batch_size", ")", "\n", "h_init", "=", "move_to_cuda", "(", "h_init", ")", "\n", "c_init", "=", "move_to_cuda", "(", "c_init", ")", "\n", "\n", "# Forward steps for lstm", "\n", "", "result", "=", "self", ".", "model", "(", "batch", ",", "h_init", ",", "c_init", ")", "\n", "hiddens", ",", "cells", ",", "outputs", "=", "zip", "(", "*", "result", ")", "if", "self", ".", "ngpus", ">", "1", "else", "result", "\n", "\n", "# Calculate loss", "\n", "loss", "=", "0", "\n", "batch_trg", "=", "batch_trg", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "# [seq_len, batch]", "\n", "if", "self", ".", "ngpus", ">", "1", ":", "\n", "                        ", "for", "t", "in", "range", "(", "len", "(", "outputs", "[", "0", "]", ")", ")", ":", "\n", "# length ngpus list of outputs at that time step", "\n", "                            ", "loss", "+=", "self", ".", "loss_fn", "(", "[", "outputs", "[", "i", "]", "[", "t", "]", "for", "i", "in", "range", "(", "len", "(", "outputs", ")", ")", "]", ",", "batch_trg", "[", "t", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "for", "t", "in", "range", "(", "len", "(", "outputs", ")", ")", ":", "\n", "                            ", "loss", "+=", "self", ".", "loss_fn", "(", "outputs", "[", "t", "]", ",", "batch_trg", "[", "t", "]", ")", "\n", "", "", "loss_value", "=", "loss", ".", "item", "(", ")", "/", "self", ".", "hp", ".", "lm_seq_len", "\n", "\n", "# We only do bptt until lm_seq_len. Copy the hidden states so that we can continue the sequence", "\n", "if", "self", ".", "ngpus", ">", "1", ":", "\n", "                        ", "h_init", "=", "torch", ".", "cat", "(", "[", "copy_state", "(", "hiddens", "[", "i", "]", "[", "-", "1", "]", ")", "for", "i", "in", "range", "(", "self", ".", "ngpus", ")", "]", ",", "dim", "=", "0", ")", "\n", "c_init", "=", "torch", ".", "cat", "(", "[", "copy_state", "(", "cells", "[", "i", "]", "[", "-", "1", "]", ")", "for", "i", "in", "range", "(", "self", ".", "ngpus", ")", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "h_init", "=", "copy_state", "(", "hiddens", "[", "-", "1", "]", ")", "\n", "c_init", "=", "copy_state", "(", "cells", "[", "-", "1", "]", ")", "\n", "\n", "", "", "elif", "self", ".", "hp", ".", "model_type", "==", "'transformer'", ":", "\n", "# This is the decoder only version now", "\n", "                    ", "logits", "=", "self", ".", "model", "(", "move_to_cuda", "(", "batch_obj", ".", "trg", ")", ",", "move_to_cuda", "(", "batch_obj", ".", "trg_mask", ")", ")", "\n", "# logits: [batch, seq_len, vocab]", "\n", "loss", "=", "self", ".", "loss_fn", "(", "logits", ",", "move_to_cuda", "(", "batch_obj", ".", "trg_y", ")", ")", "\n", "loss", "/=", "move_to_cuda", "(", "batch_obj", ".", "ntokens", ".", "float", "(", ")", ")", "# normalize by number of non-pad tokens", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "if", "self", ".", "ngpus", ">", "1", ":", "\n", "# With the custom DataParallel, there is no gather() and the loss is calculated per", "\n", "# minibatch split on each GPU (see DataParallelCriterion's forward() -- the return", "\n", "# value is divided by the number of GPUs). We simply undo that operation here.", "\n", "# Also, note that the KLDivLoss in LabelSmoothing is already normalized by both", "\n", "# batch and seq_len, as we use size_average=False to prevent any normalization followed", "\n", "# by a manual normalization using the batch.ntokens. This oddity is because", "\n", "# KLDivLoss does not support ignore_index=PAD_ID as CrossEntropyLoss does.", "\n", "                        ", "loss_value", "*=", "len", "(", "self", ".", "opt", ".", "gpus", ".", "split", "(", "','", ")", ")", "\n", "\n", "#", "\n", "# Backward pass", "\n", "#", "\n", "", "", "gn", "=", "-", "1.0", "# dummy for val (norm can't be < 0 anyway)", "\n", "if", "optimizer", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "gn", "=", "calc_grad_norm", "(", "self", ".", "model", ")", "# not actually using this, just for printing", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "loss_avg", "=", "update_moving_avg", "(", "loss_avg", ",", "loss_value", ",", "n_fwds", "+", "1", ")", "\n", "n_fwds", "+=", "1", "\n", "\n", "# Print", "\n", "", "print_str", "=", "'Epoch={}, batch={}/{}, split={}, time={:.4f} --- '", "'loss={:.4f}, loss_avg_so_far={:.4f}, grad_norm={:.4f}'", "\n", "if", "s_idx", "%", "self", ".", "opt", ".", "print_every_nbatches", "==", "0", ":", "\n", "                ", "print", "(", "print_str", ".", "format", "(", "\n", "epoch", ",", "s_idx", ",", "nbatches", ",", "split", ",", "time", ".", "time", "(", ")", "-", "start", ",", "\n", "loss_value", ",", "loss_avg", ",", "gn", "\n", ")", ")", "\n", "if", "tb_writer", ":", "\n", "# Step for tensorboard: global steps in terms of number of reviews", "\n", "# This accounts for runs with different batch sizes", "\n", "                    ", "step", "=", "(", "epoch", "*", "nbatches", "*", "self", ".", "hp", ".", "batch_size", ")", "+", "(", "s_idx", "*", "self", ".", "hp", ".", "batch_size", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'stats/loss'", ",", "loss_value", ",", "step", ")", "\n", "\n", "# Save periodically so we don't have to wait for epoch to finish", "\n", "", "", "save_every", "=", "nbatches", "//", "10", "\n", "if", "save_every", "!=", "0", "and", "s_idx", "%", "save_every", "==", "0", ":", "\n", "                ", "save_model", "(", "self", ".", "save_dir", ",", "self", ".", "model", ",", "self", ".", "optimizer", ",", "epoch", ",", "self", ".", "opt", ",", "'intermediate'", ")", "\n", "\n", "", "", "print", "(", "'Epoch={}, split={}, --- '", "\n", "'loss_avg={:.4f}'", ".", "format", "(", "epoch", ",", "split", ",", "loss_avg", ")", ")", "\n", "\n", "return", "loss_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.train": [[198, 286], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get", "pretrain_lm.LanguageModel.dataset.get_data_loader", "pretrain_lm.LanguageModel.__len__", "pretrain_lm.LanguageModel.dataset.get_data_loader", "pretrain_lm.LanguageModel.__len__", "os.path.join", "print", "os.mkdir", "os.mkdir", "os.mkdir", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "sum", "print", "range", "len", "NotImplementedError", "pretrain_lm.LanguageModel.model.cuda", "len", "len", "models.custom_parallel.DataParallelModel", "models.custom_parallel.DataParallelCriterion", "models.nn_utils.OptWrapper", "pretrain_lm.LanguageModel.model.eval", "pretrain_lm.LanguageModel.run_epoch", "pretrain_lm.LanguageModel.tb_val_writer.add_scalar", "models.nn_utils.save_model", "torch.Embedding", "torch.Embedding", "models.mlstm.StackedLSTM", "models.mlstm.StackedLSTMEncoder", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "pretrain_lm.LanguageModel.opt.gpus.split", "p.nelement", "torch.optim.Adam", "torch.optim.Adam", "NoamOpt", "pretrain_lm.LanguageModel.model.train", "pretrain_lm.LanguageModel.run_epoch", "pretrain_lm.LanguageModel.tb_tr_writer.add_scalar", "make_model", "LabelSmoothing", "pretrain_lm.LanguageModel.model.parameters", "pretrain_lm.LanguageModel.model.parameters", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "print", "pretrain_lm.LanguageModel.model.parameters"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_model", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.train", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Main train loop\n        \"\"\"", "\n", "#", "\n", "# Get data, setup", "\n", "#", "\n", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "self", ".", "opt", ".", "dataset", ")", "\n", "subwordenc", "=", "self", ".", "dataset", ".", "subwordenc", "\n", "train_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'train'", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "sample_reviews", "=", "True", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "train_nbatches", "=", "train_iter", ".", "__len__", "(", ")", "\n", "val_iter", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'val'", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "sample_reviews", "=", "False", ",", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "val_nbatches", "=", "val_iter", ".", "__len__", "(", ")", "\n", "\n", "tb_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "'tensorboard/'", ")", "\n", "print", "(", "'Tensorboard events will be logged to: {}'", ".", "format", "(", "tb_path", ")", ")", "\n", "os", ".", "mkdir", "(", "tb_path", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'train/'", ")", "\n", "os", ".", "mkdir", "(", "tb_path", "+", "'val/'", ")", "\n", "self", ".", "tb_tr_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'train/'", ")", "\n", "self", ".", "tb_val_writer", "=", "SummaryWriter", "(", "tb_path", "+", "'val/'", ")", "\n", "\n", "#", "\n", "# Get model and loss", "\n", "#", "\n", "if", "len", "(", "self", ".", "opt", ".", "load_model", ")", ">", "0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Need to save run to same directory, handle changes in hp, etc.'", ")", "\n", "# checkpoint = torch.load(opt.load_model)", "\n", "# self.model = checkpoint['model']", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "hp", ".", "model_type", "==", "'mlstm'", ":", "\n", "                ", "embed", "=", "nn", ".", "Embedding", "(", "subwordenc", ".", "vocab_size", ",", "self", ".", "hp", ".", "emb_size", ")", "\n", "lstm", "=", "StackedLSTM", "(", "mLSTM", ",", "\n", "self", ".", "hp", ".", "lstm_layers", ",", "self", ".", "hp", ".", "emb_size", ",", "self", ".", "hp", ".", "hidden_size", ",", "\n", "subwordenc", ".", "vocab_size", ",", "\n", "self", ".", "hp", ".", "lstm_dropout", ",", "\n", "layer_norm", "=", "self", ".", "hp", ".", "lstm_ln", ")", "\n", "self", ".", "model", "=", "StackedLSTMEncoder", "(", "embed", ",", "lstm", ")", "\n", "self", ".", "loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "PAD_ID", ")", "\n", "", "elif", "self", ".", "hp", ".", "model_type", "==", "'transformer'", ":", "\n", "                ", "self", ".", "model", "=", "make_model", "(", "subwordenc", ".", "vocab_size", ",", "subwordenc", ".", "vocab_size", ",", "N", "=", "self", ".", "hp", ".", "tsfr_blocks", ",", "\n", "d_model", "=", "self", ".", "hp", ".", "hidden_size", ",", "d_ff", "=", "self", ".", "hp", ".", "tsfr_ff_size", ",", "\n", "dropout", "=", "self", ".", "hp", ".", "tsfr_dropout", ",", "tie_embs", "=", "self", ".", "hp", ".", "tsfr_tie_embs", ",", "\n", "decoder_only", "=", "True", ")", "\n", "self", ".", "loss_fn", "=", "LabelSmoothing", "(", "size", "=", "subwordenc", ".", "vocab_size", ",", "smoothing", "=", "self", ".", "hp", ".", "tsfr_label_smooth", ")", "\n", "", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "cuda", "(", ")", "\n", "", "self", ".", "ngpus", "=", "1", "\n", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", ":", "\n", "            ", "self", ".", "ngpus", "=", "len", "(", "self", ".", "opt", ".", "gpus", ".", "split", "(", "','", ")", ")", "\n", "self", ".", "model", "=", "DataParallelModel", "(", "self", ".", "model", ")", "\n", "self", ".", "loss_fn", "=", "DataParallelCriterion", "(", "self", ".", "loss_fn", ")", "\n", "\n", "", "n_params", "=", "sum", "(", "[", "p", ".", "nelement", "(", ")", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'Number of parameters: {}'", ".", "format", "(", "n_params", ")", ")", "\n", "\n", "#", "\n", "# Get optimizer", "\n", "#", "\n", "if", "self", ".", "hp", ".", "optim", "==", "'normal'", ":", "\n", "            ", "self", ".", "optimizer", "=", "OptWrapper", "(", "self", ".", "model", ",", "self", ".", "hp", ".", "lm_clip", ",", "\n", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "hp", ".", "lm_lr", ")", ")", "\n", "", "elif", "self", ".", "hp", ".", "optim", "==", "'noam'", ":", "\n", "            ", "d_model", "=", "self", ".", "model", ".", "module", ".", "tgt_embed", "[", "0", "]", ".", "d_model", "if", "self", ".", "ngpus", ">", "1", "else", "self", ".", "model", ".", "tgt_embed", "[", "0", "]", ".", "d_model", "\n", "self", ".", "optimizer", "=", "NoamOpt", "(", "d_model", ",", "2", ",", "self", ".", "hp", ".", "noam_warmup", ",", "\n", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "0", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "eps", "=", "1e-9", ")", ")", "\n", "\n", "#", "\n", "# Train epochs", "\n", "#", "\n", "", "for", "e", "in", "range", "(", "hp", ".", "max_nepochs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "model", ".", "train", "(", ")", "\n", "loss_avg", "=", "self", ".", "run_epoch", "(", "train_iter", ",", "train_nbatches", ",", "e", ",", "'train'", ",", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "tb_writer", "=", "self", ".", "tb_tr_writer", ")", "\n", "self", ".", "tb_tr_writer", ".", "add_scalar", "(", "'overall_stats/loss_avg'", ",", "loss_avg", ",", "e", ")", "\n", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "                ", "print", "(", "'Exiting from training early'", ")", "\n", "\n", "", "self", ".", "model", ".", "eval", "(", ")", "\n", "loss_avg", "=", "self", ".", "run_epoch", "(", "val_iter", ",", "val_nbatches", ",", "e", ",", "'val'", ",", "optimizer", "=", "None", ")", "\n", "self", ".", "tb_val_writer", ".", "add_scalar", "(", "'overall_stats/loss_avg'", ",", "loss_avg", ",", "e", ")", "\n", "save_model", "(", "self", ".", "save_dir", ",", "self", ".", "model", ",", "self", ".", "optimizer", ",", "e", ",", "self", ".", "opt", ",", "loss_avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.create_lm_data_iter": [[29, 50], ["range", "min", "data.narrow().long", "models.nn_utils.Batch", "data.size", "data.size", "data.narrow"], "function", ["None"], ["def", "create_lm_data_iter", "(", "data", ",", "lm_seq_len", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        data: [batch_size, -1] tensor (e.g. result of batchify())\n        lm_seq_len: int\n        model_type: str (mlstm or transformer)\n\n    Returns:\n        iterator that returns Batch's\n\n        batch.src = [batch_size, seq_len+1] tensor\n        the mlstm model transposes this and does src.t()[t] for every time step, predicting src.t()[t+1]\n    \"\"\"", "\n", "nbatches", "=", "(", "data", ".", "size", "(", "1", ")", "-", "2", ")", "//", "lm_seq_len", "+", "1", "# up to and including end of sequences", "\n", "for", "i", "in", "range", "(", "nbatches", ")", ":", "\n", "        ", "start", "=", "i", "*", "lm_seq_len", "\n", "length", "=", "min", "(", "lm_seq_len", ",", "data", ".", "size", "(", "1", ")", "-", "start", ")", "# + 1 for target", "\n", "batch", "=", "data", ".", "narrow", "(", "1", ",", "start", ",", "length", ")", ".", "long", "(", ")", "\n", "\n", "yield", "Batch", "(", "batch", ",", "trg", "=", "batch", ",", "pad", "=", "PAD_ID", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.copy_state": [[52, 62], ["isinstance", "state.detach().clone().requires_grad_", "state[].detach().clone().requires_grad_", "state[].detach().clone().requires_grad_", "state.detach().clone", "state[].detach().clone", "state[].detach().clone", "state.detach", "state[].detach", "state[].detach"], "function", ["None"], ["", "", "def", "copy_state", "(", "state", ")", ":", "\n", "    ", "if", "isinstance", "(", "state", ",", "tuple", ")", ":", "\n", "# return (Variable(state[0].data), Variable(state[1].data))", "\n", "# Detach from graph (otherwise computation graph goes across batches, and", "\n", "# backward() will be called twice). Need to set grad to true after cloning", "\n", "# because clone() uses same requires_grad (False after detach)", "\n", "        ", "return", "(", "state", "[", "0", "]", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "requires_grad_", "(", ")", ",", "\n", "state", "[", "1", "]", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "requires_grad_", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "state", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "requires_grad_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.__init__": [[49, 54], ["data_loaders.summ_dataset_factory.SummDatasetFactory.get"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get"], ["    ", "def", "__init__", "(", "self", ",", "hp", ",", "opt", ")", ":", "\n", "        ", "self", ".", "hp", "=", "hp", "\n", "self", ".", "opt", "=", "opt", "\n", "\n", "self", ".", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "opt", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.get_test_set_data_iter": [[55, 60], ["run_evaluations.Evaluations.dataset.get_data_loader"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader"], ["", "def", "get_test_set_data_iter", "(", "self", ",", "batch_size", "=", "1", ")", ":", "\n", "        ", "dl", "=", "self", ".", "dataset", ".", "get_data_loader", "(", "split", "=", "'test'", ",", "sample_reviews", "=", "False", ",", "n_docs", "=", "self", ".", "hp", ".", "n_docs", ",", "\n", "category", "=", "self", ".", "opt", ".", "az_cat", ",", "\n", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "return", "dl", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.run_summarization_baseline": [[61, 141], ["run_evaluations.Evaluations.get_test_set_data_iter", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "print", "print", "range", "os.path.join", "os.path.join", "utils.save_file", "os.path.join", "utils.save_file", "print", "print", "print", "print", "evaluator.get_avg_stats_dicts().items", "os.path.join", "evaluator.plot_rouge_distributions", "Exception", "run_evaluations.Evaluations.extractive_baseline", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "len", "os.path.exists", "os.makedirs", "dict", "print", "print", "print", "os.path.join", "utils.save_file", "os.path.join", "evaluator.to_csv", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "int", "run_evaluations.Evaluations.ledes_baseline", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "run_evaluations.Evaluations.dataset.prepare_batch", "models.nn_utils.calc_lm_nll", "evaluator.get_avg_stats_dicts", "stat.upper", "evaluator.to_str", "run_evaluations.Evaluations.best_or_worst_review_baseline", "range", "numpy.isnan", "utils.update_moving_avg", "print", "method.split", "run_evaluations.Evaluations.best_or_worst_review_baseline", "len", "models.nn_utils.calc_lm_nll.detach().cpu().numpy", "models.nn_utils.calc_lm_nll.item", "run_evaluations.Evaluations.lm_autoenc_baseline", "models.nn_utils.calc_lm_nll.detach().cpu", "models.nn_utils.calc_lm_nll.detach"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.get_test_set_data_iter", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.plot_rouge_distributions", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.extractive_baseline", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_csv", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.ledes_baseline", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_lm_nll", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_str", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.best_or_worst_review_baseline", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.best_or_worst_review_baseline", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.lm_autoenc_baseline"], ["", "def", "run_summarization_baseline", "(", "self", ",", "method", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            method: str ('extractive', 'ledes-<n>', 'best_review', 'lm_autoenc')\n\n        Saves outputs to: outputs/eval/<dataset>/<n_docs>/<method>\n        \"\"\"", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", "if", "method", "==", "'lm_autoenc'", "else", "1", "\n", "dl", "=", "self", ".", "get_test_set_data_iter", "(", "batch_size", "=", "batch_size", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "clf_model", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_clf", ")", "[", "'model'", "]", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'You should run on a cuda machine to load and use the classifcation model'", ")", "\n", "\n", "", "print", "(", "'\\n'", ",", "'='", "*", "50", ")", "\n", "print", "(", "'Running {} baseline'", ".", "format", "(", "method", ")", ")", "\n", "if", "method", "==", "'extractive'", ":", "\n", "            ", "evaluator", ",", "summaries", ",", "acc", ",", "per_rating_acc", "=", "self", ".", "extractive_baseline", "(", "dl", ",", "clf_model", ")", "\n", "", "elif", "'ledes'", "in", "method", ":", "# e.g. ledes-2", "\n", "            ", "n", "=", "int", "(", "method", ".", "split", "(", "'-'", ")", "[", "1", "]", ")", "\n", "evaluator", ",", "summaries", ",", "acc", ",", "per_rating_acc", "=", "self", ".", "ledes_baseline", "(", "dl", ",", "n", ",", "clf_model", ")", "\n", "", "elif", "method", "==", "'best_review'", ":", "\n", "            ", "evaluator", ",", "summaries", ",", "acc", ",", "per_rating_acc", "=", "self", ".", "best_or_worst_review_baseline", "(", "dl", ",", "'best'", ",", "clf_model", ")", "\n", "", "elif", "method", "==", "'worst_review'", ":", "\n", "            ", "evaluator", ",", "summaries", ",", "acc", ",", "per_rating_acc", "=", "self", ".", "best_or_worst_review_baseline", "(", "dl", ",", "'worst'", ",", "clf_model", ")", "\n", "", "elif", "method", "==", "'lm_autoenc'", ":", "\n", "            ", "evaluator", ",", "summaries", ",", "acc", ",", "per_rating_acc", "=", "self", ".", "lm_autoenc_baseline", "(", "dl", ",", "clf_model", ")", "\n", "\n", "# Calculate NLL of summaries using fixed, pretrained LM", "\n", "", "pretrained_lm", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "pretrained_lm", "=", "pretrained_lm", ".", "module", "if", "isinstance", "(", "pretrained_lm", ",", "nn", ".", "DataParallel", ")", "else", "pretrained_lm", "\n", "avg_nll", "=", "0.0", "\n", "loop_idx", "=", "0", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "summaries", ")", ",", "batch_size", ")", ":", "\n", "            ", "batch_summs", "=", "summaries", "[", "i", ":", "i", "+", "batch_size", "]", "\n", "batch_texts", "=", "[", "d", "[", "'summary'", "]", "for", "d", "in", "batch_summs", "]", "\n", "dummy_ratings", "=", "[", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", "for", "_", "in", "range", "(", "len", "(", "batch_texts", ")", ")", "]", "\n", "try", ":", "\n", "                ", "batch_x", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "batch_texts", ",", "dummy_ratings", ")", "\n", "nll", "=", "calc_lm_nll", "(", "pretrained_lm", ",", "batch_x", ")", "\n", "if", "not", "np", ".", "isnan", "(", "nll", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ":", "\n", "                    ", "avg_nll", "=", "update_moving_avg", "(", "avg_nll", ",", "nll", ".", "item", "(", ")", ",", "loop_idx", "+", "1", ")", "\n", "loop_idx", "+=", "1", "\n", "", "else", ":", "\n", "# lm_autoenc baseline has a rare edge case where a nan is produced", "\n", "                    ", "continue", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "# worst_review in the Amazon dataset has a rare edge case", "\n", "# where the worst review is an empty string.", "\n", "# No reviews should be empty, but it appears to just be one or two reviews", "\n", "                ", "print", "(", "e", ")", "\n", "continue", "\n", "\n", "# Save summaries, stats, rouge scores, etc.", "\n", "", "", "dataset_dir", "=", "self", ".", "opt", ".", "dataset", "if", "self", ".", "opt", ".", "az_cat", "is", "None", "else", "'amazon_{}'", ".", "format", "(", "self", ".", "opt", ".", "az_cat", ")", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "OUTPUTS_EVAL_DIR", ",", "dataset_dir", ",", "'n_docs_{}'", ".", "format", "(", "self", ".", "hp", ".", "n_docs", ")", ",", "method", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "", "summs_out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'summaries.json'", ")", "\n", "save_file", "(", "summaries", ",", "summs_out_fp", ")", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'stats.json'", ")", "\n", "save_file", "(", "{", "'acc'", ":", "acc", ",", "'per_rating_acc'", ":", "per_rating_acc", ",", "'nll'", ":", "avg_nll", "}", ",", "out_fp", ")", "\n", "\n", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "'Rating accuracy: '", ",", "acc", ")", "\n", "print", "(", "'NLL: '", ",", "avg_nll", ")", "\n", "print", "(", "'Per rating accuracy: '", ",", "dict", "(", "per_rating_acc", ")", ")", "\n", "for", "stat", ",", "rouge_dict", "in", "evaluator", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "stat", ".", "upper", "(", ")", ")", "\n", "print", "(", "evaluator", ".", "to_str", "(", "rouge_dict", ")", ")", "\n", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'avg_{}-rouges.json'", ".", "format", "(", "stat", ")", ")", "\n", "save_file", "(", "rouge_dict", ",", "out_fp", ")", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'avg_{}-rouges.csv'", ".", "format", "(", "stat", ")", ")", "\n", "evaluator", ".", "to_csv", "(", "rouge_dict", ",", "out_fp", ")", "\n", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'{}-rouges.pdf'", ")", "\n", "evaluator", ".", "plot_rouge_distributions", "(", "show", "=", "self", ".", "opt", ".", "show_figs", ",", "out_fp", "=", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.extractive_baseline": [[142, 187], ["evaluation.eval_utils.EvalMetrics", "CentroidW2VSummarizer", "collections.defaultdict", "collections.defaultdict", "enumerate", "enumerate", "utils.update_moving_avg.item", "data_loaders.summ_dataset.SummDataset.split_docs", "CentroidW2VSummarizer.summarize", "evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "models.nn_utils.classify_summ_batch", "metadata.items", "summaries.append", "data_loaders.summ_dataset.SummDataset.concat_docs", "print", "utils.update_moving_avg", "ratings[].item", "pred_ratings[].item", "pred_probs[].item", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.concat_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "extractive_baseline", "(", "self", ",", "data_iter", ",", "clf_model", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Run an extractive method\n        \"\"\"", "\n", "evaluator", "=", "EvalMetrics", "(", "remove_stopwords", "=", "self", ".", "hp", ".", "remove_stopwords", ",", "\n", "use_stemmer", "=", "self", ".", "hp", ".", "use_stemmer", ",", "\n", "store_all", "=", "True", ")", "\n", "summarizer", "=", "CentroidW2VSummarizer", "(", "WORD2VEC_PATH", ",", "length_limit", "=", "2", ",", "\n", "topic_threshold", "=", "0.3", ",", "sim_threshold", "=", "0.95", ",", "\n", "reordering", "=", "True", ",", "subtract_centroid", "=", "False", ",", "keep_first", "=", "False", ",", "\n", "bow_param", "=", "0", ",", "length_param", "=", "0", ",", "position_param", "=", "0", ",", "\n", "debug", "=", "False", ")", "\n", "\n", "summaries", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", ",", "(", "texts", ",", "ratings", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "            ", "for", "j", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "# texts is a list of of length batch_size", "\n", "# each item in texts is a str, i.e. n_docs documents concatenated together", "\n", "                ", "src_docs", "=", "SummDataset", ".", "split_docs", "(", "text", ")", "\n", "# limit is number of words", "\n", "# concatenate documents without the token", "\n", "summary", "=", "summarizer", ".", "summarize", "(", "SummDataset", ".", "concat_docs", "(", "src_docs", ",", "edok_token", "=", "False", ")", ",", "\n", "limit", "=", "self", ".", "dataset", ".", "conf", ".", "extractive_max_len", ")", "\n", "evaluator", ".", "batch_update_avg_rouge", "(", "[", "summary", "]", ",", "[", "src_docs", "]", ")", "\n", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "[", "summary", "]", ",", "[", "ratings", "[", "j", "]", "]", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "\n", "if", "acc", "is", "None", ":", "\n", "                    ", "print", "(", "'Summary was too short to classify'", ")", "\n", "pred_rating", ",", "pred_prob", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "                    ", "pred_rating", ",", "pred_prob", "=", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "\n", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ",", "i", "*", "len", "(", "texts", ")", "+", "j", "+", "1", ")", "\n", "\n", "", "dic", "=", "{", "'docs'", ":", "text", ",", "'summary'", ":", "summary", ",", "'rating'", ":", "ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_rating", ",", "'pred_prob'", ":", "pred_prob", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "summaries", ".", "append", "(", "dic", ")", "\n", "\n", "", "", "return", "evaluator", ",", "summaries", ",", "accuracy", ".", "item", "(", ")", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.ledes_baseline": [[188, 248], ["evaluation.eval_utils.EvalMetrics", "collections.defaultdict", "collections.defaultdict", "enumerate", "enumerate", "utils.update_moving_avg.item", "data_loaders.summ_dataset.SummDataset.split_docs", "evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "models.nn_utils.classify_summ_batch", "metadata.items", "summaries.append", "nltk.sent_tokenize", "print", "utils.update_moving_avg", "ratings[].item", "len", "len", "len", "pred_ratings[].item", "pred_probs[].item", "nltk.word_tokenize", "summary.append", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "ledes_baseline", "(", "self", ",", "data_iter", ",", "n", "=", "1", ",", "clf_model", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Add up until the first n sentences from each review, or until the maximum review length is exceeded\n        \"\"\"", "\n", "evaluator", "=", "EvalMetrics", "(", "remove_stopwords", "=", "self", ".", "hp", ".", "remove_stopwords", ",", "\n", "use_stemmer", "=", "self", ".", "hp", ".", "use_stemmer", ",", "\n", "store_all", "=", "True", ")", "\n", "summaries", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", ",", "(", "texts", ",", "ratings", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "# texts is a list of of length batch_size", "\n", "# each item in texts is a str, i.e. n_docs documents concatenated together", "\n", "            ", "for", "j", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "                ", "src_docs", "=", "SummDataset", ".", "split_docs", "(", "text", ")", "\n", "\n", "summary", "=", "[", "]", "\n", "doc_sents", "=", "[", "nltk", ".", "sent_tokenize", "(", "doc", ")", "for", "doc", "in", "src_docs", "]", "\n", "summary_len", "=", "0", "\n", "doc_idx", ",", "sent_idx", "=", "0", ",", "0", "\n", "\n", "# Keep adding sentences as long as summary isn't over maximum length and", "\n", "# there are still sentences to add", "\n", "while", "(", "summary_len", "<", "self", ".", "dataset", ".", "conf", ".", "review_max_len", ")", "and", "(", "sent_idx", "<", "n", ")", ":", "\n", "# Current document has this many sentences", "\n", "                    ", "if", "sent_idx", "<", "len", "(", "doc_sents", "[", "doc_idx", "]", ")", ":", "\n", "                        ", "sent", "=", "doc_sents", "[", "doc_idx", "]", "[", "sent_idx", "]", "\n", "sent_tok_len", "=", "len", "(", "nltk", ".", "word_tokenize", "(", "sent", ")", ")", "\n", "\n", "# Adding sentence won't exceed maximum length", "\n", "if", "summary_len", "+", "sent_tok_len", "<=", "self", ".", "dataset", ".", "conf", ".", "review_max_len", ":", "\n", "                            ", "summary", ".", "append", "(", "sent", ")", "\n", "summary_len", "+=", "sent_tok_len", "\n", "\n", "# Move on to next document", "\n", "", "", "doc_idx", "=", "(", "doc_idx", "+", "1", ")", "%", "len", "(", "src_docs", ")", "\n", "if", "doc_idx", "==", "0", ":", "# back to the first doc, all first sentences have been added", "\n", "                        ", "sent_idx", "+=", "1", "\n", "\n", "", "", "summary", "=", "' '", ".", "join", "(", "summary", ")", "\n", "evaluator", ".", "batch_update_avg_rouge", "(", "[", "summary", "]", ",", "[", "src_docs", "]", ")", "\n", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "[", "summary", "]", ",", "[", "ratings", "[", "j", "]", "]", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "\n", "if", "acc", "is", "None", ":", "\n", "                    ", "print", "(", "'Summary was too short to classify'", ")", "\n", "pred_rating", ",", "pred_prob", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "                    ", "pred_rating", ",", "pred_prob", "=", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "\n", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ",", "i", "*", "len", "(", "texts", ")", "+", "j", "+", "1", ")", "\n", "\n", "", "dic", "=", "{", "'docs'", ":", "text", ",", "'summary'", ":", "summary", ",", "'rating'", ":", "ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_rating", ",", "'pred_prob'", ":", "pred_prob", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "summaries", ".", "append", "(", "dic", ")", "\n", "\n", "", "", "return", "evaluator", ",", "summaries", ",", "accuracy", ".", "item", "(", ")", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.best_or_worst_review_baseline": [[249, 312], ["evaluation.eval_utils.EvalMetrics", "collections.defaultdict", "collections.defaultdict", "enumerate", "enumerate", "utils.update_moving_avg.item", "data_loaders.summ_dataset.SummDataset.split_docs", "evaluation.eval_utils.EvalMetrics.update_with_evaluator", "metadata.items", "summaries.append", "evaluation.eval_utils.EvalMetrics", "evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "models.nn_utils.classify_summ_batch", "print", "utils.update_moving_avg", "ratings[].item", "pred_ratings[].item", "pred_probs[].item", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.update_with_evaluator", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "best_or_worst_review_baseline", "(", "self", ",", "data_iter", ",", "method", "=", "'best'", ",", "clf_model", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        When summarizing n_docs reviews, calculate the average ROUGE1-F for each review as if it was the summary.\n        Choose the document with the best / worst score.\n\n        Note: it'd be far more efficient to calculate best and worst at the same time as all the rouges\n        are already calculated...\n        \"\"\"", "\n", "evaluator", "=", "EvalMetrics", "(", "remove_stopwords", "=", "self", ".", "hp", ".", "remove_stopwords", ",", "\n", "use_stemmer", "=", "self", ".", "hp", ".", "use_stemmer", ",", "\n", "store_all", "=", "True", ")", "\n", "summaries", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", ",", "(", "texts", ",", "ratings", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "# texts is a list of of length batch_size", "\n", "# each item in texts is a str, i.e. n_docs documents concatenated together", "\n", "            ", "for", "j", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "                ", "bw_evaluator", "=", "None", "\n", "bw_rouge1_f", "=", "0.0", "if", "method", "==", "'best'", "else", "1.0", "\n", "bw_doc", "=", "None", "\n", "\n", "# Set each document as the summary and find the best one", "\n", "src_docs", "=", "SummDataset", ".", "split_docs", "(", "text", ")", "\n", "for", "doc", "in", "src_docs", ":", "\n", "                    ", "cur_evaluator", "=", "EvalMetrics", "(", "remove_stopwords", "=", "self", ".", "hp", ".", "remove_stopwords", ",", "\n", "use_stemmer", "=", "self", ".", "hp", ".", "use_stemmer", ",", "\n", "store_all", "=", "True", ")", "\n", "avg_rouges", ",", "_", ",", "_", ",", "_", "=", "cur_evaluator", ".", "batch_update_avg_rouge", "(", "[", "doc", "]", ",", "[", "src_docs", "]", ")", "\n", "is_better_worse", "=", "(", "method", "==", "'best'", "and", "(", "avg_rouges", "[", "'rouge1'", "]", "[", "'f'", "]", ">=", "bw_rouge1_f", ")", ")", "or", "(", "method", "==", "'worst'", "and", "(", "avg_rouges", "[", "'rouge1'", "]", "[", "'f'", "]", "<=", "bw_rouge1_f", ")", ")", "\n", "if", "is_better_worse", ":", "\n", "                        ", "bw_evaluator", "=", "cur_evaluator", "\n", "bw_rouge1_f", "=", "avg_rouges", "[", "'rouge1'", "]", "[", "'f'", "]", "\n", "bw_doc", "=", "doc", "\n", "\n", "", "", "evaluator", ".", "update_with_evaluator", "(", "bw_evaluator", ")", "\n", "\n", "try", ":", "\n", "                    ", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "[", "bw_doc", "]", ",", "[", "ratings", "[", "j", "]", "]", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "# worst_review in the Amazon dataset has a rare edge case", "\n", "# where the worst review is an empty string.", "\n", "# No reviews should be empty, but it appears to just be one or two reviews", "\n", "                    ", "pass", "\n", "\n", "", "if", "acc", "is", "None", ":", "\n", "                    ", "print", "(", "'Summary was too short to classify'", ")", "\n", "pred_rating", ",", "pred_prob", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "                    ", "pred_rating", ",", "pred_prob", "=", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "\n", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ",", "i", "*", "len", "(", "texts", ")", "+", "j", "+", "1", ")", "\n", "\n", "", "dic", "=", "{", "'docs'", ":", "text", ",", "'summary'", ":", "bw_doc", ",", "'rating'", ":", "ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_rating", ",", "'pred_prob'", ":", "pred_prob", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "summaries", ".", "append", "(", "dic", ")", "\n", "\n", "", "", "return", "evaluator", ",", "summaries", ",", "accuracy", ".", "item", "(", ")", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.lm_autoenc_baseline": [[313, 401], ["models.mlstm.StackedLSTMDecoder", "copy.deepcopy", "copy.deepcopy", "float", "train_sum.Summarizer", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "models.summarization.SummarizationModel", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "train_sum.Summarizer.sum_model.eval", "collections.defaultdict", "collections.defaultdict", "enumerate", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "copy.deepcopy", "copy.deepcopy", "len", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "train_sum.Summarizer.sum_model.cuda", "models.custom_parallel.DataParallelModel", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "train_sum.Summarizer.run_epoch", "models.nn_utils.classify_summ_batch", "range", "utils.update_moving_avg.item", "len", "run_evaluations.Evaluations.opt.gpus.split", "data_iter.__len__", "print", "utils.update_moving_avg", "len", "metadata.items", "results.append", "ratings_batch[].item", "pred_ratings[].item", "pred_probs[].item", "len", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.pretrain_lm.LanguageModel.run_epoch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "lm_autoenc_baseline", "(", "self", ",", "data_iter", ",", "clf_model", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Use the pretrained language model to initialize an encoder-decoder model. This is basically the\n        unsupervised abstractive summarization model without training.\n        \"\"\"", "\n", "\n", "# Load encoder decoder by initializing with languag emodel", "\n", "docs_enc", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "docs_enc", "=", "docs_enc", ".", "module", "if", "isinstance", "(", "docs_enc", ",", "nn", ".", "DataParallel", ")", "else", "docs_enc", "\n", "summ_dec", "=", "StackedLSTMDecoder", "(", "copy", ".", "deepcopy", "(", "docs_enc", ".", "embed", ")", ",", "copy", ".", "deepcopy", "(", "docs_enc", ".", "rnn", ")", ")", "\n", "\n", "# Create Summarizer so that we can use run_epoch()", "\n", "# Copy hp and opt as we're modifying some params. This way there won't be any unexpected errors", "\n", "# if it's used by another method", "\n", "hp", "=", "copy", ".", "deepcopy", "(", "self", ".", "hp", ")", "\n", "hp", ".", "sum_cycle", "=", "False", "\n", "hp", ".", "autoenc_docs", "=", "False", "\n", "hp", ".", "sum_clf", "=", "False", "\n", "opt", "=", "copy", ".", "deepcopy", "(", "self", ".", "opt", ")", "\n", "opt", ".", "print_every_nbatches", "=", "float", "(", "'inf'", ")", "\n", "\n", "summarizer", "=", "Summarizer", "(", "hp", ",", "opt", ",", "'/tmp/'", ")", "\n", "summarizer", ".", "tb_val_sub_writer", "=", "None", "\n", "summarizer", ".", "tau", "=", "self", ".", "hp", ".", "tau", "\n", "summarizer", ".", "ngpus", "=", "1", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", "==", "1", "else", "len", "(", "self", ".", "opt", ".", "gpus", ".", "split", "(", "','", ")", ")", "\n", "summarizer", ".", "sum_model", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "\n", "summarizer", ".", "dataset", "=", "self", ".", "dataset", "\n", "\n", "summarizer", ".", "fixed_lm", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "summarizer", ".", "fixed_lm", "=", "summarizer", ".", "fixed_lm", ".", "module", "if", "isinstance", "(", "summarizer", ".", "fixed_lm", ",", "nn", ".", "DataParallel", ")", "else", "summarizer", ".", "fixed_lm", "\n", "\n", "# Create SummarizationModel", "\n", "docs_autodec", ",", "combine_encs_h_net", ",", "combine_encs_c_net", "=", "None", ",", "None", ",", "None", "\n", "summ_enc", ",", "docs_dec", ",", "discrim_model", ",", "clf_model_arg", ",", "fixed_lm", "=", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "summarizer", ".", "sum_model", "=", "SummarizationModel", "(", "docs_enc", ",", "docs_autodec", ",", "\n", "combine_encs_h_net", ",", "combine_encs_c_net", ",", "summ_dec", ",", "\n", "summ_enc", ",", "docs_dec", ",", "discrim_model", ",", "clf_model_arg", ",", "fixed_lm", ",", "\n", "hp", ",", "self", ".", "dataset", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "summarizer", ".", "sum_model", ".", "cuda", "(", ")", "\n", "", "if", "summarizer", ".", "ngpus", ">", "1", ":", "\n", "            ", "summarizer", ".", "sum_model", "=", "DataParallelModel", "(", "summarizer", ".", "sum_model", ")", "\n", "", "summarizer", ".", "sum_model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "stats_avgs", ",", "evaluator", ",", "summaries", "=", "summarizer", ".", "run_epoch", "(", "\n", "data_iter", ",", "data_iter", ".", "__len__", "(", ")", ",", "0", ",", "'test'", ",", "\n", "store_all_rouges", "=", "True", ",", "store_all_summaries", "=", "True", ",", "\n", "save_intermediate", "=", "False", ",", "run_val_subset", "=", "False", ")", "\n", "\n", "#", "\n", "# Pass summaries through classifier", "\n", "#", "\n", "# Note: I know that since the SummarizationModel already calculates the classification accuracy", "\n", "# if sum_clf=True. Hence, technically, I could refactor it to add everything that I'd like to compute", "\n", "# in the forward pass and add to stats(). However, I think it's cleaner /easier to just do everything", "\n", "# I want here, especially if I add more things like per rating counts and accuracy. Plus,", "\n", "# it's just one pass through the test set -- which I'll run infrequently to evaluate a trained model.", "\n", "# I think that it takes more time is fine.", "\n", "#", "\n", "", "results", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", ",", "(", "texts", ",", "ratings_batch", ",", "metadata", ")", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "            ", "summaries_batch", "=", "summaries", "[", "i", "*", "self", ".", "hp", ".", "batch_size", ":", "i", "*", "self", ".", "hp", ".", "batch_size", "+", "len", "(", "texts", ")", "]", "\n", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "summaries_batch", ",", "ratings_batch", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "\n", "if", "acc", "is", "None", ":", "\n", "                ", "print", "(", "'Summary was too short to classify'", ")", "\n", "pred_ratings", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", "]", "\n", "pred_probs", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ",", "i", "+", "1", ")", "\n", "\n", "", "for", "j", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", ":", "\n", "                ", "dic", "=", "{", "'docs'", ":", "texts", "[", "j", "]", ",", "\n", "'summary'", ":", "summaries_batch", "[", "j", "]", ",", "\n", "'rating'", ":", "ratings_batch", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_prob'", ":", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "results", ".", "append", "(", "dic", ")", "\n", "\n", "", "", "return", "evaluator", ",", "results", ",", "accuracy", ".", "item", "(", ")", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.run_clf_baseline": [[402, 475], ["print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "collections.defaultdict", "collections.defaultdict", "run_evaluations.Evaluations.get_test_set_data_iter", "enumerate", "range", "print", "print", "print", "os.path.join", "os.path.join", "utils.save_file", "os.path.join", "utils.save_file", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "torch.DataParallel.cuda", "len", "torch.DataParallel", "torch.DataParallel", "torch.DataParallel", "enumerate", "models.nn_utils.classify_summ_batch", "utils.update_moving_avg", "range", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "len", "run_evaluations.Evaluations.dataset.prepare_batch", "models.nn_utils.calc_lm_nll", "utils.update_moving_avg", "utils.update_moving_avg.item", "os.path.exists", "os.makedirs", "data_loaders.summ_dataset.SummDataset.split_docs", "data_loaders.summ_dataset.SummDataset.concat_docs", "summaries_batch.append", "len", "metadata.items", "summaries.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "models.nn_utils.calc_lm_nll.item", "utils.update_moving_avg.item", "ratings_batch[].item", "pred_ratings[].item", "pred_probs[].item", "range", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.get_test_set_data_iter", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_lm_nll", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.concat_docs"], ["", "def", "run_clf_baseline", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate the classification accuracy when the input is all the reviews concatenated together. This provdies\n        a sort of ceiling on how well each of the summarization methods can do, as the classification model\n        is not perfect either.\n        \"\"\"", "\n", "print", "(", "'\\n'", ",", "'='", "*", "50", ")", "\n", "print", "(", "'Running classifier baseline'", ")", "\n", "\n", "# Load classifier", "\n", "clf_model", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_clf", ")", "[", "'model'", "]", "\n", "clf_model", "=", "clf_model", ".", "module", "if", "isinstance", "(", "clf_model", ",", "nn", ".", "DataParallel", ")", "else", "clf_model", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "clf_model", ".", "cuda", "(", ")", "\n", "", "if", "len", "(", "self", ".", "opt", ".", "gpus", ")", ">", "1", ":", "\n", "            ", "clf_model", "=", "nn", ".", "DataParallel", "(", "clf_model", ")", "\n", "\n", "", "summaries", "=", "[", "]", "\n", "accuracy", "=", "0.0", "\n", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "dl", "=", "self", ".", "get_test_set_data_iter", "(", "self", ".", "hp", ".", "batch_size", ")", "\n", "for", "i", ",", "(", "texts", ",", "ratings_batch", ",", "metadata", ")", "in", "enumerate", "(", "dl", ")", ":", "\n", "            ", "summaries_batch", "=", "[", "]", "\n", "for", "j", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "# texts is a list of of length batch_size", "\n", "# each item in texts is a str, i.e. n_docs documents concatenated together", "\n", "# concatenate documents without the token", "\n", "                ", "src_docs", "=", "SummDataset", ".", "split_docs", "(", "text", ")", "\n", "summary", "=", "SummDataset", ".", "concat_docs", "(", "src_docs", ",", "edok_token", "=", "False", ")", "\n", "summaries_batch", ".", "append", "(", "summary", ")", "\n", "\n", "", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "=", "classify_summ_batch", "(", "clf_model", ",", "summaries_batch", ",", "ratings_batch", ",", "self", ".", "dataset", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "accuracy", "=", "update_moving_avg", "(", "accuracy", ",", "acc", ",", "i", "+", "1", ")", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "summaries_batch", ")", ")", ":", "\n", "                ", "dic", "=", "{", "'docs'", ":", "summaries_batch", "[", "j", "]", ",", "\n", "'rating'", ":", "ratings_batch", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_rating'", ":", "pred_ratings", "[", "j", "]", ".", "item", "(", ")", ",", "\n", "'pred_prob'", ":", "pred_probs", "[", "j", "]", ".", "item", "(", ")", "}", "\n", "for", "k", ",", "values", "in", "metadata", ".", "items", "(", ")", ":", "\n", "                    ", "dic", "[", "k", "]", "=", "values", "[", "j", "]", "\n", "", "summaries", ".", "append", "(", "dic", ")", "\n", "\n", "\n", "# Calculate NLL of summaries using fixed, pretrained LM", "\n", "", "", "pretrained_lm", "=", "torch", ".", "load", "(", "self", ".", "opt", ".", "load_lm", ")", "[", "'model'", "]", "# StackedLSTMEncoder", "\n", "pretrained_lm", "=", "pretrained_lm", ".", "module", "if", "isinstance", "(", "pretrained_lm", ",", "nn", ".", "DataParallel", ")", "else", "pretrained_lm", "\n", "avg_nll", "=", "0.0", "\n", "batch_size", "=", "self", ".", "hp", ".", "batch_size", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "summaries", ")", ",", "batch_size", ")", ":", "\n", "            ", "batch_summs", "=", "summaries", "[", "i", ":", "i", "+", "batch_size", "]", "\n", "batch_texts", "=", "[", "d", "[", "'docs'", "]", "for", "d", "in", "batch_summs", "]", "\n", "dummy_ratings", "=", "[", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", "for", "_", "in", "range", "(", "len", "(", "batch_texts", ")", ")", "]", "\n", "batch_x", ",", "_", ",", "_", "=", "self", ".", "dataset", ".", "prepare_batch", "(", "batch_texts", ",", "dummy_ratings", ")", "\n", "nll", "=", "calc_lm_nll", "(", "pretrained_lm", ",", "batch_x", ")", "\n", "avg_nll", "=", "update_moving_avg", "(", "avg_nll", ",", "nll", ".", "item", "(", ")", ",", "i", "+", "1", ")", "\n", "\n", "# Print and save accuracies, summaries, etc.", "\n", "", "print", "(", "'NLL: '", ",", "avg_nll", ")", "\n", "print", "(", "'Accuracy: '", ",", "accuracy", ".", "item", "(", ")", ")", "\n", "print", "(", "'Per rating accuracy: '", ",", "per_rating_acc", ")", "\n", "\n", "dataset_dir", "=", "self", ".", "opt", ".", "dataset", "if", "self", ".", "opt", ".", "az_cat", "is", "None", "else", "'amazon_{}'", ".", "format", "(", "self", ".", "opt", ".", "az_cat", ")", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "OUTPUTS_EVAL_DIR", ",", "dataset_dir", ",", "'n_docs_{}'", ".", "format", "(", "self", ".", "hp", ".", "n_docs", ")", ",", "'clf_baseline'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "out_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'summaries.json'", ")", "\n", "save_file", "(", "summaries", ",", "out_fp", ")", "\n", "out_fp", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'stats.json'", ")", "\n", "save_file", "(", "{", "'acc'", ":", "accuracy", ".", "item", "(", ")", ",", "'per_rating_acc'", ":", "per_rating_acc", ",", "'nll'", ":", "avg_nll", "}", ",", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.run_varying_ndocs_evaluation": [[476, 479], ["None"], "methods", ["None"], ["", "def", "run_varying_ndocs_evaluation", "(", "self", ")", ":", "\n", "        ", "\"\"\"Vary n_docs at inference and calculate various stats\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.run_std_vs_acc": [[480, 483], ["None"], "methods", ["None"], ["", "def", "run_std_vs_acc", "(", "self", ")", ":", "\n", "        ", "\"\"\"Plot standard deviation of reviews being summarized against classification accuracy of summary\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.run_evaluations.Evaluations.collect_results": [[484, 526], ["os.path.join", "len", "range", "os.path.join", "utils.save_file", "os.path.join", "utils.load_file", "all_summs.append", "len", "[].items", "range", "aggregated.append", "set", "len", "len", "range", "set", "len", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file"], ["", "def", "collect_results", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Aggregate results so that we can easily compare them\n        \"\"\"", "\n", "# Just hard coding the ones we're most interested in right now", "\n", "# Only including these in the output file just makes it easier to scroll through and read", "\n", "METHODS", "=", "[", "'unsup_abstractive'", ",", "'lm_autoenc'", ",", "'extractive'", "]", "\n", "for", "n_docs", "in", "[", "8", "]", ":", "\n", "            ", "ndocs_dir", "=", "os", ".", "path", ".", "join", "(", "OUTPUTS_EVAL_DIR", ",", "self", ".", "opt", ".", "dataset", ",", "'n_docs_{}'", ".", "format", "(", "n_docs", ")", ")", "\n", "\n", "# Collect all summaries", "\n", "all_summs", "=", "[", "]", "\n", "for", "method", "in", "METHODS", ":", "\n", "                ", "summs_fp", "=", "os", ".", "path", ".", "join", "(", "ndocs_dir", ",", "method", ",", "'summaries.json'", ")", "\n", "summs", "=", "load_file", "(", "summs_fp", ")", "\n", "all_summs", ".", "append", "(", "summs", ")", "\n", "# Combine them", "\n", "", "assert", "len", "(", "set", "(", "[", "len", "(", "summs", ")", "for", "summs", "in", "all_summs", "]", ")", ")", "==", "1", ",", "'All methods should be calculated on the same test set and have the same number of summaries'", "\n", "\n", "# Combine summaries", "\n", "aggregated", "=", "[", "]", "\n", "n_summs", "=", "len", "(", "all_summs", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "n_summs", ")", ":", "\n", "                ", "docs", "=", "[", "all_summs", "[", "j", "]", "[", "i", "]", "[", "'docs'", "]", "for", "j", "in", "range", "(", "len", "(", "METHODS", ")", ")", "]", "\n", "assert", "len", "(", "set", "(", "docs", ")", ")", "==", "1", ",", "'The documents being summarized should be the same / in the same order'", "\n", "\n", "agg", "=", "{", "}", "\n", "# Get data related to original reviews", "\n", "for", "k", ",", "v", "in", "all_summs", "[", "0", "]", "[", "i", "]", ".", "items", "(", ")", ":", "\n", "                    ", "if", "k", "not", "in", "[", "'summary'", ",", "'pred_rating'", ",", "'pred_prob'", "]", ":", "\n", "                        ", "agg", "[", "k", "]", "=", "v", "\n", "# Add summary data for each method", "\n", "", "", "for", "j", "in", "range", "(", "len", "(", "METHODS", ")", ")", ":", "\n", "                    ", "agg", "[", "METHODS", "[", "j", "]", "]", "=", "{", "'summary'", ":", "all_summs", "[", "j", "]", "[", "i", "]", "[", "'summary'", "]", ",", "\n", "'pred_rating'", ":", "all_summs", "[", "j", "]", "[", "i", "]", "[", "'pred_rating'", "]", ",", "\n", "'pred_prob'", ":", "all_summs", "[", "j", "]", "[", "i", "]", "[", "'pred_prob'", "]", "}", "\n", "", "aggregated", ".", "append", "(", "agg", ")", "\n", "\n", "# Combine them", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "ndocs_dir", ",", "'aggregated'", ",", "'summaries.json'", ")", "\n", "save_file", "(", "aggregated", ",", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.project_settings.DatasetConfig.__init__": [[20, 68], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "\n", "if", "name", "==", "'yelp'", ":", "\n", "            ", "self", ".", "review_max_len", "=", "150", "\n", "self", ".", "extractive_max_len", "=", "38", "# 99.5th percentile of reviews", "\n", "self", ".", "item_min_reviews", "=", "50", "\n", "self", ".", "item_max_reviews", "=", "260", "# 90th percentile", "\n", "self", ".", "vocab_size", "=", "32000", "# target vocab size when building subwordenc", "\n", "\n", "# Paths", "\n", "self", ".", "dir_path", "=", "'datasets/yelp_dataset/'", "\n", "self", ".", "reviews_path", "=", "'datasets/yelp_dataset/review.json'", "\n", "self", ".", "businesses_path", "=", "'datasets/yelp_dataset/business.json'", "\n", "self", ".", "processed_path", "=", "'datasets/yelp_dataset/processed/'", "\n", "self", ".", "subwordenc_path", "=", "'datasets/yelp_dataset/processed/subwordenc_32000_maxrevs260_fixed.pkl'", "\n", "\n", "# Trained models", "\n", "self", ".", "lm_path", "=", "'stable_checkpoints/lm/mlstm/yelp/batch_size_512-lm_lr_0.001-notes_data260_fixed/'", "'lm_e24_2.88.pt'", "\n", "self", ".", "clf_path", "=", "'stable_checkpoints/clf/cnn/yelp/batch_size_256-notes_data260_fixed/'", "'clf_e10_l0.6760_a0.7092.pt'", "\n", "self", ".", "sum_path", "=", "'stable_checkpoints/sum/mlstm/yelp/'", "'batch_size_16-notes_cycloss_honly-sum_lr_0.0005-tau_2.0/'", "'sum_e0_tot3.32_r1f0.27.pt'", "\n", "self", ".", "autoenc_path", "=", "'stable_checkpoints/sum/mlstm/yelp/'", "'autoenc_only_True-batch_size_16-sum_cycle_False-sum_lr_0.0005-tau_2.0/sum_e22_tot2.16_r1f0.03.pt'", "\n", "\n", "", "elif", "name", "==", "'amazon'", ":", "\n", "# Params", "\n", "            ", "self", ".", "review_max_len", "=", "150", "\n", "self", ".", "extractive_max_len", "=", "38", "# 99.5th percentile of reviews", "\n", "self", ".", "item_min_reviews", "=", "50", "\n", "self", ".", "item_max_reviews", "=", "260", "# 90th percentile", "\n", "self", ".", "vocab_size", "=", "32000", "# target vocab size when building subwordenc", "\n", "\n", "# Paths", "\n", "self", ".", "dir_path", "=", "'datasets/amazon_dataset/'", "\n", "self", ".", "processed_path", "=", "'datasets/amazon_dataset/processed/'", "\n", "self", ".", "subwordenc_path", "=", "'datasets/amazon_dataset/processed/subwordenc_32000_secondpass.pkl'", "\n", "\n", "# Trained models", "\n", "self", ".", "lm_path", "=", "'stable_checkpoints/lm/mlstm/amazon/batch_size_256-lm_lr_0.001/lm_e25_3.08.pt'", "\n", "self", ".", "clf_path", "=", "'stable_checkpoints/clf/cnn/amazon/batch_size_256-clf_lr_0.0001/'", "'clf_e15_l0.7415_a0.7115_d0.0000.pt'", "\n", "self", ".", "sum_path", "=", "'stable_checkpoints/sum/mlstm/amazon/batch_size_16-notes_both-sum_lr_0.0005-tau_2.0/'", "'sum_e1_tot4.14_r1f0.26.pt'", "\n", "self", ".", "autoenc_path", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.project_settings.HParams.__init__": [[71, 201], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "###############################################", "\n", "#", "\n", "# MODEL GENERAL", "\n", "#", "\n", "###############################################", "\n", "        ", "self", ".", "model_type", "=", "'mlstm'", "# mlstm, transformer", "\n", "self", ".", "emb_size", "=", "256", "\n", "self", ".", "hidden_size", "=", "512", "\n", "\n", "# transformer", "\n", "self", ".", "tsfr_blocks", "=", "6", "\n", "self", ".", "tsfr_ff_size", "=", "2048", "\n", "self", ".", "tsfr_nheads", "=", "8", "\n", "self", ".", "tsfr_dropout", "=", "0.1", "\n", "self", ".", "tsfr_tie_embs", "=", "False", "\n", "self", ".", "tsfr_label_smooth", "=", "0.1", "# range from [0.0, 1.0]; -1 means use regular CrossEntropyLoss", "\n", "\n", "# (m)lstm", "\n", "self", ".", "lstm_layers", "=", "1", "\n", "self", ".", "lstm_dropout", "=", "0.1", "\n", "self", ".", "lstm_ln", "=", "True", "# layer normalization", "\n", "\n", "# TextCNN", "\n", "self", ".", "cnn_filter_sizes", "=", "[", "3", ",", "4", ",", "5", "]", "\n", "self", ".", "cnn_n_feat_maps", "=", "128", "\n", "self", ".", "cnn_dropout", "=", "0.5", "\n", "\n", "#", "\n", "# Decoding (sampling words)", "\n", "#", "\n", "self", ".", "tau", "=", "2.0", "# temperature for softmax", "\n", "self", ".", "g_eps", "=", "1e-10", "# Gumbel softmax", "\n", "\n", "###############################################", "\n", "# SUMMARIZATION MODEL SPECIFIC", "\n", "###############################################", "\n", "self", ".", "sum_cycle", "=", "True", "# use cycle loss", "\n", "self", ".", "cycle_loss", "=", "'enc'", "# When 'rec', reconstruct original texts. When 'enc', compare rev_enc and sum_enc embs", "\n", "self", ".", "early_cycle", "=", "False", "# When True, compute CosSim b/n mean and individual representations", "\n", "self", ".", "extract_loss", "=", "False", "# use loss comparing summary to extractive summary", "\n", "self", ".", "autoenc_docs", "=", "True", "# add autoencoding loss", "\n", "self", ".", "autoenc_only", "=", "False", "# only perform autoencoding of reviews (would be used to pretrain)", "\n", "self", ".", "autoenc_docs_tie_dec", "=", "True", "# use same decoder for summaries and review autoencoder", "\n", "self", ".", "tie_enc", "=", "True", "# use same encoder for encoding documents and encoding summary", "\n", "self", ".", "sum_label_smooth", "=", "False", "# for autoenc_loss and reconstruction cycle_loss", "\n", "self", ".", "sum_label_smooth_val", "=", "0.1", "\n", "self", ".", "load_ae_freeze", "=", "False", "# load pretrained autoencoder and freeze", "\n", "self", ".", "cos_wgt", "=", "1.0", "# weight for cycle loss and early cycle loss", "\n", "self", ".", "cos_honly", "=", "True", "# compute cosine similarity losses using hiddens only, not hiddens + cells", "\n", "\n", "self", ".", "track_ppl", "=", "True", "# use a fixed (pretraind) language model to calculate NLL of summaries", "\n", "\n", "# Discriminator", "\n", "self", ".", "sum_discrim", "=", "False", "# add Discriminator loss", "\n", "self", ".", "wgan_lam", "=", "10.0", "\n", "self", ".", "discrim_lr", "=", "0.0001", "\n", "self", ".", "discrim_clip", "=", "5.0", "\n", "self", ".", "discrim_model", "=", "'cnn'", "\n", "self", ".", "discrim_onehot", "=", "True", "\n", "\n", "self", ".", "sum_clf", "=", "True", "# calculate classification loss and accuracy", "\n", "self", ".", "sum_clf_lr", "=", "0.0", "# when 0, don't backwards() etc", "\n", "\n", "self", ".", "sum_lr", "=", "0.0001", "\n", "self", ".", "sum_clip", "=", "5.0", "# clip gradients", "\n", "self", ".", "train_subset", "=", "1.0", "# train on this ratio of the training set (speed up experimentation, try to overfit)", "\n", "self", ".", "freeze_embed", "=", "True", "# don't further train embedding layers", "\n", "\n", "self", ".", "concat_docs", "=", "False", "# for one item, concatenate docs into long doc; else encode reviews separately", "\n", "self", ".", "combine_encs", "=", "'mean'", "# Combining separately encoded reviews: 'ff' for feedforward, 'mean' for mean, 'gru'", "\n", "self", ".", "combine_tie_hc", "=", "True", "# Use the same FF / GRU to combine the hidden states and cell states", "\n", "self", ".", "combine_encs_gru_bi", "=", "True", "# bidirectional gru to combine reviews", "\n", "self", ".", "combine_encs_gru_nlayers", "=", "1", "\n", "self", ".", "combine_encs_gru_dropout", "=", "0.1", "\n", "\n", "self", ".", "decay_tau", "=", "False", "\n", "self", ".", "decay_interval_size", "=", "1000", "\n", "self", ".", "decay_tau_alpha", "=", "0.1", "\n", "self", ".", "decay_tau_method", "=", "'minus'", "\n", "self", ".", "min_tau", "=", "0.4", "\n", "\n", "self", ".", "docs_attn", "=", "False", "\n", "self", ".", "docs_attn_hidden_size", "=", "32", "\n", "self", ".", "docs_attn_learn_alpha", "=", "True", "\n", "\n", "###############################################", "\n", "# LANGUAGE MODEL SPECIFIC", "\n", "###############################################", "\n", "self", ".", "lm_lr", "=", "0.0005", "\n", "self", ".", "lm_seq_len", "=", "256", "\n", "\n", "# language model and mlstm (transformer has its own schedule)", "\n", "self", ".", "lm_clip", "=", "5.0", "# clip gradients", "\n", "# decay at end of epoch", "\n", "self", ".", "lm_lr_decay", "=", "1.0", "# 1 = no decay for 'times'", "\n", "self", ".", "lm_lr_decay_method", "=", "'times'", "# 'times', 'minus'", "\n", "\n", "###############################################", "\n", "# CLASSIFIER SPECIFIC", "\n", "###############################################", "\n", "self", ".", "clf_lr", "=", "0.0001", "\n", "self", ".", "clf_clip", "=", "5.0", "\n", "self", ".", "clf_onehot", "=", "True", "\n", "self", ".", "clf_mse", "=", "False", "# treat as regression problem and use MSE instead of cross entropy", "\n", "\n", "###############################################", "\n", "# TRAINING AND DATA REPRESENTATION", "\n", "###############################################", "\n", "self", ".", "seed", "=", "1234", "\n", "self", ".", "batch_size", "=", "128", "\n", "self", ".", "n_docs", "=", "8", "\n", "self", ".", "n_docs_min", "=", "-", "1", "\n", "self", ".", "n_docs_max", "=", "-", "1", "\n", "self", ".", "max_nepochs", "=", "50", "\n", "self", ".", "notes", "=", "''", "# notes about run", "\n", "\n", "self", ".", "optim", "=", "'normal'", "# normal or noam", "\n", "self", ".", "noam_warmup", "=", "4000", "# number of warmup steps to linearly increase learning rate before decaying it", "\n", "\n", "#", "\n", "# UTILS / MISCELLANEOUS", "\n", "#", "\n", "self", ".", "debug", "=", "False", "\n", "\n", "###############################################", "\n", "# EVALUATION", "\n", "###############################################", "\n", "self", ".", "use_stemmer", "=", "True", "# when calculating rouge", "\n", "self", ".", "remove_stopwords", "=", "False", "# when calculating rouge", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.__init__": [[169, 171], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "f", ")", ":", "\n", "        ", "self", ".", "f", "=", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write": [[172, 175], ["utils.FlushFile.f.write", "utils.FlushFile.f.flush"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.flush"], ["", "def", "write", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "f", ".", "write", "(", "x", ")", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.flush": [[176, 183], ["utils.FlushFile.f.flush"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.flush"], ["", "def", "flush", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        If exception is thrown or Ctrl+C exits, python flushes all open files. This isn't necessary since\n        the buffer will be empty, but do this so AttributeError: 'FlushFile' object has no attribute 'flush'\n        isn't shown.\n        \"\"\"", "\n", "self", ".", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg": [[26, 30], ["float", "float"], "function", ["None"], ["def", "update_moving_avg", "(", "avg_so_far", ",", "new_val", ",", "n", ")", ":", "\n", "# First time, n = 1", "\n", "    ", "new_avg", "=", "avg_so_far", "*", "(", "n", "-", "1", ")", "/", "float", "(", "n", ")", "+", "new_val", "/", "float", "(", "n", ")", "\n", "return", "new_avg", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file": [[38, 54], ["os.path.dirname", "os.path.splitext", "os.path.isdir", "os.makedirs", "print", "open", "pickle.dump", "open", "json.dump", "f.write"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.dump", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.dump", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write"], ["", "def", "save_file", "(", "data", ",", "path", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "dir", "=", "os", ".", "path", ".", "dirname", "(", "path", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dir", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "'Saving: {}'", ".", "format", "(", "path", ")", ")", "\n", "\n", "", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "\n", "if", "ext", "==", "'.pkl'", ":", "\n", "        ", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "data", ",", "f", ",", "protocol", "=", "2", ")", "\n", "", "", "elif", "ext", "==", "'.json'", ":", "\n", "        ", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "4", ",", "separators", "=", "(", "','", ",", "': '", ")", ",", "sort_keys", "=", "True", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "# add trailing newline for POSIX compatibility", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file": [[56, 65], ["os.path.splitext", "open", "pickle.load", "open", "json.load"], "function", ["None"], ["", "", "", "def", "load_file", "(", "path", ",", "append_path", "=", "None", ")", ":", "\n", "    ", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "\n", "if", "ext", "==", "'.pkl'", ":", "\n", "        ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "", "elif", "ext", "==", "'.json'", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.sync_run_data_to_bigstore": [[73, 115], ["os.path.dirname", "os.path.basename", "os.path.join", "os.path.join", "subprocess.Popen", "print", "utils.sync_run_data_to_bigstore.execute_cmd"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.update_tensorboard.execute_cmd"], ["", "def", "sync_run_data_to_bigstore", "(", "run_dir", ",", "exp_sub_dir", "=", "''", ",", "method", "=", "'rsync'", ",", "tb_only", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Save everything but the (large) models to the Bigstore bucket periodically during training. This way\n    we can rsync locally and view the tensorboard results.\n\n    Args:\n        run_dir: str (e.g. checkpoints/sum/mlstm/yelp/<name-of-experiment>)\n        exp_sub_dir: str\n            sub directory within <bigstore> to save to, i.e.\n            checkpoints/sum/mlstm/yelp/<exp_sub_dir>/<name-of-experiment>\n        method: str ('cp' or 'rsync')\n        tb_only: boolean (only rsync the tensorboard directory\n    \"\"\"", "\n", "\n", "def", "execute_cmd", "(", "cmd", ")", ":", "\n", "        ", "p", "=", "subprocess", ".", "Popen", "(", "cmd", ".", "split", "(", ")", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "STDOUT", ")", "\n", "\n", "", "dirname", "=", "os", ".", "path", ".", "dirname", "(", "run_dir", ")", "# checkpoints/sum/mlstm/yelp/", "\n", "basename", "=", "os", ".", "path", ".", "basename", "(", "run_dir", ")", "# batch_size_4..", "\n", "bigstore_exp_dir", "=", "os", ".", "path", ".", "join", "(", "'gs://{}'", ".", "format", "(", "os", ".", "environ", "[", "'BS_UNSUP_BUCKET'", "]", ")", ",", "dirname", ",", "exp_sub_dir", ")", "\n", "bigstore_full_dir", "=", "os", ".", "path", ".", "join", "(", "bigstore_exp_dir", ",", "basename", ")", "\n", "# <bigstore>/checkpoints/sum/mlstm/yelp/<exp_sub_dir>/batch_size_4/..", "\n", "\n", "# First time -- we have to copy to create directory on Bigstore. We need a dummy file though", "\n", "# (Technically, there's no directories)", "\n", "if", "method", "==", "'cp'", ":", "\n", "        ", "if", "exp_sub_dir", "!=", "''", ":", "# create <exp_sub_dir> subdirectory", "\n", "            ", "Path", "(", "'/tmp/placeholder.txt'", ")", ".", "touch", "(", ")", "\n", "cmd", "=", "'gsutil cp /tmp/placeholder.txt {}/'", ".", "format", "(", "bigstore_exp_dir", ")", "\n", "print", "(", "cmd", ")", "\n", "execute_cmd", "(", "cmd", ")", "\n", "\n", "", "cmd", "=", "'gsutil cp -r  {} {}'", ".", "format", "(", "run_dir", ",", "bigstore_full_dir", ")", "\n", "print", "(", "cmd", ")", "\n", "execute_cmd", "(", "cmd", ")", "\n", "\n", "", "elif", "method", "==", "'rsync'", ":", "\n", "        ", "src_dir", "=", "os", ".", "path", ".", "join", "(", "run_dir", ",", "'tensorboard'", ")", "if", "tb_only", "else", "run_dir", "\n", "dest_dir", "=", "os", ".", "path", ".", "join", "(", "bigstore_full_dir", ",", "'tensorboard'", ")", "if", "tb_only", "else", "bigstore_full_dir", "\n", "cmd", "=", "\"gsutil rsync -r -x '.*\\.pt$' {} {}\"", ".", "format", "(", "src_dir", ",", "dest_dir", ")", "\n", "print", "(", "cmd", ")", "\n", "execute_cmd", "(", "cmd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.copy_tree_ignore_except": [[117, 156], ["print", "print", "print", "shutil.copytree", "os.path.basename", "os.path.isdir", "os.path.join", "c.endswith", "ignored.append"], "function", ["None"], ["", "", "def", "copy_tree_ignore_except", "(", "src_dir", ",", "dest_dir", ",", "\n", "file_exts", "=", "[", "'.py'", "]", ",", "\n", "ignore_dirs", "=", "[", "'checkpoints'", ",", "'external'", ",", "'datasets'", ",", "'stable_checkpoints'", ",", "'outputs'", "]", ")", ":", "\n", "    ", "\"\"\"\n    Same as shutil.copytree except that we only copy files have an extension found in file_exts\n\n    Args:\n        src_dir: str (path)\n        dest_dir: str (path)\n        file_exts: list of strs\n        ignore_dirs: list of strs\n    \"\"\"", "\n", "print", "(", "'Copying tree from \"{}\" to \"{}\"'", ".", "format", "(", "src_dir", ",", "dest_dir", ")", ")", "\n", "print", "(", "'Keeping only files with the following extensions: {}'", ".", "format", "(", "', '", ".", "join", "(", "file_exts", ")", ")", ")", "\n", "print", "(", "'Ignoring the following directories completely: {}'", ".", "format", "(", "', '", ".", "join", "(", "ignore_dirs", ")", ")", ")", "\n", "\n", "def", "ignore_filter", "(", "cur_dir", ",", "contents", ")", ":", "\n", "# contents are from os.listdir() and could be files or directories", "\n", "\n", "# ignore this directory completely", "\n", "        ", "if", "os", ".", "path", ".", "basename", "(", "cur_dir", ")", "in", "ignore_dirs", ":", "\n", "            ", "return", "contents", "\n", "\n", "", "ignored", "=", "[", "]", "\n", "for", "c", "in", "contents", ":", "\n", "            ", "if", "c", "in", "ignore_dirs", ":", "\n", "                ", "continue", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "c", ")", ")", ":", "# isn't a directory", "\n", "# ignore files that don't have desired extension", "\n", "                ", "ignore", "=", "True", "\n", "for", "ext", "in", "file_exts", ":", "\n", "                    ", "if", "c", ".", "endswith", "(", "ext", ")", ":", "\n", "                        ", "ignore", "=", "False", "\n", "", "", "if", "ignore", ":", "\n", "                    ", "ignored", ".", "append", "(", "c", ")", "\n", "", "", "", "return", "ignored", "\n", "\n", "# ignore is a callable that receives directory being visited, and list of its contents", "\n", "", "shutil", ".", "copytree", "(", "src_dir", ",", "dest_dir", ",", "ignore", "=", "ignore_filter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_run_data": [[185, 228], ["print", "os.path.isdir", "print", "os.mkdir", "os.path.join", "os.path.exists", "utils.copy_tree_ignore_except", "utils.save_file", "print", "input", "shutil.rmtree", "vars", "os.path.join", "open", "f.write", "datetime.datetime.now().strftime", "f.write", "f.write", "print", "print", "print", "shutil.rmtree", "print", "os.path.join", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.copy_tree_ignore_except", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write"], ["", "", "def", "save_run_data", "(", "path_to_dir", ",", "hp", ")", ":", "\n", "    ", "\"\"\"\n    1) Save stdout to file\n    2) Save files to path_to_dir:\n        - code_snapshot/: Snapshot of code (.py files)\n        - hp.json: dict of HParams object\n        - run_details.txt: command used and start time\n    \"\"\"", "\n", "print", "(", "'Saving run data to: {}'", ".", "format", "(", "path_to_dir", ")", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "path_to_dir", ")", ":", "\n", "        ", "print", "(", "'Data already exists in this directory (presumably from a previous run)'", ")", "\n", "inp", "=", "input", "(", "'Enter \"y\" if you are sure you want to remove all the old contents: '", ")", "\n", "if", "inp", "==", "'y'", ":", "\n", "            ", "print", "(", "'Removing old contents'", ")", "\n", "shutil", ".", "rmtree", "(", "path_to_dir", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Exiting'", ")", "\n", "raise", "SystemExit", "\n", "", "", "print", "(", "'Creating directory and saving data'", ")", "\n", "os", ".", "mkdir", "(", "path_to_dir", ")", "\n", "\n", "# Redirect stdout (print statements) to file", "\n", "# if not hp.debug:", "\n", "#     sys.stdout = FlushFile(open(os.path.join(path_to_dir, 'stdout.txt'), 'w'))", "\n", "\n", "# Save snapshot of code", "\n", "snapshot_dir", "=", "os", ".", "path", ".", "join", "(", "path_to_dir", ",", "'code_snapshot'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "snapshot_dir", ")", ":", "# shutil doesn't work if dest already exists", "\n", "        ", "shutil", ".", "rmtree", "(", "snapshot_dir", ")", "\n", "", "copy_tree_ignore_except", "(", "'.'", ",", "snapshot_dir", ")", "\n", "\n", "# Save hyperparms", "\n", "save_file", "(", "vars", "(", "hp", ")", ",", "os", ".", "path", ".", "join", "(", "path_to_dir", ",", "'hp.json'", ")", ",", "verbose", "=", "True", ")", "\n", "\n", "# Save some command used to run, start time", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path_to_dir", ",", "'run_details.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'Command:\\n'", ")", "\n", "cmd", "=", "' '", ".", "join", "(", "sys", ".", "argv", ")", "\n", "start_time", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%B%d_%H-%M-%S'", ")", "\n", "f", ".", "write", "(", "cmd", "+", "'\\n'", ")", "\n", "f", ".", "write", "(", "'Start time: {}'", ".", "format", "(", "start_time", ")", ")", "\n", "print", "(", "'Command used to start program:\\n'", ",", "cmd", ")", "\n", "print", "(", "'Start time: {}'", ".", "format", "(", "start_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.create_argparse_and_update_hp": [[230, 269], ["argparse.ArgumentParser", "vars().items", "argparse.ArgumentParser.parse_known_args", "vars().items", "type", "argparse.ArgumentParser.add_argument", "sorted", "v.lower", "vars", "vars", "setattr", "v.lower", "argparse.ArgumentTypeError", "run_name.append", "str", "uuid.uuid4"], "function", ["None"], ["", "", "def", "create_argparse_and_update_hp", "(", "hp", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        hp: instance of HParams object\n\n    Returns:\n        (updated) hp\n        run_name: str (can be used to create directory and store training results)\n        parser: argparse object (can be used to add more arguments)\n    \"\"\"", "\n", "\n", "def", "str2bool", "(", "v", ")", ":", "\n", "        ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "            ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n", "# Create argparse with an option for every param in hp", "\n", "", "", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "for", "param", ",", "default_value", "in", "vars", "(", "hp", ")", ".", "items", "(", ")", ":", "\n", "        ", "param_type", "=", "type", "(", "default_value", ")", "\n", "param_type", "=", "str2bool", "if", "param_type", "==", "bool", "else", "param_type", "\n", "parser", ".", "add_argument", "(", "'--{}'", ".", "format", "(", "param", ")", ",", "dest", "=", "param", ",", "default", "=", "None", ",", "type", "=", "param_type", ")", "\n", "", "opt", ",", "unknown", "=", "parser", ".", "parse_known_args", "(", ")", "\n", "\n", "# Update hp if any command line arguments passed", "\n", "# Also create description of run", "\n", "run_name", "=", "[", "]", "\n", "for", "param", ",", "value", "in", "vars", "(", "opt", ")", ".", "items", "(", ")", ":", "\n", "        ", "if", "value", "is", "not", "None", ":", "\n", "            ", "setattr", "(", "hp", ",", "param", ",", "value", ")", "\n", "if", "param", "!=", "'model_type'", ":", "\n", "                ", "run_name", ".", "append", "(", "'{}_{}'", ".", "format", "(", "param", ",", "value", ")", ")", "\n", "", "", "", "run_name", "=", "'-'", ".", "join", "(", "sorted", "(", "run_name", ")", ")", "\n", "run_name", "=", "(", "'default_'", "+", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "[", ":", "8", "]", ")", "if", "(", "run_name", "==", "''", ")", "else", "run_name", "\n", "\n", "return", "hp", ",", "run_name", ",", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.generate_from_lm.batchify": [[72, 90], ["data.view().contiguous.narrow", "data.view().contiguous.view().contiguous", "data.view().contiguous.size", "data.view().contiguous.view"], "function", ["None"], ["", "def", "batchify", "(", "data", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        data: 1D Tensor\n        batch_size: int\n    Returns:\n        data: reshaped Tensor of size (batch_size, -1)\n        Example where data is non-negative integers and batch_size = 4\n        [[0  1  2  3  4  5  6 ]\n         [7  8  9  10 11 12 13]\n         [14 15 16 17 18 19 20]\n         [21 22 23 24 25 26 27]]\n    Note: not currently using this anymore. Was used when reading in data from text fileW\n    \"\"\"", "\n", "nbatch", "=", "data", ".", "size", "(", "0", ")", "//", "batch_size", "\n", "data", "=", "data", ".", "narrow", "(", "0", ",", "0", ",", "nbatch", "*", "batch_size", ")", "# same as slice", "\n", "data", "=", "data", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpPytorchDataset.__init__": [[38, 172], ["project_settings.DatasetConfig", "numpy.random.seed", "yelp_dataset.YelpPytorchDataset.load_all_items", "utils.load_file", "len", "float", "os.path.join", "utils.load_file.items", "int", "range", "utils.load_file.items", "min", "utils.load_file.items", "len", "set", "utils.load_file.values", "float", "math.ceil", "numpy.mean", "math.ceil", "range", "len", "random.choice", "set().difference", "numpy.random.choice", "set.update", "range", "math.floor", "list", "set", "utils.load_file.values", "range"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpPytorchDataset.load_all_items", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file"], ["def", "__init__", "(", "self", ",", "\n", "split", "=", "None", ",", "\n", "n_reviews", "=", "None", ",", "\n", "n_reviews_min", "=", "None", ",", "\n", "n_reviews_max", "=", "None", ",", "\n", "subset", "=", "None", ",", "\n", "seed", "=", "0", ",", "\n", "sample_reviews", "=", "True", ",", "\n", "item_max_reviews", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            split: str ('train', val', 'test')\n            n_reviews: int\n\n            n_reviews_min: int\n            n_reviews_max: int\n                - When these two are provided, then there will be variable n_reviews (i.e. two different\n                training examples may be composed of different number of reviews to summarize)\n                - Some of this\n\n            subset: float (Value in [0.0, 1.0]. If given, then dataset is truncated to subset of the businesses\n            seed: int (set seed because we will be using np.random.choice to sample reviews if sample_reviews=True)\n            sample_reviews: boolean\n                - When True, __getitem_ will sample n_reviews reviews for each item. The number of times a item appears\n                in the dataset is dependent on uniform_items.\n                - When False, each item will appear math.floor(number of reviews item has / n_reviews) times\n                so that almost every review is seen (with up to n_reviews - 1 reviews not seen).\n                    - Setting False is useful for (a) validation / test, and (b) simply iterating over all the reviews\n                    (e.g. to build the vocabulary).\n            item_max_reviews: int (maximum number of reviews a item can have)\n                - This is used to remove outliers from the data. This is especially important if uniform_items=False,\n                as there may be a large number of reviews in a training epoch coming from a single item. This also\n                still matters when uniform_items=True, as items an outlier number of reviews will have reviews\n                that are never sampled.\n                - For the Yelp dataset, there are 11,870 items in the training set with at least 50 reviews\n                no longer than 150 subtokens. The breakdown of the distribution in the training set is:\n                    Percentile  |  percentile_n_reviews  |  n_items  |  total_revs\n                        50                  89                5945         391829\n                        75                  150               8933         733592\n                        90                  260               10695        1075788\n                        95                  375               11278        1255540\n                        99                  817               11751        1503665\n                        99.5                1090              11810        1558673\n                        99.9                1943              11858        1626489\n        \"\"\"", "\n", "self", ".", "split", "=", "split", "\n", "\n", "self", ".", "n_reviews", "=", "n_reviews", "\n", "self", ".", "n_reviews_min", "=", "n_reviews_min", "\n", "self", ".", "n_reviews_max", "=", "n_reviews_max", "\n", "\n", "self", ".", "subset", "=", "subset", "\n", "self", ".", "sample_reviews", "=", "sample_reviews", "\n", "item_max_reviews", "=", "float", "(", "'inf'", ")", "if", "item_max_reviews", "is", "None", "else", "item_max_reviews", "\n", "self", ".", "item_max_reviews", "=", "item_max_reviews", "\n", "\n", "self", ".", "ds_conf", "=", "DatasetConfig", "(", "'yelp'", ")", "# used for paths", "\n", "\n", "# Set random seed so that choice is always the same across experiments", "\n", "# Especially necessary for test set (along with shuffle=False in the DataLoader)", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "self", ".", "items", "=", "self", ".", "load_all_items", "(", ")", "\n", "\n", "# Create map from idx-th data point to item", "\n", "item_to_nreviews", "=", "load_file", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "ds_conf", ".", "processed_path", ",", "'{}/store-to-nreviews.json'", ".", "format", "(", "split", ")", ")", ")", "\n", "self", ".", "idx_to_item", "=", "{", "}", "\n", "\n", "if", "sample_reviews", ":", "\n", "            ", "if", "n_reviews_min", "and", "n_reviews_max", ":", "\n", "                ", "self", ".", "idx_to_nreviews", "=", "{", "}", "\n", "self", ".", "idx_to_item_idxs", "=", "{", "}", "# indices of reviews", "\n", "\n", "ns", "=", "[", "4", ",", "8", ",", "16", "]", "\n", "# ns = range(n_reviews_min, n_reviews_max+1, 4)  # e.g. [4,8,12,16]", "\n", "idx", "=", "0", "\n", "for", "item", ",", "n_reviews", "in", "item_to_nreviews", ".", "items", "(", ")", ":", "\n", "                    ", "item_n", "=", "0", "\n", "selected_idxs", "=", "set", "(", ")", "\n", "while", "item_n", "<", "n_reviews", ":", "\n", "# Keep selecting batches of reviews from this store (without replacement)", "\n", "                        ", "cur_n", "=", "random", ".", "choice", "(", "ns", ")", "\n", "if", "item_n", "+", "cur_n", ">", "n_reviews", ":", "\n", "                            ", "break", "\n", "", "available_idxs", "=", "set", "(", "range", "(", "n_reviews", ")", ")", ".", "difference", "(", "selected_idxs", ")", "\n", "cur_idxs", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "available_idxs", ")", ",", "cur_n", ",", "replace", "=", "False", ")", "\n", "selected_idxs", ".", "update", "(", "cur_idxs", ")", "\n", "\n", "# update", "\n", "self", ".", "idx_to_item", "[", "idx", "]", "=", "item", "\n", "self", ".", "idx_to_nreviews", "[", "idx", "]", "=", "cur_n", "\n", "self", ".", "idx_to_item_idxs", "[", "idx", "]", "=", "cur_idxs", "\n", "item_n", "+=", "cur_n", "\n", "idx", "+=", "1", "\n", "\n", "", "", "", "else", ":", "\n", "# Get the number of times each item will appear in a pass through this dataset", "\n", "                ", "item_min_reviews", "=", "min", "(", "item_to_nreviews", ".", "values", "(", ")", ")", "\n", "if", "item_max_reviews", "==", "float", "(", "'inf'", ")", ":", "\n", "                    ", "n_per_item", "=", "math", ".", "ceil", "(", "item_min_reviews", "/", "n_reviews", ")", "\n", "", "else", ":", "\n", "                    ", "n_per_item", "=", "np", ".", "mean", "(", "[", "n", "for", "n", "in", "item_to_nreviews", ".", "values", "(", ")", "if", "n", "<=", "item_max_reviews", "]", ")", "\n", "n_per_item", "=", "math", ".", "ceil", "(", "n_per_item", "/", "n_reviews", ")", "\n", "# print('Each item will appear {} times'.format(n_per_item))", "\n", "\n", "", "idx", "=", "0", "\n", "for", "item", ",", "n_reviews", "in", "item_to_nreviews", ".", "items", "(", ")", ":", "\n", "                    ", "if", "n_reviews", "<=", "item_max_reviews", ":", "\n", "                        ", "for", "_", "in", "range", "(", "n_per_item", ")", ":", "\n", "                            ", "self", ".", "idx_to_item", "[", "idx", "]", "=", "item", "\n", "idx", "+=", "1", "\n", "", "", "", "", "", "else", ":", "\n", "# __getitem__ will not sample", "\n", "            ", "idx", "=", "0", "\n", "self", ".", "idx_to_item_startidx", "=", "{", "}", "\n", "# idx items idx of one dataset item. item_startidx is the idx within that item's reviews.", "\n", "tot", "=", "0", "\n", "for", "item", ",", "item_n_reviews", "in", "item_to_nreviews", ".", "items", "(", ")", ":", "\n", "                ", "if", "item_n_reviews", "<=", "item_max_reviews", ":", "\n", "                    ", "tot", "+=", "item_n_reviews", "\n", "item_startidx", "=", "0", "\n", "for", "_", "in", "range", "(", "math", ".", "floor", "(", "item_n_reviews", "/", "n_reviews", ")", ")", ":", "\n", "                        ", "self", ".", "idx_to_item", "[", "idx", "]", "=", "item", "\n", "self", ".", "idx_to_item_startidx", "[", "idx", "]", "=", "item_startidx", "\n", "idx", "+=", "1", "\n", "item_startidx", "+=", "n_reviews", "\n", "\n", "", "", "", "", "if", "self", ".", "subset", ":", "\n", "            ", "end", "=", "int", "(", "self", ".", "subset", "*", "len", "(", "self", ".", "idx_to_item", ")", ")", "\n", "for", "idx", "in", "range", "(", "end", ",", "len", "(", "self", ".", "idx_to_item", ")", ")", ":", "\n", "                ", "del", "self", ".", "idx_to_item", "[", "idx", "]", "\n", "\n", "", "", "self", ".", "n", "=", "len", "(", "self", ".", "idx_to_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpPytorchDataset.load_all_items": [[173, 184], ["print", "open", "f.readlines", "json.loads"], "methods", ["None"], ["", "def", "load_all_items", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return dictionary from item id to dict\n        \"\"\"", "\n", "print", "(", "'Loading all items'", ")", "\n", "items", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "ds_conf", ".", "businesses_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "                ", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "items", "[", "line", "[", "'business_id'", "]", "]", "=", "line", "\n", "", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpPytorchDataset.__getitem__": [[185, 228], ["os.path.join", "utils.load_file", "zip", "data_loaders.summ_dataset.SummDataset.concat_docs", "int", "numpy.round", "numpy.mean", "print", "len", "numpy.random.choice", "numpy.random.choice"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.concat_docs"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# Map idx to item and load reviews", "\n", "        ", "item", "=", "self", ".", "idx_to_item", "[", "idx", "]", "# id", "\n", "fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "ds_conf", ".", "processed_path", ",", "'{}/{}_reviews.json'", ".", "format", "(", "self", ".", "split", ",", "item", ")", ")", "\n", "reviews", "=", "load_file", "(", "fp", ")", "\n", "\n", "# Get reviews from item", "\n", "if", "self", ".", "sample_reviews", ":", "\n", "            ", "if", "self", ".", "n_reviews_min", "and", "self", ".", "n_reviews_max", ":", "\n", "                ", "review_idxs", "=", "self", ".", "idx_to_item_idxs", "[", "idx", "]", "\n", "reviews", "=", "[", "reviews", "[", "r_idx", "]", "for", "r_idx", "in", "review_idxs", "]", "\n", "", "else", ":", "\n", "                ", "if", "len", "(", "reviews", ")", "<", "self", ".", "n_reviews", ":", "\n", "                    ", "reviews", "=", "np", ".", "random", ".", "choice", "(", "reviews", ",", "size", "=", "self", ".", "n_reviews", ",", "replace", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "reviews", "=", "np", ".", "random", ".", "choice", "(", "reviews", ",", "size", "=", "self", ".", "n_reviews", ",", "replace", "=", "False", ")", "\n", "", "", "", "else", ":", "\n", "            ", "start_idx", "=", "self", ".", "idx_to_item_startidx", "[", "idx", "]", "\n", "reviews", "=", "reviews", "[", "start_idx", ":", "start_idx", "+", "self", ".", "n_reviews", "]", "\n", "\n", "# Collect data for this item", "\n", "", "texts", ",", "ratings", "=", "zip", "(", "*", "[", "(", "s", "[", "'text'", "]", ",", "s", "[", "'stars'", "]", ")", "for", "s", "in", "reviews", "]", ")", "\n", "texts", "=", "SummDataset", ".", "concat_docs", "(", "texts", ",", "edok_token", "=", "True", ")", "\n", "avg_rating", "=", "int", "(", "np", ".", "round", "(", "np", ".", "mean", "(", "ratings", ")", ")", ")", "\n", "\n", "try", ":", "\n", "            ", "categories", "=", "'---'", ".", "join", "(", "self", ".", "items", "[", "item", "]", "[", "'categories'", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "e", ")", "\n", "categories", "=", "'---'", "\n", "", "metadata", "=", "{", "'item'", ":", "item", ",", "\n", "'city'", ":", "self", ".", "items", "[", "item", "]", "[", "'city'", "]", ",", "\n", "'categories'", ":", "categories", "}", "\n", "\n", "# try:", "\n", "#     metadata = {'item': item,", "\n", "#                 'city': self.items[item]['city'],", "\n", "#                 'categories': '---'.join(self.items[item]['categories'])}", "\n", "# except Exception as e:", "\n", "#     print(e)", "\n", "#     pdb.set_trace()", "\n", "\n", "return", "texts", ",", "avg_rating", ",", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpPytorchDataset.__len__": [[229, 231], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.VariableNDocsSampler.__init__": [[242, 277], ["torch.utils.data.sampler.Sampler.__init__", "collections.defaultdict", "dataset.idx_to_nreviews.items", "torch.cuda.device_count", "collections.defaultdict.items", "random.shuffle", "nreviews_to_idxs[].append", "int", "print", "set", "len", "set().difference", "numpy.random.choice", "dataloader_idxs.append", "set.update", "set().difference", "list", "set", "set"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["def", "__init__", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "super", "(", "VariableNDocsSampler", ",", "self", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n", "# Group data points together by how the number of reviews", "\n", "# This way the SummarizationModel will be fed a batch of points, each summarizing", "\n", "# the same number of reviews. This is important as the model reshapes tensors", "\n", "# by n_docs, which allows it to be done in parallel.", "\n", "nreviews_to_idxs", "=", "defaultdict", "(", "list", ")", "\n", "for", "idx", ",", "nreviews", "in", "dataset", ".", "idx_to_nreviews", ".", "items", "(", ")", ":", "\n", "            ", "nreviews_to_idxs", "[", "nreviews", "]", ".", "append", "(", "idx", ")", "\n", "\n", "# This is hard-coded: the summarization model I've been training takes about", "\n", "# 10 GB on one GPU for batch_size = 4 and n_docs = 8. We'll scale the batch_size", "\n", "# relative to the n_docs for the given minibatch such that", "\n", "# batch_size * n_docs  / ngpus = 32, so as to use all the GPU memory as much", "\n", "# as possible. For n_docs_min=4 and n_docs_max=16, n_docs is in [4,8,12,16]", "\n", "# (this is hard-coded in currently in YelpPytorchDataset).", "\n", "", "ngpus", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "dataloader_idxs", "=", "[", "]", "# list of lists of indices, each sublist is a minibatch", "\n", "for", "nreviews", ",", "idxs", "in", "nreviews_to_idxs", ".", "items", "(", ")", ":", "\n", "            ", "batch_size", "=", "int", "(", "32", "/", "nreviews", "*", "ngpus", ")", "\n", "print", "(", "nreviews", ",", "batch_size", ")", "\n", "selected_idxs", "=", "set", "(", ")", "\n", "while", "len", "(", "set", "(", "idxs", ")", ".", "difference", "(", "selected_idxs", ")", ")", ">", "batch_size", ":", "\n", "# There is enough unselected points to form a batch", "\n", "                ", "available_idxs", "=", "set", "(", "idxs", ")", ".", "difference", "(", "selected_idxs", ")", "\n", "cur_idxs", "=", "np", ".", "random", ".", "choice", "(", "list", "(", "available_idxs", ")", ",", "batch_size", ",", "replace", "=", "False", ")", "\n", "dataloader_idxs", ".", "append", "(", "cur_idxs", ")", "\n", "selected_idxs", ".", "update", "(", "cur_idxs", ")", "\n", "\n", "", "", "random", ".", "shuffle", "(", "dataloader_idxs", ")", "\n", "\n", "self", ".", "dataloader_idxs", "=", "dataloader_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.VariableNDocsSampler.__iter__": [[278, 280], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "dataloader_idxs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.VariableNDocsSampler.__len__": [[281, 283], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataloader_idxs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.__init__": [[289, 296], ["data_loaders.summ_dataset.SummReviewDataset.__init__", "project_settings.DatasetConfig", "utils.load_file"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "YelpDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "'yelp'", "\n", "self", ".", "conf", "=", "DatasetConfig", "(", "'yelp'", ")", "\n", "self", ".", "n_ratings_labels", "=", "5", "\n", "self", ".", "reviews", "=", "None", "\n", "self", ".", "subwordenc", "=", "load_file", "(", "self", ".", "conf", ".", "subwordenc_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.load_all_reviews": [[302, 312], ["print", "open", "f.readlines", "reviews.append", "json.loads"], "methods", ["None"], ["", "def", "load_all_reviews", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return list of dictionaries\n        \"\"\"", "\n", "print", "(", "'Loading all reviews'", ")", "\n", "reviews", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "conf", ".", "reviews_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "                ", "reviews", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "", "", "return", "reviews", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.get_data_loader": [[313, 331], ["yelp_dataset.YelpPytorchDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "yelp_dataset.VariableNDocsSampler"], "methods", ["None"], ["", "def", "get_data_loader", "(", "self", ",", "split", "=", "'train'", ",", "\n", "n_docs", "=", "8", ",", "n_docs_min", "=", "None", ",", "n_docs_max", "=", "None", ",", "\n", "subset", "=", "None", ",", "seed", "=", "0", ",", "sample_reviews", "=", "True", ",", "\n", "category", "=", "None", ",", "# for compatability with AmazonDataset, which filters in AmazonPytorchDataset", "\n", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        Return iterator over specific split in dataset\n        \"\"\"", "\n", "ds", "=", "YelpPytorchDataset", "(", "split", "=", "split", ",", "\n", "n_reviews", "=", "n_docs", ",", "n_reviews_min", "=", "n_docs_min", ",", "n_reviews_max", "=", "n_docs_max", ",", "\n", "subset", "=", "subset", ",", "seed", "=", "seed", ",", "sample_reviews", "=", "sample_reviews", ",", "\n", "item_max_reviews", "=", "self", ".", "conf", ".", "item_max_reviews", ")", "\n", "\n", "if", "n_docs_min", "and", "n_docs_max", ":", "\n", "            ", "loader", "=", "DataLoader", "(", "ds", ",", "batch_sampler", "=", "VariableNDocsSampler", "(", "ds", ")", ",", "num_workers", "=", "num_workers", ")", "\n", "", "else", ":", "\n", "            ", "loader", "=", "DataLoader", "(", "ds", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.save_processed_splits": [[337, 407], ["print", "print", "collections.defaultdict", "sum", "print", "print", "print", "list", "sum", "print", "print", "enumerate", "print", "print", "print", "split_to_item_to_nreviews.items", "yelp_dataset.YelpDataset.load_all_reviews", "collections.defaultdict.keys", "len", "item_to_n.values", "int", "int", "int", "sorted", "os.path.join", "utils.save_file", "os.path.join", "utils.save_file", "len", "item_to_reviews[].append", "len", "len", "len", "item_to_n.items", "yelp_dataset.YelpDataset.subwordenc.encode", "collections.defaultdict.values"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "save_processed_splits", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Save train, val, and test splits. Splits are across items (e.g. a item is either in train, val, or test).\n        Iterates over all reviews in the original dataset. Tries to get close to a 80-10-10 split.\n\n        Args:\n            review_max_len: int (maximum length in subtokens a review can be)\n            item_min_reviews: int (min number of reviews a item must have)\n        \"\"\"", "\n", "review_max_len", "=", "self", ".", "conf", ".", "review_max_len", "\n", "item_min_reviews", "=", "self", ".", "conf", ".", "item_min_reviews", "\n", "\n", "print", "(", "'Saving processed splits'", ")", "\n", "if", "self", ".", "reviews", "is", "None", ":", "\n", "            ", "self", ".", "reviews", "=", "self", ".", "load_all_reviews", "(", ")", "\n", "\n", "", "print", "(", "'Filtering reviews longer than: {}'", ".", "format", "(", "review_max_len", ")", ")", "\n", "item_to_reviews", "=", "defaultdict", "(", "list", ")", "\n", "for", "r", "in", "self", ".", "reviews", ":", "\n", "            ", "if", "len", "(", "self", ".", "subwordenc", ".", "encode", "(", "r", "[", "'text'", "]", ")", ")", "<", "review_max_len", ":", "\n", "                ", "item_to_reviews", "[", "r", "[", "'business_id'", "]", "]", ".", "append", "(", "r", ")", "\n", "\n", "# Calculate target amount of reviews per item", "\n", "", "", "n", "=", "sum", "(", "[", "len", "(", "revs", ")", "for", "revs", "in", "item_to_reviews", ".", "values", "(", ")", "]", ")", "\n", "print", "(", "'Total number of reviews before filtering: {}'", ".", "format", "(", "len", "(", "self", ".", "reviews", ")", ")", ")", "\n", "print", "(", "'Total number of reviews after filtering: {}'", ".", "format", "(", "n", ")", ")", "\n", "\n", "print", "(", "'Filtering items with less than {} reviews'", ".", "format", "(", "item_min_reviews", ")", ")", "\n", "item_to_n", "=", "{", "}", "\n", "for", "item", "in", "list", "(", "item_to_reviews", ".", "keys", "(", ")", ")", ":", "# have to do list and keys for python3 to delete in-place", "\n", "# for item, reviews in item_to_reviews.items():", "\n", "            ", "n", "=", "len", "(", "item_to_reviews", "[", "item", "]", ")", "\n", "if", "n", "<", "item_min_reviews", ":", "\n", "                ", "del", "item_to_reviews", "[", "item", "]", "\n", "", "else", ":", "\n", "                ", "item_to_n", "[", "item", "]", "=", "n", "\n", "", "", "n", "=", "sum", "(", "item_to_n", ".", "values", "(", ")", ")", "\n", "print", "(", "'Total number of reviews after filtering: {}'", ".", "format", "(", "n", ")", ")", "\n", "print", "(", "'Total number of items after filtering: {}'", ".", "format", "(", "len", "(", "item_to_n", ")", ")", ")", "\n", "\n", "# Construct splits", "\n", "n_tr", ",", "n_val", ",", "n_te", "=", "int", "(", "0.8", "*", "n", ")", ",", "int", "(", "0.1", "*", "n", ")", ",", "int", "(", "0.1", "*", "n", ")", "\n", "cur_n_tr", ",", "cur_n_val", ",", "cur_n_te", "=", "0", ",", "0", ",", "0", "\n", "split_to_item_to_nreviews", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", "}", "\n", "# In descending order of number of reviews per item", "\n", "for", "i", ",", "(", "item", ",", "n", ")", "in", "enumerate", "(", "sorted", "(", "item_to_n", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", ")", ":", "\n", "# once every ten items, save to val / test if we haven't yet hit the target number", "\n", "            ", "if", "(", "i", "%", "10", "==", "8", ")", "and", "(", "cur_n_val", "<", "n_val", ")", ":", "\n", "                ", "split", "=", "'val'", "\n", "cur_n_val", "+=", "n", "\n", "", "elif", "(", "i", "%", "10", "==", "9", ")", "and", "(", "cur_n_te", "<", "n_te", ")", ":", "\n", "                ", "split", "=", "'test'", "\n", "cur_n_te", "+=", "n", "\n", "", "else", ":", "\n", "                ", "split", "=", "'train'", "\n", "cur_n_tr", "+=", "n", "\n", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "conf", ".", "processed_path", ",", "'{}/{}_reviews.json'", ".", "format", "(", "split", ",", "item", ")", ")", "\n", "save_file", "(", "item_to_reviews", "[", "item", "]", ",", "out_fp", ",", "verbose", "=", "False", ")", "\n", "\n", "split_to_item_to_nreviews", "[", "split", "]", "[", "item", "]", "=", "n", "\n", "\n", "", "print", "(", "'Number of train reviews: {} / {}'", ".", "format", "(", "cur_n_tr", ",", "n_tr", ")", ")", "\n", "print", "(", "'Number of val reviews: {} / {}'", ".", "format", "(", "cur_n_val", ",", "n_val", ")", ")", "\n", "print", "(", "'Number of test reviews: {} / {}'", ".", "format", "(", "cur_n_te", ",", "n_te", ")", ")", "\n", "\n", "# This file is used by YelpPytorchDataset", "\n", "for", "split", ",", "item_to_nreviews", "in", "split_to_item_to_nreviews", ".", "items", "(", ")", ":", "\n", "            ", "out_fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "conf", ".", "processed_path", ",", "'{}/store-to-nreviews.json'", ".", "format", "(", "split", ")", ")", "\n", "save_file", "(", "item_to_nreviews", ",", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.print_original_data_stats": [[408, 454], ["set", "set", "collections.defaultdict", "set", "print", "print", "print", "print", "sorted", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "pdb.set_trace", "yelp_dataset.YelpDataset.load_all_reviews", "set.add", "set.add", "int", "int", "int", "yelp_dataset.YelpDataset.subwordenc.encode", "review_lens.append", "set.update", "print", "collections.defaultdict.items", "print", "len", "len", "len", "len", "len", "numpy.mean", "numpy.median", "numpy.percentile", "numpy.percentile", "len", "float", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "", "def", "print_original_data_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and print some statistics on the original dataset\n        \"\"\"", "\n", "businesses", "=", "set", "(", ")", "\n", "users", "=", "set", "(", ")", "\n", "rating_to_count", "=", "defaultdict", "(", "int", ")", "\n", "n_useful", ",", "n_funny", ",", "n_cool", "=", "0", ",", "0", ",", "0", "# reviews marked as useful, funny, or cool", "\n", "review_lens", "=", "[", "]", "\n", "tokens", "=", "set", "(", ")", "\n", "\n", "if", "self", ".", "reviews", "is", "None", ":", "\n", "            ", "self", ".", "reviews", "=", "self", ".", "load_all_reviews", "(", ")", "\n", "\n", "", "for", "r", "in", "self", ".", "reviews", ":", "\n", "            ", "businesses", ".", "add", "(", "r", "[", "'review_id'", "]", ")", "\n", "users", ".", "add", "(", "r", "[", "'user_id'", "]", ")", "\n", "rating_to_count", "[", "r", "[", "'stars'", "]", "]", "+=", "1", "\n", "n_useful", "+=", "int", "(", "r", "[", "'useful'", "]", "!=", "0", ")", "\n", "n_funny", "+=", "int", "(", "r", "[", "'funny'", "]", "!=", "0", ")", "\n", "n_cool", "+=", "int", "(", "r", "[", "'cool'", "]", "!=", "0", ")", "\n", "\n", "tokenized", "=", "self", ".", "subwordenc", ".", "encode", "(", "r", "[", "'text'", "]", ")", "\n", "review_lens", ".", "append", "(", "len", "(", "tokenized", ")", ")", "\n", "# tokenized = nltk.word_tokenize(r['text'].lower())", "\n", "# review_lens.append(len(r['text']))", "\n", "tokens", ".", "update", "(", "tokenized", ")", "\n", "print", "(", "len", "(", "tokenized", ")", ")", "\n", "\n", "", "print", "(", "'Total number of reviews: {}'", ".", "format", "(", "len", "(", "self", ".", "reviews", ")", ")", ")", "\n", "print", "(", "'Number of unique businesses: {}'", ".", "format", "(", "len", "(", "businesses", ")", ")", ")", "\n", "print", "(", "'Number of unique users: {}'", ".", "format", "(", "len", "(", "users", ")", ")", ")", "\n", "print", "(", "'Number of reviews per star rating:'", ")", "\n", "for", "stars", ",", "count", "in", "sorted", "(", "rating_to_count", ".", "items", "(", ")", ")", ":", "\n", "            ", "print", "(", "'-- {} stars: {:.2f} reviews; {} of dataset'", ".", "format", "(", "stars", ",", "count", ",", "float", "(", "count", ")", "/", "len", "(", "self", ".", "reviews", ")", ")", ")", "\n", "", "print", "(", "'Number of reviews marked as:'", ")", "\n", "print", "(", "'-- useful: {}'", ".", "format", "(", "n_useful", ")", ")", "\n", "print", "(", "'-- funny: {}'", ".", "format", "(", "n_funny", ")", ")", "\n", "print", "(", "'-- cool: {}'", ".", "format", "(", "n_cool", ")", ")", "\n", "print", "(", "'Length of review:'", ")", "\n", "print", "(", "'-- mean: {}'", ".", "format", "(", "np", ".", "mean", "(", "review_lens", ")", ")", ")", "\n", "print", "(", "'-- median: {}'", ".", "format", "(", "np", ".", "median", "(", "review_lens", ")", ")", ")", "\n", "print", "(", "'-- 75th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "review_lens", ",", "75", ")", ")", ")", "\n", "print", "(", "'-- 90th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "review_lens", ",", "90", ")", ")", ")", "\n", "print", "(", "'Number of unique tokens: {}'", ".", "format", "(", "len", "(", "tokens", ")", ")", ")", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.yelp_dataset.YelpDataset.print_filtered_data_stats": [[455, 479], ["collections.defaultdict", "print", "sorted", "print", "print", "print", "print", "yelp_dataset.YelpDataset.get_data_loader", "collections.defaultdict.items", "print", "enumerate", "numpy.mean", "numpy.percentile", "numpy.percentile", "all_rev_lens.append", "len", "float", "len", "yelp_dataset.YelpDataset.subwordenc.encode", "ratings[].item"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "print_filtered_data_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and print some statistics on the filtered dataset. This is what we use for\n        training, validation, and testing.\n        \"\"\"", "\n", "\n", "all_rev_lens", "=", "[", "]", "\n", "rating_to_count", "=", "defaultdict", "(", "int", ")", "\n", "for", "split", "in", "[", "'train'", ",", "'val'", ",", "'test'", "]", ":", "\n", "            ", "dl", "=", "self", ".", "get_data_loader", "(", "split", "=", "split", ",", "n_reviews", "=", "1", ",", "sample_reviews", "=", "False", ",", "\n", "batch_size", "=", "1", ",", "num_workers", "=", "0", ",", "shuffle", "=", "False", ")", "\n", "for", "texts", ",", "ratings", "in", "dl", ":", "\n", "                ", "for", "i", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "                    ", "all_rev_lens", ".", "append", "(", "len", "(", "self", ".", "subwordenc", ".", "encode", "(", "text", ")", ")", ")", "\n", "rating_to_count", "[", "ratings", "[", "i", "]", ".", "item", "(", ")", "]", "+=", "1", "\n", "\n", "", "", "", "print", "(", "'Number of reviews per star rating:'", ")", "\n", "for", "rating", ",", "count", "in", "sorted", "(", "rating_to_count", ".", "items", "(", ")", ")", ":", "\n", "            ", "print", "(", "'-- {} stars: {:.2f} reviews; {} of dataset'", ".", "format", "(", "rating", ",", "count", ",", "\n", "float", "(", "count", ")", "/", "len", "(", "all_rev_lens", ")", ")", ")", "\n", "", "print", "(", "'Length of review:'", ")", "\n", "print", "(", "'-- mean: {}'", ".", "format", "(", "np", ".", "mean", "(", "all_rev_lens", ")", ")", ")", "\n", "print", "(", "'-- 75th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "all_rev_lens", ",", "75", ")", ")", ")", "\n", "print", "(", "'-- 90th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "all_rev_lens", ",", "90", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.__init__": [[14, 16], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get": [[17, 23], ["data_loaders.amazon_dataset.AmazonDataset", "data_loaders.yelp_dataset.YelpDataset"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get", "(", "name", ")", ":", "\n", "        ", "if", "name", "==", "'amazon'", ":", "\n", "            ", "return", "AmazonDataset", "(", ")", "\n", "", "elif", "name", "==", "'yelp'", ":", "\n", "            ", "return", "YelpDataset", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__init__": [[31, 126], ["project_settings.DatasetConfig", "numpy.random.seed", "utils.load_file", "len", "float", "os.path.join", "print", "amazon_dataset.AmazonDataset.load_all_reviews", "min", "utils.load_file.items", "utils.load_file.items", "int", "range", "utils.load_file.values", "float", "math.ceil", "numpy.mean", "math.ceil", "len", "range", "range", "len", "math.floor", "utils.load_file.values"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews"], ["def", "__init__", "(", "self", ",", "split", "=", "None", ",", "n_docs", "=", "None", ",", "\n", "subset", "=", "None", ",", "\n", "seed", "=", "0", ",", "\n", "sample_reviews", "=", "True", ",", "\n", "category", "=", "None", ",", "\n", "item_max_reviews", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            split: str ('train', val', 'test')\n            n_docs: int\n            subset: float (Value in [0.0, 1.0]. If given, then dataset is truncated to subset of the businesses\n            seed: int (set seed because we will be using np.random.choice to sample reviews if sample_reviews=True)\n            sample_reviews: boolean\n                - When True, __getitem_ will sample n_docs reviews for each item. The number of times a item appears\n                in the dataset is dependent on uniform_items.\n                - When False, each item will appear math.floor(number of reviews item has / n_docs) times\n                so that almost every review is seen (with up to n_docs - 1 reviews not seen).\n                    - Setting False is useful for (a) validation / test, and (b) simply iterating over all the reviews\n                    (e.g. to build the vocabulary).\n            item_max_reviews: int (maximum number of reviews a item can have)\n                - This is used to remove outliers from the data. This is especially important if uniform_items=False,\n                as there may be a large number of reviews in a training epoch coming from a single item. This also\n                still matters when uniform_items=True, as items an outlier number of reviews will have reviews\n                that are never sampled.\n                - For the Amazon dataset, there are 11,870 items in the training set with at least 50 reviews\n                no longer than 150 subtokens. The breakdown of the distribution in the training set is:\n                    Percentile  |  percentile_n_reviews  |  n_items  |  total_revs\n                    TODO?\n        \"\"\"", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "n_docs", "=", "n_docs", "\n", "self", ".", "subset", "=", "subset", "\n", "self", ".", "sample_reviews", "=", "sample_reviews", "\n", "item_max_reviews", "=", "float", "(", "'inf'", ")", "if", "item_max_reviews", "is", "None", "else", "item_max_reviews", "\n", "self", ".", "item_max_reviews", "=", "item_max_reviews", "\n", "\n", "self", ".", "ds_conf", "=", "DatasetConfig", "(", "'amazon'", ")", "\n", "\n", "# Set random seed so that choice is always the same across experiments", "\n", "# Especially necessary for test set (along with shuffle=False in the DataLoader)", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# Create map from idx-th data point to item", "\n", "item_to_nreviews", "=", "load_file", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "ds_conf", ".", "processed_path", ",", "'{}/item-to-nreviews.json'", ".", "format", "(", "split", ")", ")", ")", "\n", "self", ".", "idx_to_item", "=", "{", "}", "\n", "\n", "# Filter to only one category", "\n", "if", "category", ":", "\n", "            ", "print", "(", "'Filtering to only reviews in: {}'", ".", "format", "(", "category", ")", ")", "\n", "_", ",", "item_to_reviews", "=", "AmazonDataset", ".", "load_all_reviews", "(", ")", "\n", "for", "item", "in", "item_to_reviews", ":", "\n", "                ", "cat", "=", "item_to_reviews", "[", "item", "]", "[", "0", "]", "[", "'category'", "]", "\n", "if", "cat", "!=", "category", ":", "\n", "                    ", "if", "item", "in", "item_to_nreviews", ":", "# item_to_nreviews is only for items and reviews in processed splits", "\n", "                        ", "del", "item_to_nreviews", "[", "item", "]", "\n", "\n", "", "", "", "", "if", "sample_reviews", ":", "\n", "# Get the number of times each item will appear in a pass through this dataset", "\n", "            ", "item_min_reviews", "=", "min", "(", "item_to_nreviews", ".", "values", "(", ")", ")", "\n", "if", "item_max_reviews", "==", "float", "(", "'inf'", ")", ":", "\n", "                ", "n_per_item", "=", "math", ".", "ceil", "(", "item_min_reviews", "/", "n_docs", ")", "\n", "", "else", ":", "\n", "                ", "n_per_item", "=", "np", ".", "mean", "(", "[", "n", "for", "n", "in", "item_to_nreviews", ".", "values", "(", ")", "if", "n", "<=", "item_max_reviews", "]", ")", "\n", "n_per_item", "=", "math", ".", "ceil", "(", "n_per_item", "/", "n_docs", ")", "\n", "# print('Each item will appear {} times'.format(n_per_item))", "\n", "\n", "", "idx", "=", "0", "\n", "for", "item", ",", "n_reviews", "in", "item_to_nreviews", ".", "items", "(", ")", ":", "\n", "                ", "if", "n_reviews", "<=", "item_max_reviews", ":", "\n", "                    ", "for", "_", "in", "range", "(", "n_per_item", ")", ":", "\n", "                        ", "self", ".", "idx_to_item", "[", "idx", "]", "=", "item", "\n", "idx", "+=", "1", "\n", "", "", "", "", "else", ":", "\n", "# __getitem__ will not sample", "\n", "            ", "idx", "=", "0", "\n", "self", ".", "idx_to_item_startidx", "=", "{", "}", "\n", "# idx items idx of one dataset item. item_startidx is the idx within that item's reviews.", "\n", "tot", "=", "0", "\n", "for", "item", ",", "n_reviews", "in", "item_to_nreviews", ".", "items", "(", ")", ":", "\n", "                ", "if", "n_reviews", "<=", "item_max_reviews", ":", "\n", "                    ", "tot", "+=", "n_reviews", "\n", "item_startidx", "=", "0", "\n", "for", "_", "in", "range", "(", "math", ".", "floor", "(", "n_reviews", "/", "n_docs", ")", ")", ":", "\n", "                        ", "self", ".", "idx_to_item", "[", "idx", "]", "=", "item", "\n", "self", ".", "idx_to_item_startidx", "[", "idx", "]", "=", "item_startidx", "\n", "idx", "+=", "1", "\n", "item_startidx", "+=", "n_docs", "\n", "\n", "", "", "", "", "if", "self", ".", "subset", ":", "\n", "            ", "end", "=", "int", "(", "self", ".", "subset", "*", "len", "(", "self", ".", "idx_to_item", ")", ")", "\n", "for", "idx", "in", "range", "(", "end", ",", "len", "(", "self", ".", "idx_to_item", ")", ")", ":", "\n", "                ", "del", "self", ".", "idx_to_item", "[", "idx", "]", "\n", "\n", "", "", "self", ".", "n", "=", "len", "(", "self", ".", "idx_to_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__getitem__": [[127, 151], ["os.path.join", "utils.load_file", "zip", "data_loaders.summ_dataset.SummDataset.concat_docs", "int", "numpy.round", "len", "numpy.random.choice", "numpy.random.choice", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.concat_docs"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "# Map idx to item and load reviews", "\n", "        ", "item", "=", "self", ".", "idx_to_item", "[", "idx", "]", "\n", "fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "ds_conf", ".", "processed_path", ",", "'{}/{}_reviews.json'", ".", "format", "(", "self", ".", "split", ",", "item", ")", ")", "\n", "reviews", "=", "load_file", "(", "fp", ")", "\n", "\n", "# Get reviews from item", "\n", "if", "self", ".", "sample_reviews", ":", "\n", "            ", "if", "len", "(", "reviews", ")", "<", "self", ".", "n_docs", ":", "\n", "                ", "reviews", "=", "np", ".", "random", ".", "choice", "(", "reviews", ",", "size", "=", "self", ".", "n_docs", ",", "replace", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "reviews", "=", "np", ".", "random", ".", "choice", "(", "reviews", ",", "size", "=", "self", ".", "n_docs", ",", "replace", "=", "False", ")", "\n", "", "", "else", ":", "\n", "            ", "start_idx", "=", "self", ".", "idx_to_item_startidx", "[", "idx", "]", "\n", "reviews", "=", "reviews", "[", "start_idx", ":", "start_idx", "+", "self", ".", "n_docs", "]", "\n", "\n", "# Collect data to be returned", "\n", "", "texts", ",", "ratings", "=", "zip", "(", "*", "[", "(", "s", "[", "'reviewText'", "]", ",", "s", "[", "'overall'", "]", ")", "for", "s", "in", "reviews", "]", ")", "\n", "texts", "=", "SummDataset", ".", "concat_docs", "(", "texts", ",", "edok_token", "=", "True", ")", "\n", "avg_rating", "=", "int", "(", "np", ".", "round", "(", "np", ".", "mean", "(", "ratings", ")", ")", ")", "\n", "metadata", "=", "{", "'item'", ":", "item", ",", "'category'", ":", "reviews", "[", "0", "]", "[", "'category'", "]", "}", "\n", "# all the reviews are for the same item, each review will have same category so use 0-th", "\n", "\n", "return", "texts", ",", "avg_rating", ",", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonPytorchDataset.__len__": [[152, 154], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.__init__": [[157, 164], ["data_loaders.summ_dataset.SummReviewDataset.__init__", "project_settings.DatasetConfig", "utils.load_file"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "AmazonDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "name", "=", "'amazon'", "\n", "self", ".", "conf", "=", "DatasetConfig", "(", "'amazon'", ")", "\n", "self", ".", "n_ratings_labels", "=", "5", "\n", "self", ".", "reviews", "=", "None", "\n", "self", ".", "subwordenc", "=", "load_file", "(", "self", ".", "conf", ".", "subwordenc_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews": [[165, 186], ["collections.defaultdict", "os.listdir", "project_settings.DatasetConfig", "fn.endswith", "[].replace", "open().readlines", "json.loads", "reviews.append", "item_to_reviews[].append", "open", "fn.split", "os.path.join"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_all_reviews", "(", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            reviews: list of dicts\n            item_to_reviews: dict, key=str (item id), value=list of dicts\n        \"\"\"", "\n", "reviews", "=", "[", "]", "\n", "item_to_reviews", "=", "defaultdict", "(", "list", ")", "\n", "amazon_dir", "=", "DatasetConfig", "(", "'amazon'", ")", ".", "dir_path", "\n", "for", "fn", "in", "os", ".", "listdir", "(", "amazon_dir", ")", ":", "\n", "            ", "if", "fn", ".", "endswith", "(", "'.json'", ")", ":", "\n", "                ", "cat", "=", "fn", ".", "split", "(", "'.json'", ")", "[", "0", "]", ".", "replace", "(", "'_5'", ",", "''", ")", "\n", "# print(cat)", "\n", "for", "line", "in", "open", "(", "os", ".", "path", ".", "join", "(", "amazon_dir", ",", "fn", ")", ",", "'r'", ")", ".", "readlines", "(", ")", ":", "\n", "                    ", "rev", "=", "json", ".", "loads", "(", "line", ")", "\n", "rev", "[", "'category'", "]", "=", "cat", "\n", "reviews", ".", "append", "(", "rev", ")", "\n", "item_to_reviews", "[", "rev", "[", "'asin'", "]", "]", ".", "append", "(", "rev", ")", "\n", "\n", "", "", "", "return", "reviews", ",", "item_to_reviews", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader": [[187, 198], ["amazon_dataset.AmazonPytorchDataset", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "get_data_loader", "(", "self", ",", "split", "=", "'train'", ",", "n_docs", "=", "8", ",", "subset", "=", "None", ",", "sample_reviews", "=", "True", ",", "\n", "category", "=", "None", ",", "\n", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        Return iterator over specific split in dataset\n        \"\"\"", "\n", "ds", "=", "AmazonPytorchDataset", "(", "split", "=", "split", ",", "n_docs", "=", "n_docs", ",", "subset", "=", "subset", ",", "sample_reviews", "=", "sample_reviews", ",", "\n", "category", "=", "category", ",", "\n", "item_max_reviews", "=", "self", ".", "conf", ".", "item_max_reviews", ")", "\n", "loader", "=", "DataLoader", "(", "ds", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.save_processed_splits": [[204, 276], ["print", "print", "collections.defaultdict", "sum", "print", "print", "print", "list", "sum", "print", "print", "enumerate", "print", "print", "print", "split_to_item_to_nreviews.items", "amazon_dataset.AmazonDataset.load_all_reviews", "collections.defaultdict.keys", "len", "item_to_n.values", "int", "int", "int", "sorted", "os.path.join", "utils.save_file", "os.path.join", "utils.save_file", "len", "item_to_reviews[].append", "len", "len", "len", "item_to_n.items", "amazon_dataset.AmazonDataset.subwordenc.encode", "collections.defaultdict.values"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "save_processed_splits", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Save train, val, and test splits. Splits are across items (e.g. a item is either in train, val, or test).\n        Iterates over all reviews in the original dataset. Tries to get close to a 80-10-10 split.\n\n        Args:\n            review_max_len: int (maximum length in subtokens a review can be)\n            item_min_reviews: int (min number of reviews a item must have)\n            out_dir: str (path to save splits to, e.g. datasets/amazon_dataset/proccessed/)\n        \"\"\"", "\n", "review_max_len", "=", "self", ".", "conf", ".", "review_max_len", "\n", "item_min_reviews", "=", "self", ".", "conf", ".", "item_min_reviews", "\n", "\n", "print", "(", "'Saving processed splits'", ")", "\n", "if", "self", ".", "reviews", "is", "None", ":", "\n", "            ", "self", ".", "reviews", ",", "_", "=", "AmazonDataset", ".", "load_all_reviews", "(", ")", "\n", "\n", "# # Note: we actually do more filtering in the Pytorch dataset class", "\n", "", "print", "(", "'Filtering reviews longer than: {}'", ".", "format", "(", "review_max_len", ")", ")", "\n", "item_to_reviews", "=", "defaultdict", "(", "list", ")", "\n", "for", "r", "in", "self", ".", "reviews", ":", "\n", "            ", "if", "len", "(", "self", ".", "subwordenc", ".", "encode", "(", "r", "[", "'reviewText'", "]", ")", ")", "<", "review_max_len", ":", "\n", "                ", "item_to_reviews", "[", "r", "[", "'asin'", "]", "]", ".", "append", "(", "r", ")", "\n", "\n", "# Calculate target amount of reviews per item", "\n", "", "", "n", "=", "sum", "(", "[", "len", "(", "revs", ")", "for", "revs", "in", "item_to_reviews", ".", "values", "(", ")", "]", ")", "\n", "print", "(", "'Total number of reviews before filtering: {}'", ".", "format", "(", "len", "(", "self", ".", "reviews", ")", ")", ")", "\n", "print", "(", "'Total number of reviews after filtering: {}'", ".", "format", "(", "n", ")", ")", "\n", "\n", "# Note: we actually do more filtering in the Pytorch Dataset class", "\n", "print", "(", "'Filtering items with less than {} reviews'", ".", "format", "(", "item_min_reviews", ")", ")", "\n", "item_to_n", "=", "{", "}", "\n", "for", "item", "in", "list", "(", "item_to_reviews", ".", "keys", "(", ")", ")", ":", "# have to do list and keys for python3 to delete in-place", "\n", "            ", "n", "=", "len", "(", "item_to_reviews", "[", "item", "]", ")", "\n", "if", "n", "<", "self", ".", "conf", ".", "item_min_reviews", ":", "\n", "                ", "del", "item_to_reviews", "[", "item", "]", "\n", "", "else", ":", "\n", "                ", "item_to_n", "[", "item", "]", "=", "n", "\n", "", "", "n", "=", "sum", "(", "item_to_n", ".", "values", "(", ")", ")", "\n", "print", "(", "'Total number of reviews after filtering: {}'", ".", "format", "(", "n", ")", ")", "\n", "print", "(", "'Total number of items after filtering: {}'", ".", "format", "(", "len", "(", "item_to_n", ")", ")", ")", "\n", "\n", "# Construct splits", "\n", "n_tr", ",", "n_val", ",", "n_te", "=", "int", "(", "0.8", "*", "n", ")", ",", "int", "(", "0.1", "*", "n", ")", ",", "int", "(", "0.1", "*", "n", ")", "\n", "cur_n_tr", ",", "cur_n_val", ",", "cur_n_te", "=", "0", ",", "0", ",", "0", "\n", "split_to_item_to_nreviews", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", "}", "\n", "# In descending order of number of reviews per item", "\n", "for", "i", ",", "(", "item", ",", "n", ")", "in", "enumerate", "(", "sorted", "(", "item_to_n", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", ")", ":", "\n", "# once every ten items, save to val / test if we haven't yet hit the target number", "\n", "            ", "if", "(", "i", "%", "10", "==", "8", ")", "and", "(", "cur_n_val", "<", "n_val", ")", ":", "\n", "                ", "split", "=", "'val'", "\n", "cur_n_val", "+=", "n", "\n", "", "elif", "(", "i", "%", "10", "==", "9", ")", "and", "(", "cur_n_te", "<", "n_te", ")", ":", "\n", "                ", "split", "=", "'test'", "\n", "cur_n_te", "+=", "n", "\n", "", "else", ":", "\n", "                ", "split", "=", "'train'", "\n", "cur_n_tr", "+=", "n", "\n", "\n", "", "out_fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "conf", ".", "processed_path", ",", "'{}/{}_reviews.json'", ".", "format", "(", "split", ",", "item", ")", ")", "\n", "save_file", "(", "item_to_reviews", "[", "item", "]", ",", "out_fp", ",", "verbose", "=", "False", ")", "\n", "\n", "split_to_item_to_nreviews", "[", "split", "]", "[", "item", "]", "=", "n", "\n", "\n", "", "print", "(", "'Number of train reviews: {} / {}'", ".", "format", "(", "cur_n_tr", ",", "n_tr", ")", ")", "\n", "print", "(", "'Number of val reviews: {} / {}'", ".", "format", "(", "cur_n_val", ",", "n_val", ")", ")", "\n", "print", "(", "'Number of test reviews: {} / {}'", ".", "format", "(", "cur_n_te", ",", "n_te", ")", ")", "\n", "\n", "# This file is used by AmazonPytorchDataset", "\n", "for", "split", ",", "item_to_nreviews", "in", "split_to_item_to_nreviews", ".", "items", "(", ")", ":", "\n", "            ", "out_fp", "=", "os", ".", "path", ".", "join", "(", "self", ".", "conf", ".", "processed_path", ",", "'{}/item-to-nreviews.json'", ".", "format", "(", "split", ")", ")", "\n", "save_file", "(", "item_to_nreviews", ",", "out_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.print_filtered_data_stats": [[277, 313], ["collections.defaultdict", "collections.defaultdict", "amazon_dataset.AmazonDataset.get_data_loader", "print", "print", "print", "print", "sorted", "print", "sorted", "print", "print", "print", "print", "enumerate", "collections.defaultdict.items", "print", "collections.defaultdict.items", "print", "len", "all_rev_lens.append", "len", "numpy.mean", "numpy.percentile", "numpy.percentile", "amazon_dataset.AmazonDataset.subwordenc.encode", "ratings[].item", "float", "len", "float", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "", "def", "print_filtered_data_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and print some statistics on the filtered dataset. This is what we use for\n        training, validation, and testing.\n        \"\"\"", "\n", "for", "split", "in", "[", "'train'", ",", "'val'", ",", "'test'", "]", ":", "\n", "            ", "all_rev_lens", "=", "[", "]", "\n", "rating_to_count", "=", "defaultdict", "(", "int", ")", "\n", "cat_to_count", "=", "defaultdict", "(", "int", ")", "\n", "dl", "=", "self", ".", "get_data_loader", "(", "split", "=", "split", ",", "n_docs", "=", "1", ",", "sample_reviews", "=", "False", ",", "\n", "batch_size", "=", "1", ",", "num_workers", "=", "0", ",", "shuffle", "=", "False", ")", "\n", "for", "texts", ",", "ratings", ",", "metadata", "in", "dl", ":", "\n", "                ", "for", "i", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "# note: this just loops once right now as n_docs=1", "\n", "                    ", "rev_len", "=", "len", "(", "self", ".", "subwordenc", ".", "encode", "(", "text", ")", ")", "\n", "all_rev_lens", ".", "append", "(", "rev_len", ")", "\n", "rating_to_count", "[", "ratings", "[", "i", "]", ".", "item", "(", ")", "]", "+=", "1", "\n", "cat_to_count", "[", "metadata", "[", "'category'", "]", "[", "0", "]", "]", "+=", "1", "\n", "# data loader maps metadata dict into key: list of values", "\n", "\n", "", "", "print", "(", "''", ",", "'='", "*", "50", ",", "''", ")", "\n", "print", "(", "'Split: {}'", ".", "format", "(", "split", ")", ")", "\n", "print", "(", "'Total number of reviews: {}'", ".", "format", "(", "len", "(", "all_rev_lens", ")", ")", ")", "\n", "print", "(", "'Number of reviews per star rating:'", ")", "\n", "for", "rating", ",", "count", "in", "sorted", "(", "rating_to_count", ".", "items", "(", ")", ")", ":", "\n", "                ", "print", "(", "'-- {} stars: {:.2f} reviews; {} of dataset'", "\n", ".", "format", "(", "rating", ",", "count", ",", "float", "(", "count", ")", "/", "len", "(", "all_rev_lens", ")", ")", ")", "\n", "\n", "", "print", "(", "'Number of reviews per category:'", ")", "\n", "for", "cat", ",", "count", "in", "sorted", "(", "cat_to_count", ".", "items", "(", ")", ")", ":", "\n", "                ", "print", "(", "'-- {}: {:.2f} reviews; {} of dataset'", "\n", ".", "format", "(", "cat", ",", "count", ",", "float", "(", "count", ")", "/", "len", "(", "all_rev_lens", ")", ")", ")", "\n", "\n", "", "print", "(", "'Length of review:'", ")", "\n", "print", "(", "'-- mean: {}'", ".", "format", "(", "np", ".", "mean", "(", "all_rev_lens", ")", ")", ")", "\n", "print", "(", "'-- 75th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "all_rev_lens", ",", "75", ")", ")", ")", "\n", "print", "(", "'-- 90th percentile: {}'", ".", "format", "(", "np", ".", "percentile", "(", "all_rev_lens", ",", "90", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.print_original_data_stats": [[314, 328], ["print", "print", "print", "pdb.set_trace", "amazon_dataset.AmazonDataset.load_all_reviews", "lens.append", "numpy.median", "numpy.percentile", "numpy.percentile", "len", "amazon_dataset.AmazonDataset.subwordenc.encode"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.load_all_reviews", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "", "def", "print_original_data_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Calculate and print some statistics on the original dataset\n        \"\"\"", "\n", "if", "self", ".", "reviews", "is", "None", ":", "\n", "            ", "self", ".", "reviews", ",", "self", ".", "item_to_reviews", "=", "AmazonDataset", ".", "load_all_reviews", "(", ")", "\n", "", "lens", "=", "[", "]", "\n", "for", "rev", "in", "self", ".", "reviews", ":", "\n", "            ", "lens", ".", "append", "(", "len", "(", "self", ".", "subwordenc", ".", "encode", "(", "rev", "[", "'reviewText'", "]", ")", ")", ")", "\n", "\n", "", "print", "(", "np", ".", "median", "(", "lens", ")", ")", "\n", "print", "(", "np", ".", "percentile", "(", "lens", ",", "75", ")", ")", "\n", "print", "(", "np", ".", "percentile", "(", "lens", ",", "90", ")", ")", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.encode": [[64, 87], ["six.moves.range", "ret.append", "len", "ret.append"], "function", ["None"], ["def", "encode", "(", "text", ")", ":", "\n", "    ", "\"\"\"Encode a unicode string as a list of tokens.\n\n    Args:\n      text: a unicode string\n    Returns:\n      a list of tokens as Unicode strings\n    \"\"\"", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "ret", "=", "[", "]", "\n", "token_start", "=", "0", "\n", "# Classify each character in the input string", "\n", "is_alnum", "=", "[", "c", "in", "_ALPHANUMERIC_CHAR_SET", "for", "c", "in", "text", "]", "\n", "for", "pos", "in", "range", "(", "1", ",", "len", "(", "text", ")", ")", ":", "\n", "        ", "if", "is_alnum", "[", "pos", "]", "!=", "is_alnum", "[", "pos", "-", "1", "]", ":", "\n", "            ", "token", "=", "text", "[", "token_start", ":", "pos", "]", "\n", "if", "token", "!=", "u\" \"", "or", "token_start", "==", "0", ":", "\n", "                ", "ret", ".", "append", "(", "token", ")", "\n", "", "token_start", "=", "pos", "\n", "", "", "final_token", "=", "text", "[", "token_start", ":", "]", "\n", "ret", ".", "append", "(", "final_token", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.decode": [[89, 104], ["enumerate", "ret.append", "ret.append"], "function", ["None"], ["", "def", "decode", "(", "tokens", ")", ":", "\n", "    ", "\"\"\"Decode a list of tokens to a unicode string.\n\n    Args:\n      tokens: a list of Unicode strings\n    Returns:\n      a unicode string\n    \"\"\"", "\n", "token_is_alnum", "=", "[", "t", "[", "0", "]", "in", "_ALPHANUMERIC_CHAR_SET", "for", "t", "in", "tokens", "]", "\n", "ret", "=", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "i", ">", "0", "and", "token_is_alnum", "[", "i", "-", "1", "]", "and", "token_is_alnum", "[", "i", "]", ":", "\n", "            ", "ret", ".", "append", "(", "u\" \"", ")", "\n", "", "ret", ".", "append", "(", "token", ")", "\n", "", "return", "\"\"", ".", "join", "(", "ret", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer._read_filepattern": [[106, 144], ["sorted", "tensorflow.gfile.Glob", "tensorflow.gfile.Open", "line.strip", "doc.append", "f.read"], "function", ["None"], ["", "def", "_read_filepattern", "(", "filepattern", ",", "max_lines", "=", "None", ",", "split_on_newlines", "=", "True", ")", ":", "\n", "    ", "\"\"\"Reads files matching a wildcard pattern, yielding the contents.\n\n    Args:\n      filepattern: A wildcard pattern matching one or more files.\n      max_lines: If set, stop reading after reading this many lines.\n      split_on_newlines: A boolean. If true, then split files by lines and strip\n          leading and trailing whitespace from each line. Otherwise, treat each\n          file as a single string.\n\n    Yields:\n      The contents of the files as lines, if split_on_newlines is True, or\n      the entire contents of each file if False.\n    \"\"\"", "\n", "filenames", "=", "sorted", "(", "tf", ".", "gfile", ".", "Glob", "(", "filepattern", ")", ")", "\n", "lines_read", "=", "0", "\n", "for", "filename", "in", "filenames", ":", "\n", "        ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ")", "as", "f", ":", "\n", "            ", "if", "split_on_newlines", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "yield", "line", ".", "strip", "(", ")", "\n", "lines_read", "+=", "1", "\n", "if", "max_lines", "and", "lines_read", ">=", "max_lines", ":", "\n", "                        ", "return", "\n", "\n", "", "", "", "else", ":", "\n", "                ", "if", "max_lines", ":", "\n", "                    ", "doc", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "                        ", "doc", ".", "append", "(", "line", ")", "\n", "lines_read", "+=", "1", "\n", "if", "max_lines", "and", "lines_read", ">=", "max_lines", ":", "\n", "                            ", "yield", "\"\"", ".", "join", "(", "doc", ")", "\n", "return", "\n", "", "", "yield", "\"\"", ".", "join", "(", "doc", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "yield", "f", ".", "read", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.corpus_token_counts": [[146, 168], ["collections.Counter", "tokenizer._read_filepattern", "collections.Counter.update", "tokenizer.encode", "_native_to_unicode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer._read_filepattern", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "", "", "", "", "def", "corpus_token_counts", "(", "\n", "text_filepattern", ",", "corpus_max_lines", ",", "split_on_newlines", "=", "True", ")", ":", "\n", "    ", "\"\"\"Read the corpus and compute a dictionary of token counts.\n\n    Args:\n      text_filepattern: A pattern matching one or more files.\n      corpus_max_lines: An integer; maximum total lines to read.\n      split_on_newlines: A boolean. If true, then split files by lines and strip\n          leading and trailing whitespace from each line. Otherwise, treat each\n          file as a single string.\n\n    Returns:\n      a dictionary mapping token to count.\n    \"\"\"", "\n", "counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "doc", "in", "_read_filepattern", "(", "\n", "text_filepattern", ",", "\n", "max_lines", "=", "corpus_max_lines", ",", "\n", "split_on_newlines", "=", "split_on_newlines", ")", ":", "\n", "        ", "counts", ".", "update", "(", "encode", "(", "_native_to_unicode", "(", "doc", ")", ")", ")", "\n", "\n", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.vocab_token_counts": [[170, 194], ["enumerate", "tokenizer._read_filepattern", "line.rsplit", "int", "tensorflow.logging.warning", "_native_to_unicode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer._read_filepattern"], ["", "def", "vocab_token_counts", "(", "text_filepattern", ",", "max_lines", ")", ":", "\n", "    ", "\"\"\"Read a vocab file and return a dictionary of token counts.\n\n    Reads a two-column CSV file of tokens and their frequency in a dataset. The\n    tokens are presumed to be generated by encode() or the equivalent.\n\n    Args:\n      text_filepattern: A pattern matching one or more files.\n      max_lines: An integer; maximum total lines to read.\n\n    Returns:\n      a dictionary mapping token to count.\n    \"\"\"", "\n", "ret", "=", "{", "}", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "\n", "_read_filepattern", "(", "text_filepattern", ",", "max_lines", "=", "max_lines", ")", ")", ":", "\n", "        ", "if", "\",\"", "not", "in", "line", ":", "\n", "            ", "tf", ".", "logging", ".", "warning", "(", "\"Malformed vocab line #%d '%s'\"", ",", "i", ",", "line", ")", "\n", "continue", "\n", "\n", "", "token", ",", "count", "=", "line", ".", "rsplit", "(", "\",\"", ",", "1", ")", "\n", "ret", "[", "_native_to_unicode", "(", "token", ")", "]", "=", "int", "(", "count", ")", "\n", "\n", "", "return", "ret", "\n", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.__init__": [[95, 97], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_reserved_ids", "=", "NUM_RESERVED_TOKENS", ")", ":", "\n", "        ", "self", ".", "_num_reserved_ids", "=", "num_reserved_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.num_reserved_ids": [[98, 101], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_reserved_ids", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_reserved_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.encode": [[102, 117], ["int", "s.split"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "\"\"\"Transform a human-readable string into a sequence of int ids.\n\n        The ids should be in the range [num_reserved_ids, vocab_size). Ids [0,\n        num_reserved_ids) are reserved.\n\n        EOS is not appended.\n\n        Args:\n          s: human-readable string to be converted.\n\n        Returns:\n          ids: list of integers\n        \"\"\"", "\n", "return", "[", "int", "(", "w", ")", "+", "self", ".", "_num_reserved_ids", "for", "w", "in", "s", ".", "split", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.decode": [[118, 134], ["text_encoder.strip_ids", "text_encoder.TextEncoder.decode_list", "list", "six.moves.range"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.strip_ids", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.decode_list"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "\"\"\"Transform a sequence of int ids into a human-readable string.\n\n        EOS is not expected in ids.\n\n        Args:\n          ids: list of integers to be converted.\n          strip_extraneous: bool, whether to strip off extraneous tokens\n            (EOS and PAD).\n\n        Returns:\n          s: human-readable string.\n        \"\"\"", "\n", "if", "strip_extraneous", ":", "\n", "            ", "ids", "=", "strip_ids", "(", "ids", ",", "list", "(", "range", "(", "self", ".", "_num_reserved_ids", "or", "0", ")", ")", ")", "\n", "", "return", "\" \"", ".", "join", "(", "self", ".", "decode_list", "(", "ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.decode_list": [[135, 155], ["str", "decoded_ids.append", "decoded_ids.append", "int"], "methods", ["None"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Transform a sequence of int ids into a their string versions.\n\n        This method supports transforming individual input/output ids to their\n        string versions so that sequence to/from text conversions can be visualized\n        in a human readable format.\n\n        Args:\n          ids: list of integers to be converted.\n\n        Returns:\n          strs: list of human-readable string.\n        \"\"\"", "\n", "decoded_ids", "=", "[", "]", "\n", "for", "id_", "in", "ids", ":", "\n", "            ", "if", "0", "<=", "id_", "<", "self", ".", "_num_reserved_ids", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "RESERVED_TOKENS", "[", "int", "(", "id_", ")", "]", ")", "\n", "", "else", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "id_", "-", "self", ".", "_num_reserved_ids", ")", "\n", "", "", "return", "[", "str", "(", "d", ")", "for", "d", "in", "decoded_ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TextEncoder.vocab_size": [[156, 159], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ByteTextEncoder.encode": [[164, 172], ["isinstance", "s.encode.encode.encode", "s.encode.encode.encode", "ord"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "numres", "=", "self", ".", "_num_reserved_ids", "\n", "if", "six", ".", "PY2", ":", "\n", "            ", "if", "isinstance", "(", "s", ",", "unicode", ")", ":", "\n", "                ", "s", "=", "s", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "return", "[", "ord", "(", "c", ")", "+", "numres", "for", "c", "in", "s", "]", "\n", "# Python3: explicitly convert to UTF-8", "\n", "", "return", "[", "c", "+", "numres", "for", "c", "in", "s", ".", "encode", "(", "\"utf-8\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ByteTextEncoder.decode": [[173, 188], ["text_encoder.strip_ids", "list", "decoded_ids.append", "decoded_ids.append", "six.moves.range", "int2byte", "int"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.strip_ids"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "if", "strip_extraneous", ":", "\n", "            ", "ids", "=", "strip_ids", "(", "ids", ",", "list", "(", "range", "(", "self", ".", "_num_reserved_ids", "or", "0", ")", ")", ")", "\n", "", "numres", "=", "self", ".", "_num_reserved_ids", "\n", "decoded_ids", "=", "[", "]", "\n", "int2byte", "=", "six", ".", "int2byte", "\n", "for", "id_", "in", "ids", ":", "\n", "            ", "if", "0", "<=", "id_", "<", "numres", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "RESERVED_TOKENS_BYTES", "[", "int", "(", "id_", ")", "]", ")", "\n", "", "else", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "int2byte", "(", "id_", "-", "numres", ")", ")", "\n", "", "", "if", "six", ".", "PY2", ":", "\n", "            ", "return", "\"\"", ".", "join", "(", "decoded_ids", ")", "\n", "# Python3: join byte arrays and then decode string", "\n", "", "return", "b\"\"", ".", "join", "(", "decoded_ids", ")", ".", "decode", "(", "\"utf-8\"", ",", "\"replace\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ByteTextEncoder.decode_list": [[189, 200], ["decoded_ids.append", "decoded_ids.append", "int2byte", "int"], "methods", ["None"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "numres", "=", "self", ".", "_num_reserved_ids", "\n", "decoded_ids", "=", "[", "]", "\n", "int2byte", "=", "six", ".", "int2byte", "\n", "for", "id_", "in", "ids", ":", "\n", "            ", "if", "0", "<=", "id_", "<", "numres", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "RESERVED_TOKENS_BYTES", "[", "int", "(", "id_", ")", "]", ")", "\n", "", "else", ":", "\n", "                ", "decoded_ids", ".", "append", "(", "int2byte", "(", "id_", "-", "numres", ")", ")", "\n", "# Python3: join byte arrays and then decode string", "\n", "", "", "return", "decoded_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ByteTextEncoder.vocab_size": [[201, 204], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "2", "**", "8", "+", "self", ".", "_num_reserved_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ClassLabelEncoder.__init__": [[209, 220], ["text_encoder.TextEncoder.__init__", "tensorflow.gfile.Open", "label.strip", "f.readlines"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["def", "__init__", "(", "self", ",", "class_labels", "=", "None", ",", "class_labels_fname", "=", "None", ")", ":", "\n", "        ", "super", "(", "ClassLabelEncoder", ",", "self", ")", ".", "__init__", "(", "num_reserved_ids", "=", "0", ")", "\n", "\n", "assert", "class_labels", "or", "class_labels_fname", "\n", "assert", "not", "(", "class_labels", "and", "class_labels_fname", ")", "\n", "\n", "if", "class_labels_fname", ":", "\n", "            ", "with", "tf", ".", "gfile", ".", "Open", "(", "class_labels_fname", ")", "as", "f", ":", "\n", "                ", "class_labels", "=", "[", "label", ".", "strip", "(", ")", "for", "label", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "", "", "self", ".", "_class_labels", "=", "class_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ClassLabelEncoder.encode": [[221, 224], ["text_encoder.ClassLabelEncoder._class_labels.index"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "label_str", "=", "s", "\n", "return", "self", ".", "_class_labels", ".", "index", "(", "label_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ClassLabelEncoder.decode": [[225, 234], ["isinstance", "isinstance", "numpy.squeeze", "len"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "del", "strip_extraneous", "\n", "label_id", "=", "ids", "\n", "if", "isinstance", "(", "label_id", ",", "list", ")", ":", "\n", "            ", "assert", "len", "(", "label_id", ")", "==", "1", "\n", "label_id", ",", "=", "label_id", "\n", "", "if", "isinstance", "(", "label_id", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "label_id", "=", "np", ".", "squeeze", "(", "label_id", ")", "\n", "", "return", "self", ".", "_class_labels", "[", "label_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ClassLabelEncoder.decode_list": [[235, 237], ["None"], "methods", ["None"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "_class_labels", "[", "i", "]", "for", "i", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ClassLabelEncoder.vocab_size": [[238, 241], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_class_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.OneHotClassLabelEncoder.__init__": [[246, 256], ["text_encoder.TextEncoder.__init__", "tensorflow.gfile.Open", "label.strip", "f.readlines"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["def", "__init__", "(", "self", ",", "class_labels", "=", "None", ",", "class_labels_fname", "=", "None", ")", ":", "\n", "        ", "super", "(", "OneHotClassLabelEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "class_labels", "or", "class_labels_fname", "\n", "assert", "not", "(", "class_labels", "and", "class_labels_fname", ")", "\n", "\n", "if", "class_labels_fname", ":", "\n", "            ", "with", "tf", ".", "gfile", ".", "Open", "(", "class_labels_fname", ")", "as", "f", ":", "\n", "                ", "class_labels", "=", "[", "label", ".", "strip", "(", ")", "for", "label", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "", "", "self", ".", "_class_labels", "=", "class_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.OneHotClassLabelEncoder.encode": [[257, 263], ["numpy.zeros", "numpy.zeros.tolist", "numpy.zeros.fill", "text_encoder.OneHotClassLabelEncoder._class_labels.index"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "label_str", ",", "on_value", "=", "1", ",", "off_value", "=", "0", ")", ":", "# pylint: disable=arguments-differ", "\n", "        ", "e", "=", "np", ".", "zeros", "(", "self", ".", "vocab_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "if", "off_value", "!=", "0", ":", "\n", "            ", "e", ".", "fill", "(", "off_value", ")", "\n", "", "e", "[", "self", ".", "_class_labels", ".", "index", "(", "label_str", ")", "]", "=", "on_value", "\n", "return", "e", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.OneHotClassLabelEncoder.decode": [[264, 272], ["isinstance", "isinstance", "numpy.squeeze().astype().tolist", "len", "numpy.squeeze().astype().tolist.index", "numpy.squeeze().astype", "numpy.squeeze"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "del", "strip_extraneous", "\n", "label_id", "=", "ids", "\n", "if", "isinstance", "(", "label_id", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "label_id", "=", "np", ".", "squeeze", "(", "label_id", ")", ".", "astype", "(", "np", ".", "int8", ")", ".", "tolist", "(", ")", "\n", "", "assert", "isinstance", "(", "label_id", ",", "list", ")", "\n", "assert", "len", "(", "label_id", ")", "==", "self", ".", "vocab_size", "\n", "return", "self", ".", "_class_labels", "[", "label_id", ".", "index", "(", "1", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.OneHotClassLabelEncoder.vocab_size": [[273, 276], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_class_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.__init__": [[281, 313], ["text_encoder.TextEncoder.__init__", "text_encoder.TokenTextEncoder._init_vocab_from_file", "text_encoder.TokenTextEncoder._init_vocab_from_list"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab_from_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab_from_list"], ["def", "__init__", "(", "self", ",", "\n", "vocab_filename", ",", "\n", "reverse", "=", "False", ",", "\n", "vocab_list", "=", "None", ",", "\n", "replace_oov", "=", "None", ",", "\n", "num_reserved_ids", "=", "NUM_RESERVED_TOKENS", ")", ":", "\n", "        ", "\"\"\"Initialize from a file or list, one token per line.\n\n        Handling of reserved tokens works as follows:\n        - When initializing from a list, we add reserved tokens to the vocab.\n        - When initializing from a file, we do not add reserved tokens to the vocab.\n        - When saving vocab files, we save reserved tokens to the file.\n\n        Args:\n          vocab_filename: If not None, the full filename to read vocab from. If this\n             is not None, then vocab_list should be None.\n          reverse: Boolean indicating if tokens should be reversed during encoding\n             and decoding.\n          vocab_list: If not None, a list of elements of the vocabulary. If this is\n             not None, then vocab_filename should be None.\n          replace_oov: If not None, every out-of-vocabulary token seen when\n             encoding will be replaced by this string (which must be in vocab).\n          num_reserved_ids: Number of IDs to save for reserved tokens like <EOS>.\n        \"\"\"", "\n", "super", "(", "TokenTextEncoder", ",", "self", ")", ".", "__init__", "(", "num_reserved_ids", "=", "num_reserved_ids", ")", "\n", "self", ".", "_reverse", "=", "reverse", "\n", "self", ".", "_replace_oov", "=", "replace_oov", "\n", "if", "vocab_filename", ":", "\n", "            ", "self", ".", "_init_vocab_from_file", "(", "vocab_filename", ")", "\n", "", "else", ":", "\n", "            ", "assert", "vocab_list", "is", "not", "None", "\n", "self", ".", "_init_vocab_from_list", "(", "vocab_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.encode": [[314, 323], ["sentence.strip().split", "sentence.strip"], "methods", ["None"], ["", "", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "\"\"\"Converts a space-separated string of tokens to a list of ids.\"\"\"", "\n", "sentence", "=", "s", "\n", "tokens", "=", "sentence", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "self", ".", "_replace_oov", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "[", "t", "if", "t", "in", "self", ".", "_token_to_id", "else", "self", ".", "_replace_oov", "\n", "for", "t", "in", "tokens", "]", "\n", "", "ret", "=", "[", "self", ".", "_token_to_id", "[", "tok", "]", "for", "tok", "in", "tokens", "]", "\n", "return", "ret", "[", ":", ":", "-", "1", "]", "if", "self", ".", "_reverse", "else", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.decode": [[324, 326], ["text_encoder.TokenTextEncoder.decode_list"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.decode_list"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "self", ".", "decode_list", "(", "ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.decode_list": [[327, 330], ["reversed", "text_encoder.TokenTextEncoder._safe_id_to_token"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._safe_id_to_token"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "seq", "=", "reversed", "(", "ids", ")", "if", "self", ".", "_reverse", "else", "ids", "\n", "return", "[", "self", ".", "_safe_id_to_token", "(", "i", ")", "for", "i", "in", "seq", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.vocab_size": [[331, 334], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_id_to_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._safe_id_to_token": [[335, 337], ["text_encoder.TokenTextEncoder._id_to_token.get"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get"], ["", "def", "_safe_id_to_token", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "_id_to_token", ".", "get", "(", "idx", ",", "\"ID_%d\"", "%", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab_from_file": [[338, 352], ["text_encoder.TokenTextEncoder._init_vocab", "tensorflow.gfile.Open", "text_encoder.TokenTextEncoder._init_vocab_from_file.token_gen"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab"], ["", "def", "_init_vocab_from_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load vocab from a file.\n\n        Args:\n          filename: The file to load vocabulary from.\n        \"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ")", "as", "f", ":", "\n", "            ", "tokens", "=", "[", "token", ".", "strip", "(", ")", "for", "token", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "", "def", "token_gen", "(", ")", ":", "\n", "            ", "for", "token", "in", "tokens", ":", "\n", "                ", "yield", "token", "\n", "\n", "", "", "self", ".", "_init_vocab", "(", "token_gen", "(", ")", ",", "add_reserved_tokens", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab_from_list": [[353, 369], ["text_encoder.TokenTextEncoder._init_vocab", "text_encoder.TokenTextEncoder._init_vocab_from_file.token_gen"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab"], ["", "def", "_init_vocab_from_list", "(", "self", ",", "vocab_list", ")", ":", "\n", "        ", "\"\"\"Initialize tokens from a list of tokens.\n\n        It is ok if reserved tokens appear in the vocab list. They will be\n        removed. The set of tokens in vocab_list should be unique.\n\n        Args:\n          vocab_list: A list of tokens.\n        \"\"\"", "\n", "\n", "def", "token_gen", "(", ")", ":", "\n", "            ", "for", "token", "in", "vocab_list", ":", "\n", "                ", "if", "token", "not", "in", "RESERVED_TOKENS", ":", "\n", "                    ", "yield", "token", "\n", "\n", "", "", "", "self", ".", "_init_vocab", "(", "token_gen", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder._init_vocab": [[370, 386], ["text_encoder.TokenTextEncoder._id_to_token.update", "dict", "text_encoder.TokenTextEncoder._id_to_token.update", "len", "enumerate", "enumerate", "six.iteritems"], "methods", ["None"], ["", "def", "_init_vocab", "(", "self", ",", "token_generator", ",", "add_reserved_tokens", "=", "True", ")", ":", "\n", "        ", "\"\"\"Initialize vocabulary with tokens from token_generator.\"\"\"", "\n", "\n", "self", ".", "_id_to_token", "=", "{", "}", "\n", "non_reserved_start_index", "=", "0", "\n", "\n", "if", "add_reserved_tokens", ":", "\n", "            ", "self", ".", "_id_to_token", ".", "update", "(", "enumerate", "(", "RESERVED_TOKENS", ")", ")", "\n", "non_reserved_start_index", "=", "len", "(", "RESERVED_TOKENS", ")", "\n", "\n", "", "self", ".", "_id_to_token", ".", "update", "(", "\n", "enumerate", "(", "token_generator", ",", "start", "=", "non_reserved_start_index", ")", ")", "\n", "\n", "# _token_to_id is the reverse of _id_to_token", "\n", "self", ".", "_token_to_id", "=", "dict", "(", "(", "v", ",", "k", ")", "\n", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "self", ".", "_id_to_token", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.TokenTextEncoder.store_to_file": [[387, 399], ["tensorflow.gfile.Open", "six.moves.range", "len", "f.write"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write"], ["", "def", "store_to_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Write vocab file to disk.\n\n        Vocab files have one token per line. The file ends in a newline. Reserved\n        tokens are written to the vocab file as well.\n\n        Args:\n          filename: Full path of the file to store the vocab to.\n        \"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_id_to_token", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "self", ".", "_id_to_token", "[", "i", "]", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.__init__": [[482, 494], ["set", "text_encoder.TextEncoder.__init__", "text_encoder.SubwordTextEncoder._load_from_file"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._load_from_file"], ["def", "__init__", "(", "self", ",", "filename", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize and read from a file, if provided.\n\n        Args:\n          filename: filename from which to read vocab. If None, do not load a\n            vocab\n        \"\"\"", "\n", "self", ".", "_alphabet", "=", "set", "(", ")", "\n", "self", ".", "filename", "=", "filename", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "self", ".", "_load_from_file", "(", "filename", ")", "\n", "", "super", "(", "SubwordTextEncoder", ",", "self", ")", ".", "__init__", "(", "num_reserved_ids", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.encode": [[495, 505], ["text_encoder.SubwordTextEncoder._tokens_to_subtoken_ids", "data_loaders.tokenizer.encode", "text_encoder.native_to_unicode"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._tokens_to_subtoken_ids", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode"], ["", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "\"\"\"Converts a native string to a list of subtoken ids.\n\n        Args:\n          s: a native string.\n        Returns:\n          a list of integers in the range [0, vocab_size)\n        \"\"\"", "\n", "return", "self", ".", "_tokens_to_subtoken_ids", "(", "\n", "tokenizer", ".", "encode", "(", "native_to_unicode", "(", "s", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.encode_without_tokenizing": [[506, 522], ["text_encoder.SubwordTextEncoder._tokens_to_subtoken_ids", "text_encoder.native_to_unicode"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._tokens_to_subtoken_ids", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode"], ["", "def", "encode_without_tokenizing", "(", "self", ",", "token_text", ")", ":", "\n", "        ", "\"\"\"Converts string to list of subtoken ids without calling tokenizer.\n\n        This treats `token_text` as a single token and directly converts it\n        to subtoken ids. This may be useful when the default tokenizer doesn't\n        do what we want (e.g., when encoding text with tokens composed of lots of\n        nonalphanumeric characters). It is then up to the caller to make sure that\n        raw text is consistently converted into tokens. Only use this if you are\n        sure that `encode` doesn't suit your needs.\n\n        Args:\n          token_text: A native string representation of a single token.\n        Returns:\n          A list of subword token ids; i.e., integers in the range [0, vocab_size).\n        \"\"\"", "\n", "return", "self", ".", "_tokens_to_subtoken_ids", "(", "[", "native_to_unicode", "(", "token_text", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.decode": [[523, 538], ["text_encoder.unicode_to_native", "text_encoder.strip_ids", "data_loaders.tokenizer.decode", "list", "text_encoder.SubwordTextEncoder._subtoken_ids_to_tokens", "six.moves.range"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.unicode_to_native", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.strip_ids", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._subtoken_ids_to_tokens"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of subtoken ids to a native string.\n\n        Args:\n          ids: a list of integers in the range [0, vocab_size)\n          strip_extraneous: bool, whether to strip off extraneous tokens\n            (EOS and PAD).\n\n        Returns:\n          a native string\n        \"\"\"", "\n", "if", "strip_extraneous", ":", "\n", "            ", "ids", "=", "strip_ids", "(", "ids", ",", "list", "(", "range", "(", "self", ".", "_num_reserved_ids", "or", "0", ")", ")", ")", "\n", "", "return", "unicode_to_native", "(", "\n", "tokenizer", ".", "decode", "(", "self", ".", "_subtoken_ids_to_tokens", "(", "ids", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.decode_list": [[539, 541], ["text_encoder.SubwordTextEncoder._subtoken_id_to_subtoken_string"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._subtoken_id_to_subtoken_string"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "_subtoken_id_to_subtoken_string", "(", "s", ")", "for", "s", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.vocab_size": [[542, 546], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"The subtoken vocabulary size.\"\"\"", "\n", "return", "len", "(", "self", ".", "_all_subtoken_strings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._tokens_to_subtoken_ids": [[547, 559], ["ret.extend", "text_encoder.SubwordTextEncoder._token_to_subtoken_ids"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._token_to_subtoken_ids"], ["", "def", "_tokens_to_subtoken_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a list of tokens to a list of subtoken ids.\n\n        Args:\n          tokens: a list of strings.\n        Returns:\n          a list of integers in the range [0, vocab_size)\n        \"\"\"", "\n", "ret", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ret", ".", "extend", "(", "self", ".", "_token_to_subtoken_ids", "(", "token", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._token_to_subtoken_ids": [[560, 576], ["text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_ids", "hash", "text_encoder._escape_token"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_ids", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder._escape_token"], ["", "def", "_token_to_subtoken_ids", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\"Converts token to a list of subtoken ids.\n\n        Args:\n          token: a string.\n        Returns:\n          a list of integers in the range [0, vocab_size)\n        \"\"\"", "\n", "cache_location", "=", "hash", "(", "token", ")", "%", "self", ".", "_cache_size", "\n", "cache_key", ",", "cache_value", "=", "self", ".", "_cache", "[", "cache_location", "]", "\n", "if", "cache_key", "==", "token", ":", "\n", "            ", "return", "cache_value", "\n", "", "ret", "=", "self", ".", "_escaped_token_to_subtoken_ids", "(", "\n", "_escape_token", "(", "token", ",", "self", ".", "_alphabet", ")", ")", "\n", "self", ".", "_cache", "[", "cache_location", "]", "=", "(", "token", ",", "ret", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._subtoken_ids_to_tokens": [[577, 595], ["concatenated.split", "text_encoder.SubwordTextEncoder._subtoken_id_to_subtoken_string", "text_encoder._unescape_token", "ret.append"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._subtoken_id_to_subtoken_string", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder._unescape_token"], ["", "def", "_subtoken_ids_to_tokens", "(", "self", ",", "subtokens", ")", ":", "\n", "        ", "\"\"\"Converts a list of subtoken ids to a list of tokens.\n\n        Args:\n          subtokens: a list of integers in the range [0, vocab_size)\n        Returns:\n          a list of strings.\n        \"\"\"", "\n", "concatenated", "=", "\"\"", ".", "join", "(", "\n", "[", "self", ".", "_subtoken_id_to_subtoken_string", "(", "s", ")", "for", "s", "in", "subtokens", "]", ")", "\n", "split", "=", "concatenated", ".", "split", "(", "\"_\"", ")", "\n", "ret", "=", "[", "]", "\n", "for", "t", "in", "split", ":", "\n", "            ", "if", "t", ":", "\n", "                ", "unescaped", "=", "_unescape_token", "(", "t", "+", "\"_\"", ")", "\n", "if", "unescaped", ":", "\n", "                    ", "ret", ".", "append", "(", "unescaped", ")", "\n", "", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._subtoken_id_to_subtoken_string": [[596, 601], ["None"], "methods", ["None"], ["", "def", "_subtoken_id_to_subtoken_string", "(", "self", ",", "subtoken", ")", ":", "\n", "        ", "\"\"\"Converts a subtoken integer ID to a subtoken string.\"\"\"", "\n", "if", "0", "<=", "subtoken", "<", "self", ".", "vocab_size", ":", "\n", "            ", "return", "self", ".", "_all_subtoken_strings", "[", "subtoken", "]", "\n", "", "return", "u\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_strings": [[602, 631], ["len", "six.moves.range", "min", "ret.append"], "methods", ["None"], ["", "def", "_escaped_token_to_subtoken_strings", "(", "self", ",", "escaped_token", ")", ":", "\n", "        ", "\"\"\"Converts an escaped token string to a list of subtoken strings.\n\n        Args:\n          escaped_token: An escaped token as a unicode string.\n        Returns:\n          A list of subtokens as unicode strings.\n        \"\"\"", "\n", "# NOTE: This algorithm is greedy; it won't necessarily produce the \"best\"", "\n", "# list of subtokens.", "\n", "ret", "=", "[", "]", "\n", "start", "=", "0", "\n", "token_len", "=", "len", "(", "escaped_token", ")", "\n", "while", "start", "<", "token_len", ":", "\n", "            ", "for", "end", "in", "range", "(", "\n", "min", "(", "token_len", ",", "start", "+", "self", ".", "_max_subtoken_len", ")", ",", "start", ",", "-", "1", ")", ":", "\n", "                ", "subtoken", "=", "escaped_token", "[", "start", ":", "end", "]", "\n", "if", "subtoken", "in", "self", ".", "_subtoken_string_to_id", ":", "\n", "                    ", "ret", ".", "append", "(", "subtoken", ")", "\n", "start", "=", "end", "\n", "break", "\n", "\n", "", "", "else", ":", "# Did not break", "\n", "# If there is no possible encoding of the escaped token then one of the", "\n", "# characters in the token is not in the alphabet. This should be", "\n", "# impossible and would be indicative of a bug.", "\n", "                ", "assert", "False", ",", "\"Token substring not found in subtoken vocabulary.\"", "\n", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_ids": [[632, 643], ["text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_strings"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_strings"], ["", "def", "_escaped_token_to_subtoken_ids", "(", "self", ",", "escaped_token", ")", ":", "\n", "        ", "\"\"\"Converts an escaped token string to a list of subtoken IDs.\n\n        Args:\n          escaped_token: An escaped token as a unicode string.\n        Returns:\n          A list of subtoken IDs as integers.\n        \"\"\"", "\n", "return", "[", "\n", "self", ".", "_subtoken_string_to_id", "[", "subtoken", "]", "\n", "for", "subtoken", "in", "self", ".", "_escaped_token_to_subtoken_strings", "(", "escaped_token", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.build_from_generator": [[645, 676], ["collections.defaultdict", "cls.build_to_target_size", "data_loaders.tokenizer.encode", "text_encoder.native_to_unicode"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.build_to_target_size", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode"], ["", "@", "classmethod", "\n", "def", "build_from_generator", "(", "cls", ",", "\n", "generator", ",", "\n", "target_vocab_size", ",", "\n", "max_subtoken_length", "=", "None", ",", "\n", "reserved_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\"Builds a SubwordTextEncoder from the generated text.\n\n        Args:\n          generator: yields text.\n          target_vocab_size: int, approximate vocabulary size to create.\n          max_subtoken_length: Maximum length of a subtoken. If this is not set,\n            then the runtime and memory use of creating the vocab is quadratic in\n            the length of the longest token. If this is set, then it is instead\n            O(max_subtoken_length * length of longest token).\n          reserved_tokens: List of reserved tokens. The global variable\n            `RESERVED_TOKENS` must be a prefix of `reserved_tokens`. If this\n            argument is `None`, it will use `RESERVED_TOKENS`.\n\n        Returns:\n          SubwordTextEncoder with `vocab_size` approximately `target_vocab_size`.\n        \"\"\"", "\n", "token_counts", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "for", "item", "in", "generator", ":", "\n", "            ", "for", "tok", "in", "tokenizer", ".", "encode", "(", "native_to_unicode", "(", "item", ")", ")", ":", "\n", "                ", "token_counts", "[", "tok", "]", "+=", "1", "\n", "", "", "encoder", "=", "cls", ".", "build_to_target_size", "(", "\n", "target_vocab_size", ",", "token_counts", ",", "1", ",", "1e3", ",", "\n", "max_subtoken_length", "=", "max_subtoken_length", ",", "\n", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.build_to_target_size": [[677, 750], ["text_encoder.SubwordTextEncoder.build_to_target_size.bisect"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_to_target_size", "(", "cls", ",", "\n", "target_size", ",", "\n", "token_counts", ",", "\n", "min_val", ",", "\n", "max_val", ",", "\n", "max_subtoken_length", "=", "None", ",", "\n", "reserved_tokens", "=", "None", ",", "\n", "num_iterations", "=", "4", ")", ":", "\n", "        ", "\"\"\"Builds a SubwordTextEncoder that has `vocab_size` near `target_size`.\n\n        Uses simple recursive binary search to find a minimum token count that most\n        closely matches the `target_size`.\n\n        Args:\n          target_size: Desired vocab_size to approximate.\n          token_counts: A dictionary of token counts, mapping string to int.\n          min_val: An integer; lower bound for the minimum token count.\n          max_val: An integer; upper bound for the minimum token count.\n          max_subtoken_length: Maximum length of a subtoken. If this is not set,\n            then the runtime and memory use of creating the vocab is quadratic in\n            the length of the longest token. If this is set, then it is instead\n            O(max_subtoken_length * length of longest token).\n          reserved_tokens: List of reserved tokens. The global variable\n            `RESERVED_TOKENS` must be a prefix of `reserved_tokens`. If this\n            argument is `None`, it will use `RESERVED_TOKENS`.\n          num_iterations: An integer; how many iterations of refinement.\n\n        Returns:\n          A SubwordTextEncoder instance.\n\n        Raises:\n          ValueError: If `min_val` is greater than `max_val`.\n        \"\"\"", "\n", "if", "min_val", ">", "max_val", ":", "\n", "            ", "raise", "ValueError", "(", "\"Lower bound for the minimum token count \"", "\n", "\"is greater than the upper bound.\"", ")", "\n", "", "if", "target_size", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Target size must be positive.\"", ")", "\n", "\n", "", "if", "reserved_tokens", "is", "None", ":", "\n", "            ", "reserved_tokens", "=", "RESERVED_TOKENS", "\n", "\n", "", "def", "bisect", "(", "min_val", ",", "max_val", ")", ":", "\n", "            ", "\"\"\"Bisection to find the right size.\"\"\"", "\n", "present_count", "=", "(", "max_val", "+", "min_val", ")", "//", "2", "\n", "tf", ".", "logging", ".", "info", "(", "\"Trying min_count %d\"", "%", "present_count", ")", "\n", "subtokenizer", "=", "cls", "(", ")", "\n", "subtokenizer", ".", "build_from_token_counts", "(", "\n", "token_counts", ",", "present_count", ",", "num_iterations", ",", "\n", "max_subtoken_length", "=", "max_subtoken_length", ",", "\n", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "\n", "# Being within 1% of the target size is ok.", "\n", "is_ok", "=", "abs", "(", "subtokenizer", ".", "vocab_size", "-", "target_size", ")", "*", "100", "<", "target_size", "\n", "# If min_val == max_val, we can't do any better than this.", "\n", "if", "is_ok", "or", "min_val", ">=", "max_val", "or", "present_count", "<", "2", ":", "\n", "                ", "return", "subtokenizer", "\n", "\n", "", "if", "subtokenizer", ".", "vocab_size", ">", "target_size", ":", "\n", "                ", "other_subtokenizer", "=", "bisect", "(", "present_count", "+", "1", ",", "max_val", ")", "\n", "", "else", ":", "\n", "                ", "other_subtokenizer", "=", "bisect", "(", "min_val", ",", "present_count", "-", "1", ")", "\n", "\n", "", "if", "other_subtokenizer", "is", "None", ":", "\n", "                ", "return", "subtokenizer", "\n", "\n", "", "if", "(", "abs", "(", "other_subtokenizer", ".", "vocab_size", "-", "target_size", ")", "<", "\n", "abs", "(", "subtokenizer", ".", "vocab_size", "-", "target_size", ")", ")", ":", "\n", "                ", "return", "other_subtokenizer", "\n", "", "return", "subtokenizer", "\n", "\n", "", "return", "bisect", "(", "min_val", ",", "max_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.build_from_token_counts": [[751, 858], ["itertools.chain", "text_encoder.SubwordTextEncoder._init_alphabet_from_tokens", "text_encoder.SubwordTextEncoder._init_subtokens_from_list", "six.moves.range", "zip", "six.iterkeys", "list", "tensorflow.logging.info", "collections.defaultdict", "six.iteritems", "six.iteritems", "six.moves.range", "new_subtoken_strings.extend", "new_subtoken_strings.sort", "text_encoder.SubwordTextEncoder._init_subtokens_from_list", "tensorflow.logging.info", "text_encoder.native_to_unicode", "text_encoder._escape_token", "text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_strings", "len", "ValueError", "six.moves.range", "len", "len_to_subtoken_strings[].add", "len", "len", "min", "len", "len_to_subtoken_strings.append", "six.moves.range", "collections.defaultdict.get", "set", "new_subtoken_strings.append"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_alphabet_from_tokens", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_subtokens_from_list", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_subtokens_from_list", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder._escape_token", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._escaped_token_to_subtoken_strings", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get"], ["", "def", "build_from_token_counts", "(", "self", ",", "\n", "token_counts", ",", "\n", "min_count", ",", "\n", "num_iterations", "=", "4", ",", "\n", "reserved_tokens", "=", "None", ",", "\n", "max_subtoken_length", "=", "None", ")", ":", "\n", "        ", "\"\"\"Train a SubwordTextEncoder based on a dictionary of word counts.\n\n        Args:\n          token_counts: a dictionary of Unicode strings to int.\n          min_count: an integer - discard subtokens with lower counts.\n          num_iterations: an integer.  how many iterations of refinement.\n          reserved_tokens: List of reserved tokens. The global variable\n            `RESERVED_TOKENS` must be a prefix of `reserved_tokens`. If this\n            argument is `None`, it will use `RESERVED_TOKENS`.\n          max_subtoken_length: Maximum length of a subtoken. If this is not set,\n            then the runtime and memory use of creating the vocab is quadratic in\n            the length of the longest token. If this is set, then it is instead\n            O(max_subtoken_length * length of longest token).\n\n        Raises:\n          ValueError: if reserved is not 0 or len(RESERVED_TOKENS). In this case, it\n            is not clear what the space is being reserved for, or when it will be\n            filled in.\n        \"\"\"", "\n", "if", "reserved_tokens", "is", "None", ":", "\n", "            ", "reserved_tokens", "=", "RESERVED_TOKENS", "\n", "", "else", ":", "\n", "# There is not complete freedom in replacing RESERVED_TOKENS.", "\n", "            ", "for", "default", ",", "proposed", "in", "zip", "(", "RESERVED_TOKENS", ",", "reserved_tokens", ")", ":", "\n", "                ", "if", "default", "!=", "proposed", ":", "\n", "                    ", "raise", "ValueError", "(", "\"RESERVED_TOKENS must be a prefix of \"", "\n", "\"reserved_tokens.\"", ")", "\n", "\n", "# Initialize the alphabet. Note, this must include reserved tokens or it can", "\n", "# result in encoding failures.", "\n", "", "", "", "alphabet_tokens", "=", "chain", "(", "six", ".", "iterkeys", "(", "token_counts", ")", ",", "\n", "[", "native_to_unicode", "(", "t", ")", "for", "t", "in", "reserved_tokens", "]", ")", "\n", "\n", "self", ".", "_init_alphabet_from_tokens", "(", "alphabet_tokens", ")", "\n", "\n", "# Bootstrap the initial list of subtokens with the characters from the", "\n", "# alphabet plus the escaping characters.", "\n", "self", ".", "_init_subtokens_from_list", "(", "list", "(", "self", ".", "_alphabet", ")", ",", "\n", "reserved_tokens", "=", "reserved_tokens", ")", "\n", "\n", "# We build iteratively.  On each iteration, we segment all the words,", "\n", "# then count the resulting potential subtokens, keeping the ones", "\n", "# with high enough counts for our new vocabulary.", "\n", "if", "min_count", "<", "1", ":", "\n", "            ", "min_count", "=", "1", "\n", "", "for", "i", "in", "range", "(", "num_iterations", ")", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Iteration {0}\"", ".", "format", "(", "i", ")", ")", "\n", "\n", "# Collect all substrings of the encoded token that break along current", "\n", "# subtoken boundaries.", "\n", "subtoken_counts", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "for", "token", ",", "count", "in", "six", ".", "iteritems", "(", "token_counts", ")", ":", "\n", "                ", "escaped_token", "=", "_escape_token", "(", "token", ",", "self", ".", "_alphabet", ")", "\n", "subtokens", "=", "self", ".", "_escaped_token_to_subtoken_strings", "(", "escaped_token", ")", "\n", "start", "=", "0", "\n", "for", "subtoken", "in", "subtokens", ":", "\n", "                    ", "last_position", "=", "len", "(", "escaped_token", ")", "+", "1", "\n", "if", "max_subtoken_length", "is", "not", "None", ":", "\n", "                        ", "last_position", "=", "min", "(", "last_position", ",", "start", "+", "max_subtoken_length", ")", "\n", "\n", "", "for", "end", "in", "range", "(", "start", "+", "1", ",", "last_position", ")", ":", "\n", "                        ", "new_subtoken", "=", "escaped_token", "[", "start", ":", "end", "]", "\n", "subtoken_counts", "[", "new_subtoken", "]", "+=", "count", "\n", "", "start", "+=", "len", "(", "subtoken", ")", "\n", "\n", "# Array of sets of candidate subtoken strings, by length.", "\n", "", "", "len_to_subtoken_strings", "=", "[", "]", "\n", "for", "subtoken_string", ",", "count", "in", "six", ".", "iteritems", "(", "subtoken_counts", ")", ":", "\n", "                ", "lsub", "=", "len", "(", "subtoken_string", ")", "\n", "if", "count", ">=", "min_count", ":", "\n", "                    ", "while", "len", "(", "len_to_subtoken_strings", ")", "<=", "lsub", ":", "\n", "                        ", "len_to_subtoken_strings", ".", "append", "(", "set", "(", ")", ")", "\n", "", "len_to_subtoken_strings", "[", "lsub", "]", ".", "add", "(", "subtoken_string", ")", "\n", "\n", "# Consider the candidates longest to shortest, so that if we accept", "\n", "# a longer subtoken string, we can decrement the counts of its prefixes.", "\n", "", "", "new_subtoken_strings", "=", "[", "]", "\n", "for", "lsub", "in", "range", "(", "len", "(", "len_to_subtoken_strings", ")", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "                ", "subtoken_strings", "=", "len_to_subtoken_strings", "[", "lsub", "]", "\n", "for", "subtoken_string", "in", "subtoken_strings", ":", "\n", "                    ", "count", "=", "subtoken_counts", "[", "subtoken_string", "]", "\n", "if", "count", ">=", "min_count", ":", "\n", "# Exclude alphabet tokens here, as they must be included later,", "\n", "# explicitly, regardless of count.", "\n", "                        ", "if", "subtoken_string", "not", "in", "self", ".", "_alphabet", ":", "\n", "                            ", "new_subtoken_strings", ".", "append", "(", "(", "count", ",", "subtoken_string", ")", ")", "\n", "", "for", "l", "in", "range", "(", "1", ",", "lsub", ")", ":", "\n", "                            ", "subtoken_counts", "[", "subtoken_string", "[", ":", "l", "]", "]", "-=", "count", "\n", "\n", "# Include the alphabet explicitly to guarantee all strings are encodable.", "\n", "", "", "", "", "new_subtoken_strings", ".", "extend", "(", "(", "subtoken_counts", ".", "get", "(", "a", ",", "0", ")", ",", "a", ")", "\n", "for", "a", "in", "self", ".", "_alphabet", ")", "\n", "new_subtoken_strings", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n", "# Reinitialize to the candidate vocabulary.", "\n", "new_subtoken_strings", "=", "[", "subtoken", "for", "_", ",", "subtoken", "in", "new_subtoken_strings", "]", "\n", "if", "reserved_tokens", ":", "\n", "                ", "new_subtoken_strings", "=", "reserved_tokens", "+", "new_subtoken_strings", "\n", "\n", "", "self", ".", "_init_subtokens_from_list", "(", "new_subtoken_strings", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"vocab_size = %d\"", "%", "self", ".", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.all_subtoken_strings": [[859, 862], ["tuple"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "all_subtoken_strings", "(", "self", ")", ":", "\n", "        ", "return", "tuple", "(", "self", ".", "_all_subtoken_strings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.dump": [[863, 869], ["print", "six.iteritems", "sorted"], "methods", ["None"], ["", "def", "dump", "(", "self", ")", ":", "\n", "        ", "\"\"\"Debugging dump of the current subtoken vocabulary.\"\"\"", "\n", "subtoken_strings", "=", "[", "(", "i", ",", "s", ")", "\n", "for", "s", ",", "i", "in", "six", ".", "iteritems", "(", "self", ".", "_subtoken_string_to_id", ")", "]", "\n", "print", "(", "u\", \"", ".", "join", "(", "u\"{0} : '{1}'\"", ".", "format", "(", "i", ",", "s", ")", "\n", "for", "i", ",", "s", "in", "sorted", "(", "subtoken_strings", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_subtokens_from_list": [[870, 902], ["max", "len", "len", "enumerate"], "methods", ["None"], ["", "def", "_init_subtokens_from_list", "(", "self", ",", "subtoken_strings", ",", "reserved_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize token information from a list of subtoken strings.\n\n        Args:\n          subtoken_strings: a list of subtokens\n          reserved_tokens: List of reserved tokens. We must have `reserved_tokens`\n            as None or the empty list, or else the global variable `RESERVED_TOKENS`\n            must be a prefix of `reserved_tokens`.\n\n        Raises:\n          ValueError: if reserved is not 0 or len(RESERVED_TOKENS). In this case, it\n            is not clear what the space is being reserved for, or when it will be\n            filled in.\n        \"\"\"", "\n", "if", "reserved_tokens", "is", "None", ":", "\n", "            ", "reserved_tokens", "=", "[", "]", "\n", "\n", "", "if", "reserved_tokens", ":", "\n", "            ", "self", ".", "_all_subtoken_strings", "=", "reserved_tokens", "+", "subtoken_strings", "\n", "", "else", ":", "\n", "            ", "self", ".", "_all_subtoken_strings", "=", "subtoken_strings", "\n", "\n", "# we remember the maximum length of any subtoken to avoid having to", "\n", "# check arbitrarily long strings.", "\n", "", "self", ".", "_max_subtoken_len", "=", "max", "(", "[", "len", "(", "s", ")", "for", "s", "in", "subtoken_strings", "]", ")", "\n", "self", ".", "_subtoken_string_to_id", "=", "{", "\n", "s", ":", "i", "+", "len", "(", "reserved_tokens", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "subtoken_strings", ")", "if", "s", "\n", "}", "\n", "# Initialize the cache to empty.", "\n", "self", ".", "_cache_size", "=", "2", "**", "20", "\n", "self", ".", "_cache", "=", "[", "(", "None", ",", "None", ")", "]", "*", "self", ".", "_cache_size", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_alphabet_from_tokens": [[903, 909], ["None"], "methods", ["None"], ["", "def", "_init_alphabet_from_tokens", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Initialize alphabet from an iterable of token or subtoken strings.\"\"\"", "\n", "# Include all characters from all tokens in the alphabet to guarantee that", "\n", "# any token can be encoded. Additionally, include all escaping characters.", "\n", "self", ".", "_alphabet", "=", "{", "c", "for", "token", "in", "tokens", "for", "c", "in", "token", "}", "\n", "self", ".", "_alphabet", "|=", "_ESCAPE_CHARS", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._load_from_file_object": [[910, 926], ["text_encoder.SubwordTextEncoder._init_subtokens_from_list", "text_encoder.SubwordTextEncoder._init_alphabet_from_tokens", "line.strip", "subtoken_strings.append", "text_encoder.native_to_unicode", "line.strip.startswith", "line.strip.endswith", "line.strip.startswith", "line.strip.endswith"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_subtokens_from_list", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._init_alphabet_from_tokens", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode"], ["", "def", "_load_from_file_object", "(", "self", ",", "f", ")", ":", "\n", "        ", "\"\"\"Load from a file object.\n\n        Args:\n          f: File object to load vocabulary from\n        \"\"\"", "\n", "subtoken_strings", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "s", "=", "line", ".", "strip", "(", ")", "\n", "# Some vocab files wrap words in single quotes, but others don't", "\n", "if", "(", "(", "s", ".", "startswith", "(", "\"'\"", ")", "and", "s", ".", "endswith", "(", "\"'\"", ")", ")", "or", "\n", "(", "s", ".", "startswith", "(", "\"\\\"\"", ")", "and", "s", ".", "endswith", "(", "\"\\\"\"", ")", ")", ")", ":", "\n", "                ", "s", "=", "s", "[", "1", ":", "-", "1", "]", "\n", "", "subtoken_strings", ".", "append", "(", "native_to_unicode", "(", "s", ")", ")", "\n", "", "self", ".", "_init_subtokens_from_list", "(", "subtoken_strings", ")", "\n", "self", ".", "_init_alphabet_from_tokens", "(", "subtoken_strings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._load_from_file": [[927, 933], ["tensorflow.gfile.Exists", "ValueError", "tensorflow.gfile.Open", "text_encoder.SubwordTextEncoder._load_from_file_object"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder._load_from_file_object"], ["", "def", "_load_from_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load from a vocab file.\"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "filename", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"File %s not found\"", "%", "filename", ")", "\n", "", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ")", "as", "f", ":", "\n", "            ", "self", ".", "_load_from_file_object", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.store_to_file": [[934, 941], ["tensorflow.gfile.Open", "f.write", "f.write", "text_encoder.unicode_to_native", "text_encoder.unicode_to_native"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.unicode_to_native", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.unicode_to_native"], ["", "", "def", "store_to_file", "(", "self", ",", "filename", ",", "add_single_quotes", "=", "True", ")", ":", "\n", "        ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "subtoken_string", "in", "self", ".", "_all_subtoken_strings", ":", "\n", "                ", "if", "add_single_quotes", ":", "\n", "                    ", "f", ".", "write", "(", "\"'\"", "+", "unicode_to_native", "(", "subtoken_string", ")", "+", "\"'\\n\"", ")", "\n", "", "else", ":", "\n", "                    ", "f", ".", "write", "(", "unicode_to_native", "(", "subtoken_string", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.__init__": [[946, 951], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_reserved_ids", "=", "0", ",", "height", "=", "None", ",", "width", "=", "None", ",", "channels", "=", "3", ")", ":", "\n", "        ", "assert", "num_reserved_ids", "==", "0", "\n", "self", ".", "_height", "=", "height", "\n", "self", ".", "_width", "=", "width", "\n", "self", ".", "_channels", "=", "channels", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.num_reserved_ids": [[952, 955], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_reserved_ids", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.encode": [[956, 972], ["im.imread", "tensorflow.logging.warning", "NotImplementedError"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "\"\"\"Transform a string with a filename into a list of RGB integers.\n\n        Args:\n          s: path to the file with an image.\n\n        Returns:\n          ids: list of integers\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "matplotlib", ".", "image", "as", "im", "# pylint: disable=g-import-not-at-top", "\n", "", "except", "ImportError", "as", "e", ":", "\n", "            ", "tf", ".", "logging", ".", "warning", "(", "\n", "\"Reading an image requires matplotlib to be installed: %s\"", ",", "e", ")", "\n", "raise", "NotImplementedError", "(", "\"Image reading not implemented.\"", ")", "\n", "", "return", "im", ".", "imread", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.decode": [[973, 1010], ["tempfile.mkstemp", "int", "len", "ValueError", "tensorflow.Graph().as_default", "tensorflow.constant", "tensorflow.image.encode_png", "tensorflow.write_file", "math.sqrt", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.Session", "sess.run", "tensorflow.Graph", "len", "len", "len", "str"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "\"\"\"Transform a sequence of int ids into an image file.\n\n        Args:\n          ids: list of integers to be converted.\n          strip_extraneous: unused\n\n        Returns:\n          Path to the temporary file where the image was saved.\n\n        Raises:\n          ValueError: if the ids are not of the appropriate size.\n        \"\"\"", "\n", "del", "strip_extraneous", "\n", "_", ",", "tmp_file_path", "=", "tempfile", ".", "mkstemp", "(", "\"_decode.png\"", ")", "\n", "if", "self", ".", "_height", "is", "None", "or", "self", ".", "_width", "is", "None", ":", "\n", "            ", "size", "=", "int", "(", "math", ".", "sqrt", "(", "len", "(", "ids", ")", "/", "self", ".", "_channels", ")", ")", "\n", "length", "=", "size", "*", "size", "*", "self", ".", "_channels", "\n", "", "else", ":", "\n", "            ", "size", "=", "None", "\n", "length", "=", "self", ".", "_height", "*", "self", ".", "_width", "*", "self", ".", "_channels", "\n", "", "if", "len", "(", "ids", ")", "!=", "length", ":", "\n", "            ", "raise", "ValueError", "(", "\"Length of ids (%d) must be height (%d) x width (%d) x \"", "\n", "\"channels (%d); %d != %d.\\n Ids: %s\"", "\n", "%", "(", "len", "(", "ids", ")", ",", "self", ".", "_height", ",", "self", ".", "_width", ",", "self", ".", "_channels", ",", "\n", "len", "(", "ids", ")", ",", "length", ",", "\" \"", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "ids", "]", ")", ")", ")", "\n", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "            ", "raw", "=", "tf", ".", "constant", "(", "ids", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "if", "size", "is", "None", ":", "\n", "                ", "img", "=", "tf", ".", "reshape", "(", "raw", ",", "[", "self", ".", "_height", ",", "self", ".", "_width", ",", "self", ".", "_channels", "]", ")", "\n", "", "else", ":", "\n", "                ", "img", "=", "tf", ".", "reshape", "(", "raw", ",", "[", "size", ",", "size", ",", "self", ".", "_channels", "]", ")", "\n", "", "png", "=", "tf", ".", "image", ".", "encode_png", "(", "img", ")", "\n", "op", "=", "tf", ".", "write_file", "(", "tmp_file_path", ",", "png", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "                ", "sess", ".", "run", "(", "op", ")", "\n", "", "", "return", "tmp_file_path", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.decode_list": [[1011, 1021], ["text_encoder.ImageEncoder.decode"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode"], ["", "def", "decode_list", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Transform a sequence of int ids into an image file.\n\n        Args:\n          ids: list of integers to be converted.\n\n        Returns:\n          Singleton list: path to the temporary file where the image was saved.\n        \"\"\"", "\n", "return", "[", "self", ".", "decode", "(", "ids", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.ImageEncoder.vocab_size": [[1022, 1025], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "256", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode": [[1030, 1040], ["float", "s.split"], "methods", ["None"], ["def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "\"\"\"Transform a string (space separated float values) into a float array.\n\n        Args:\n          s: space separated float values.\n\n        Returns:\n          Array of float values.\n        \"\"\"", "\n", "return", "[", "float", "(", "w", ")", "for", "w", "in", "s", ".", "split", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode": [[1041, 1056], ["None"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "ids", ",", "strip_extraneous", "=", "False", ")", ":", "\n", "        ", "\"\"\"Transform sequence of float values into string (float values).\n\n        Args:\n          ids: array of floats to be converted.\n          strip_extraneous: unused\n\n        Returns:\n          String having space separated float values.\n\n        Raises:\n          ValueError: if the ids are not of the appropriate size.\n        \"\"\"", "\n", "del", "strip_extraneous", "\n", "return", "\" \"", ".", "join", "(", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.native_to_unicode": [[60, 62], ["text_encoder.is_unicode", "text_encoder.to_unicode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.is_unicode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.to_unicode"], ["def", "native_to_unicode", "(", "s", ")", ":", "\n", "    ", "return", "s", "if", "is_unicode", "(", "s", ")", "else", "to_unicode", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.unicode_to_native": [[64, 69], ["text_encoder.is_unicode", "s.encode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.is_unicode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "unicode_to_native", "(", "s", ")", ":", "\n", "    ", "if", "six", ".", "PY2", ":", "\n", "        ", "return", "s", ".", "encode", "(", "\"utf-8\"", ")", "if", "is_unicode", "(", "s", ")", "else", "s", "\n", "", "else", ":", "\n", "        ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.is_unicode": [[71, 79], ["isinstance", "isinstance"], "function", ["None"], ["", "", "def", "is_unicode", "(", "s", ")", ":", "\n", "    ", "if", "six", ".", "PY2", ":", "\n", "        ", "if", "isinstance", "(", "s", ",", "unicode", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "s", ",", "str", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.to_unicode": [[81, 86], ["text_encoder.is_unicode", "s.decode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.is_unicode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode"], ["", "def", "to_unicode", "(", "s", ",", "ignore_errors", "=", "False", ")", ":", "\n", "    ", "if", "is_unicode", "(", "s", ")", ":", "\n", "        ", "return", "s", "\n", "", "error_mode", "=", "\"ignore\"", "if", "ignore_errors", "else", "\"strict\"", "\n", "return", "s", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "error_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.to_unicode_ignore_errors": [[88, 90], ["text_encoder.to_unicode"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.to_unicode"], ["", "def", "to_unicode_ignore_errors", "(", "s", ")", ":", "\n", "    ", "return", "to_unicode", "(", "s", ",", "ignore_errors", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder._escape_token": [[401, 424], ["token.replace().replace.replace().replace", "isinstance", "ValueError", "token.replace().replace.replace", "type", "ord"], "function", ["None"], ["", "", "", "", "def", "_escape_token", "(", "token", ",", "alphabet", ")", ":", "\n", "    ", "\"\"\"Escape away underscores and OOV characters and append '_'.\n\n    This allows the token to be expressed as the concatenation of a list\n    of subtokens from the vocabulary. The underscore acts as a sentinel\n    which allows us to invertibly concatenate multiple such lists.\n\n    Args:\n      token: A unicode string to be escaped.\n      alphabet: A set of all characters in the vocabulary's alphabet.\n\n    Returns:\n      escaped_token: An escaped unicode string.\n\n    Raises:\n      ValueError: If the provided token is not unicode.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "token", ",", "six", ".", "text_type", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Expected string type for token, got %s\"", "%", "type", "(", "token", ")", ")", "\n", "\n", "", "token", "=", "token", ".", "replace", "(", "u\"\\\\\"", ",", "u\"\\\\\\\\\"", ")", ".", "replace", "(", "u\"_\"", ",", "u\"\\\\u\"", ")", "\n", "ret", "=", "[", "c", "if", "c", "in", "alphabet", "and", "c", "!=", "u\"\\n\"", "else", "r\"\\%d;\"", "%", "ord", "(", "c", ")", "for", "c", "in", "token", "]", "\n", "return", "u\"\"", ".", "join", "(", "ret", ")", "+", "\"_\"", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder._unescape_token": [[426, 447], ["_UNESCAPE_REGEX.sub", "escaped_token.endswith", "m.group", "six.unichr", "int", "m.group", "m.group"], "function", ["None"], ["", "def", "_unescape_token", "(", "escaped_token", ")", ":", "\n", "    ", "\"\"\"Inverse of _escape_token().\n\n    Args:\n      escaped_token: a unicode string\n\n    Returns:\n      token: a unicode string\n    \"\"\"", "\n", "\n", "def", "match", "(", "m", ")", ":", "\n", "        ", "if", "m", ".", "group", "(", "1", ")", "is", "None", ":", "\n", "            ", "return", "u\"_\"", "if", "m", ".", "group", "(", "0", ")", "==", "u\"\\\\u\"", "else", "u\"\\\\\"", "\n", "\n", "", "try", ":", "\n", "            ", "return", "six", ".", "unichr", "(", "int", "(", "m", ".", "group", "(", "1", ")", ")", ")", "\n", "", "except", "(", "ValueError", ",", "OverflowError", ")", "as", "_", ":", "\n", "            ", "return", "u\"\\u3013\"", "# Unicode for undefined character.", "\n", "\n", "", "", "trimmed", "=", "escaped_token", "[", ":", "-", "1", "]", "if", "escaped_token", ".", "endswith", "(", "\"_\"", ")", "else", "escaped_token", "\n", "return", "_UNESCAPE_REGEX", ".", "sub", "(", "match", ",", "trimmed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.strip_ids": [[1058, 1064], ["list", "list.pop"], "function", ["None"], ["", "", "def", "strip_ids", "(", "ids", ",", "ids_to_strip", ")", ":", "\n", "    ", "\"\"\"Strip ids_to_strip from the end ids.\"\"\"", "\n", "ids", "=", "list", "(", "ids", ")", "\n", "while", "ids", "[", "-", "1", "]", "in", "ids_to_strip", ":", "\n", "        ", "ids", ".", "pop", "(", ")", "\n", "", "return", "ids", "\n", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.__init__": [[15, 19], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "name", "=", "None", "# yelp, amazon; set in dataset-specific constructor", "\n", "self", ".", "conf", "=", "None", "# dataset-specific config", "\n", "self", ".", "subwordenc", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.concat_docs": [[20, 29], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "concat_docs", "(", "docs", ",", "edok_token", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Concatenate a list of strs by joining them with the end of doc token\n        \"\"\"", "\n", "if", "edok_token", ":", "\n", "            ", "return", "' {} '", ".", "format", "(", "EDOC_TOK", ")", ".", "join", "(", "docs", ")", "\n", "", "else", ":", "\n", "            ", "return", "' '", ".", "join", "(", "docs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs": [[30, 36], ["docs.split"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "split_docs", "(", "docs", ")", ":", "\n", "        ", "\"\"\"\n        Return a list of strs (docs is one str)\n        \"\"\"", "\n", "return", "docs", ".", "split", "(", "' {} '", ".", "format", "(", "EDOC_TOK", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.__init__": [[39, 41], ["summ_dataset.SummDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SummReviewDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch": [[42, 113], ["enumerate", "zip", "torch.LongTensor", "torch.stack", "max", "len", "numpy.zeros", "enumerate", "torch.from_numpy", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "summ_dataset.SummDataset.split_docs", "len", "batch.append", "numpy.zeros", "models.nn_utils.move_to_cuda.astype", "summ_dataset.SummReviewDataset.subwordenc.encode", "summ_dataset.SummReviewDataset.insert", "summ_dataset.SummReviewDataset.append", "summ_dataset.SummReviewDataset.subwordenc.encode", "len", "doc_ids.insert", "doc_ids.append"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummDataset.split_docs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode"], ["", "def", "prepare_batch", "(", "self", ",", "\n", "texts_batch", ",", "ratings_batch", ",", "\n", "global_prepend_id", "=", "None", ",", "global_append_id", "=", "None", ",", "\n", "doc_prepend_id", "=", "None", ",", "doc_append_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Prepare batch of texts and labels from DataLoader as input into nn.\n\n        Args:\n            texts_batch: list of str's\n                - length batch_size\n                - each str is a concatenated group of document\n            ratings_batch: list of size-1 LongTensor's\n                - length_batch_size\n\n            global_prepend_id: int (prepend GO)\n            global_append_id: int (append EOS)\n            doc_prepend_id: int (prepend DOC before start of each review)\n            doc_append_id: int (append /DOC after end of each review)\n\n        Returns: (cuda)\n            x: LongTensor of size [batch_size, max_seq_len]\n            lengths: LongTensor (length of each text in subtokens)\n            labels: LongTensor of size [batch_size]\n        \"\"\"", "\n", "# Original ratings go from 1-5", "\n", "labels_batch", "=", "[", "rating", "-", "1", "for", "rating", "in", "ratings_batch", "]", "\n", "\n", "batch", "=", "[", "]", "\n", "for", "i", ",", "text", "in", "enumerate", "(", "texts_batch", ")", ":", "\n", "# Split apart by docs and potentially add delimiters", "\n", "            ", "docs", "=", "SummDataset", ".", "split_docs", "(", "text", ")", "# list of strs", "\n", "if", "doc_prepend_id", "or", "doc_append_id", ":", "\n", "                ", "docs_ids", "=", "[", "self", ".", "subwordenc", ".", "encode", "(", "doc", ")", "for", "doc", "in", "docs", "]", "\n", "if", "doc_prepend_id", ":", "\n", "                    ", "for", "doc_ids", "in", "docs_ids", ":", "\n", "                        ", "doc_ids", ".", "insert", "(", "0", ",", "doc_prepend_id", ")", "\n", "", "", "if", "doc_append_id", ":", "\n", "                    ", "for", "doc_ids", "in", "docs_ids", ":", "\n", "                        ", "doc_ids", ".", "append", "(", "doc_append_id", ")", "\n", "", "", "docs_ids", "=", "[", "id", "for", "doc_ids", "in", "docs_ids", "for", "id", "in", "doc_ids", "]", "# flatten", "\n", "subtoken_ids", "=", "docs_ids", "\n", "", "else", ":", "\n", "                ", "subtoken_ids", "=", "self", ".", "subwordenc", ".", "encode", "(", "' '", ".", "join", "(", "docs", ")", ")", "\n", "\n", "# Add start and end token for concatenated set of documents", "\n", "", "if", "global_prepend_id", ":", "\n", "                ", "subtoken_ids", ".", "insert", "(", "0", ",", "global_prepend_id", ")", "\n", "", "if", "global_append_id", ":", "\n", "                ", "subtoken_ids", ".", "append", "(", "global_append_id", ")", "\n", "", "seq_len", "=", "len", "(", "subtoken_ids", ")", "\n", "batch", ".", "append", "(", "(", "subtoken_ids", ",", "seq_len", ",", "labels_batch", "[", "i", "]", ")", ")", "\n", "\n", "", "texts_ids", ",", "lengths", ",", "labels", "=", "zip", "(", "*", "batch", ")", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "lengths", ")", "\n", "labels", "=", "torch", ".", "stack", "(", "labels", ")", "\n", "\n", "# Pad each text", "\n", "max_seq_len", "=", "max", "(", "lengths", ")", "\n", "batch_size", "=", "len", "(", "batch", ")", "\n", "x", "=", "np", ".", "zeros", "(", "(", "batch_size", ",", "max_seq_len", ")", ")", "\n", "for", "i", ",", "text_ids", "in", "enumerate", "(", "texts_ids", ")", ":", "\n", "            ", "padded", "=", "np", ".", "zeros", "(", "max_seq_len", ")", "\n", "padded", "[", ":", "len", "(", "text_ids", ")", "]", "=", "text_ids", "\n", "x", "[", "i", ",", ":", "]", "=", "padded", "\n", "", "x", "=", "torch", ".", "from_numpy", "(", "x", ".", "astype", "(", "int", ")", ")", "\n", "\n", "x", "=", "move_to_cuda", "(", "x", ")", "\n", "lengths", "=", "move_to_cuda", "(", "lengths", ")", "\n", "labels", "=", "move_to_cuda", "(", "labels", ")", "\n", "\n", "return", "x", ",", "lengths", ",", "labels", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.build_subword_encoder.main": [[39, 73], ["print", "data_loaders.text_encoder.SubwordTextEncoder.build_to_target_size", "print", "os.path.join", "text_encoder.SubwordTextEncoder.build_to_target_size.store_to_file", "os.path.join", "utils.save_file", "pdb.set_trace", "data_loaders.summ_dataset_factory.SummDatasetFactory.get", "SummDatasetFactory.get.get_data_loader", "print", "print", "data_loaders.tokenizer.corpus_token_counts", "open", "data_loaders.tokenizer.corpus_token_counts", "ValueError", "f.write"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.build_to_target_size", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.SubwordTextEncoder.store_to_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.save_file", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset_factory.SummDatasetFactory.get", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.amazon_dataset.AmazonDataset.get_data_loader", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.corpus_token_counts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.tokenizer.corpus_token_counts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write"], ["def", "main", "(", "opt", ")", ":", "\n", "    ", "if", "opt", ".", "dataset", ":", "\n", "        ", "dataset", "=", "SummDatasetFactory", ".", "get", "(", "opt", ".", "dataset", ")", "\n", "dl", "=", "dataset", ".", "get_data_loader", "(", "split", "=", "'train'", ",", "n_docs", "=", "1", ",", "sample_reviews", "=", "False", ",", "\n", "batch_size", "=", "1", ",", "num_workers", "=", "0", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "'Writing reviews to file'", ")", "\n", "with", "open", "(", "'/tmp/{}_data.txt'", ".", "format", "(", "opt", ".", "dataset", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "texts", ",", "ratings", ",", "metadata", "in", "dl", ":", "\n", "                ", "f", ".", "write", "(", "'{}\\n'", ".", "format", "(", "texts", "[", "0", "]", ")", ")", "\n", "", "", "print", "(", "'Creating token counts'", ")", "\n", "token_counts", "=", "tokenizer", ".", "corpus_token_counts", "(", "\n", "'/tmp/{}_data.txt'", ".", "format", "(", "opt", ".", "dataset", ")", ",", "\n", "opt", ".", "corpus_max_lines", ",", "\n", "split_on_newlines", "=", "True", ")", "\n", "", "elif", "opt", ".", "corpus_filepattern", ":", "\n", "        ", "token_counts", "=", "tokenizer", ".", "corpus_token_counts", "(", "\n", "opt", ".", "corpus_filepattern", ",", "\n", "opt", ".", "corpus_max_lines", ",", "\n", "split_on_newlines", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Must provide --dataset or provide --corpus_filepattern'", ")", "\n", "\n", "", "print", "(", "'Building to target size'", ")", "\n", "encoder", "=", "text_encoder", ".", "SubwordTextEncoder", ".", "build_to_target_size", "(", "\n", "opt", ".", "target_size", ",", "token_counts", ",", "0", ",", "1e9", ",", "\n", "reserved_tokens", "=", "RESERVED_TOKENS", ")", "\n", "\n", "print", "(", "'Saving tokenizer'", ")", "\n", "vocab_fp", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "opt", ".", "output_fn", "+", "'.txt'", ")", "# stores vocab coutns", "\n", "encoder", ".", "store_to_file", "(", "vocab_fp", ")", "\n", "enc_fp", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "output_dir", ",", "opt", ".", "output_fn", "+", "'.pkl'", ")", "\n", "save_file", "(", "encoder", ",", "enc_fp", ",", "verbose", "=", "True", ")", "\n", "\n", "pdb", ".", "set_trace", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.update_tensorboard.execute_cmd": [[23, 25], ["subprocess.call", "cmd.split"], "function", ["None"], ["def", "execute_cmd", "(", "cmd", ")", ":", "\n", "    ", "p", "=", "subprocess", ".", "call", "(", "cmd", ".", "split", "(", ")", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "STDOUT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.TensorBoardWSGI.__init__": [[163, 228], ["path_prefix.endswith", "set", "set.add", "plugin.get_plugin_apps.items", "ValueError", "_VALID_PLUGIN_RE.match", "ValueError", "ValueError", "plugin.get_plugin_apps", "tensorflow.logging.warning", "route.startswith", "ValueError", "type", "type", "str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "plugins", ",", "path_prefix", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"Constructs TensorBoardWSGI instance.\n\n    Args:\n      plugins: A list of base_plugin.TBPlugin subclass instances.\n      path_prefix: A prefix of the path when app isn't served from root.\n\n    Returns:\n      A WSGI application for the set of all TBPlugin instances.\n\n    Raises:\n      ValueError: If some plugin has no plugin_name\n      ValueError: If some plugin has an invalid plugin_name (plugin\n          names must only contain [A-Za-z0-9_.-])\n      ValueError: If two plugins have the same plugin_name\n      ValueError: If some plugin handles a route that does not start\n          with a slash\n    \"\"\"", "\n", "self", ".", "_plugins", "=", "plugins", "\n", "if", "path_prefix", ".", "endswith", "(", "'/'", ")", ":", "\n", "      ", "self", ".", "_path_prefix", "=", "path_prefix", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "      ", "self", ".", "_path_prefix", "=", "path_prefix", "\n", "\n", "", "self", ".", "data_applications", "=", "{", "\n", "# TODO(@chihuahua): Delete this RPC once we have skylark rules that", "\n", "# obviate the need for the frontend to determine which plugins are", "\n", "# active.", "\n", "self", ".", "_path_prefix", "+", "DATA_PREFIX", "+", "PLUGINS_LISTING_ROUTE", ":", "\n", "self", ".", "_serve_plugins_listing", ",", "\n", "}", "\n", "\n", "# Serve the routes from the registered plugins using their name as the route", "\n", "# prefix. For example if plugin z has two routes /a and /b, they will be", "\n", "# served as /data/plugin/z/a and /data/plugin/z/b.", "\n", "plugin_names_encountered", "=", "set", "(", ")", "\n", "for", "plugin", "in", "self", ".", "_plugins", ":", "\n", "      ", "if", "plugin", ".", "plugin_name", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "'Plugin %s has no plugin_name'", "%", "plugin", ")", "\n", "", "if", "not", "_VALID_PLUGIN_RE", ".", "match", "(", "plugin", ".", "plugin_name", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'Plugin %s has invalid name %r'", "%", "(", "plugin", ",", "\n", "plugin", ".", "plugin_name", ")", ")", "\n", "", "if", "plugin", ".", "plugin_name", "in", "plugin_names_encountered", ":", "\n", "        ", "raise", "ValueError", "(", "'Duplicate plugins for name %s'", "%", "plugin", ".", "plugin_name", ")", "\n", "", "plugin_names_encountered", ".", "add", "(", "plugin", ".", "plugin_name", ")", "\n", "\n", "try", ":", "\n", "        ", "plugin_apps", "=", "plugin", ".", "get_plugin_apps", "(", ")", "\n", "", "except", "Exception", "as", "e", ":", "# pylint: disable=broad-except", "\n", "        ", "if", "type", "(", "plugin", ")", "is", "core_plugin", ".", "CorePlugin", ":", "# pylint: disable=unidiomatic-typecheck", "\n", "          ", "raise", "\n", "", "tf", ".", "logging", ".", "warning", "(", "'Plugin %s failed. Exception: %s'", ",", "\n", "plugin", ".", "plugin_name", ",", "str", "(", "e", ")", ")", "\n", "continue", "\n", "", "for", "route", ",", "app", "in", "plugin_apps", ".", "items", "(", ")", ":", "\n", "        ", "if", "not", "route", ".", "startswith", "(", "'/'", ")", ":", "\n", "          ", "raise", "ValueError", "(", "'Plugin named %r handles invalid route %r: '", "\n", "'route does not start with a slash'", "%", "\n", "(", "plugin", ".", "plugin_name", ",", "route", ")", ")", "\n", "", "if", "type", "(", "plugin", ")", "is", "core_plugin", ".", "CorePlugin", ":", "# pylint: disable=unidiomatic-typecheck", "\n", "          ", "path", "=", "self", ".", "_path_prefix", "+", "route", "\n", "", "else", ":", "\n", "          ", "path", "=", "self", ".", "_path_prefix", "+", "DATA_PREFIX", "+", "PLUGIN_PREFIX", "+", "'/'", "+", "plugin", ".", "plugin_name", "+", "route", "\n", "", "self", ".", "data_applications", "[", "path", "]", "=", "app", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.TensorBoardWSGI._serve_plugins_listing": [[229, 248], ["tensorboard.backend.http_util.Respond", "time.time", "plugin.is_active", "tensorflow.logging.info", "time.time"], "methods", ["None"], ["", "", "", "@", "wrappers", ".", "Request", ".", "application", "\n", "def", "_serve_plugins_listing", "(", "self", ",", "request", ")", ":", "\n", "    ", "\"\"\"Serves an object mapping plugin name to whether it is enabled.\n\n    Args:\n      request: The werkzeug.Request object.\n\n    Returns:\n      A werkzeug.Response object.\n    \"\"\"", "\n", "response", "=", "{", "}", "\n", "for", "plugin", "in", "self", ".", "_plugins", ":", "\n", "      ", "start", "=", "time", ".", "time", "(", ")", "\n", "response", "[", "plugin", ".", "plugin_name", "]", "=", "plugin", ".", "is_active", "(", ")", "\n", "elapsed", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "'Plugin listing: is_active() for %s took %0.3f seconds'", ",", "\n", "plugin", ".", "plugin_name", ",", "elapsed", ")", "\n", "", "return", "http_util", ".", "Respond", "(", "request", ",", "response", ",", "'application/json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.TensorBoardWSGI.__call__": [[249, 276], ["werkzeug.wrappers.Request", "six.moves.urllib.parse.urlparse", "tensorboard_application._clean_path", "tensorflow.logging.warning", "tensorboard.backend.http_util.Respond"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application._clean_path"], ["", "def", "__call__", "(", "self", ",", "environ", ",", "start_response", ")", ":", "# pylint: disable=invalid-name", "\n", "    ", "\"\"\"Central entry point for the TensorBoard application.\n\n    This method handles routing to sub-applications. It does simple routing\n    using regular expression matching.\n\n    This __call__ method conforms to the WSGI spec, so that instances of this\n    class are WSGI applications.\n\n    Args:\n      environ: See WSGI spec.\n      start_response: See WSGI spec.\n\n    Returns:\n      A werkzeug Response.\n    \"\"\"", "\n", "request", "=", "wrappers", ".", "Request", "(", "environ", ")", "\n", "parsed_url", "=", "urlparse", ".", "urlparse", "(", "request", ".", "path", ")", "\n", "clean_path", "=", "_clean_path", "(", "parsed_url", ".", "path", ",", "self", ".", "_path_prefix", ")", "\n", "\n", "# pylint: disable=too-many-function-args", "\n", "if", "clean_path", "in", "self", ".", "data_applications", ":", "\n", "      ", "return", "self", ".", "data_applications", "[", "clean_path", "]", "(", "environ", ",", "start_response", ")", "\n", "", "else", ":", "\n", "      ", "tf", ".", "logging", ".", "warning", "(", "'path %s not found, sending 404'", ",", "clean_path", ")", "\n", "return", "http_util", ".", "Respond", "(", "request", ",", "'Not found'", ",", "'text/plain'", ",", "code", "=", "404", ")", "(", "\n", "environ", ",", "start_response", ")", "\n", "# pylint: enable=too-many-function-args", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.tensor_size_guidance_from_flags": [[54, 66], ["dict", "flags.samples_per_plugin.split", "token.strip().split", "int", "token.strip"], "function", ["None"], ["def", "tensor_size_guidance_from_flags", "(", "flags", ")", ":", "\n", "  ", "\"\"\"Apply user per-summary size guidance overrides.\"\"\"", "\n", "\n", "tensor_size_guidance", "=", "dict", "(", "DEFAULT_TENSOR_SIZE_GUIDANCE", ")", "\n", "if", "not", "flags", "or", "not", "flags", ".", "samples_per_plugin", ":", "\n", "    ", "return", "tensor_size_guidance", "\n", "\n", "", "for", "token", "in", "flags", ".", "samples_per_plugin", ".", "split", "(", "','", ")", ":", "\n", "    ", "k", ",", "v", "=", "token", ".", "strip", "(", ")", ".", "split", "(", "'='", ")", "\n", "tensor_size_guidance", "[", "k", "]", "=", "int", "(", "v", ")", "\n", "\n", "", "return", "tensor_size_guidance", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.standard_tensorboard_wsgi": [[68, 130], ["tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer", "tensorboard_application.get_database_info", "tensorboard.plugins.base_plugin.TBContext", "tensorboard_application.TensorBoardWSGIApp", "default.get_assets_zip_provider", "constructor", "tensorboard_application.tensor_size_guidance_from_flags"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.get_database_info", "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.TensorBoardWSGIApp", "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.tensor_size_guidance_from_flags"], ["", "def", "standard_tensorboard_wsgi", "(", "\n", "logdir", ",", "\n", "purge_orphaned_data", ",", "\n", "reload_interval", ",", "\n", "plugins", ",", "\n", "db_uri", "=", "\"\"", ",", "\n", "assets_zip_provider", "=", "None", ",", "\n", "path_prefix", "=", "\"\"", ",", "\n", "window_title", "=", "\"\"", ",", "\n", "max_reload_threads", "=", "None", ",", "\n", "flags", "=", "None", ")", ":", "\n", "  ", "\"\"\"Construct a TensorBoardWSGIApp with standard plugins and multiplexer.\n\n  Args:\n    logdir: The path to the directory containing events files.\n    purge_orphaned_data: Whether to purge orphaned data.\n    reload_interval: The interval at which the backend reloads more data in\n        seconds.  Zero means load once at startup; negative means never load.\n    plugins: A list of constructor functions for TBPlugin subclasses.\n    db_uri: A String containing the URI of the SQL database for persisting\n        data, or empty for memory-only mode.\n    assets_zip_provider: See TBContext documentation for more information.\n        If this value is not specified, this function will attempt to load\n        the `tensorboard.default` module to use the default. This behavior\n        might be removed in the future.\n    path_prefix: A prefix of the path when app isn't served from root.\n    window_title: A string specifying the the window title.\n    max_reload_threads: The max number of threads that TensorBoard can use\n        to reload runs. Not relevant for db mode. Each thread reloads one run\n        at a time.\n    flags: A dict of the runtime flags provided to the application, or None.\n  Returns:\n    The new TensorBoard WSGI application.\n  \"\"\"", "\n", "if", "assets_zip_provider", "is", "None", ":", "\n", "    ", "from", "tensorboard", "import", "default", "\n", "assets_zip_provider", "=", "default", ".", "get_assets_zip_provider", "(", ")", "\n", "", "multiplexer", "=", "event_multiplexer", ".", "EventMultiplexer", "(", "\n", "size_guidance", "=", "DEFAULT_SIZE_GUIDANCE", ",", "\n", "tensor_size_guidance", "=", "tensor_size_guidance_from_flags", "(", "flags", ")", ",", "\n", "purge_orphaned_data", "=", "purge_orphaned_data", ",", "\n", "max_reload_threads", "=", "max_reload_threads", ")", "\n", "db_module", ",", "db_connection_provider", "=", "get_database_info", "(", "db_uri", ")", "\n", "# In DB mode, always disable loading event files.", "\n", "if", "db_connection_provider", ":", "\n", "    ", "reload_interval", "=", "-", "1", "\n", "", "plugin_name_to_instance", "=", "{", "}", "\n", "context", "=", "base_plugin", ".", "TBContext", "(", "\n", "db_module", "=", "db_module", ",", "\n", "db_connection_provider", "=", "db_connection_provider", ",", "\n", "db_uri", "=", "db_uri", ",", "\n", "flags", "=", "flags", ",", "\n", "logdir", "=", "logdir", ",", "\n", "multiplexer", "=", "multiplexer", ",", "\n", "assets_zip_provider", "=", "assets_zip_provider", ",", "\n", "plugin_name_to_instance", "=", "plugin_name_to_instance", ",", "\n", "window_title", "=", "window_title", ")", "\n", "plugin_instances", "=", "[", "constructor", "(", "context", ")", "for", "constructor", "in", "plugins", "]", "\n", "for", "plugin_instance", "in", "plugin_instances", ":", "\n", "    ", "plugin_name_to_instance", "[", "plugin_instance", ".", "plugin_name", "]", "=", "plugin_instance", "\n", "", "return", "TensorBoardWSGIApp", "(", "\n", "logdir", ",", "plugin_instances", ",", "multiplexer", ",", "reload_interval", ",", "path_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.TensorBoardWSGIApp": [[132, 158], ["tensorboard_application.parse_event_files_spec", "tensorboard_application.TensorBoardWSGI", "tensorboard_application.start_reloading_multiplexer"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.parse_event_files_spec", "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.start_reloading_multiplexer"], ["", "def", "TensorBoardWSGIApp", "(", "logdir", ",", "plugins", ",", "multiplexer", ",", "reload_interval", ",", "\n", "path_prefix", ")", ":", "\n", "  ", "\"\"\"Constructs the TensorBoard application.\n\n  Args:\n    logdir: the logdir spec that describes where data will be loaded.\n      may be a directory, or comma,separated list of directories, or colons\n      can be used to provide named directories\n    plugins: A list of base_plugin.TBPlugin subclass instances.\n    multiplexer: The EventMultiplexer with TensorBoard data to serve\n    reload_interval: How often (in seconds) to reload the Multiplexer.\n      Zero means reload just once at startup; negative means never load.\n    path_prefix: A prefix of the path when app isn't served from root.\n\n  Returns:\n    A WSGI application that implements the TensorBoard backend.\n\n  Raises:\n    ValueError: If something is wrong with the plugin configuration.\n  \"\"\"", "\n", "path_to_run", "=", "parse_event_files_spec", "(", "logdir", ")", "\n", "if", "reload_interval", ">=", "0", ":", "\n", "# We either reload the multiplexer once when TensorBoard starts up, or we", "\n", "# continuously reload the multiplexer.", "\n", "    ", "start_reloading_multiplexer", "(", "multiplexer", ",", "path_to_run", ",", "reload_interval", ")", "\n", "", "return", "TensorBoardWSGI", "(", "plugins", ",", "path_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.parse_event_files_spec": [[279, 320], ["re.compile", "logdir.split", "specification.partition", "re.compile.match", "os.path.realpath", "re.compile.match", "os.path.splitdrive"], "function", ["None"], ["", "", "", "def", "parse_event_files_spec", "(", "logdir", ")", ":", "\n", "  ", "\"\"\"Parses `logdir` into a map from paths to run group names.\n\n  The events files flag format is a comma-separated list of path specifications.\n  A path specification either looks like 'group_name:/path/to/directory' or\n  '/path/to/directory'; in the latter case, the group is unnamed. Group names\n  cannot start with a forward slash: /foo:bar/baz will be interpreted as a\n  spec with no name and path '/foo:bar/baz'.\n\n  Globs are not supported.\n\n  Args:\n    logdir: A comma-separated list of run specifications.\n  Returns:\n    A dict mapping directory paths to names like {'/path/to/directory': 'name'}.\n    Groups without an explicit name are named after their path. If logdir is\n    None, returns an empty dict, which is helpful for testing things that don't\n    require any valid runs.\n  \"\"\"", "\n", "files", "=", "{", "}", "\n", "if", "logdir", "is", "None", ":", "\n", "    ", "return", "files", "\n", "# Make sure keeping consistent with ParseURI in core/lib/io/path.cc", "\n", "", "uri_pattern", "=", "re", ".", "compile", "(", "'[a-zA-Z][0-9a-zA-Z.]*://.*'", ")", "\n", "for", "specification", "in", "logdir", ".", "split", "(", "','", ")", ":", "\n", "# Check if the spec contains group. A spec start with xyz:// is regarded as", "\n", "# URI path spec instead of group spec. If the spec looks like /foo:bar/baz,", "\n", "# then we assume it's a path with a colon. If the spec looks like", "\n", "# [a-zA-z]:\\foo then we assume its a Windows path and not a single letter", "\n", "# group", "\n", "    ", "if", "(", "uri_pattern", ".", "match", "(", "specification", ")", "is", "None", "and", "':'", "in", "specification", "and", "\n", "specification", "[", "0", "]", "!=", "'/'", "and", "not", "os", ".", "path", ".", "splitdrive", "(", "specification", ")", "[", "0", "]", ")", ":", "\n", "# We split at most once so run_name:/path:with/a/colon will work.", "\n", "      ", "run_name", ",", "_", ",", "path", "=", "specification", ".", "partition", "(", "':'", ")", "\n", "", "else", ":", "\n", "      ", "run_name", "=", "None", "\n", "path", "=", "specification", "\n", "", "if", "uri_pattern", ".", "match", "(", "path", ")", "is", "None", ":", "\n", "      ", "path", "=", "os", ".", "path", ".", "realpath", "(", "path", ")", "\n", "", "files", "[", "path", "]", "=", "run_name", "\n", "", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.reload_multiplexer": [[322, 338], ["time.time", "tensorflow.logging.info", "six.iteritems", "tensorflow.logging.info", "multiplexer.Reload", "tensorflow.logging.info", "multiplexer.AddRunsFromDirectory", "time.time"], "function", ["None"], ["", "def", "reload_multiplexer", "(", "multiplexer", ",", "path_to_run", ")", ":", "\n", "  ", "\"\"\"Loads all runs into the multiplexer.\n\n  Args:\n    multiplexer: The `EventMultiplexer` to add runs to and reload.\n    path_to_run: A dict mapping from paths to run names, where `None` as the run\n      name is interpreted as a run name equal to the path.\n  \"\"\"", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'TensorBoard reload process beginning'", ")", "\n", "for", "(", "path", ",", "name", ")", "in", "six", ".", "iteritems", "(", "path_to_run", ")", ":", "\n", "    ", "multiplexer", ".", "AddRunsFromDirectory", "(", "path", ",", "name", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'TensorBoard reload process: Reload the whole Multiplexer'", ")", "\n", "multiplexer", ".", "Reload", "(", ")", "\n", "duration", "=", "time", ".", "time", "(", ")", "-", "start", "\n", "tf", ".", "logging", ".", "info", "(", "'TensorBoard done reloading. Load took %0.3f secs'", ",", "duration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.start_reloading_multiplexer": [[340, 378], ["threading.Thread", "threading.Thread.start", "ValueError", "tensorboard_application.reload_multiplexer", "time.sleep"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.reload_multiplexer"], ["", "def", "start_reloading_multiplexer", "(", "multiplexer", ",", "path_to_run", ",", "load_interval", ")", ":", "\n", "  ", "\"\"\"Starts a thread to automatically reload the given multiplexer.\n\n  If `load_interval` is positive, the thread will reload the multiplexer\n  by calling `ReloadMultiplexer` every `load_interval` seconds, starting\n  immediately. Otherwise, reloads the multiplexer once and never again.\n\n  Args:\n    multiplexer: The `EventMultiplexer` to add runs to and reload.\n    path_to_run: A dict mapping from paths to run names, where `None` as the run\n      name is interpreted as a run name equal to the path.\n    load_interval: An integer greater than or equal to 0. If positive, how many\n      seconds to wait after one load before starting the next load. Otherwise,\n      reloads the multiplexer once and never again (no continuous reloading).\n\n  Returns:\n    A started `threading.Thread` that reloads the multiplexer.\n\n  Raises:\n    ValueError: If `load_interval` is negative.\n  \"\"\"", "\n", "if", "load_interval", "<", "0", ":", "\n", "    ", "raise", "ValueError", "(", "'load_interval is negative: %d'", "%", "load_interval", ")", "\n", "\n", "# We don't call multiplexer.Reload() here because that would make", "\n", "# AddRunsFromDirectory block until the runs have all loaded.", "\n", "", "def", "_reload", "(", ")", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "reload_multiplexer", "(", "multiplexer", ",", "path_to_run", ")", "\n", "if", "load_interval", "==", "0", ":", "\n", "# Only load the multiplexer once. Do not continuously reload.", "\n", "        ", "break", "\n", "", "time", ".", "sleep", "(", "load_interval", ")", "\n", "\n", "", "", "thread", "=", "threading", ".", "Thread", "(", "target", "=", "_reload", ",", "name", "=", "'Reloader'", ")", "\n", "thread", ".", "daemon", "=", "True", "\n", "thread", ".", "start", "(", ")", "\n", "return", "thread", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.get_database_info": [[380, 400], ["six.moves.urllib.parse.urlparse", "ValueError", "tensorboard_application.create_sqlite_connection_provider"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.create_sqlite_connection_provider"], ["", "def", "get_database_info", "(", "db_uri", ")", ":", "\n", "  ", "\"\"\"Returns TBContext fields relating to SQL database.\n\n  Args:\n    db_uri: A string URI expressing the DB file, e.g. \"sqlite:~/tb.db\".\n\n  Returns:\n    A tuple with the db_module and db_connection_provider TBContext fields. If\n    db_uri was empty, then (None, None) is returned.\n\n  Raises:\n    ValueError: If db_uri scheme is not supported.\n  \"\"\"", "\n", "if", "not", "db_uri", ":", "\n", "    ", "return", "None", ",", "None", "\n", "", "scheme", "=", "urlparse", ".", "urlparse", "(", "db_uri", ")", ".", "scheme", "\n", "if", "scheme", "==", "'sqlite'", ":", "\n", "    ", "return", "sqlite3", ",", "create_sqlite_connection_provider", "(", "db_uri", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "'Only sqlite DB URIs are supported now: '", "+", "db_uri", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application.create_sqlite_connection_provider": [[402, 426], ["six.moves.urllib.parse.urlparse", "os.path.expanduser", "tensorboard_application._get_connect_params", "ValueError", "ValueError", "ValueError", "tensorboard.db.Connection", "sqlite3.connect"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application._get_connect_params"], ["", "", "def", "create_sqlite_connection_provider", "(", "db_uri", ")", ":", "\n", "  ", "\"\"\"Returns function that returns SQLite Connection objects.\n\n  Args:\n    db_uri: A string URI expressing the DB file, e.g. \"sqlite:~/tb.db\".\n\n  Returns:\n    A function that returns a new PEP-249 DB Connection, which must be closed,\n    each time it is called.\n\n  Raises:\n    ValueError: If db_uri is not a valid sqlite file URI.\n  \"\"\"", "\n", "uri", "=", "urlparse", ".", "urlparse", "(", "db_uri", ")", "\n", "if", "uri", ".", "scheme", "!=", "'sqlite'", ":", "\n", "    ", "raise", "ValueError", "(", "'Scheme is not sqlite: '", "+", "db_uri", ")", "\n", "", "if", "uri", ".", "netloc", ":", "\n", "    ", "raise", "ValueError", "(", "'Can not connect to SQLite over network: '", "+", "db_uri", ")", "\n", "", "if", "uri", ".", "path", "==", "':memory:'", ":", "\n", "    ", "raise", "ValueError", "(", "'Memory mode SQLite not supported: '", "+", "db_uri", ")", "\n", "", "path", "=", "os", ".", "path", ".", "expanduser", "(", "uri", ".", "path", ")", "\n", "params", "=", "_get_connect_params", "(", "uri", ".", "query", ")", "\n", "# TODO(@jart): Add thread-local pooling.", "\n", "return", "lambda", ":", "db", ".", "Connection", "(", "sqlite3", ".", "connect", "(", "path", ",", "**", "params", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application._get_connect_params": [[428, 433], ["six.moves.urllib.parse.parse_qs", "any", "ValueError", "json.loads", "urlparse.parse_qs.items", "len", "urlparse.parse_qs.values"], "function", ["None"], ["", "def", "_get_connect_params", "(", "query", ")", ":", "\n", "  ", "params", "=", "urlparse", ".", "parse_qs", "(", "query", ")", "\n", "if", "any", "(", "len", "(", "v", ")", ">", "2", "for", "v", "in", "params", ".", "values", "(", ")", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'DB URI params list has duplicate keys: '", "+", "query", ")", "\n", "", "return", "{", "k", ":", "json", ".", "loads", "(", "v", "[", "0", "]", ")", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.scripts.tensorboard_application._clean_path": [[435, 453], ["path.endswith"], "function", ["None"], ["", "def", "_clean_path", "(", "path", ",", "path_prefix", "=", "\"\"", ")", ":", "\n", "  ", "\"\"\"Cleans the path of the request.\n\n  Removes the ending '/' if the request begins with the path prefix and pings a\n  non-empty route.\n\n  Arguments:\n    path: The path of a request.\n    path_prefix: The prefix string that every route of this TensorBoard instance\n    starts with.\n\n  Returns:\n    The route to use to serve the request (with the path prefix stripped if\n    applicable).\n  \"\"\"", "\n", "if", "path", "!=", "path_prefix", "+", "'/'", "and", "path", ".", "endswith", "(", "'/'", ")", ":", "\n", "    ", "return", "path", "[", ":", "-", "1", "]", "\n", "", "return", "path", "\n", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.mLSTM.__init__": [[18, 34], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "layer_norm", "=", "False", ")", ":", "\n", "        ", "super", "(", "mLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "layer_norm", "=", "layer_norm", "\n", "\n", "self", ".", "wx", "=", "nn", ".", "Linear", "(", "input_size", ",", "4", "*", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "wh", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "4", "*", "hidden_size", ",", "bias", "=", "True", ")", "\n", "self", ".", "wmx", "=", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "wmh", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "\n", "if", "layer_norm", ":", "\n", "            ", "self", ".", "wx_norm", "=", "nn", ".", "LayerNorm", "(", "input_size", ")", "\n", "self", ".", "wh_norm", "=", "nn", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "self", ".", "wmx_norm", "=", "nn", ".", "LayerNorm", "(", "input_size", ")", "\n", "self", ".", "wmh_norm", "=", "nn", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.mLSTM.forward": [[35, 57], ["gates.chunk", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "mlstm.mLSTM.wmh_norm", "mlstm.mLSTM.wmx_norm", "mlstm.mLSTM.wmx", "mlstm.mLSTM.wmh", "mlstm.mLSTM.wh_norm", "mlstm.mLSTM.wx_norm", "mlstm.mLSTM.wx", "mlstm.mLSTM.wh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "data", ",", "hidden", ",", "cell", ")", ":", "\n", "        ", "hx", ",", "cx", "=", "hidden", ",", "cell", "\n", "\n", "hx", "=", "self", ".", "wmh_norm", "(", "hx", ")", "if", "self", ".", "layer_norm", "else", "hx", "\n", "data_wm", "=", "self", ".", "wmx_norm", "(", "data", ")", "if", "self", ".", "layer_norm", "else", "data", "\n", "m", "=", "self", ".", "wmx", "(", "data_wm", ")", "*", "self", ".", "wmh", "(", "hx", ")", "\n", "\n", "m", "=", "self", ".", "wh_norm", "(", "m", ")", "if", "self", ".", "layer_norm", "else", "m", "\n", "data_w", "=", "self", ".", "wx_norm", "(", "data", ")", "if", "self", ".", "layer_norm", "else", "data", "\n", "gates", "=", "self", ".", "wx", "(", "data_w", ")", "+", "self", ".", "wh", "(", "m", ")", "\n", "\n", "i", ",", "f", ",", "o", ",", "u", "=", "gates", ".", "chunk", "(", "4", ",", "1", ")", "\n", "\n", "i", "=", "torch", ".", "sigmoid", "(", "i", ")", "\n", "f", "=", "torch", ".", "sigmoid", "(", "f", ")", "\n", "u", "=", "torch", ".", "tanh", "(", "u", ")", "\n", "o", "=", "torch", ".", "sigmoid", "(", "o", ")", "\n", "\n", "cy", "=", "f", "*", "cx", "+", "i", "*", "u", "\n", "hy", "=", "o", "*", "torch", ".", "tanh", "(", "cy", ")", "\n", "\n", "return", "hy", ",", "cy", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.__init__": [[60, 80], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "range", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "cell", "mlstm.StackedLSTM.add_module"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cell", ",", "num_layers", ",", "input_size", ",", "hidden_size", ",", "output_size", ",", "dropout", ",", "layer_norm", "=", "False", ")", ":", "\n", "        ", "super", "(", "StackedLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n", "self", ".", "layer_norm", "=", "layer_norm", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "h2o", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "output_size", ")", "\n", "if", "layer_norm", ":", "\n", "            ", "self", ".", "h2o_norm", "=", "nn", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "\n", "", "self", ".", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "layer", "=", "cell", "(", "input_size", ",", "hidden_size", ",", "layer_norm", "=", "layer_norm", ")", "\n", "self", ".", "add_module", "(", "'layer_%d'", "%", "i", ",", "layer", ")", "\n", "self", ".", "layers", "+=", "[", "layer", "]", "\n", "input_size", "=", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.forward": [[81, 117], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "mlstm.StackedLSTM.h2o", "getattr", "getattr.", "mlstm.StackedLSTM.h2o_norm", "len", "mlstm.StackedLSTM.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "cell", ")", ":", "\n", "        ", "\"\"\"\n        One time step\n\n        Args:\n            input: [batch_size, input_size]\n            hidden: [batch_size, n_layers, hidden]\n            cell: [batch_size, n_layers, hidden]\n\n        Returns:\n            hidden: [batch_size, n_layers, hidden]\n            cell: [batch_size, n_layers, hidden]\n            output: [batch_size, output_size]\n        \"\"\"", "\n", "h_0", ",", "c_0", "=", "hidden", ",", "cell", "\n", "h_1", ",", "c_1", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "'layer_{}'", ".", "format", "(", "i", ")", ")", "\n", "h_1_i", ",", "c_1_i", "=", "layer", "(", "input", ",", "h_0", "[", ":", ",", "i", ",", ":", "]", ",", "c_0", "[", ":", ",", "i", ",", ":", "]", ")", "\n", "if", "i", "==", "0", ":", "\n", "                ", "input", "=", "h_1_i", "\n", "", "else", ":", "\n", "                ", "input", "=", "input", "+", "h_1_i", "\n", "", "if", "i", "!=", "len", "(", "self", ".", "layers", ")", ":", "\n", "                ", "input", "=", "self", ".", "dropout", "(", "input", ")", "\n", "", "h_1", "+=", "[", "h_1_i", "]", "\n", "c_1", "+=", "[", "c_1_i", "]", "\n", "\n", "", "h_1", "=", "torch", ".", "stack", "(", "h_1", ",", "dim", "=", "1", ")", "# [batch, n_layers, hidden]", "\n", "c_1", "=", "torch", ".", "stack", "(", "c_1", ",", "dim", "=", "1", ")", "# [batch, n_layers, hidden]", "\n", "\n", "if", "self", ".", "layer_norm", ":", "\n", "            ", "input", "=", "self", ".", "h2o_norm", "(", "input", ")", "\n", "", "output", "=", "self", ".", "h2o", "(", "input", ")", "\n", "\n", "return", "h_1", ",", "c_1", ",", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0": [[118, 122], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "state0", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "h_0", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "num_layers", ",", "self", ".", "hidden_size", ",", "requires_grad", "=", "False", ")", "\n", "c_0", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "num_layers", ",", "self", ".", "hidden_size", ",", "requires_grad", "=", "False", ")", "\n", "return", "h_0", ",", "c_0", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMEncoder.__init__": [[125, 129], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed", ",", "rnn", ")", ":", "\n", "        ", "super", "(", "StackedLSTMEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed", "=", "embed", "\n", "self", ".", "rnn", "=", "rnn", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMEncoder.forward": [[130, 167], ["input.size", "range", "mlstm.StackedLSTMEncoder.rnn", "input.dim", "mlstm.StackedLSTMEncoder.embed", "hiddens.append", "cells.append", "outputs.append", "input.dim", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "cell", ",", "\n", "all_hiddens", "=", "False", ",", "all_cells", "=", "False", ",", "all_outputs", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Embed input and forward through rnn seq_len time steps\n\n        Args:\n            input: [batch_size, seq_len] or [batch_size, seq_len, vocab_size]\n            hidden: [batch_size, n_layers, hidden_size]\n            cell: [batch_size, n_layers, hidden_size]\n\n            all_hiddens: boolean (return hidden state at every time step, otherwise return last hidden)\n            all_cells: boolean (return cell state at every time step, otherwise return last cell)\n            all_outputs: boolean (return output at every time step, otherwise return last output)\n\n        Returns:\n            hiddens: length seq_len list of hidden states\n            cells:  length seq_len list of cell states\n            outputs: length seq_len list of [batch, output_size] tensors\n\n        \"\"\"", "\n", "hiddens", ",", "cells", ",", "outputs", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "seq_len", "=", "input", ".", "size", "(", "1", ")", "\n", "for", "t", "in", "range", "(", "seq_len", ")", ":", "\n", "            ", "if", "input", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "emb", "=", "self", ".", "embed", "(", "input", "[", ":", ",", "t", "]", ")", "\n", "", "elif", "input", ".", "dim", "(", ")", "==", "3", ":", "# e.g. Gumbel softmax summaries", "\n", "                ", "emb", "=", "torch", ".", "matmul", "(", "input", "[", ":", ",", "t", ",", ":", "]", ",", "self", ".", "embed", ".", "weight", ")", "\n", "\n", "", "hidden", ",", "cell", ",", "output", "=", "self", ".", "rnn", "(", "emb", ",", "hidden", ",", "cell", ")", "\n", "if", "all_hiddens", "or", "(", "t", "==", "seq_len", "-", "1", ")", ":", "\n", "                ", "hiddens", ".", "append", "(", "hidden", ")", "\n", "", "if", "all_cells", "or", "(", "t", "==", "seq_len", "-", "1", ")", ":", "\n", "                ", "cells", ".", "append", "(", "cell", ")", "\n", "", "if", "all_outputs", "or", "(", "t", "==", "seq_len", "-", "1", ")", ":", "\n", "                ", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "", "", "return", "hiddens", ",", "cells", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMDecoder.__init__": [[170, 197], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed", ",", "rnn", ",", "\n", "use_docs_attn", "=", "False", ",", "attn_emb_size", "=", "None", ",", "attn_hidden_size", "=", "None", ",", "attn_learn_alpha", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            embed: nn.Embedding\n            rnn: StackedLSTM\n            use_emb_attn: boolean (whether to attend over input embedding's of size [batch, attn_emb_size])\n                - can be used to attend over multiple document representations\n            attn_emb_size: int (size of embeddings being attended over, e.g. hidden_size for documents)\n            attn_hidden_size: int (size of intermediate attention representation)\n            attn_learn_alpha: boolean (whether to average the context with previous hidden state or learn the weighting)\n        \"\"\"", "\n", "super", "(", "StackedLSTMDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed", "=", "embed", "\n", "self", ".", "rnn", "=", "rnn", "\n", "\n", "self", ".", "use_docs_attn", "=", "use_docs_attn", "\n", "self", ".", "attn_emb_size", "=", "attn_emb_size", "\n", "self", ".", "attn_hidden_size", "=", "attn_hidden_size", "\n", "self", ".", "attn_learn_alpha", "=", "attn_learn_alpha", "\n", "\n", "if", "use_docs_attn", ":", "\n", "            ", "self", ".", "attn_lin1", "=", "nn", ".", "Linear", "(", "attn_emb_size", ",", "attn_hidden_size", ")", "\n", "self", ".", "attn_act1", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "attn_lin2", "=", "nn", ".", "Linear", "(", "attn_hidden_size", ",", "1", ")", "\n", "self", ".", "context_alpha", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "[", "0.5", "]", ")", ")", "if", "self", ".", "attn_learn_alpha", "else", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMDecoder.forward": [[198, 329], ["init_input.size", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda().fill_", "init_input.long", "range", "targets.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "torch.Tensor().fill_().long", "mlstm.StackedLSTMDecoder.rnn", "models.nn_utils.logits_to_prob", "models.nn_utils.prob_to_vocab_id", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "range", "models.nn_utils.move_to_cuda", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "mlstm.StackedLSTMDecoder.embed", "mlstm.StackedLSTMDecoder.attn_lin1", "mlstm.StackedLSTMDecoder.attn_lin2", "torch.softmax", "torch.softmax", "torch.softmax", "context.sum.sum.sum", "decoded_texts.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "mlstm.StackedLSTMDecoder.attn_act1", "models.nn_utils.move_to_cuda.sum().item", "subwordenc.decode", "decoded_ids[].long().tolist", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "models.nn_utils.move_to_cuda.sum", "decoded_ids[].long"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.logits_to_prob", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.prob_to_vocab_id", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode"], ["", "", "def", "forward", "(", "self", ",", "init_hidden", ",", "init_cell", ",", "init_input", ",", "\n", "targets", "=", "None", ",", "\n", "seq_len", "=", "None", ",", "eos_id", "=", "EOS_ID", ",", "non_pad_prob_val", "=", "0", ",", "\n", "softmax_method", "=", "'softmax'", ",", "sample_method", "=", "'sample'", ",", "\n", "tau", "=", "1.0", ",", "eps", "=", "1e-10", ",", "gumbel_hard", "=", "False", ",", "\n", "encoder_hiddens", "=", "None", ",", "encoder_inputs", "=", "None", ",", "attend_to_embs", "=", "None", ",", "\n", "subwordenc", "=", "None", ",", "\n", "return_last_state", "=", "False", ",", "k", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        Decode. If targets is given, then use teacher forcing.\n\n        Notes:\n            This is also used by beam search by setting seq_len=1 and k=beam_size.\n            The comments talk about [batch * k^(t+1), but in practice this should only ever\n            be called with seq_len=1 (and hence t=0). The results from one beam step are then pruned,\n            before beam search repeats the step.\n\n        Args:\n            init_hidden: [batch_size, n_layers, hidden_size]\n            init_cell: [batch_size, n_layers, hidden_size]\n            init_input: [batch_size] (e.g. <EDOC> ids)\n\n            # For teacher forcing\n            targets: [batch_size, trg_seq_len]\n\n            # For non-teacher forcing\n            seq_len: int (length to generate)\n            eos_id: int (generate until every sequence in batch has generated eos, or until seq_len)\n            non_pad_prob_val: float\n                When replacing tokens after eos_id, set probability of non-pad tokens to this value\n                A small epsilon may be used if the log of the probs will be computed for a NLLLoss in order\n                to prevent Nans.\n\n            # Sampling, temperature, etc.\n            softmax_method: str (which version of softmax to get probabilities; 'gumbel' or 'softmax')\n            sample_method: str (how to sample words given probabilities; 'greedy', 'sample')\n            tau: float (temperature for softmax)\n            eps: float (controls sampling from Gumbel)\n            gumbel_hard: boolean (whether to produce one hot encodings for Gumbel Softmax)\n            subwordenc: SubwordTokenizer\n                (returns text if given)\n\n            # Additional inputs\n            encoder_hiddens: [batch_size, seq_len, hidden_size]\n                Hiddens at each time step. Would be used for attention\n            encoder_inputs: [batch_size, seq_len]\n                Would be used for a pointer network\n            attend_to_embs: [batch_size, n_docs, n_layers, hidden_size]\n                maybe just [batch, *, hidden]?\n                Embs to attend to. Could be last hidden states (i.e. document representations)\n\n            # Beam search\n            return_last_state: bool\n                (states used for beam search)\n            k: int (i.e. beam width)\n\n        Returns:\n            decoded_probs: [batch * k^(gen_len), gen_len, vocab]\n            decoded_ids: [batch * k^(gen_len), gen_len]\n            decoded_texts: list of str's if subwordenc is given\n            extra: dict of additional outputs\n        \"\"\"", "\n", "batch_size", "=", "init_input", ".", "size", "(", "0", ")", "\n", "output_len", "=", "seq_len", "if", "seq_len", "is", "not", "None", "else", "targets", ".", "size", "(", "1", ")", "\n", "vocab_size", "=", "self", ".", "rnn", ".", "h2o", ".", "out_features", "\n", "\n", "decoded_probs", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", "*", "k", ",", "output_len", ",", "vocab_size", ")", ")", "\n", "decoded_ids", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", "*", "k", ",", "output_len", ")", ".", "long", "(", ")", ")", "\n", "extra", "=", "{", "}", "\n", "\n", "rows_with_eos", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", "*", "k", ")", ".", "long", "(", ")", ")", "# track which sequences have generated eos_id", "\n", "pad_ids", "=", "move_to_cuda", "(", "torch", ".", "Tensor", "(", "batch_size", "*", "k", ")", ".", "fill_", "(", "PAD_ID", ")", ".", "long", "(", ")", ")", "\n", "pad_prob", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", "*", "k", ",", "vocab_size", ")", ")", ".", "fill_", "(", "non_pad_prob_val", ")", "\n", "pad_prob", "[", ":", ",", "PAD_ID", "]", "=", "1.0", "\n", "\n", "hidden", ",", "cell", "=", "init_hidden", ",", "init_cell", "# [batch, n_layers, hidden]", "\n", "input", "=", "init_input", ".", "long", "(", ")", "\n", "\n", "for", "t", "in", "range", "(", "output_len", ")", ":", "\n", "            ", "if", "gumbel_hard", "and", "t", "!=", "0", ":", "\n", "                ", "input_emb", "=", "torch", ".", "matmul", "(", "input", ",", "self", ".", "embed", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "input_emb", "=", "self", ".", "embed", "(", "input", ")", "# [batch, emb_size]", "\n", "\n", "", "if", "self", ".", "use_docs_attn", ":", "\n", "                ", "attn_wts", "=", "self", ".", "attn_lin1", "(", "attend_to_embs", ")", "# [batch, n_docs, n_layers, attn_size]", "\n", "attn_wts", "=", "self", ".", "attn_lin2", "(", "self", ".", "attn_act1", "(", "attn_wts", ")", ")", "# [batch, n_docs, n_layers, 1]", "\n", "attn_wts", "=", "F", ".", "softmax", "(", "attn_wts", ",", "dim", "=", "1", ")", "# [batch, n_docs, n_layers, 1]", "\n", "context", "=", "attn_wts", "*", "attend_to_embs", "# [batch, n_docs, n_layers, hidden]", "\n", "context", "=", "context", ".", "sum", "(", "dim", "=", "1", ")", "# [batch, n_layers, hidden]", "\n", "hidden", "=", "self", ".", "context_alpha", "*", "context", "+", "(", "1", "-", "self", ".", "context_alpha", ")", "*", "hidden", "\n", "\n", "", "hidden", ",", "cell", ",", "output", "=", "self", ".", "rnn", "(", "input_emb", ",", "hidden", ",", "cell", ")", "\n", "prob", "=", "logits_to_prob", "(", "output", ",", "softmax_method", ",", "\n", "tau", "=", "tau", ",", "eps", "=", "eps", ",", "gumbel_hard", "=", "gumbel_hard", ")", "# [batch, vocab]", "\n", "prob", ",", "id", "=", "prob_to_vocab_id", "(", "prob", ",", "sample_method", ",", "k", "=", "k", ")", "# [batch * k^(t+1)]", "\n", "\n", "# If sequence (row) has *previously* produced an EOS,", "\n", "# replace prob with one hot (probability one for pad) and id with pad", "\n", "prob", "=", "torch", ".", "where", "(", "(", "rows_with_eos", "==", "1", ")", ".", "unsqueeze", "(", "1", ")", ",", "pad_prob", ",", "prob", ")", "# unsqueeze to broadcast", "\n", "id", "=", "torch", ".", "where", "(", "rows_with_eos", "==", "1", ",", "pad_ids", ",", "id", ")", "\n", "# Now update rows_with_eos to include this time step", "\n", "# This has to go after the above! Otherwise EOS is replaced as well", "\n", "rows_with_eos", "=", "rows_with_eos", "|", "(", "id", "==", "eos_id", ")", ".", "long", "(", ")", "\n", "\n", "decoded_probs", "[", ":", ",", "t", ",", ":", "]", "=", "prob", "\n", "decoded_ids", "[", ":", ",", "t", "]", "=", "id", "\n", "\n", "# Get next input", "\n", "if", "targets", "is", "not", "None", ":", "# teacher forcing", "\n", "                ", "input", "=", "targets", "[", ":", ",", "t", "]", "# [batch]", "\n", "", "else", ":", "# non-teacher forcing", "\n", "                ", "if", "gumbel_hard", ":", "\n", "                    ", "input", "=", "prob", "\n", "", "else", ":", "\n", "                    ", "input", "=", "id", "# [batch * k^(t+1)]", "\n", "\n", "# Terminate early if not teacher forcing and all sequences have generated an eos", "\n", "", "", "if", "targets", "is", "None", ":", "\n", "                ", "if", "rows_with_eos", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "(", "batch_size", "*", "k", ")", ":", "\n", "                    ", "break", "\n", "\n", "# if return_last_state:", "\n", "#     extra['last_state'] = states", "\n", "\n", "", "", "", "decoded_texts", "=", "[", "]", "\n", "if", "subwordenc", ":", "\n", "            ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "decoded_texts", ".", "append", "(", "subwordenc", ".", "decode", "(", "decoded_ids", "[", "i", "]", ".", "long", "(", ")", ".", "tolist", "(", ")", ")", ")", "\n", "\n", "", "", "return", "decoded_probs", ",", "decoded_ids", ",", "decoded_texts", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMEncoderDecoder.__init__": [[332, 338], ["torch.Module.__init__", "mlstm.StackedLSTMEncoder", "mlstm.StackedLSTMDecoder"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed", ",", "rnn", ")", ":", "\n", "        ", "super", "(", "StackedLSTMEncoderDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed", "=", "embed", "\n", "self", ".", "rnn", "=", "rnn", "\n", "self", ".", "encoder", "=", "StackedLSTMEncoder", "(", "embed", ",", "rnn", ")", "\n", "self", ".", "decoder", "=", "StackedLSTMDecoder", "(", "embed", ",", "rnn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTMEncoderDecoder.forward": [[339, 368], ["mlstm.StackedLSTMEncoderDecoder.encoder", "mlstm.StackedLSTMEncoderDecoder.decoder", "input.size", "mlstm.StackedLSTMEncoderDecoder.rnn.state0", "models.nn_utils.logits_to_prob", "models.nn_utils.prob_to_vocab_id", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.logits_to_prob", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.prob_to_vocab_id", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda"], ["", "def", "forward", "(", "self", ",", "input", ",", "\n", "enc_init_h", "=", "None", ",", "enc_init_c", "=", "None", ",", "\n", "dec_init_input", "=", "None", ",", "dec_kwargs", "=", "{", "}", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input: [batch_size, seq_len]\n            enc_init_h: [batch_size, n_layers, hidden_size]\n            enc_init_c: [batch_size, n_layers, hidden_size]\n            dec_init_input: [batch_size]\n            dec_kwargs: dict\n\n        Returns:\n            Output of StackedLSTMDecoder's forward()\n        \"\"\"", "\n", "if", "(", "enc_init_h", "is", "None", ")", "and", "(", "enc_init_c", "is", "None", ")", ":", "\n", "            ", "batch_size", "=", "input", ".", "size", "(", "0", ")", "\n", "enc_init_h", ",", "enc_init_c", "=", "self", ".", "rnn", ".", "state0", "(", "batch_size", ")", "\n", "enc_init_h", ",", "enc_init_c", "=", "move_to_cuda", "(", "enc_init_h", ")", ",", "move_to_cuda", "(", "enc_init_c", ")", "\n", "\n", "", "hiddens", ",", "cells", ",", "outputs", "=", "self", ".", "encoder", "(", "input", ",", "enc_init_h", ",", "enc_init_c", ")", "\n", "\n", "# Get states and input for decoder", "\n", "last_hidden", ",", "last_cell", ",", "last_logits", "=", "hiddens", "[", "-", "1", "]", ",", "cells", "[", "-", "1", "]", ",", "outputs", "[", "-", "1", "]", "\n", "if", "dec_init_input", "is", "None", ":", "\n", "            ", "last_probs", "=", "logits_to_prob", "(", "last_logits", ",", "method", "=", "'softmax'", ")", "# [batch, vocab]", "\n", "_", ",", "dec_init_input", "=", "prob_to_vocab_id", "(", "last_probs", ",", "'greedy'", ")", "# [batch]", "\n", "\n", "", "probs", ",", "ids", ",", "texts", ",", "extra", "=", "self", ".", "decoder", "(", "last_hidden", ",", "last_cell", ",", "dec_init_input", ",", "**", "dec_kwargs", ")", "\n", "return", "probs", ",", "ids", ",", "texts", ",", "extra", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.LabelSmoothing.__init__": [[210, 220], ["torch.Module.__init__", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "padding_idx", "=", "PAD_ID", ",", "smoothing", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "LabelSmoothing", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# False so that we can normalize by the number of non-pad tokens after", "\n", "# (nn.CrossEntropyLoss() has a ignore_index=0 for this, but not KLDivLoss)", "\n", "self", ".", "criterion", "=", "nn", ".", "KLDivLoss", "(", "size_average", "=", "False", ")", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "confidence", "=", "1.0", "-", "smoothing", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "size", "=", "size", "# number of labels, e.g. vocab size", "\n", "self", ".", "true_dist", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.LabelSmoothing.forward": [[221, 244], ["torch.log_softmax.contiguous().view", "target.contiguous().view.contiguous().view.contiguous().view", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.detach().clone", "F.log_softmax.detach().clone.fill_", "F.log_softmax.detach().clone.scatter_", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "nn_utils.LabelSmoothing.criterion", "torch.log_softmax.size", "torch.log_softmax.size", "target.contiguous().view.contiguous().view.unsqueeze", "torch.nonzero.dim", "torch.nonzero.dim", "torch.nonzero.dim", "F.log_softmax.detach().clone.index_fill_", "torch.log_softmax.contiguous", "target.contiguous().view.contiguous().view.contiguous", "torch.log_softmax.detach", "torch.nonzero.squeeze", "torch.nonzero.squeeze", "torch.nonzero.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: [batch_size, seq_len, self.size (num_labels)]\n            target: [batch_size. seq_len]\n        \"\"\"", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "target", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "x", "=", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "self", ".", "size", "\n", "# true_dist = x.clone()", "\n", "true_dist", "=", "x", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "true_dist", ".", "fill_", "(", "self", ".", "smoothing", "/", "(", "self", ".", "size", "-", "2", ")", ")", "# smoothing mass distributed through vocab", "\n", "true_dist", ".", "scatter_", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "# correct word has confidence", "\n", "true_dist", "[", ":", ",", "self", ".", "padding_idx", "]", "=", "0", "# replace all indices that have pad with 0? but why's it just one value", "\n", "mask", "=", "torch", ".", "nonzero", "(", "target", "==", "self", ".", "padding_idx", ")", "\n", "# if no values in target == padding_idx, returns tensor([]) which has dim() == 1", "\n", "if", "mask", ".", "dim", "(", ")", ">", "1", ":", "\n", "# if mask.dim() > 0:", "\n", "            ", "true_dist", ".", "index_fill_", "(", "0", ",", "mask", ".", "squeeze", "(", ")", ",", "0.0", ")", "\n", "", "self", ".", "true_dist", "=", "true_dist", "\n", "loss", "=", "self", ".", "criterion", "(", "x", ",", "true_dist", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.NoamOpt.__init__": [[378, 385], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model_size", ",", "factor", "=", "2", ",", "warmup", "=", "4000", ",", "optimizer", "=", "None", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "_step", "=", "0", "\n", "self", ".", "warmup", "=", "warmup", "\n", "self", ".", "factor", "=", "factor", "\n", "self", ".", "model_size", "=", "model_size", "\n", "self", ".", "_rate", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.NoamOpt.step": [[386, 394], ["nn_utils.NoamOpt.rate", "nn_utils.NoamOpt.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.NoamOpt.rate", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update parameters and rate\"\"\"", "\n", "self", ".", "_step", "+=", "1", "\n", "rate", "=", "self", ".", "rate", "(", ")", "\n", "for", "p", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "p", "[", "'lr'", "]", "=", "rate", "\n", "", "self", ".", "_rate", "=", "rate", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.NoamOpt.rate": [[395, 402], ["min"], "methods", ["None"], ["", "def", "rate", "(", "self", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implement `lrate` above\"\"\"", "\n", "if", "step", "is", "None", ":", "\n", "            ", "step", "=", "self", ".", "_step", "\n", "", "return", "self", ".", "factor", "*", "(", "self", ".", "model_size", "**", "(", "-", "0.5", ")", "*", "\n", "min", "(", "step", "**", "(", "-", "0.5", ")", ",", "step", "*", "self", ".", "warmup", "**", "(", "-", "1.5", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.OptWrapper.__init__": [[409, 428], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "clip", ",", "optimizer", ",", "\n", "epoch_nsteps", "=", "None", ",", "epoch_decay", "=", "None", ",", "decay_method", "=", "'times'", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model: nn.Module (mLSTM)\n            clip: float (value at which to clip)\n            optimizer: optim instasnce\n            epoch_nsteps: number of steps in an epoch\n                - can be used to update learning rate after epoch is done\n            epoch_decay: float (amount to decay by)\n            decay_method: str ('times', 'minus')\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "clip", "=", "clip", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "epoch_nsteps", "=", "epoch_nsteps", "\n", "self", ".", "epoch_decay", "=", "epoch_decay", "\n", "self", ".", "decay_method", "=", "decay_method", "\n", "self", ".", "_nstep", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.OptWrapper.step": [[429, 443], ["nn_utils.clip_gradient", "nn_utils.OptWrapper.optimizer.step", "print"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.clip_gradient", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "self", ".", "_nstep", "+=", "1", "\n", "clip_gradient", "(", "self", ".", "model", ",", "self", ".", "clip", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "# Note: torch.optim now actually contains learning rate schedulers", "\n", "# However, they do not do gradient clipping", "\n", "# update learning rate at end of epoch potentially", "\n", "if", "(", "self", ".", "epoch_nsteps", ")", "and", "(", "self", ".", "_nstep", "==", "self", ".", "epoch_nsteps", ")", ":", "\n", "            ", "print", "(", "'Decaying learning rate by {} and {}'", ".", "format", "(", "self", ".", "epoch_decay", ",", "self", ".", "decay_method", ")", ")", "\n", "for", "p", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "if", "self", ".", "decay_method", "==", "'times'", ":", "\n", "                    ", "p", "[", "'lr'", "]", "*=", "self", ".", "epoch_decay", "\n", "", "elif", "self", ".", "decay_method", "==", "'minus'", ":", "\n", "                    ", "p", "[", "'lr'", "]", "-=", "self", ".", "epoch_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.__init__": [[451, 478], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "init_val", ",", "\n", "interval_size", "=", "None", ",", "intervals", "=", "None", ",", "intervals_vals", "=", "None", ",", "\n", "alpha", "=", "None", ",", "method", "=", "None", ",", "\n", "min_val", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            init_val: float (starting value)\n            One of these two should be set:\n            interval_size: int (update the value every interval_size steps)\n            intervals: list of ints (update the value at these steps)\n            intervals_vals: list of floats (if intervals is given, decay to these values)\n            alpha: float (amount to decay by)\n            method: str\n                'times': multiply the value by alpha when decaying\n                'minus': subtract alpha from the current value when decaying\n            min_val: float (lowest the value can become)\n        \"\"\"", "\n", "self", ".", "init_val", "=", "init_val", "\n", "self", ".", "interval_size", "=", "interval_size", "\n", "self", ".", "intervals", "=", "intervals", "\n", "self", ".", "intervals_vals", "=", "intervals_vals", "\n", "self", ".", "cur_interval", "=", "0", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "min_val", "=", "min_val", "\n", "self", ".", "cur_step", "=", "0", "\n", "self", ".", "val", "=", "init_val", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.step": [[479, 489], ["nn_utils.StepAnnealer.update_val", "len", "nn_utils.StepAnnealer.update_val"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.update_val", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.update_val"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "self", ".", "cur_step", "+=", "1", "\n", "if", "self", ".", "interval_size", ":", "\n", "            ", "if", "self", ".", "cur_step", "%", "self", ".", "interval_size", "==", "0", ":", "\n", "                ", "self", ".", "update_val", "(", ")", "\n", "", "", "elif", "self", ".", "intervals", ":", "\n", "            ", "if", "self", ".", "cur_interval", "<=", "len", "(", "self", ".", "intervals", ")", "-", "1", ":", "\n", "                ", "if", "self", ".", "cur_step", "==", "self", ".", "intervals", "[", "self", ".", "cur_interval", "]", ":", "\n", "                    ", "self", ".", "update_val", "(", ")", "\n", "", "", "self", ".", "cur_interval", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.StepAnnealer.update_val": [[490, 499], ["max"], "methods", ["None"], ["", "", "def", "update_val", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "intervals_vals", ":", "\n", "            ", "self", ".", "cur_val", "=", "self", ".", "intervals_vals", "[", "self", ".", "cur_interval", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "method", "==", "'times'", ":", "\n", "                ", "new_val", "=", "self", ".", "val", "*", "self", ".", "alpha", "\n", "", "elif", "self", ".", "method", "==", "'minus'", ":", "\n", "                ", "new_val", "=", "self", ".", "val", "-", "self", ".", "alpha", "\n", "", "self", ".", "val", "=", "max", "(", "new_val", ",", "self", ".", "min_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.Batch.__init__": [[557, 585], ["nn_utils.Batch.make_std_mask"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.Batch.make_std_mask"], ["def", "__init__", "(", "self", ",", "src", ",", "trg", "=", "None", ",", "pad", "=", "PAD_ID", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src: [batch_size, src_seq_len]\n            trg: [batch_size, trg_seq_len]\n            pad: int (id of pad token)\n        \"\"\"", "\n", "self", ".", "src", "=", "src", "\n", "self", ".", "src_mask", "=", "(", "src", "!=", "pad", ")", ".", "unsqueeze", "(", "-", "2", ")", "# [batch, 1, src_seq_len]", "\n", "if", "trg", "is", "not", "None", ":", "\n", "# \"This masking, ***combined with the fact that the output embeddings are offset by one position***,", "\n", "# ensures that the predictions for position i can depend only only on the known outputs at", "\n", "# positions less than i\"", "\n", "#", "\n", "# I.e. The targets are inputs into the decoder. If we didn't use the subsequent mask,", "\n", "# we would be able to use future tokens to predict the i-th token.", "\n", "# If we didn't offset by one position, then the i-th token would be used to predict the", "\n", "# i-th token.", "\n", "#", "\n", "# In order to offset by one position, we set trg (the inputs to the decoder) up until", "\n", "# but not including the last token (:-1), as the last token would not have a next token", "\n", "# to predict. We also set trg_y (the targets used to calculate the loss with the output", "\n", "# probabilities of the decoder) to 1:, i.e. the next tokens.", "\n", "            ", "self", ".", "trg", "=", "trg", "[", ":", ",", ":", "-", "1", "]", "# [batch, trg_seq_len - 1]", "\n", "self", ".", "trg_y", "=", "trg", "[", ":", ",", "1", ":", "]", "# [batch, trg_seq_len - 1]", "\n", "self", ".", "trg_mask", "=", "self", ".", "make_std_mask", "(", "self", ".", "trg", ",", "pad", ")", "\n", "self", ".", "ntokens", "=", "(", "self", ".", "trg_y", "!=", "pad", ")", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.Batch.make_std_mask": [[586, 599], ["subsequent_mask().type_as", "nn_utils.subsequent_mask", "tgt.size"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.subsequent_mask"], ["", "", "@", "staticmethod", "\n", "def", "make_std_mask", "(", "tgt", ",", "pad", ")", ":", "\n", "        ", "\"\"\"\n        Create a mask to hide padding and future words.\n        Args:\n            tgt: [batch_size, seq_len] LongTensor\n            pad: int (id of pad token)\n        \"\"\"", "\n", "# mask padding", "\n", "tgt_mask", "=", "(", "tgt", "!=", "pad", ")", ".", "unsqueeze", "(", "-", "2", ")", "# [batch_size, 1, seq_len]", "\n", "# mask future words", "\n", "tgt_mask", "=", "tgt_mask", "&", "subsequent_mask", "(", "tgt", ".", "size", "(", "-", "1", ")", ")", ".", "type_as", "(", "tgt_mask", ")", "# [batch_size, seq_len, seq_len]", "\n", "return", "tgt_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda": [[23, 31], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "type", "tuple", "x.cuda.cuda", "t.cuda"], "function", ["None"], ["def", "move_to_cuda", "(", "x", ")", ":", "\n", "    ", "\"\"\"Move tensor to cuda\"\"\"", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "if", "type", "(", "x", ")", "==", "tuple", ":", "\n", "            ", "x", "=", "tuple", "(", "[", "t", ".", "cuda", "(", ")", "for", "t", "in", "x", "]", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "cuda", "(", ")", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.setup_gpus": [[33, 38], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed"], "function", ["None"], ["", "def", "setup_gpus", "(", "gpu_ids", ",", "seed", ")", ":", "\n", "    ", "os", ".", "environ", "[", "'CUDA_VISIBLE_DEVICES'", "]", "=", "gpu_ids", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_model": [[45, 66], ["save_fn.format.format", "os.path.join", "print", "torch.save", "torch.save", "torch.save", "type"], "function", ["None"], ["", "", "def", "save_model", "(", "save_dir", ",", "model", ",", "optimizer", ",", "epoch", ",", "opt", ",", "extra_fn", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        save_dir: str (path to directory)\n        model: nn.Module\n        optimizer: wrapped Optimizer instance\n        epoch: int\n        opt: argparse (options)\n        extra_fn: append to filename, e.g. <loss_avg>\n    \"\"\"", "\n", "checkpoint", "=", "{", "\n", "'model'", ":", "model", ",", "\n", "'optimizer'", ":", "optimizer", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'opt'", ":", "opt", "\n", "}", "\n", "save_fn", "=", "'{}_e{}_{:.2f}.pt'", "if", "type", "(", "extra_fn", ")", "==", "float", "else", "'{}_e{}_{}.pt'", "\n", "save_fn", "=", "save_fn", ".", "format", "(", "opt", ".", "save_model_fn", ",", "epoch", ",", "extra_fn", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "save_fn", ")", "\n", "print", "(", "'Saving to: {}'", ".", "format", "(", "save_path", ")", ")", "\n", "torch", ".", "save", "(", "checkpoint", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.save_models": [[68, 97], ["models.items", "optimizers.items", "save_fn.format.format", "os.path.join", "print", "torch.save", "torch.save", "torch.save", "type"], "function", ["None"], ["", "def", "save_models", "(", "save_dir", ",", "models", ",", "optimizers", ",", "epoch", ",", "opt", ",", "extra_fn", ")", ":", "\n", "    ", "\"\"\"\n    Save multiple models and optimizers. Currently used by summarization model, which consists of\n    a language model, a discriminator, and a classifier.\n    Args:\n        save_dir: str (path to directory)\n        models: dict\n            key: str (name)\n            value: some object\n        optimizers: dict\n            key: str (name)\n            value: wrapped Optimizer instance\n        epoch: int\n        opt: argparse (options)\n        extra_fn: append to filename, e.g. <loss_avg>\n    \"\"\"", "\n", "checkpoint", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'opt'", ":", "opt", "\n", "}", "\n", "for", "name", ",", "model", "in", "models", ".", "items", "(", ")", ":", "\n", "        ", "checkpoint", "[", "name", "]", "=", "model", "\n", "", "for", "name", ",", "optimizer", "in", "optimizers", ".", "items", "(", ")", ":", "\n", "        ", "checkpoint", "[", "name", "]", "=", "optimizer", "\n", "", "save_fn", "=", "'{}_e{}_{:.2f}.pt'", "if", "type", "(", "extra_fn", ")", "==", "float", "else", "'{}_e{}_{}.pt'", "\n", "save_fn", "=", "save_fn", ".", "format", "(", "opt", ".", "save_model_fn", ",", "epoch", ",", "extra_fn", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "save_fn", ")", "\n", "print", "(", "'Saving to: {}'", ".", "format", "(", "save_path", ")", ")", "\n", "torch", ".", "save", "(", "checkpoint", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.update_hp_from_loaded_model": [[99, 112], ["utils.load_file", "utils.load_file.items", "os.path.join", "os.path.dirname", "setattr"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.load_file"], ["", "def", "update_hp_from_loaded_model", "(", "load_path", ",", "hp", "=", "None", ",", "exclude", "=", "None", ",", "include_match", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        load_path: str\n    Returns:\n    \"\"\"", "\n", "old_hp", "=", "load_file", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "load_path", ")", ",", "'hp.json'", ")", ")", "\n", "for", "name", ",", "value", "in", "old_hp", ".", "items", "(", ")", ":", "\n", "# hp[name] = value", "\n", "# TODO: fix this", "\n", "        ", "if", "(", "name", "not", "in", "exclude", ")", "and", "(", "name", "in", "include", ")", ":", "\n", "            ", "setattr", "(", "hp", ",", "name", ",", "value", ")", "\n", "", "", "return", "hp", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.logits_to_prob": [[119, 137], ["ValueError", "torch.gumbel_softmax", "torch.softmax"], "function", ["None"], ["", "def", "logits_to_prob", "(", "logits", ",", "method", ",", "\n", "tau", "=", "1.0", ",", "eps", "=", "1e-10", ",", "gumbel_hard", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        logits: [batch_size, vocab_size]\n        method: 'gumbel', 'softmax'\n        gumbel_hard: boolean\n        topk: int (used for beam search)\n    Returns: [batch_size, vocab_size]\n    \"\"\"", "\n", "if", "tau", "==", "0.0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'Temperature should not be 0. If you want greedy decoding, pass \"greedy\" to prob_to_vocab_id()'", ")", "\n", "", "if", "method", "==", "'gumbel'", ":", "\n", "        ", "prob", "=", "F", ".", "gumbel_softmax", "(", "logits", ",", "tau", "=", "tau", ",", "eps", "=", "eps", ",", "hard", "=", "gumbel_hard", ")", "\n", "", "elif", "method", "==", "'softmax'", ":", "\n", "        ", "prob", "=", "F", ".", "softmax", "(", "logits", "/", "tau", ",", "dim", "=", "1", ")", "\n", "", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.prob_to_vocab_id": [[139, 166], ["prob.repeat().view.size", "prob.repeat().view.repeat().view", "torch.multinomial.view", "torch.topk", "torch.topk", "torch.topk", "torch.multinomial", "torch.multinomial", "torch.multinomial", "prob.repeat().view.repeat"], "function", ["None"], ["", "def", "prob_to_vocab_id", "(", "prob", ",", "method", ",", "k", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Produce vocab id given probability distribution over vocab\n    Args:\n        prob: [batch_size, vocab_size]\n        method: str ('greedy', 'sample')\n        k: int (used for beam search)\n    Returns:\n        prob: [batch_size * k, vocab_size]\n            Rows are repeated:\n                [[0.3, 0.2, 0.5],\n                 [0.1, 0.7, 0.2]]\n            Becomes (with k=2):\n                [[0.3, 0.2, 0.5],\n                 [0.3, 0.2, 0.5],\n                 [0.1, 0.7, 0.2]\n                 [0.1, 0.7, 0.2]]\n        ids: [batch_size * k] LongTensor\n    \"\"\"", "\n", "if", "method", "==", "'greedy'", ":", "\n", "        ", "_", ",", "ids", "=", "torch", ".", "topk", "(", "prob", ",", "k", ",", "dim", "=", "1", ")", "\n", "", "elif", "method", "==", "'sample'", ":", "\n", "        ", "ids", "=", "torch", ".", "multinomial", "(", "prob", ",", "k", ")", "\n", "", "batch_size", "=", "prob", ".", "size", "(", "0", ")", "\n", "prob", "=", "prob", ".", "repeat", "(", "1", ",", "k", ")", ".", "view", "(", "batch_size", "*", "k", ",", "-", "1", ")", "\n", "ids", "=", "ids", ".", "view", "(", "-", "1", ")", "\n", "return", "prob", ",", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_grad_norm": [[173, 181], ["model.parameters", "math.sqrt", "p.grad.norm"], "function", ["None"], ["", "def", "calc_grad_norm", "(", "model", ")", ":", "\n", "    ", "\"\"\"Computes a gradient clipping coefficient based on gradient norm.\"\"\"", "\n", "totalnorm", "=", "0", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "p", ".", "requires_grad", "and", "(", "p", ".", "grad", "is", "not", "None", ")", ":", "\n", "            ", "modulenorm", "=", "p", ".", "grad", ".", "norm", "(", ")", "\n", "totalnorm", "+=", "modulenorm", "**", "2", "\n", "", "", "return", "math", ".", "sqrt", "(", "totalnorm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.clip_gradient": [[183, 188], ["model.parameters", "p.grad.clamp"], "function", ["None"], ["", "def", "clip_gradient", "(", "model", ",", "clip", ")", ":", "\n", "    ", "\"\"\"Clip the gradient.\"\"\"", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "p", ".", "requires_grad", "and", "(", "p", ".", "grad", "is", "not", "None", ")", ":", "\n", "            ", "p", ".", "grad", "=", "p", ".", "grad", ".", "clamp", "(", "-", "clip", ",", "clip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.freeze": [[190, 193], ["module.parameters"], "function", ["None"], ["", "", "", "def", "freeze", "(", "module", ")", ":", "\n", "    ", "for", "param", "in", "module", ".", "parameters", "(", ")", ":", "\n", "        ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_clf_acc": [[246, 262], ["logits.size", "logits.dim", "logits.round().long", "torch.max", "torch.max", "torch.max", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "logits.round", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq", "torch.eq", "torch.eq"], "function", ["None"], ["", "", "def", "calc_clf_acc", "(", "logits", ",", "labels", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        logits:\n            [batch_size, n_labels] FloatTensor or\n            [batch_size] FloatTensor (i.e. when classifier is being trained with MSE)\n        labels: [batch_size] LongTensor\n    Returns: float\n    \"\"\"", "\n", "batch_size", "=", "logits", ".", "size", "(", "0", ")", "\n", "if", "logits", ".", "dim", "(", ")", "==", "1", ":", "\n", "        ", "indices", "=", "logits", ".", "round", "(", ")", ".", "long", "(", ")", "\n", "", "else", ":", "\n", "        ", "_", ",", "indices", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "1", ")", "\n", "", "acc", "=", "torch", ".", "eq", "(", "indices", ",", "labels", ")", ".", "sum", "(", ")", ".", "float", "(", ")", "/", "batch_size", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_per_rating_acc": [[263, 285], ["range", "true_ratings.size", "utils.update_moving_avg", "true_ratings[].item", "pred_ratings[].item"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "calc_per_rating_acc", "(", "pred_ratings", ",", "true_ratings", ",", "per_rating_counts", ",", "per_rating_acc", ")", ":", "\n", "    ", "\"\"\"\n    Calculate the accuracy of each star rating\n\n    Args:\n        pred_ratings: 1D Tensor (e.g. batch_size)\n        true_ratings: 1D Tensor (e.g. batch_size)\n        per_rating_counts: dict: rating to int\n        per_rating_acc: dict: rating to float\n\n    Returns:\n        Updated per_rating_counts and per_rating_acc\n    \"\"\"", "\n", "for", "b_idx", "in", "range", "(", "true_ratings", ".", "size", "(", "0", ")", ")", ":", "\n", "        ", "true_rating", ",", "pred_rating", "=", "true_ratings", "[", "b_idx", "]", ".", "item", "(", ")", ",", "pred_ratings", "[", "b_idx", "]", ".", "item", "(", ")", "\n", "per_rating_counts", "[", "true_rating", "]", "+=", "1", "\n", "avg_so_far", "=", "per_rating_acc", "[", "true_rating", "]", "\n", "item_acc", "=", "true_rating", "==", "pred_rating", "\n", "rating_count", "=", "per_rating_counts", "[", "true_rating", "]", "\n", "per_rating_acc", "[", "true_rating", "]", "=", "update_moving_avg", "(", "avg_so_far", ",", "item_acc", ",", "rating_count", ")", "\n", "\n", "", "return", "per_rating_counts", ",", "per_rating_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.classify_summ_batch": [[287, 326], ["dataset.prepare_batch", "collections.defaultdict", "collections.defaultdict", "clf_model", "nn_utils.calc_clf_acc", "torch.max", "torch.max", "torch.max", "nn_utils.calc_per_rating_acc", "torch.softmax"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.summ_dataset.SummReviewDataset.prepare_batch", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_clf_acc", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_per_rating_acc"], ["", "def", "classify_summ_batch", "(", "clf_model", ",", "summary_batch", ",", "ratings_batch", ",", "dataset", ",", "\n", "per_rating_counts", "=", "None", ",", "per_rating_acc", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Used to evaluate models on test set (run_evaluations.py, test() in train_sum.py)\n\n    Args:\n        clf_model: pretrained TextClassifier\n        summary_batch: list of strs (length batch_size)\n        ratings_batch: list of 1D tensors (from data_iter, length batch_size)\n        dataset: YelpDataset instance\n            (Really prepare_batch() should be a static method... or not part of YelpDataset. This should've been\n            refactored at some point.)\n        per_rating_counts: dict: rating to int\n        per_rating_acc: dict: rating to acc\n\n    Returns:\n        acc: 0D float Tensor\n        per_rating_acc: dict: int to acc\n        pred_ratings: [batch_size] Tensor\n        pred_probs: [batch_size] Tensor\n    \"\"\"", "\n", "docs_ids", ",", "_", ",", "labels", "=", "dataset", ".", "prepare_batch", "(", "summary_batch", ",", "ratings_batch", ")", "\n", "\n", "if", "(", "per_rating_counts", "is", "None", ")", "and", "(", "per_rating_acc", ")", "is", "None", ":", "\n", "        ", "per_rating_counts", "=", "defaultdict", "(", "int", ")", "\n", "per_rating_acc", "=", "defaultdict", "(", "int", ")", "\n", "\n", "", "try", ":", "\n", "        ", "logits", "=", "clf_model", "(", "docs_ids", ")", "\n", "acc", "=", "calc_clf_acc", "(", "logits", ",", "labels", ")", "\n", "pred_probs", ",", "pred_ratings", "=", "torch", ".", "max", "(", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", ",", "dim", "=", "1", ")", "\n", "pred_ratings", "=", "pred_ratings", "+", "1", "# [batch]", "\n", "true_ratings", "=", "labels", "+", "1", "\n", "per_rating_counts", ",", "per_rating_acc", "=", "calc_per_rating_acc", "(", "pred_ratings", ",", "true_ratings", ",", "\n", "per_rating_counts", ",", "per_rating_acc", ")", "\n", "", "except", "Exception", "as", "e", ":", "# summary is too short for CNN-based classifier, which has 3,4,5 width kernels", "\n", "        ", "acc", ",", "pred_ratings", ",", "pred_probs", "=", "None", ",", "None", ",", "None", "\n", "\n", "", "return", "acc", ",", "per_rating_counts", ",", "per_rating_acc", ",", "pred_ratings", ",", "pred_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.calc_lm_nll": [[328, 365], ["input.size", "lm.rnn.state0", "lm", "nn_utils.move_to_cuda", "nn_utils.move_to_cuda", "enumerate", "nn_utils.move_to_cuda", "nn_utils.move_to_cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.log", "torch.log", "torch.log", "torch.where", "torch.where", "torch.where", "move_to_cuda.mean", "torch.max", "torch.max", "torch.max", "torch.softmax"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda"], ["", "def", "calc_lm_nll", "(", "lm", ",", "input", ")", ":", "\n", "    ", "\"\"\"\n    Calculate the negative log likelihood of the input according to a language model lm\n\n    Args:\n        lm: StackedLSTMEncoder (trained language model)\n        input:\n            [batch_size, seq_len] (e.g. text prepared using prepare_batch())\n\n    Returns: 0D float tensor\n    \"\"\"", "\n", "batch_size", "=", "input", ".", "size", "(", "0", ")", "\n", "\n", "# Encode with fixed lm", "\n", "h_init", ",", "c_init", "=", "lm", ".", "rnn", ".", "state0", "(", "batch_size", ")", "\n", "h_init", ",", "c_init", "=", "move_to_cuda", "(", "h_init", ")", ",", "move_to_cuda", "(", "c_init", ")", "\n", "_", ",", "_", ",", "outputs", "=", "lm", "(", "input", ",", "h_init", ",", "c_init", ")", "\n", "\n", "# Calculate the number of non-pad tokens so that we can calculate the NLL using", "\n", "# only the tokens up to and including the EDOC token", "\n", "n_nonpads", "=", "(", "input", "!=", "PAD_ID", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "float", "(", ")", "# [batch]", "\n", "zero_logprobs", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", ")", ")", "\n", "logprobs", "=", "move_to_cuda", "(", "torch", ".", "zeros", "(", "batch_size", ")", ")", "\n", "\n", "# So this n_nonpads issue means we go longer than than expected.", "\n", "# But we calculate max_logprobs as F.softmax(output of feeding in zeros into encoder))", "\n", "# ( which may be 0's? I'm not sure). And with vocab size that large... if anything it's worse than it hsould be?", "\n", "\n", "# Keeping outputs as a list (as opposed to concatenating into a tensor) to reduce memory", "\n", "for", "t", ",", "output", "in", "enumerate", "(", "outputs", ")", ":", "# list of [batch, vocab]", "\n", "        ", "max_logprobs", "=", "torch", ".", "log", "(", "torch", ".", "max", "(", "F", ".", "softmax", "(", "output", ",", "dim", "=", "1", ")", ",", "dim", "=", "1", ")", "[", "0", "]", ")", "\n", "max_logprobs", "=", "torch", ".", "where", "(", "t", "<", "n_nonpads", ",", "max_logprobs", ",", "zero_logprobs", ")", "\n", "logprobs", "+=", "max_logprobs", "\n", "", "logprobs", "/=", "n_nonpads", "\n", "mean_nll", "=", "-", "logprobs", ".", "mean", "(", ")", "\n", "\n", "return", "mean_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.convert_to_onehot": [[506, 520], ["x.unsqueeze.unsqueeze", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "torch.LongTensor().zero_", "nn_utils.move_to_cuda", "move_to_cuda.scatter_", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "x.unsqueeze.size", "x.unsqueeze.size"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda"], ["", "", "", "def", "convert_to_onehot", "(", "x", ",", "size", ")", ":", "\n", "    ", "\"\"\"\n    Convert a dense vector of ints into a one hot representation\n    Args:\n        x: [batch_size, seq_len] LongTensor\n        size: int (number of labels, e.g. vocab_size)\n    Returns:\n        [batch_size, seq_len, size] LongTensor\n    \"\"\"", "\n", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "# [batch, seq_len, 1]", "\n", "x_onehot", "=", "torch", ".", "LongTensor", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ",", "size", ")", ".", "zero_", "(", ")", "# [batch, seq_len, size]", "\n", "x_onehot", "=", "move_to_cuda", "(", "x_onehot", ")", "\n", "x_onehot", ".", "scatter_", "(", "2", ",", "x", ",", "1", ")", "# 2nd dimension, indices, fill with 1's", "\n", "return", "x_onehot", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.convert_onehot_to_dense": [[522, 529], ["torch.argmax", "torch.argmax", "torch.argmax"], "function", ["None"], ["", "def", "convert_onehot_to_dense", "(", "onehot", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        onehot: [batch_size, seq_len, vocab_size]\n    Returns: [batch_size, seq_len]\n    \"\"\"", "\n", "return", "torch", ".", "argmax", "(", "onehot", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.subsequent_mask": [[534, 550], ["numpy.triu().astype", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.triu", "numpy.ones"], "function", ["None"], ["", "def", "subsequent_mask", "(", "size", ")", ":", "\n", "    ", "\"\"\"\n    Mask out subsequent positions, e.g. on target\n    Args:\n        size: int (e.g. seq_len)\n    Returns:\n        [1, size, size] ByteTensor\n        Example:\n            [[[1, 0, 0],\n              [1, 1, 0],\n              [1, 1, 1]]]\n    \"\"\"", "\n", "attn_shape", "=", "(", "1", ",", "size", ",", "size", ")", "\n", "subsequent_mask", "=", "np", ".", "triu", "(", "np", ".", "ones", "(", "attn_shape", ")", ",", "k", "=", "1", ")", ".", "astype", "(", "'uint8'", ")", "# upper triangle are 1's", "\n", "subsequent_mask", "=", "torch", ".", "from_numpy", "(", "subsequent_mask", ")", "==", "0", "\n", "return", "subsequent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.AllReduce.forward": [[40, 51], ["sorted", "torch.reduce_add_coalesced", "torch.reduce_add_coalesced", "torch.broadcast_coalesced", "torch.broadcast_coalesced", "tuple", "inputs[].get_device", "range", "range", "len", "len", "i[].get_device"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "num_inputs", ",", "*", "inputs", ")", ":", "\n", "        ", "ctx", ".", "num_inputs", "=", "num_inputs", "\n", "ctx", ".", "target_gpus", "=", "[", "inputs", "[", "i", "]", ".", "get_device", "(", ")", "for", "i", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "num_inputs", ")", "]", "\n", "inputs", "=", "[", "inputs", "[", "i", ":", "i", "+", "num_inputs", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "num_inputs", ")", "]", "\n", "# sort before reduce sum", "\n", "inputs", "=", "sorted", "(", "inputs", ",", "key", "=", "lambda", "i", ":", "i", "[", "0", "]", ".", "get_device", "(", ")", ")", "\n", "results", "=", "comm", ".", "reduce_add_coalesced", "(", "inputs", ",", "ctx", ".", "target_gpus", "[", "0", "]", ")", "\n", "outputs", "=", "comm", ".", "broadcast_coalesced", "(", "results", ",", "ctx", ".", "target_gpus", ")", "\n", "return", "tuple", "(", "[", "t", "for", "tensors", "in", "outputs", "for", "t", "in", "tensors", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.AllReduce.backward": [[52, 60], ["torch.reduce_add_coalesced", "torch.reduce_add_coalesced", "torch.broadcast_coalesced", "torch.broadcast_coalesced", "tuple", "range", "len", "torch.autograd.Variable", "torch.autograd.Variable"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "*", "inputs", ")", ":", "\n", "        ", "inputs", "=", "[", "i", ".", "data", "for", "i", "in", "inputs", "]", "\n", "inputs", "=", "[", "inputs", "[", "i", ":", "i", "+", "ctx", ".", "num_inputs", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "ctx", ".", "num_inputs", ")", "]", "\n", "results", "=", "comm", ".", "reduce_add_coalesced", "(", "inputs", ",", "ctx", ".", "target_gpus", "[", "0", "]", ")", "\n", "outputs", "=", "comm", ".", "broadcast_coalesced", "(", "results", ",", "ctx", ".", "target_gpus", ")", "\n", "return", "(", "None", ",", ")", "+", "tuple", "(", "[", "Variable", "(", "t", ")", "for", "tensors", "in", "outputs", "for", "t", "in", "tensors", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.forward": [[63, 68], ["sorted", "torch.reduce_add", "torch.reduce_add", "inputs[].get_device", "range", "len", "i.get_device"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "*", "inputs", ")", ":", "\n", "        ", "ctx", ".", "target_gpus", "=", "[", "inputs", "[", "i", "]", ".", "get_device", "(", ")", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", "]", "\n", "inputs", "=", "sorted", "(", "inputs", ",", "key", "=", "lambda", "i", ":", "i", ".", "get_device", "(", ")", ")", "\n", "return", "comm", ".", "reduce_add", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.Reduce.backward": [[69, 72], ["torch.nn.parallel._functions.Broadcast.apply", "torch.nn.parallel._functions.Broadcast.apply"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "gradOutput", ")", ":", "\n", "        ", "return", "Broadcast", ".", "apply", "(", "ctx", ".", "target_gpus", ",", "gradOutput", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.DataParallelModel.gather": [[103, 105], ["None"], "methods", ["None"], ["def", "gather", "(", "self", ",", "outputs", ",", "output_device", ")", ":", "\n", "        ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.DataParallelModel.replicate": [[106, 110], ["super().replicate", "custom_parallel.execute_replication_callbacks"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.DataParallelModel.replicate", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.execute_replication_callbacks"], ["", "def", "replicate", "(", "self", ",", "module", ",", "device_ids", ")", ":", "\n", "        ", "modules", "=", "super", "(", "DataParallelModel", ",", "self", ")", ".", "replicate", "(", "module", ",", "device_ids", ")", "\n", "execute_replication_callbacks", "(", "modules", ")", "\n", "return", "modules", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.DataParallelCriterion.forward": [[133, 144], ["custom_parallel.DataParallelCriterion.scatter", "custom_parallel.DataParallelCriterion.replicate", "custom_parallel._criterion_parallel_apply", "custom_parallel.DataParallelCriterion.module", "len", "custom_parallel.DataParallelCriterion.module", "Reduce.apply", "len", "len"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.DataParallelModel.replicate", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel._criterion_parallel_apply"], ["def", "forward", "(", "self", ",", "inputs", ",", "*", "targets", ",", "**", "kwargs", ")", ":", "\n", "# input should be already scatterd", "\n", "# scattering the targets instead", "\n", "        ", "if", "not", "self", ".", "device_ids", ":", "\n", "            ", "return", "self", ".", "module", "(", "inputs", ",", "*", "targets", ",", "**", "kwargs", ")", "\n", "", "targets", ",", "kwargs", "=", "self", ".", "scatter", "(", "targets", ",", "kwargs", ",", "self", ".", "device_ids", ")", "\n", "if", "len", "(", "self", ".", "device_ids", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "module", "(", "inputs", ",", "*", "targets", "[", "0", "]", ",", "**", "kwargs", "[", "0", "]", ")", "\n", "", "replicas", "=", "self", ".", "replicate", "(", "self", ".", "module", ",", "self", ".", "device_ids", "[", ":", "len", "(", "inputs", ")", "]", ")", "\n", "outputs", "=", "_criterion_parallel_apply", "(", "replicas", ",", "inputs", ",", "targets", ",", "kwargs", ")", "\n", "return", "Reduce", ".", "apply", "(", "*", "outputs", ")", "/", "len", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.allreduce": [[32, 37], ["AllReduce.apply"], "function", ["None"], ["def", "allreduce", "(", "*", "inputs", ")", ":", "\n", "    ", "\"\"\"Cross GPU all reduce autograd operation for calculate mean and\n    variance in SyncBN.\n    \"\"\"", "\n", "return", "AllReduce", ".", "apply", "(", "*", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel._criterion_parallel_apply": [[148, 200], ["threading.Lock", "range", "len", "len", "len", "len", "torch.is_grad_enabled", "torch.is_grad_enabled", "len", "custom_parallel._criterion_parallel_apply._worker"], "function", ["None"], ["", "", "def", "_criterion_parallel_apply", "(", "modules", ",", "inputs", ",", "targets", ",", "kwargs_tup", "=", "None", ",", "devices", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "modules", ")", "==", "len", "(", "inputs", ")", "\n", "assert", "len", "(", "targets", ")", "==", "len", "(", "inputs", ")", "\n", "if", "kwargs_tup", ":", "\n", "        ", "assert", "len", "(", "modules", ")", "==", "len", "(", "kwargs_tup", ")", "\n", "", "else", ":", "\n", "        ", "kwargs_tup", "=", "(", "{", "}", ",", ")", "*", "len", "(", "modules", ")", "\n", "", "if", "devices", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "modules", ")", "==", "len", "(", "devices", ")", "\n", "", "else", ":", "\n", "        ", "devices", "=", "[", "None", "]", "*", "len", "(", "modules", ")", "\n", "\n", "", "lock", "=", "threading", ".", "Lock", "(", ")", "\n", "results", "=", "{", "}", "\n", "if", "torch_ver", "!=", "\"0.3\"", ":", "\n", "        ", "grad_enabled", "=", "torch", ".", "is_grad_enabled", "(", ")", "\n", "\n", "", "def", "_worker", "(", "i", ",", "module", ",", "input", ",", "target", ",", "kwargs", ",", "device", "=", "None", ")", ":", "\n", "        ", "if", "torch_ver", "!=", "\"0.3\"", ":", "\n", "            ", "torch", ".", "set_grad_enabled", "(", "grad_enabled", ")", "\n", "", "if", "device", "is", "None", ":", "\n", "            ", "device", "=", "get_a_var", "(", "input", ")", ".", "get_device", "(", ")", "\n", "", "try", ":", "\n", "            ", "with", "torch", ".", "cuda", ".", "device", "(", "device", ")", ":", "\n", "                ", "output", "=", "module", "(", "*", "(", "(", "input", ",", ")", "+", "target", ")", ",", "**", "kwargs", ")", "\n", "", "with", "lock", ":", "\n", "                ", "results", "[", "i", "]", "=", "output", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "with", "lock", ":", "\n", "                ", "results", "[", "i", "]", "=", "e", "\n", "\n", "", "", "", "if", "len", "(", "modules", ")", ">", "1", ":", "\n", "        ", "threads", "=", "[", "threading", ".", "Thread", "(", "target", "=", "_worker", ",", "\n", "args", "=", "(", "i", ",", "module", ",", "input", ",", "target", ",", "\n", "kwargs", ",", "device", ")", ",", ")", "\n", "for", "i", ",", "(", "module", ",", "input", ",", "target", ",", "kwargs", ",", "device", ")", "in", "\n", "enumerate", "(", "zip", "(", "modules", ",", "inputs", ",", "targets", ",", "kwargs_tup", ",", "devices", ")", ")", "]", "\n", "\n", "for", "thread", "in", "threads", ":", "\n", "            ", "thread", ".", "start", "(", ")", "\n", "", "for", "thread", "in", "threads", ":", "\n", "            ", "thread", ".", "join", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "_worker", "(", "0", ",", "modules", "[", "0", "]", ",", "inputs", "[", "0", "]", ",", "kwargs_tup", "[", "0", "]", ",", "devices", "[", "0", "]", ")", "\n", "\n", "", "outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "inputs", ")", ")", ":", "\n", "        ", "output", "=", "results", "[", "i", "]", "\n", "if", "isinstance", "(", "output", ",", "Exception", ")", ":", "\n", "            ", "raise", "output", "\n", "", "outputs", ".", "append", "(", "output", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.execute_replication_callbacks": [[210, 232], ["len", "enumerate", "list", "custom_parallel.CallbackContext", "enumerate", "master_copy.modules", "range", "module.modules", "hasattr", "m.__data_parallel_replicate__"], "function", ["None"], ["", "def", "execute_replication_callbacks", "(", "modules", ")", ":", "\n", "    ", "\"\"\"\n    Execute an replication callback `__data_parallel_replicate__` on each module created\n    by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead\n    of calling the callback of any slave copies.\n    \"\"\"", "\n", "master_copy", "=", "modules", "[", "0", "]", "\n", "nr_modules", "=", "len", "(", "list", "(", "master_copy", ".", "modules", "(", ")", ")", ")", "\n", "ctxs", "=", "[", "CallbackContext", "(", ")", "for", "_", "in", "range", "(", "nr_modules", ")", "]", "\n", "\n", "for", "i", ",", "module", "in", "enumerate", "(", "modules", ")", ":", "\n", "        ", "for", "j", ",", "m", "in", "enumerate", "(", "module", ".", "modules", "(", ")", ")", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'__data_parallel_replicate__'", ")", ":", "\n", "                ", "m", ".", "__data_parallel_replicate__", "(", "ctxs", "[", "j", "]", ",", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.patch_replication_callback": [[234, 259], ["isinstance", "functools.wraps", "old_replicate", "custom_parallel.execute_replication_callbacks"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.custom_parallel.execute_replication_callbacks"], ["", "", "", "", "def", "patch_replication_callback", "(", "data_parallel", ")", ":", "\n", "    ", "\"\"\"\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    \"\"\"", "\n", "\n", "assert", "isinstance", "(", "data_parallel", ",", "DataParallel", ")", "\n", "\n", "old_replicate", "=", "data_parallel", ".", "replicate", "\n", "\n", "@", "functools", ".", "wraps", "(", "old_replicate", ")", "\n", "def", "new_replicate", "(", "module", ",", "device_ids", ")", ":", "\n", "        ", "modules", "=", "old_replicate", "(", "module", ",", "device_ids", ")", "\n", "execute_replication_callbacks", "(", "modules", ")", "\n", "return", "modules", "\n", "\n", "", "data_parallel", ".", "replicate", "=", "new_replicate", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.summarization.SummarizationModel.__init__": [[19, 50], ["torch.Module.__init__", "torch.CosineSimilarity", "torch.CosineSimilarity", "torch.CosineSimilarity", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "models.nn_utils.LabelSmoothing", "torch.NLLLoss", "torch.NLLLoss", "torch.NLLLoss"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "docs_enc", ",", "docs_autodec", ",", "\n", "combine_encs_h_net", ",", "combine_encs_c_net", ",", "summ_dec", ",", "\n", "summ_enc", ",", "docs_dec", ",", "\n", "discrim_model", ",", "clf_model", ",", "\n", "fixed_lm", ",", "\n", "hp", ",", "dataset", ")", ":", "\n", "        ", "super", "(", "SummarizationModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "docs_enc", "=", "docs_enc", "\n", "self", ".", "docs_autodec", "=", "docs_autodec", "\n", "self", ".", "combine_encs_h_net", "=", "combine_encs_h_net", "\n", "self", ".", "combine_encs_c_net", "=", "combine_encs_c_net", "\n", "self", ".", "summ_dec", "=", "summ_dec", "\n", "self", ".", "summ_enc", "=", "summ_enc", "\n", "self", ".", "docs_dec", "=", "docs_dec", "\n", "self", ".", "discrim_model", "=", "discrim_model", "\n", "self", ".", "clf_model", "=", "clf_model", "\n", "self", ".", "fixed_lm", "=", "fixed_lm", "\n", "\n", "self", ".", "hp", "=", "hp", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n", "if", "self", ".", "hp", ".", "sum_label_smooth", ":", "\n", "            ", "self", ".", "rec_crit", "=", "LabelSmoothing", "(", "size", "=", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ",", "\n", "smoothing", "=", "self", ".", "hp", ".", "sum_label_smooth_val", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rec_crit", "=", "nn", ".", "NLLLoss", "(", "ignore_index", "=", "PAD_ID", ")", "\n", "", "self", ".", "cos_crit", "=", "nn", ".", "CosineSimilarity", "(", "dim", "=", "1", ")", "\n", "self", ".", "clf_crit", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "self", ".", "stats", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.summarization.SummarizationModel.forward": [[51, 340], ["docs_ids.view.view.size", "summarization.SummarizationModel.docs_enc.rnn.state0", "summarization.SummarizationModel.docs_enc", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "models.nn_utils.move_to_cuda", "summarization.SummarizationModel.summ_dec", "docs_ids.view.view.size", "docs_ids.view.view.view", "docs_ids.view.view.size", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "models.nn_utils.move_to_cuda", "summarization.SummarizationModel.docs_autodec", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "summarization.SummarizationModel.rec_crit", "docs_enc_h.view.view.size", "docs_enc_h.view.view.view", "docs_enc_c.view.view.view", "hasattr", "docs_enc_h_comb[].unsqueeze().transpose.repeat().view", "docs_enc_c_comb[].unsqueeze().transpose.repeat().view", "summarization.SummarizationModel.summ_enc.rnn.state0", "summarization.SummarizationModel.summ_enc", "summarization.SummarizationModel.summ_enc.rnn.state0", "summarization.SummarizationModel.summ_enc", "summ_enc_h.repeat().view", "summ_enc_c.repeat().view", "summarization.SummarizationModel.stats.update", "summarization.SummarizationModel.stats.update", "docs_ids.view.view.size", "torch.log.view", "torch.log.view", "torch.log.view", "docs_ids.view.view.view", "docs_enc_h.view.view.mean", "docs_enc_c.view.view.mean", "docs_enc_h_comb[].unsqueeze().transpose.size", "docs_enc_h_comb[].unsqueeze().transpose.size", "docs_enc_c_comb[].unsqueeze().transpose.size", "docs_enc_c_comb[].unsqueeze().transpose.size", "summarization.SummarizationModel.cos_crit().mean", "summarization.SummarizationModel.cos_crit().mean", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "summarization.SummarizationModel.cos_crit().mean", "summarization.SummarizationModel.cos_crit().mean", "summ_enc_h.size", "summ_enc_h.size", "summ_enc_c.size", "summ_enc_c.size", "print", "models.nn_utils.convert_to_onehot", "summarization.SummarizationModel.discrim_model", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "torch.zeros_like().long", "summarization.SummarizationModel.discrim_model", "summ_probs.size", "print", "summarization.SummarizationModel.clf_model", "summarization.SummarizationModel.clf_crit", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "docs_ids.view.view.get_device", "print", "summarization.SummarizationModel.dataset.subwordenc.decode", "print", "print", "torch.log.size", "torch.log.size", "torch.log.size", "docs_ids.view.view.get_device", "print", "print", "docs_enc_h.view.view.transpose().view", "docs_enc_c.view.view.transpose().view", "summarization.SummarizationModel.combine_encs_h_net", "summarization.SummarizationModel.combine_encs_c_net", "range", "docs_enc_h_comb[].unsqueeze().transpose.repeat", "docs_enc_c_comb[].unsqueeze().transpose.repeat", "summ_enc_h.repeat", "summ_enc_c.repeat", "summarization.SummarizationModel.cos_crit().mean", "summarization.SummarizationModel.cos_crit().mean", "models.nn_utils.move_to_cuda", "summarization.SummarizationModel.docs_dec", "probs.size", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "summarization.SummarizationModel.rec_crit", "summ_probs.size", "models.nn_utils.convert_to_onehot.float().detach().requires_grad_", "summ_probs.detach().requires_grad_", "summ_probs.long", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "torch.eq().sum().float", "summarization.SummarizationModel.encode", "tb_writer.add_text", "range", "dec_text.encode", "tb_writer.add_text", "range", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "models.nn_utils.move_to_cuda", "models.nn_utils.move_to_cuda", "docs_enc_h.view.view.view", "docs_enc_h.view.view.view", "summarization.SummarizationModel.combine_encs_h_net", "summarization.SummarizationModel.combine_encs_c_net", "docs_enc_h_comb[].unsqueeze().transpose", "docs_enc_c_comb[].unsqueeze().transpose", "summarization.SummarizationModel.cos_crit", "summarization.SummarizationModel.cos_crit", "summarization.SummarizationModel.cos_crit", "summarization.SummarizationModel.cos_crit", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "cycle_tgt_ids.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "docs_enc_h.view.view.size", "docs_enc_h.view.view.transpose", "docs_enc_c.view.view.transpose", "docs_enc_h_comb[].unsqueeze().transpose.repeat().view.view", "docs_enc_h.view.view.view().detach", "docs_enc_c_comb[].unsqueeze().transpose.repeat().view.view", "docs_enc_c.view.view.view().detach", "summ_enc_h.view", "ext_enc_h.view().detach", "summ_enc_c.view", "ext_enc_c.view().detach", "summarization.SummarizationModel.cos_crit", "summarization.SummarizationModel.cos_crit", "cycle_tgt_ids.view", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "docs_ids.view.view.get_device", "print", "print", "models.nn_utils.convert_to_onehot.float().detach", "summ_probs.detach", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "torch.eq().sum", "docs_enc_h_comb[].unsqueeze", "docs_enc_c_comb[].unsqueeze", "summ_enc_h.repeat().view.view", "docs_enc_h.view.view.view().detach", "summ_enc_c.repeat().view.view", "docs_enc_c.view.view.view().detach", "cycle_tgt_ids.size", "texts[].encode", "tb_writer.add_text", "models.nn_utils.move_to_cuda", "docs_enc_h.view.view.view", "docs_enc_c.view.view.view", "ext_enc_h.view", "ext_enc_c.view", "range", "models.nn_utils.convert_to_onehot.float", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "docs_enc_h.view.view.view", "docs_enc_c.view.view.view", "models.nn_utils.move_to_cuda", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.mlstm.StackedLSTM.state0", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.convert_to_onehot", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.decode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.data_loaders.text_encoder.RealEncoder.encode", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda", "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.nn_utils.move_to_cuda"], ["", "def", "forward", "(", "self", ",", "docs_ids", ",", "labels", ",", "\n", "cycle_tgt_ids", "=", "None", ",", "\n", "extract_summ_ids", "=", "None", ",", "\n", "tau", "=", "None", ",", "\n", "adv_step", "=", "None", ",", "real_ids", "=", "None", ",", "\n", "minibatch_idx", "=", "None", ",", "print_every_nbatches", "=", "None", ",", "\n", "tb_writer", "=", "None", ",", "tb_step", "=", "None", ",", "\n", "wass_loss", "=", "None", ",", "grad_pen_loss", "=", "None", ",", "adv_gen_loss", "=", "None", ",", "clf_loss", "=", "None", ",", "clf_acc", "=", "None", ",", "clf_avg_diff", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            docs_ids: [batch, max_len (concatenated reviews)] when concat_docs=True\n                      [batch, n_docs, max_len] when concat_docs=False\n            labels: [batch]\n                - ratings for classification\n            cycle_tgt_ids: [batch, n_docs, seq_len]\n            extract_summ_ids: [batch, max_sum_len]\n                - summaries from extractive model\n            tau: float\n                Passed in instead of using self.hp.tau because tau may be\n                obtained from a StepAnnealer if there is a scheduled decay.\n            adv_step: str ('discrim' or 'gen')\n                - whether to compute discriminator step (with detach to only train Discriminator), or\n                just the generator step (pass in generated summaries and return .mean())\n            real_ids: [batch, max_rev_len]\n                - reviews used for Discriminator\n\n            minibatch_idx: int (how many minibatches in current epoch)\n            print_every_nbatches: int\n            tb_writer: Tensorboard SummaryWriter\n            tb_step: int (used for writer)\n\n            The remaining are 0-D float Tensors to handle an edge case where the summary is too short for the\n            TextCNN. The current average is passed in.\n\n        Returns:\n            stats: dict (str to 0-D tensors)\n                - contains losses\n            summ_texts: list of strs\n        \"\"\"", "\n", "batch_size", "=", "docs_ids", ".", "size", "(", "0", ")", "\n", "\n", "##########################################################", "\n", "# ENCODE DOCUMENTS", "\n", "##########################################################", "\n", "# Print a review if we're autoencoding or using cycle reconstruction loss so that we can", "\n", "# check how well the reconstruction is", "\n", "if", "self", ".", "hp", ".", "autoenc_docs", "or", "(", "self", ".", "hp", ".", "cycle_loss", "==", "'rec'", ")", ":", "\n", "            ", "if", "minibatch_idx", "%", "print_every_nbatches", "==", "0", ":", "\n", "                ", "if", "docs_ids", ".", "get_device", "(", ")", "==", "0", ":", "\n", "                    ", "print", "(", "'\\n'", ",", "'-'", "*", "100", ")", "\n", "orig_rev_text", "=", "self", ".", "dataset", ".", "subwordenc", ".", "decode", "(", "docs_ids", "[", "0", "]", "[", "0", "]", ")", "\n", "print", "(", "'ORIGINAL REVIEW: '", ",", "orig_rev_text", ".", "encode", "(", "'utf8'", ")", ")", "\n", "print", "(", "'-'", "*", "100", ")", "\n", "if", "tb_writer", ":", "\n", "                        ", "tb_writer", ".", "add_text", "(", "'auto_or_rec/orig_review'", ",", "orig_rev_text", ",", "tb_step", ")", "\n", "\n", "", "", "", "", "if", "not", "self", ".", "hp", ".", "concat_docs", ":", "\n", "            ", "n_docs", "=", "docs_ids", ".", "size", "(", "1", ")", "# TODO: need to get data loader to choose items with same n_docs", "\n", "docs_ids", "=", "docs_ids", ".", "view", "(", "-", "1", ",", "docs_ids", ".", "size", "(", "-", "1", ")", ")", "# [batch * n_docs, len]", "\n", "\n", "", "h_init", ",", "c_init", "=", "self", ".", "docs_enc", ".", "rnn", ".", "state0", "(", "docs_ids", ".", "size", "(", "0", ")", ")", "\n", "h_init", ",", "c_init", "=", "move_to_cuda", "(", "h_init", ")", ",", "move_to_cuda", "(", "c_init", ")", "\n", "hiddens", ",", "cells", ",", "outputs", "=", "self", ".", "docs_enc", "(", "docs_ids", ",", "h_init", ",", "c_init", ")", "\n", "docs_enc_h", ",", "docs_enc_c", "=", "hiddens", "[", "-", "1", "]", ",", "cells", "[", "-", "1", "]", "# [_, n_layers, hidden]", "\n", "\n", "##########################################################", "\n", "# DECODE INTO SUMMARIES AND / OR ORIGINAL REVIEWS", "\n", "##########################################################", "\n", "\n", "# Autoencoder - decode into original reviews", "\n", "if", "self", ".", "hp", ".", "autoenc_docs", ":", "\n", "            ", "assert", "(", "self", ".", "hp", ".", "concat_docs", "==", "False", ")", ",", "'Docs must be encoded individually for autoencoder. Set concat_docs=False'", "\n", "init_input", "=", "torch", ".", "LongTensor", "(", "[", "EDOC_ID", "for", "_", "in", "range", "(", "docs_enc_h", ".", "size", "(", "0", ")", ")", "]", ")", "# batch * n_docs", "\n", "init_input", "=", "move_to_cuda", "(", "init_input", ")", "\n", "docs_autodec_probs", ",", "_", ",", "docs_autodec_texts", ",", "_", "=", "self", ".", "docs_autodec", "(", "docs_enc_h", ",", "docs_enc_c", ",", "init_input", ",", "\n", "targets", "=", "docs_ids", ",", "\n", "eos_id", "=", "EDOC_ID", ",", "non_pad_prob_val", "=", "1e-14", ",", "\n", "softmax_method", "=", "'softmax'", ",", "\n", "sample_method", "=", "'greedy'", ",", "\n", "tau", "=", "tau", ",", "\n", "subwordenc", "=", "self", ".", "dataset", ".", "subwordenc", ")", "\n", "\n", "docs_autodec_logprobs", "=", "torch", ".", "log", "(", "docs_autodec_probs", ")", "\n", "autoenc_loss", "=", "self", ".", "rec_crit", "(", "docs_autodec_logprobs", ".", "view", "(", "-", "1", ",", "docs_autodec_logprobs", ".", "size", "(", "-", "1", ")", ")", ",", "\n", "docs_ids", ".", "view", "(", "-", "1", ")", ")", "\n", "if", "self", ".", "hp", ".", "sum_label_smooth", ":", "\n", "                ", "autoenc_loss", "/=", "(", "docs_ids", "!=", "move_to_cuda", "(", "torch", ".", "tensor", "(", "PAD_ID", ")", ")", ")", ".", "sum", "(", ")", ".", "float", "(", ")", "\n", "", "self", ".", "stats", "[", "'autoenc_loss'", "]", "=", "autoenc_loss", "\n", "\n", "if", "minibatch_idx", "%", "print_every_nbatches", "==", "0", ":", "\n", "                ", "if", "docs_ids", ".", "get_device", "(", ")", "==", "0", ":", "\n", "                    ", "dec_text", "=", "docs_autodec_texts", "[", "0", "]", "\n", "print", "(", "'DECODED REVIEW: '", ",", "dec_text", ".", "encode", "(", "'utf8'", ")", ")", "\n", "print", "(", "'-'", "*", "100", ",", "'\\n'", ")", "\n", "if", "tb_writer", ":", "\n", "                        ", "tb_writer", ".", "add_text", "(", "'auto_or_rec/auto_dec_review'", ",", "dec_text", ",", "tb_step", ")", "\n", "\n", "# Early return if we're only computing auto-encoder (don't have to decode into summaries)", "\n", "", "", "", "if", "self", ".", "hp", ".", "autoenc_only", ":", "\n", "                ", "dummy_summ_texts", "=", "[", "'a dummy review'", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "return", "self", ".", "stats", ",", "dummy_summ_texts", "\n", "\n", "# Decode into summary", "\n", "", "", "if", "not", "self", ".", "hp", ".", "concat_docs", ":", "\n", "            ", "_", ",", "n_layers", ",", "hidden_size", "=", "docs_enc_h", ".", "size", "(", ")", "\n", "docs_enc_h", "=", "docs_enc_h", ".", "view", "(", "batch_size", ",", "n_docs", ",", "n_layers", ",", "hidden_size", ")", "\n", "docs_enc_c", "=", "docs_enc_c", ".", "view", "(", "batch_size", ",", "n_docs", ",", "n_layers", ",", "hidden_size", ")", "\n", "if", "self", ".", "hp", ".", "combine_encs", "==", "'mean'", ":", "\n", "                ", "docs_enc_h_comb", "=", "docs_enc_h", ".", "mean", "(", "dim", "=", "1", ")", "\n", "docs_enc_c_comb", "=", "docs_enc_c", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "hp", ".", "combine_encs", "==", "'ff'", ":", "\n", "                ", "docs_enc_h_comb", "=", "docs_enc_h", ".", "transpose", "(", "1", ",", "2", ")", ".", "view", "(", "batch_size", ",", "n_layers", ",", "-", "1", ")", "\n", "# [batch, n_layers, n_docs * hidden]", "\n", "docs_enc_c_comb", "=", "docs_enc_c", ".", "transpose", "(", "1", ",", "2", ")", ".", "view", "(", "batch_size", ",", "n_layers", ",", "-", "1", ")", "\n", "docs_enc_h_comb", "=", "self", ".", "combine_encs_h_net", "(", "docs_enc_h_comb", ")", "# [batch, n_layers, hidden]", "\n", "docs_enc_c_comb", "=", "self", ".", "combine_encs_c_net", "(", "docs_enc_c_comb", ")", "\n", "", "elif", "self", ".", "hp", ".", "combine_encs", "==", "'gru'", ":", "\n", "                ", "n_directions", "=", "2", "if", "self", ".", "hp", ".", "combine_encs_gru_bi", "else", "1", "\n", "init_h", "=", "torch", ".", "zeros", "(", "self", ".", "hp", ".", "combine_encs_gru_nlayers", "*", "n_directions", ",", "batch_size", ",", "hidden_size", ")", "\n", "init_c", "=", "torch", ".", "zeros", "(", "self", ".", "hp", ".", "combine_encs_gru_nlayers", "*", "n_directions", ",", "batch_size", ",", "hidden_size", ")", "\n", "init_h", "=", "move_to_cuda", "(", "init_h", ")", "\n", "init_c", "=", "move_to_cuda", "(", "init_c", ")", "\n", "docs_enc_h_comb", "=", "docs_enc_h", ".", "view", "(", "batch_size", ",", "n_docs", "*", "n_layers", ",", "hidden_size", ")", "\n", "docs_enc_c_comb", "=", "docs_enc_h", ".", "view", "(", "batch_size", ",", "n_docs", "*", "n_layers", ",", "hidden_size", ")", "\n", "# [batch, n_directions * gru_nlayers, hidden]", "\n", "# self.combine_encs_h_net.flatten_parameters()", "\n", "# self.combine_encs_c_net.flatten_parameters()", "\n", "_", ",", "docs_enc_h_comb", "=", "self", ".", "combine_encs_h_net", "(", "docs_enc_h_comb", ",", "init_h", ")", "\n", "_", ",", "docs_enc_c_comb", "=", "self", ".", "combine_encs_c_net", "(", "docs_enc_c_comb", ",", "init_c", ")", "\n", "# [n_directions * gru_nlayers, batch, hidden]", "\n", "docs_enc_h_comb", "=", "docs_enc_h_comb", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "# last layer TODO: last or combine?", "\n", "docs_enc_c_comb", "=", "docs_enc_c_comb", "[", "-", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ".", "transpose", "(", "0", ",", "1", ")", "# last layer", "\n", "\n", "\n", "", "", "softmax_method", "=", "'gumbel'", "\n", "sample_method", "=", "'greedy'", "\n", "if", "self", ".", "hp", ".", "early_cycle", ":", "\n", "            ", "softmax_method", "=", "'softmax'", "\n", "# sample_method = 'sample'", "\n", "", "init_input", "=", "torch", ".", "LongTensor", "(", "[", "EDOC_ID", "for", "_", "in", "range", "(", "batch_size", ")", "]", ")", "\n", "init_input", "=", "move_to_cuda", "(", "init_input", ")", "\n", "# Backwards compatibility with models trained before dataset refactoring", "\n", "# We could use the code_snapshot saved at the time the model was trained, but I've added some", "\n", "# useful things (e.g. tracking NLL of summaries)", "\n", "tgt_summ_seq_len", "=", "self", ".", "dataset", ".", "conf", ".", "review_max_len", "if", "hasattr", "(", "self", ".", "dataset", ",", "'conf'", ")", "else", "self", ".", "hp", ".", "yelp_review_max_len", "\n", "summ_probs", ",", "_", ",", "summ_texts", ",", "_", "=", "self", ".", "summ_dec", "(", "docs_enc_h_comb", ",", "docs_enc_c_comb", ",", "init_input", ",", "\n", "seq_len", "=", "tgt_summ_seq_len", ",", "eos_id", "=", "EDOC_ID", ",", "\n", "# seq_len=self.dataset.conf.review_max_len, eos_id=EDOC_ID,", "\n", "softmax_method", "=", "softmax_method", ",", "sample_method", "=", "sample_method", ",", "\n", "tau", "=", "tau", ",", "eps", "=", "self", ".", "hp", ".", "g_eps", ",", "gumbel_hard", "=", "True", ",", "\n", "attend_to_embs", "=", "docs_enc_h", ",", "\n", "subwordenc", "=", "self", ".", "dataset", ".", "subwordenc", ")", "\n", "\n", "# [batch, max_summ_len, vocab];  [batch] of str's", "\n", "\n", "# Compute a cosine similarity loss between the (mean) summary representation that's fed to the", "\n", "# summary decoder and each of the original encoded reviews.", "\n", "# With this setup, there's no need for the summary encoder or back propagating through the summary.", "\n", "if", "self", ".", "hp", ".", "early_cycle", ":", "\n", "# Repeat each summary representation n_docs times to match shape of tensor with individual reviews", "\n", "            ", "docs_enc_h_comb_rep", "=", "docs_enc_h_comb", ".", "repeat", "(", "1", ",", "n_docs", ",", "1", ")", ".", "view", "(", "batch_size", "*", "n_docs", ",", "docs_enc_h_comb", ".", "size", "(", "1", ")", ",", "docs_enc_h_comb", ".", "size", "(", "2", ")", ")", "\n", "docs_enc_c_comb_rep", "=", "docs_enc_c_comb", ".", "repeat", "(", "1", ",", "n_docs", ",", "1", ")", ".", "view", "(", "batch_size", "*", "n_docs", ",", "docs_enc_c_comb", ".", "size", "(", "1", ")", ",", "docs_enc_c_comb", ".", "size", "(", "2", ")", ")", "\n", "\n", "loss", "=", "-", "self", ".", "cos_crit", "(", "docs_enc_h_comb_rep", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "docs_enc_h", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "if", "not", "self", ".", "hp", ".", "cos_honly", ":", "\n", "                   ", "loss", "-=", "self", ".", "cos_crit", "(", "docs_enc_c_comb_rep", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "docs_enc_c", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "", "self", ".", "stats", "[", "'early_cycle_loss'", "]", "=", "loss", "*", "self", ".", "hp", ".", "cos_wgt", "\n", "\n", "##########################################################", "\n", "# CYCLE LOSS and / or  EXTRACTIVE SUMMARY LOSS", "\n", "##########################################################", "\n", "\n", "# Encode summaries", "\n", "", "if", "self", ".", "hp", ".", "sum_cycle", "or", "self", ".", "hp", ".", "extract_loss", ":", "\n", "            ", "init_h", ",", "init_c", "=", "self", ".", "summ_enc", ".", "rnn", ".", "state0", "(", "batch_size", ")", "\n", "init_h", ",", "init_c", "=", "move_to_cuda", "(", "init_h", ")", ",", "move_to_cuda", "(", "init_c", ")", "\n", "hiddens", ",", "cells", ",", "outputs", "=", "self", ".", "summ_enc", "(", "summ_probs", ",", "init_h", ",", "init_c", ")", "\n", "summ_enc_h", ",", "summ_enc_c", "=", "hiddens", "[", "-", "1", "]", ",", "cells", "[", "-", "1", "]", "# [batch, n_layers, hidden], ''", "\n", "\n", "# Extractive vs. abstractive summary loss", "\n", "", "if", "self", ".", "hp", ".", "extract_loss", ":", "\n", "# Encode extractive summary", "\n", "            ", "init_h", ",", "init_c", "=", "self", ".", "summ_enc", ".", "rnn", ".", "state0", "(", "batch_size", ")", "\n", "init_h", ",", "init_c", "=", "move_to_cuda", "(", "init_h", ")", ",", "move_to_cuda", "(", "init_c", ")", "\n", "ext_hiddens", ",", "ext_cells", ",", "ext_outputs", "=", "self", ".", "summ_enc", "(", "extract_summ_ids", ",", "init_h", ",", "init_c", ")", "\n", "ext_enc_h", ",", "ext_enc_c", "=", "ext_hiddens", "[", "-", "1", "]", ",", "ext_cells", "[", "-", "1", "]", "# [batch, n_layers, hidden], ''", "\n", "loss", "=", "-", "self", ".", "cos_crit", "(", "summ_enc_h", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "ext_enc_h", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "if", "not", "self", ".", "hp", ".", "cos_honly", ":", "\n", "                   ", "loss", "-=", "self", ".", "cos_crit", "(", "summ_enc_c", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "ext_enc_c", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "", "self", ".", "stats", "[", "'extract_loss'", "]", "=", "loss", "\n", "\n", "# Reconstruction or encoder cycle loss", "\n", "", "if", "self", ".", "hp", ".", "sum_cycle", ":", "\n", "# Repeat each summary representation n_docs times to match shape of tensor with individual reviews", "\n", "            ", "summ_enc_h_rep", "=", "summ_enc_h", ".", "repeat", "(", "1", ",", "n_docs", ",", "1", ")", ".", "view", "(", "batch_size", "*", "n_docs", ",", "summ_enc_h", ".", "size", "(", "1", ")", ",", "summ_enc_h", ".", "size", "(", "2", ")", ")", "\n", "summ_enc_c_rep", "=", "summ_enc_c", ".", "repeat", "(", "1", ",", "n_docs", ",", "1", ")", ".", "view", "(", "batch_size", "*", "n_docs", ",", "summ_enc_c", ".", "size", "(", "1", ")", ",", "summ_enc_c", ".", "size", "(", "2", ")", ")", "\n", "\n", "if", "self", ".", "hp", ".", "cycle_loss", "==", "'enc'", ":", "\n", "                ", "assert", "(", "self", ".", "hp", ".", "concat_docs", "==", "False", ")", ",", "'Docs must have been encoded individually for autoencoder. Set concat_docs=False'", "\n", "# (It's possible to have cycle_loss=enc and concat_docs=False, you just have to als encode them", "\n", "# separately. Didn't add that b/c I think I'll always have concat_docs=False from now on)", "\n", "# docs_enc_h, docs_enc_c: [batch, n_docs, n_layers, hidden]", "\n", "loss", "=", "-", "self", ".", "cos_crit", "(", "summ_enc_h_rep", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "docs_enc_h", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "if", "not", "self", ".", "hp", ".", "cos_honly", ":", "\n", "                    ", "loss", "-=", "self", ".", "cos_crit", "(", "summ_enc_c_rep", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "\n", "docs_enc_c", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "", "self", ".", "stats", "[", "'cycle_loss'", "]", "=", "loss", "*", "self", ".", "hp", ".", "cos_wgt", "\n", "", "elif", "self", ".", "hp", ".", "cycle_loss", "==", "'rec'", ":", "\n", "                ", "init_input", "=", "move_to_cuda", "(", "torch", ".", "LongTensor", "(", "[", "EDOC_ID", "for", "_", "in", "range", "(", "batch_size", "*", "n_docs", ")", "]", ")", ")", "\n", "probs", ",", "ids", ",", "texts", ",", "extra", "=", "self", ".", "docs_dec", "(", "summ_enc_h_rep", ",", "summ_enc_c_rep", ",", "init_input", ",", "\n", "targets", "=", "cycle_tgt_ids", ".", "view", "(", "-", "1", ",", "cycle_tgt_ids", ".", "size", "(", "-", "1", ")", ")", ",", "\n", "eos_id", "=", "EDOC_ID", ",", "non_pad_prob_val", "=", "1e-14", ",", "\n", "softmax_method", "=", "'softmax'", ",", "sample_method", "=", "'sample'", ",", "\n", "tau", "=", "tau", ",", "\n", "subwordenc", "=", "self", ".", "dataset", ".", "subwordenc", ")", "\n", "vocab_size", "=", "probs", ".", "size", "(", "-", "1", ")", "\n", "logprobs", "=", "torch", ".", "log", "(", "probs", ")", ".", "view", "(", "-", "1", ",", "vocab_size", ")", "\n", "loss", "=", "self", ".", "rec_crit", "(", "logprobs", ",", "cycle_tgt_ids", ".", "view", "(", "-", "1", ")", ")", "\n", "if", "self", ".", "hp", ".", "sum_label_smooth", ":", "\n", "                    ", "loss", "/=", "(", "cycle_tgt_ids", "!=", "move_to_cuda", "(", "torch", ".", "tensor", "(", "PAD_ID", ")", ")", ")", ".", "sum", "(", ")", ".", "float", "(", ")", "\n", "", "self", ".", "stats", "[", "'cycle_loss'", "]", "=", "loss", "*", "self", ".", "hp", ".", "cos_wgt", "\n", "\n", "if", "minibatch_idx", "%", "print_every_nbatches", "==", "0", ":", "\n", "                    ", "if", "docs_ids", ".", "get_device", "(", ")", "==", "0", ":", "\n", "                        ", "print", "(", "'DECODED REVIEW: '", ",", "texts", "[", "0", "]", ".", "encode", "(", "'utf8'", ")", ")", "\n", "print", "(", "'-'", "*", "100", ",", "'\\n'", ")", "\n", "if", "tb_writer", ":", "\n", "                            ", "tb_writer", ".", "add_text", "(", "'auto_or_rec/rec_review'", ",", "texts", "[", "0", "]", ",", "tb_step", ")", "\n", "\n", "##########################################################", "\n", "# DISCRIMINATOR", "\n", "##########################################################", "\n", "# TODO: Remove this -- discriminator is not used", "\n", "\n", "# Goal: self.discrim_model should be good at distinguishing between generated canonical review and", "\n", "# original reviews. The adv_loss returns difference between gen and real (plus the gradient penalty)", "\n", "# To train the discriminator, we want to minimize this value (gen is small, real is large)", "\n", "# To train the rest, want to maximize gen (or minimize -gen)", "\n", "", "", "", "", "", "if", "adv_step", "==", "'discrim'", ":", "\n", "            ", "if", "(", "self", ".", "hp", ".", "discrim_model", "==", "'cnn'", ")", "and", "(", "summ_probs", ".", "size", "(", "1", ")", "<", "5", ")", ":", "# conv filters are 3,4,5", "\n", "                ", "print", "(", "'Summary length is less than 5... skipping Discriminator model because it uses a CNN '", "\n", "'with a convolution kernel of size 5'", ")", "\n", "", "else", ":", "\n", "                ", "real_ids_onehot", "=", "convert_to_onehot", "(", "real_ids", ",", "self", ".", "dataset", ".", "subwordenc", ".", "vocab_size", ")", "\n", "result", "=", "self", ".", "discrim_model", "(", "real_ids_onehot", ".", "float", "(", ")", ".", "detach", "(", ")", ".", "requires_grad_", "(", ")", ",", "\n", "summ_probs", ".", "detach", "(", ")", ".", "requires_grad_", "(", ")", ")", "\n", "gen_mean", ",", "real_mean", ",", "grad_pen", "=", "result", "[", "0", "]", ",", "result", "[", "1", "]", ",", "result", "[", "2", "]", "\n", "wass_loss", "=", "gen_mean", "-", "real_mean", "\n", "grad_pen_loss", "=", "self", ".", "hp", ".", "wgan_lam", "*", "grad_pen", "\n", "\n", "", "adv_loss", "=", "wass_loss", "+", "grad_pen_loss", "\n", "self", ".", "stats", ".", "update", "(", "{", "'wass_loss'", ":", "wass_loss", ",", "'grad_pen_loss'", ":", "grad_pen_loss", ",", "'adv_loss'", ":", "adv_loss", "}", ")", "\n", "", "elif", "adv_step", "==", "'gen'", ":", "\n", "            ", "dummy_real", "=", "torch", ".", "zeros_like", "(", "summ_probs", ")", ".", "long", "(", ")", "\n", "result", "=", "self", ".", "discrim_model", "(", "dummy_real", ",", "summ_probs", ")", "\n", "gen_mean", "=", "result", "[", "0", "]", "\n", "self", ".", "stats", "[", "'adv_gen_loss'", "]", "=", "-", "1", "*", "gen_mean", "\n", "\n", "##########################################################", "\n", "# CLASSIFIER", "\n", "##########################################################", "\n", "\n", "", "if", "self", ".", "hp", ".", "sum_clf", ":", "\n", "            ", "if", "summ_probs", ".", "size", "(", "1", ")", "<", "5", ":", "# conv filters are 3,4,5", "\n", "                ", "print", "(", "'Summary length is less than 5... skipping classification model because it uses a CNN '", "\n", "'with a convolution kernel of size 5'", ")", "\n", "", "else", ":", "\n", "                ", "logits", "=", "self", ".", "clf_model", "(", "summ_probs", ".", "long", "(", ")", ")", "\n", "clf_loss", "=", "self", ".", "clf_crit", "(", "logits", ",", "labels", ")", "\n", "\n", "_", ",", "indices", "=", "torch", ".", "max", "(", "logits", ",", "dim", "=", "1", ")", "\n", "clf_avg_diff", "=", "(", "labels", "-", "indices", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "clf_acc", "=", "torch", ".", "eq", "(", "indices", ",", "labels", ")", ".", "sum", "(", ")", ".", "float", "(", ")", "/", "batch_size", "\n", "\n", "", "self", ".", "stats", ".", "update", "(", "{", "'clf_loss'", ":", "clf_loss", ",", "'clf_acc'", ":", "clf_acc", ",", "'clf_avg_diff'", ":", "clf_avg_diff", "}", ")", "\n", "\n", "", "return", "self", ".", "stats", ",", "summ_texts", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.text_cnn.BasicTextCNN.__init__": [[16, 35], ["torch.Module.__init__", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "filter_sizes", ",", "n_feat_maps", ",", "emb_size", ",", "dropout_prob", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            filter_sizes: list of ints\n                - Size of convolution window (referred to as filter widths in original paper)\n            n_feat_maps: int\n                - Number of output feature maps for each filter size\n            emb_size: int\n                - Size of word embeddings the model operates over\n            dropout_prob: float\n        \"\"\"", "\n", "super", "(", "BasicTextCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "filter_sizes", "=", "filter_sizes", "\n", "self", ".", "n_feat_maps", "=", "n_feat_maps", "\n", "\n", "self", ".", "act", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "cnn_modlist", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Conv2d", "(", "1", ",", "n_feat_maps", ",", "(", "filter_size", ",", "emb_size", ")", ")", "for", "filter_size", "in", "filter_sizes", "]", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.models.text_cnn.BasicTextCNN.forward": [[36, 57], ["x.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "text_cnn.BasicTextCNN.dropout", "text_cnn.BasicTextCNN.act", "torch.max_pool2d().squeeze().squeeze", "torch.max_pool2d().squeeze().squeeze", "torch.max_pool2d().squeeze().squeeze", "torch.max_pool2d().squeeze().squeeze", "cnn", "torch.max_pool2d().squeeze", "torch.max_pool2d().squeeze", "torch.max_pool2d().squeeze", "torch.max_pool2d().squeeze", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "cnn_relu.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: [batch, seq_len, emb_size]\n\n        Returns:\n\n        \"\"\"", "\n", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# [batch, 1, seq_len, emb_size]", "\n", "\n", "cnn_relus", "=", "[", "self", ".", "act", "(", "cnn", "(", "x", ")", ")", "for", "cnn", "in", "self", ".", "cnn_modlist", "]", "\n", "# Each is [batch, n_feat_maps, seq_len-filter_size+1, 1]", "\n", "\n", "# Pool over time dimension", "\n", "pooled", "=", "[", "F", ".", "max_pool2d", "(", "cnn_relu", ",", "(", "cnn_relu", ".", "size", "(", "2", ")", ",", "1", ")", ")", ".", "squeeze", "(", "3", ")", ".", "squeeze", "(", "2", ")", "for", "cnn_relu", "in", "cnn_relus", "]", "\n", "# Each is [batch, n_feat_maps]", "\n", "\n", "outputs", "=", "T", ".", "cat", "(", "pooled", ",", "1", ")", "# [batch, n_feat_maps * len(filter_sizes)]", "\n", "outputs", "=", "self", ".", "dropout", "(", "outputs", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.tokenize.tokenize": [[25, 50], ["re.sub.lower", "re.sub", "re.split", "re.match", "stemmer.stem", "len"], "function", ["None"], ["def", "tokenize", "(", "text", ",", "stemmer", ")", ":", "\n", "    ", "\"\"\"Tokenize input text into a list of tokens.\n    This approach aims to replicate the approach taken by Chin-Yew Lin in\n    the original ROUGE implementation.\n    Args:\n      text: A text blob to tokenize.\n      stemmer: An optional stemmer.\n    Returns:\n      A list of string tokens extracted from input text.\n    \"\"\"", "\n", "\n", "# Convert everything to lowercase.", "\n", "text", "=", "text", ".", "lower", "(", ")", "\n", "# Replace any non-alpha-numeric characters with spaces.", "\n", "text", "=", "re", ".", "sub", "(", "r\"[^a-z0-9]+\"", ",", "\" \"", ",", "text", ")", "\n", "\n", "tokens", "=", "re", ".", "split", "(", "r\"\\s+\"", ",", "text", ")", "\n", "if", "stemmer", ":", "\n", "# Only stem words more than 3 characters long.", "\n", "        ", "tokens", "=", "[", "stemmer", ".", "stem", "(", "x", ")", "if", "len", "(", "x", ")", ">", "3", "else", "x", "for", "x", "in", "tokens", "]", "\n", "\n", "# One final check to drop any empty or invalid tokens.", "\n", "", "tokens", "=", "[", "x", "for", "x", "in", "tokens", "if", "re", ".", "match", "(", "r\"^[a-z0-9]+$\"", ",", "x", ")", "]", "\n", "\n", "return", "tokens", "\n", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BaseScorer.score": [[41, 50], ["None"], "methods", ["None"], ["@", "abc", ".", "abstractmethod", "\n", "def", "score", "(", "self", ",", "target", ",", "prediction", ")", ":", "\n", "        ", "\"\"\"Calculates score between the target and prediction.\n        Args:\n          target: Text containing the target (ground truth) text.\n          prediction: Text containing the predicted text.\n        Returns:\n          A dict mapping each score_type (string) to Score object.\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BootstrapAggregator.__init__": [[76, 96], ["collections.defaultdict", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "confidence_interval", "=", "0.95", ",", "\n", "n_samples", "=", "1000", ")", ":", "\n", "        ", "\"\"\"Initializes a BootstrapAggregator object.\n        Args:\n          confidence_interval: Confidence interval to compute on the mean as a\n            decimal.\n          n_samples: Number of samples to use for bootstrap resampling.\n        Raises:\n          ValueError: If invalid argument is given.\n        \"\"\"", "\n", "\n", "if", "confidence_interval", "<", "0", "or", "confidence_interval", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"confidence_interval must be in range [0, 1]\"", ")", "\n", "", "if", "n_samples", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"n_samples must be positive\"", ")", "\n", "\n", "", "self", ".", "_n_samples", "=", "n_samples", "\n", "self", ".", "_confidence_interval", "=", "confidence_interval", "\n", "self", ".", "_scores", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BootstrapAggregator.add_scores": [[97, 106], ["six.iteritems", "scoring.BootstrapAggregator._scores[].append"], "methods", ["None"], ["", "def", "add_scores", "(", "self", ",", "scores", ")", ":", "\n", "        ", "\"\"\"Adds a sample for future aggregation.\n        Args:\n          scores: Dict mapping score_type strings to Score object.\n        \"\"\"", "\n", "\n", "for", "score_type", ",", "score", "in", "six", ".", "iteritems", "(", "scores", ")", ":", "\n", "            ", "self", ".", "_scores", "[", "score_type", "]", ".", "append", "(", "(", "score", ".", "precision", ",", "score", ".", "recall", ",", "\n", "score", ".", "fmeasure", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BootstrapAggregator.aggregate": [[107, 127], ["six.iteritems", "numpy.vstack", "scoring.BootstrapAggregator._bootstrap_resample", "tuple", "scoring.AggregateScore", "scoring.Score", "six.moves.xrange"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BootstrapAggregator._bootstrap_resample"], ["", "", "def", "aggregate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Aggregates scores previously added using add_scores.\n        Returns:\n          A dict mapping score_type to AggregateScore objects.\n        \"\"\"", "\n", "\n", "result", "=", "{", "}", "\n", "for", "score_type", ",", "scores", "in", "six", ".", "iteritems", "(", "self", ".", "_scores", ")", ":", "\n", "# Stack scores into a 2-d matrix of (sample, measure).", "\n", "            ", "score_matrix", "=", "np", ".", "vstack", "(", "scores", ")", "\n", "# Percentiles are returned as (interval, measure).", "\n", "percentiles", "=", "self", ".", "_bootstrap_resample", "(", "score_matrix", ")", "\n", "# Extract the three intervals (low, mid, high).", "\n", "intervals", "=", "tuple", "(", "(", "Score", "(", "\n", "precision", "=", "percentiles", "[", "j", ",", "0", "]", ",", "\n", "recall", "=", "percentiles", "[", "j", ",", "1", "]", ",", "\n", "fmeasure", "=", "percentiles", "[", "j", ",", "2", "]", ")", "for", "j", "in", "xrange", "(", "3", ")", ")", ")", "\n", "result", "[", "score_type", "]", "=", "AggregateScore", "(", "\n", "low", "=", "intervals", "[", "0", "]", ",", "mid", "=", "intervals", "[", "1", "]", ",", "high", "=", "intervals", "[", "2", "]", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.BootstrapAggregator._bootstrap_resample": [[128, 153], ["numpy.zeros", "six.moves.xrange", "numpy.percentile", "numpy.random.choice", "numpy.mean", "numpy.array", "numpy.arange"], "methods", ["None"], ["", "def", "_bootstrap_resample", "(", "self", ",", "matrix", ")", ":", "\n", "        ", "\"\"\"Performs bootstrap resampling on a matrix of scores.\n        Args:\n          matrix: A 2-d matrix of (sample, measure).\n        Returns:\n          A 2-d matrix of (bounds, measure). There are three bounds: low (row 0),\n          mid (row 1) and high (row 2). Mid is always the mean, while low and high\n          bounds are specified by self._confidence_interval (which defaults to 0.95\n          meaning it will return the 2.5th and 97.5th percentiles for a 95%\n          confidence interval on the mean).\n        \"\"\"", "\n", "\n", "# Matrix of (bootstrap sample, measure).", "\n", "sample_mean", "=", "np", ".", "zeros", "(", "(", "self", ".", "_n_samples", ",", "matrix", ".", "shape", "[", "1", "]", ")", ")", "\n", "for", "i", "in", "xrange", "(", "self", ".", "_n_samples", ")", ":", "\n", "            ", "sample_idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "np", ".", "arange", "(", "matrix", ".", "shape", "[", "0", "]", ")", ",", "size", "=", "matrix", ".", "shape", "[", "0", "]", ")", "\n", "sample", "=", "matrix", "[", "sample_idx", ",", ":", "]", "\n", "sample_mean", "[", "i", ",", ":", "]", "=", "np", ".", "mean", "(", "sample", ",", "axis", "=", "0", ")", "\n", "\n", "# Take percentiles on the estimate of the mean using bootstrap samples.", "\n", "# Final result is a (bounds, measure) matrix.", "\n", "", "percentile_delta", "=", "(", "1", "-", "self", ".", "_confidence_interval", ")", "/", "2", "\n", "q", "=", "100", "*", "np", ".", "array", "(", "[", "percentile_delta", ",", "0.5", ",", "1", "-", "percentile_delta", "]", ")", "\n", "return", "np", ".", "percentile", "(", "sample_mean", ",", "q", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.fmeasure": [[155, 162], ["None"], "function", ["None"], ["", "", "def", "fmeasure", "(", "precision", ",", "recall", ")", ":", "\n", "    ", "\"\"\"Computes f-measure given precision and recall values.\"\"\"", "\n", "\n", "if", "precision", "+", "recall", ">", "0", ":", "\n", "        ", "return", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "", "else", ":", "\n", "        ", "return", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.__init__": [[29, 63], ["evaluation.rouge_scorer.RougeScorer", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "set", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "nltk.corpus.stopwords.words"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict"], ["    ", "def", "__init__", "(", "self", ",", "remove_stopwords", "=", "False", ",", "use_stemmer", "=", "True", ",", "store_all", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            remove_stopwords: boolean (remove stop words before calculating rouge)\n            use_stemmer: boolean (stem words before calculating rouge)\n            store_all: boolean\n                - whether to store the 4 rouge stats for every summary. This could be used to plot the\n                distribution of the stats instead of just looking at the average.\n        \"\"\"", "\n", "self", ".", "remove_stopwords", "=", "remove_stopwords", "\n", "if", "remove_stopwords", ":", "\n", "            ", "self", ".", "stopwords", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "", "self", ".", "use_stemmer", "=", "use_stemmer", "\n", "self", ".", "store_all", "=", "store_all", "\n", "\n", "# python implementation of ROUGE", "\n", "self", ".", "rouge_scorer", "=", "RougeScorer", "(", "[", "'rouge1'", ",", "'rouge2'", ",", "'rougeL'", "]", ",", "use_stemmer", "=", "use_stemmer", ")", "\n", "\n", "# Every time update_avg_rouge() is called, the rouges are calculated between a summary and n_docs reviews.", "\n", "# Using those rouge scores, four statistics are computed: the mean, max, min, and std.", "\n", "# The following dictionaries are then updated using those statistics, i.e. they will be a mean of", "\n", "# each of those four stats.", "\n", "self", ".", "_updates", "=", "0", "\n", "self", ".", "avg_avg_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "self", ".", "avg_min_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "self", ".", "avg_max_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "self", ".", "avg_std_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "\n", "if", "self", ".", "store_all", ":", "\n", "            ", "self", ".", "avg_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", "list", ")", "\n", "self", ".", "min_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", "list", ")", "\n", "self", ".", "max_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", "list", ")", "\n", "self", ".", "std_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict": [[69, 77], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["", "", "def", "get_rouge_defaultdict", "(", "self", ",", "default_type", "=", "float", ")", ":", "\n", "        ", "\"\"\"\n        Return dict of default dicts.\n        \"\"\"", "\n", "dict", "=", "{", "'rouge1'", ":", "defaultdict", "(", "default_type", ")", ",", "\n", "'rouge2'", ":", "defaultdict", "(", "default_type", ")", ",", "\n", "'rougeL'", ":", "defaultdict", "(", "default_type", ")", "}", "\n", "return", "dict", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts": [[78, 83], ["None"], "methods", ["None"], ["", "def", "get_avg_stats_dicts", "(", "self", ")", ":", "\n", "        ", "return", "{", "'avg'", ":", "self", ".", "avg_avg_rouges", ",", "\n", "'min'", ":", "self", ".", "avg_min_rouges", ",", "\n", "'max'", ":", "self", ".", "avg_max_rouges", ",", "\n", "'std'", ":", "self", ".", "avg_std_rouges", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_list_stats_dicts": [[84, 89], ["None"], "methods", ["None"], ["", "def", "get_list_stats_dicts", "(", "self", ")", ":", "\n", "        ", "return", "{", "'avg'", ":", "self", ".", "avg_rouges", ",", "\n", "'min'", ":", "self", ".", "min_rouges", ",", "\n", "'max'", ":", "self", ".", "max_rouges", ",", "\n", "'std'", ":", "self", ".", "std_rouges", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.update_with_evaluator": [[90, 114], ["eval_utils.EvalMetrics.get_avg_stats_dicts().items", "eval_utils.EvalMetrics.get_list_stats_dicts().items", "getattr", "getattr.items", "getattr", "getattr.items", "eval_utils.EvalMetrics.get_avg_stats_dicts", "d.items", "eval_utils.EvalMetrics.get_list_stats_dicts", "d.items", "utils.update_moving_avg", "[].extend"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_avg_stats_dicts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_list_stats_dicts", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "update_with_evaluator", "(", "self", ",", "evaluator", ")", ":", "\n", "        ", "\"\"\"\n        Use another EvalMetrics object to update the self.* rouge dicts. This is used\n        by best_review_baseline() in run_evaluations.\n\n        Args:\n            evaluator: EvalMetrics instance\n        \"\"\"", "\n", "self", ".", "_updates", "+=", "1", "# global count", "\n", "\n", "# Update moving averages", "\n", "for", "stat", ",", "rouge_dict", "in", "self", ".", "get_avg_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "src_rouge_dict", "=", "getattr", "(", "evaluator", ",", "'avg_{}_rouges'", ".", "format", "(", "stat", ")", ")", "\n", "for", "rouge_name", ",", "d", "in", "src_rouge_dict", ".", "items", "(", ")", ":", "\n", "                ", "for", "metric", ",", "score", "in", "d", ".", "items", "(", ")", ":", "\n", "                    ", "cur_score", "=", "rouge_dict", "[", "rouge_name", "]", "[", "metric", "]", "\n", "rouge_dict", "[", "rouge_name", "]", "[", "metric", "]", "=", "update_moving_avg", "(", "cur_score", ",", "score", ",", "self", ".", "_updates", ")", "\n", "\n", "# Add to lists", "\n", "", "", "", "for", "stat", ",", "rouge_dict", "in", "self", ".", "get_list_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "src_rouge_dict", "=", "getattr", "(", "evaluator", ",", "'{}_rouges'", ".", "format", "(", "stat", ")", ")", "\n", "for", "rouge_name", ",", "d", "in", "src_rouge_dict", ".", "items", "(", ")", ":", "\n", "                ", "for", "metric", ",", "scores", "in", "d", ".", "items", "(", ")", ":", "\n", "                    ", "rouge_dict", "[", "rouge_name", "]", "[", "metric", "]", ".", "extend", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.calc_rouges": [[120, 138], ["eval_utils.EvalMetrics.rouge_scorer.score", "nltk.tokenize.word_tokenize", "nltk.tokenize.word_tokenize"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.score"], ["", "", "", "", "def", "calc_rouges", "(", "self", ",", "source", ",", "summary", ")", ":", "\n", "        ", "\"\"\"\n        Wrapper around the rouge_scorer. Removes stop words potentially,\n\n        Args:\n            source: str\n            summary: str\n\n        Returns:\n            dict: keys are strs, values are rouge objects with ('precision', 'recall', and 'fmeasure' fields)\n        \"\"\"", "\n", "if", "self", ".", "remove_stopwords", ":", "\n", "            ", "source", "=", "' '", ".", "join", "(", "[", "w", "for", "w", "in", "word_tokenize", "(", "source", ")", "if", "w", "not", "in", "self", ".", "stopwords", "]", ")", "\n", "summary", "=", "' '", ".", "join", "(", "[", "w", "for", "w", "in", "word_tokenize", "(", "summary", ")", "if", "w", "not", "in", "self", ".", "stopwords", "]", ")", "\n", "\n", "", "rouges", "=", "self", ".", "rouge_scorer", ".", "score", "(", "source", ",", "summary", ")", "\n", "\n", "return", "rouges", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.batch_update_avg_rouge": [[139, 208], ["eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "enumerate", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.get_rouge_defaultdict", "eval_utils.EvalMetrics.items", "eval_utils.EvalMetrics.calc_rouges", "eval_utils.EvalMetrics.items", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "utils.update_moving_avg", "getattr", "[].append", "numpy.mean", "numpy.min", "numpy.max", "numpy.std", "[].append", "[].append", "[].append", "[].append"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_rouge_defaultdict", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.calc_rouges", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.update_moving_avg"], ["", "def", "batch_update_avg_rouge", "(", "self", ",", "summaries", ",", "source_docs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            summaries: list of strs\n            source_docs: list of lists of strs\n        Returns: 4 (avg, min, max, std) rouge dicts for this batch\n        \"\"\"", "\n", "# Store average of the four statistics for this batch", "\n", "batch_avg_avg_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "batch_avg_min_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "batch_avg_max_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "batch_avg_std_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "\n", "for", "i", ",", "summary", "in", "enumerate", "(", "summaries", ")", ":", "\n", "            ", "docs", "=", "source_docs", "[", "i", "]", "\n", "\n", "# Compute rouges between summary and each document", "\n", "rouges", "=", "self", ".", "get_rouge_defaultdict", "(", "list", ")", "\n", "for", "doc", "in", "docs", ":", "\n", "                ", "scores", "=", "self", ".", "calc_rouges", "(", "doc", ",", "summary", ")", "\n", "for", "rouge_name", ",", "rouge_obj", "in", "scores", ".", "items", "(", ")", ":", "# rouge_name = rouge1, rouge2, rougeL", "\n", "                    ", "for", "metric", "in", "[", "'precision'", ",", "'recall'", ",", "'fmeasure'", "]", ":", "\n", "                        ", "score", "=", "getattr", "(", "rouge_obj", ",", "metric", ")", "\n", "rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", ".", "append", "(", "score", ")", "# [0] for first letter", "\n", "\n", "# Compute statistics and update batch and global averages", "\n", "", "", "", "avg_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "min_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "max_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "std_rouges", "=", "self", ".", "get_rouge_defaultdict", "(", ")", "\n", "self", ".", "_updates", "+=", "1", "# global count", "\n", "for", "rouge_name", ",", "rouge_obj", "in", "rouges", ".", "items", "(", ")", ":", "\n", "                ", "for", "metric", "in", "[", "'precision'", ",", "'recall'", ",", "'fmeasure'", "]", ":", "\n", "                    ", "scores", "=", "rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "\n", "avg", ",", "min", ",", "max", ",", "std", "=", "np", ".", "mean", "(", "scores", ")", ",", "np", ".", "min", "(", "scores", ")", ",", "np", ".", "max", "(", "scores", ")", ",", "np", ".", "std", "(", "scores", ")", "\n", "avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "avg", "\n", "min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "min", "\n", "max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "max", "\n", "std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "std", "\n", "\n", "# update batch averages", "\n", "cur_avg_avg", "=", "batch_avg_avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_min", "=", "batch_avg_min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_max", "=", "batch_avg_max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_std", "=", "batch_avg_std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "batch_avg_avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_avg", ",", "avg", ",", "i", "+", "1", ")", "\n", "batch_avg_min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_min", ",", "min", ",", "i", "+", "1", ")", "\n", "batch_avg_max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_max", ",", "max", ",", "i", "+", "1", ")", "\n", "batch_avg_std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_std", ",", "std", ",", "i", "+", "1", ")", "\n", "\n", "# update global averages", "\n", "cur_avg_avg", "=", "self", ".", "avg_avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_min", "=", "self", ".", "avg_min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_max", "=", "self", ".", "avg_max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "cur_avg_std", "=", "self", ".", "avg_std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "\n", "self", ".", "avg_avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_avg", ",", "avg", ",", "self", ".", "_updates", ")", "\n", "self", ".", "avg_min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_min", ",", "min", ",", "self", ".", "_updates", ")", "\n", "self", ".", "avg_max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_max", ",", "max", ",", "self", ".", "_updates", ")", "\n", "self", ".", "avg_std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", "=", "update_moving_avg", "(", "cur_avg_std", ",", "std", ",", "self", ".", "_updates", ")", "\n", "\n", "# Add to dictionary storing all stats", "\n", "if", "self", ".", "store_all", ":", "\n", "                        ", "self", ".", "avg_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", ".", "append", "(", "avg", ")", "\n", "self", ".", "min_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", ".", "append", "(", "min", ")", "\n", "self", ".", "max_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", ".", "append", "(", "max", ")", "\n", "self", ".", "std_rouges", "[", "rouge_name", "]", "[", "metric", "[", "0", "]", "]", ".", "append", "(", "std", ")", "\n", "\n", "", "", "", "", "return", "batch_avg_avg_rouges", ",", "batch_avg_min_rouges", ",", "batch_avg_max_rouges", ",", "batch_avg_std_rouges", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_str": [[214, 229], ["sorted", "rouge_dict.items", "sorted", "d.items", "strs.append"], "methods", ["None"], ["", "def", "to_str", "(", "self", ",", "rouge_dict", ")", ":", "\n", "        ", "\"\"\"\n        Convert dict of dicts of rouge scores to a readable string\n\n        Example output:\n        rouge1-f=0.1576, rouge1-p=0.1143, rouge1-r=0.1925, \\\n        rouge2-f=0.0000, rouge2-p=0.0000, rouge2-r=0.0000, \\\n        rougeL-f=0.0950, rougeL-p=0.0714, rougeL-r=0.1021\n        \"\"\"", "\n", "strs", "=", "[", "]", "\n", "for", "rouge_name", ",", "d", "in", "sorted", "(", "rouge_dict", ".", "items", "(", ")", ")", ":", "\n", "            ", "for", "metric", ",", "score", "in", "sorted", "(", "d", ".", "items", "(", ")", ")", ":", "\n", "                ", "strs", ".", "append", "(", "'{}-{}={:.4f}'", ".", "format", "(", "rouge_name", ",", "metric", ",", "score", ")", ")", "\n", "", "", "str", "=", "', '", ".", "join", "(", "strs", ")", "\n", "return", "str", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.plot_rouge_distributions": [[230, 246], ["eval_utils.EvalMetrics.get_list_stats_dicts().items", "matplotlib.subplots", "matplotlib.subplots", "enumerate", "fig.suptitle", "eval_utils.EvalMetrics.get_list_stats_dicts", "sorted", "enumerate", "matplotlib.show", "matplotlib.show", "matplotlib.savefig", "matplotlib.savefig", "rouge_dict.items", "sorted", "axarr[].hist", "axarr[].set_title", "out_fp.format", "d.items"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.get_list_stats_dicts"], ["", "def", "plot_rouge_distributions", "(", "self", ",", "show", "=", "False", ",", "out_fp", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Create a 3 (rouge1, rouge2, rougeL) by 3 (f, p, r) plot out of\n        dict of dicts of lists of rouge scores\n        \"\"\"", "\n", "for", "stat", ",", "rouge_dict", "in", "self", ".", "get_list_stats_dicts", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "fig", ",", "axarr", "=", "plt", ".", "subplots", "(", "3", ",", "3", ")", "\n", "for", "i", ",", "(", "rouge_name", ",", "d", ")", "in", "enumerate", "(", "sorted", "(", "rouge_dict", ".", "items", "(", ")", ")", ")", ":", "\n", "                ", "for", "j", ",", "(", "metric", ",", "scores", ")", "in", "enumerate", "(", "sorted", "(", "d", ".", "items", "(", ")", ")", ")", ":", "\n", "                    ", "axarr", "[", "i", ",", "j", "]", ".", "hist", "(", "scores", ",", "alpha", "=", "0.8", ",", "bins", "=", "25", ")", "\n", "axarr", "[", "i", ",", "j", "]", ".", "set_title", "(", "'{}_{}'", ".", "format", "(", "rouge_name", ",", "metric", ")", ")", "\n", "", "", "fig", ".", "suptitle", "(", "'Distribution of {} ROUGE'", ".", "format", "(", "stat", ")", ")", "\n", "if", "show", ":", "\n", "                ", "plt", ".", "show", "(", ")", "\n", "", "if", "out_fp", ":", "\n", "                ", "plt", ".", "savefig", "(", "out_fp", ".", "format", "(", "stat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.eval_utils.EvalMetrics.to_csv": [[247, 262], ["open", "f.write", "sorted", "rouge_dict.items", "f.write"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write", "home.repos.pwc.inspect_result.sosuperic_MeanSum.None.utils.FlushFile.write"], ["", "", "", "def", "to_csv", "(", "self", ",", "rouge_dict", ",", "out_fp", ")", ":", "\n", "        ", "\"\"\"\n        rouge: dict of dicts\n        out_fp: str\n\n        Output:\n            Rouge,  F, precision, recall\n            rouge1\n            rouge2\n            rougeL\n        \"\"\"", "\n", "with", "open", "(", "out_fp", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "'Rouge,F,Precision,Recall\\n'", ")", "\n", "for", "rouge_name", ",", "scores", "in", "sorted", "(", "rouge_dict", ".", "items", "(", ")", ")", ":", "\n", "                ", "f", ".", "write", "(", "'{},{},{},{}\\n'", ".", "format", "(", "rouge_name", ",", "scores", "[", "'f'", "]", ",", "scores", "[", "'p'", "]", ",", "scores", "[", "'r'", "]", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.__init__": [[51, 66], ["nltk.stem.porter.PorterStemmer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "rouge_types", ",", "use_stemmer", "=", "False", ")", ":", "\n", "    ", "\"\"\"Initializes a new RougeScorer.\n    Valid rouge types that can be computed are:\n      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n      rougeL: Longest common subsequence based scoring.\n    Args:\n      rouge_types: A list of rouge types to calculate.\n      use_stemmer: Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching.\n    Returns:\n      A dict mapping rouge types to Score tuples.\n    \"\"\"", "\n", "\n", "self", ".", "rouge_types", "=", "rouge_types", "\n", "self", ".", "_stemmer", "=", "porter", ".", "PorterStemmer", "(", ")", "if", "use_stemmer", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer.RougeScorer.score": [[67, 99], ["evaluation.tokenize.tokenize", "evaluation.tokenize.tokenize", "rouge_scorer._score_lcs", "re.match", "int", "rouge_scorer._create_ngrams", "rouge_scorer._create_ngrams", "rouge_scorer._score_ngrams", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.tokenize.tokenize", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.tokenize.tokenize", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._score_lcs", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._create_ngrams", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._create_ngrams", "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._score_ngrams"], ["", "def", "score", "(", "self", ",", "target", ",", "prediction", ")", ":", "\n", "    ", "\"\"\"Calculates rouge scores between the target and prediction.\n    Args:\n      target: Text containing the target (ground truth) text.\n      prediction: Text containing the predicted text.\n    Returns:\n      A dict mapping each rouge type to a Score object.\n    Raises:\n      ValueError: If an invalid rouge type is encountered.\n    \"\"\"", "\n", "\n", "target_tokens", "=", "tokenize", ".", "tokenize", "(", "target", ",", "self", ".", "_stemmer", ")", "\n", "prediction_tokens", "=", "tokenize", ".", "tokenize", "(", "prediction", ",", "self", ".", "_stemmer", ")", "\n", "result", "=", "{", "}", "\n", "\n", "for", "rouge_type", "in", "self", ".", "rouge_types", ":", "\n", "      ", "if", "rouge_type", "==", "\"rougeL\"", ":", "\n", "# Rouge from longest common subsequences.", "\n", "        ", "scores", "=", "_score_lcs", "(", "target_tokens", ",", "prediction_tokens", ")", "\n", "", "elif", "re", ".", "match", "(", "r\"rouge[0-9]$\"", ",", "rouge_type", ")", ":", "\n", "# Rouge from n-grams.", "\n", "        ", "n", "=", "int", "(", "rouge_type", "[", "5", ":", "]", ")", "\n", "if", "n", "<=", "0", ":", "\n", "          ", "raise", "ValueError", "(", "\"rougen requires positive n: %s\"", "%", "rouge_type", ")", "\n", "", "target_ngrams", "=", "_create_ngrams", "(", "target_tokens", ",", "n", ")", "\n", "prediction_ngrams", "=", "_create_ngrams", "(", "prediction_tokens", ",", "n", ")", "\n", "scores", "=", "_score_ngrams", "(", "target_ngrams", ",", "prediction_ngrams", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid rouge type: %s\"", "%", "rouge_type", ")", "\n", "", "result", "[", "rouge_type", "]", "=", "scores", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._create_ngrams": [[101, 114], ["collections.Counter", "tuple", "six.moves.xrange", "len"], "function", ["None"], ["", "", "def", "_create_ngrams", "(", "tokens", ",", "n", ")", ":", "\n", "  ", "\"\"\"Creates ngrams from the given list of tokens.\n  Args:\n    tokens: A list of tokens from which ngrams are created.\n    n: Number of tokens to use, e.g. 2 for bigrams.\n  Returns:\n    A dictionary mapping each bigram to the number of occurrences.\n  \"\"\"", "\n", "\n", "ngrams", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "ngram", "in", "(", "tuple", "(", "tokens", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "xrange", "(", "len", "(", "tokens", ")", "-", "n", "+", "1", ")", ")", ":", "\n", "    ", "ngrams", "[", "ngram", "]", "+=", "1", "\n", "", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._score_lcs": [[116, 145], ["numpy.zeros", "six.moves.xrange", "evaluation.scoring.fmeasure", "evaluation.scoring.Score", "evaluation.scoring.Score", "len", "len", "six.moves.xrange", "len", "len", "max"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.fmeasure"], ["", "def", "_score_lcs", "(", "target_tokens", ",", "prediction_tokens", ")", ":", "\n", "  ", "\"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n  Args:\n    target_tokens: Tokens from the target text.\n    prediction_tokens: Tokens from the predicted text.\n  Returns:\n    A Score object containing computed scores.\n  \"\"\"", "\n", "\n", "if", "not", "target_tokens", "or", "not", "prediction_tokens", ":", "\n", "    ", "return", "scoring", ".", "Score", "(", "precision", "=", "0", ",", "recall", "=", "0", ",", "fmeasure", "=", "0", ")", "\n", "\n", "# Compute length of LCS from the bottom up in a table (DP appproach).", "\n", "", "cols", "=", "len", "(", "prediction_tokens", ")", "+", "1", "\n", "rows", "=", "len", "(", "target_tokens", ")", "+", "1", "\n", "lcs_table", "=", "np", ".", "zeros", "(", "(", "rows", ",", "cols", ")", ")", "\n", "for", "i", "in", "xrange", "(", "1", ",", "rows", ")", ":", "\n", "    ", "for", "j", "in", "xrange", "(", "1", ",", "cols", ")", ":", "\n", "      ", "if", "target_tokens", "[", "i", "-", "1", "]", "==", "prediction_tokens", "[", "j", "-", "1", "]", ":", "\n", "        ", "lcs_table", "[", "i", ",", "j", "]", "=", "lcs_table", "[", "i", "-", "1", ",", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "        ", "lcs_table", "[", "i", ",", "j", "]", "=", "max", "(", "lcs_table", "[", "i", "-", "1", ",", "j", "]", ",", "lcs_table", "[", "i", ",", "j", "-", "1", "]", ")", "\n", "", "", "", "lcs_length", "=", "lcs_table", "[", "-", "1", ",", "-", "1", "]", "\n", "\n", "precision", "=", "lcs_length", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "lcs_length", "/", "len", "(", "target_tokens", ")", "\n", "fmeasure", "=", "scoring", ".", "fmeasure", "(", "precision", ",", "recall", ")", "\n", "\n", "return", "scoring", ".", "Score", "(", "precision", "=", "precision", ",", "recall", "=", "recall", ",", "fmeasure", "=", "fmeasure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.rouge_scorer._score_ngrams": [[147, 170], ["six.iterkeys", "sum", "sum", "evaluation.scoring.fmeasure", "evaluation.scoring.Score", "min", "target_ngrams.values", "prediction_ngrams.values", "max", "max"], "function", ["home.repos.pwc.inspect_result.sosuperic_MeanSum.evaluation.scoring.fmeasure"], ["", "def", "_score_ngrams", "(", "target_ngrams", ",", "prediction_ngrams", ")", ":", "\n", "  ", "\"\"\"Compute n-gram based rouge scores.\n  Args:\n    target_ngrams: A Counter object mapping each ngram to number of\n      occurrences for the target text.\n    prediction_ngrams: A Counter object mapping each ngram to number of\n      occurrences for the prediction text.\n  Returns:\n    A Score object containing computed scores.\n  \"\"\"", "\n", "\n", "intersection_ngrams_count", "=", "0", "\n", "for", "ngram", "in", "six", ".", "iterkeys", "(", "target_ngrams", ")", ":", "\n", "    ", "intersection_ngrams_count", "+=", "min", "(", "target_ngrams", "[", "ngram", "]", ",", "\n", "prediction_ngrams", "[", "ngram", "]", ")", "\n", "", "target_ngrams_count", "=", "sum", "(", "target_ngrams", ".", "values", "(", ")", ")", "\n", "prediction_ngrams_count", "=", "sum", "(", "prediction_ngrams", ".", "values", "(", ")", ")", "\n", "\n", "precision", "=", "intersection_ngrams_count", "/", "max", "(", "prediction_ngrams_count", ",", "1", ")", "\n", "recall", "=", "intersection_ngrams_count", "/", "max", "(", "target_ngrams_count", ",", "1", ")", "\n", "fmeasure", "=", "scoring", ".", "fmeasure", "(", "precision", ",", "recall", ")", "\n", "\n", "return", "scoring", ".", "Score", "(", "precision", "=", "precision", ",", "recall", "=", "recall", ",", "fmeasure", "=", "fmeasure", ")", "", "", ""]]}