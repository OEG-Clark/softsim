{"home.repos.pwc.inspect_result.thu-coai_LAUG.LAUG.__init__.get_root_path": [[10, 12], ["os.path.dirname", "os.path.dirname", "os.path.abspath"], "function", ["None"], ["def", "get_root_path", "(", ")", ":", "\n", "    ", "return", "dirname", "(", "dirname", "(", "abspath", "(", "__file__", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.train_util.init_logging_handler": [[9, 19], ["time.strftime", "logging.StreamHandler", "logging.FileHandler", "logging.basicConfig", "logging.getLogger", "logging.getLogger.setLevel", "os.path.exists", "os.makedirs", "time.localtime"], "function", ["None"], ["def", "init_logging_handler", "(", "log_dir", ",", "extra", "=", "''", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "log_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "log_dir", ")", "\n", "", "current_time", "=", "time", ".", "strftime", "(", "\"%Y-%m-%d-%H-%M-%S\"", ",", "time", ".", "localtime", "(", ")", ")", "\n", "\n", "stderr_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "'{}/log_{}.txt'", ".", "format", "(", "log_dir", ",", "current_time", "+", "extra", ")", ")", "\n", "logging", ".", "basicConfig", "(", "handlers", "=", "[", "stderr_handler", ",", "file_handler", "]", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.train_util.to_device": [[21, 29], ["type", "data.items", "enumerate", "v.to", "item.to"], "function", ["None"], ["", "def", "to_device", "(", "data", ")", ":", "\n", "    ", "if", "type", "(", "data", ")", "==", "dict", ":", "\n", "        ", "for", "k", ",", "v", "in", "data", ".", "items", "(", ")", ":", "\n", "            ", "data", "[", "k", "]", "=", "v", ".", "to", "(", "device", "=", "DEVICE", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "idx", ",", "item", "in", "enumerate", "(", "data", ")", ":", "\n", "            ", "data", "[", "idx", "]", "=", "item", ".", "to", "(", "device", "=", "DEVICE", ")", "\n", "", "", "return", "data", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.module.Module.train": [[7, 10], ["None"], "methods", ["None"], ["    ", "def", "train", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Model training entry point\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.module.Module.test": [[11, 14], ["None"], "methods", ["None"], ["", "def", "test", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Model testing entry point\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.module.Module.from_cache": [[15, 18], ["None"], "methods", ["None"], ["", "def", "from_cache", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"restore internal state for multi-turn dialog\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.module.Module.to_cache": [[19, 22], ["None"], "methods", ["None"], ["", "def", "to_cache", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"save internal state for multi-turn dialog\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.module.Module.init_session": [[23, 26], ["None"], "methods", ["None"], ["", "def", "init_session", "(", "self", ")", ":", "\n", "        ", "\"\"\"Init the class variables for a new session.\"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.set_default_mininterval": [[42, 45], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "set_default_mininterval", "(", "value", ":", "float", ")", "->", "None", ":", "\n", "        ", "Tqdm", ".", "default_mininterval", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.set_slower_interval": [[46, 58], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "set_slower_interval", "(", "use_slower_interval", ":", "bool", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        If ``use_slower_interval`` is ``True``, we will dramatically slow down ``tqdm's`` default\n        output rate.  ``tqdm's`` default output rate is great for interactively watching progress,\n        but it is not great for log files.  You might want to set this if you are primarily going\n        to be looking at output through log files, not the terminal.\n        \"\"\"", "\n", "if", "use_slower_interval", ":", "\n", "            ", "Tqdm", ".", "default_mininterval", "=", "10.0", "\n", "", "else", ":", "\n", "            ", "Tqdm", ".", "default_mininterval", "=", "0.1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm": [[59, 67], ["tqdm.tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm"], ["", "", "@", "staticmethod", "\n", "def", "tqdm", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "new_kwargs", "=", "{", "\n", "'mininterval'", ":", "Tqdm", ".", "default_mininterval", ",", "\n", "**", "kwargs", "\n", "}", "\n", "\n", "return", "_tqdm", "(", "*", "args", ",", "**", "new_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.url_to_filename": [[86, 102], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["", "def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.filename_to_url": [[104, 126], ["os.path.join", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.cached_path": [[127, 154], ["isinstance", "os.path.expanduser", "urllib.parse.urlparse", "str", "allennlp_file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "\n", "", "url_or_filename", "=", "os", ".", "path", ".", "expanduser", "(", "url_or_filename", ")", "\n", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.is_url_or_existing_file": [[155, 165], ["os.path.expanduser", "urllib.parse.urlparse", "str", "os.path.exists"], "function", ["None"], ["", "", "def", "is_url_or_existing_file", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", ",", "None", "]", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine check if it's url or an existing file path.\n    \"\"\"", "\n", "if", "url_or_filename", "is", "None", ":", "\n", "        ", "return", "False", "\n", "", "url_or_filename", "=", "os", ".", "path", ".", "expanduser", "(", "str", "(", "url_or_filename", ")", ")", "\n", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "return", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", "or", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.split_s3_path": [[166, 177], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.s3_request": [[179, 196], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_s3_resource": [[198, 206], ["boto3.session.Session", "boto3.session.Session.get_credentials", "boto3.session.Session.resource", "boto3.session.Session.resource", "botocore.client.Config"], "function", ["None"], ["", "def", "get_s3_resource", "(", ")", ":", "\n", "    ", "session", "=", "boto3", ".", "session", ".", "Session", "(", ")", "\n", "if", "session", ".", "get_credentials", "(", ")", "is", "None", ":", "\n", "# Use unsigned requests.", "\n", "        ", "s3_resource", "=", "session", ".", "resource", "(", "\"s3\"", ",", "config", "=", "botocore", ".", "client", ".", "Config", "(", "signature_version", "=", "botocore", ".", "UNSIGNED", ")", ")", "\n", "", "else", ":", "\n", "        ", "s3_resource", "=", "session", ".", "resource", "(", "\"s3\"", ")", "\n", "", "return", "s3_resource", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.s3_etag": [[208, 215], ["allennlp_file_utils.get_s3_resource", "allennlp_file_utils.split_s3_path", "get_s3_resource.Object"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_s3_resource", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "get_s3_resource", "(", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.s3_get": [[217, 223], ["allennlp_file_utils.get_s3_resource", "allennlp_file_utils.split_s3_path", "get_s3_resource.Bucket().download_fileobj", "get_s3_resource.Bucket"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_s3_resource", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "get_s3_resource", "(", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.session_with_backoff": [[224, 239], ["requests.Session", "requests.packages.urllib3.util.retry.Retry", "requests.Session.mount", "requests.Session.mount", "requests.adapters.HTTPAdapter", "requests.adapters.HTTPAdapter"], "function", ["None"], ["", "def", "session_with_backoff", "(", ")", "->", "requests", ".", "Session", ":", "\n", "    ", "\"\"\"\n    We ran into an issue where http requests to s3 were timing out,\n    possibly because we were making too many requests too quickly.\n    This helper function returns a requests session that has retry-with-backoff\n    built in.\n\n    see https://stackoverflow.com/questions/23267409/how-to-implement-retry-mechanism-into-python-requests-library\n    \"\"\"", "\n", "session", "=", "requests", ".", "Session", "(", ")", "\n", "retries", "=", "Retry", "(", "total", "=", "5", ",", "backoff_factor", "=", "1", ",", "status_forcelist", "=", "[", "502", ",", "503", ",", "504", "]", ")", "\n", "session", ".", "mount", "(", "'http://'", ",", "HTTPAdapter", "(", "max_retries", "=", "retries", ")", ")", "\n", "session", ".", "mount", "(", "'https://'", ",", "HTTPAdapter", "(", "max_retries", "=", "retries", ")", ")", "\n", "\n", "return", "session", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.http_get": [[240, 251], ["allennlp_file_utils.session_with_backoff", "session.get", "session.get.headers.get", "allennlp_file_utils.Tqdm.tqdm", "session.get.iter_content", "Tqdm.tqdm.close", "int", "Tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.session_with_backoff", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "with", "session_with_backoff", "(", ")", "as", "session", ":", "\n", "        ", "req", "=", "session", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "Tqdm", ".", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "            ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "                ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_from_cache": [[254, 310], ["os.makedirs", "url.startswith", "allennlp_file_utils.url_to_filename", "os.path.join", "allennlp_file_utils.s3_etag", "session.head.headers.get", "os.path.exists", "allennlp_file_utils.session_with_backoff", "session.head", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "allennlp_file_utils.s3_get", "allennlp_file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.url_to_filename", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.s3_etag", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.session_with_backoff", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.s3_get", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.http_get"], ["", "", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "with", "session_with_backoff", "(", ")", "as", "session", ":", "\n", "            ", "response", "=", "session", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.read_set_from_file": [[312, 322], ["set", "open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \"\"\"", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.get_file_extension": [[324, 328], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path": [[8, 14], ["print", "LAUG.util.allennlp_file_utils.cached_path", "str", "pathlib.Path", "pathlib.Path.home"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["def", "cached_path", "(", "file_path", ",", "cached_dir", "=", "None", ")", ":", "\n", "    ", "print", "(", "'Load from'", ",", "file_path", ")", "\n", "if", "not", "cached_dir", ":", "\n", "        ", "cached_dir", "=", "str", "(", "Path", "(", "Path", ".", "home", "(", ")", "/", "'.LAUG'", ")", "/", "\"cache\"", ")", "\n", "\n", "", "return", "allennlp_cached_path", "(", "file_path", ",", "cached_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.read_zipped_json": [[16, 19], ["zipfile.ZipFile", "json.load", "zipfile.ZipFile.open"], "function", ["None"], ["", "def", "read_zipped_json", "(", "zip_path", ",", "filepath", ")", ":", "\n", "    ", "archive", "=", "zipfile", ".", "ZipFile", "(", "zip_path", ",", "'r'", ")", "\n", "return", "json", ".", "load", "(", "archive", ".", "open", "(", "filepath", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.dump_json": [[21, 23], ["json.dump", "open"], "function", ["None"], ["", "def", "dump_json", "(", "content", ",", "filepath", ")", ":", "\n", "    ", "json", ".", "dump", "(", "content", ",", "open", "(", "filepath", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", ",", "indent", "=", "2", ",", "ensure_ascii", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.write_zipped_json": [[25, 28], ["zipfile.ZipFile", "zf.write"], "function", ["None"], ["", "def", "write_zipped_json", "(", "zip_path", ",", "filepath", ")", ":", "\n", "    ", "with", "zipfile", ".", "ZipFile", "(", "zip_path", ",", "'w'", ",", "zipfile", ".", "ZIP_DEFLATED", ")", "as", "zf", ":", "\n", "        ", "zf", ".", "write", "(", "filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.get_root_path": [[30, 32], ["os.path.abspath", "os.path.join", "os.path.abspath"], "function", ["None"], ["", "", "def", "get_root_path", "(", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ",", "'../../..'", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.ModuleDataloader.__init__": [[7, 9], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset_dataloader", ":", "DatasetDataloader", ")", ":", "\n", "        ", "self", ".", "dataset_dataloader", "=", "dataset_dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.ModuleDataloader.load_data": [[10, 13], ["module_dataloader.ModuleDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["", "@", "abstractmethod", "\n", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.SingleTurnNLUDataloader.load_data": [[16, 20], ["kwargs.setdefault", "kwargs.setdefault", "module_dataloader.SingleTurnNLUDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.MultiTurnNLUDataloader.load_data": [[23, 28], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.MultiTurnNLUDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.AgentDSTDataloader.load_data": [[31, 40], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.AgentDSTDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "100", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'belief_state'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_opponent_utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_self_utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'ontology'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.CrossWOZAgentDSTDataloader.load_data": [[42, 50], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.CrossWOZAgentDSTDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "100", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'sys_state_init'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_opponent_utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_self_utterance'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.UserDSTDataloader.load_data": [[53, 60], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.UserDSTDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'belief_state'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_opponent_utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'last_self_utterance'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.ActPolicyDataloader.load_data": [[63, 70], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.ActPolicyDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'belief_state'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'terminated'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "2", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.ActUserPolicyDataloader.load_data": [[73, 80], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.ActUserPolicyDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'goal'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'terminated'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "2", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.WordPolicyDataloader.load_data": [[83, 89], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.WordPolicyDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'belief_state'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "3", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.SingleTurnNLGDataloader.load_data": [[92, 96], ["kwargs.setdefault", "kwargs.setdefault", "module_dataloader.SingleTurnNLGDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.module_dataloader.MultiTurnNLGDataloader.load_data": [[99, 105], ["kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "kwargs.setdefault", "module_dataloader.MultiTurnNLGDataloader.dataset_dataloader.load_data"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data"], ["    ", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", ".", "setdefault", "(", "'utterance'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'dialog_act'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context'", ",", "True", ")", "\n", "kwargs", ".", "setdefault", "(", "'context_window_size'", ",", "3", ")", "\n", "return", "self", ".", "dataset_dataloader", ".", "load_data", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.DatasetDataloader.__init__": [[14, 16], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "data", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.DatasetDataloader.load_data": [[17, 26], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "load_data", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        load data from file, according to what is need\n        :param args:\n        :param kwargs:\n        :return: data\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.MultiWOZDataloader.__init__": [[30, 33], ["dataset_dataloader.DatasetDataloader.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "zh", "=", "False", ")", ":", "\n", "        ", "super", "(", "MultiWOZDataloader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "zh", "=", "zh", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.MultiWOZDataloader.load_data": [[34, 122], ["list", "os.path.join", "dialog_act.items", "filter", "LAUG.util.file_util.read_zipped_json", "print", "LAUG.util.file_util.read_zipped_json.items", "os.path.join", "json.load", "sorted", "os.path.join", "enumerate", "open", "domain_intent.split", "tuples.append", "len", "dataset_dataloader.MultiWOZDataloader.load_data.da2tuples"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json"], ["", "def", "load_data", "(", "self", ",", "\n", "data_dir", "=", "None", ",", "\n", "data_key", "=", "'all'", ",", "\n", "role", "=", "'all'", ",", "\n", "utterance", "=", "False", ",", "\n", "dialog_act", "=", "False", ",", "\n", "context", "=", "False", ",", "\n", "context_window_size", "=", "0", ",", "\n", "context_dialog_act", "=", "False", ",", "\n", "belief_state", "=", "False", ",", "\n", "last_opponent_utterance", "=", "False", ",", "\n", "last_self_utterance", "=", "False", ",", "\n", "ontology", "=", "False", ",", "\n", "session_id", "=", "False", ",", "\n", "span_info", "=", "False", ",", "\n", "terminated", "=", "False", ",", "\n", "goal", "=", "False", "\n", ")", ":", "\n", "        ", "if", "data_dir", "is", "None", ":", "\n", "            ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "DATA_ROOT", ",", "'multiwoz'", "+", "(", "'_zh'", "if", "self", ".", "zh", "else", "''", ")", ")", "\n", "\n", "", "def", "da2tuples", "(", "dialog_act", ")", ":", "\n", "            ", "tuples", "=", "[", "]", "\n", "for", "domain_intent", ",", "svs", "in", "dialog_act", ".", "items", "(", ")", ":", "\n", "                ", "for", "slot", ",", "value", "in", "sorted", "(", "svs", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "                    ", "domain", ",", "intent", "=", "domain_intent", ".", "split", "(", "'-'", ")", "\n", "tuples", ".", "append", "(", "[", "intent", ",", "domain", ",", "slot", ",", "value", "]", ")", "\n", "", "", "return", "tuples", "\n", "\n", "", "assert", "role", "in", "[", "'sys'", ",", "'usr'", ",", "'all'", "]", "\n", "info_list", "=", "list", "(", "filter", "(", "eval", ",", "[", "'utterance'", ",", "'dialog_act'", ",", "'context'", ",", "'context_dialog_act'", ",", "'belief_state'", ",", "\n", "'last_opponent_utterance'", ",", "'last_self_utterance'", ",", "'session_id'", ",", "'span_info'", ",", "\n", "'terminated'", ",", "'goal'", "]", ")", ")", "\n", "self", ".", "data", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", ",", "'role'", ":", "role", ",", "'human_val'", ":", "{", "}", "}", "\n", "if", "data_key", "==", "'all'", ":", "\n", "            ", "data_key_list", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "", "else", ":", "\n", "            ", "data_key_list", "=", "[", "data_key", "]", "\n", "", "for", "data_key", "in", "data_key_list", ":", "\n", "            ", "data", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'{}.json.zip'", ".", "format", "(", "data_key", ")", ")", ",", "'{}.json'", ".", "format", "(", "data_key", ")", ")", "\n", "print", "(", "'loaded {}, size {}'", ".", "format", "(", "data_key", ",", "len", "(", "data", ")", ")", ")", "\n", "for", "x", "in", "info_list", ":", "\n", "                ", "self", ".", "data", "[", "data_key", "]", "[", "x", "]", "=", "[", "]", "\n", "", "for", "sess_id", ",", "sess", "in", "data", ".", "items", "(", ")", ":", "\n", "                ", "cur_context", "=", "[", "]", "\n", "cur_context_dialog_act", "=", "[", "]", "\n", "for", "i", ",", "turn", "in", "enumerate", "(", "sess", "[", "'log'", "]", ")", ":", "\n", "                    ", "text", "=", "turn", "[", "'text'", "]", "\n", "da", "=", "da2tuples", "(", "turn", "[", "'dialog_act'", "]", ")", "\n", "if", "role", "==", "'sys'", "and", "i", "%", "2", "==", "0", ":", "\n", "                        ", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "continue", "\n", "", "elif", "role", "==", "'usr'", "and", "i", "%", "2", "==", "1", ":", "\n", "                        ", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "continue", "\n", "", "if", "utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'utterance'", "]", ".", "append", "(", "text", ")", "\n", "", "if", "dialog_act", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'dialog_act'", "]", ".", "append", "(", "da", ")", "\n", "", "if", "context", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'context'", "]", ".", "append", "(", "cur_context", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "context_dialog_act", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'context_dialog_act'", "]", ".", "append", "(", "cur_context_dialog_act", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "belief_state", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'belief_state'", "]", ".", "append", "(", "turn", "[", "'metadata'", "]", ")", "\n", "", "if", "last_opponent_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_opponent_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "1", "]", "if", "len", "(", "cur_context", ")", ">=", "1", "else", "''", ")", "\n", "", "if", "last_self_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_self_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "2", "]", "if", "len", "(", "cur_context", ")", ">=", "2", "else", "''", ")", "\n", "", "if", "session_id", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'session_id'", "]", ".", "append", "(", "sess_id", ")", "\n", "", "if", "span_info", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'span_info'", "]", ".", "append", "(", "turn", "[", "'span_info'", "]", ")", "\n", "", "if", "terminated", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'terminated'", "]", ".", "append", "(", "i", "+", "2", ">=", "len", "(", "sess", "[", "'log'", "]", ")", ")", "\n", "", "if", "goal", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'goal'", "]", ".", "append", "(", "sess", "[", "'goal'", "]", ")", "\n", "", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "", "", "", "if", "ontology", ":", "\n", "            ", "ontology_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ontology.json'", ")", "\n", "self", ".", "data", "[", "'ontology'", "]", "=", "json", ".", "load", "(", "open", "(", "ontology_path", ")", ")", "\n", "\n", "", "return", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.CamrestDataloader.__init__": [[125, 127], ["dataset_dataloader.DatasetDataloader.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "CamrestDataloader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.CamrestDataloader.load_data": [[128, 205], ["os.path.abspath", "list", "os.path.join", "dialog_act.items", "filter", "LAUG.util.file_util.read_zipped_json", "print", "os.path.abspath", "sorted", "os.path.join", "tuples.append", "len", "dataset_dataloader.CamrestDataloader.load_data.da2tuples"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json"], ["", "def", "load_data", "(", "self", ",", "\n", "data_dir", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ",", "'../../../../data/camrest'", ")", ")", ",", "\n", "data_key", "=", "'all'", ",", "\n", "role", "=", "'all'", ",", "\n", "utterance", "=", "False", ",", "\n", "dialog_act", "=", "False", ",", "\n", "context", "=", "False", ",", "\n", "context_window_size", "=", "0", ",", "\n", "context_dialog_act", "=", "False", ",", "\n", "last_opponent_utterance", "=", "False", ",", "\n", "last_self_utterance", "=", "False", ",", "\n", "session_id", "=", "False", ",", "\n", "terminated", "=", "False", ",", "\n", "goal", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "def", "da2tuples", "(", "dialog_act", ")", ":", "\n", "            ", "tuples", "=", "[", "]", "\n", "for", "intent", ",", "svs", "in", "dialog_act", ".", "items", "(", ")", ":", "\n", "                ", "for", "slot", ",", "value", "in", "sorted", "(", "svs", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "                    ", "tuples", ".", "append", "(", "[", "intent", ",", "slot", ",", "value", "]", ")", "\n", "", "", "return", "tuples", "\n", "\n", "", "assert", "role", "in", "[", "'sys'", ",", "'usr'", ",", "'all'", "]", "\n", "info_list", "=", "list", "(", "filter", "(", "eval", ",", "[", "'utterance'", ",", "'dialog_act'", ",", "'context'", ",", "'context_dialog_act'", ",", "\n", "'last_opponent_utterance'", ",", "'last_self_utterance'", ",", "'session_id'", ",", "\n", "'terminated'", ",", "'goal'", "]", ")", ")", "\n", "self", ".", "data", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", ",", "'role'", ":", "role", "}", "\n", "if", "data_key", "==", "'all'", ":", "\n", "            ", "data_key_list", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "", "else", ":", "\n", "            ", "data_key_list", "=", "[", "data_key", "]", "\n", "", "for", "data_key", "in", "data_key_list", ":", "\n", "            ", "data", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'{}.json.zip'", ".", "format", "(", "data_key", ")", ")", ",", "'{}.json'", ".", "format", "(", "data_key", ")", ")", "\n", "print", "(", "'loaded {}, size {}'", ".", "format", "(", "data_key", ",", "len", "(", "data", ")", ")", ")", "\n", "for", "x", "in", "info_list", ":", "\n", "                ", "self", ".", "data", "[", "data_key", "]", "[", "x", "]", "=", "[", "]", "\n", "", "for", "sess", "in", "data", ":", "\n", "                ", "cur_context", "=", "[", "]", "\n", "cur_context_dialog_act", "=", "[", "]", "\n", "for", "turn", "in", "sess", "[", "'dial'", "]", ":", "\n", "                    ", "turn_id", "=", "turn", "[", "'turn'", "]", "\n", "for", "side_id", "in", "[", "'usr'", ",", "'sys'", "]", ":", "\n", "                        ", "if", "side_id", "==", "'usr'", ":", "\n", "                            ", "text", "=", "turn", "[", "side_id", "]", "[", "'transcript'", "]", "\n", "", "else", ":", "\n", "                            ", "text", "=", "turn", "[", "side_id", "]", "[", "'sent'", "]", "\n", "", "da", "=", "da2tuples", "(", "turn", "[", "side_id", "]", "[", "'dialog_act'", "]", ")", "\n", "if", "{", "role", ",", "side_id", "}", "==", "{", "'usr'", ",", "'sys'", "}", ":", "\n", "                            ", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "continue", "\n", "", "if", "utterance", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'utterance'", "]", ".", "append", "(", "text", ")", "\n", "", "if", "dialog_act", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'dialog_act'", "]", ".", "append", "(", "da", ")", "\n", "", "if", "context", "and", "context_window_size", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'context'", "]", ".", "append", "(", "cur_context", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "context_dialog_act", "and", "context_window_size", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'context_dialog_act'", "]", ".", "append", "(", "\n", "cur_context_dialog_act", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "last_opponent_utterance", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'last_opponent_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "1", "]", "if", "len", "(", "cur_context", ")", ">=", "1", "else", "''", ")", "\n", "", "if", "last_self_utterance", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'last_self_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "2", "]", "if", "len", "(", "cur_context", ")", ">=", "2", "else", "''", ")", "\n", "", "if", "session_id", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'session_id'", "]", ".", "append", "(", "sess", "[", "'dialogue_id'", "]", ")", "\n", "", "if", "terminated", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'terminated'", "]", ".", "append", "(", "turn_id", ">=", "len", "(", "sess", "[", "'dial'", "]", ")", ")", "\n", "", "if", "goal", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'goal'", "]", ".", "append", "(", "sess", "[", "'goal'", "]", ")", "\n", "", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "\n", "", "", "", "", "return", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.CrossWOZDataloader.__init__": [[208, 211], ["dataset_dataloader.DatasetDataloader.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "en", "=", "False", ")", ":", "\n", "        ", "super", "(", "CrossWOZDataloader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "en", "=", "en", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.CrossWOZDataloader.load_data": [[212, 303], ["list", "os.path.join", "filter", "LAUG.util.file_util.read_zipped_json", "print", "LAUG.util.file_util.read_zipped_json.items", "tuples.append", "os.path.join", "enumerate", "len", "dataset_dataloader.CrossWOZDataloader.load_data.da2tuples"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json"], ["", "def", "load_data", "(", "self", ",", "\n", "data_dir", "=", "None", ",", "\n", "data_key", "=", "'all'", ",", "\n", "role", "=", "'all'", ",", "\n", "utterance", "=", "False", ",", "\n", "dialog_act", "=", "False", ",", "\n", "context", "=", "False", ",", "\n", "context_window_size", "=", "0", ",", "\n", "context_dialog_act", "=", "False", ",", "\n", "user_state", "=", "False", ",", "\n", "sys_state", "=", "False", ",", "\n", "sys_state_init", "=", "False", ",", "\n", "last_opponent_utterance", "=", "False", ",", "\n", "last_self_utterance", "=", "False", ",", "\n", "session_id", "=", "False", ",", "\n", "terminated", "=", "False", ",", "\n", "goal", "=", "False", ",", "\n", "final_goal", "=", "False", ",", "\n", "task_description", "=", "False", "\n", ")", ":", "\n", "        ", "if", "data_dir", "is", "None", ":", "\n", "            ", "data_dir", "=", "os", ".", "path", ".", "join", "(", "DATA_ROOT", ",", "'crosswoz'", "+", "(", "'_en'", "if", "self", ".", "en", "else", "''", ")", ")", "\n", "\n", "", "def", "da2tuples", "(", "dialog_act", ")", ":", "\n", "            ", "tuples", "=", "[", "]", "\n", "for", "act", "in", "dialog_act", ":", "\n", "                ", "tuples", ".", "append", "(", "[", "act", "[", "0", "]", ",", "act", "[", "1", "]", ",", "act", "[", "2", "]", ",", "act", "[", "3", "]", "]", ")", "\n", "", "return", "tuples", "\n", "\n", "", "assert", "role", "in", "[", "'sys'", ",", "'usr'", ",", "'all'", "]", "\n", "info_list", "=", "list", "(", "filter", "(", "eval", ",", "[", "'utterance'", ",", "'dialog_act'", ",", "'context'", ",", "'context_dialog_act'", ",", "\n", "'user_state'", ",", "'sys_state'", ",", "'sys_state_init'", ",", "\n", "'last_opponent_utterance'", ",", "'last_self_utterance'", ",", "'session_id'", ",", "\n", "'terminated'", ",", "'goal'", ",", "'final_goal'", ",", "'task_description'", "]", ")", ")", "\n", "self", ".", "data", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", ",", "'role'", ":", "role", ",", "'human_val'", ":", "{", "}", "}", "\n", "if", "data_key", "==", "'all'", ":", "\n", "            ", "data_key_list", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "", "else", ":", "\n", "            ", "data_key_list", "=", "[", "data_key", "]", "\n", "", "for", "data_key", "in", "data_key_list", ":", "\n", "            ", "data", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'{}.json.zip'", ".", "format", "(", "data_key", ")", ")", ",", "'{}.json'", ".", "format", "(", "data_key", ")", ")", "\n", "print", "(", "'loaded {}, size {}'", ".", "format", "(", "data_key", ",", "len", "(", "data", ")", ")", ")", "\n", "for", "x", "in", "info_list", ":", "\n", "                ", "self", ".", "data", "[", "data_key", "]", "[", "x", "]", "=", "[", "]", "\n", "", "for", "sess_id", ",", "sess", "in", "data", ".", "items", "(", ")", ":", "\n", "                ", "cur_context", "=", "[", "]", "\n", "cur_context_dialog_act", "=", "[", "]", "\n", "for", "i", ",", "turn", "in", "enumerate", "(", "sess", "[", "'messages'", "]", ")", ":", "\n", "                    ", "text", "=", "turn", "[", "'content'", "]", "\n", "da", "=", "da2tuples", "(", "turn", "[", "'dialog_act'", "]", ")", "\n", "if", "{", "role", ",", "turn", "[", "'role'", "]", "}", "==", "{", "'usr'", ",", "'sys'", "}", ":", "\n", "                        ", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "continue", "\n", "", "if", "utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'utterance'", "]", ".", "append", "(", "text", ")", "\n", "", "if", "dialog_act", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'dialog_act'", "]", ".", "append", "(", "da", ")", "\n", "", "if", "context", "and", "context_window_size", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'context'", "]", ".", "append", "(", "cur_context", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "context_dialog_act", "and", "context_window_size", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'context_dialog_act'", "]", ".", "append", "(", "cur_context_dialog_act", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "role", "in", "[", "'usr'", ",", "'all'", "]", "and", "user_state", "and", "turn", "[", "'role'", "]", "==", "'usr'", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'user_state'", "]", ".", "append", "(", "turn", "[", "'user_state'", "]", ")", "\n", "", "if", "role", "in", "[", "'sys'", ",", "'all'", "]", "and", "sys_state", "and", "turn", "[", "'role'", "]", "==", "'sys'", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'sys_state'", "]", ".", "append", "(", "turn", "[", "'sys_state'", "]", ")", "\n", "", "if", "role", "in", "[", "'sys'", ",", "'all'", "]", "and", "sys_state_init", ":", "\n", "                        ", "if", "turn", "[", "'role'", "]", "==", "'sys'", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'sys_state_init'", "]", ".", "append", "(", "turn", "[", "'sys_state_init'", "]", ")", "\n", "", "else", ":", "\n", "                            ", "self", ".", "data", "[", "data_key", "]", "[", "'sys_state_init'", "]", ".", "append", "(", "{", "}", ")", "\n", "", "", "if", "last_opponent_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_opponent_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "1", "]", "if", "len", "(", "cur_context", ")", ">=", "1", "else", "''", ")", "\n", "", "if", "last_self_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_self_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "2", "]", "if", "len", "(", "cur_context", ")", ">=", "2", "else", "''", ")", "\n", "", "if", "session_id", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'session_id'", "]", ".", "append", "(", "sess_id", ")", "\n", "", "if", "terminated", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'terminated'", "]", ".", "append", "(", "i", "+", "2", ">=", "len", "(", "sess", "[", "'messages'", "]", ")", ")", "\n", "", "if", "goal", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'goal'", "]", ".", "append", "(", "sess", "[", "'goal'", "]", ")", "\n", "", "if", "final_goal", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'final_goal'", "]", ".", "append", "(", "sess", "[", "'final_goal'", "]", ")", "\n", "", "if", "task_description", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'task_description'", "]", ".", "append", "(", "sess", "[", "'task description'", "]", ")", "\n", "", "cur_context", ".", "append", "(", "text", ")", "\n", "cur_context_dialog_act", ".", "append", "(", "da", ")", "\n", "\n", "", "", "", "return", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.DealOrNotDataloader.__init__": [[306, 308], ["dataset_dataloader.DatasetDataloader.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "DealOrNotDataloader", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.dataloader.dataset_dataloader.DealOrNotDataloader.load_data": [[309, 389], ["os.path.abspath", "list", "os.path.join", "filter", "zipfile.ZipFile", "zipfile.ZipFile.open().readlines", "print", "os.path.abspath", "os.path.join", "line.strip().split.strip().split.decode", "line.strip().split.strip().split.strip().split", "dataset_dataloader.DealOrNotDataloader.load_data.get_tag"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode"], ["", "def", "load_data", "(", "self", ",", "\n", "data_dir", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ",", "'../../../../data/deal_or_not'", ")", ")", ",", "\n", "data_key", "=", "'all'", ",", "\n", "role", "=", "'all'", ",", "\n", "utterance", "=", "False", ",", "\n", "context", "=", "False", ",", "\n", "context_window_size", "=", "0", ",", "\n", "last_opponent_utterance", "=", "False", ",", "\n", "last_self_utterance", "=", "False", ",", "\n", "session_id", "=", "False", ",", "\n", "terminated", "=", "False", ",", "\n", "goal", "=", "False", ",", "\n", "output", "=", "False", ",", "\n", "partner_input", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "def", "get_tag", "(", "tokens", ",", "tag", ")", ":", "\n", "            ", "return", "tokens", "[", "tokens", ".", "index", "(", "'<'", "+", "tag", "+", "'>'", ")", "+", "1", ":", "tokens", ".", "index", "(", "'</'", "+", "tag", "+", "'>'", ")", "]", "\n", "\n", "", "assert", "role", "in", "[", "'YOU'", ",", "'THEM'", ",", "'all'", "]", "\n", "info_list", "=", "list", "(", "\n", "filter", "(", "eval", ",", "[", "'utterance'", ",", "'context'", ",", "'last_opponent_utterance'", ",", "'last_self_utterance'", ",", "'session_id'", ",", "\n", "'terminated'", ",", "'goal'", ",", "'output'", ",", "'partner_input'", "]", ")", ")", "\n", "self", ".", "data", "=", "{", "'train'", ":", "{", "}", ",", "'val'", ":", "{", "}", ",", "'test'", ":", "{", "}", ",", "'role'", ":", "role", "}", "\n", "if", "data_key", "==", "'all'", ":", "\n", "            ", "data_key_list", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "", "else", ":", "\n", "            ", "data_key_list", "=", "[", "data_key", "]", "\n", "\n", "", "for", "data_key", "in", "data_key_list", ":", "\n", "            ", "archive", "=", "zipfile", ".", "ZipFile", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'deal_or_not.zip'", ")", ",", "'r'", ")", "\n", "data", "=", "archive", ".", "open", "(", "f'{data_key}.txt'", ",", "'r'", ")", ".", "readlines", "(", ")", "\n", "print", "(", "'loaded {}, size {}'", ".", "format", "(", "data_key", ",", "len", "(", "data", ")", ")", ")", "\n", "for", "x", "in", "info_list", ":", "\n", "                ", "self", ".", "data", "[", "data_key", "]", "[", "x", "]", "=", "[", "]", "\n", "", "for", "line", "in", "data", ":", "\n", "                ", "line", "=", "line", ".", "decode", "(", "encoding", "=", "'utf-8'", ")", "\n", "cur_context", "=", "[", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "dialog", "=", "get_tag", "(", "line", ",", "'dialogue'", ")", "\n", "first_role", "=", "dialog", "[", "0", "]", ".", "strip", "(", "':'", ")", "\n", "second_role", "=", "[", "'THEM'", ",", "'YOU'", "]", "[", "first_role", "==", "'THEM'", "]", "\n", "count", "=", "0", "\n", "while", "'<eos>'", "in", "dialog", "or", "'<selection>'", "in", "dialog", ":", "\n", "                    ", "count", "+=", "1", "\n", "if", "'<eos>'", "in", "dialog", ":", "\n", "                        ", "text", "=", "' '", ".", "join", "(", "dialog", "[", "1", ":", "dialog", ".", "index", "(", "'<eos>'", ")", "]", ")", "\n", "dialog", "=", "dialog", "[", "dialog", ".", "index", "(", "'<eos>'", ")", "+", "1", ":", "]", "\n", "", "elif", "'<selection>'", "in", "dialog", ":", "\n", "                        ", "text", "=", "'<selection>'", "\n", "dialog", "=", "[", "]", "\n", "", "if", "role", "==", "first_role", "and", "count", "%", "2", "==", "0", ":", "\n", "                        ", "cur_context", ".", "append", "(", "text", ")", "\n", "continue", "\n", "", "elif", "role", "==", "second_role", "and", "count", "%", "2", "==", "1", ":", "\n", "                        ", "cur_context", ".", "append", "(", "text", ")", "\n", "continue", "\n", "", "if", "utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'utterance'", "]", ".", "append", "(", "text", ")", "\n", "", "if", "context", "and", "context_window_size", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'context'", "]", ".", "append", "(", "cur_context", "[", "-", "context_window_size", ":", "]", ")", "\n", "", "if", "last_opponent_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_opponent_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "1", "]", "if", "len", "(", "cur_context", ")", ">=", "1", "else", "''", ")", "\n", "", "if", "last_self_utterance", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'last_self_utterance'", "]", ".", "append", "(", "\n", "cur_context", "[", "-", "2", "]", "if", "len", "(", "cur_context", ")", ">=", "2", "else", "''", ")", "\n", "", "if", "session_id", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'session_id'", "]", ".", "append", "(", "count", ")", "\n", "", "if", "terminated", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'terminated'", "]", ".", "append", "(", "'<eos>'", "not", "in", "dialog", ")", "\n", "", "if", "goal", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'goal'", "]", ".", "append", "(", "get_tag", "(", "line", ",", "'input'", ")", ")", "\n", "", "if", "output", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'output'", "]", ".", "append", "(", "get_tag", "(", "line", ",", "'output'", ")", ")", "\n", "", "if", "partner_input", ":", "\n", "                        ", "self", ".", "data", "[", "data_key", "]", "[", "'partner_input'", "]", ".", "append", "(", "get_tag", "(", "line", ",", "'partner_input'", ")", ")", "\n", "", "cur_context", ".", "append", "(", "text", ")", "\n", "\n", "", "", "", "return", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.__init__": [[5, 9], ["open", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "path", "=", "path", "\n", "self", ".", "index", "=", "open", "(", "path", ",", "'w'", ")", "\n", "self", ".", "index", ".", "write", "(", "'<html><body>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_title": [[10, 14], ["htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_title", "(", "self", ",", "title", ")", ":", "\n", "        ", "self", ".", "title", "=", "title", "\n", "\n", "self", ".", "index", ".", "write", "(", "'<h1>%s</h1>'", "%", "title", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line": [[15, 17], ["htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_line", "(", "self", ",", "content", ")", ":", "\n", "        ", "self", ".", "index", ".", "write", "(", "'<h3>%s</h3>'", "%", "content", ")", "\n", "# self.index.write('<br/>')", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_done": [[19, 22], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.close"], "methods", ["None"], ["", "def", "write_done", "(", "self", ")", ":", "\n", "        ", "self", ".", "index", ".", "write", "(", "'</body></html>'", ")", "\n", "self", ".", "index", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_table": [[23, 37], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_table", "(", "self", ",", "cols", ",", "data", ")", ":", "\n", "        ", "self", ".", "index", ".", "write", "(", "'<table width=\"900\" border=\"1\">'", ")", "\n", "self", ".", "index", ".", "write", "(", "'<tr><th>&nbsp;</th>'", ")", "\n", "for", "col", "in", "cols", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<th>%s</th>'", "%", "col", ")", "\n", "", "self", ".", "index", ".", "write", "(", "'</tr>'", ")", "\n", "\n", "for", "line", "in", "data", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<tr>'", ")", "\n", "for", "d", "in", "line", ":", "\n", "                ", "self", ".", "index", ".", "write", "(", "'<td align=\"center\">%s</td>'", "%", "d", ")", "\n", "", "self", ".", "index", ".", "write", "(", "'</tr>'", ")", "\n", "\n", "", "self", ".", "index", ".", "write", "(", "'</table>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_nlu_fail": [[38, 61], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "len", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_nlu_fail", "(", "self", ",", "dalist", ",", "mode", "=", "'System'", ")", ":", "\n", "        ", "self", ".", "index", ".", "write", "(", "'<b> %s NLU Failed Dialog Act:</b>'", "%", "mode", ")", "\n", "if", "len", "(", "dalist", ")", "==", "0", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<p>Nothing</p>'", ")", "\n", "return", "\n", "", "self", ".", "index", ".", "write", "(", "'<ul>'", ")", "\n", "for", "i", "in", "dalist", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<li>'", ")", "\n", "self", ".", "index", ".", "write", "(", "i", "[", "0", "]", ")", "\n", "self", ".", "index", ".", "write", "(", "'<ul>'", ")", "\n", "self", ".", "index", ".", "write", "(", "'<li>Occur Num:   %s</li>'", "%", "i", "[", "1", "]", ")", "\n", "self", ".", "index", ".", "write", "(", "'<li>NLU Output</li>'", ")", "\n", "self", ".", "index", ".", "write", "(", "'<ul>'", ")", "\n", "for", "j", "in", "i", "[", "2", "]", ":", "\n", "                ", "self", ".", "index", ".", "write", "(", "'<li>%s'", "%", "j", "[", "0", "]", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'Occur Num:    %s'", "%", "j", "[", "1", "]", ")", "\n", "self", ".", "index", ".", "write", "(", "'</li>'", ")", "\n", "", "self", ".", "index", ".", "write", "(", "'</ul></ul>'", ")", "\n", "", "self", ".", "index", ".", "write", "(", "'</ul>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_png": [[62, 70], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_png", "(", "self", ")", ":", "\n", "        ", "domain_freq_path", "=", "'Frequency_of_domain.png'", "\n", "perform_path", "=", "'Performance_for_each_domain.png'", "\n", "self", ".", "index", ".", "write", "(", "'<table><tr>'", ")", "\n", "\n", "self", ".", "index", ".", "write", "(", "'<td><img src=\"%s\" width=600 border=0/></td>'", "%", "domain_freq_path", ")", "\n", "self", ".", "index", ".", "write", "(", "'<td><img src=\"%s\" width=600 border=0/></td>'", "%", "perform_path", ")", "\n", "self", ".", "index", ".", "write", "(", "'</tr></table>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_dialog_loop_png": [[71, 78], ["os.path.exists", "htmlwriter.HTMLWriter.write_line", "htmlwriter.HTMLWriter.index.write"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line"], ["", "def", "write_dialog_loop_png", "(", "self", ",", "modelname", ")", ":", "\n", "        ", "path", "=", "'Proportions_of_the_dialogue_loop.png'", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "'results/%s/%s'", "%", "(", "modelname", ",", "path", ")", ")", ":", "\n", "\n", "            ", "self", ".", "write_line", "(", "'Dialogue Loop'", ")", "\n", "self", ".", "index", ".", "write", "(", "'<img src=\"%s\" width=800 />'", "%", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_list": [[79, 95], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "len", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "", "def", "write_list", "(", "self", ",", "title", ",", "dalist", ")", ":", "\n", "\n", "        ", "self", ".", "index", ".", "write", "(", "'<b> %s</b>'", "%", "title", ")", "\n", "if", "len", "(", "dalist", ")", "==", "0", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<p>Nothing</p>'", ")", "\n", "return", "\n", "", "self", ".", "index", ".", "write", "(", "'<ul>'", ")", "\n", "for", "i", "in", "dalist", ":", "\n", "            ", "self", ".", "index", ".", "write", "(", "'<li>'", ")", "\n", "self", ".", "index", ".", "write", "(", "i", "[", "0", "]", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'&nbsp;'", ")", "\n", "self", ".", "index", ".", "write", "(", "'Occur Num:     %s</li>'", "%", "i", "[", "1", "]", ")", "\n", "", "self", ".", "index", ".", "write", "(", "'</ul>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_metric": [[96, 101], ["htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.index.write"], "methods", ["None"], ["", "def", "write_metric", "(", "self", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ")", ":", "\n", "        ", "self", ".", "index", ".", "write", "(", "'<p> Success Rate: %.1f %%</p>'", "%", "(", "100", "*", "suc", ")", ")", "\n", "self", ".", "index", ".", "write", "(", "'<p> (Precision, Recall, F1)   :   (%.3f,  %.3f,  %.3f) </p>'", "%", "(", "pre", ",", "rec", ",", "f1", ")", ")", "\n", "self", ".", "index", ".", "write", "(", "'<p> Average Dialog Turn (Succ): %.3f </p>'", "%", "turn_suc", ")", "\n", "self", ".", "index", ".", "write", "(", "'<p> Average Dialog Turn (All): %.3f </p>'", "%", "turn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_domain": [[103, 115], ["htmlwriter.HTMLWriter.write_line", "htmlwriter.HTMLWriter.index.write", "htmlwriter.HTMLWriter.write_line", "htmlwriter.HTMLWriter.write_metric", "htmlwriter.HTMLWriter.write_nlu_fail", "htmlwriter.HTMLWriter.write_nlu_fail", "htmlwriter.HTMLWriter.write_list", "htmlwriter.HTMLWriter.write_list", "htmlwriter.HTMLWriter.write_list", "htmlwriter.HTMLWriter.write_list", "htmlwriter.HTMLWriter.index.write"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_nlu_fail", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_nlu_fail", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_list", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_list", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_list", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_list"], ["", "def", "write_domain", "(", "self", ",", "domain", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ",", "dalist_sys", ",", "dalist_usr", ",", "cyclist", ",", "badlist", ",", "rnilist", ",", "inrlist", ")", ":", "\n", "        ", "self", ".", "write_line", "(", "'Domain %s'", "%", "domain", ")", "\n", "self", ".", "index", ".", "write", "(", "'<div style=\"margin-left:50\">'", ")", "\n", "self", ".", "write_line", "(", "'Overall Results'", ")", "\n", "self", ".", "write_metric", "(", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ")", "\n", "self", ".", "write_nlu_fail", "(", "dalist_sys", ",", "'System'", ")", "\n", "self", ".", "write_nlu_fail", "(", "dalist_usr", ",", "'User'", ")", "\n", "self", ".", "write_list", "(", "'Dialog Loop'", ",", "cyclist", ")", "\n", "self", ".", "write_list", "(", "'Bad Inform Dialog Act'", ",", "badlist", ")", "\n", "self", ".", "write_list", "(", "'Request But Not Inform Dialog Act'", ",", "rnilist", ")", "\n", "self", ".", "write_list", "(", "'Inform But Not Request Dialog Act'", ",", "inrlist", ")", "\n", "self", ".", "index", ".", "write", "(", "'</div>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.report_HTML": [[116, 119], ["htmlwriter.HTMLWriter.write_line", "htmlwriter.HTMLWriter.write_table"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_table"], ["", "def", "report_HTML", "(", "self", ",", "cols", ",", "table", ")", ":", "\n", "        ", "self", ".", "write_line", "(", "'Metric'", ")", "\n", "self", ".", "write_table", "(", "cols", ",", "table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.example.build_user_agent_bertnlu": [[10, 17], ["LAUG.nlu.jointBERT.multiwoz.BERTNLU", "LAUG.policy.rule.multiwoz.RulePolicy", "LAUG.nlg.template.multiwoz.TemplateNLG", "LAUG.dialog_agent.PipelineAgent"], "function", ["None"], ["def", "build_user_agent_bertnlu", "(", ")", ":", "\n", "    ", "user_nlu", "=", "BERTNLU", "(", ")", "\n", "user_dst", "=", "None", "\n", "user_policy", "=", "RulePolicy", "(", "character", "=", "'usr'", ")", "\n", "user_nlg", "=", "TemplateNLG", "(", "is_user", "=", "True", ")", "\n", "user_agent", "=", "PipelineAgent", "(", "user_nlu", ",", "user_dst", ",", "user_policy", ",", "user_nlg", ",", "'user'", ")", "\n", "return", "user_agent", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.example.build_sys_agent_bertnlu": [[19, 26], ["LAUG.nlu.jointBERT.multiwoz.BERTNLU", "LAUG.dst.rule.multiwoz.RuleDST", "LAUG.policy.rule.multiwoz.RulePolicy", "LAUG.nlg.template.multiwoz.TemplateNLG", "LAUG.dialog_agent.PipelineAgent"], "function", ["None"], ["", "def", "build_sys_agent_bertnlu", "(", ")", ":", "\n", "    ", "sys_nlu", "=", "BERTNLU", "(", ")", "\n", "sys_dst", "=", "RuleDST", "(", ")", "\n", "sys_policy", "=", "RulePolicy", "(", "character", "=", "'sys'", ")", "\n", "sys_nlg", "=", "TemplateNLG", "(", "is_user", "=", "False", ")", "\n", "sys_agent", "=", "PipelineAgent", "(", "sys_nlu", ",", "sys_dst", ",", "sys_policy", ",", "sys_nlg", ",", "'sys'", ")", "\n", "return", "sys_agent", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.example.build_sys_agent_svmnlu": [[28, 35], ["LAUG.nlu.svm.multiwoz.SVMNLU", "LAUG.dst.rule.multiwoz.RuleDST", "LAUG.policy.rule.multiwoz.RulePolicy", "LAUG.nlg.template.multiwoz.TemplateNLG", "LAUG.dialog_agent.PipelineAgent"], "function", ["None"], ["", "def", "build_sys_agent_svmnlu", "(", ")", ":", "\n", "    ", "sys_nlu", "=", "SVMNLU", "(", ")", "\n", "sys_dst", "=", "RuleDST", "(", ")", "\n", "sys_policy", "=", "RulePolicy", "(", "character", "=", "'sys'", ")", "\n", "sys_nlg", "=", "TemplateNLG", "(", "is_user", "=", "False", ")", "\n", "sys_agent", "=", "PipelineAgent", "(", "sys_nlu", ",", "sys_dst", ",", "sys_policy", ",", "sys_nlg", ",", "'sys'", ")", "\n", "return", "sys_agent", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.__init__": [[12, 27], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "domain", ")", ":", "\n", "        ", "self", ".", "domain", "=", "domain", "\n", "self", ".", "tot_num", "=", "0", "\n", "self", ".", "suc_num", "=", "0", "\n", "self", ".", "pre", "=", "0", "\n", "self", ".", "rec", "=", "0", "\n", "self", ".", "f1", "=", "0", "\n", "self", ".", "turn_num", "=", "0", "\n", "self", ".", "turn_num_suc", "=", "0", "\n", "self", ".", "failed_nlu_da_sys", "=", "{", "}", "\n", "self", ".", "failed_nlu_da_usr", "=", "{", "}", "\n", "self", ".", "cycle_start_da", "=", "{", "}", "\n", "self", ".", "bad_inform", "=", "{", "}", "\n", "self", ".", "inform_not_reqt", "=", "{", "}", "\n", "self", ".", "reqt_not_inform", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.add_to_dict": [[28, 32], ["None"], "methods", ["None"], ["", "def", "add_to_dict", "(", "self", ",", "da", ",", "dict_record", ")", ":", "\n", "        ", "if", "da", "not", "in", "dict_record", ":", "\n", "            ", "dict_record", "[", "da", "]", "=", "0", "\n", "", "dict_record", "[", "da", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.record": [[33, 88], ["helper.DomainRecorder.add_to_dict", "helper.DomainRecorder.add_to_dict", "helper.DomainRecorder.add_to_dict", "helper.DomainRecorder.add_to_dict", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.add_to_dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.add_to_dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.add_to_dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.add_to_dict"], ["", "def", "record", "(", "self", ",", "suc", ",", "inform", ",", "fail1", ",", "fail2", ",", "cycle", ",", "turn", ")", ":", "\n", "        ", "self", ".", "tot_num", "+=", "1", "\n", "self", ".", "suc_num", "+=", "suc", "\n", "\n", "TP", ",", "FP", ",", "FN", ",", "_", ",", "__", ",", "___", "=", "inform", "\n", "\n", "try", ":", "\n", "            ", "rec", "=", "TP", "/", "(", "TP", "+", "FN", ")", "\n", "", "except", "ZeroDivisionError", ":", "\n", "            ", "rec", "=", "0", "\n", "pre", "=", "0", "\n", "f1", "=", "0", "\n", "", "try", ":", "\n", "            ", "pre", "=", "TP", "/", "(", "TP", "+", "FP", ")", "\n", "f1", "=", "2", "*", "pre", "*", "rec", "/", "(", "pre", "+", "rec", ")", "\n", "", "except", "ZeroDivisionError", ":", "\n", "            ", "pre", "=", "0", "\n", "f1", "=", "0", "\n", "\n", "", "self", ".", "pre", "+=", "pre", "\n", "self", ".", "rec", "+=", "rec", "\n", "self", ".", "f1", "+=", "f1", "\n", "self", ".", "turn_num", "+=", "turn", "\n", "if", "suc", "==", "1", ":", "\n", "            ", "self", ".", "turn_num_suc", "+=", "turn", "\n", "\n", "", "for", "da", "in", "_", ":", "\n", "            ", "self", ".", "add_to_dict", "(", "da", ",", "self", ".", "bad_inform", ")", "\n", "\n", "", "for", "da", "in", "__", ":", "\n", "            ", "self", ".", "add_to_dict", "(", "da", ",", "self", ".", "reqt_not_inform", ")", "\n", "\n", "", "for", "da", "in", "___", ":", "\n", "            ", "self", ".", "add_to_dict", "(", "da", ",", "self", ".", "inform_not_reqt", ")", "\n", "\n", "\n", "", "for", "da", "in", "fail1", ":", "\n", "            ", "if", "da", "[", "0", "]", "not", "in", "self", ".", "failed_nlu_da_sys", ":", "\n", "                ", "self", ".", "failed_nlu_da_sys", "[", "da", "[", "0", "]", "]", "=", "[", "0", ",", "{", "}", "]", "\n", "", "self", ".", "failed_nlu_da_sys", "[", "da", "[", "0", "]", "]", "[", "0", "]", "+=", "1", "\n", "if", "da", "[", "1", "]", "not", "in", "self", ".", "failed_nlu_da_sys", "[", "da", "[", "0", "]", "]", "[", "1", "]", ":", "\n", "                ", "self", ".", "failed_nlu_da_sys", "[", "da", "[", "0", "]", "]", "[", "1", "]", "[", "da", "[", "1", "]", "]", "=", "0", "\n", "", "self", ".", "failed_nlu_da_sys", "[", "da", "[", "0", "]", "]", "[", "1", "]", "[", "da", "[", "1", "]", "]", "+=", "1", "\n", "\n", "", "for", "da", "in", "fail2", ":", "\n", "            ", "if", "da", "[", "0", "]", "not", "in", "self", ".", "failed_nlu_da_usr", ":", "\n", "                ", "self", ".", "failed_nlu_da_usr", "[", "da", "[", "0", "]", "]", "=", "[", "0", ",", "{", "}", "]", "\n", "", "self", ".", "failed_nlu_da_usr", "[", "da", "[", "0", "]", "]", "[", "0", "]", "+=", "1", "\n", "if", "da", "[", "1", "]", "not", "in", "self", ".", "failed_nlu_da_usr", "[", "da", "[", "0", "]", "]", "[", "1", "]", ":", "\n", "                ", "self", ".", "failed_nlu_da_usr", "[", "da", "[", "0", "]", "]", "[", "1", "]", "[", "da", "[", "1", "]", "]", "=", "0", "\n", "", "self", ".", "failed_nlu_da_usr", "[", "da", "[", "0", "]", "]", "[", "1", "]", "[", "da", "[", "1", "]", "]", "+=", "1", "\n", "\n", "", "if", "suc", "==", "0", "and", "len", "(", "cycle", ")", ">", "0", ":", "\n", "            ", "da", "=", "cycle", "[", "-", "1", "]", "\n", "self", ".", "add_to_dict", "(", "da", ",", "self", ".", "cycle_start_da", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.get_info": [[89, 96], ["sum"], "methods", ["None"], ["", "", "def", "get_info", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tot_num", "==", "0", ":", "\n", "            ", "return", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "", "y", "=", "[", "self", ".", "cycle_start_da", "[", "i", "]", "for", "i", "in", "self", ".", "cycle_start_da", "]", "\n", "cycle_tot", "=", "sum", "(", "y", ")", "\n", "tmp", "=", "0", "if", "self", ".", "suc_num", "==", "0", "else", "self", ".", "turn_num_suc", "/", "self", ".", "suc_num", "\n", "return", "self", ".", "tot_num", ",", "self", ".", "suc_num", "/", "self", ".", "tot_num", ",", "self", ".", "pre", "/", "self", ".", "tot_num", ",", "self", ".", "rec", "/", "self", ".", "tot_num", ",", "self", ".", "f1", "/", "self", ".", "tot_num", ",", "cycle_tot", ",", "tmp", ",", "self", ".", "turn_num", "/", "self", ".", "tot_num", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.format_result": [[97, 136], ["sorted", "sorted", "sorted", "sorted", "sorted", "sorted", "sorted.append", "sorted.append", "sorted", "sorted"], "methods", ["None"], ["", "def", "format_result", "(", "self", ")", ":", "\n", "        ", "dalist_sys", "=", "[", "]", "\n", "for", "da", "in", "self", ".", "failed_nlu_da_sys", ":", "\n", "            ", "tmp", "=", "[", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "failed_nlu_da_sys", "[", "da", "]", "[", "0", "]", ",", "[", "(", "'-'", ".", "join", "(", "fda", ")", ",", "self", ".", "failed_nlu_da_sys", "[", "da", "]", "[", "1", "]", "[", "fda", "]", ")", "for", "fda", "in", "self", ".", "failed_nlu_da_sys", "[", "da", "]", "[", "1", "]", "]", "]", "\n", "dalist_sys", ".", "append", "(", "tmp", ")", "\n", "\n", "", "dalist_usr", "=", "[", "]", "\n", "for", "da", "in", "self", ".", "failed_nlu_da_usr", ":", "\n", "            ", "tmp", "=", "[", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "failed_nlu_da_usr", "[", "da", "]", "[", "0", "]", ",", "[", "(", "'-'", ".", "join", "(", "fda", ")", ",", "self", ".", "failed_nlu_da_usr", "[", "da", "]", "[", "1", "]", "[", "fda", "]", ")", "for", "fda", "in", "self", ".", "failed_nlu_da_usr", "[", "da", "]", "[", "1", "]", "]", "]", "\n", "dalist_usr", ".", "append", "(", "tmp", ")", "\n", "\n", "", "cyclist", "=", "[", "(", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "cycle_start_da", "[", "da", "]", ")", "for", "da", "in", "self", ".", "cycle_start_da", "]", "\n", "badlist", "=", "[", "(", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "bad_inform", "[", "da", "]", ")", "for", "da", "in", "self", ".", "bad_inform", "]", "\n", "rnilist", "=", "[", "(", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "reqt_not_inform", "[", "da", "]", ")", "for", "da", "in", "self", ".", "reqt_not_inform", "]", "\n", "inrlist", "=", "[", "(", "'-'", ".", "join", "(", "da", ")", ",", "self", ".", "inform_not_reqt", "[", "da", "]", ")", "for", "da", "in", "self", ".", "inform_not_reqt", "]", "\n", "\n", "dalist_sys", "=", "sorted", "(", "dalist_sys", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "dalist_sys", "=", "dalist_sys", "[", ":", "10", "]", "\n", "for", "t", "in", "dalist_sys", ":", "\n", "            ", "t", "[", "2", "]", "=", "sorted", "(", "t", "[", "2", "]", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "t", "[", "2", "]", "=", "t", "[", "2", "]", "[", ":", "5", "]", "\n", "\n", "", "dalist_usr", "=", "sorted", "(", "dalist_usr", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "dalist_usr", "=", "dalist_usr", "[", ":", "10", "]", "\n", "for", "t", "in", "dalist_usr", ":", "\n", "            ", "t", "[", "2", "]", "=", "sorted", "(", "t", "[", "2", "]", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "t", "[", "2", "]", "=", "t", "[", "2", "]", "[", ":", "5", "]", "\n", "\n", "", "cyclist", "=", "sorted", "(", "cyclist", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "cyclist", "=", "cyclist", "[", ":", "10", "]", "\n", "badlist", "=", "sorted", "(", "badlist", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "badlist", "=", "badlist", "[", ":", "10", "]", "\n", "rnilist", "=", "sorted", "(", "rnilist", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "rnilist", "=", "rnilist", "[", ":", "10", "]", "\n", "inrlist", "=", "sorted", "(", "inrlist", ",", "key", "=", "lambda", "da", ":", "da", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "inrlist", "=", "inrlist", "[", ":", "10", "]", "\n", "\n", "\n", "return", "dalist_sys", ",", "dalist_usr", ",", "cyclist", ",", "badlist", ",", "rnilist", ",", "inrlist", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.__init__": [[139, 143], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_name", "=", "'test_model'", ",", "dataset", "=", "'multiwoz'", ")", ":", "\n", "        ", "self", ".", "recorders", "=", "{", "}", "\n", "self", ".", "modelname", "=", "model_name", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.split_domain": [[144, 154], ["len", "[].lower", "ret.append"], "methods", ["None"], ["", "def", "split_domain", "(", "self", ",", "domain", ",", "das", ")", ":", "\n", "        ", "ret", "=", "[", "]", "\n", "for", "i", "in", "das", ":", "\n", "            ", "if", "len", "(", "i", ")", ">", "0", "and", "i", "[", "0", "]", "[", "1", "]", ".", "lower", "(", ")", "==", "domain", ":", "\n", "                ", "for", "da", "in", "i", ":", "\n", "                    ", "tmp", "=", "(", "da", "[", "0", "]", ",", "da", "[", "1", "]", ",", "da", "[", "2", "]", ",", "da", "[", "3", "]", ")", "\n", "if", "tmp", "not", "in", "ret", ":", "\n", "                        ", "ret", ".", "append", "(", "tmp", ")", "\n", "\n", "", "", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.split_domain_nlu": [[155, 164], ["len", "[].lower", "ret.append"], "methods", ["None"], ["", "def", "split_domain_nlu", "(", "self", ",", "domain", ",", "das", ")", ":", "\n", "        ", "ret", "=", "[", "]", "\n", "for", "i", "in", "das", ":", "\n", "            ", "if", "len", "(", "i", "[", "0", "]", ")", ">", "0", "and", "i", "[", "0", "]", "[", "1", "]", ".", "lower", "(", ")", "==", "domain", ":", "\n", "                ", "if", "i", "not", "in", "ret", ":", "\n", "                    ", "da1", "=", "(", "i", "[", "0", "]", "[", "0", "]", ",", "i", "[", "0", "]", "[", "1", "]", ",", "i", "[", "0", "]", "[", "2", "]", ",", "i", "[", "0", "]", "[", "3", "]", ")", "\n", "da2", "=", "(", "i", "[", "1", "]", "[", "0", "]", ",", "i", "[", "1", "]", "[", "1", "]", ",", "i", "[", "1", "]", "[", "2", "]", ",", "i", "[", "1", "]", "[", "3", "]", ")", "\n", "ret", ".", "append", "(", "(", "da1", ",", "da2", ")", ")", "\n", "", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.record": [[165, 173], ["helper.Reporter.split_domain_nlu", "helper.Reporter.split_domain_nlu", "helper.Reporter.split_domain", "helper.Reporter.recorders[].record", "helper.DomainRecorder", "turn.count"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.split_domain_nlu", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.split_domain_nlu", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.split_domain", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.record"], ["", "def", "record", "(", "self", ",", "domain", ",", "suc", ",", "inform", ",", "fail1", ",", "fail2", ",", "cycle", ",", "turn", ")", ":", "\n", "        ", "if", "domain", "not", "in", "self", ".", "recorders", ":", "\n", "            ", "self", ".", "recorders", "[", "domain", "]", "=", "DomainRecorder", "(", "domain", ")", "\n", "", "fail1", "=", "self", ".", "split_domain_nlu", "(", "domain", ",", "fail1", ")", "\n", "fail2", "=", "self", ".", "split_domain_nlu", "(", "domain", ",", "fail2", ")", "\n", "cycle", "=", "self", ".", "split_domain", "(", "domain", ",", "cycle", ")", "\n", "turn_num", "=", "turn", ".", "count", "(", "domain", ")", "*", "2", "\n", "self", ".", "recorders", "[", "domain", "]", ".", "record", "(", "suc", ",", "inform", ",", "fail1", ",", "fail2", ",", "cycle", ",", "turn_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.report": [[174, 217], ["os.chdir", "os.chdir", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_title", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_line", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_line", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_line", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_line", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_metric", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_png", "helper.Reporter.format_result", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.report_HTML", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_dialog_loop_png", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_done", "helper.Reporter.plot", "helper.Reporter.plot_freq", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "helper.Reporter.recorders[].get_info", "helper.Reporter.recorders[].format_result", "LAUG.util.analysis_tool.htmlwriter.HTMLWriter.write_domain", "time.strftime"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_title", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_line", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_png", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.format_result", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.report_HTML", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_dialog_loop_png", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_done", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot_freq", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.get_info", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.format_result", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.htmlwriter.HTMLWriter.write_domain"], ["", "def", "report", "(", "self", ",", "com", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "'results/'", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "'results'", ")", "\n", "\n", "", "os", ".", "chdir", "(", "'results/'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "modelname", "+", "'/'", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "self", ".", "modelname", ")", "\n", "\n", "", "os", ".", "chdir", "(", "os", ".", "path", ".", "pardir", ")", "\n", "\n", "\n", "\n", "\n", "writer", "=", "HTMLWriter", "(", "'results/%s/report_%s.html'", "%", "(", "self", ".", "modelname", ",", "self", ".", "dataset", ")", ")", "\n", "\n", "writer", ".", "write_title", "(", "'Test Report'", ")", "\n", "writer", ".", "write_line", "(", "'Model Name: %s'", "%", "self", ".", "modelname", ")", "\n", "writer", ".", "write_line", "(", "'Dataset: %s'", "%", "self", ".", "dataset", ")", "\n", "writer", ".", "write_line", "(", "'Time: %s'", "%", "(", "time", ".", "strftime", "(", "\"%Y-%m-%d %H:%M:%S\"", ")", ")", ")", "\n", "\n", "\n", "\n", "writer", ".", "write_line", "(", "'Overall Results'", ")", "\n", "writer", ".", "write_metric", "(", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ")", "\n", "\n", "writer", ".", "write_png", "(", ")", "\n", "\n", "_", ",", "__", "=", "self", ".", "format_result", "(", ")", "\n", "writer", ".", "report_HTML", "(", "_", ",", "__", ")", "\n", "\n", "writer", ".", "write_dialog_loop_png", "(", "self", ".", "modelname", ")", "\n", "\n", "for", "domain", "in", "self", ".", "recorders", ":", "\n", "            ", "_", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "_", ",", "turn_suc", ",", "turn", "=", "self", ".", "recorders", "[", "domain", "]", ".", "get_info", "(", ")", "\n", "_", ",", "__", ",", "___", ",", "____", ",", "_____", ",", "______", "=", "self", ".", "recorders", "[", "domain", "]", ".", "format_result", "(", ")", "\n", "writer", ".", "write_domain", "(", "domain", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "turn_suc", ",", "turn", ",", "_", ",", "__", ",", "___", ",", "____", ",", "_____", ",", "______", ")", "\n", "\n", "", "writer", ".", "write_done", "(", ")", "\n", "\n", "\n", "self", ".", "plot", "(", ")", "\n", "self", ".", "plot_freq", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot_pi": [[218, 236], ["sum", "matplotlib.pie", "matplotlib.pie", "matplotlib.axis", "matplotlib.axis", "t.set_size", "t.set_size"], "methods", ["None"], ["", "def", "plot_pi", "(", "self", ",", "y", ",", "labels", ")", ":", "\n", "\n", "        ", "s", "=", "sum", "(", "y", ")", "\n", "sizes", "=", "[", "i", "*", "100", "/", "s", "for", "i", "in", "y", "]", "\n", "\n", "patches", ",", "l_text", ",", "p_text", "=", "plt", ".", "pie", "(", "\n", "sizes", ",", "\n", "autopct", "=", "'%1.1f%%'", ",", "#\u663e\u793a\u767e\u5206\u6bd4\u65b9\u5f0f", "\n", "shadow", "=", "False", ",", "#\u9634\u5f71\u6548\u679c", "\n", "startangle", "=", "90", ",", "\n", "labels", "=", "labels", ",", "\n", "pctdistance", "=", "0.7", "\n", ")", "\n", "plt", ".", "axis", "(", "'equal'", ")", "\n", "for", "t", "in", "l_text", ":", "\n", "            ", "t", ".", "set_size", "(", "25", ")", "\n", "", "for", "t", "in", "p_text", ":", "\n", "            ", "t", ".", "set_size", "(", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot": [[237, 298], ["matplotlib.figure", "matplotlib.figure", "matplotlib.title", "matplotlib.title", "sum", "matplotlib.pie", "matplotlib.pie", "matplotlib.axis", "matplotlib.axis", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "matplotlib.figure", "matplotlib.figure", "list", "numpy.array", "matplotlib.tick_params", "matplotlib.tick_params", "matplotlib.tick_params", "matplotlib.tick_params", "matplotlib.ylabel", "matplotlib.ylabel", "matplotlib.ylim", "matplotlib.ylim", "matplotlib.xlabel", "matplotlib.xlabel", "matplotlib.title", "matplotlib.title", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.legend", "matplotlib.legend", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "helper.Reporter.recorders[].get_info", "t.set_size", "t.set_size", "range", "len", "i.lower", "i.lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.get_info"], ["", "", "def", "plot", "(", "self", ",", "plot_each_domain", "=", "False", ")", ":", "\n", "        ", "labels", "=", "[", "'Attraction'", ",", "'Taxi'", ",", "'Restaurant'", ",", "'Train'", ",", "'Police'", ",", "'Hotel'", ",", "'Hospital'", "]", "\n", "\n", "domains", "=", "[", "i", "for", "i", "in", "labels", "if", "i", ".", "lower", "(", ")", "in", "self", ".", "recorders", "]", "\n", "infos", "=", "[", "self", ".", "recorders", "[", "i", ".", "lower", "(", ")", "]", ".", "get_info", "(", ")", "for", "i", "in", "domains", "]", "\n", "\n", "### tot_num", "\n", "# plt.subplot(1, 2, 1)", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "7", ")", ",", "dpi", "=", "300", ")", "\n", "font", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "25", "}", "\n", "plt", ".", "title", "(", "'Frequency of domain'", ",", "font", ",", "pad", "=", "30", ")", "\n", "y", "=", "[", "i", "[", "0", "]", "for", "i", "in", "infos", "]", "\n", "s", "=", "sum", "(", "y", ")", "\n", "sizes", "=", "[", "i", "*", "100", "/", "s", "for", "i", "in", "y", "]", "\n", "\n", "patches", ",", "l_text", ",", "p_text", "=", "plt", ".", "pie", "(", "\n", "sizes", ",", "\n", "autopct", "=", "'%1.1f%%'", ",", "#\u663e\u793a\u767e\u5206\u6bd4\u65b9\u5f0f", "\n", "shadow", "=", "False", ",", "#\u9634\u5f71\u6548\u679c", "\n", "startangle", "=", "90", ",", "\n", "labels", "=", "domains", ",", "\n", "pctdistance", "=", "0.7", "\n", ")", "\n", "plt", ".", "axis", "(", "'equal'", ")", "\n", "for", "t", "in", "l_text", ":", "\n", "            ", "t", ".", "set_size", "(", "25", ")", "\n", "", "for", "t", "in", "p_text", ":", "\n", "            ", "t", ".", "set_size", "(", "20", ")", "\n", "", "plt", ".", "savefig", "(", "'results/%s/Frequency_of_domain.png'", "%", "self", ".", "modelname", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "12", ",", "7", ")", ",", "dpi", "=", "300", ")", "\n", "\n", "font1", "=", "{", "'weight'", ":", "'normal'", ",", "'size'", ":", "20", "}", "\n", "\n", "font2", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "22", "}", "\n", "\n", "font3", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "35", "}", "\n", "\n", "x1", "=", "list", "(", "range", "(", "len", "(", "domains", ")", ")", ")", "\n", "x1", "=", "np", ".", "array", "(", "x1", ")", "\n", "x2", "=", "x1", "+", "0.1", "\n", "x3", "=", "x2", "+", "0.1", "\n", "x4", "=", "x3", "+", "0.1", "\n", "plt", ".", "tick_params", "(", "axis", "=", "'y'", ",", "labelsize", "=", "20", ")", "\n", "plt", ".", "tick_params", "(", "axis", "=", "'x'", ",", "labelsize", "=", "22", ")", "\n", "plt", ".", "ylabel", "(", "'score'", ",", "font2", ")", "\n", "plt", ".", "ylim", "(", "0", ",", "1", ")", "\n", "plt", ".", "xlabel", "(", "'domain'", ",", "font2", ")", "\n", "plt", ".", "title", "(", "'Performance for each domain'", ",", "font3", ",", "pad", "=", "16", ")", "\n", "y", "=", "[", "i", "[", "1", "]", "for", "i", "in", "infos", "]", "\n", "plt", ".", "bar", "(", "x1", ",", "y", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Success rate'", ")", "\n", "y", "=", "[", "i", "[", "2", "]", "for", "i", "in", "infos", "]", "\n", "plt", ".", "bar", "(", "x2", ",", "y", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "tick_label", "=", "domains", ",", "label", "=", "'Precision'", ")", "\n", "y", "=", "[", "i", "[", "3", "]", "for", "i", "in", "infos", "]", "\n", "plt", ".", "bar", "(", "x3", ",", "y", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Recall'", ")", "\n", "y", "=", "[", "i", "[", "4", "]", "for", "i", "in", "infos", "]", "\n", "plt", ".", "bar", "(", "x4", ",", "y", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Inform F1'", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "2", ",", "prop", "=", "font1", ")", "\n", "plt", ".", "savefig", "(", "'results/%s/Performance_for_each_domain.png'", "%", "self", ".", "modelname", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot_freq": [[299, 321], ["matplotlib.figure", "matplotlib.figure", "list", "matplotlib.figure", "matplotlib.figure", "helper.Reporter.plot_pi", "matplotlib.title", "matplotlib.title", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "helper.Reporter.recorders[].get_info", "range", "sum", "i.lower", "len", "i.lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.plot_pi", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.get_info"], ["", "def", "plot_freq", "(", "self", ")", ":", "\n", "\n", "        ", "labels", "=", "[", "'Attraction'", ",", "'Taxi'", ",", "'Restaurant'", ",", "'Train'", ",", "'Police'", ",", "'Hotel'", ",", "'Hospital'", "]", "\n", "\n", "domains", "=", "[", "i", "for", "i", "in", "labels", "if", "i", ".", "lower", "(", ")", "in", "self", ".", "recorders", "]", "\n", "infos", "=", "[", "self", ".", "recorders", "[", "i", ".", "lower", "(", ")", "]", ".", "get_info", "(", ")", "for", "i", "in", "domains", "]", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "x", "=", "list", "(", "range", "(", "1", ",", "len", "(", "domains", ")", "+", "1", ")", ")", "\n", "# ax = plt.subplot(2, 2, 1)", "\n", "y", "=", "[", "i", "[", "5", "]", "for", "i", "in", "infos", "]", "\n", "\n", "if", "sum", "(", "y", ")", "==", "0", ":", "return", "\n", "\n", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "7", ")", ",", "dpi", "=", "300", ")", "\n", "self", ".", "plot_pi", "(", "y", ",", "domains", ")", "\n", "font", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "25", "}", "\n", "plt", ".", "title", "(", "'Proportions of the dialogue loop'", ",", "font", ",", "pad", "=", "30", ")", "\n", "plt", ".", "savefig", "(", "'results/%s/Proportions_of_the_dialogue_loop.png'", "%", "self", ".", "modelname", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.format_result": [[322, 338], ["range", "helper.Reporter.recorders[].get_info", "len", "tmp.append", "tmp.append", "tmp.append", "tmp.append", "table.append"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.DomainRecorder.get_info"], ["", "def", "format_result", "(", "self", ")", ":", "\n", "        ", "infos", "=", "[", "self", ".", "recorders", "[", "i", "]", ".", "get_info", "(", ")", "for", "i", "in", "self", ".", "recorders", "]", "\n", "domains", "=", "[", "i", "for", "i", "in", "self", ".", "recorders", "]", "\n", "\n", "cols", "=", "[", "'Total Num'", ",", "'Succ Rate'", ",", "'Precision'", ",", "'Recall'", ",", "'F1'", ",", "'Dialog Loop Failed Rate'", ",", "'Dialog Turn (Succ)'", ",", "'Dialog Turn (All)'", "]", "\n", "table", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "infos", ")", ")", ":", "\n", "            ", "tmp", "=", "[", "domains", "[", "i", "]", "]", "\n", "tmp", ".", "append", "(", "infos", "[", "i", "]", "[", "0", "]", ")", "\n", "tmp", "+=", "[", "'%.3f'", "%", "t", "for", "t", "in", "infos", "[", "i", "]", "[", "1", ":", "5", "]", "]", "\n", "tmp", ".", "append", "(", "'%.3f'", "%", "(", "infos", "[", "i", "]", "[", "5", "]", "/", "infos", "[", "i", "]", "[", "0", "]", ")", ")", "\n", "tmp", ".", "append", "(", "'%.3f'", "%", "infos", "[", "i", "]", "[", "6", "]", ")", "\n", "tmp", ".", "append", "(", "'%.3f'", "%", "infos", "[", "i", "]", "[", "7", "]", ")", "\n", "table", ".", "append", "(", "tmp", ")", "\n", "\n", "", "return", "cols", ",", "table", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.__init__": [[15, 18], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "user_agent", ",", "dataset", "=", "'multiwoz'", ")", ":", "\n", "        ", "self", ".", "user_agent", "=", "user_agent", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.build_sess": [[19, 30], ["LAUG.evaluator.multiwoz_eval.MultiWozEvaluator", "LAUG.dialog_agent.BiSession"], "methods", ["None"], ["", "def", "build_sess", "(", "self", ",", "sys_agent", ")", ":", "\n", "        ", "if", "self", ".", "dataset", "==", "'multiwoz'", ":", "\n", "            ", "evaluator", "=", "MultiWozEvaluator", "(", ")", "\n", "", "else", ":", "\n", "            ", "evaluator", "=", "None", "\n", "\n", "", "if", "evaluator", "is", "None", ":", "\n", "            ", "self", ".", "sess", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "sess", "=", "BiSession", "(", "sys_agent", "=", "sys_agent", ",", "user_agent", "=", "self", ".", "user_agent", ",", "kb_query", "=", "None", ",", "evaluator", "=", "evaluator", ")", "\n", "", "return", "self", ".", "sess", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.sample_dialog": [[31, 58], ["analyzer.Analyzer.build_sess", "analyzer.Analyzer.init_session", "print", "pprint.pprint.pprint", "print", "range", "print", "print", "print", "print", "print", "print", "print", "pprint.pprint.pprint", "print", "analyzer.Analyzer.next_turn", "print", "print", "print", "analyzer.Analyzer.user_agent.policy.policy.goal.task_complete", "analyzer.Analyzer.evaluator.task_success", "analyzer.Analyzer.evaluator.book_rate", "analyzer.Analyzer.evaluator.inform_F1", "analyzer.Analyzer.evaluator.final_goal_analyze"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.build_sess", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.init_session"], ["", "def", "sample_dialog", "(", "self", ",", "sys_agent", ")", ":", "\n", "        ", "sess", "=", "self", ".", "build_sess", "(", "sys_agent", ")", "\n", "sys_response", "=", "''", "if", "self", ".", "user_agent", ".", "nlu", "else", "[", "]", "\n", "sess", ".", "init_session", "(", ")", "\n", "print", "(", "'init goal:'", ")", "\n", "pprint", "(", "sess", ".", "evaluator", ".", "goal", ")", "\n", "print", "(", "'-'", "*", "50", ")", "\n", "for", "i", "in", "range", "(", "40", ")", ":", "\n", "            ", "sys_response", ",", "user_response", ",", "session_over", ",", "reward", "=", "sess", ".", "next_turn", "(", "sys_response", ")", "\n", "print", "(", "'user:'", ",", "user_response", ")", "\n", "# print('user in da:', sess.user_agent.get_in_da())", "\n", "# print('user out da:', sess.user_agent.get_out_da())", "\n", "print", "(", "'sys:'", ",", "sys_response", ")", "\n", "# print('sys in da:', sess.sys_agent.get_in_da())", "\n", "# print('sys out da:', sess.sys_agent.get_out_da())", "\n", "print", "(", ")", "\n", "if", "session_over", "is", "True", ":", "\n", "                ", "break", "\n", "", "", "print", "(", "'task complete:'", ",", "sess", ".", "user_agent", ".", "policy", ".", "policy", ".", "goal", ".", "task_complete", "(", ")", ")", "\n", "print", "(", "'task success:'", ",", "sess", ".", "evaluator", ".", "task_success", "(", ")", ")", "\n", "print", "(", "'book rate:'", ",", "sess", ".", "evaluator", ".", "book_rate", "(", ")", ")", "\n", "print", "(", "'inform precision/recall/f1:'", ",", "sess", ".", "evaluator", ".", "inform_F1", "(", ")", ")", "\n", "print", "(", "f\"percentage of domains that satisfies the database constraints: {sess.evaluator.final_goal_analyze()}\"", ")", "\n", "print", "(", "'-'", "*", "50", ")", "\n", "print", "(", "'final goal:'", ")", "\n", "pprint", "(", "sess", ".", "evaluator", ".", "goal", ")", "\n", "print", "(", "'='", "*", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.comprehensive_analyze": [[59, 230], ["analyzer.Analyzer.build_sess", "LAUG.util.analysis_tool.helper.Reporter", "logging.getLogger", "logging.basicConfig", "os.path.join", "open", "open", "tqdm.tqdm.tqdm", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "open.close", "LAUG.util.analysis_tool.helper.Reporter.report", "random.randint", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "range", "random.seed", "numpy.random.seed", "torch.manual_seed", "goal_seeds.pop", "analyzer.Analyzer.init_session", "range", "analyzer.Analyzer.evaluator.task_success", "analyzer.Analyzer.user_agent.policy.policy.goal.task_complete", "analyzer.Analyzer.evaluator.book_rate", "analyzer.Analyzer.evaluator.inform_F1", "analyzer.Analyzer.evaluator.final_goal_analyze", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "range", "analyzer.Analyzer.next_turn", "print", "print", "print", "print", "usr_da_list.append", "precision.append", "recall.append", "f1.append", "match.append", "len", "len", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.info", "logging.info", "LAUG.util.analysis_tool.helper.Reporter.record", "analyzer.Analyzer.user_agent.get_in_da", "analyzer.Analyzer.user_agent.get_out_da", "hasattr", "isinstance", "analyzer.Analyzer.user_agent.get_out_da", "isinstance", "hasattr", "analyzer.Analyzer.sys_agent.get_out_da", "analyzer.Analyzer.user_agent.get_out_da", "len", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "domain_set.append", "cycle_start.append", "domain_turn.append", "analyzer.Analyzer.evaluator.domain_success", "analyzer.Analyzer.evaluator.domain_reqt_inform_analyze", "analyzer.Analyzer.sys_agent.get_in_da", "analyzer.Analyzer.user_agent.get_out_da", "analyzer.Analyzer.user_agent.get_out_da", "analyzer.Analyzer.sys_agent.get_in_da", "analyzer.Analyzer.sys_agent.get_in_da", "analyzer.Analyzer.user_agent.get_in_da", "analyzer.Analyzer.user_agent.get_in_da", "len", "usr_da_list.count", "len", "len", "[].lower", "da.split", "da.split", "da.split", "failed_da_sys.append", "failed_da_usr.append"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.build_sess", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.report", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.init_session", "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.helper.Reporter.record"], ["", "def", "comprehensive_analyze", "(", "self", ",", "sys_agent", ",", "model_name", ",", "total_dialog", "=", "100", ")", ":", "\n", "        ", "sess", "=", "self", ".", "build_sess", "(", "sys_agent", ")", "\n", "\n", "goal_seeds", "=", "[", "random", ".", "randint", "(", "1", ",", "100000", ")", "for", "_", "in", "range", "(", "total_dialog", ")", "]", "\n", "precision", "=", "[", "]", "\n", "recall", "=", "[", "]", "\n", "f1", "=", "[", "]", "\n", "match", "=", "[", "]", "\n", "suc_num", "=", "0", "\n", "complete_num", "=", "0", "\n", "turn_num", "=", "0", "\n", "turn_suc_num", "=", "0", "\n", "num_domains", "=", "0", "\n", "num_domains_satisfying_constraints", "=", "0", "\n", "num_dialogs_satisfying_constraints", "=", "0", "\n", "\n", "reporter", "=", "Reporter", "(", "model_name", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", ",", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "'results'", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "'results'", ")", "\n", "", "output_dir", "=", "os", ".", "path", ".", "join", "(", "'results'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "output_dir", ")", "\n", "", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'res.txt'", ")", ",", "'w'", ")", "\n", "flog", "=", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'log.txt'", ")", ",", "'w'", ")", "\n", "\n", "for", "j", "in", "tqdm", "(", "range", "(", "total_dialog", ")", ",", "desc", "=", "\"dialogue\"", ")", ":", "\n", "            ", "sys_response", "=", "''", "if", "self", ".", "user_agent", ".", "nlu", "else", "[", "]", "\n", "random", ".", "seed", "(", "goal_seeds", "[", "0", "]", ")", "\n", "np", ".", "random", ".", "seed", "(", "goal_seeds", "[", "0", "]", ")", "\n", "torch", ".", "manual_seed", "(", "goal_seeds", "[", "0", "]", ")", "\n", "goal_seeds", ".", "pop", "(", "0", ")", "\n", "sess", ".", "init_session", "(", ")", "\n", "\n", "usr_da_list", "=", "[", "]", "\n", "failed_da_sys", "=", "[", "]", "\n", "failed_da_usr", "=", "[", "]", "\n", "last_sys_da", "=", "None", "\n", "\n", "step", "=", "0", "\n", "\n", "# print('init goal:',file=f)", "\n", "# # print(sess.evaluator.goal, file=f)", "\n", "# # pprint(sess.evaluator.goal)", "\n", "# print(sess.user_agent.policy.policy.goal.domain_goals, file=f)", "\n", "# print('-' * 50,file=f)", "\n", "\n", "for", "i", "in", "range", "(", "40", ")", ":", "\n", "                ", "sys_response", ",", "user_response", ",", "session_over", ",", "reward", "=", "sess", ".", "next_turn", "(", "\n", "sys_response", ")", "\n", "print", "(", "'user in'", ",", "sess", ".", "user_agent", ".", "get_in_da", "(", ")", ",", "file", "=", "flog", ")", "\n", "print", "(", "'user out'", ",", "sess", ".", "user_agent", ".", "get_out_da", "(", ")", ",", "file", "=", "flog", ")", "\n", "#", "\n", "# print('sys in', sess.sys_agent.get_in_da(),file=flog)", "\n", "# print('sys out', sess.sys_agent.get_out_da(),file=flog)", "\n", "print", "(", "'user:'", ",", "user_response", ",", "file", "=", "flog", ")", "\n", "print", "(", "'sys:'", ",", "sys_response", ",", "file", "=", "flog", ")", "\n", "\n", "step", "+=", "2", "\n", "\n", "if", "hasattr", "(", "sess", ".", "sys_agent", ",", "\"get_in_da\"", ")", "and", "isinstance", "(", "sess", ".", "sys_agent", ".", "get_in_da", "(", ")", ",", "list", ")", "and", "sess", ".", "user_agent", ".", "get_out_da", "(", ")", "!=", "[", "]", "and", "sess", ".", "user_agent", ".", "get_out_da", "(", ")", "!=", "sess", ".", "sys_agent", ".", "get_in_da", "(", ")", ":", "\n", "                    ", "for", "da1", "in", "sess", ".", "user_agent", ".", "get_out_da", "(", ")", ":", "\n", "                        ", "for", "da2", "in", "sess", ".", "sys_agent", ".", "get_in_da", "(", ")", ":", "\n", "                            ", "if", "da1", "!=", "da2", "and", "da1", "is", "not", "None", "and", "da2", "is", "not", "None", "and", "(", "da1", ",", "da2", ")", "not", "in", "failed_da_sys", ":", "\n", "                                ", "failed_da_sys", ".", "append", "(", "(", "da1", ",", "da2", ")", ")", "\n", "\n", "", "", "", "", "if", "isinstance", "(", "last_sys_da", ",", "list", ")", "and", "last_sys_da", "is", "not", "None", "and", "last_sys_da", "!=", "[", "]", "and", "sess", ".", "user_agent", ".", "get_in_da", "(", ")", "!=", "last_sys_da", ":", "\n", "                    ", "for", "da1", "in", "last_sys_da", ":", "\n", "                        ", "for", "da2", "in", "sess", ".", "user_agent", ".", "get_in_da", "(", ")", ":", "\n", "                            ", "if", "da1", "!=", "da2", "and", "da1", "is", "not", "None", "and", "da2", "is", "not", "None", "and", "(", "da1", ",", "da2", ")", "not", "in", "failed_da_usr", ":", "\n", "                                ", "failed_da_usr", ".", "append", "(", "(", "da1", ",", "da2", ")", ")", "\n", "\n", "", "", "", "", "last_sys_da", "=", "sess", ".", "sys_agent", ".", "get_out_da", "(", ")", "if", "hasattr", "(", "sess", ".", "sys_agent", ",", "\"get_out_da\"", ")", "else", "None", "\n", "usr_da_list", ".", "append", "(", "sess", ".", "user_agent", ".", "get_out_da", "(", ")", ")", "\n", "\n", "if", "session_over", ":", "\n", "                    ", "break", "\n", "\n", "", "", "task_success", "=", "sess", ".", "evaluator", ".", "task_success", "(", ")", "\n", "task_complete", "=", "sess", ".", "user_agent", ".", "policy", ".", "policy", ".", "goal", ".", "task_complete", "(", ")", "\n", "book_rate", "=", "sess", ".", "evaluator", ".", "book_rate", "(", ")", "\n", "stats", "=", "sess", ".", "evaluator", ".", "inform_F1", "(", ")", "\n", "percentage", "=", "sess", ".", "evaluator", ".", "final_goal_analyze", "(", ")", "\n", "if", "task_success", ":", "\n", "                ", "suc_num", "+=", "1", "\n", "turn_suc_num", "+=", "step", "\n", "", "if", "task_complete", ":", "\n", "                ", "complete_num", "+=", "1", "\n", "", "if", "stats", "[", "2", "]", "is", "not", "None", ":", "\n", "                ", "precision", ".", "append", "(", "stats", "[", "0", "]", ")", "\n", "recall", ".", "append", "(", "stats", "[", "1", "]", ")", "\n", "f1", ".", "append", "(", "stats", "[", "2", "]", ")", "\n", "", "if", "book_rate", "is", "not", "None", ":", "\n", "                ", "match", ".", "append", "(", "book_rate", ")", "\n", "", "if", "len", "(", "sess", ".", "evaluator", ".", "goal", ")", ">", "0", ":", "\n", "                ", "num_domains", "+=", "len", "(", "sess", ".", "evaluator", ".", "goal", ")", "\n", "num_domains_satisfying_constraints", "+=", "len", "(", "sess", ".", "evaluator", ".", "goal", ")", "*", "percentage", "\n", "", "num_dialogs_satisfying_constraints", "+=", "(", "percentage", "==", "1", ")", "\n", "if", "(", "j", "+", "1", ")", "%", "100", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"model name %s\"", ",", "model_name", ")", "\n", "logger", ".", "info", "(", "\"dialogue %d\"", ",", "j", "+", "1", ")", "\n", "logger", ".", "info", "(", "sess", ".", "evaluator", ".", "goal", ")", "\n", "logger", ".", "info", "(", "'task complete: %.3f'", ",", "complete_num", "/", "(", "j", "+", "1", ")", ")", "\n", "logger", ".", "info", "(", "'task success: %.3f'", ",", "suc_num", "/", "(", "j", "+", "1", ")", ")", "\n", "logger", ".", "info", "(", "'book rate: %.3f'", ",", "np", ".", "mean", "(", "match", ")", ")", "\n", "logger", ".", "info", "(", "'inform precision/recall/f1: %.3f %.3f %.3f'", ",", "np", ".", "mean", "(", "precision", ")", ",", "np", ".", "mean", "(", "recall", ")", ",", "np", ".", "mean", "(", "f1", ")", ")", "\n", "logging", ".", "info", "(", "\"percentage of domains that satisfy the database constraints: %.3f\"", "%", "(", "1", "if", "num_domains", "==", "0", "else", "(", "num_domains_satisfying_constraints", "/", "num_domains", ")", ")", ")", "\n", "logging", ".", "info", "(", "\"percentage of dialogs that satisfy the database constraints: %.3f\"", "%", "(", "num_dialogs_satisfying_constraints", "/", "(", "j", "+", "1", ")", ")", ")", "\n", "", "domain_set", "=", "[", "]", "\n", "for", "da", "in", "sess", ".", "evaluator", ".", "usr_da_array", ":", "\n", "                ", "if", "da", ".", "split", "(", "'-'", ")", "[", "0", "]", "!=", "'general'", "and", "da", ".", "split", "(", "'-'", ")", "[", "0", "]", "not", "in", "domain_set", ":", "\n", "                    ", "domain_set", ".", "append", "(", "da", ".", "split", "(", "'-'", ")", "[", "0", "]", ")", "\n", "\n", "", "", "turn_num", "+=", "step", "\n", "\n", "da_list", "=", "usr_da_list", "\n", "cycle_start", "=", "[", "]", "\n", "for", "da", "in", "usr_da_list", ":", "\n", "                ", "if", "len", "(", "da", ")", "==", "1", "and", "da", "[", "0", "]", "[", "2", "]", "==", "'general'", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "usr_da_list", ".", "count", "(", "da", ")", ">", "1", "and", "da", "not", "in", "cycle_start", ":", "\n", "                    ", "cycle_start", ".", "append", "(", "da", ")", "\n", "\n", "", "", "domain_turn", "=", "[", "]", "\n", "for", "da", "in", "usr_da_list", ":", "\n", "                ", "if", "len", "(", "da", ")", ">", "0", "and", "da", "[", "0", "]", "is", "not", "None", "and", "len", "(", "da", "[", "0", "]", ")", ">", "2", ":", "\n", "                    ", "domain_turn", ".", "append", "(", "da", "[", "0", "]", "[", "1", "]", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "for", "domain", "in", "domain_set", ":", "\n", "                ", "reporter", ".", "record", "(", "domain", ",", "sess", ".", "evaluator", ".", "domain_success", "(", "domain", ")", ",", "sess", ".", "evaluator", ".", "domain_reqt_inform_analyze", "(", "domain", ")", ",", "failed_da_sys", ",", "failed_da_usr", ",", "cycle_start", ",", "domain_turn", ")", "\n", "\n", "", "", "tmp", "=", "0", "if", "suc_num", "==", "0", "else", "turn_suc_num", "/", "suc_num", "\n", "print", "(", "\"=\"", "*", "100", ")", "\n", "print", "(", "\"complete number of dialogs/tot:\"", ",", "complete_num", "/", "total_dialog", ")", "\n", "print", "(", "\"success number of dialogs/tot:\"", ",", "suc_num", "/", "total_dialog", ")", "\n", "print", "(", "\"average precision:\"", ",", "np", ".", "mean", "(", "precision", ")", ")", "\n", "print", "(", "\"average recall:\"", ",", "np", ".", "mean", "(", "recall", ")", ")", "\n", "print", "(", "\"average f1:\"", ",", "np", ".", "mean", "(", "f1", ")", ")", "\n", "print", "(", "'average book rate:'", ",", "np", ".", "mean", "(", "match", ")", ")", "\n", "print", "(", "\"average turn (succ):\"", ",", "tmp", ")", "\n", "print", "(", "\"average turn (all):\"", ",", "turn_num", "/", "total_dialog", ")", "\n", "print", "(", "\"percentage of domains that satisfy the database constraints: %.3f\"", "%", "(", "1", "if", "num_domains", "==", "0", "else", "(", "num_domains_satisfying_constraints", "/", "num_domains", ")", ")", ")", "\n", "print", "(", "\"percentage of dialogs that satisfy the database constraints: %.3f\"", "%", "(", "num_dialogs_satisfying_constraints", "/", "total_dialog", ")", ")", "\n", "print", "(", "\"=\"", "*", "100", ")", "\n", "print", "(", "\"complete number of dialogs/tot:\"", ",", "complete_num", "/", "total_dialog", ",", "file", "=", "f", ")", "\n", "print", "(", "\"success number of dialogs/tot:\"", ",", "suc_num", "/", "total_dialog", ",", "file", "=", "f", ")", "\n", "print", "(", "\"average precision:\"", ",", "np", ".", "mean", "(", "precision", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"average recall:\"", ",", "np", ".", "mean", "(", "recall", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"average f1:\"", ",", "np", ".", "mean", "(", "f1", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "'average book rate:'", ",", "np", ".", "mean", "(", "match", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"average turn (succ):\"", ",", "tmp", ",", "file", "=", "f", ")", "\n", "print", "(", "\"average turn (all):\"", ",", "turn_num", "/", "total_dialog", ",", "file", "=", "f", ")", "\n", "print", "(", "\"percentage of domains that satisfy the database constraints: %.3f\"", "%", "(", "1", "if", "num_domains", "==", "0", "else", "(", "num_domains_satisfying_constraints", "/", "num_domains", ")", ")", ",", "file", "=", "f", ")", "\n", "print", "(", "\"percentage of dialogs that satisfy the database constraints: %.3f\"", "%", "(", "num_dialogs_satisfying_constraints", "/", "total_dialog", ")", ",", "file", "=", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "reporter", ".", "report", "(", "complete_num", "/", "total_dialog", ",", "suc_num", "/", "total_dialog", ",", "np", ".", "mean", "(", "precision", ")", ",", "np", ".", "mean", "(", "recall", ")", ",", "np", ".", "mean", "(", "f1", ")", ",", "tmp", ",", "turn_num", "/", "total_dialog", ")", "\n", "\n", "return", "complete_num", "/", "total_dialog", ",", "suc_num", "/", "total_dialog", ",", "np", ".", "mean", "(", "precision", ")", ",", "np", ".", "mean", "(", "recall", ")", ",", "np", ".", "mean", "(", "f1", ")", ",", "np", ".", "mean", "(", "match", ")", ",", "turn_num", "/", "total_dialog", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.compare_models": [[231, 283], ["random.randint", "range", "list", "numpy.array", "matplotlib.figure", "matplotlib.tick_params", "matplotlib.tick_params", "matplotlib.ylabel", "matplotlib.ylim", "matplotlib.xlabel", "matplotlib.title", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.bar", "matplotlib.legend", "matplotlib.savefig", "matplotlib.close", "len", "len", "len", "len", "random.seed", "numpy.random.seed", "torch.manual_seed", "analyzer.Analyzer.comprehensive_analyze", "y0.append", "y1.append", "y2.append", "y3.append", "y4.append", "y5.append", "y6.append", "range", "os.path.exists", "os.mkdir", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.analysis_tool.analyzer.Analyzer.comprehensive_analyze"], ["", "def", "compare_models", "(", "self", ",", "agent_list", ",", "model_name", ",", "total_dialog", "=", "100", ")", ":", "\n", "        ", "if", "len", "(", "agent_list", ")", "!=", "len", "(", "model_name", ")", ":", "\n", "            ", "return", "\n", "", "if", "len", "(", "agent_list", ")", "<=", "0", ":", "\n", "            ", "return", "\n", "\n", "", "seed", "=", "random", ".", "randint", "(", "1", ",", "100000", ")", "\n", "\n", "y0", ",", "y1", ",", "y2", ",", "y3", ",", "y4", ",", "y5", ",", "y6", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "agent_list", ")", ")", ":", "\n", "            ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# print(model_name[i], total_dialog)", "\n", "complete", ",", "suc", ",", "pre", ",", "rec", ",", "f1", ",", "match", ",", "turn", "=", "self", ".", "comprehensive_analyze", "(", "agent_list", "[", "i", "]", ",", "model_name", "[", "i", "]", ",", "total_dialog", ")", "\n", "y0", ".", "append", "(", "complete", ")", "\n", "y1", ".", "append", "(", "suc", ")", "\n", "y2", ".", "append", "(", "pre", ")", "\n", "y3", ".", "append", "(", "rec", ")", "\n", "y4", ".", "append", "(", "f1", ")", "\n", "y5", ".", "append", "(", "match", ")", "\n", "y6", ".", "append", "(", "turn", ")", "\n", "\n", "", "x1", "=", "list", "(", "range", "(", "1", ",", "1", "+", "len", "(", "model_name", ")", ")", ")", "\n", "x1", "=", "np", ".", "array", "(", "x1", ")", "\n", "x2", "=", "x1", "+", "0.1", "\n", "x3", "=", "x2", "+", "0.1", "\n", "x4", "=", "x3", "+", "0.1", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "12", ",", "7", ")", ",", "dpi", "=", "300", ")", "\n", "\n", "font1", "=", "{", "'weight'", ":", "'normal'", ",", "'size'", ":", "20", "}", "\n", "\n", "font2", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "22", "}", "\n", "\n", "font3", "=", "{", "'weight'", ":", "'bold'", ",", "'size'", ":", "35", "}", "\n", "plt", ".", "tick_params", "(", "axis", "=", "'y'", ",", "labelsize", "=", "20", ")", "\n", "plt", ".", "tick_params", "(", "axis", "=", "'x'", ",", "labelsize", "=", "22", ")", "\n", "plt", ".", "ylabel", "(", "'score'", ",", "font2", ")", "\n", "plt", ".", "ylim", "(", "0", ",", "1", ")", "\n", "plt", ".", "xlabel", "(", "'system'", ",", "font2", ")", "\n", "plt", ".", "title", "(", "'Comparison of different systems'", ",", "font3", ",", "pad", "=", "16", ")", "\n", "\n", "plt", ".", "bar", "(", "x1", ",", "y0", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Task complete'", ")", "\n", "plt", ".", "bar", "(", "x2", ",", "y1", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "tick_label", "=", "model_name", ",", "label", "=", "'Success rate'", ")", "\n", "plt", ".", "bar", "(", "x3", ",", "y4", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Inform F1'", ")", "\n", "plt", ".", "bar", "(", "x4", ",", "y5", ",", "width", "=", "0.1", ",", "align", "=", "'center'", ",", "label", "=", "'Book rate'", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "2", ",", "prop", "=", "font1", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "'results/'", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "'results'", ")", "\n", "", "plt", ".", "savefig", "(", "'results/compare_results.jpg'", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.state.default_state": [[1, 90], ["dict"], "function", ["None"], ["def", "default_state", "(", ")", ":", "\n", "    ", "state", "=", "dict", "(", "user_action", "=", "[", "]", ",", "\n", "system_action", "=", "[", "]", ",", "\n", "belief_state", "=", "{", "}", ",", "\n", "request_state", "=", "{", "}", ",", "\n", "terminated", "=", "False", ",", "\n", "history", "=", "[", "]", ")", "\n", "state", "[", "'belief_state'", "]", "=", "{", "\n", "\"police\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "}", "\n", "}", ",", "\n", "\"hotel\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", ",", "\n", "\"people\"", ":", "\"\"", ",", "\n", "\"day\"", ":", "\"\"", ",", "\n", "\"stay\"", ":", "\"\"", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"name\"", ":", "\"\"", ",", "\n", "\"area\"", ":", "\"\"", ",", "\n", "\"parking\"", ":", "\"\"", ",", "\n", "\"pricerange\"", ":", "\"\"", ",", "\n", "\"stars\"", ":", "\"\"", ",", "\n", "\"internet\"", ":", "\"\"", ",", "\n", "\"type\"", ":", "\"\"", "\n", "}", "\n", "}", ",", "\n", "\"attraction\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"type\"", ":", "\"\"", ",", "\n", "\"name\"", ":", "\"\"", ",", "\n", "\"area\"", ":", "\"\"", "\n", "}", "\n", "}", ",", "\n", "\"restaurant\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", ",", "\n", "\"people\"", ":", "\"\"", ",", "\n", "\"day\"", ":", "\"\"", ",", "\n", "\"time\"", ":", "\"\"", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"food\"", ":", "\"\"", ",", "\n", "\"pricerange\"", ":", "\"\"", ",", "\n", "\"name\"", ":", "\"\"", ",", "\n", "\"area\"", ":", "\"\"", ",", "\n", "}", "\n", "}", ",", "\n", "\"hospital\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"department\"", ":", "\"\"", "\n", "}", "\n", "}", ",", "\n", "\"taxi\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"leaveAt\"", ":", "\"\"", ",", "\n", "\"destination\"", ":", "\"\"", ",", "\n", "\"departure\"", ":", "\"\"", ",", "\n", "\"arriveBy\"", ":", "\"\"", "\n", "}", "\n", "}", ",", "\n", "\"train\"", ":", "{", "\n", "\"book\"", ":", "{", "\n", "\"booked\"", ":", "[", "]", ",", "\n", "\"people\"", ":", "\"\"", "\n", "}", ",", "\n", "\"semi\"", ":", "{", "\n", "\"leaveAt\"", ":", "\"\"", ",", "\n", "\"destination\"", ":", "\"\"", ",", "\n", "\"day\"", ":", "\"\"", ",", "\n", "\"arriveBy\"", ":", "\"\"", ",", "\n", "\"departure\"", ":", "\"\"", "\n", "}", "\n", "}", "\n", "}", "\n", "return", "state", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.lexicalize.delexicalize_da": [[5, 21], ["delexicalized_da.append", "counter.setdefault", "str"], "function", ["None"], ["def", "delexicalize_da", "(", "da", ",", "requestable", ")", ":", "\n", "    ", "delexicalized_da", "=", "[", "]", "\n", "counter", "=", "{", "}", "\n", "for", "intent", ",", "domain", ",", "slot", ",", "value", "in", "da", ":", "\n", "        ", "if", "intent", "in", "requestable", ":", "\n", "            ", "v", "=", "'?'", "\n", "", "else", ":", "\n", "            ", "if", "slot", "==", "'none'", ":", "\n", "                ", "v", "=", "'none'", "\n", "", "else", ":", "\n", "                ", "k", "=", "'-'", ".", "join", "(", "[", "intent", ",", "domain", ",", "slot", "]", ")", "\n", "counter", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "counter", "[", "k", "]", "+=", "1", "\n", "v", "=", "str", "(", "counter", "[", "k", "]", ")", "\n", "", "", "delexicalized_da", ".", "append", "(", "[", "domain", ",", "intent", ",", "slot", ",", "v", "]", ")", "\n", "", "return", "delexicalized_da", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.lexicalize.flat_da": [[23, 26], ["None"], "function", ["None"], ["", "def", "flat_da", "(", "delexicalized_da", ")", ":", "\n", "    ", "flaten", "=", "[", "'-'", ".", "join", "(", "x", ")", "for", "x", "in", "delexicalized_da", "]", "\n", "return", "flaten", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.lexicalize.deflat_da": [[28, 38], ["copy.deepcopy", "da.split", "dialog_act[].append"], "function", ["None"], ["", "def", "deflat_da", "(", "meta", ")", ":", "\n", "    ", "meta", "=", "deepcopy", "(", "meta", ")", "\n", "dialog_act", "=", "{", "}", "\n", "for", "da", "in", "meta", ":", "\n", "        ", "d", ",", "i", ",", "s", ",", "v", "=", "da", ".", "split", "(", "'-'", ")", "\n", "k", "=", "'-'", ".", "join", "(", "(", "d", ",", "i", ")", ")", "\n", "if", "k", "not", "in", "dialog_act", ":", "\n", "            ", "dialog_act", "[", "k", "]", "=", "[", "]", "\n", "", "dialog_act", "[", "k", "]", ".", "append", "(", "[", "s", ",", "v", "]", ")", "\n", "", "return", "dialog_act", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.lexicalize.lexicalize_da": [[40, 85], ["copy.deepcopy", "copy.deepcopy.items", "copy.deepcopy.items", "k.split", "domain.lower", "domain_intent.split", "tuples.append", "intent.lower", "domain.lower", "domain.lower", "pair[].lower", "str", "len", "int", "domain.lower", "len", "domain.lower"], "function", ["None"], ["", "def", "lexicalize_da", "(", "meta", ",", "entities", ",", "state", ",", "requestable", ",", "cur_domain", "=", "None", ")", ":", "\n", "    ", "meta", "=", "deepcopy", "(", "meta", ")", "\n", "\n", "for", "k", ",", "v", "in", "meta", ".", "items", "(", ")", ":", "\n", "        ", "domain", ",", "intent", "=", "k", ".", "split", "(", "'-'", ")", "\n", "if", "domain", ".", "lower", "(", ")", "in", "[", "'general'", "]", ":", "\n", "            ", "continue", "\n", "", "elif", "intent", "in", "requestable", ":", "\n", "            ", "for", "pair", "in", "v", ":", "\n", "                ", "pair", "[", "1", "]", "=", "'?'", "\n", "", "", "elif", "intent", ".", "lower", "(", ")", "in", "[", "'nooffer'", ",", "'nobook'", "]", ":", "\n", "            ", "if", "domain", ".", "lower", "(", ")", "in", "[", "'booking'", "]", ":", "\n", "                ", "if", "cur_domain", "and", "cur_domain", "in", "entities", ":", "\n", "                    ", "domain", "=", "cur_domain", "\n", "", "else", ":", "\n", "                    ", "continue", "\n", "", "", "for", "pair", "in", "v", ":", "\n", "                ", "if", "pair", "[", "0", "]", "in", "state", "[", "domain", ".", "lower", "(", ")", "]", "[", "'semi'", "]", ":", "\n", "                    ", "pair", "[", "1", "]", "=", "state", "[", "domain", ".", "lower", "(", ")", "]", "[", "'semi'", "]", "[", "pair", "[", "0", "]", "]", "\n", "", "else", ":", "\n", "                    ", "pair", "[", "1", "]", "=", "'none'", "\n", "", "", "", "else", ":", "\n", "            ", "if", "domain", ".", "lower", "(", ")", "in", "[", "'booking'", "]", ":", "\n", "                ", "if", "cur_domain", "and", "cur_domain", "in", "entities", ":", "\n", "                    ", "domain", "=", "cur_domain", "\n", "", "else", ":", "\n", "                    ", "continue", "\n", "", "", "for", "pair", "in", "v", ":", "\n", "                ", "if", "pair", "[", "1", "]", "==", "'none'", ":", "\n", "                    ", "continue", "\n", "", "elif", "pair", "[", "0", "]", ".", "lower", "(", ")", "==", "'choice'", ":", "\n", "                    ", "pair", "[", "1", "]", "=", "str", "(", "len", "(", "entities", "[", "domain", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "n", "=", "int", "(", "pair", "[", "1", "]", ")", "-", "1", "\n", "if", "len", "(", "entities", "[", "domain", "]", ")", ">", "n", "and", "pair", "[", "0", "]", "in", "REF_SYS_DA", "[", "domain", "]", "and", "REF_SYS_DA", "[", "domain", "]", "[", "pair", "[", "0", "]", "]", "in", "entities", "[", "domain", "]", "[", "n", "]", ":", "\n", "                        ", "pair", "[", "1", "]", "=", "entities", "[", "domain", "]", "[", "n", "]", "[", "REF_SYS_DA", "[", "domain", "]", "[", "pair", "[", "0", "]", "]", "]", "\n", "", "else", ":", "\n", "                        ", "pair", "[", "1", "]", "=", "'none'", "\n", "", "", "", "", "", "tuples", "=", "[", "]", "\n", "for", "domain_intent", ",", "svs", "in", "meta", ".", "items", "(", ")", ":", "\n", "        ", "for", "slot", ",", "value", "in", "svs", ":", "\n", "            ", "domain", ",", "intent", "=", "domain_intent", ".", "split", "(", "'-'", ")", "\n", "tuples", ".", "append", "(", "[", "intent", ",", "domain", ",", "slot", ",", "value", "]", ")", "\n", "", "", "return", "tuples", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.asr_span_detection.translateNumberToEnglish": [[9, 35], ["str().isnumeric", "str", "asr_span_detection.translateNumberToEnglish", "int", "len", "int", "int", "str", "str().split", "str", "str", "int", "len", "len", "len", "str", "str", "int", "str", "int", "asr_span_detection.getUnderThreeNumberString", "str", "int", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.getUnderThreeNumberString"], ["def", "translateNumberToEnglish", "(", "number", ")", ":", "\n", "    ", "if", "str", "(", "number", ")", ".", "isnumeric", "(", ")", ":", "\n", "        ", "if", "str", "(", "number", ")", "[", "0", "]", "==", "'0'", "and", "len", "(", "str", "(", "number", ")", ")", ">", "1", ":", "\n", "            ", "return", "translateNumberToEnglish", "(", "int", "(", "number", "[", "1", ":", "]", ")", ")", ";", "\n", "", "if", "int", "(", "number", ")", "<", "20", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", ")", "]", ";", "\n", "", "elif", "int", "(", "number", ")", "<", "100", ":", "\n", "            ", "if", "str", "(", "number", ")", "[", "1", "]", "==", "'0'", ":", "\n", "                ", "return", "IN_HUNDRED_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "0", "]", ")", "]", ";", "\n", "", "else", ":", "\n", "                ", "return", "IN_HUNDRED_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "0", "]", ")", "]", "+", "\" \"", "+", "NUMBER_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "1", "]", ")", "]", ";", "\n", "", "", "else", ":", "\n", "            ", "strNumber", "=", "str", "(", "number", ")", "\n", "numberArray", "=", "str", "(", "strNumber", ")", ".", "split", "(", "\",\"", ")", ";", "\n", "stringResult", "=", "\"\"", ";", "\n", "groupCount", "=", "len", "(", "numberArray", ")", "+", "1", ";", "\n", "for", "groupNumber", "in", "numberArray", ":", "\n", "                ", "if", "groupCount", ">", "1", "and", "groupNumber", "[", "0", ":", "]", "!=", "\"000\"", ":", "\n", "                    ", "stringResult", "+=", "str", "(", "getUnderThreeNumberString", "(", "str", "(", "groupNumber", ")", ")", ")", "+", "\" \"", ";", "\n", "", "else", ":", "\n", "                    ", "break", ";", "\n", "", "groupCount", "-=", "1", ";", "\n", "if", "groupCount", ">", "1", ":", "\n", "                    ", "stringResult", "+=", "BASE_CONSTANT", "[", "groupCount", "]", "+", "\" \"", ";", "\n", "", "", "endPoint", "=", "len", "(", "stringResult", ")", "-", "len", "(", "\" hundred,\"", ")", ";", "\n", "return", "stringResult", ";", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.asr_span_detection.getUnderThreeNumberString": [[36, 46], ["str().isnumeric", "len", "len", "asr_span_detection.translateNumberToEnglish", "str", "int", "len", "len", "asr_span_detection.translateNumberToEnglish", "int", "int"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish"], ["", "", "", "def", "getUnderThreeNumberString", "(", "number", ")", ":", "\n", "    ", "if", "str", "(", "number", ")", ".", "isnumeric", "(", ")", "and", "len", "(", "number", ")", "<", "4", ":", "\n", "        ", "if", "len", "(", "number", ")", "<", "3", ":", "\n", "            ", "return", "translateNumberToEnglish", "(", "int", "(", "number", ")", ")", ";", "\n", "", "elif", "len", "(", "number", ")", "==", "3", "and", "number", "[", "0", ":", "]", "==", "\"000\"", ":", "\n", "            ", "return", "\" \"", ";", "\n", "", "elif", "len", "(", "number", ")", "==", "3", "and", "number", "[", "1", ":", "]", "==", "\"00\"", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", "[", "0", "]", ")", "]", "+", "\"  \"", "+", "BASE_CONSTANT", "[", "1", "]", ";", "\n", "", "else", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", "[", "0", "]", ")", "]", "+", "\"  \"", "+", "BASE_CONSTANT", "[", "1", "]", "+", "\" and \"", "+", "translateNumberToEnglish", "(", "(", "number", "[", "1", ":", "]", ")", ")", ";", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.asr_span_detection.translateTimeToEnglish": [[47, 53], ["t.split.split", "asr_span_detection.translateNumberToEnglish", "asr_span_detection.translateNumberToEnglish", "asr_span_detection.translateNumberToEnglish"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish"], ["", "", "", "def", "translateTimeToEnglish", "(", "t", ")", ":", "\n", "    ", "t", "=", "t", ".", "split", "(", "':'", ")", "\n", "if", "t", "[", "1", "]", "!=", "'00'", ":", "\n", "      ", "return", "translateNumberToEnglish", "(", "t", "[", "0", "]", ")", "+", "' '", "+", "translateNumberToEnglish", "(", "t", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "      ", "return", "translateNumberToEnglish", "(", "t", "[", "0", "]", ")", "+", "' '", "+", "'o\\'clock'", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.asr_span_detection.span_typer": [[54, 63], ["s.split.isnumeric", "s.split.find", "s.split.split", "len", "s[].isnumeric", "s[].isnumeric"], "function", ["None"], ["", "", "def", "span_typer", "(", "s", ")", ":", "\n", "    ", "if", "s", ".", "isnumeric", "(", ")", ":", "\n", "        ", "return", "\"number\"", "\n", "", "if", "s", ".", "find", "(", "':'", ")", ">=", "0", ":", "\n", "        ", "s", "=", "s", ".", "split", "(", "':'", ")", "\n", "if", "len", "(", "s", ")", "==", "2", ":", "\n", "            ", "if", "s", "[", "0", "]", ".", "isnumeric", "(", ")", "and", "s", "[", "1", "]", ".", "isnumeric", "(", ")", ":", "\n", "                ", "return", "\"time\"", "\n", "", "", "", "return", "\"none\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.asr_span_detection.span_error_detect": [[65, 125], ["span_list[].lower", "asr_span_detection.span_typer", "asr_span_detection.translateTimeToEnglish", "asr_span_detection.translateNumberToEnglish", "new_text.find", "new_text.count", "new_text.find", "new_text.find", "new_text.count", "span_list[].lower.split", "new_text.split", "range", "range", "len", "new_text.find", "len", "len", "len", "max", "span_list[].lower.split", "len", "min", "range", "span_list[].lower.split", "span.split.index", "max", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.span_typer", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateTimeToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish"], ["", "def", "span_error_detect", "(", "original_text", ",", "new_text", ",", "span_list", ")", ":", "\n", "#input:original_text,new_text,one span_info [slot,slot,span,start,end]", "\n", "#output: is_span_changed? ,is_span_found? , new span_info [slot,slot,new span,new start,new end]", "\n", "    ", "span", "=", "span_list", "[", "2", "]", ".", "lower", "(", ")", "\n", "span_type", "=", "span_typer", "(", "span", ")", "\n", "if", "span_type", "==", "\"time\"", ":", "\n", "        ", "span2", "=", "translateTimeToEnglish", "(", "span", ")", "\n", "", "if", "span_type", "==", "\"number\"", ":", "\n", "        ", "span2", "=", "translateNumberToEnglish", "(", "span", ")", "\n", "", "if", "span_type", "==", "\"none\"", ":", "\n", "        ", "span2", "=", "span", "\n", "", "span_changed", ",", "span_found", "=", "0", ",", "0", "\n", "if", "new_text", ".", "find", "(", "span", ")", ">=", "0", ":", "\n", "        ", "span_changed", ",", "span_found", "=", "0", ",", "1", "\n", "span_start", "=", "new_text", ".", "count", "(", "' '", ",", "0", ",", "new_text", ".", "find", "(", "span", ")", ")", "\n", "span_end", "=", "span_start", "+", "len", "(", "span", ".", "split", "(", ")", ")", "-", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "span", ",", "span_start", ",", "span_end", "]", "\n", "", "elif", "new_text", ".", "find", "(", "span2", ")", ">=", "0", ":", "\n", "        ", "span_changed", ",", "span_found", "=", "1", ",", "1", "\n", "span", "=", "span2", "\n", "span_start", "=", "new_text", ".", "count", "(", "' '", ",", "0", ",", "new_text", ".", "find", "(", "span", ")", ")", "\n", "span_end", "=", "span_start", "+", "len", "(", "span", ".", "split", "(", ")", ")", "-", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "span", ",", "span_start", ",", "span_end", "]", "\n", "", "else", ":", "\n", "        ", "span", "=", "span2", "\n", "span_words", "=", "span", ".", "split", "(", ")", "\n", "new_words", "=", "new_text", ".", "split", "(", ")", "\n", "tag", "=", "[", "0", "]", "*", "len", "(", "new_words", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "new_words", ")", ")", ":", "\n", "            ", "if", "new_words", "[", "i", "]", "in", "span_words", ":", "\n", "                ", "tag", "[", "i", "]", "=", "1", "\n", "", "", "max_start", ",", "max_end", "=", "-", "1", ",", "-", "1", "\n", "max_match", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "len", "(", "new_words", ")", ")", ":", "\n", "            ", "if", "tag", "[", "i", "]", "==", "1", ":", "\n", "                ", "anchor", "=", "i", "\n", "start", "=", "i", "-", "span_words", ".", "index", "(", "new_words", "[", "anchor", "]", ")", "\n", "end", "=", "min", "(", "start", "+", "len", "(", "span_words", ")", "-", "1", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "match", "=", "0", "\n", "s", ",", "e", "=", "start", ",", "end", "\n", "while", "new_words", "[", "s", "]", "[", "0", "]", "!=", "span_words", "[", "s", "-", "start", "]", "[", "0", "]", "and", "s", "<", "anchor", ":", "\n", "                    ", "s", "+=", "1", "\n", "", "while", "new_words", "[", "e", "]", "[", "0", "]", "!=", "span_words", "[", "e", "-", "start", "]", "[", "0", "]", "and", "e", ">", "anchor", ":", "\n", "                    ", "e", "-=", "1", "\n", "", "for", "j", "in", "range", "(", "s", ",", "e", "+", "1", ")", ":", "\n", "                    ", "if", "tag", "[", "j", "]", "==", "1", ":", "\n", "                        ", "match", "+=", "1", "\n", "", "elif", "new_words", "[", "j", "]", "[", "0", "]", "==", "span_words", "[", "j", "-", "start", "]", "[", "0", "]", ":", "\n", "                        ", "match", "+=", "0.5", "\n", "", "", "if", "match", ">=", "max", "(", "len", "(", "span_words", ")", "/", "2", ",", "1", ",", "max_match", ")", ":", "\n", "                    ", "max_match", "=", "match", "\n", "max_start", ",", "max_end", "=", "s", ",", "e", "\n", "", "", "", "if", "max_match", ">=", "max", "(", "len", "(", "span_words", ")", "-", "1", ",", "1", ")", ":", "\n", "            ", "span_changed", ",", "span_found", "=", "1", ",", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", "\n", "\n", "\n", "", "", "if", "span_found", "==", "0", ":", "\n", "        ", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "span_list", "[", "2", "]", ",", "\"not found\"", ",", "\"not found\"", "]", "\n", "", "return", "span_changed", ",", "span_found", ",", "new_span_list", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.digit_normalize": [[21, 26], ["enumerate"], "function", ["None"], ["def", "digit_normalize", "(", "utt_list", ")", ":", "\n", "    ", "for", "i", ",", "text", "in", "enumerate", "(", "utt_list", ")", ":", "\n", "        ", "if", "text", "in", "word2digit", ":", "\n", "            ", "utt_list", "[", "i", "]", "=", "word2digit", "[", "text", "]", "\n", "", "", "return", "utt_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.phrase_idx_utt": [[27, 39], ["paraphrase_span_detection.digit_normalize", "len", "range", "fuzzywuzzy.fuzz.ratio", "candidates.append", "sorted", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.digit_normalize"], ["", "def", "phrase_idx_utt", "(", "value_list", ",", "utt_list", ",", "constraint", "=", "[", "]", ")", ":", "\n", "    ", "value_list", "=", "digit_normalize", "(", "value_list", ")", "\n", "utt_list", "=", "digit_normalize", "(", "utt_list", ")", "\n", "candidates", "=", "[", "]", "\n", "l", "=", "len", "(", "value_list", ")", "\n", "for", "i", "in", "[", "l", ",", "l", "-", "1", ",", "l", "+", "1", "]", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "continue", "\n", "", "for", "j", "in", "range", "(", "len", "(", "utt_list", ")", "-", "i", "+", "1", ")", ":", "\n", "            ", "if", "constraint", "and", "j", "<=", "constraint", "[", "0", "]", "and", "constraint", "[", "0", "]", "<", "j", "+", "i", ":", "\n", "                ", "if", "j", "==", "constraint", "[", "0", "]", ":", "\n", "                    ", "constraint", ".", "append", "(", "constraint", ".", "pop", "(", "0", ")", ")", "\n", "", "continue", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.preprocess": [[40, 73], ["nlp.disable_pipes", "dict", "da.items", "nlp", "len", "[].lower", "slots.append", "paraphrase_span_detection.phrase_idx_utt", "key.split", "nlp", "slot.lower", "range", "range", "len", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.phrase_idx_utt"], ["", "score", "=", "fuzz", ".", "ratio", "(", "' '", ".", "join", "(", "utt_list", "[", "j", ":", "j", "+", "i", "]", ")", ",", "' '", ".", "join", "(", "value_list", ")", ")", "\n", "if", "score", ">", "threshold", ":", "\n", "                ", "candidates", ".", "append", "(", "(", "score", ",", "j", ",", "j", "+", "i", "-", "1", ")", ")", "\n", "", "", "", "return", "sorted", "(", "candidates", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", "0", "]", "[", "1", ":", "]", "if", "candidates", "else", "None", "\n", "\n", "", "def", "preprocess", "(", "utt", ",", "da", ")", ":", "\n", "    ", "'''\n    utt: str\n    da: dict {'domain-intent': [slot, value]}\n    '''", "\n", "with", "nlp", ".", "disable_pipes", "(", "'tagger'", ",", "'parser'", ")", ":", "\n", "        ", "'''\n        get tokens, recover the paraphrased entity \n        '''", "\n", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "nlp", "(", "utt", ")", "]", "\n", "for", "key", ",", "pair", "in", "da", ".", "items", "(", ")", ":", "\n", "            ", "constraint", "=", "[", "]", "\n", "intent", "=", "key", ".", "split", "(", "'-'", ")", "[", "1", "]", ".", "lower", "(", ")", "\n", "if", "intent", "not", "in", "[", "'inform'", "]", ":", "\n", "                ", "continue", "\n", "", "for", "slot", ",", "value", "in", "pair", ":", "\n", "                ", "if", "slot", ".", "lower", "(", ")", "in", "[", "'name'", ",", "'dest'", ",", "'depart'", "]", ":", "\n", "                    ", "value_tokens", "=", "[", "token", ".", "text", "for", "token", "in", "nlp", "(", "value", ")", "]", "\n", "span", "=", "phrase_idx_utt", "(", "value_tokens", ",", "tokens", ",", "constraint", ")", "\n", "if", "span", "is", "not", "None", ":", "\n", "                        ", "for", "i", "in", "range", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", "+", "1", ")", ":", "\n", "                            ", "constraint", ".", "append", "(", "i", ")", "\n", "", "tokens", "[", "span", "[", "0", "]", ":", "span", "[", "1", "]", "+", "1", "]", "=", "value_tokens", "\n", "\n", "", "", "", "", "'''\n        get labels, tag or slot or none\n        '''", "\n", "labels", "=", "dict", "(", ")", "\n", "for", "key", ",", "pair", "in", "da", ".", "items", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.dbquery.Database.__init__": [[12, 22], ["object.__init__", "open", "json.load", "os.path.join", "os.path.dirname", "os.path.dirname", "os.path.dirname", "os.path.dirname", "os.path.abspath"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Database", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# loading databases", "\n", "domains", "=", "[", "'restaurant'", ",", "'hotel'", ",", "'attraction'", ",", "'train'", ",", "'hospital'", ",", "'taxi'", ",", "'police'", "]", "\n", "self", ".", "dbs", "=", "{", "}", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "\n", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", ")", ")", ")", ",", "\n", "'data/multiwoz/db/{}_db.json'", ".", "format", "(", "domain", ")", ")", ")", "as", "f", ":", "\n", "                ", "self", ".", "dbs", "[", "domain", "]", "=", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.dbquery.Database.query": [[23, 87], ["list", "enumerate", "copy.deepcopy", "map", "zip", "zip", "itertools.chain", "copy.deepcopy", "copy.deepcopy", "found.append", "random.choice", "random.choice", "copy.deepcopy", "len", "len", "str", "x[].lower", "department.strip().lower", "k.lower", "key.lower", "random.randint", "range", "int", "int", "department.strip", "int", "int", "int", "int", "val.split", "record[].split", "int", "int", "record[].strip", "val.split", "record[].split", "val.split", "record[].split", "val.split", "record[].split", "val.strip().lower", "record[].strip().lower", "fuzzywuzzy.fuzz.partial_ratio", "val.strip().lower", "record[].strip().lower", "val.strip", "record[].strip", "val.strip", "record[].strip"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["", "", "", "def", "query", "(", "self", ",", "domain", ",", "constraints", ",", "ignore_open", "=", "False", ",", "soft_contraints", "=", "(", ")", ",", "fuzzy_match_ratio", "=", "60", ")", ":", "\n", "        ", "\"\"\"Returns the list of entities for a given domain\n        based on the annotation of the belief state\"\"\"", "\n", "# query the db", "\n", "if", "domain", "==", "'taxi'", ":", "\n", "            ", "return", "[", "{", "'taxi_colors'", ":", "random", ".", "choice", "(", "self", ".", "dbs", "[", "domain", "]", "[", "'taxi_colors'", "]", ")", ",", "\n", "'taxi_types'", ":", "random", ".", "choice", "(", "self", ".", "dbs", "[", "domain", "]", "[", "'taxi_types'", "]", ")", ",", "\n", "'taxi_phone'", ":", "''", ".", "join", "(", "[", "str", "(", "random", ".", "randint", "(", "1", ",", "9", ")", ")", "for", "_", "in", "range", "(", "11", ")", "]", ")", "}", "]", "\n", "", "if", "domain", "==", "'police'", ":", "\n", "            ", "return", "deepcopy", "(", "self", ".", "dbs", "[", "'police'", "]", ")", "\n", "", "if", "domain", "==", "'hospital'", ":", "\n", "            ", "department", "=", "None", "\n", "for", "key", ",", "val", "in", "constraints", ":", "\n", "                ", "if", "key", "==", "'department'", ":", "\n", "                    ", "department", "=", "val", "\n", "", "", "if", "not", "department", ":", "\n", "                ", "return", "deepcopy", "(", "self", ".", "dbs", "[", "'hospital'", "]", ")", "\n", "", "else", ":", "\n", "                ", "return", "[", "deepcopy", "(", "x", ")", "for", "x", "in", "self", ".", "dbs", "[", "'hospital'", "]", "if", "x", "[", "'department'", "]", ".", "lower", "(", ")", "==", "department", ".", "strip", "(", ")", ".", "lower", "(", ")", "]", "\n", "", "", "constraints", "=", "list", "(", "map", "(", "lambda", "ele", ":", "ele", "if", "not", "(", "ele", "[", "0", "]", "==", "'area'", "and", "ele", "[", "1", "]", "==", "'center'", ")", "else", "(", "'area'", ",", "'centre'", ")", ",", "constraints", ")", ")", "\n", "\n", "found", "=", "[", "]", "\n", "for", "i", ",", "record", "in", "enumerate", "(", "self", ".", "dbs", "[", "domain", "]", ")", ":", "\n", "            ", "constraints_iterator", "=", "zip", "(", "constraints", ",", "[", "False", "]", "*", "len", "(", "constraints", ")", ")", "\n", "soft_contraints_iterator", "=", "zip", "(", "soft_contraints", ",", "[", "True", "]", "*", "len", "(", "soft_contraints", ")", ")", "\n", "for", "(", "key", ",", "val", ")", ",", "fuzzy_match", "in", "chain", "(", "constraints_iterator", ",", "soft_contraints_iterator", ")", ":", "\n", "                ", "if", "val", "==", "\"\"", "or", "val", "==", "\"dont care\"", "or", "val", "==", "'not mentioned'", "or", "val", "==", "\"don't care\"", "or", "val", "==", "\"dontcare\"", "or", "val", "==", "\"do n't care\"", ":", "\n", "                    ", "pass", "\n", "", "else", ":", "\n", "                    ", "try", ":", "\n", "                        ", "record_keys", "=", "[", "k", ".", "lower", "(", ")", "for", "k", "in", "record", "]", "\n", "if", "key", ".", "lower", "(", ")", "not", "in", "record_keys", ":", "\n", "                            ", "continue", "\n", "", "if", "key", "==", "'leaveAt'", ":", "\n", "                            ", "val1", "=", "int", "(", "val", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "*", "100", "+", "int", "(", "val", ".", "split", "(", "':'", ")", "[", "1", "]", ")", "\n", "val2", "=", "int", "(", "record", "[", "'leaveAt'", "]", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "*", "100", "+", "int", "(", "record", "[", "'leaveAt'", "]", ".", "split", "(", "':'", ")", "[", "1", "]", ")", "\n", "if", "val1", ">", "val2", ":", "\n", "                                ", "break", "\n", "", "", "elif", "key", "==", "'arriveBy'", ":", "\n", "                            ", "val1", "=", "int", "(", "val", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "*", "100", "+", "int", "(", "val", ".", "split", "(", "':'", ")", "[", "1", "]", ")", "\n", "val2", "=", "int", "(", "record", "[", "'arriveBy'", "]", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "*", "100", "+", "int", "(", "record", "[", "'arriveBy'", "]", ".", "split", "(", "':'", ")", "[", "1", "]", ")", "\n", "if", "val1", "<", "val2", ":", "\n", "                                ", "break", "\n", "# elif ignore_open and key in ['destination', 'departure', 'name']:", "\n", "", "", "elif", "ignore_open", "and", "key", "in", "[", "'destination'", ",", "'departure'", "]", ":", "\n", "                            ", "continue", "\n", "", "elif", "record", "[", "key", "]", ".", "strip", "(", ")", "==", "'?'", ":", "\n", "# '?' matches any constraint", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "                            ", "if", "not", "fuzzy_match", ":", "\n", "                                ", "if", "val", ".", "strip", "(", ")", ".", "lower", "(", ")", "!=", "record", "[", "key", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", ":", "\n", "                                    ", "break", "\n", "", "", "else", ":", "\n", "                                ", "if", "fuzz", ".", "partial_ratio", "(", "val", ".", "strip", "(", ")", ".", "lower", "(", ")", ",", "record", "[", "key", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", ")", "<", "fuzzy_match_ratio", ":", "\n", "                                    ", "break", "\n", "", "", "", "", "except", ":", "\n", "                        ", "continue", "\n", "", "", "", "else", ":", "\n", "                ", "res", "=", "deepcopy", "(", "record", ")", "\n", "res", "[", "'Ref'", "]", "=", "'{0:08d}'", ".", "format", "(", "i", ")", "\n", "found", ".", "append", "(", "res", ")", "\n", "\n", "", "", "return", "found", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.span_detection.span_detect": [[5, 86], ["span_list[].lower", "detection_utils.replacer", "detection_utils.span_typer", "new_text.split", "detection_utils.translateTimeToEnglish", "detection_utils.translateNumberToEnglish", "new_text.find", "new_text.count", "new_text.find", "new_text.find", "new_text.count", "detection_utils.replacer.split", "paraphrase_span_detection.phrase_idx_utt", "len", "new_text.find", "original_text.split", "detection_utils.replacer.split", "len", "range", "detection_utils.replacer.split", "len", "len", "len", "len", "len", "range", "len", "range", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.replacer", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.span_typer", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateTimeToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.phrase_idx_utt"], ["def", "span_detect", "(", "original_text", ",", "new_text", ",", "span_list", ")", ":", "\n", "#input:original_text,new_text,one span_info [slot,slot,span,start,end]", "\n", "#output:is_span_found? , is_span_changed? , new span_info [slot,slot,new span,new start,new end]", "\n", "    ", "span", "=", "span_list", "[", "2", "]", ".", "lower", "(", ")", "\n", "span", "=", "replacer", "(", "span", ")", "\n", "span_type", "=", "span_typer", "(", "span", ")", "\n", "new_words", "=", "new_text", ".", "split", "(", ")", "\n", "if", "span_type", "==", "\"time\"", ":", "\n", "        ", "span2", "=", "translateTimeToEnglish", "(", "span", ")", "\n", "", "if", "span_type", "==", "\"number\"", ":", "\n", "        ", "span2", "=", "translateNumberToEnglish", "(", "span", ")", "\n", "", "if", "span_type", "==", "\"none\"", ":", "\n", "        ", "span2", "=", "span", "\n", "", "span_changed", ",", "span_found", "=", "0", ",", "0", "\n", "if", "new_text", ".", "find", "(", "span", ")", ">=", "0", ":", "\n", "        ", "span_changed", ",", "span_found", "=", "0", ",", "1", "\n", "span_start", "=", "new_text", ".", "count", "(", "' '", ",", "0", ",", "new_text", ".", "find", "(", "span", ")", ")", "\n", "span_end", "=", "span_start", "+", "len", "(", "span", ".", "split", "(", ")", ")", "-", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "span_start", ":", "span_end", "+", "1", "]", ")", ",", "span_start", ",", "span_end", "]", "\n", "", "elif", "new_text", ".", "find", "(", "span2", ")", ">=", "0", ":", "\n", "        ", "span_changed", ",", "span_found", "=", "1", ",", "1", "\n", "span", "=", "span2", "\n", "span_start", "=", "new_text", ".", "count", "(", "' '", ",", "0", ",", "new_text", ".", "find", "(", "span", ")", ")", "\n", "span_end", "=", "span_start", "+", "len", "(", "span", ".", "split", "(", ")", ")", "-", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "span_start", ":", "span_end", "+", "1", "]", ")", ",", "span_start", ",", "span_end", "]", "\n", "", "else", ":", "\n", "        ", "span", "=", "span2", "\n", "span_words", "=", "span", ".", "split", "(", ")", "\n", "\n", "result", "=", "phrase_idx_utt", "(", "span_words", ",", "new_words", ")", "\n", "if", "result", "is", "not", "None", ":", "\n", "            ", "max_start", ",", "max_end", "=", "result", "\n", "span_changed", ",", "span_found", "=", "1", ",", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", "\n", "", "else", ":", "\n", "            ", "origin_split", "=", "original_text", ".", "split", "(", ")", "\n", "new_split", "=", "new_words", "\n", "ok", "=", "0", "\n", "origin_start", "=", "span_list", "[", "3", "]", "-", "1", "\n", "if", "origin_start", ">=", "0", ":", "\n", "                ", "if", "origin_start", "-", "1", ">=", "0", "and", "origin_split", "[", "origin_start", "]", "in", "[", "'.'", ",", "','", ",", "'?'", "]", ":", "\n", "                    ", "origin_start", "-=", "1", "\n", "", "start_word", "=", "origin_split", "[", "origin_start", "]", "\n", "for", "start", "in", "range", "(", "len", "(", "new_split", ")", ")", ":", "\n", "                    ", "if", "new_split", "[", "start", "]", "==", "start_word", ":", "\n", "                        ", "break", "\n", "", "", "start", "+=", "1", "\n", "", "else", ":", "\n", "                ", "start", "=", "0", "\n", "", "if", "span_list", "[", "4", "]", "+", "1", "<", "len", "(", "origin_split", ")", "and", "start", "<", "len", "(", "new_split", ")", ":", "\n", "                ", "end_word", "=", "origin_split", "[", "span_list", "[", "4", "]", "+", "1", "]", "\n", "if", "end_word", "not", "in", "[", "'.'", ",", "','", ",", "'?'", "]", ":", "\n", "                    ", "if", "span_list", "[", "4", "]", "+", "1", "<", "len", "(", "origin_split", ")", ":", "\n", "                        ", "end_word", "=", "origin_split", "[", "span_list", "[", "4", "]", "+", "1", "]", "\n", "for", "end", "in", "range", "(", "start", ",", "len", "(", "new_split", ")", ")", ":", "\n", "                            ", "if", "new_split", "[", "end", "]", "==", "end_word", ":", "\n", "                                ", "ok", "=", "1", "\n", "break", "\n", "", "", "end", "-=", "1", "\n", "\n", "", "", "else", ":", "\n", "                    ", "if", "span_list", "[", "4", "]", "+", "2", "<", "len", "(", "origin_split", ")", ":", "\n", "                        ", "end_word", "=", "origin_split", "[", "span_list", "[", "4", "]", "+", "2", "]", "\n", "for", "end", "in", "range", "(", "start", ",", "len", "(", "new_split", ")", ")", ":", "\n", "                            ", "if", "new_split", "[", "end", "]", "==", "end_word", ":", "\n", "                                ", "ok", "=", "1", "\n", "break", "\n", "", "", "end", "-=", "1", "\n", "", "else", ":", "\n", "                        ", "ok", "=", "1", "\n", "end", "=", "len", "(", "new_split", ")", "-", "1", "\n", "", "", "", "else", ":", "\n", "                ", "ok", "=", "1", "\n", "end", "=", "len", "(", "new_split", ")", "-", "1", "\n", "", "if", "start", "<=", "end", "and", "ok", "==", "1", ":", "\n", "                ", "span_changed", ",", "span_found", "=", "1", ",", "1", "\n", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "start", ":", "end", "+", "1", "]", ")", ",", "start", ",", "end", "]", "\n", "\n", "", "", "", "if", "span_found", "==", "0", ":", "\n", "        ", "new_span_list", "=", "[", "span_list", "[", "0", "]", ",", "span_list", "[", "1", "]", ",", "span_list", "[", "2", "]", ",", "0", ",", "0", "]", "\n", "", "return", "new_span_list", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish": [[10, 42], ["str().isnumeric", "print", "str", "detection_utils.translateNumberToEnglish", "int", "len", "int", "int", "str", "str().split", "str", "str", "int", "len", "len", "len", "str", "str", "int", "str", "int", "detection_utils.getUnderThreeNumberString", "str", "int", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.getUnderThreeNumberString"], ["def", "translateNumberToEnglish", "(", "number", ")", ":", "\n", "    ", "if", "str", "(", "number", ")", ".", "isnumeric", "(", ")", ":", "\n", "        ", "if", "str", "(", "number", ")", "[", "0", "]", "==", "'0'", "and", "len", "(", "str", "(", "number", ")", ")", ">", "1", ":", "\n", "            ", "return", "translateNumberToEnglish", "(", "int", "(", "number", "[", "1", ":", "]", ")", ")", ";", "\n", "", "if", "int", "(", "number", ")", "<", "20", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", ")", "]", ";", "\n", "", "elif", "int", "(", "number", ")", "<", "100", ":", "\n", "            ", "if", "str", "(", "number", ")", "[", "1", "]", "==", "'0'", ":", "\n", "                ", "return", "IN_HUNDRED_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "0", "]", ")", "]", ";", "\n", "", "else", ":", "\n", "                ", "return", "IN_HUNDRED_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "0", "]", ")", "]", "+", "\" \"", "+", "NUMBER_CONSTANT", "[", "int", "(", "str", "(", "number", ")", "[", "1", "]", ")", "]", ";", "\n", "", "", "else", ":", "\n", "#locale.setlocale(locale.LC_ALL, \"English_United States.1252\");", "\n", "#strNumber = locale.format(\"%d\"    , number, grouping=True);", "\n", "            ", "strNumber", "=", "str", "(", "number", ")", "\n", "numberArray", "=", "str", "(", "strNumber", ")", ".", "split", "(", "\",\"", ")", ";", "\n", "stringResult", "=", "\"\"", ";", "\n", "groupCount", "=", "len", "(", "numberArray", ")", "+", "1", ";", "\n", "for", "groupNumber", "in", "numberArray", ":", "\n", "                ", "if", "groupCount", ">", "1", "and", "groupNumber", "[", "0", ":", "]", "!=", "\"000\"", ":", "\n", "                    ", "stringResult", "+=", "str", "(", "getUnderThreeNumberString", "(", "str", "(", "groupNumber", ")", ")", ")", "+", "\" \"", ";", "\n", "", "else", ":", "\n", "                    ", "break", ";", "\n", "", "groupCount", "-=", "1", ";", "\n", "if", "groupCount", ">", "1", ":", "\n", "                    ", "stringResult", "+=", "BASE_CONSTANT", "[", "groupCount", "]", "+", "\" \"", ";", "\n", "", "", "endPoint", "=", "len", "(", "stringResult", ")", "-", "len", "(", "\" hundred,\"", ")", ";", "\n", "#return stringResult[0:endPoint];", "\n", "return", "stringResult", ";", "\n", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"please input a number!\"", ")", ";", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.getUnderThreeNumberString": [[44, 54], ["str().isnumeric", "len", "len", "detection_utils.translateNumberToEnglish", "str", "int", "len", "len", "detection_utils.translateNumberToEnglish", "int", "int"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish"], ["", "", "def", "getUnderThreeNumberString", "(", "number", ")", ":", "\n", "    ", "if", "str", "(", "number", ")", ".", "isnumeric", "(", ")", "and", "len", "(", "number", ")", "<", "4", ":", "\n", "        ", "if", "len", "(", "number", ")", "<", "3", ":", "\n", "            ", "return", "translateNumberToEnglish", "(", "int", "(", "number", ")", ")", ";", "\n", "", "elif", "len", "(", "number", ")", "==", "3", "and", "number", "[", "0", ":", "]", "==", "\"000\"", ":", "\n", "            ", "return", "\" \"", ";", "\n", "", "elif", "len", "(", "number", ")", "==", "3", "and", "number", "[", "1", ":", "]", "==", "\"00\"", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", "[", "0", "]", ")", "]", "+", "\"  \"", "+", "BASE_CONSTANT", "[", "1", "]", ";", "\n", "", "else", ":", "\n", "            ", "return", "NUMBER_CONSTANT", "[", "int", "(", "number", "[", "0", "]", ")", "]", "+", "\"  \"", "+", "BASE_CONSTANT", "[", "1", "]", "+", "\" and \"", "+", "translateNumberToEnglish", "(", "(", "number", "[", "1", ":", "]", ")", ")", ";", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateTimeToEnglish": [[55, 61], ["t.split.split", "detection_utils.translateNumberToEnglish", "detection_utils.translateNumberToEnglish", "detection_utils.translateNumberToEnglish"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.translateNumberToEnglish"], ["", "", "", "def", "translateTimeToEnglish", "(", "t", ")", ":", "\n", "    ", "t", "=", "t", ".", "split", "(", "':'", ")", "\n", "if", "t", "[", "1", "]", "!=", "'00'", ":", "\n", "      ", "return", "translateNumberToEnglish", "(", "t", "[", "0", "]", ")", "+", "' '", "+", "translateNumberToEnglish", "(", "t", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "      ", "return", "translateNumberToEnglish", "(", "t", "[", "0", "]", ")", "+", "' '", "+", "'o\\'clock'", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.span_typer": [[62, 71], ["s.split.isnumeric", "s.split.find", "s.split.split", "len", "s[].isnumeric", "s[].isnumeric"], "function", ["None"], ["", "", "def", "span_typer", "(", "s", ")", ":", "\n", "    ", "if", "s", ".", "isnumeric", "(", ")", ":", "\n", "        ", "return", "\"number\"", "\n", "", "if", "s", ".", "find", "(", "':'", ")", ">=", "0", ":", "\n", "        ", "s", "=", "s", ".", "split", "(", "':'", ")", "\n", "if", "len", "(", "s", ")", "==", "2", ":", "\n", "            ", "if", "s", "[", "0", "]", ".", "isnumeric", "(", ")", "and", "s", "[", "1", "]", ".", "isnumeric", "(", ")", ":", "\n", "                ", "return", "\"time\"", "\n", "", "", "", "return", "\"none\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.detection_utils.replacer": [[72, 79], ["s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace"], "function", ["None"], ["", "def", "replacer", "(", "s", ")", ":", "\n", "    ", "s", "=", "s", ".", "replace", "(", "' n\\'t'", ",", "'n\\'t'", ")", "\n", "s", "=", "s", ".", "replace", "(", "' \\'ll'", ",", "'\\'ll'", ")", "\n", "s", "=", "s", ".", "replace", "(", "'centre'", ",", "'center'", ")", "\n", "s", "=", "s", ".", "replace", "(", "'-star'", ",", "' star'", ")", "\n", "s", "=", "s", ".", "replace", "(", "'guesthouse'", ",", "'guest house'", ")", "\n", "return", "s", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.__init__": [[11, 25], ["dict", "util.Helper"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "multiwoz", ":", "MultiwozDatasetType", ",", "\n", "db_loader", ":", "MultiSourceDBLoader", ",", "\n", "inform_intents", "=", "(", "'inform'", ",", ")", ",", "\n", "slot_value_replacement_probability", "=", "0.25", ",", "\n", "alpha_sr", "=", "0.1", ",", "alpha_ri", "=", "0.1", ",", "alpha_rs", "=", "0.1", ",", "p_rd", "=", "0.1", ",", "num_aug", "=", "2", ")", ":", "\n", "# attributes for slot value replacement", "\n", "        ", "self", ".", "db_loader", "=", "db_loader", "\n", "self", ".", "inform_intents", "=", "inform_intents", "\n", "self", ".", "slot_value_replacement_probability", "=", "slot_value_replacement_probability", "\n", "\n", "# attributes for EDA.", "\n", "self", ".", "eda_config", "=", "dict", "(", "alpha_sr", "=", "alpha_sr", ",", "alpha_ri", "=", "alpha_ri", ",", "alpha_rs", "=", "alpha_rs", ",", "p_rd", "=", "p_rd", ",", "num_aug", "=", "num_aug", ")", "\n", "self", ".", "multiwoz", "=", "multiwoz", "\n", "self", ".", "helper", "=", "Helper", "(", "multiwoz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._get_excluding_indexes": [[26, 28], ["multiwoz_eda.MultiwozEDA.helper._get_excluding_indexes"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper._get_excluding_indexes"], ["", "def", "_get_excluding_indexes", "(", "self", ",", "words", ",", "span_info", ",", "dialog_act", ")", ":", "\n", "        ", "return", "self", ".", "helper", ".", "_get_excluding_indexes", "(", "words", ",", "span_info", ",", "dialog_act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._augment_sentence_only": [[29, 39], ["tokenize_util.convert_sentence_to_tokens", "multiwoz_eda.MultiwozEDA._get_excluding_indexes", "task_oriented_eda.eda", "new_span_info.append", "tokenize_util.convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.convert_sentence_to_tokens", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper._get_excluding_indexes", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.eda", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.convert_tokens_to_string"], ["", "def", "_augment_sentence_only", "(", "self", ",", "sentence", ":", "SentenceType", ",", "span_info", ",", "dialog_act", ")", ":", "\n", "        ", "\"\"\"don't change DA (span indexes may change)\"\"\"", "\n", "words", "=", "convert_sentence_to_tokens", "(", "sentence", ")", "\n", "excluding_indexes", "=", "self", ".", "_get_excluding_indexes", "(", "words", ",", "span_info", ",", "dialog_act", ")", "\n", "\n", "for", "new_words", ",", "index_map", "in", "eda", "(", "words", ",", "**", "self", ".", "eda_config", ",", "excluding_indexes", "=", "excluding_indexes", ")", ":", "\n", "            ", "new_span_info", "=", "[", "]", "\n", "for", "x", "in", "span_info", ":", "\n", "                ", "new_span_info", ".", "append", "(", "[", "*", "x", "[", ":", "3", "]", ",", "index_map", "[", "x", "[", "3", "]", "]", ",", "index_map", "[", "x", "[", "4", "]", "]", "]", ")", "\n", "", "yield", "convert_tokens_to_string", "(", "new_words", ")", ",", "new_span_info", ",", "dialog_act", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_sentence_only": [[40, 42], ["list", "multiwoz_eda.MultiwozEDA._augment_sentence_only"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._augment_sentence_only"], ["", "", "def", "augment_sentence_only", "(", "self", ",", "sentence", ":", "SentenceType", ",", "span_info", ",", "dialog_act", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "_augment_sentence_only", "(", "sentence", ",", "span_info", ",", "dialog_act", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._augment_sample": [[43, 88], ["util.AugmentationRecorder", "util.iter_dialogues", "deepcopy", "db.slot_value_replace.replace_slot_values_in_turn", "util.is_span_info_consistent_with_text", "db.slot_value_replace.assert_correct_turn", "tokenize_util.tokenize", "util.choice", "util.is_span_info_consistent_with_text", "util.p_str", "util.AugmentationRecorder.add_augmented_dialog", "multiwoz_eda.MultiwozEDA._augment_sentence_only", "turn.items"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.iter_dialogues", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.replace_slot_values_in_turn", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.is_span_info_consistent_with_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.assert_correct_turn", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.is_span_info_consistent_with_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.AugmentationRecorder.add_augmented_dialog", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._augment_sentence_only"], ["", "def", "_augment_sample", "(", "self", ",", "sample", ":", "MultiwozSampleType", ",", "mode", "=", "'usr'", ")", "->", "AugmentationRecorder", ":", "\n", "        ", "recorder", "=", "AugmentationRecorder", "(", "sample", ")", "\n", "\n", "for", "turn_index", ",", "turn", "in", "iter_dialogues", "(", "sample", ",", "mode", "=", "mode", ")", ":", "\n", "            ", "if", "not", "is_span_info_consistent_with_text", "(", "turn", "[", "'text'", "]", ",", "turn", "[", "'span_info'", "]", ")", ":", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "assert_correct_turn", "(", "turn", ")", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "", "from", "copy", "import", "deepcopy", "\n", "orig_turn", "=", "deepcopy", "(", "turn", ")", "\n", "new_turn", "=", "replace_slot_values_in_turn", "(", "\n", "turn", ",", "\n", "self", ".", "db_loader", ",", "\n", "p", "=", "self", ".", "slot_value_replacement_probability", ",", "\n", "inform_intents", "=", "self", ".", "inform_intents", "\n", ")", "\n", "augmented", "=", "new_turn", "!=", "turn", "\n", "turn", "=", "new_turn", "\n", "\n", "try", ":", "\n", "                ", "text", "=", "turn", "[", "'text'", "]", "\n", "span_info", "=", "turn", "[", "'span_info'", "]", "\n", "dialog_act", "=", "turn", "[", "'dialog_act'", "]", "\n", "tokens", "=", "tokenize", "(", "text", ")", "\n", "augmented_sentence", ",", "augmented_span_info", ",", "augmented_dialog_act", "=", "choice", "(", "\n", "self", ".", "_augment_sentence_only", "(", "tokens", ",", "span_info", ",", "dialog_act", ")", "\n", ")", "\n", "", "except", "(", "ValueError", ",", "IndexError", ")", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "assert", "is_span_info_consistent_with_text", "(", "augmented_sentence", ",", "augmented_span_info", ")", ",", "p_str", "(", "\n", "[", "orig_turn", ",", "turn", "]", ")", "\n", "augmented", "=", "True", "\n", "turn", "=", "{", "\n", "'text'", ":", "augmented_sentence", ",", "\n", "'span_info'", ":", "augmented_span_info", ",", "\n", "'dialog_act'", ":", "augmented_dialog_act", ",", "\n", "**", "{", "k", ":", "v", "for", "k", ",", "v", "in", "turn", ".", "items", "(", ")", "if", "k", "not", "in", "(", "'text'", ",", "'span_info'", ",", "'dialog_act'", ")", "}", "\n", "}", "\n", "\n", "", "if", "augmented", ":", "\n", "                ", "recorder", ".", "add_augmented_dialog", "(", "turn_index", ",", "turn", ")", "\n", "", "", "return", "recorder", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_sample": [[89, 91], ["multiwoz_eda.MultiwozEDA._augment_sample().get_augmented_sample", "multiwoz_eda.MultiwozEDA._augment_sample"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.AugmentationRecorder.get_augmented_sample", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA._augment_sample"], ["", "def", "augment_sample", "(", "self", ",", "sample", ":", "MultiwozSampleType", ",", "mode", "=", "'usr'", ")", "->", "MultiwozSampleType", ":", "\n", "        ", "return", "self", ".", "_augment_sample", "(", "sample", ",", "mode", "=", "mode", ")", ".", "get_augmented_sample", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_multiwoz_dataset": [[94, 104], ["tqdm.tqdm", "multiwoz_eda.MultiwozEDA.multiwoz.items", "multiwoz_eda.MultiwozEDA.augment_sample", "multiwoz_eda.MultiwozEDA.multiwoz.items", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_sample"], ["def", "augment_multiwoz_dataset", "(", "self", ",", "mode", "=", "'usr'", ",", "progress_bar", "=", "True", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'usr'", ",", "'user'", ",", "'sys'", ",", "'all'", ")", "\n", "res", "=", "{", "}", "\n", "if", "progress_bar", ":", "\n", "            ", "items", "=", "tqdm", ".", "tqdm", "(", "self", ".", "multiwoz", ".", "items", "(", ")", ",", "total", "=", "len", "(", "self", ".", "multiwoz", ")", ")", "\n", "", "else", ":", "\n", "            ", "items", "=", "self", ".", "multiwoz", ".", "items", "(", ")", "\n", "", "for", "sample_id", ",", "sample", "in", "items", ":", "\n", "            ", "res", "[", "sample_id", "]", "=", "self", ".", "augment_sample", "(", "sample", ",", "mode", "=", "mode", ")", "\n", "", "return", "res", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.aug_with_sgd_db.multiwoz_eda_config.__init__": [[15, 53], ["aug_with_sgd_db.read_zipped_json", "os.path.join", "os.path.join", "loader_args.append", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoader", "os.path.join", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json"], ["frames_frames_domain_slot_map", "=", "{", "\n", "# ('frame', 'category'): ('hotel', 'category'),", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotel'", ",", "'location'", ")", ",", "\n", "# ('frame', 'gst_rating'): ('hotel', 'gst_rating'),", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotel'", ",", "'name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'trip'", ",", "'or_city'", ")", ",", "\n", "# ('frame', 'seat'): ('trip', 'seat'),", "\n", "}", "\n", "\n", "frames_sgd_domain_slot_map", "=", "{", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotels'", ",", "'dst_city'", ")", ",", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotels'", ",", "'hotel_name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'travel'", ",", "'location'", ")", ",", "\n", "}", "\n", "frames_db_dir", "=", "os", ".", "path", ".", "join", "(", "REPO_ROOT", ",", "\"LAUG/aug/Word_Perturbation/db/frames-db/\"", ")", "\n", "sgd_db_dir", "=", "os", ".", "path", ".", "join", "(", "REPO_ROOT", ",", "\"LAUG/aug/Word_Perturbation/db/sgd-db/\"", ")", "\n", "\n", "loader_args", "=", "[", "\n", "MultiSourceDBLoaderArgs", "(", "frames_db_dir", ",", "frames_frames_domain_slot_map", ")", ",", "\n", "MultiSourceDBLoaderArgs", "(", "sgd_db_dir", ",", "frames_sgd_domain_slot_map", ")", "\n", "]", "\n", "self", ".", "db_loader", "=", "MultiSourceDBLoader", "(", "loader_args", ")", "\n", "\n", "\n", "\n", "\n", "", "", "def", "main", "(", "frames_filepath", ",", "output_filepath", ",", "\n", "frames_db_dir", ",", "\n", "sgd_db_dir", ",", "\n", "alpha_sr", "=", "0.1", ",", "alpha_ri", "=", "0.1", ",", "alpha_rs", "=", "0.1", ",", "p_rd", "=", "0.1", ",", "num_aug", "=", "2", ",", "\n", "p_slot_value_replacement", "=", "0.25", ")", ":", "\n", "    ", "frames", "=", "load_json", "(", "frames_filepath", ")", "\n", "\n", "frames_frames_domain_slot_map", "=", "{", "\n", "# ('frame', 'category'): ('hotel', 'category'),", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotel'", ",", "'location'", ")", ",", "\n", "# ('frame', 'gst_rating'): ('hotel', 'gst_rating'),", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.aug_with_sgd_db.read_zipped_json": [[8, 12], ["print", "zipfile.ZipFile", "json.load", "zipfile.ZipFile.open"], "function", ["None"], ["return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n", "\n", "", "class", "frames_eda_config", ":", "\n", "    ", "def", "__init__", "(", "self", ",", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.aug_with_sgd_db.main": [[54, 103], ["LAUG.aug.Word_Perturbation.multiwoz.util.load_json", "os.path.join", "loader_args.append", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoader", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA.augment_multiwoz_dataset", "LAUG.aug.Word_Perturbation.multiwoz.util.dump_json", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.load_json", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_multiwoz_dataset", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.dump_json"], ["(", "'frame'", ",", "'name'", ")", ":", "(", "'hotel'", ",", "'name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'trip'", ",", "'or_city'", ")", ",", "\n", "# ('frame', 'seat'): ('trip', 'seat'),", "\n", "}", "\n", "\n", "frames_sgd_domain_slot_map", "=", "{", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotels'", ",", "'dst_city'", ")", ",", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotels'", ",", "'hotel_name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'travel'", ",", "'location'", ")", ",", "\n", "}", "\n", "loader_args", "=", "[", "\n", "MultiSourceDBLoaderArgs", "(", "frames_db_dir", ",", "frames_frames_domain_slot_map", ")", ",", "\n", "MultiSourceDBLoaderArgs", "(", "sgd_db_dir", ",", "frames_sgd_domain_slot_map", ")", "\n", "]", "\n", "db_loader", "=", "MultiSourceDBLoader", "(", "loader_args", ")", "\n", "\n", "eda", "=", "MultiwozEDA", "(", "frames", ",", "db_loader", ",", "\n", "inform_intents", "=", "(", "'inform'", ",", "'switch_frame'", ",", "'confirm'", ")", ",", "\n", "slot_value_replacement_probability", "=", "p_slot_value_replacement", ",", "\n", "alpha_sr", "=", "alpha_sr", ",", "alpha_ri", "=", "alpha_ri", ",", "alpha_rs", "=", "alpha_rs", ",", "p_rd", "=", "p_rd", ",", "num_aug", "=", "num_aug", ")", "\n", "result", "=", "eda", ".", "augment_multiwoz_dataset", "(", "'usr'", ")", "\n", "\n", "dump_json", "(", "result", ",", "output_filepath", ",", "indent", "=", "4", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "import", "argparse", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--frames_filepath\"", ",", "default", "=", "'multiwoz.json'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_filepath'", ",", "'--output'", ",", "'-o'", ",", "default", "=", "'augmented_multiwoz.json'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha_sr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'probability of replacement'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha_ri'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'probability of insertion'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha_rs'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'probability of swap'", ")", "\n", "parser", ".", "add_argument", "(", "'--p_rd'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "\"probability of deletion\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_aug'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"generate `num_aug` candidates with EDA and randomly choose one dialog as augmented dialog.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--p_slot_value_replacement'", ",", "'-p_svr'", ",", "type", "=", "float", ",", "default", "=", "0.25", ",", "\n", "help", "=", "'probability to replace a slot value.'", ")", "\n", "parser", ".", "add_argument", "(", "'--sgd_db_dir'", ",", "'--sgd'", ",", "help", "=", "'dir of sgd db.'", ")", "\n", "parser", ".", "add_argument", "(", "'--frames_db_dir'", ",", "help", "=", "'dir of frames db'", ")", "\n", "opts", "=", "parser", ".", "parse_args", "(", ")", "\n", "main", "(", "**", "vars", "(", "opts", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.get_only_chars": [[40, 47], ["re.sub.lower", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.lstrip"], "function", ["None"], ["def", "get_only_chars", "(", "line", ")", ":", "\n", "    ", "line", "=", "line", ".", "lower", "(", ")", "\n", "line", "=", "re", ".", "sub", "(", "r\"[\u2019']\"", ",", "''", ",", "line", ")", "\n", "line", "=", "re", ".", "sub", "(", "r'[\\t\\n\\-]'", ",", "\" \"", ",", "line", ")", "# replace hyphens with spaces", "\n", "line", "=", "re", ".", "sub", "(", "r'[^a-z ]'", ",", "' '", ",", "line", ")", "\n", "line", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "line", ")", "\n", "return", "line", ".", "lstrip", "(", "' '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_replacement": [[56, 112], ["words.copy", "list", "collections.defaultdict", "enumerate", "list", "random.shuffle", "range", "task_oriented_eda.get_synonyms", "changes.sort", "len", "range", "word2index[].append", "len", "random.choice", "len", "len", "len", "enumerate", "random.choice.split", "token.strip", "changes.append", "range", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.get_synonyms", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["def", "random_replacement", "(", "words", ",", "n", ",", "excluding_indexes", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    randomly replace n words with synonyms\n\n    Args:\n        words: input words\n        n: num of replaced words\n        excluding_indexes: these words won't be replaced\n\n    Returns:\n        new_words (List[str])\n        index_map (Dict[int, int]) map an index in words to an index in new_words\n    \"\"\"", "\n", "\n", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "indexes", "=", "list", "(", "range", "(", "len", "(", "new_words", ")", ")", ")", "\n", "forbidden", "=", "[", "False", "for", "_", "in", "range", "(", "len", "(", "new_words", ")", ")", "]", "\n", "if", "excluding_indexes", "is", "not", "None", ":", "\n", "        ", "for", "i", "in", "excluding_indexes", ":", "\n", "            ", "forbidden", "[", "i", "]", "=", "True", "\n", "\n", "", "", "word2index", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "if", "word", "not", "in", "stop_words", "and", "not", "forbidden", "[", "i", "]", ":", "\n", "            ", "word2index", "[", "word", "]", ".", "append", "(", "i", ")", "\n", "", "", "random_words", "=", "list", "(", "word2index", ")", "\n", "random", ".", "shuffle", "(", "random_words", ")", "\n", "\n", "num_replaced", "=", "0", "\n", "changes", "=", "[", "]", "\n", "for", "random_word", "in", "random_words", ":", "\n", "        ", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "\n", "if", "len", "(", "synonyms", ")", ">=", "1", ":", "\n", "            ", "synonym", "=", "random", ".", "choice", "(", "synonyms", ")", "\n", "synonym_tokens", "=", "[", "token", "for", "token", "in", "synonym", ".", "split", "(", ")", "if", "token", ".", "strip", "(", ")", "]", "\n", "if", "len", "(", "synonym_tokens", ")", "==", "1", ":", "\n", "                ", "for", "i", "in", "word2index", "[", "random_word", "]", ":", "\n", "                    ", "new_words", "[", "i", "]", "=", "synonym_tokens", "[", "0", "]", "\n", "indexes", "[", "i", "]", "=", "None", "\n", "", "", "else", ":", "\n", "# if synonym has more than 1 words and simply insert synonym, index map will be wrong.", "\n", "                ", "for", "i", "in", "word2index", "[", "random_word", "]", ":", "\n", "                    ", "changes", ".", "append", "(", "(", "i", ",", "synonym_tokens", ")", ")", "\n", "", "", "num_replaced", "+=", "1", "\n", "", "if", "num_replaced", ">=", "n", ":", "# only replace up to n words", "\n", "            ", "break", "\n", "\n", "", "", "if", "changes", ":", "\n", "        ", "changes", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "offset", "=", "0", "\n", "for", "i", ",", "synonym_tokens", "in", "changes", ":", "\n", "            ", "i", "+=", "offset", "\n", "new_words", "[", "i", ":", "i", "+", "1", "]", "=", "synonym_tokens", "\n", "indexes", "[", "i", ":", "i", "+", "1", "]", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "synonym_tokens", ")", ")", "]", "\n", "offset", "+=", "len", "(", "synonym_tokens", ")", "-", "1", "\n", "", "", "return", "new_words", ",", "{", "v", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "indexes", ")", "if", "v", "is", "not", "None", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.replacement": [[114, 131], ["words.copy", "task_oriented_eda.get_synonyms", "len", "random.choice", "len", "random.choice.split", "token.strip", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.get_synonyms", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["", "def", "replacement", "(", "words", ",", "index", ":", "int", ")", ":", "\n", "# returns: new_words, start, end, synonym_tokens", "\n", "# new_words[start: end+1] == synonym_tokens", "\n", "    ", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "word", "=", "words", "[", "index", "]", "\n", "synonyms", "=", "get_synonyms", "(", "word", ")", "\n", "if", "len", "(", "synonyms", ")", ">", "0", ":", "\n", "        ", "synonym", "=", "random", ".", "choice", "(", "synonyms", ")", "\n", "synonym_tokens", "=", "[", "token", "for", "token", "in", "synonym", ".", "split", "(", ")", "if", "token", ".", "strip", "(", ")", "]", "\n", "if", "len", "(", "synonym_tokens", ")", "==", "1", ":", "\n", "            ", "new_words", "[", "index", "]", "=", "synonym_tokens", "[", "0", "]", "\n", "return", "new_words", ",", "index", ",", "index", ",", "synonym_tokens", "\n", "", "else", ":", "\n", "            ", "new_words", "[", "index", ":", "index", "+", "1", "]", "=", "synonym_tokens", "\n", "return", "new_words", ",", "index", ",", "index", "+", "len", "(", "synonym_tokens", ")", "-", "1", ",", "synonym_tokens", "\n", "", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.get_synonyms": [[133, 145], ["functools.lru_cache", "set", "nltk.corpus.wordnet.synsets", "list", "syn.lemmas", "set.remove", "l.name().replace().replace().lower", "set.add", "l.name().replace().replace", "l.name().replace", "l.name"], "function", ["None"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "1000", ")", "\n", "def", "get_synonyms", "(", "word", ")", ":", "\n", "    ", "synonyms", "=", "set", "(", ")", "\n", "for", "syn", "in", "wordnet", ".", "synsets", "(", "word", ")", ":", "\n", "        ", "for", "l", "in", "syn", ".", "lemmas", "(", ")", ":", "\n", "            ", "synonym", "=", "l", ".", "name", "(", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ".", "replace", "(", "\"-\"", ",", "\" \"", ")", ".", "lower", "(", ")", "\n", "synonym", "=", "\"\"", ".", "join", "(", "char", "for", "char", "in", "synonym", "if", "char", "in", "ascii_lowercase_and_space", ")", ".", "strip", "(", ")", "\n", "if", "synonym", ":", "\n", "                ", "synonyms", ".", "add", "(", "synonym", ")", "\n", "", "", "", "if", "word", "in", "synonyms", ":", "\n", "        ", "synonyms", ".", "remove", "(", "word", ")", "\n", "", "return", "list", "(", "synonyms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_deletion": [[152, 190], ["enumerate", "len", "len", "random.randint", "range", "new_words.append", "indexes.append", "len", "random.uniform", "len", "enumerate"], "function", ["None"], ["", "def", "random_deletion", "(", "words", ",", "p", ",", "excluding_indexes", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    remove each word with probability p.\n\n    Args:\n        words: input words\n        p: delete probability\n        excluding_indexes: these words won't be removed.\n\n    Returns:\n\n    \"\"\"", "\n", "# obviously, if there's only one word, don't delete it", "\n", "if", "len", "(", "words", ")", "==", "1", ":", "\n", "        ", "return", "words", ",", "{", "0", ":", "0", "}", "\n", "\n", "# randomly delete words with probability p", "\n", "", "new_words", "=", "[", "]", "\n", "indexes", "=", "[", "]", "\n", "forbidden", "=", "[", "False", "for", "_", "in", "range", "(", "len", "(", "words", ")", ")", "]", "\n", "if", "excluding_indexes", "is", "not", "None", ":", "\n", "        ", "for", "i", "in", "excluding_indexes", ":", "\n", "            ", "forbidden", "[", "i", "]", "=", "True", "\n", "", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "if", "forbidden", "[", "i", "]", ":", "\n", "            ", "remained", "=", "True", "\n", "", "else", ":", "\n", "            ", "remained", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", ">", "p", "\n", "", "if", "remained", ":", "\n", "            ", "new_words", ".", "append", "(", "word", ")", "\n", "indexes", ".", "append", "(", "i", ")", "\n", "\n", "# if you end up deleting all words, just return a random word", "\n", "", "", "if", "len", "(", "new_words", ")", "==", "0", ":", "\n", "        ", "rand_int", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "words", ")", "-", "1", ")", "\n", "return", "[", "words", "[", "rand_int", "]", "]", ",", "{", "rand_int", ":", "0", "}", "\n", "\n", "", "return", "new_words", ",", "{", "v", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "indexes", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_swap": [[197, 220], ["words.copy", "list", "range", "range", "list", "list.copy", "task_oriented_eda.swap_word", "len", "set", "set", "range", "enumerate", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.swap_word"], ["", "def", "random_swap", "(", "words", ",", "n", ",", "excluding_indexes", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    randomly swap n pairs of words\n\n    Args:\n        words: input words\n        n: num of pairs\n        excluding_indexes: these words won't be swapped\n\n    Returns:\n\n    \"\"\"", "\n", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "indexes", "=", "list", "(", "range", "(", "len", "(", "words", ")", ")", ")", "\n", "if", "excluding_indexes", "is", "not", "None", ":", "\n", "        ", "allow_indexes", "=", "set", "(", "range", "(", "len", "(", "words", ")", ")", ")", "-", "set", "(", "excluding_indexes", ")", "\n", "allow_indexes", "=", "list", "(", "allow_indexes", ")", "\n", "", "else", ":", "\n", "        ", "allow_indexes", "=", "indexes", ".", "copy", "(", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "new_words", "=", "swap_word", "(", "new_words", ",", "indexes", ",", "allow_indexes", ")", "\n", "", "return", "new_words", ",", "{", "v", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "indexes", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.swap_word": [[222, 233], ["range", "len", "random.choice", "random.choice"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["", "def", "swap_word", "(", "new_words", ",", "indexes", ",", "allow_indexes", ")", ":", "\n", "    ", "if", "len", "(", "allow_indexes", ")", "<=", "1", ":", "\n", "        ", "return", "new_words", "\n", "", "for", "_", "in", "range", "(", "4", ")", ":", "\n", "        ", "i", "=", "random", ".", "choice", "(", "allow_indexes", ")", "\n", "j", "=", "random", ".", "choice", "(", "allow_indexes", ")", "\n", "if", "i", "!=", "j", ":", "\n", "            ", "new_words", "[", "i", "]", ",", "new_words", "[", "j", "]", "=", "new_words", "[", "j", "]", ",", "new_words", "[", "i", "]", "\n", "indexes", "[", "i", "]", ",", "indexes", "[", "j", "]", "=", "indexes", "[", "j", "]", ",", "indexes", "[", "i", "]", "\n", "break", "\n", "", "", "return", "new_words", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_insertion": [[240, 254], ["words.copy", "list", "range", "range", "task_oriented_eda.add_word", "len", "range", "len", "enumerate"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.add_word"], ["", "def", "random_insertion", "(", "words", ",", "n", ",", "excluding_indexes", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    randomly insert n words.\n    \"\"\"", "\n", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "indexes", "=", "list", "(", "range", "(", "len", "(", "new_words", ")", ")", ")", "\n", "forbidden", "=", "[", "False", "for", "_", "in", "range", "(", "len", "(", "new_words", ")", ")", "]", "\n", "if", "excluding_indexes", "is", "not", "None", ":", "\n", "        ", "for", "i", "in", "excluding_indexes", ":", "\n", "            ", "forbidden", "[", "i", "]", "=", "True", "\n", "\n", "", "", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "add_word", "(", "new_words", ",", "indexes", ",", "forbidden", ")", "\n", "", "return", "new_words", ",", "{", "v", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "indexes", ")", "if", "v", "is", "not", "None", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.add_word": [[256, 285], ["range", "sum", "len", "len", "random.randint", "task_oriented_eda.get_synonyms", "random.randint", "len", "len", "random_synonym.split", "token.strip", "range", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.get_synonyms"], ["", "def", "add_word", "(", "new_words", ",", "indexes", ",", "forbidden", ")", ":", "\n", "    ", "if", "sum", "(", "forbidden", ")", "==", "len", "(", "new_words", ")", ":", "\n", "        ", "return", "\n", "", "synonyms", "=", "[", "]", "\n", "counter", "=", "0", "\n", "\n", "while", "len", "(", "synonyms", ")", "<", "1", ":", "\n", "        ", "counter", "+=", "1", "\n", "if", "counter", ">=", "15", ":", "\n", "            ", "return", "\n", "\n", "", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "old_idx", "=", "indexes", "[", "idx", "]", "\n", "if", "old_idx", "is", "None", "or", "forbidden", "[", "old_idx", "]", ":", "\n", "            ", "continue", "\n", "", "random_word", "=", "new_words", "[", "idx", "]", "\n", "synonyms", "=", "get_synonyms", "(", "random_word", ")", "\n", "\n", "", "random_synonym", "=", "synonyms", "[", "0", "]", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "        ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "new_words", ")", "-", "1", ")", "\n", "old_idx", "=", "indexes", "[", "idx", "]", "\n", "if", "old_idx", "is", "None", "or", "not", "forbidden", "[", "old_idx", "]", ":", "\n", "            ", "random_synonym_tokens", "=", "[", "token", "for", "token", "in", "random_synonym", ".", "split", "(", ")", "if", "token", ".", "strip", "(", ")", "]", "\n", "# new_words.insert(idx, random_synonym)", "\n", "# indexes.insert(idx, None)", "\n", "new_words", "[", "idx", ":", "idx", "]", "=", "random_synonym_tokens", "\n", "indexes", "[", "idx", ":", "idx", "]", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "random_synonym_tokens", ")", ")", "]", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.eda": [[291, 345], ["len", "max", "max", "max", "set", "set.add", "range", "range", "range", "range", "random.shuffle", "int", "int", "int", "int", "tuple", "task_oriented_eda.random_replacement", "task_oriented_eda.random_insertion", "task_oriented_eda.random_swap", "task_oriented_eda.random_deletion", "tuple", "set.add", "augmented_sentences.append", "tuple", "set.add", "augmented_sentences.append", "tuple", "set.add", "augmented_sentences.append", "tuple", "set.add", "augmented_sentences.append", "tuple", "tuple", "tuple", "tuple", "random.uniform"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_replacement", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_insertion", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_swap", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.task_oriented_eda.random_deletion"], ["", "", "", "def", "eda", "(", "words", ",", "alpha_sr", "=", "0.1", ",", "alpha_ri", "=", "0.1", ",", "alpha_rs", "=", "0.1", ",", "p_rd", "=", "0.1", ",", "num_aug", "=", "9", ",", "excluding_indexes", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", "=", "None", ")", "->", "List", "[", "Tuple", "[", "list", ",", "dict", "]", "]", ":", "\n", "# sentence = get_only_chars(sentence)", "\n", "# words = sentence.split(' ')", "\n", "    ", "words", "=", "[", "word", "for", "word", "in", "words", "if", "word", "is", "not", "''", "]", "\n", "num_words", "=", "len", "(", "words", ")", "\n", "\n", "augmented_sentences", ":", "List", "[", "Tuple", "[", "list", ",", "dict", "]", "]", "=", "[", "]", "\n", "num_new_per_technique", "=", "int", "(", "num_aug", "/", "4", ")", "+", "1", "\n", "n_sr", "=", "max", "(", "1", ",", "int", "(", "alpha_sr", "*", "num_words", ")", ")", "\n", "n_ri", "=", "max", "(", "1", ",", "int", "(", "alpha_ri", "*", "num_words", ")", ")", "\n", "n_rs", "=", "max", "(", "1", ",", "int", "(", "alpha_rs", "*", "num_words", ")", ")", "\n", "\n", "seen", "=", "set", "(", ")", "\n", "seen", ".", "add", "(", "tuple", "(", "words", ")", ")", "\n", "\n", "# sr", "\n", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "        ", "a_words", ",", "index_map", "=", "random_replacement", "(", "words", ",", "n_sr", ",", "excluding_indexes", ")", "\n", "if", "tuple", "(", "a_words", ")", "not", "in", "seen", ":", "\n", "            ", "seen", ".", "add", "(", "tuple", "(", "a_words", ")", ")", "\n", "augmented_sentences", ".", "append", "(", "(", "a_words", ",", "index_map", ")", ")", "\n", "\n", "# ri", "\n", "", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "        ", "a_words", ",", "index_map", "=", "random_insertion", "(", "words", ",", "n_ri", ",", "excluding_indexes", ")", "\n", "if", "tuple", "(", "a_words", ")", "not", "in", "seen", ":", "\n", "            ", "seen", ".", "add", "(", "tuple", "(", "a_words", ")", ")", "\n", "augmented_sentences", ".", "append", "(", "(", "a_words", ",", "index_map", ")", ")", "\n", "\n", "# rs", "\n", "", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "        ", "a_words", ",", "index_map", "=", "random_swap", "(", "words", ",", "n_rs", ",", "excluding_indexes", ")", "\n", "if", "tuple", "(", "a_words", ")", "not", "in", "seen", ":", "\n", "            ", "seen", ".", "add", "(", "tuple", "(", "a_words", ")", ")", "\n", "augmented_sentences", ".", "append", "(", "(", "a_words", ",", "index_map", ")", ")", "\n", "\n", "# rd", "\n", "", "", "for", "_", "in", "range", "(", "num_new_per_technique", ")", ":", "\n", "        ", "a_words", ",", "index_map", "=", "random_deletion", "(", "words", ",", "p_rd", ",", "excluding_indexes", ")", "\n", "if", "tuple", "(", "a_words", ")", "not", "in", "seen", ":", "\n", "            ", "seen", ".", "add", "(", "tuple", "(", "a_words", ")", ")", "\n", "augmented_sentences", ".", "append", "(", "(", "a_words", ",", "index_map", ")", ")", "\n", "\n", "# augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]", "\n", "", "", "shuffle", "(", "augmented_sentences", ")", "\n", "\n", "# trim so that we have the desired number of augmented sentences", "\n", "if", "num_aug", ">=", "1", ":", "\n", "        ", "augmented_sentences", "=", "augmented_sentences", "[", ":", "num_aug", "]", "\n", "", "else", ":", "\n", "        ", "keep_prob", "=", "num_aug", "\n", "augmented_sentences", "=", "[", "s", "for", "s", "in", "augmented_sentences", "if", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "keep_prob", "]", "\n", "\n", "", "return", "augmented_sentences", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run.main": [[62, 169], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "run.set_seed", "parser.parse_args.model_type.lower", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained.eval", "logger.info", "open", "range", "json.dump", "i.strip", "len", "logger.info", "range", "tokenizer_class.from_pretrained.batch_encode_plus", "torch.LongTensor().to", "torch.LongTensor().to", "len", "torch.LongTensor().to", "torch.LongTensor().to", "position_ids.masked_fill_", "model_class.from_pretrained.generate", "[].tolist", "enumerate", "open", "logger.info", "min", "raw_inputs.append", "torch.LongTensor().to.cumsum", "output_tests.append", "torch.cuda.is_available", "torch.cuda.is_available", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "any", "logger.info", "tokenizer_class.from_pretrained.decode", "examples.append", "MODEL_CLASSES.keys", "int", "lines.split", "[].tolist.reshape", "len", "tokenizer_class.from_pretrained.control_codes.values", "tokenizer.decode.find", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.nlg.nlg.NLG.generate", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.__init__": [[77, 80], ["util._get_all_slots"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._get_all_slots"], ["    ", "def", "__init__", "(", "self", ",", "multiwoz", ")", ":", "\n", "        ", "self", ".", "multiwoz", "=", "multiwoz", "\n", "self", ".", "slots", "=", "_get_all_slots", "(", "multiwoz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.get_unique_slots": [[81, 83], ["None"], "methods", ["None"], ["", "def", "get_unique_slots", "(", "self", ",", "domain", ")", ":", "\n", "        ", "return", "self", ".", "slots", "[", "domain", "]", "[", "'unique'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.split_str": [[84, 88], ["functools.lru_cache", "s.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "lru_cache", "(", "1000", ")", "\n", "def", "split_str", "(", "s", ")", ":", "\n", "        ", "return", "s", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.relevant_words_of_slot": [[128, 138], ["isinstance", "res.append"], "methods", ["None"], ["def", "relevant_words_of_slot", "(", "self", ",", "domain", ",", "slot", ")", ":", "\n", "        ", "if", "domain", "not", "in", "self", ".", "_words_about_slot", "or", "slot", "not", "in", "self", ".", "_words_about_slot", "[", "domain", "]", ":", "\n", "            ", "return", "[", "slot", "]", "\n", "", "if", "isinstance", "(", "self", ".", "_words_about_slot", "[", "domain", "]", "[", "slot", "]", ",", "str", ")", ":", "\n", "            ", "res", "=", "[", "self", ".", "_words_about_slot", "[", "domain", "]", "[", "slot", "]", "]", "\n", "if", "slot", "!=", "res", "[", "-", "1", "]", ":", "\n", "                ", "res", ".", "append", "(", "slot", ")", "\n", "", "return", "res", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_words_about_slot", "[", "domain", "]", "[", "slot", "]", "+", "[", "slot", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.relevant_words_of_domain": [[148, 152], ["None"], "methods", ["None"], ["def", "relevant_words_of_domain", "(", "self", ",", "domain", ")", ":", "\n", "        ", "if", "domain", "not", "in", "self", ".", "_words_about_domain", ":", "\n", "            ", "return", "[", "domain", "]", "\n", "", "return", "self", ".", "_words_about_domain", "[", "domain", "]", "+", "[", "domain", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.contain_word": [[153, 164], ["isinstance", "re.compile", "isinstance", "re.compile.search", "re.compile.startswith", "re.compile.endswith"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "contain_word", "(", "sentence", ":", "str", ",", "word", ")", ":", "\n", "        ", "if", "isinstance", "(", "word", ",", "str", ")", ":", "\n", "            ", "if", "not", "word", ".", "startswith", "(", "r'\\b'", ")", ":", "\n", "                ", "word", "=", "r'\\b'", "+", "word", "\n", "", "if", "not", "word", ".", "endswith", "(", "r'\\b'", ")", ":", "\n", "                ", "word", "+=", "r'\\b'", "\n", "", "word", "=", "re", ".", "compile", "(", "word", ")", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "word", ",", "RePatternType", ")", "\n", "", "return", "word", ".", "search", "(", "sentence", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper._get_excluding_indexes": [[165, 226], ["set", "set", "set", "dialog_act.items", "enumerate", "set.update", "[].lower", "set.add", "set.add", "[].lower", "set.add", "v.lower", "util.Helper.relevant_words_of_domain", "util.Helper.relevant_words_of_slot", "util.is_punctuation", "range", "set.add", "enumerate", "isinstance", "isinstance", "isinstance", "isinstance", "set.add", "slot.lower", "tokenize_util.tokenize", "range", "tokenize_util.tokenize", "range", "set.update", "domain_intent.split", "domain_intent.split", "len", "len", "len", "range", "len", "word.match", "len", "len", "len", "range", "len", "word.match", "len", "isinstance", "slot.lower", "set.add", "set.add", "set.add", "set.add", "all", "set.update", "all", "set.update", "range", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.relevant_words_of_domain", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.Helper.relevant_words_of_slot", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.is_punctuation", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "def", "_get_excluding_indexes", "(", "self", ",", "words", ",", "span_info", ",", "dialog_act", ")", ":", "\n", "        ", "\"\"\"exclude some words, so that the label keeps the same after augmented.\"\"\"", "\n", "excluding_indexes", "=", "set", "(", ")", "\n", "domains", "=", "set", "(", ")", "\n", "slots", "=", "set", "(", ")", "\n", "for", "domain_intent", ",", "slot", ",", "value", ",", "start", ",", "end", "in", "span_info", ":", "\n", "            ", "excluding_indexes", ".", "update", "(", "range", "(", "start", ",", "end", "+", "1", ")", ")", "\n", "domain", "=", "domain_intent", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", "\n", "domains", ".", "add", "(", "domain", ")", "\n", "slots", ".", "add", "(", "(", "domain", ",", "slot", ".", "lower", "(", ")", ")", ")", "\n", "", "for", "domain_intent", ",", "slot_value_list", "in", "dialog_act", ".", "items", "(", ")", ":", "\n", "            ", "domain", "=", "domain_intent", ".", "split", "(", "'-'", ")", "[", "0", "]", ".", "lower", "(", ")", "\n", "domains", ".", "add", "(", "domain", ")", "\n", "for", "slot", ",", "value", "in", "slot_value_list", ":", "\n", "                ", "slots", ".", "add", "(", "(", "domain", ",", "slot", ".", "lower", "(", ")", "if", "isinstance", "(", "slot", ",", "str", ")", "else", "slot", ")", ")", "\n", "\n", "", "", "word2index", "=", "{", "v", ".", "lower", "(", ")", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "words", ")", "}", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "for", "word", "in", "self", ".", "relevant_words_of_domain", "(", "domain", ")", ":", "\n", "                ", "if", "isinstance", "(", "word", ",", "str", ")", ":", "\n", "                    ", "ws", "=", "tokenize", "(", "word", ")", "\n", "if", "len", "(", "ws", ")", "==", "1", ":", "\n", "                        ", "if", "ws", "[", "0", "]", "in", "word2index", ":", "\n", "                            ", "excluding_indexes", ".", "add", "(", "word2index", "[", "word", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "n", "=", "len", "(", "ws", ")", "\n", "N", "=", "len", "(", "words", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "                            ", "if", "i", "+", "n", "<=", "N", "and", "all", "(", "ws", "[", "j", "]", "==", "words", "[", "i", "+", "j", "]", "for", "j", "in", "range", "(", "n", ")", ")", ":", "\n", "                                ", "excluding_indexes", ".", "update", "(", "range", "(", "i", ",", "i", "+", "n", ")", ")", "\n", "\n", "", "", "", "", "if", "isinstance", "(", "word", ",", "RePatternType", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "                        ", "if", "word", ".", "match", "(", "words", "[", "i", "]", ")", ":", "\n", "                            ", "excluding_indexes", ".", "add", "(", "i", ")", "\n", "", "", "", "", "", "for", "domain", ",", "slot", "in", "slots", ":", "\n", "            ", "for", "word", "in", "self", ".", "relevant_words_of_slot", "(", "domain", ",", "slot", ")", ":", "\n", "                ", "if", "isinstance", "(", "word", ",", "str", ")", ":", "\n", "                    ", "ws", "=", "tokenize", "(", "word", ")", "\n", "if", "len", "(", "ws", ")", "==", "1", ":", "\n", "                        ", "if", "ws", "[", "0", "]", "in", "word2index", ":", "\n", "                            ", "excluding_indexes", ".", "add", "(", "word2index", "[", "word", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "n", "=", "len", "(", "ws", ")", "\n", "N", "=", "len", "(", "words", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "                            ", "if", "i", "+", "n", "<=", "N", "and", "all", "(", "ws", "[", "j", "]", "==", "words", "[", "i", "+", "j", "]", "for", "j", "in", "range", "(", "N", ")", ")", ":", "\n", "                                ", "excluding_indexes", ".", "update", "(", "range", "(", "i", ",", "i", "+", "n", ")", ")", "\n", "\n", "", "", "", "", "if", "isinstance", "(", "word", ",", "RePatternType", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "                        ", "if", "word", ".", "match", "(", "words", "[", "i", "]", ")", ":", "\n", "                            ", "excluding_indexes", ".", "add", "(", "i", ")", "\n", "\n", "", "", "", "", "", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "if", "is_punctuation", "(", "word", ")", ":", "\n", "                ", "excluding_indexes", ".", "add", "(", "i", ")", "\n", "", "elif", "word", "==", "'reference'", "and", "i", "+", "1", "<", "len", "(", "words", ")", "and", "words", "[", "i", "+", "1", "]", "==", "'number'", ":", "\n", "# exclude \"reference number\"", "\n", "                ", "excluding_indexes", ".", "update", "(", "(", "i", ",", "i", "+", "1", ")", ")", "\n", "", "", "return", "excluding_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.AugmentationRecorder.__init__": [[272, 275], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "original_sample", ":", "MultiwozSampleType", ")", ":", "\n", "        ", "self", ".", "original_sample", "=", "original_sample", "\n", "self", ".", "augmented_turns", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.AugmentationRecorder.add_augmented_dialog": [[276, 278], ["util.AugmentationRecorder.augmented_turns.append"], "methods", ["None"], ["", "def", "add_augmented_dialog", "(", "self", ",", "turn_index", ",", "turn", ")", ":", "\n", "        ", "self", ".", "augmented_turns", ".", "append", "(", "(", "turn_index", ",", "turn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.AugmentationRecorder.get_augmented_sample": [[279, 290], ["copy.deepcopy", "collections.Counter", "random.random"], "methods", ["None"], ["", "def", "get_augmented_sample", "(", "self", ")", "->", "MultiwozSampleType", ":", "\n", "        ", "sample", "=", "deepcopy", "(", "self", ".", "original_sample", ")", "\n", "turns", "=", "sample", "[", "'log'", "]", "\n", "counter", "=", "Counter", "(", ")", "\n", "for", "turn_index", ",", "turn", "in", "self", ".", "augmented_turns", ":", "\n", "# if there is more than one augmented text", "\n", "# random choose one", "\n", "            ", "counter", "[", "turn_index", "]", "+=", "1", "\n", "if", "random", ".", "random", "(", ")", "*", "counter", "[", "turn_index", "]", "<=", "1", ":", "\n", "                ", "turns", "[", "turn_index", "]", "=", "{", "'turn_index'", ":", "turn_index", ",", "'augmented'", ":", "True", ",", "**", "turn", "}", "\n", "", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.load_json": [[18, 21], ["open", "json.load"], "function", ["None"], ["def", "load_json", "(", "filepath", ")", ":", "\n", "    ", "with", "open", "(", "filepath", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.dump_json": [[23, 28], ["os.makedirs", "os.path.dirname", "open", "json_kwargs.setdefault", "json.dump", "os.path.abspath"], "function", ["None"], ["", "", "def", "dump_json", "(", "obj", ",", "filepath", ",", "**", "json_kwargs", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "filepath", ")", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "filepath", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "json_kwargs", ".", "setdefault", "(", "'indent'", ",", "4", ")", "\n", "json", ".", "dump", "(", "obj", ",", "f", ",", "**", "json_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str": [[31, 35], ["io.StringIO", "pprint.pprint", "io.StringIO.getvalue"], "function", ["None"], ["", "", "def", "p_str", "(", "obj", ")", ":", "\n", "    ", "sio", "=", "StringIO", "(", ")", "\n", "pprint", "(", "obj", ",", "sio", ")", "\n", "return", "sio", ".", "getvalue", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.is_punctuation": [[41, 43], ["punctuation_pattern.match"], "function", ["None"], ["def", "is_punctuation", "(", "word", ")", ":", "\n", "    ", "return", "punctuation_pattern", ".", "match", "(", "word", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._get_all_slots": [[46, 68], ["collections.defaultdict", "multiwoz.values", "isinstance", "functools.reduce", "[].copy", "logs.values.values", "dialog_act.items", "slots[].values", "set", "domain_intent.lower().split", "slots[].setdefault().add", "domain_intent.lower", "slots[].setdefault", "isinstance", "slot.lower", "set"], "function", ["None"], ["", "def", "_get_all_slots", "(", "multiwoz", ":", "MultiwozDatasetType", ")", ":", "\n", "    ", "slots", "=", "defaultdict", "(", "dict", ")", "# Dict[str, Dict[str, Set[str]]]; Dict[Sample_ID, Dict[intent, Set[Slot]]]", "\n", "for", "sample", "in", "multiwoz", ".", "values", "(", ")", ":", "\n", "        ", "logs", "=", "sample", "[", "'log'", "]", "\n", "if", "isinstance", "(", "logs", ",", "dict", ")", ":", "\n", "            ", "logs", "=", "logs", ".", "values", "(", ")", "\n", "", "for", "turn", "in", "logs", ":", "\n", "            ", "dialog_act", "=", "turn", "[", "'dialog_act'", "]", "\n", "for", "domain_intent", ",", "slot_value_list", "in", "dialog_act", ".", "items", "(", ")", ":", "\n", "                ", "domain", ",", "intent", "=", "domain_intent", ".", "lower", "(", ")", ".", "split", "(", "'-'", ")", "\n", "for", "slot", ",", "value", "in", "slot_value_list", ":", "\n", "                    ", "slots", "[", "domain", "]", ".", "setdefault", "(", "intent", ",", "set", "(", ")", ")", ".", "add", "(", "slot", ".", "lower", "(", ")", "if", "isinstance", "(", "slot", ",", "str", ")", "else", "slot", ")", "\n", "", "", "", "", "for", "domain", "in", "slots", ":", "\n", "        ", "s", "=", "reduce", "(", "(", "lambda", "s1", ",", "s2", ":", "s1", "|", "s2", ")", ",", "slots", "[", "domain", "]", ".", "values", "(", ")", ",", "set", "(", ")", ")", "\n", "slots", "[", "domain", "]", "[", "'all'", "]", "=", "s", "\n", "", "for", "domain", "in", "slots", ":", "\n", "        ", "unique_slots", "=", "slots", "[", "domain", "]", "[", "'all'", "]", ".", "copy", "(", ")", "\n", "for", "other_domain", "in", "slots", ":", "\n", "            ", "if", "other_domain", "!=", "domain", ":", "\n", "                ", "unique_slots", "-=", "slots", "[", "other_domain", "]", "[", "'all'", "]", "\n", "", "", "slots", "[", "domain", "]", "[", "'unique'", "]", "=", "unique_slots", "\n", "", "return", "slots", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._iter_dialogues": [[230, 241], ["isinstance", "enumerate", "isinstance", "dialog.get", "RuntimeError", "dialogues.items"], "function", ["None"], ["", "", "def", "_iter_dialogues", "(", "sample", ":", "MultiwozSampleType", ")", ":", "\n", "    ", "dialogues", "=", "sample", "[", "'log'", "]", "\n", "if", "isinstance", "(", "dialogues", ",", "list", ")", ":", "\n", "        ", "for", "i", ",", "dialog", "in", "enumerate", "(", "dialogues", ")", ":", "\n", "            ", "turn", "=", "dialog", ".", "get", "(", "'turn'", ",", "i", ")", "\n", "yield", "turn", ",", "dialog", "\n", "", "", "elif", "isinstance", "(", "dialogues", ",", "dict", ")", ":", "\n", "# assume key is `turn`", "\n", "        ", "yield", "from", "dialogues", ".", "items", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"unknown format.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.iter_dialogues": [[243, 251], ["util._iter_dialogues"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._iter_dialogues"], ["", "", "def", "iter_dialogues", "(", "sample", ":", "MultiwozSampleType", ",", "mode", "=", "'usr'", ")", ":", "\n", "    ", "assert", "mode", "in", "(", "'usr'", ",", "'user'", ",", "'all'", ",", "'sys'", ")", "\n", "for", "turn", ",", "dialog", "in", "_iter_dialogues", "(", "sample", ")", ":", "\n", "        ", "if", "mode", "in", "(", "\"usr\"", ",", "'user'", ")", "and", "turn", "%", "2", "==", "1", ":", "\n", "            ", "continue", "\n", "", "if", "mode", "==", "'sys'", "and", "turn", "%", "2", "==", "0", ":", "\n", "            ", "continue", "\n", "", "yield", "turn", ",", "dialog", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice": [[257, 268], ["enumerate", "hasattr", "hasattr", "random.choice", "ValueError", "random.random"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["def", "choice", "(", "seq", ":", "Iterable", ")", ":", "\n", "    ", "if", "hasattr", "(", "seq", ",", "'__len__'", ")", "and", "hasattr", "(", "seq", ",", "'__getitem__'", ")", ":", "\n", "        ", "return", "random", ".", "choice", "(", "seq", ")", "\n", "\n", "", "r", "=", "_EmptySequence", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "seq", ",", "1", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "*", "i", "<=", "1", ":", "\n", "            ", "r", "=", "x", "\n", "", "", "if", "r", "is", "_EmptySequence", ":", "\n", "        ", "raise", "ValueError", "(", "\"empty sequence\"", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._equal_words": [[293, 298], ["all", "len", "len", "w1.lower", "w2.lower", "zip"], "function", ["None"], ["", "", "def", "_equal_words", "(", "words1", ",", "words2", ",", "ignore_case", ")", ":", "\n", "    ", "if", "not", "ignore_case", ":", "\n", "        ", "return", "words1", "==", "words2", "\n", "", "else", ":", "\n", "        ", "return", "len", "(", "words1", ")", "==", "len", "(", "words2", ")", "and", "all", "(", "w1", ".", "lower", "(", ")", "==", "w2", ".", "lower", "(", ")", "for", "w1", ",", "w2", "in", "zip", "(", "words1", ",", "words2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.is_span_info_consistent_with_text": [[300, 306], ["tokenize_util.convert_sentence_to_tokens", "all", "len", "len", "util._equal_words", "tokenize_util.tokenize", "tuple"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.convert_sentence_to_tokens", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util._equal_words", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "", "def", "is_span_info_consistent_with_text", "(", "sentence", ":", "SentenceType", ",", "span_info", ":", "List", "[", "list", "]", ",", "ignore_case", "=", "True", ")", "->", "bool", ":", "\n", "    ", "\"\"\"check whether the span info is consistent with text.\"\"\"", "\n", "words", "=", "convert_sentence_to_tokens", "(", "sentence", ")", "\n", "return", "all", "(", "\n", "_equal_words", "(", "words", "[", "start", ":", "end", "+", "1", "]", ",", "tokenize", "(", "span", ")", ",", "ignore_case", ")", "for", "domain_intent", ",", "slot", ",", "span", ",", "start", ",", "end", "in", "\n", "span_info", ")", "and", "len", "(", "{", "tuple", "(", "x", "[", "-", "2", ":", "]", ")", "for", "x", "in", "span_info", "}", ")", "==", "len", "(", "span_info", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize": [[4, 6], ["sentence.split", "token.strip"], "function", ["None"], ["def", "tokenize", "(", "sentence", ":", "str", ")", "->", "TokenListType", ":", "\n", "    ", "return", "[", "token", "for", "token", "in", "sentence", ".", "split", "(", ")", "if", "token", ".", "strip", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.convert_sentence_to_tokens": [[7, 13], ["isinstance", "tokenize_util.tokenize", "isinstance"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "def", "convert_sentence_to_tokens", "(", "sentence", ":", "SentenceType", ")", "->", "TokenListType", ":", "\n", "    ", "if", "isinstance", "(", "sentence", ",", "str", ")", ":", "\n", "        ", "return", "tokenize", "(", "sentence", ")", "\n", "", "else", ":", "\n", "        ", "assert", "isinstance", "(", "sentence", ",", "list", ")", "\n", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.convert_tokens_to_string": [[14, 16], ["None"], "function", ["None"], ["", "", "def", "convert_tokens_to_string", "(", "tokens", ":", "TokenListType", ")", "->", "str", ":", "\n", "    ", "return", "' '", ".", "join", "(", "tokens", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.copynlu.COPYNLU.__init__": [[26, 50], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.data.DatasetReader.from_params", "copynlu.COPYNLU.model.eval", "torch.cuda.is_available", "os.path.isfile", "convlab2.util.file_util.cached_path", "Exception"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["def", "__init__", "(", "self", ",", "\n", "archive_file", "=", "DEFAULT_ARCHIVE_FILE", ",", "\n", "cuda_device", "=", "DEFAULT_CUDA_DEVICE", ",", "\n", "model_file", "=", "'https://convlab.blob.core.windows.net/convlab-2/copynet_multiwoz_context.tar.gz'", ",", "\n", "context_size", "=", "3", ")", ":", "\n", "        ", "\"\"\" Constructor for NLU class. \"\"\"", "\n", "\n", "self", ".", "context_size", "=", "context_size", "\n", "cuda_device", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "DEFAULT_CUDA_DEVICE", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "archive_file", ")", ":", "\n", "            ", "if", "not", "model_file", ":", "\n", "                ", "raise", "Exception", "(", "\"No model for COPYNLU is specified!\"", ")", "\n", "\n", "", "archive_file", "=", "cached_path", "(", "model_file", ")", "\n", "\n", "", "archive", "=", "load_archive", "(", "archive_file", ",", "\n", "cuda_device", "=", "cuda_device", ")", "\n", "\n", "dataset_reader_params", "=", "archive", ".", "config", "[", "\"dataset_reader\"", "]", "\n", "self", ".", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "dataset_reader_params", ")", "\n", "self", ".", "model", "=", "archive", ".", "model", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.copynlu.COPYNLU.predict": [[52, 91], ["list", "copynlu.COPYNLU.dataset_reader.text_to_instance", "copynlu.COPYNLU.model.forward_on_instance", "range", "da_seq.replace.replace.replace", "convlab2.nlu.copynet.utils.seq2dict", "convlab2.nlu.copynet.utils.seq2dict.items", "len", "len", "sum", "list", "list", "copynlu.COPYNLU.model.vocab.get_token_from_index", "domain_intent.split", "intent.capitalize.capitalize.capitalize", "slot.capitalize.capitalize.capitalize", "tuples.append", "list.index", "domain.capitalize.capitalize.capitalize"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict"], ["", "def", "predict", "(", "self", ",", "utterance", ",", "context", "=", "list", "(", ")", ")", ":", "\n", "        ", "\"\"\"\n        Predict the dialog act of a natural language utterance and apply error model.\n        Args:\n            utterance (str): A natural language utterance.\n        Returns:\n            output (dict): The dialog act of utterance.\n        \"\"\"", "\n", "if", "len", "(", "utterance", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "instance", "=", "self", ".", "dataset_reader", ".", "text_to_instance", "(", "utterance", ")", "\n", "outputs", "=", "self", ".", "model", ".", "forward_on_instance", "(", "instance", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "outputs", "[", "'predictions'", "]", ")", ")", ":", "\n", "            ", "if", "sum", "(", "outputs", "[", "'predictions'", "]", "[", "i", "]", ">=", "self", ".", "model", ".", "_target_vocab_size", ")", ":", "\n", "                ", "prediction", "=", "list", "(", "outputs", "[", "'predictions'", "]", "[", "i", "]", ")", "\n", "break", "\n", "", "", "else", ":", "\n", "            ", "prediction", "=", "list", "(", "outputs", "[", "'predictions'", "]", "[", "0", "]", ")", "\n", "", "source_tokens", "=", "outputs", "[", "'metadata'", "]", "[", "'source_tokens'", "]", "\n", "\n", "if", "self", ".", "model", ".", "_end_index", "in", "prediction", ":", "\n", "            ", "prediction", "=", "prediction", "[", ":", "prediction", ".", "index", "(", "self", ".", "model", ".", "_end_index", ")", "]", "\n", "", "prediction", "=", "[", "self", ".", "model", ".", "vocab", ".", "get_token_from_index", "(", "index", ",", "self", ".", "model", ".", "_target_namespace", ")", "if", "index", "<", "self", ".", "model", ".", "_target_vocab_size", "else", "source_tokens", "[", "\n", "index", "-", "self", ".", "model", ".", "_target_vocab_size", "]", "for", "index", "in", "prediction", "]", "\n", "\n", "da_seq", "=", "' '", ".", "join", "(", "prediction", ")", "\n", "da_seq", "=", "da_seq", ".", "replace", "(", "'i d'", ",", "'id'", ")", "\n", "output", "=", "seq2dict", "(", "da_seq", ")", "\n", "tuples", "=", "[", "]", "\n", "for", "domain_intent", ",", "svs", "in", "output", ".", "items", "(", ")", ":", "\n", "            ", "for", "slot", ",", "value", "in", "svs", ":", "\n", "                ", "domain", ",", "intent", "=", "domain_intent", ".", "split", "(", "'-'", ")", "\n", "if", "domain", "!=", "'general'", ":", "\n", "                    ", "domain", "=", "domain", ".", "capitalize", "(", ")", "\n", "", "intent", "=", "intent", ".", "capitalize", "(", ")", "\n", "slot", "=", "slot", ".", "capitalize", "(", ")", "\n", "tuples", ".", "append", "(", "[", "intent", ",", "domain", ",", "slot", ",", "value", "]", ")", "\n", "", "", "return", "tuples", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.read_zipped_json": [[12, 16], ["print", "zipfile.ZipFile", "json.load", "zipfile.ZipFile.open"], "function", ["None"], ["def", "read_zipped_json", "(", "filepath", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"zip file path = \"", ",", "filepath", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "filepath", ",", "'r'", ")", "\n", "return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.postprocess.is_slot_da": [[5, 11], ["da[].split"], "function", ["None"], ["def", "is_slot_da", "(", "da", ")", ":", "\n", "    ", "tag_da", "=", "{", "'Inform'", ",", "'Select'", ",", "'Recommend'", ",", "'NoOffer'", ",", "'NoBook'", ",", "'OfferBook'", ",", "'OfferBooked'", ",", "'Book'", "}", "\n", "not_tag_slot", "=", "{", "'Internet'", ",", "'Parking'", ",", "'none'", "}", "\n", "if", "da", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "tag_da", "and", "da", "[", "1", "]", "not", "in", "not_tag_slot", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.postprocess.calculateF1": [[13, 40], ["x.lower", "x.lower", "x[].lower", "x[].lower"], "function", ["None"], ["", "def", "calculateF1", "(", "predict_golden", ",", "split", "=", "False", ")", ":", "\n", "    ", "TP", ",", "FP", ",", "FN", "=", "0", ",", "0", ",", "0", "\n", "for", "item", "in", "predict_golden", ":", "\n", "        ", "predicts", "=", "item", "[", "'predict'", "]", "\n", "if", "split", ":", "\n", "            ", "predicts", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "]", "for", "x", "in", "predicts", "]", "\n", "", "else", ":", "\n", "            ", "predicts", "=", "[", "x", "for", "x", "in", "predicts", "]", "\n", "\n", "", "labels", "=", "item", "[", "'golden'", "]", "\n", "if", "split", ":", "\n", "            ", "labels", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "]", "for", "x", "in", "labels", "]", "\n", "", "else", ":", "\n", "            ", "labels", "=", "[", "x", "for", "x", "in", "labels", "]", "\n", "\n", "#print('predicts = ', predicts)", "\n", "#print('labels = ', labels)", "\n", "#print('='*100)", "\n", "", "for", "ele", "in", "predicts", ":", "\n", "            ", "if", "ele", "in", "labels", ":", "\n", "                ", "TP", "+=", "1", "\n", "", "else", ":", "\n", "                ", "FP", "+=", "1", "\n", "", "", "for", "ele", "in", "labels", ":", "\n", "            ", "if", "ele", "not", "in", "predicts", ":", "\n", "                ", "FN", "+=", "1", "\n", "# print(TP, FP, FN)", "\n", "", "", "", "precision", "=", "1.0", "*", "TP", "/", "(", "TP", "+", "FP", ")", "if", "TP", "+", "FP", "else", "0.", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.postprocess.tag2triples": [[42, 62], ["len", "len", "len", "tag.startswith", "tag[].split", "triples.append", "len", "tag_seq[].startswith"], "function", ["None"], ["F1", "=", "2.0", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "if", "precision", "+", "recall", "else", "0.", "\n", "#print('*'*100)", "\n", "return", "precision", ",", "recall", ",", "F1", "\n", "\n", "\n", "", "def", "tag2triples", "(", "word_seq", ",", "tag_seq", ")", ":", "\n", "    ", "assert", "len", "(", "word_seq", ")", "==", "len", "(", "tag_seq", ")", "\n", "triples", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "tag_seq", ")", ":", "\n", "        ", "tag", "=", "tag_seq", "[", "i", "]", "\n", "if", "tag", ".", "startswith", "(", "'B'", ")", ":", "\n", "            ", "domain", ",", "slot", "=", "tag", "[", "2", ":", "]", ".", "split", "(", "'+'", ")", "\n", "value", "=", "word_seq", "[", "i", "]", "\n", "j", "=", "i", "+", "1", "\n", "while", "j", "<", "len", "(", "tag_seq", ")", ":", "\n", "                ", "if", "tag_seq", "[", "j", "]", ".", "startswith", "(", "'I'", ")", "and", "tag_seq", "[", "j", "]", "[", "2", ":", "]", "==", "tag", "[", "2", ":", "]", ":", "\n", "                    ", "value", "+=", "' '", "+", "word_seq", "[", "j", "]", "\n", "i", "+=", "1", "\n", "j", "+=", "1", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.postprocess.intent2triples": [[64, 70], ["re.split", "triples.append"], "function", ["None"], ["", "", "triples", ".", "append", "(", "[", "domain", ",", "slot", ",", "value", "]", ")", "\n", "", "i", "+=", "1", "\n", "", "return", "triples", "\n", "\n", "\n", "", "def", "intent2triples", "(", "intent_seq", ")", ":", "\n", "    ", "triples", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.postprocess.recover_intent": [[72, 134], ["tag_logits.size", "range", "range", "range", "range", "intent.split", "range", "i.split", "intents.append", "enumerate", "postprocess.tag2triples", "overall.append", "overall.append", "overall.append", "reqs.append", "torch.max", "i.split", "tag.split", "tags.append", "tags.append", "len", "recover_tags.append", "tag_id.item"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.tag2triples"], ["        ", "intent", ",", "slot", ",", "value", "=", "re", ".", "split", "(", "'[+*]'", ",", "intent", ")", "\n", "triples", ".", "append", "(", "[", "intent", ",", "slot", ",", "value", "]", ")", "\n", "", "return", "triples", "\n", "\n", "\n", "", "def", "recover_intent", "(", "dataloader", ",", "intent_logits", ",", "req_logits", ",", "tag_logits", ",", "tag_mask_tensor", ",", "ori_word_seq", ",", "new2ori", ")", ":", "\n", "    ", "max_seq_len", "=", "tag_logits", ".", "size", "(", "1", ")", "\n", "intents", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "dataloader", ".", "intent_dim", ")", ":", "\n", "        ", "if", "intent_logits", "[", "j", "]", ">", "0", ":", "\n", "            ", "intents", ".", "append", "(", "dataloader", ".", "id2intent", "[", "j", "]", ")", "\n", "\n", "", "", "reqs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "req_dim", ")", ":", "\n", "        ", "req_intent", "=", "dataloader", ".", "id2req", "[", "i", "]", "\n", "for", "j", "in", "range", "(", "dataloader", ".", "req_slot_dim", ")", ":", "\n", "            ", "if", "req_logits", "[", "i", "]", "[", "j", "]", ">", "0", ":", "\n", "                ", "req_slot", "=", "dataloader", ".", "id2reqslot", "[", "j", "]", "\n", "reqs", ".", "append", "(", "req_intent", "+", "'-'", "+", "req_slot", ")", "\n", "#req_slot = dataloader.id2reqslot[torch.max(req_logits[i], dim=-1)[-1].item()]", "\n", "\n", "# if req_logits[j]>0:", "\n", "#     reqs.append(dataloader.id2req[j])", "\n", "#reqs.append(req_intent + '-' + req_slot)", "\n", "", "", "", "tag_intent", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "slot_intent_dim", ")", ":", "\n", "        ", "tags", "=", "[", "]", "\n", "intent", "=", "dataloader", ".", "id2slotintent", "[", "i", "]", "\n", "domain", ",", "base_intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "\n", "for", "j", "in", "range", "(", "3", ",", "max_seq_len", "-", "1", ")", ":", "\n", "            ", "if", "tag_mask_tensor", "[", "i", "]", "[", "j", "]", "==", "1", ":", "\n", "                ", "value", ",", "tag_id", "=", "torch", ".", "max", "(", "tag_logits", "[", "i", "]", "[", "j", "]", ",", "dim", "=", "-", "1", ")", "\n", "tag", "=", "dataloader", ".", "id2tag", "[", "tag_id", ".", "item", "(", ")", "]", "\n", "if", "tag", "!=", "'O'", ":", "\n", "                    ", "prefix", ",", "slot", "=", "tag", ".", "split", "(", "'-'", ")", "\n", "real_tag", "=", "prefix", "+", "'-'", "+", "intent", "+", "'+'", "+", "slot", "\n", "tags", ".", "append", "(", "real_tag", ")", "\n", "", "else", ":", "\n", "                    ", "tags", ".", "append", "(", "'O'", ")", "\n", "\n", "", "", "", "if", "tags", "!=", "[", "]", ":", "\n", "            ", "recover_tags", "=", "[", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "                ", "if", "new2ori", "[", "i", "]", ">=", "len", "(", "recover_tags", ")", ":", "\n", "                    ", "recover_tags", ".", "append", "(", "tag", ")", "\n", "", "", "tag_intent", "+=", "tag2triples", "(", "ori_word_seq", ",", "recover_tags", ")", "\n", "\n", "\n", "", "", "overall", "=", "[", "]", "\n", "for", "i", "in", "intents", ":", "\n", "        ", "if", "i", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "GENERAL_TYPE", ":", "\n", "            ", "overall", ".", "append", "(", "[", "i", ",", "None", ",", "None", "]", ")", "\n", "\n", "", "", "for", "i", "in", "reqs", ":", "\n", "        ", "domain", ",", "intent", ",", "slot", "=", "i", ".", "split", "(", "'-'", ")", "\n", "intent", "=", "domain", "+", "'-'", "+", "intent", "\n", "if", "intent", "in", "intents", ":", "\n", "            ", "overall", ".", "append", "(", "[", "intent", ",", "slot", ",", "None", "]", ")", "\n", "\n", "", "", "for", "i", "in", "tag_intent", ":", "\n", "        ", "intent", "=", "i", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.da2triples": [[13, 20], ["dialog_act.items", "triples.append"], "function", ["None"], ["    ", "print", "(", "\"zip file path = \"", ",", "filepath", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "filepath", ",", "'r'", ")", "\n", "return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "\n", "    ", "cur_dir", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.preprocess": [[22, 148], ["os.path.dirname", "os.path.join", "os.path.join", "print", "print", "print", "print", "print", "print", "print", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "os.path.abspath", "os.path.exists", "os.makedirs", "preprocess.read_zipped_json", "print", "data[].items", "print", "json.dump", "len", "len", "len", "len", "len", "len", "len", "open", "open", "open", "open", "open", "open", "open", "os.path.join", "enumerate", "open", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "turn[].split", "enumerate", "turn[].update", "processed_data[].append", "context.append", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "len", "os.path.join", "context.append", "dialog_act[].append", "enumerate", "turn[].pop", "context.append", "intents.append", "slot_intents.append", "tags.append", "used_tags.append", "dacts.split", "preprocess.da2triples", "i.split", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "span[].split", "tags.append", "used_tags.append", "span[].split", "tags.append", "used_tags.append", "intents.append", "processed_da.append", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "intents.append", "reqs.append", "req_slots.append", "processed_da.append", "span[].capitalize", "span[].capitalize", "span[].capitalize", "span[].capitalize", "intent.capitalize", "intent.capitalize"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json", "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.da2triples"], ["\n", "keys", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "results", "=", "{", "}", "\n", "results_test", "=", "{", "}", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "data_key", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "key", "+", "'.json.zip'", ")", ",", "key", "+", "'.json'", ")", "\n", "print", "(", "'load {}, size {}'", ".", "format", "(", "key", ",", "len", "(", "data_key", ")", ")", ")", "\n", "if", "key", "==", "'train'", "or", "key", "==", "'val'", ":", "\n", "            ", "results", "=", "dict", "(", "results", ",", "**", "data_key", ")", "\n", "", "else", ":", "\n", "            ", "results_test", "=", "dict", "(", "results_test", ",", "**", "data_key", ")", "\n", "\n", "", "", "def", "write_file", "(", "name", ",", "data", ",", "k", ")", ":", "\n", "        ", "with", "open", "(", "f'{name}.tsv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "ID", "in", "data", ":", "\n", "                ", "sess", "=", "data", "[", "ID", "]", "[", "'log'", "]", "\n", "da_uttr_history", "=", "[", "]", "\n", "for", "turn", "in", "sess", ":", "\n", "                    ", "if", "not", "turn", "[", "'dialog_act'", "]", ":", "\n", "                        ", "da_uttr_history", ".", "append", "(", "turn", "[", "'text'", "]", ".", "lower", "(", ")", ")", "\n", "continue", "\n", "", "da_seq", "=", "dict2seq", "(", "dict2dict", "(", "turn", "[", "'dialog_act'", "]", ")", ")", ".", "replace", "(", "'_'", ",", "' '", ")", ".", "lower", "(", ")", "\n", "da_uttr", "=", "turn", "[", "'text'", "]", ".", "lower", "(", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "k", ")", ":", "\n", "                        ", "pad", "=", "' '", "if", "i", ">", "1", "else", "' $ '", "\n", "if", "len", "(", "da_uttr_history", ")", ">=", "i", ":", "\n", "                            ", "da_uttr", "=", "da_uttr_history", "[", "-", "i", "]", "+", "pad", "+", "da_uttr", "\n", "\n", "", "", "f", ".", "write", "(", "f'{da_uttr}\\t{da_seq}\\n'", ")", "\n", "da_uttr_history", ".", "append", "(", "turn", "[", "'text'", "]", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "", "", "k", "=", "3", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data'", ")", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data'", ")", ")", "\n", "", "write_file", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data/train'", ")", ",", "results", ",", "k", ")", "\n", "write_file", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data/test'", ")", ",", "results_test", ",", "k", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.BERTNLU.__init__": [[16, 62], ["os.path.join", "json.load", "os.path.dirname", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "json.load", "LAUG.nlu.jointBERT_new.dataloader.Dataloader", "print", "print", "os.path.join", "print", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.load_state_dict", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.to", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.eval", "spacy.load", "print", "os.path.dirname", "open", "os.path.dirname", "os.path.exists", "LAUG.nlu.jointBERT_new.multiwoz.preprocess.preprocess", "open", "open", "open", "open", "open", "len", "len", "os.path.exists", "print", "LAUG.util.file_util.cached_path", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "zipfile.ZipFile.close", "torch.load", "os.path.abspath", "torch.cuda.is_available", "os.path.abspath", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.preprocess", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["\n", "return", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.BERTNLU.predict": [[63, 94], ["list", "nlu.BERTNLU.dataloader.bert_tokenize", "nlu.BERTNLU.dataloader.pad_batch", "tuple", "LAUG.nlu.jointBERT_new.multiwoz.postprocess.recover_intent", "len", "nlu.BERTNLU.dataloader.tokenizer.encode", "nlu.BERTNLU.dataloader.tokenizer.encode", "torch.no_grad", "nlu.BERTNLU.model.forward", "intent.split", "dialog_act.append", "nlu.BERTNLU.nlp", "token.text.strip", "nlu.BERTNLU.dataloader.seq_intent2id", "t.to", "unidecode.unidecode.unidecode", "len", "type", "len", "nlu.BERTNLU.dataloader.seq_tag2id", "nlu.BERTNLU.dataloader.req_transfer"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.bert_tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.pad_batch", "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.recover_intent", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.forward", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_intent2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_tag2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.req_transfer"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run_single.set_seed": [[56, 61], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run_single.top_k_top_p_filtering": [[63, 92], ["min", "float", "logits.size", "torch.sort", "torch.sort", "torch.cumsum", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "torch.softmax", "torch.topk", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size x vocabulary size)\n            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "top_k", "=", "min", "(", "top_k", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "dim", "=", "1", ",", "index", "=", "sorted_indices", ",", "src", "=", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run_single.sample_sequence": [[94, 137], ["torch.tensor", "torch.tensor", "context.unsqueeze().repeat.unsqueeze().repeat", "torch.no_grad", "torch.no_grad", "range", "context.unsqueeze().repeat.unsqueeze", "model", "range", "run_single.top_k_top_p_filtering", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.tensor().view", "torch.tensor().view", "set", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.multinomial", "torch.multinomial", "generated[].tolist", "torch.softmax", "torch.zeros", "torch.zeros", "torch.full", "torch.full", "torch.tensor", "torch.tensor", "torch.argmax", "torch.argmax"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.top_k_top_p_filtering", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "sample_sequence", "(", "model", ",", "length", ",", "context", ",", "num_samples", "=", "1", ",", "temperature", "=", "1", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "repetition_penalty", "=", "1.0", ",", "\n", "is_xlnet", "=", "False", ",", "is_xlm_mlm", "=", "False", ",", "xlm_mask_token", "=", "None", ",", "xlm_lang", "=", "None", ",", "device", "=", "'cpu'", ")", ":", "\n", "    ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "context", "=", "context", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "num_samples", ",", "1", ")", "\n", "generated", "=", "context", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "length", ")", ":", "\n", "\n", "            ", "inputs", "=", "{", "'input_ids'", ":", "generated", "}", "\n", "if", "is_xlnet", ":", "\n", "# XLNet is a direct (predict same token, not next token) and bi-directional model by default", "\n", "# => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "zeros", "(", "(", "1", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "perm_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "perm_mask", "[", ":", ",", ":", ",", "-", "1", "]", "=", "1.0", "# Previous tokens don't see last token", "\n", "target_mapping", "=", "torch", ".", "zeros", "(", "(", "1", ",", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "target_mapping", "[", "0", ",", "0", ",", "-", "1", "]", "=", "1.0", "# predict last token", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", ",", "'perm_mask'", ":", "perm_mask", ",", "'target_mapping'", ":", "target_mapping", "}", "\n", "\n", "", "if", "is_xlm_mlm", "and", "xlm_mask_token", ":", "\n", "# XLM MLM models are direct models (predict same token, not next token)", "\n", "# => need one additional dummy token in the input (will be masked and guessed)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "full", "(", "(", "1", ",", "1", ")", ",", "xlm_mask_token", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", "}", "\n", "\n", "", "if", "xlm_lang", "is", "not", "None", ":", "\n", "                ", "inputs", "[", "\"langs\"", "]", "=", "torch", ".", "tensor", "(", "[", "xlm_lang", "]", "*", "inputs", "[", "\"input_ids\"", "]", ".", "shape", "[", "1", "]", ",", "device", "=", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "", "outputs", "=", "model", "(", "**", "inputs", ")", "# Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "/", "(", "temperature", "if", "temperature", ">", "0", "else", "1.", ")", "\n", "\n", "# repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "                ", "for", "_", "in", "set", "(", "generated", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                    ", "next_token_logits", "[", "i", ",", "_", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "filtered_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "if", "temperature", "==", "0", ":", "# greedy sampling:", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_token", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", "\n", "", "generated", "=", "torch", ".", "cat", "(", "(", "generated", ",", "next_token", ")", ",", "dim", "=", "1", ")", "\n", "", "", "return", "generated", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run_single.main": [[139, 270], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "run_single.set_seed", "parser.parse_args.model_type.lower", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained.eval", "logger.info", "open", "range", "json.dump", "i.strip", "len", "logger.info", "tokenizer_class.from_pretrained.encode", "run_single.sample_sequence", "out[].tolist", "lines.split", "output_tests.append", "open", "logger.info", "hasattr", "hasattr", "tokenizer_class.from_pretrained.decode", "lines.split.append", "torch.cuda.is_available", "torch.cuda.is_available", "lines.split", "any", "logger.info", "bool", "tokenizer.decode.strip().replace().lower", "MODEL_CLASSES.keys", "int", "tokenizer_class.from_pretrained.lang2id.keys", "input", "tokenizer.decode.find", "tokenizer.decode.strip().replace", "tokenizer_class.from_pretrained.control_codes.values", "len", "len", "str", "tokenizer.decode.strip", "list", "tokenizer_class.from_pretrained.lang2id.keys"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.sample_sequence", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Model type selected in the list: \"", "+", "\", \"", ".", "join", "(", "MODEL_CLASSES", ".", "keys", "(", ")", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name selected in the list: \"", "+", "\", \"", ".", "join", "(", "ALL_MODELS", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--prompt\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--padding_text\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--xlm_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Optional language when used with the XLM model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length\"", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_samples\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--temperature\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"temperature of 0 implies greedy sampling\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--repetition_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"primarily useful for CTRL model; in that case, use 1.2\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_k\"", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_p\"", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--stop_token'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Token at which text generation is stopped\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--input_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--output_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--nc'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"number of sentence\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--use_token\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "\n", "# parser.add_argument('--use_token', type=int, default=1,", "\n", "# help=\"number of sentence\")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "set_seed", "(", "args", ")", "\n", "\n", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "args", ".", "length", "<", "0", "and", "model", ".", "config", ".", "max_position_embeddings", ">", "0", ":", "\n", "        ", "args", ".", "length", "=", "model", ".", "config", ".", "max_position_embeddings", "\n", "", "elif", "0", "<", "model", ".", "config", ".", "max_position_embeddings", "<", "args", ".", "length", ":", "\n", "        ", "args", ".", "length", "=", "model", ".", "config", ".", "max_position_embeddings", "# No generation bigger than model size ", "\n", "", "elif", "args", ".", "length", "<", "0", ":", "\n", "        ", "args", ".", "length", "=", "MAX_LENGTH", "# avoid infinite loop", "\n", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "if", "args", ".", "model_type", "in", "[", "\"ctrl\"", "]", ":", "\n", "        ", "if", "args", ".", "temperature", ">", "0.7", ":", "\n", "            ", "logger", ".", "info", "(", "'CTRL typically works better with lower temperatures (and lower top_k).'", ")", "\n", "\n", "", "", "fin", "=", "open", "(", "args", ".", "input_file", ")", "\n", "inputs", "=", "[", "i", ".", "strip", "(", ")", "for", "i", "in", "fin", "]", "\n", "output_tests", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "1", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"PROGRESS: {int(idx/len(inputs)*100)}%\"", ")", "\n", "xlm_lang", "=", "None", "\n", "# XLM Language usage detailed in the issues #1414", "\n", "if", "args", ".", "model_type", "in", "[", "\"xlm\"", "]", "and", "hasattr", "(", "tokenizer", ",", "'lang2id'", ")", "and", "hasattr", "(", "model", ".", "config", ",", "'use_lang_emb'", ")", "and", "model", ".", "config", ".", "use_lang_emb", ":", "\n", "            ", "if", "args", ".", "xlm_lang", ":", "\n", "                ", "language", "=", "args", ".", "xlm_lang", "\n", "", "else", ":", "\n", "                ", "language", "=", "None", "\n", "while", "language", "not", "in", "tokenizer", ".", "lang2id", ".", "keys", "(", ")", ":", "\n", "                    ", "language", "=", "input", "(", "\"Using XLM. Select language in \"", "+", "str", "(", "list", "(", "tokenizer", ".", "lang2id", ".", "keys", "(", ")", ")", ")", "+", "\" >>> \"", ")", "\n", "", "", "xlm_lang", "=", "tokenizer", ".", "lang2id", "[", "language", "]", "\n", "\n", "# XLM masked-language modeling (MLM) models need masked token (see details in sample_sequence)", "\n", "", "is_xlm_mlm", "=", "args", ".", "model_type", "in", "[", "\"xlm\"", "]", "and", "'mlm'", "in", "args", ".", "model_name_or_path", "\n", "if", "is_xlm_mlm", ":", "\n", "            ", "xlm_mask_token", "=", "tokenizer", ".", "mask_token_id", "\n", "", "else", ":", "\n", "            ", "xlm_mask_token", "=", "None", "\n", "\n", "# raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")", "\n", "", "lines", "=", "inputs", "[", "idx", "]", "\n", "raw_text", "=", "lines", ".", "split", "(", "' & '", ")", "[", "0", "]", "+", "' & '", "\n", "if", "args", ".", "model_type", "in", "[", "\"transfo-xl\"", ",", "\"xlnet\"", "]", ":", "\n", "# Models with memory likes to have a long prompt for short inputs.", "\n", "            ", "raw_text", "=", "(", "args", ".", "padding_text", "if", "args", ".", "padding_text", "else", "PADDING_TEXT", ")", "+", "raw_text", "\n", "\n", "", "context_tokens", "=", "tokenizer", ".", "encode", "(", "raw_text", ",", "add_special_tokens", "=", "False", ")", "\n", "\n", "if", "args", ".", "model_type", "==", "\"ctrl\"", ":", "\n", "            ", "if", "not", "any", "(", "context_tokens", "[", "0", "]", "==", "x", "for", "x", "in", "tokenizer", ".", "control_codes", ".", "values", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"WARNING! You are not starting your generation from a control code so you won't get good results\"", ")", "\n", "", "", "out", "=", "sample_sequence", "(", "\n", "model", "=", "model", ",", "\n", "context", "=", "context_tokens", ",", "\n", "num_samples", "=", "args", ".", "num_samples", ",", "\n", "length", "=", "args", ".", "length", ",", "\n", "temperature", "=", "args", ".", "temperature", ",", "\n", "top_k", "=", "args", ".", "top_k", ",", "\n", "top_p", "=", "args", ".", "top_p", ",", "\n", "repetition_penalty", "=", "args", ".", "repetition_penalty", ",", "\n", "is_xlnet", "=", "bool", "(", "args", ".", "model_type", "==", "\"xlnet\"", ")", ",", "\n", "is_xlm_mlm", "=", "is_xlm_mlm", ",", "\n", "xlm_mask_token", "=", "xlm_mask_token", ",", "\n", "xlm_lang", "=", "xlm_lang", ",", "\n", "device", "=", "args", ".", "device", ",", "\n", ")", "\n", "out", "=", "out", "[", ":", ",", "len", "(", "context_tokens", ")", ":", "]", ".", "tolist", "(", ")", "\n", "#examples = []", "\n", "examples", "=", "lines", ".", "split", "(", "' & '", ")", "\n", "for", "o", "in", "out", ":", "\n", "            ", "text", "=", "tokenizer", ".", "decode", "(", "o", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "text", "=", "text", "[", ":", "text", ".", "find", "(", "args", ".", "stop_token", ")", "if", "args", ".", "stop_token", "else", "None", "]", "\n", "examples", ".", "append", "(", "text", ".", "strip", "(", ")", ".", "lower", "(", ")", ")", "\n", "\n", "", "output_tests", ".", "append", "(", "examples", ")", "\n", "# break", "\n", "# if args.prompt:", "\n", "# break", "\n", "", "import", "json", "\n", "json", ".", "dump", "(", "output_tests", ",", "open", "(", "args", ".", "output_file", ",", "'w'", ")", ",", "indent", "=", "2", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.run.set_seed": [[56, 61], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.GPTNLU.__init__": [[17, 54], ["os.path.dirname", "os.path.join", "torch.device", "convlab2.nlu.gpt.decode.set_seed", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "nlu.GPTNLU.model.to", "nlu.GPTNLU.model.eval", "os.path.abspath", "os.path.isfile", "convlab2.util.file_util.cached_path", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "torch.cuda.device_count", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["\n", "return", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.GPTNLU.predict": [[55, 101], ["list", "range", "nlu.GPTNLU.tokenizer.encode", "convlab2.nlu.gpt.decode.sample_sequence", "out[].tolist", "nlu.GPTNLU.tokenizer.decode", "text.replace.replace.replace", "convlab2.nlu.gpt.utils.seq2dict", "convlab2.nlu.gpt.utils.seq2dict.items", "len", "text.replace.replace.split", "len", "domain_intent.split", "intent.capitalize.capitalize.capitalize", "slot.capitalize.capitalize.capitalize", "tuples.append", "text.replace.replace.find", "domain.capitalize.capitalize.capitalize", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.sample_sequence", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.MILU.__init__": [[28, 55], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.data.tokenizers.word_splitter.SpacyWordSplitter", "nlu.MILU.tokenizer.spacy.tokenizer.add_special_case", "allennlp.data.DatasetReader.from_params", "nlu.MILU.model.eval", "torch.cuda.is_available", "os.path.isfile", "LAUG.util.file_util.cached_path", "Exception"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.nlu.MILU.predict": [[56, 83], ["list", "nlu.MILU.tokenizer.split_words", "nlu.MILU.dataset_reader.text_to_instance", "nlu.MILU.model.forward_on_instance", "outputs[].items", "len", "sum", "nlu.MILU.tokenizer.split_words", "len", "domain_intent.split", "tuples.append", "nlu.MILU.tokenizer.split_words"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.__init__": [[18, 54], ["os.path.dirname", "os.path.join", "torch.device", "LAUG.nlg.scgpt.decode.set_seed", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "scgpt.SCGPT.model.to", "scgpt.SCGPT.model.eval", "os.path.abspath", "os.path.isfile", "LAUG.util.file_util.cached_path", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "torch.cuda.device_count", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["    ", "def", "__init__", "(", "self", ",", "\n", "archive_file", "=", "DEFAULT_ARCHIVE_FILE", ",", "\n", "use_cuda", "=", "True", ",", "\n", "is_user", "=", "False", ",", "\n", "model_file", "=", "'https://convlab.blob.core.windows.net/convlab-2/nlg-gpt-multiwoz.zip'", ")", ":", "\n", "        ", "model_dir", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "archive_file", ")", ":", "\n", "            ", "archive_file", "=", "cached_path", "(", "model_file", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "archive_file", ",", "'r'", ")", "\n", "archive", ".", "extractall", "(", "model_dir", ")", "\n", "\n", "", "self", ".", "model_name_or_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "'multiwoz'", ")", "\n", "self", ".", "length", "=", "50", "\n", "self", ".", "num_samples", "=", "5", "\n", "self", ".", "temperature", "=", "1.0", "\n", "self", ".", "repetition_penalty", "=", "1.0", "\n", "self", ".", "top_k", "=", "50", "\n", "self", ".", "top_p", "=", "0.9", "\n", "self", ".", "seed", "=", "42", "\n", "self", ".", "stop_token", "=", "'<|endoftext|>'", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "use_cuda", "else", "\"cpu\"", ")", "\n", "set_seed", "(", "self", ".", "seed", ",", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", "\n", "\n", "model_class", ",", "tokenizer_class", "=", "GPT2LMHeadModel", ",", "GPT2Tokenizer", "\n", "self", ".", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "self", ".", "model_name_or_path", ")", "\n", "self", ".", "model", "=", "model_class", ".", "from_pretrained", "(", "self", ".", "model_name_or_path", ")", "\n", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "if", "self", ".", "length", "<", "0", "and", "self", ".", "model", ".", "config", ".", "max_position_embeddings", ">", "0", ":", "\n", "            ", "self", ".", "length", "=", "self", ".", "model", ".", "config", ".", "max_position_embeddings", "\n", "", "elif", "0", "<", "self", ".", "model", ".", "config", ".", "max_position_embeddings", "<", "self", ".", "length", ":", "\n", "            ", "self", ".", "length", "=", "self", ".", "model", ".", "config", ".", "max_position_embeddings", "# No generation bigger than model size ", "\n", "", "elif", "self", ".", "length", "<", "0", ":", "\n", "            ", "self", ".", "length", "=", "self", ".", "MAX_LENGTH", "# avoid infinite loop", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.init_session": [[55, 63], ["None"], "methods", ["None"], ["", "", "def", "init_session", "(", "self", ")", ":", "\n", "        ", "self", ".", "sess_domains", "=", "{", "'Attraction'", ":", "False", ",", "\n", "'Hospital'", ":", "False", ",", "\n", "'Hotel'", ":", "False", ",", "\n", "'Police'", ":", "False", ",", "\n", "'Restaurant'", ":", "False", ",", "\n", "'Taxi'", ":", "False", ",", "\n", "'Train'", ":", "False", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.generate": [[64, 92], ["LAUG.nlg.scgpt.utils.tuple2seq", "set", "scgpt.SCGPT.tokenizer.encode", "LAUG.nlg.scgpt.decode.sample_sequence", "out[].tolist", "numpy.random.choice", "scgpt.SCGPT.tokenizer.decode", "scgpt.SCGPT.split", "raw_text.replace.replace.replace", "domain.lower", "scgpt.SCGPT.find", "domain.lower", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.tuple2seq", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.sample_sequence", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode"], ["", "def", "generate", "(", "self", ",", "meta", ")", ":", "\n", "\n", "        ", "raw_text", "=", "tuple2seq", "(", "meta", ")", "\n", "domains", "=", "set", "(", "[", "item", "[", "1", "]", "for", "item", "in", "meta", "]", ")", "\n", "for", "domain", "in", "domains", ":", "\n", "            ", "if", "domain", "!=", "'general'", "and", "not", "self", ".", "sess_domains", "[", "domain", "]", ":", "\n", "                ", "raw_text", "=", "raw_text", ".", "replace", "(", "domain", ".", "lower", "(", ")", ",", "domain", ".", "lower", "(", ")", "+", "' *'", ",", "1", ")", "\n", "self", ".", "sess_domains", "[", "domain", "]", "=", "True", "\n", "", "", "context_tokens", "=", "self", ".", "tokenizer", ".", "encode", "(", "raw_text", ",", "add_special_tokens", "=", "False", ")", "\n", "out", "=", "sample_sequence", "(", "\n", "model", "=", "self", ".", "model", ",", "\n", "context", "=", "context_tokens", ",", "\n", "num_samples", "=", "self", ".", "num_samples", ",", "\n", "length", "=", "self", ".", "length", ",", "\n", "temperature", "=", "self", ".", "temperature", ",", "\n", "top_k", "=", "self", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "top_p", ",", "\n", "repetition_penalty", "=", "self", ".", "repetition_penalty", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "out", "=", "out", "[", ":", ",", "len", "(", "context_tokens", ")", ":", "]", ".", "tolist", "(", ")", "\n", "index", "=", "np", ".", "random", ".", "choice", "(", "[", "0", ",", "1", ",", "2", ",", "3", "]", ",", "p", "=", "[", "0.4", ",", "0.3", ",", "0.2", ",", "0.1", "]", ")", "\n", "o", "=", "out", "[", "index", "]", "\n", "text", "=", "self", ".", "tokenizer", ".", "decode", "(", "o", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "text", "=", "text", ".", "split", "(", "'& '", ")", "[", "-", "1", "]", "\n", "text", "=", "text", "[", ":", "text", ".", "find", "(", "self", ".", "stop_token", ")", "if", "self", ".", "stop_token", "else", "None", "]", "\n", "\n", "return", "text", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.init_domain": [[67, 75], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.write_file": [[76, 93], ["open", "preprocess.init_domain", "eval", "LAUG.nlg.scgpt.utils.dict2seq().replace", "set", "turn[].replace().replace", "f.write", "str().replace", "LAUG.nlg.scgpt.utils.dict2seq", "da_seq.replace.replace", "turn[].replace", "str", "LAUG.nlg.scgpt.utils.dict2dict", "key.split", "turn[].keys", "domain.lower", "domain.lower"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.preprocess.init_domain", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.dict2seq", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.dict2dict"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.TTS.text2wav": [[7, 10], ["gtts.gTTS().save", "pydub.audio_segment.AudioSegment.from_mp3().set_frame_rate().export", "gtts.gTTS", "pydub.audio_segment.AudioSegment.from_mp3().set_frame_rate", "pydub.audio_segment.AudioSegment.from_mp3"], "function", ["None"], ["def", "text2wav", "(", "text", ",", "language", "=", "'en'", ",", "filename", "=", "'temp'", ",", "tld", "=", "'cn'", ")", ":", "\n", "    ", "gTTS", "(", "text", "=", "text", ",", "tld", "=", "tld", ",", "lang", "=", "language", ")", ".", "save", "(", "filename", "+", "\".mp3\"", ")", "\n", "AudioSegment", ".", "from_mp3", "(", "filename", "+", "\".mp3\"", ")", ".", "set_frame_rate", "(", "16000", ")", ".", "export", "(", "filename", "+", "\".wav\"", ",", "format", "=", "\"wav\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.Speech_Recognition.Speech_Recognition.__init__": [[10, 15], ["LAUG.aug.Speech_Recognition.ASR.wav2text"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset", "=", "'multiwoz'", ",", "temp_file", "=", "'temp'", ",", "tld", "=", "'com'", ")", ":", "\n", "\n", "        ", "self", ".", "wav2text", "=", "wav2text", "(", ")", "\n", "self", ".", "temp_file", "=", "temp_file", "\n", "self", ".", "tld", "=", "tld", "\n", "", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.Speech_Recognition.Speech_Recognition.aug": [[15, 30], ["Speech_Recognition.Speech_Recognition.wav2text.run", "new_span_info.append", "LAUG.aug.Speech_Recognition.TTS.text2wav", "LAUG.aug.Speech_Recognition.multiwoz.span_detection.span_detect", "print"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.wav2text.run", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.TTS.text2wav", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.span_detection.span_detect"], ["", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n", "        ", "ok", "=", "0", "\n", "while", "ok", "==", "0", ":", "\n", "            ", "try", ":", "\n", "                ", "text2wav", "(", "text", ",", "tld", "=", "self", ".", "tld", ",", "filename", "=", "self", ".", "temp_file", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "ok", "=", "0", "\n", "print", "(", "\"gTTS error occur!\"", ")", "\n", "", "else", ":", "\n", "                ", "ok", "=", "1", "\n", "", "", "new_text", "=", "self", ".", "wav2text", ".", "run", "(", "self", ".", "temp_file", "+", "\".wav\"", ")", "\n", "new_span_info", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "            ", "new_span_info", ".", "append", "(", "span_detect", "(", "text", ",", "new_text", ",", "span", ")", ")", "\n", "", "return", "new_text", ",", "new_span_info", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.VersionAction.__init__": [[82, 84], ["argparse.Action.__init__"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "VersionAction", ",", "self", ")", ".", "__init__", "(", "nargs", "=", "0", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.VersionAction.__call__": [[85, 88], ["print", "exit", "deepspeech.version"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "print", "(", "'DeepSpeech '", ",", "version", "(", ")", ")", "\n", "exit", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.wav2text.__init__": [[91, 126], ["print", "timeit.default_timer", "os.path.dirname", "deepspeech.Model", "print", "deepspeech.Model.sampleRate", "os.path.abspath", "os.path.join", "timeit.default_timer", "deepspeech.Model.setBeamWidth", "print", "timeit.default_timer", "deepspeech.Model.enableExternalScorer", "print", "print", "args.hot_words.split", "os.path.join", "timeit.default_timer", "deepspeech.Model.setScorerAlphaBeta", "word_boost.split", "deepspeech.Model.addHotWord", "float"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", ")", ":", "\n", "\n", "        ", "print", "(", "'Loading model from file {}'", ".", "format", "(", "args", ".", "model", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "model_load_start", "=", "timer", "(", ")", "\n", "# sphinx-doc: python_ref_model_start", "\n", "model_path", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "\n", "ds", "=", "Model", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "args", ".", "model", ")", ")", "\n", "# sphinx-doc: python_ref_model_stop", "\n", "model_load_end", "=", "timer", "(", ")", "-", "model_load_start", "\n", "print", "(", "'Loaded model in {:.3}s.'", ".", "format", "(", "model_load_end", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "if", "args", ".", "beam_width", ":", "\n", "            ", "ds", ".", "setBeamWidth", "(", "args", ".", "beam_width", ")", "\n", "\n", "", "self", ".", "desired_sample_rate", "=", "ds", ".", "sampleRate", "(", ")", "\n", "\n", "\n", "\n", "if", "args", ".", "scorer", ":", "\n", "            ", "print", "(", "'Loading scorer from files {}'", ".", "format", "(", "args", ".", "scorer", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "scorer_load_start", "=", "timer", "(", ")", "\n", "ds", ".", "enableExternalScorer", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "args", ".", "scorer", ")", ")", "\n", "scorer_load_end", "=", "timer", "(", ")", "-", "scorer_load_start", "\n", "print", "(", "'Loaded scorer in {:.3}s.'", ".", "format", "(", "scorer_load_end", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "if", "args", ".", "lm_alpha", "and", "args", ".", "lm_beta", ":", "\n", "                ", "ds", ".", "setScorerAlphaBeta", "(", "args", ".", "lm_alpha", ",", "args", ".", "lm_beta", ")", "\n", "\n", "", "", "if", "args", ".", "hot_words", ":", "\n", "            ", "print", "(", "'Adding hot-words'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "for", "word_boost", "in", "args", ".", "hot_words", ".", "split", "(", "','", ")", ":", "\n", "                ", "word", ",", "boost", "=", "word_boost", ".", "split", "(", "':'", ")", "\n", "ds", ".", "addHotWord", "(", "word", ",", "float", "(", "boost", ")", ")", "\n", "", "", "self", ".", "ds", "=", "ds", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.wav2text.run": [[127, 147], ["wave.open", "wave.open.getframerate", "wave.open.close", "timeit.default_timer", "ASR.wav2text.ds.stt", "print", "ASR.convert_samplerate", "numpy.frombuffer", "wave.open.getnframes", "timeit.default_timer", "wave.open.readframes", "wave.open.getnframes"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.convert_samplerate"], ["", "def", "run", "(", "self", ",", "audio", ")", ":", "\n", "        ", "fin", "=", "wave", ".", "open", "(", "audio", ",", "'rb'", ")", "\n", "fs_orig", "=", "fin", ".", "getframerate", "(", ")", "\n", "if", "fs_orig", "!=", "self", ".", "desired_sample_rate", ":", "\n", "            ", "print", "(", "'Warning: original sample rate ({}) is different than {}hz. Resampling might produce erratic speech recognition.'", ".", "format", "(", "fs_orig", ",", "desired_sample_rate", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "fs_new", ",", "audio", "=", "convert_samplerate", "(", "args", ".", "audio", ",", "desired_sample_rate", ")", "\n", "", "else", ":", "\n", "            ", "audio", "=", "np", ".", "frombuffer", "(", "fin", ".", "readframes", "(", "fin", ".", "getnframes", "(", ")", ")", ",", "np", ".", "int16", ")", "\n", "\n", "", "audio_length", "=", "fin", ".", "getnframes", "(", ")", "*", "(", "1", "/", "fs_orig", ")", "\n", "fin", ".", "close", "(", ")", "\n", "\n", "inference_start", "=", "timer", "(", ")", "\n", "# sphinx-doc: python_ref_inference_start", "\n", "text", "=", "self", ".", "ds", ".", "stt", "(", "audio", ")", "\n", "#print(text)", "\n", "# sphinx-doc: python_ref_inference_stop", "\n", "inference_end", "=", "timer", "(", ")", "-", "inference_start", "\n", "#print('Inference took %0.3fs for %0.3fs audio file.' % (inference_end, audio_length), file=sys.stderr)", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.convert_samplerate": [[22, 32], ["quote", "subprocess.check_output", "numpy.frombuffer", "shlex.split", "RuntimeError", "OSError"], "function", ["None"], ["", "def", "convert_samplerate", "(", "audio_path", ",", "desired_sample_rate", ")", ":", "\n", "    ", "sox_cmd", "=", "'sox {} --type raw --bits 16 --channels 1 --rate {} --encoding signed-integer --endian little --compression 0.0 --no-dither - '", ".", "format", "(", "quote", "(", "audio_path", ")", ",", "desired_sample_rate", ")", "\n", "try", ":", "\n", "        ", "output", "=", "subprocess", ".", "check_output", "(", "shlex", ".", "split", "(", "sox_cmd", ")", ",", "stderr", "=", "subprocess", ".", "PIPE", ")", "\n", "", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "\n", "        ", "raise", "RuntimeError", "(", "'SoX returned non-zero status: {}'", ".", "format", "(", "e", ".", "stderr", ")", ")", "\n", "", "except", "OSError", "as", "e", ":", "\n", "        ", "raise", "OSError", "(", "e", ".", "errno", ",", "'SoX not found, use {}hz files or install it: {}'", ".", "format", "(", "desired_sample_rate", ",", "e", ".", "strerror", ")", ")", "\n", "\n", "", "return", "desired_sample_rate", ",", "np", ".", "frombuffer", "(", "output", ",", "np", ".", "int16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.metadata_to_string": [[34, 36], ["None"], "function", ["None"], ["", "def", "metadata_to_string", "(", "metadata", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "token", ".", "text", "for", "token", "in", "metadata", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.words_from_candidate_transcript": [[38, 69], ["enumerate", "dict", "round", "round", "word_list.append", "len", "len"], "function", ["None"], ["", "def", "words_from_candidate_transcript", "(", "metadata", ")", ":", "\n", "    ", "word", "=", "\"\"", "\n", "word_list", "=", "[", "]", "\n", "word_start_time", "=", "0", "\n", "# Loop through each character", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "metadata", ".", "tokens", ")", ":", "\n", "# Append character to word if it's not a space", "\n", "        ", "if", "token", ".", "text", "!=", "\" \"", ":", "\n", "            ", "if", "len", "(", "word", ")", "==", "0", ":", "\n", "# Log the start time of the new word", "\n", "                ", "word_start_time", "=", "token", ".", "start_time", "\n", "\n", "", "word", "=", "word", "+", "token", ".", "text", "\n", "# Word boundary is either a space or the last character in the array", "\n", "", "if", "token", ".", "text", "==", "\" \"", "or", "i", "==", "len", "(", "metadata", ".", "tokens", ")", "-", "1", ":", "\n", "            ", "word_duration", "=", "token", ".", "start_time", "-", "word_start_time", "\n", "\n", "if", "word_duration", "<", "0", ":", "\n", "                ", "word_duration", "=", "0", "\n", "\n", "", "each_word", "=", "dict", "(", ")", "\n", "each_word", "[", "\"word\"", "]", "=", "word", "\n", "each_word", "[", "\"start_time\"", "]", "=", "round", "(", "word_start_time", ",", "4", ")", "\n", "each_word", "[", "\"duration\"", "]", "=", "round", "(", "word_duration", ",", "4", ")", "\n", "\n", "word_list", ".", "append", "(", "each_word", ")", "\n", "# Reset", "\n", "word", "=", "\"\"", "\n", "word_start_time", "=", "0", "\n", "\n", "", "", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.metadata_json_output": [[71, 78], ["dict", "json.dumps", "ASR.words_from_candidate_transcript"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Recognition.ASR.words_from_candidate_transcript"], ["", "def", "metadata_json_output", "(", "metadata", ")", ":", "\n", "    ", "json_result", "=", "dict", "(", ")", "\n", "json_result", "[", "\"transcripts\"", "]", "=", "[", "{", "\n", "\"confidence\"", ":", "transcript", ".", "confidence", ",", "\n", "\"words\"", ":", "words_from_candidate_transcript", "(", "transcript", ")", ",", "\n", "}", "for", "transcript", "in", "metadata", ".", "transcripts", "]", "\n", "return", "json", ".", "dumps", "(", "json_result", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.Text_Paraphrasing.Text_Paraphrasing.__init__": [[5, 11], ["Text_Paraphrasing.Text_Paraphrasing.model.init_session", "LAUG.nlg.scgpt.multiwoz.scgpt.SCGPT", "LAUG.nlg.scgpt.multiwoz.scgpt.SCGPT"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.scgpt.SCGPT.init_session"], ["    ", "def", "__init__", "(", "self", ",", "dataset", "=", "'multiwoz'", ")", ":", "\n", "        ", "if", "dataset", "==", "'multiwoz'", ":", "\n", "            ", "self", ".", "model", "=", "SCGPT", "(", ")", "\n", "", "if", "dataset", "==", "'frames'", ":", "\n", "            ", "self", ".", "model", "=", "SCGPT", "(", "model_file", "=", "'https://convlab.blob.core.windows.net/convlab-2/nlg-gpt-frames.zip'", ")", "\n", "", "self", ".", "model", ".", "init_session", "(", ")", "\n", "", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.Text_Paraphrasing.Text_Paraphrasing.aug": [[11, 16], ["LAUG.aug.Text_Paraphrasing.utils.span2tuple", "Text_Paraphrasing.Text_Paraphrasing.model.generate", "LAUG.aug.Text_Paraphrasing.utils.paraphrase_span_detection"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.utils.span2tuple", "home.repos.pwc.inspect_result.thu-coai_LAUG.nlg.nlg.NLG.generate", "home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.utils.paraphrase_span_detection"], ["", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n", "        ", "t", "=", "span2tuple", "(", "span_info", ")", "\n", "new_text", "=", "self", ".", "model", ".", "generate", "(", "t", ")", "\n", "new_span_info", "=", "paraphrase_span_detection", "(", "new_text", ",", "span_info", ")", "\n", "return", "new_text", ",", "new_span_info", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.utils.paraphrase_span_detection": [[4, 14], ["new_text.split", "span[].split", "LAUG.util.multiwoz.paraphrase_span_detection.phrase_idx_utt", "new_span_info.append"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.paraphrase_span_detection.phrase_idx_utt"], ["def", "paraphrase_span_detection", "(", "new_text", ",", "span_info", ")", ":", "\n", "    ", "new_words", "=", "new_text", ".", "split", "(", ")", "\n", "new_span_info", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "        ", "span_words", "=", "span", "[", "2", "]", ".", "split", "(", ")", "\n", "result", "=", "phrase_idx_utt", "(", "span_words", ",", "new_words", ")", "\n", "if", "result", "is", "not", "None", ":", "\n", "            ", "max_start", ",", "max_end", "=", "result", "\n", "new_span_info", ".", "append", "(", "[", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", ")", "\n", "", "", "return", "new_span_info", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Text_Paraphrasing.utils.span2tuple": [[16, 21], ["t.append", "span[].split", "span[].split"], "function", ["None"], ["", "def", "span2tuple", "(", "span_info", ")", ":", "\n", "    ", "t", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "        ", "t", ".", "append", "(", "(", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", ",", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "0", "]", ",", "span", "[", "1", "]", ",", "span", "[", "2", "]", ")", ")", "\n", "", "return", "t", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Word_Perturbation.Word_Perturbation.Word_Perturbation.__init__": [[6, 14], ["LAUG.aug.Word_Perturbation.multiwoz.aug_with_sgd_db.multiwoz_eda_config", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA", "LAUG.aug.Word_Perturbation.frames.aug_with_sgd_db.frames_eda_config", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset", "=", "'multiwoz'", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "if", "dataset", "==", "'multiwoz'", ":", "\n", "            ", "multiwoz_config", "=", "multiwoz_eda_config", "(", ")", "\n", "self", ".", "EDA", "=", "MultiwozEDA", "(", "multiwoz_config", ".", "multiwoz", ",", "multiwoz_config", ".", "db_loader", ")", "\n", "", "elif", "dataset", "==", "'frames'", ":", "\n", "            ", "frames_config", "=", "frames_eda_config", "(", ")", "\n", "self", ".", "EDA", "=", "MultiwozEDA", "(", "frames_config", ".", "frames", ",", "frames_config", ".", "db_loader", ")", "\n", "", "", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Word_Perturbation.Word_Perturbation.Word_Perturbation.aug": [[14, 17], ["Word_Perturbation.Word_Perturbation.EDA.augment_sentence_only"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_sentence_only"], ["", "", "def", "aug", "(", "self", ",", "text", ",", "span_info", ")", ":", "\n", "        ", "(", "new_text", ",", "new_span_info", ",", "_", ")", ",", "_", "=", "self", ".", "EDA", ".", "augment_sentence_only", "(", "text", ",", "span_info", ",", "{", "}", ")", "\n", "return", "new_text", ",", "new_span_info", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.aug_with_sgd_db.frames_eda_config.__init__": [[12, 39], ["aug_with_sgd_db.read_zipped_json", "os.path.join", "os.path.join", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoader", "os.path.join", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json"], ["    ", "def", "__init__", "(", "self", ",", ")", ":", "\n", "        ", "self", ".", "frames", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "DATA_ROOT", ",", "'frames/Ori'", ",", "'train.json.zip'", ")", ",", "'train.json'", ")", "\n", "\n", "frames_frames_domain_slot_map", "=", "{", "\n", "# ('frame', 'category'): ('hotel', 'category'),", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotel'", ",", "'location'", ")", ",", "\n", "# ('frame', 'gst_rating'): ('hotel', 'gst_rating'),", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotel'", ",", "'name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'trip'", ",", "'or_city'", ")", ",", "\n", "# ('frame', 'seat'): ('trip', 'seat'),", "\n", "}", "\n", "\n", "frames_sgd_domain_slot_map", "=", "{", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotels'", ",", "'dst_city'", ")", ",", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotels'", ",", "'hotel_name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'travel'", ",", "'location'", ")", ",", "\n", "}", "\n", "frames_db_dir", "=", "os", ".", "path", ".", "join", "(", "REPO_ROOT", ",", "\"LAUG/aug/Word_Perturbation/db/frames-db/\"", ")", "\n", "sgd_db_dir", "=", "os", ".", "path", ".", "join", "(", "REPO_ROOT", ",", "\"LAUG/aug/Word_Perturbation/db/sgd-db/\"", ")", "\n", "\n", "loader_args", "=", "[", "\n", "MultiSourceDBLoaderArgs", "(", "frames_db_dir", ",", "frames_frames_domain_slot_map", ")", ",", "\n", "MultiSourceDBLoaderArgs", "(", "sgd_db_dir", ",", "frames_sgd_domain_slot_map", ")", "\n", "]", "\n", "self", ".", "db_loader", "=", "MultiSourceDBLoader", "(", "loader_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.aug_with_sgd_db.read_zipped_json": [[5, 9], ["print", "zipfile.ZipFile", "json.load", "zipfile.ZipFile.open"], "function", ["None"], ["def", "read_zipped_json", "(", "filepath", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"zip file path = \"", ",", "filepath", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "filepath", ",", "'r'", ")", "\n", "return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.aug_with_sgd_db.main": [[43, 79], ["LAUG.aug.Word_Perturbation.multiwoz.util.load_json", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoader", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA", "LAUG.aug.Word_Perturbation.multiwoz.multiwoz_eda.MultiwozEDA.augment_multiwoz_dataset", "LAUG.aug.Word_Perturbation.multiwoz.util.dump_json", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs", "LAUG.aug.Word_Perturbation.multiwoz.db.slot_value_replace.MultiSourceDBLoaderArgs"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.load_json", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.multiwoz_eda.MultiwozEDA.augment_multiwoz_dataset", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.dump_json"], ["", "", "def", "main", "(", "frames_filepath", ",", "output_filepath", ",", "\n", "frames_db_dir", ",", "\n", "sgd_db_dir", ",", "\n", "alpha_sr", "=", "0.1", ",", "alpha_ri", "=", "0.1", ",", "alpha_rs", "=", "0.1", ",", "p_rd", "=", "0.1", ",", "num_aug", "=", "2", ",", "\n", "p_slot_value_replacement", "=", "0.25", ")", ":", "\n", "    ", "frames", "=", "load_json", "(", "frames_filepath", ")", "\n", "\n", "frames_frames_domain_slot_map", "=", "{", "\n", "# ('frame', 'category'): ('hotel', 'category'),", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotel'", ",", "'location'", ")", ",", "\n", "# ('frame', 'gst_rating'): ('hotel', 'gst_rating'),", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotel'", ",", "'name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'trip'", ",", "'or_city'", ")", ",", "\n", "# ('frame', 'seat'): ('trip', 'seat'),", "\n", "}", "\n", "\n", "frames_sgd_domain_slot_map", "=", "{", "\n", "(", "'frame'", ",", "'dst_city'", ")", ":", "(", "'hotels'", ",", "'dst_city'", ")", ",", "\n", "(", "'frame'", ",", "'name'", ")", ":", "(", "'hotels'", ",", "'hotel_name'", ")", ",", "\n", "\n", "(", "'frame'", ",", "'or_city'", ")", ":", "(", "'travel'", ",", "'location'", ")", ",", "\n", "}", "\n", "loader_args", "=", "[", "\n", "MultiSourceDBLoaderArgs", "(", "frames_db_dir", ",", "frames_frames_domain_slot_map", ")", ",", "\n", "MultiSourceDBLoaderArgs", "(", "sgd_db_dir", ",", "frames_sgd_domain_slot_map", ")", "\n", "]", "\n", "db_loader", "=", "MultiSourceDBLoader", "(", "loader_args", ")", "\n", "\n", "eda", "=", "MultiwozEDA", "(", "frames", ",", "db_loader", ",", "\n", "inform_intents", "=", "(", "'inform'", ",", "'switch_frame'", ",", "'confirm'", ")", ",", "\n", "slot_value_replacement_probability", "=", "p_slot_value_replacement", ",", "\n", "alpha_sr", "=", "alpha_sr", ",", "alpha_ri", "=", "alpha_ri", ",", "alpha_rs", "=", "alpha_rs", ",", "p_rd", "=", "p_rd", ",", "num_aug", "=", "num_aug", ")", "\n", "result", "=", "eda", ".", "augment_multiwoz_dataset", "(", "'usr'", ")", "\n", "\n", "dump_json", "(", "result", ",", "output_filepath", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json": [[12, 16], ["print", "zipfile.ZipFile", "json.load", "zipfile.ZipFile.open"], "function", ["None"], ["def", "read_zipped_json", "(", "filepath", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"zip file path = \"", ",", "filepath", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "filepath", ",", "'r'", ")", "\n", "return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.is_slot_da": [[5, 11], ["da[].split"], "function", ["None"], ["def", "is_slot_da", "(", "da", ")", ":", "\n", "    ", "tag_da", "=", "{", "'Inform'", ",", "'Select'", ",", "'Recommend'", ",", "'NoOffer'", ",", "'NoBook'", ",", "'OfferBook'", ",", "'OfferBooked'", ",", "'Book'", "}", "\n", "not_tag_slot", "=", "{", "'Internet'", ",", "'Parking'", ",", "'none'", "}", "\n", "if", "da", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "tag_da", "and", "da", "[", "1", "]", "not", "in", "not_tag_slot", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.calculateF1": [[13, 45], ["None"], "function", ["None"], ["", "def", "calculateF1", "(", "predict_golden", ",", "split", "=", "False", ")", ":", "\n", "    ", "TP", ",", "FP", ",", "FN", "=", "0", ",", "0", ",", "0", "\n", "for", "item", "in", "predict_golden", ":", "\n", "        ", "predicts", "=", "item", "[", "'predict'", "]", "\n", "if", "split", ":", "\n", "            ", "predicts", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "]", "for", "x", "in", "predicts", "]", "\n", "", "else", ":", "\n", "            ", "predicts", "=", "[", "x", "for", "x", "in", "predicts", "]", "\n", "\n", "", "labels", "=", "item", "[", "'golden'", "]", "\n", "if", "split", ":", "\n", "            ", "labels", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "]", "for", "x", "in", "labels", "]", "\n", "", "else", ":", "\n", "            ", "labels", "=", "[", "x", "for", "x", "in", "labels", "]", "\n", "\n", "#print('predicts = ', predicts)", "\n", "#print('labels = ', labels)", "\n", "#print('='*100)", "\n", "", "for", "ele", "in", "predicts", ":", "\n", "            ", "if", "ele", "in", "labels", ":", "\n", "                ", "TP", "+=", "1", "\n", "", "else", ":", "\n", "                ", "FP", "+=", "1", "\n", "", "", "for", "ele", "in", "labels", ":", "\n", "            ", "if", "ele", "not", "in", "predicts", ":", "\n", "                ", "FN", "+=", "1", "\n", "# print(TP, FP, FN)", "\n", "", "", "", "precision", "=", "1.0", "*", "TP", "/", "(", "TP", "+", "FP", ")", "if", "TP", "+", "FP", "else", "0.", "\n", "recall", "=", "1.0", "*", "TP", "/", "(", "TP", "+", "FN", ")", "if", "TP", "+", "FN", "else", "0.", "\n", "F1", "=", "2.0", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "if", "precision", "+", "recall", "else", "0.", "\n", "#print('*'*100)", "\n", "return", "precision", ",", "recall", ",", "F1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.tag2triples": [[47, 67], ["len", "len", "len", "tag.startswith", "tag[].split", "triples.append", "len", "tag_seq[].startswith"], "function", ["None"], ["", "def", "tag2triples", "(", "word_seq", ",", "tag_seq", ")", ":", "\n", "    ", "assert", "len", "(", "word_seq", ")", "==", "len", "(", "tag_seq", ")", "\n", "triples", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "tag_seq", ")", ":", "\n", "        ", "tag", "=", "tag_seq", "[", "i", "]", "\n", "if", "tag", ".", "startswith", "(", "'B'", ")", ":", "\n", "            ", "domain", ",", "slot", "=", "tag", "[", "2", ":", "]", ".", "split", "(", "'+'", ")", "\n", "value", "=", "word_seq", "[", "i", "]", "\n", "j", "=", "i", "+", "1", "\n", "while", "j", "<", "len", "(", "tag_seq", ")", ":", "\n", "                ", "if", "tag_seq", "[", "j", "]", ".", "startswith", "(", "'I'", ")", "and", "tag_seq", "[", "j", "]", "[", "2", ":", "]", "==", "tag", "[", "2", ":", "]", ":", "\n", "                    ", "value", "+=", "' '", "+", "word_seq", "[", "j", "]", "\n", "i", "+=", "1", "\n", "j", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "triples", ".", "append", "(", "[", "domain", ",", "slot", ",", "value", "]", ")", "\n", "", "i", "+=", "1", "\n", "", "return", "triples", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.intent2triples": [[69, 75], ["re.split", "triples.append"], "function", ["None"], ["", "def", "intent2triples", "(", "intent_seq", ")", ":", "\n", "    ", "triples", "=", "[", "]", "\n", "for", "intent", "in", "intent_seq", ":", "\n", "        ", "intent", ",", "slot", ",", "value", "=", "re", ".", "split", "(", "'[+*]'", ",", "intent", ")", "\n", "triples", ".", "append", "(", "[", "intent", ",", "slot", ",", "value", "]", ")", "\n", "", "return", "triples", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.recover_intent": [[77, 144], ["tag_logits.size", "range", "range", "range", "range", "intent.split", "range", "i.split", "intents.append", "enumerate", "postprocess.tag2triples", "overall.append", "overall.append", "overall.append", "reqs.append", "torch.max", "i.split", "tag.split", "tags.append", "tags.append", "len", "recover_tags.append", "tag_id.item"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.tag2triples"], ["", "def", "recover_intent", "(", "dataloader", ",", "intent_logits", ",", "req_logits", ",", "tag_logits", ",", "tag_mask_tensor", ",", "ori_word_seq", ",", "new2ori", ")", ":", "\n", "    ", "max_seq_len", "=", "tag_logits", ".", "size", "(", "1", ")", "\n", "intents", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "dataloader", ".", "intent_dim", ")", ":", "\n", "        ", "if", "intent_logits", "[", "j", "]", ">", "0", ":", "\n", "            ", "intents", ".", "append", "(", "dataloader", ".", "id2intent", "[", "j", "]", ")", "\n", "\n", "", "", "reqs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "req_dim", ")", ":", "\n", "        ", "req_intent", "=", "dataloader", ".", "id2req", "[", "i", "]", "\n", "for", "j", "in", "range", "(", "dataloader", ".", "req_slot_dim", ")", ":", "\n", "            ", "if", "req_logits", "[", "i", "]", "[", "j", "]", ">", "0", ":", "\n", "                ", "req_slot", "=", "dataloader", ".", "id2reqslot", "[", "j", "]", "\n", "reqs", ".", "append", "(", "req_intent", "+", "'-'", "+", "req_slot", ")", "\n", "#req_slot = dataloader.id2reqslot[torch.max(req_logits[i], dim=-1)[-1].item()]", "\n", "\n", "# if req_logits[j]>0:", "\n", "#     reqs.append(dataloader.id2req[j])", "\n", "#reqs.append(req_intent + '-' + req_slot)", "\n", "", "", "", "tag_intent", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "slot_intent_dim", ")", ":", "\n", "        ", "tags", "=", "[", "]", "\n", "intent", "=", "dataloader", ".", "id2slotintent", "[", "i", "]", "\n", "domain", ",", "base_intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "\n", "for", "j", "in", "range", "(", "3", ",", "max_seq_len", "-", "1", ")", ":", "\n", "            ", "if", "tag_mask_tensor", "[", "i", "]", "[", "j", "]", "==", "1", ":", "\n", "                ", "value", ",", "tag_id", "=", "torch", ".", "max", "(", "tag_logits", "[", "i", "]", "[", "j", "]", ",", "dim", "=", "-", "1", ")", "\n", "tag", "=", "dataloader", ".", "id2tag", "[", "tag_id", ".", "item", "(", ")", "]", "\n", "if", "tag", "!=", "'O'", ":", "\n", "                    ", "prefix", ",", "slot", "=", "tag", ".", "split", "(", "'-'", ")", "\n", "real_tag", "=", "prefix", "+", "'-'", "+", "intent", "+", "'+'", "+", "slot", "\n", "tags", ".", "append", "(", "real_tag", ")", "\n", "", "else", ":", "\n", "                    ", "tags", ".", "append", "(", "'O'", ")", "\n", "\n", "", "", "", "if", "tags", "!=", "[", "]", ":", "\n", "            ", "recover_tags", "=", "[", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "                ", "if", "new2ori", "[", "i", "]", ">=", "len", "(", "recover_tags", ")", ":", "\n", "                    ", "recover_tags", ".", "append", "(", "tag", ")", "\n", "", "", "tag_intent", "+=", "tag2triples", "(", "ori_word_seq", ",", "recover_tags", ")", "\n", "\n", "\n", "", "", "overall", "=", "[", "]", "\n", "for", "i", "in", "intents", ":", "\n", "        ", "if", "i", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "GENERAL_TYPE", ":", "\n", "            ", "overall", ".", "append", "(", "[", "i", ",", "None", ",", "None", "]", ")", "\n", "\n", "", "", "for", "i", "in", "reqs", ":", "\n", "        ", "domain", ",", "intent", ",", "slot", "=", "i", ".", "split", "(", "'-'", ")", "\n", "intent", "=", "domain", "+", "'-'", "+", "intent", "\n", "if", "intent", "in", "intents", ":", "\n", "            ", "overall", ".", "append", "(", "[", "intent", ",", "slot", ",", "None", "]", ")", "\n", "\n", "", "", "for", "i", "in", "tag_intent", ":", "\n", "        ", "intent", "=", "i", "[", "0", "]", "\n", "slot", "=", "i", "[", "1", "]", "\n", "if", "intent", "in", "intents", ":", "\n", "            ", "overall", ".", "append", "(", "[", "intent", ",", "i", "[", "1", "]", ",", "i", "[", "2", "]", "]", ")", "\n", "# print('intents = ', intents)", "\n", "# print('reqs = ', reqs)", "\n", "# print('tag_intent = ', tag_intent)", "\n", "# print('overall = ', overall)", "\n", "# print('='*100)", "\n", "", "", "return", "intents", ",", "reqs", ",", "tags", ",", "overall", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.da2triples": [[15, 22], ["dialog_act.items", "triples.append"], "function", ["None"], ["return", "json", ".", "load", "(", "archive", ".", "open", "(", "filename", ")", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "\n", "    ", "cur_dir", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "\n", "cur_dir", ")", ")", ")", ")", ",", "'data/frames/'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.preprocess": [[24, 181], ["os.path.dirname", "os.path.join", "os.path.join", "print", "print", "print", "print", "print", "print", "print", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "json.dump", "os.path.abspath", "os.path.exists", "os.makedirs", "preprocess.read_zipped_json", "print", "data[].items", "print", "json.dump", "len", "len", "len", "len", "len", "len", "len", "open", "open", "open", "open", "open", "open", "open", "os.path.join", "enumerate", "open", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "turn[].split", "enumerate", "turn[].update", "processed_data[].append", "context.append", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "dict().items", "len", "os.path.join", "context.append", "unidecode.unidecode", "dialog_act[].append", "enumerate", "turn[].pop", "context.append", "intents.append", "slot_intents.append", "tags.append", "used_tags.append", "dacts.split", "preprocess.da2triples", "i.split", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "span[].split", "tags.append", "used_tags.append", "span[].split", "tags.append", "used_tags.append", "intents.append", "processed_da.append", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "intents.append", "reqs.append", "req_slots.append", "processed_da.append"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.read_zipped_json", "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.preprocess.da2triples"], ["results", "=", "{", "}", "\n", "results_test", "=", "{", "}", "\n", "for", "key", "in", "keys", ":", "\n", "        ", "data_key", "=", "read_zipped_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "key", "+", "'.json.zip'", ")", ",", "key", "+", "'.json'", ")", "\n", "print", "(", "'load {}, size {}'", ".", "format", "(", "key", ",", "len", "(", "data_key", ")", ")", ")", "\n", "if", "key", "==", "'train'", "or", "key", "==", "'val'", ":", "\n", "            ", "results", "=", "dict", "(", "results", ",", "**", "data_key", ")", "\n", "", "else", ":", "\n", "            ", "results_test", "=", "dict", "(", "results_test", ",", "**", "data_key", ")", "\n", "\n", "", "", "def", "write_file", "(", "name", ",", "data", ",", "k", ")", ":", "\n", "        ", "with", "open", "(", "f'{name}.tsv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "ID", "in", "data", ":", "\n", "                ", "sess", "=", "data", "[", "ID", "]", "[", "'log'", "]", "\n", "da_uttr_history", "=", "[", "]", "\n", "for", "turn", "in", "sess", ":", "\n", "                    ", "if", "not", "turn", "[", "'dialog_act'", "]", ":", "\n", "                        ", "da_uttr_history", ".", "append", "(", "turn", "[", "'text'", "]", ".", "lower", "(", ")", ")", "\n", "continue", "\n", "", "da_seq", "=", "dict2seq", "(", "dict2dict", "(", "turn", "[", "'dialog_act'", "]", ")", ")", ".", "replace", "(", "'_'", ",", "' '", ")", ".", "lower", "(", ")", "\n", "da_uttr", "=", "turn", "[", "'text'", "]", ".", "lower", "(", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "k", ")", ":", "\n", "                        ", "pad", "=", "' '", "if", "i", ">", "1", "else", "' $ '", "\n", "if", "len", "(", "da_uttr_history", ")", ">=", "i", ":", "\n", "                            ", "da_uttr", "=", "da_uttr_history", "[", "-", "i", "]", "+", "pad", "+", "da_uttr", "\n", "\n", "", "", "f", ".", "write", "(", "f'{da_uttr}\\t{da_seq}\\n'", ")", "\n", "da_uttr_history", ".", "append", "(", "turn", "[", "'text'", "]", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "", "", "k", "=", "3", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data'", ")", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data'", ")", ")", "\n", "", "write_file", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data/train'", ")", ",", "results", ",", "k", ")", "\n", "write_file", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "'data/test'", ")", ",", "results_test", ",", "k", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.nlu.BERTNLU.__init__": [[16, 62], ["os.path.join", "json.load", "os.path.dirname", "os.path.join", "os.path.join", "json.load", "json.load", "json.load", "json.load", "json.load", "LAUG.nlu.jointBERT_new.dataloader.Dataloader", "print", "print", "os.path.join", "print", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.load_state_dict", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.to", "LAUG.nlu.jointBERT_new.jointBERT.JointBERT.eval", "spacy.load", "print", "os.path.dirname", "open", "os.path.dirname", "os.path.exists", "LAUG.nlu.jointBERT_new.frames.preprocess.preprocess", "open", "open", "open", "open", "open", "len", "len", "os.path.exists", "print", "LAUG.util.file_util.cached_path", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "zipfile.ZipFile.close", "torch.load", "os.path.abspath", "torch.cuda.is_available", "os.path.abspath", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.preprocess", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], ["\n", "return", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.nlu.BERTNLU.predict": [[63, 94], ["list", "nlu.BERTNLU.dataloader.bert_tokenize", "nlu.BERTNLU.dataloader.pad_batch", "tuple", "LAUG.nlu.jointBERT_new.frames.postprocess.recover_intent", "len", "nlu.BERTNLU.dataloader.tokenizer.encode", "nlu.BERTNLU.dataloader.tokenizer.encode", "torch.no_grad", "nlu.BERTNLU.model.forward", "intent.split", "dialog_act.append", "nlu.BERTNLU.nlp", "token.text.strip", "nlu.BERTNLU.dataloader.seq_intent2id", "t.to", "unidecode.unidecode.unidecode", "len", "type", "len", "nlu.BERTNLU.dataloader.seq_tag2id", "nlu.BERTNLU.dataloader.req_transfer"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.bert_tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.pad_batch", "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.postprocess.recover_intent", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.forward", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_intent2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_tag2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.req_transfer"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.run_single.set_seed": [[56, 61], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "args", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.run_single.top_k_top_p_filtering": [[63, 92], ["min", "float", "logits.size", "torch.sort", "torch.sort", "torch.cumsum", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "torch.softmax", "torch.topk", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size x vocabulary size)\n            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "top_k", "=", "min", "(", "top_k", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "dim", "=", "1", ",", "index", "=", "sorted_indices", ",", "src", "=", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.run_single.sample_sequence": [[94, 137], ["torch.tensor", "torch.tensor", "context.unsqueeze().repeat.unsqueeze().repeat", "torch.no_grad", "torch.no_grad", "range", "context.unsqueeze().repeat.unsqueeze", "model", "range", "run_single.top_k_top_p_filtering", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.tensor().view", "torch.tensor().view", "set", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.multinomial", "torch.multinomial", "generated[].tolist", "torch.softmax", "torch.zeros", "torch.zeros", "torch.full", "torch.full", "torch.tensor", "torch.tensor", "torch.argmax", "torch.argmax"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.top_k_top_p_filtering", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "sample_sequence", "(", "model", ",", "length", ",", "context", ",", "num_samples", "=", "1", ",", "temperature", "=", "1", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "repetition_penalty", "=", "1.0", ",", "\n", "is_xlnet", "=", "False", ",", "is_xlm_mlm", "=", "False", ",", "xlm_mask_token", "=", "None", ",", "xlm_lang", "=", "None", ",", "device", "=", "'cpu'", ")", ":", "\n", "    ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "context", "=", "context", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "num_samples", ",", "1", ")", "\n", "generated", "=", "context", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "length", ")", ":", "\n", "\n", "            ", "inputs", "=", "{", "'input_ids'", ":", "generated", "}", "\n", "if", "is_xlnet", ":", "\n", "# XLNet is a direct (predict same token, not next token) and bi-directional model by default", "\n", "# => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "zeros", "(", "(", "1", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "perm_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "perm_mask", "[", ":", ",", ":", ",", "-", "1", "]", "=", "1.0", "# Previous tokens don't see last token", "\n", "target_mapping", "=", "torch", ".", "zeros", "(", "(", "1", ",", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "target_mapping", "[", "0", ",", "0", ",", "-", "1", "]", "=", "1.0", "# predict last token", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", ",", "'perm_mask'", ":", "perm_mask", ",", "'target_mapping'", ":", "target_mapping", "}", "\n", "\n", "", "if", "is_xlm_mlm", "and", "xlm_mask_token", ":", "\n", "# XLM MLM models are direct models (predict same token, not next token)", "\n", "# => need one additional dummy token in the input (will be masked and guessed)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "full", "(", "(", "1", ",", "1", ")", ",", "xlm_mask_token", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", "}", "\n", "\n", "", "if", "xlm_lang", "is", "not", "None", ":", "\n", "                ", "inputs", "[", "\"langs\"", "]", "=", "torch", ".", "tensor", "(", "[", "xlm_lang", "]", "*", "inputs", "[", "\"input_ids\"", "]", ".", "shape", "[", "1", "]", ",", "device", "=", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "", "outputs", "=", "model", "(", "**", "inputs", ")", "# Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "/", "(", "temperature", "if", "temperature", ">", "0", "else", "1.", ")", "\n", "\n", "# repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "                ", "for", "_", "in", "set", "(", "generated", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                    ", "next_token_logits", "[", "i", ",", "_", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "filtered_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "if", "temperature", "==", "0", ":", "# greedy sampling:", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_token", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", "\n", "", "generated", "=", "torch", ".", "cat", "(", "(", "generated", ",", "next_token", ")", ",", "dim", "=", "1", ")", "\n", "", "", "return", "generated", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.run_single.main": [[139, 270], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "run_single.set_seed", "parser.parse_args.model_type.lower", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained.eval", "logger.info", "open", "range", "json.dump", "i.strip", "len", "logger.info", "tokenizer_class.from_pretrained.encode", "run_single.sample_sequence", "out[].tolist", "lines.split", "output_tests.append", "open", "logger.info", "hasattr", "hasattr", "tokenizer_class.from_pretrained.decode", "lines.split.append", "torch.cuda.is_available", "torch.cuda.is_available", "lines.split", "any", "logger.info", "bool", "tokenizer.decode.strip().lower", "MODEL_CLASSES.keys", "int", "tokenizer_class.from_pretrained.lang2id.keys", "input", "tokenizer.decode.find", "tokenizer.decode.strip", "tokenizer_class.from_pretrained.control_codes.values", "len", "len", "str", "list", "tokenizer_class.from_pretrained.lang2id.keys"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.sample_sequence", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Model type selected in the list: \"", "+", "\", \"", ".", "join", "(", "MODEL_CLASSES", ".", "keys", "(", ")", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pre-trained model or shortcut name selected in the list: \"", "+", "\", \"", ".", "join", "(", "ALL_MODELS", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--prompt\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--padding_text\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--xlm_lang\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"Optional language when used with the XLM model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--length\"", ",", "type", "=", "int", ",", "default", "=", "40", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_samples\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--temperature\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"temperature of 0 implies greedy sampling\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--repetition_penalty\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"primarily useful for CTRL model; in that case, use 1.2\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_k\"", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "\"--top_p\"", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--stop_token'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Token at which text generation is stopped\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--input_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--output_file'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--nc'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"number of sentence\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--use_token\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Avoid using CUDA when available\"", ")", "\n", "\n", "# parser.add_argument('--use_token', type=int, default=1,", "\n", "# help=\"number of sentence\")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "set_seed", "(", "args", ")", "\n", "\n", "args", ".", "model_type", "=", "args", ".", "model_type", ".", "lower", "(", ")", "\n", "model_class", ",", "tokenizer_class", "=", "MODEL_CLASSES", "[", "args", ".", "model_type", "]", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "args", ".", "model_name_or_path", ")", "\n", "model", ".", "to", "(", "args", ".", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "args", ".", "length", "<", "0", "and", "model", ".", "config", ".", "max_position_embeddings", ">", "0", ":", "\n", "        ", "args", ".", "length", "=", "model", ".", "config", ".", "max_position_embeddings", "\n", "", "elif", "0", "<", "model", ".", "config", ".", "max_position_embeddings", "<", "args", ".", "length", ":", "\n", "        ", "args", ".", "length", "=", "model", ".", "config", ".", "max_position_embeddings", "# No generation bigger than model size ", "\n", "", "elif", "args", ".", "length", "<", "0", ":", "\n", "        ", "args", ".", "length", "=", "MAX_LENGTH", "# avoid infinite loop", "\n", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "if", "args", ".", "model_type", "in", "[", "\"ctrl\"", "]", ":", "\n", "        ", "if", "args", ".", "temperature", ">", "0.7", ":", "\n", "            ", "logger", ".", "info", "(", "'CTRL typically works better with lower temperatures (and lower top_k).'", ")", "\n", "\n", "", "", "fin", "=", "open", "(", "args", ".", "input_file", ")", "\n", "inputs", "=", "[", "i", ".", "strip", "(", ")", "for", "i", "in", "fin", "]", "\n", "output_tests", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "0", ",", "len", "(", "inputs", ")", ",", "1", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"PROGRESS: {int(idx/len(inputs)*100)}%\"", ")", "\n", "xlm_lang", "=", "None", "\n", "# XLM Language usage detailed in the issues #1414", "\n", "if", "args", ".", "model_type", "in", "[", "\"xlm\"", "]", "and", "hasattr", "(", "tokenizer", ",", "'lang2id'", ")", "and", "hasattr", "(", "model", ".", "config", ",", "'use_lang_emb'", ")", "and", "model", ".", "config", ".", "use_lang_emb", ":", "\n", "            ", "if", "args", ".", "xlm_lang", ":", "\n", "                ", "language", "=", "args", ".", "xlm_lang", "\n", "", "else", ":", "\n", "                ", "language", "=", "None", "\n", "while", "language", "not", "in", "tokenizer", ".", "lang2id", ".", "keys", "(", ")", ":", "\n", "                    ", "language", "=", "input", "(", "\"Using XLM. Select language in \"", "+", "str", "(", "list", "(", "tokenizer", ".", "lang2id", ".", "keys", "(", ")", ")", ")", "+", "\" >>> \"", ")", "\n", "", "", "xlm_lang", "=", "tokenizer", ".", "lang2id", "[", "language", "]", "\n", "\n", "# XLM masked-language modeling (MLM) models need masked token (see details in sample_sequence)", "\n", "", "is_xlm_mlm", "=", "args", ".", "model_type", "in", "[", "\"xlm\"", "]", "and", "'mlm'", "in", "args", ".", "model_name_or_path", "\n", "if", "is_xlm_mlm", ":", "\n", "            ", "xlm_mask_token", "=", "tokenizer", ".", "mask_token_id", "\n", "", "else", ":", "\n", "            ", "xlm_mask_token", "=", "None", "\n", "\n", "# raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")", "\n", "", "lines", "=", "inputs", "[", "idx", "]", "\n", "raw_text", "=", "lines", ".", "split", "(", "' & '", ")", "[", "0", "]", "+", "' & '", "\n", "if", "args", ".", "model_type", "in", "[", "\"transfo-xl\"", ",", "\"xlnet\"", "]", ":", "\n", "# Models with memory likes to have a long prompt for short inputs.", "\n", "            ", "raw_text", "=", "(", "args", ".", "padding_text", "if", "args", ".", "padding_text", "else", "PADDING_TEXT", ")", "+", "raw_text", "\n", "\n", "", "context_tokens", "=", "tokenizer", ".", "encode", "(", "raw_text", ",", "add_special_tokens", "=", "False", ")", "\n", "\n", "if", "args", ".", "model_type", "==", "\"ctrl\"", ":", "\n", "            ", "if", "not", "any", "(", "context_tokens", "[", "0", "]", "==", "x", "for", "x", "in", "tokenizer", ".", "control_codes", ".", "values", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"WARNING! You are not starting your generation from a control code so you won't get good results\"", ")", "\n", "", "", "out", "=", "sample_sequence", "(", "\n", "model", "=", "model", ",", "\n", "context", "=", "context_tokens", ",", "\n", "num_samples", "=", "args", ".", "num_samples", ",", "\n", "length", "=", "args", ".", "length", ",", "\n", "temperature", "=", "args", ".", "temperature", ",", "\n", "top_k", "=", "args", ".", "top_k", ",", "\n", "top_p", "=", "args", ".", "top_p", ",", "\n", "repetition_penalty", "=", "args", ".", "repetition_penalty", ",", "\n", "is_xlnet", "=", "bool", "(", "args", ".", "model_type", "==", "\"xlnet\"", ")", ",", "\n", "is_xlm_mlm", "=", "is_xlm_mlm", ",", "\n", "xlm_mask_token", "=", "xlm_mask_token", ",", "\n", "xlm_lang", "=", "xlm_lang", ",", "\n", "device", "=", "args", ".", "device", ",", "\n", ")", "\n", "out", "=", "out", "[", ":", ",", "len", "(", "context_tokens", ")", ":", "]", ".", "tolist", "(", ")", "\n", "#examples = []", "\n", "examples", "=", "lines", ".", "split", "(", "' & '", ")", "\n", "for", "o", "in", "out", ":", "\n", "            ", "text", "=", "tokenizer", ".", "decode", "(", "o", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "text", "=", "text", "[", ":", "text", ".", "find", "(", "args", ".", "stop_token", ")", "if", "args", ".", "stop_token", "else", "None", "]", "\n", "examples", ".", "append", "(", "text", ".", "strip", "(", ")", ".", "lower", "(", ")", ")", "\n", "\n", "", "output_tests", ".", "append", "(", "examples", ")", "\n", "# break", "\n", "# if args.prompt:", "\n", "# break", "\n", "", "import", "json", "\n", "json", ".", "dump", "(", "output_tests", ",", "open", "(", "args", ".", "output_file", ",", "'w'", ")", ",", "indent", "=", "2", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.nlu.MILU.__init__": [[28, 55], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.data.tokenizers.word_splitter.SpacyWordSplitter", "nlu.MILU.tokenizer.spacy.tokenizer.add_special_case", "allennlp.data.DatasetReader.from_params", "nlu.MILU.model.eval", "torch.cuda.is_available", "os.path.isfile", "LAUG.util.file_util.cached_path", "Exception"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.frames.nlu.MILU.predict": [[56, 83], ["list", "nlu.MILU.tokenizer.split_words", "nlu.MILU.dataset_reader.text_to_instance", "nlu.MILU.model.forward_on_instance", "outputs[].items", "len", "sum", "nlu.MILU.tokenizer.split_words", "len", "domain_intent.split", "tuples.append", "nlu.MILU.tokenizer.split_words"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.BaseDBLoader.load_db": [[26, 29], ["None"], "methods", ["None"], ["    ", "def", "load_db", "(", "self", ",", "domain", ":", "str", ",", "slot", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Optional", "[", "BaseDB", "]", ":", "\n", "        ", "\"\"\"given a domain and a slot, load corresponding db.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.DBLoader.__init__": [[32, 37], ["db_loader.list_db_filepath", "os.path.isdir"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.list_db_filepath"], ["    ", "def", "__init__", "(", "self", ",", "db_dir", ")", ":", "\n", "        ", "assert", "db_dir", "and", "os", ".", "path", ".", "isdir", "(", "db_dir", ")", "\n", "self", ".", "db_dir", "=", "db_dir", "\n", "self", ".", "db_files", "=", "list_db_filepath", "(", "db_dir", ")", "# domain -->  filepath", "\n", "self", ".", "db_cache", "=", "{", "}", "# domain --> List[dict]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.DBLoader._get_db_file": [[38, 45], ["functools.lru_cache", "db_loader.DBLoader.db_files.items", "domain.lower", "dom.lower"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "25", ")", "\n", "def", "_get_db_file", "(", "self", ",", "domain", ")", ":", "\n", "        ", "if", "domain", "in", "self", ".", "db_files", ":", "\n", "            ", "return", "self", ".", "db_files", "[", "domain", "]", "\n", "", "for", "dom", ",", "filename", "in", "self", ".", "db_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "domain", ".", "lower", "(", ")", "in", "dom", ".", "lower", "(", ")", ":", "\n", "                ", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.DBLoader.load_db": [[46, 53], ["db_loader.DBLoader._get_db_file", "db.DB", "util.load_json"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.DBLoader._get_db_file", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.load_json"], ["", "", "", "def", "load_db", "(", "self", ",", "domain", ":", "str", ",", "slot", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Optional", "[", "DB", "]", ":", "\n", "        ", "filepath", "=", "self", ".", "_get_db_file", "(", "domain", ")", "\n", "if", "filepath", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "if", "domain", "not", "in", "self", ".", "db_cache", ":", "\n", "            ", "self", ".", "db_cache", "[", "domain", "]", "=", "DB", "(", "load_json", "(", "filepath", ")", ")", "\n", "", "return", "self", ".", "db_cache", "[", "domain", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.list_db_filename": [[10, 19], ["os.listdir", "re.match", "os.path.isfile", "re.match.group", "os.path.join"], "function", ["None"], ["def", "list_db_filename", "(", "db_dir", ")", ":", "\n", "    ", "filenames", "=", "os", ".", "listdir", "(", "db_dir", ")", "\n", "db_filenames", "=", "{", "}", "\n", "for", "filename", "in", "filenames", ":", "\n", "        ", "match", "=", "re", ".", "match", "(", "r'^(\\w+)_db\\.json$'", ",", "filename", ")", "\n", "if", "match", "is", "not", "None", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "db_dir", ",", "filename", ")", ")", ":", "\n", "            ", "domain", "=", "match", ".", "group", "(", "1", ")", "\n", "db_filenames", "[", "domain", "]", "=", "filename", "\n", "", "", "return", "db_filenames", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.list_db_filepath": [[21, 23], ["os.path.join", "list_db_filename().items", "db_loader.list_db_filename"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.db_loader.list_db_filename"], ["", "def", "list_db_filepath", "(", "db_dir", ")", ":", "\n", "    ", "return", "{", "domain", ":", "os", ".", "path", ".", "join", "(", "db_dir", ",", "filename", ")", "for", "domain", ",", "filename", "in", "list_db_filename", "(", "db_dir", ")", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db.DB.query": [[14, 23], ["isinstance", "callable", "isinstance", "all", "fn"], "methods", ["None"], ["def", "query", "(", "self", ",", "conditions", ":", "Union", "[", "dict", ",", "Callable", "[", "[", "dict", "]", ",", "bool", "]", ",", "None", "]", ")", "->", "List", "[", "dict", "]", ":", "\n", "        ", "if", "conditions", "is", "None", ":", "\n", "            ", "return", "self", "\n", "", "assert", "callable", "(", "conditions", ")", "or", "isinstance", "(", "conditions", ",", "dict", ")", "\n", "if", "isinstance", "(", "conditions", ",", "dict", ")", ":", "\n", "            ", "fn", "=", "lambda", "item", ":", "all", "(", "item", "[", "k", "]", "==", "conditions", "[", "k", "]", "for", "k", "in", "conditions", "if", "k", "in", "item", ")", "\n", "", "else", ":", "\n", "            ", "fn", "=", "conditions", "\n", "", "return", "[", "item", "for", "item", "in", "self", "if", "fn", "(", "item", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.db.DB.sample": [[24, 32], ["db.DB.query", "util.choice"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.db.DB.query", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["", "def", "sample", "(", "self", ",", "conditions", "=", "None", ")", "->", "Optional", "[", "dict", "]", ":", "\n", "        ", "list_", "=", "self", ".", "query", "(", "conditions", ")", "\n", "if", "list_", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "choice", "(", "list_", ")", "\n", "", "except", "(", "IndexError", ",", "ValueError", ")", ":", "\n", "                ", "pass", "\n", "", "", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader._parse_init_args": [[16, 31], ["isinstance", "isinstance", "isinstance", "isinstance", "MultiSourceDBLoaderArgs", "slot_value_replace.MultiSourceDBLoader._parse_init_args.toMultiSourceDBLoaderArgs"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "_parse_init_args", "(", "args", ")", "->", "List", "[", "MultiSourceDBLoaderArgs", "]", ":", "\n", "        ", "assert", "isinstance", "(", "args", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "isinstance", "(", "args", ",", "MultiSourceDBLoaderArgs", ")", ":", "\n", "            ", "return", "[", "args", "]", "\n", "\n", "", "def", "toMultiSourceDBLoaderArgs", "(", "arg", ")", ":", "\n", "            ", "if", "isinstance", "(", "arg", ",", "MultiSourceDBLoaderArgs", ")", ":", "\n", "                ", "return", "arg", "\n", "", "assert", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "arg", ")", "==", "len", "(", "MultiSourceDBLoaderArgs", ".", "_fields", ")", "\n", "return", "MultiSourceDBLoaderArgs", "(", "*", "arg", ")", "\n", "\n", "", "args", "=", "[", "toMultiSourceDBLoaderArgs", "(", "arg", ")", "for", "arg", "in", "args", "]", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader.__init__": [[32, 38], ["slot_value_replace.MultiSourceDBLoader._parse_init_args", "db_loader.DBLoader", "slot_value_replace.MultiSourceDBLoader.loaders_and_maps.append"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB._parse_init_args"], ["", "def", "__init__", "(", "self", ",", "args", ":", "Union", "[", "List", "[", "MultiSourceDBLoaderArgs", "]", ",", "List", "[", "tuple", "]", ",", "MultiSourceDBLoaderArgs", "]", ")", ":", "\n", "        ", "self", ".", "loaders_and_maps", "=", "[", "]", "\n", "args", "=", "self", ".", "_parse_init_args", "(", "args", ")", "\n", "for", "db_dir", ",", "domain_slot_map", "in", "args", ":", "\n", "            ", "loader", "=", "DBLoader", "(", "db_dir", ")", "\n", "self", ".", "loaders_and_maps", ".", "append", "(", "(", "loader", ",", "domain_slot_map", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader.load_db": [[39, 62], ["slot_value_replace.MultiSourceDB", "domain_slot_map.items", "dbs.extend", "loader.load_db", "domain.lower", "slot.lower", "dbs.append", "domain.lower", "domain_slot_tuple[].lower", "loader.load_db", "domain_to_db.items", "domain.lower", "slot.lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader.load_db", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader.load_db"], ["", "", "def", "load_db", "(", "self", ",", "domain", ",", "slot", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "Optional", "[", "\"MultiSourceDB\"", "]", ":", "\n", "        ", "dbs", "=", "[", "]", "\n", "for", "loader", ",", "domain_slot_map", "in", "self", ".", "loaders_and_maps", ":", "\n", "            ", "if", "slot", "is", "not", "None", ":", "\n", "                ", "if", "(", "domain", ".", "lower", "(", ")", ",", "slot", ".", "lower", "(", ")", ")", "in", "domain_slot_map", ":", "\n", "                    ", "db_domain", ",", "db_slot", "=", "domain_slot_map", "[", "(", "domain", ".", "lower", "(", ")", ",", "slot", ".", "lower", "(", ")", ")", "]", "\n", "db", "=", "loader", ".", "load_db", "(", "db_domain", ",", "db_slot", ")", "\n", "if", "db", "is", "not", "None", ":", "\n", "                        ", "dbs", ".", "append", "(", "(", "db", ",", "db_domain", ",", "domain_slot_map", ")", ")", "\n", "", "", "", "else", ":", "\n", "                ", "domain_to_db", "=", "{", "}", "\n", "for", "domain_slot_tuple", ",", "db_domain_slot_tuple", "in", "domain_slot_map", ".", "items", "(", ")", ":", "\n", "                    ", "if", "domain", ".", "lower", "(", ")", "==", "domain_slot_tuple", "[", "0", "]", ".", "lower", "(", ")", ":", "\n", "                        ", "db_domain", "=", "db_domain_slot_tuple", "[", "0", "]", "\n", "if", "db_domain", "not", "in", "domain_to_db", ":", "\n", "                            ", "db", "=", "loader", ".", "load_db", "(", "db_domain", ")", "\n", "if", "db", "is", "not", "None", ":", "\n", "                                ", "domain_to_db", "[", "db_domain", "]", "=", "db", "\n", "", "", "", "", "dbs", ".", "extend", "(", "(", "db", ",", "db_domain", ",", "domain_slot_map", ")", "for", "db_domain", ",", "db", "in", "domain_to_db", ".", "items", "(", ")", ")", "\n", "\n", "", "", "if", "not", "dbs", ":", "\n", "            ", "return", "None", "\n", "", "return", "MultiSourceDB", "(", "dbs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB._parse_init_args": [[68, 83], ["isinstance", "isinstance", "isinstance", "isinstance", "MultiSourceDBArgs", "slot_value_replace.MultiSourceDB._parse_init_args.toMultiSourceDBArgs"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "_parse_init_args", "(", "args", ")", "->", "List", "[", "MultiSourceDBArgs", "]", ":", "\n", "        ", "if", "isinstance", "(", "args", ",", "MultiSourceDBArgs", ")", ":", "\n", "            ", "return", "[", "args", "]", "\n", "", "assert", "isinstance", "(", "args", ",", "(", "list", ",", "tuple", ")", ")", "\n", "\n", "def", "toMultiSourceDBArgs", "(", "arg", ")", ":", "\n", "            ", "if", "isinstance", "(", "arg", ",", "MultiSourceDBArgs", ")", ":", "\n", "                ", "return", "arg", "\n", "", "assert", "isinstance", "(", "arg", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "arg", ")", "==", "len", "(", "MultiSourceDBArgs", ".", "_fields", ")", "\n", "return", "MultiSourceDBArgs", "(", "*", "arg", ")", "\n", "\n", "", "args", "=", "[", "toMultiSourceDBArgs", "(", "arg", ")", "for", "arg", "in", "args", "]", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB.__init__": [[84, 86], ["slot_value_replace.MultiSourceDB._parse_init_args"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB._parse_init_args"], ["", "def", "__init__", "(", "self", ",", "args", ":", "Union", "[", "MultiSourceDBArgs", ",", "List", "[", "MultiSourceDBArgs", "]", ",", "List", "[", "tuple", "]", "]", ")", ":", "\n", "        ", "self", ".", "args", "=", "self", ".", "_parse_init_args", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB.find_different_values": [[87, 99], ["db.query", "domain.lower", "slot.lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.db.DB.query"], ["", "def", "find_different_values", "(", "self", ",", "domain", ",", "slot", ",", "excluding_values", "=", "(", ")", ")", "->", "Iterable", ":", "\n", "        ", "\"\"\"find different values, which belong to the same domain and slot.\"\"\"", "\n", "for", "db", ",", "db_domain", ",", "domain_slot_map", "in", "self", ".", "args", ":", "\n", "            ", "k", "=", "(", "domain", ".", "lower", "(", ")", ",", "slot", ".", "lower", "(", ")", ")", "\n", "if", "k", "not", "in", "domain_slot_map", ":", "\n", "                ", "continue", "\n", "", "if", "domain_slot_map", "[", "k", "]", "[", "0", "]", "!=", "db_domain", ":", "\n", "                ", "continue", "\n", "", "db_domain", ",", "db_slot", "=", "domain_slot_map", "[", "k", "]", "\n", "r", "=", "db", ".", "query", "(", "\n", "lambda", "item", ":", "db_slot", "in", "item", "and", "item", "[", "db_slot", "]", "not", "in", "excluding_values", ")", "\n", "yield", "from", "(", "dict_", "[", "db_slot", "]", "for", "dict_", "in", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB.sample_value": [[100, 106], ["slot_value_replace.MultiSourceDB.find_different_values", "db.choice"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB.find_different_values", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.choice"], ["", "", "def", "sample_value", "(", "self", ",", "domain", ",", "slot", ",", "excluding_values", "=", "(", ")", ")", ":", "\n", "        ", "values", "=", "self", ".", "find_different_values", "(", "domain", ",", "slot", ",", "excluding_values", "=", "excluding_values", ")", "\n", "try", ":", "\n", "            ", "return", "choice", "(", "values", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace._get_word2indexes": [[108, 116], ["collections.defaultdict", "enumerate", "word2indexes[].append", "word2indexes[].append", "word.lower"], "function", ["None"], ["", "", "", "def", "_get_word2indexes", "(", "words", ",", "to_lower", "=", "False", ")", ":", "\n", "    ", "word2indexes", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "if", "to_lower", ":", "\n", "            ", "word2indexes", "[", "word", ".", "lower", "(", ")", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "word2indexes", "[", "word", "]", ".", "append", "(", "i", ")", "\n", "", "", "return", "word2indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace._get_positions": [[118, 124], ["len", "word_to_indexes.get"], "function", ["None"], ["", "def", "_get_positions", "(", "words", ":", "List", "[", "str", "]", ",", "word_to_indexes", ":", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ",", "value", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "first_word", "=", "value", "[", "0", "]", "\n", "N", "=", "len", "(", "value", ")", "\n", "for", "first_index", "in", "word_to_indexes", ".", "get", "(", "first_word", ",", "(", ")", ")", ":", "\n", "        ", "if", "words", "[", "first_index", ":", "first_index", "+", "N", "]", "==", "value", ":", "\n", "            ", "return", "[", "first_index", ",", "first_index", "+", "N", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.fix_text": [[126, 131], ["re.sub"], "function", ["None"], ["", "", "", "def", "fix_text", "(", "text", ")", ":", "\n", "# strip; split punctuation and word", "\n", "    ", "text", "=", "re", ".", "sub", "(", "r\"(?:^|(?<=\\s))([\\w$']+)([,.?*/!;<=>\\]\\\"]+)(?:$|(?=[A-Z\\s]))\"", ",", "r'\\1 \\2'", ",", "\n", "text", ")", "# split word and punctuation", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.fix_turn": [[133, 174], ["tokenize_util.tokenize", "slot_value_replace._get_word2indexes", "enumerate", "span_info.sort", "domain_intent.split", "range", "len", "slot_value_replace._get_positions", "range", "slot_value_replace._get_positions", "ValueError", "len", "tokenize_util.tokenize", "len", "tokenize_util.tokenize", "util.p_str", "util.p_str", "util.p_str", "util.p_str", "tokenize_util.tokenize"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace._get_word2indexes", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace._get_positions", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace._get_positions", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "def", "fix_turn", "(", "turn", ":", "dict", ")", ":", "\n", "# fix error in a turn", "\n", "# turn = {", "\n", "#     'text': ...,", "\n", "#     'span_info': ...,", "\n", "#     'dialog_act': ...", "\n", "# }", "\n", "    ", "text", "=", "turn", "[", "'text'", "]", "\n", "words", "=", "tokenize", "(", "text", ")", "\n", "word2indexes", "=", "_get_word2indexes", "(", "words", ",", "to_lower", "=", "False", ")", "\n", "span_info", "=", "turn", "[", "'span_info'", "]", "\n", "dialog_act", "=", "turn", "[", "'dialog_act'", "]", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "span_info", ")", ":", "\n", "        ", "domain_intent", ",", "slot", ",", "value", ",", "*", "positions", "=", "item", "\n", "assert", "len", "(", "positions", ")", "==", "2", "\n", "domain", ",", "intent", "=", "domain_intent", ".", "split", "(", "'-'", ")", "\n", "if", "' '", ".", "join", "(", "words", "[", "positions", "[", "0", "]", ":", "positions", "[", "1", "]", "+", "1", "]", ")", "!=", "value", ":", "\n", "            ", "positions", "=", "None", "\n", "", "if", "positions", "is", "None", ":", "\n", "            ", "positions", "=", "_get_positions", "(", "words", ",", "word2indexes", ",", "tokenize", "(", "value", ")", ")", "\n", "", "if", "positions", "is", "None", ":", "\n", "            ", "slot_value_list", "=", "dialog_act", "[", "domain_intent", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "slot_value_list", ")", ")", ":", "\n", "                ", "if", "slot_value_list", "[", "i", "]", "[", "0", "]", "==", "slot", ":", "\n", "                    ", "value", "=", "slot_value_list", "[", "i", "]", "[", "1", "]", "\n", "break", "\n", "", "", "positions", "=", "_get_positions", "(", "words", ",", "word2indexes", ",", "tokenize", "(", "value", ")", ")", "\n", "", "if", "positions", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "f\"turn: {p_str(turn)}\\nitem: {p_str(item)}\\nwords: {p_str(words)}\\n\"", "\n", "f\"word2indexes {p_str(word2indexes)}\\nvalue: {tokenize(value)}\"", ")", "\n", "", "value", "=", "' '", ".", "join", "(", "words", "[", "positions", "[", "0", "]", ":", "1", "+", "positions", "[", "1", "]", "]", ")", "\n", "item", "[", "2", "]", "=", "value", "\n", "if", "item", "[", "-", "2", ":", "]", "!=", "positions", ":", "\n", "            ", "item", "[", "-", "2", ":", "]", "=", "positions", "\n", "", "if", "domain_intent", "not", "in", "dialog_act", ":", "\n", "            ", "continue", "\n", "", "slot_value_list", "=", "dialog_act", "[", "domain_intent", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "slot_value_list", ")", ")", ":", "\n", "            ", "if", "slot_value_list", "[", "i", "]", "[", "0", "]", "==", "slot", ":", "\n", "                ", "slot_value_list", "[", "i", "]", "[", "1", "]", "=", "value", "\n", "", "", "", "span_info", ".", "sort", "(", "key", "=", "lambda", "item", ":", "item", "[", "-", "2", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.assert_correct_turn": [[176, 195], ["tokenize_util.tokenize", "new_dialog_act.items", "new_dialog_act.setdefault", "new_dialog_act[].append", "util.p_str", "all", "tokenize_util.tokenize", "tuple", "tuple", "util.p_str", "util.p_str", "util.p_str"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.util.p_str"], ["", "def", "assert_correct_turn", "(", "turn", ":", "dict", ")", ":", "\n", "    ", "text", "=", "turn", "[", "'text'", "]", "\n", "words", "=", "tokenize", "(", "text", ")", "\n", "span_info", "=", "turn", "[", "'span_info'", "]", "\n", "dialog_act", "=", "turn", "[", "'dialog_act'", "]", "\n", "new_dialog_act", "=", "{", "}", "\n", "for", "item", "in", "span_info", ":", "\n", "        ", "domain_intent", ",", "slot", ",", "value", ",", "begin", ",", "end", "=", "item", "\n", "assert", "words", "[", "begin", ":", "1", "+", "end", "]", "==", "tokenize", "(", "value", ")", ",", "f\"turn: {p_str(turn)}\\nitem: {item}\"", "\n", "new_dialog_act", ".", "setdefault", "(", "domain_intent", ",", "[", "]", ")", "\n", "new_dialog_act", "[", "domain_intent", "]", ".", "append", "(", "[", "slot", ",", "value", "]", ")", "\n", "", "for", "domain_intent", ",", "new_slot_value_list", "in", "new_dialog_act", ".", "items", "(", ")", ":", "\n", "        ", "assert", "domain_intent", "in", "dialog_act", "\n", "new_slot_value_set", "=", "{", "tuple", "(", "slot_value", ")", "for", "slot_value", "in", "new_slot_value_list", "}", "\n", "slot_value_list", "=", "dialog_act", "[", "domain_intent", "]", "\n", "slot_value_set", "=", "{", "tuple", "(", "slot_value", ")", "for", "slot_value", "in", "slot_value_list", "}", "\n", "assert", "new_slot_value_set", "<=", "slot_value_set", ",", "p_str", "(", "[", "turn", ",", "new_dialog_act", "]", ")", "\n", "diff", "=", "slot_value_set", "-", "new_slot_value_set", "\n", "assert", "all", "(", "slot", "==", "'none'", "or", "value", "==", "'?'", "for", "slot", ",", "value", "in", "diff", ")", ",", "f\"Error, {p_str(turn)}\\n{p_str(diff)}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.replace_slot_values_in_turn": [[197, 264], ["copy.deepcopy", "tokenize_util.tokenize", "span_info.sort", "any", "copy.deepcopy", "tokenize_util.tokenize.copy", "enumerate", "slot_value_replace.fix_turn", "slot_value_replace.assert_correct_turn", "domain_intent.split", "db_loader.load_db", "db_loader.load_db.sample_value", "slot_value_replace.fix_text", "range", "updated_span.append", "range", "slot_value_replace.assert_correct_turn", "intent.lower", "random.random", "len", "len", "tokenize_util.tokenize", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.fix_turn", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.assert_correct_turn", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDBLoader.load_db", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.MultiSourceDB.sample_value", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.fix_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.assert_correct_turn", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "", "def", "replace_slot_values_in_turn", "(", "turn", ":", "dict", ",", "db_loader", ":", "MultiSourceDBLoader", ",", "\n", "p", "=", "0.25", ",", "\n", "inform_intents", "=", "(", "'inform'", ",", ")", ")", ":", "\n", "    ", "orig_turn", "=", "turn", "\n", "turn", "=", "deepcopy", "(", "orig_turn", ")", "\n", "try", ":", "\n", "        ", "fix_turn", "(", "turn", ")", "\n", "assert_correct_turn", "(", "turn", ")", "\n", "", "except", ":", "\n", "        ", "return", "orig_turn", "\n", "", "text", "=", "turn", "[", "'text'", "]", "\n", "words", "=", "tokenize", "(", "text", ")", "\n", "span_info", "=", "turn", "[", "'span_info'", "]", "\n", "span_info", ".", "sort", "(", "key", "=", "lambda", "item", ":", "item", "[", "-", "2", ":", "]", ")", "\n", "dialog_act", "=", "turn", "[", "'dialog_act'", "]", "\n", "if", "any", "(", "span_info", "[", "i", "]", "[", "-", "2", "]", "<=", "span_info", "[", "i", "-", "1", "]", "[", "-", "1", "]", "for", "i", "in", "range", "(", "1", ",", "len", "(", "span_info", ")", ")", ")", ":", "\n", "        ", "return", "turn", "\n", "\n", "", "new_turn", "=", "deepcopy", "(", "turn", ")", "\n", "new_words", "=", "words", ".", "copy", "(", ")", "\n", "new_span_info", "=", "new_turn", "[", "'span_info'", "]", "\n", "new_dialog_act", "=", "new_turn", "[", "'dialog_act'", "]", "\n", "updated_span", "=", "[", "]", "\n", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "span_info", ")", ":", "\n", "        ", "domain_intent", "=", "item", "[", "0", "]", "\n", "domain", ",", "intent", "=", "domain_intent", ".", "split", "(", "'-'", ")", "\n", "slot", "=", "item", "[", "1", "]", "\n", "value", "=", "item", "[", "2", "]", "\n", "if", "intent", ".", "lower", "(", ")", "not", "in", "inform_intents", ":", "\n", "            ", "continue", "\n", "", "if", "updated_span", ":", "\n", "            ", "j", "=", "updated_span", "[", "-", "1", "]", "\n", "last_item", "=", "span_info", "[", "j", "]", "\n", "if", "item", "[", "-", "2", "]", "<=", "last_item", "[", "-", "1", "]", ":", "\n", "                ", "continue", "\n", "", "", "db", "=", "db_loader", ".", "load_db", "(", "domain", ",", "slot", ")", "\n", "if", "db", "is", "None", ":", "\n", "            ", "continue", "\n", "", "new_value", "=", "db", ".", "sample_value", "(", "domain", ",", "slot", ",", "excluding_values", "=", "(", "value", ",", "'none'", ",", "'?'", ")", ")", "\n", "if", "new_value", "is", "None", ":", "\n", "            ", "continue", "\n", "", "if", "random", ".", "random", "(", ")", ">", "p", ":", "\n", "            ", "continue", "\n", "", "new_value", "=", "fix_text", "(", "new_value", ")", "\n", "new_span_info", "[", "i", "]", "[", "2", "]", "=", "new_value", "\n", "new_slot_value_list", "=", "new_dialog_act", "[", "domain_intent", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "new_slot_value_list", ")", ")", ":", "\n", "            ", "if", "new_slot_value_list", "[", "j", "]", "[", "0", "]", "==", "slot", ":", "\n", "                ", "new_slot_value_list", "[", "j", "]", "[", "1", "]", "=", "new_value", "\n", "", "", "updated_span", ".", "append", "(", "i", ")", "\n", "# print(f'replace {item[2]} with {new_value}')", "\n", "\n", "# update new_words and span in new_span_info", "\n", "", "if", "updated_span", ":", "\n", "        ", "offset", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "span_info", ")", ")", ":", "\n", "            ", "begin", ",", "end", "=", "span_info", "[", "i", "]", "[", "-", "2", ":", "]", "\n", "new_value", "=", "new_span_info", "[", "i", "]", "[", "2", "]", "\n", "new_value", "=", "tokenize", "(", "new_value", ")", "\n", "num_words", "=", "len", "(", "new_value", ")", "\n", "new_words", "[", "offset", "+", "begin", ":", "offset", "+", "end", "+", "1", "]", "=", "new_value", "\n", "new_span_info", "[", "i", "]", "[", "-", "2", ":", "]", "=", "[", "begin", "+", "offset", ",", "begin", "+", "offset", "+", "num_words", "-", "1", "]", "\n", "offset", "+=", "num_words", "-", "(", "end", "-", "begin", "+", "1", ")", "\n", "", "new_turn", "[", "'text'", "]", "=", "' '", ".", "join", "(", "new_words", ")", "\n", "assert_correct_turn", "(", "new_turn", ")", "\n", "", "return", "new_turn", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.replace_slot_values": [[266, 292], ["copy.deepcopy", "enumerate", "slot_value_replace.replace_slot_values_in_turn"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.db.slot_value_replace.replace_slot_values_in_turn"], ["", "def", "replace_slot_values", "(", "sample", ",", "db_loader", ":", "MultiSourceDBLoader", ",", "\n", "p", "=", "0.25", ",", "\n", "inform_intents", "=", "(", "'inform'", ",", ")", ",", "\n", "mode", "=", "'usr'", ")", ":", "\n", "    ", "\"\"\"\n    replace slot values in a sample\n\n    Args:\n        sample: a dialogue\n        db_loader: it can loads a db\n        p: probability to replace if conditions are satisfied\n        inform_intents: only inform intents may be replaced.\n        mode: 'usr' or 'user': only replace on user turns;\n              'sys': on;y replace on sys turns;\n              'all': replace on all turns\n    \"\"\"", "\n", "new_sample", "=", "deepcopy", "(", "sample", ")", "\n", "for", "turn_index", ",", "turn", "in", "enumerate", "(", "sample", "[", "'log'", "]", ")", ":", "\n", "        ", "is_user", "=", "turn_index", "%", "2", "==", "0", "\n", "if", "is_user", "and", "mode", "not", "in", "(", "'usr'", ",", "'user'", ",", "'all'", ")", ":", "\n", "            ", "continue", "\n", "", "if", "not", "is_user", "and", "mode", "not", "in", "(", "'sys'", ",", "'system'", ",", "'all'", ")", ":", "\n", "            ", "continue", "\n", "", "new_turn", "=", "replace_slot_values_in_turn", "(", "turn", ",", "db_loader", ",", "p", "=", "p", ",", "inform_intents", "=", "inform_intents", ")", "\n", "new_sample", "[", "'log'", "]", "[", "turn_index", "]", "=", "new_turn", "\n", "", "return", "new_sample", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.__init__": [[43, 46], ["json.load", "open", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dataset", "=", "'multiwoz'", ",", "edit_frequency", "=", "0.3", ")", ":", "\n", "        ", "self", ".", "resources", "=", "json", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "current_path", ",", "'resources/resources_'", "+", "dataset", "+", "'.json'", ")", ",", "'r'", ")", ")", "\n", "self", ".", "edit_frequency", "=", "edit_frequency", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.protect_slots": [[48, 60], ["sentence.count", "len", "range", "sentence.find", "value.split"], "methods", ["None"], ["", "def", "protect_slots", "(", "self", ",", "word_list", ",", "spans", ",", "IP_tags", ")", ":", "\n", "        ", "sentence", "=", "' '", ".", "join", "(", "word_list", ")", "+", "' '", "\n", "for", "span", "in", "spans", ":", "\n", "            ", "value", "=", "span", "[", "2", "]", "\n", "start", "=", "sentence", ".", "count", "(", "' '", ",", "0", ",", "sentence", ".", "find", "(", "' '", "+", "value", "+", "' '", ")", ")", "\n", "lenth", "=", "len", "(", "value", ".", "split", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "start", "+", "1", ",", "start", "+", "lenth", ")", ":", "\n", "                ", "IP_tags", "[", "i", "]", "=", "0", "\n", "IP_tags", "[", "start", "]", "=", "1", "\n", "", "if", "IP_tags", "[", "start", "]", "==", "2", ":", "\n", "                ", "IP_tags", "[", "start", "]", "=", "1", "\n", "", "", "return", "IP_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_repairs": [[62, 89], ["len", "len", "Speech_Disfluency.random_01", "sentence.count", "fuzzywuzzy.fuzz.ratio", "sentence.find", "candidate.remove", "len", "Speech_Disfluency.random_pick_from_list", "Speech_Disfluency.random_pick_from_list", "Speech_Disfluency.random_pick_from_list"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_01", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_list", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_list", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_list"], ["", "def", "add_repairs", "(", "self", ",", "word_list", ",", "spans", ")", ":", "\n", "        ", "sentence", "=", "' '", "+", "' '", ".", "join", "(", "word_list", ")", "+", "' '", "\n", "if", "len", "(", "spans", ")", "==", "0", ":", "\n", "            ", "return", "word_list", "\n", "", "else", ":", "\n", "            ", "edit_possibility", "=", "self", ".", "edit_frequency", "/", "len", "(", "spans", ")", "\n", "", "for", "span", "in", "spans", ":", "\n", "            ", "if", "random_01", "(", "edit_possibility", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "value", "=", "span", "[", "2", "]", "\n", "start", "=", "sentence", ".", "count", "(", "' '", ",", "0", ",", "sentence", ".", "find", "(", "' '", "+", "value", "+", "' '", ")", ")", "-", "1", "\n", "\n", "max_ratio", ",", "max_entity", "=", "0", ",", "''", "\n", "for", "e", "in", "self", ".", "resources", "[", "\"knowledge_base\"", "]", "[", "\"entity\"", "]", ":", "\n", "                ", "ratio", "=", "fuzz", ".", "ratio", "(", "e", ",", "value", ")", "\n", "if", "ratio", ">", "max_ratio", ":", "\n", "                    ", "max_ratio", "=", "ratio", "\n", "max_entity", "=", "e", "\n", "", "", "if", "max_entity", "!=", "''", "and", "max_ratio", ">", "60", ":", "\n", "                ", "candidate", "=", "[", "]", "\n", "if", "max_entity", "in", "self", ".", "resources", "[", "\"knowledge_base\"", "]", "[", "\"entity\"", "]", ":", "\n", "                    ", "candidate", "=", "self", ".", "resources", "[", "\"knowledge_base\"", "]", "[", "\"category\"", "]", "[", "random_pick_from_list", "(", "self", ".", "resources", "[", "\"knowledge_base\"", "]", "[", "\"entity\"", "]", "[", "max_entity", "]", ")", "]", "[", "0", ":", "]", "\n", "", "if", "value", "in", "candidate", ":", "\n", "                    ", "candidate", ".", "remove", "(", "value", ")", "\n", "", "if", "len", "(", "candidate", ")", "!=", "0", ":", "\n", "                    ", "word_list", "[", "start", "]", "=", "random_pick_from_list", "(", "candidate", ")", "+", "' '", "+", "random_pick_from_list", "(", "self", ".", "resources", "[", "\"edit_terms\"", "]", ")", "+", "' '", "+", "word_list", "[", "start", "]", "\n", "", "", "", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_repeats": [[90, 95], ["range", "len", "Speech_Disfluency.random_pick_from_list"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_list"], ["", "def", "add_repeats", "(", "self", ",", "word_list", ",", "IP_tags", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "IP_tags", ")", ")", ":", "\n", "            ", "if", "IP_tags", "[", "i", "]", "==", "2", ":", "\n", "                ", "word_list", "[", "i", "]", "=", "word_list", "[", "i", "]", "+", "random_pick_from_list", "(", "[", "' '", ",", "' , '", "]", ")", "+", "word_list", "[", "i", "]", "\n", "", "", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_fillers": [[97, 102], ["range", "len", "Speech_Disfluency.random_pick_from_distribution"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_distribution"], ["", "def", "add_fillers", "(", "self", ",", "word_list", ",", "IP_tags", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "IP_tags", ")", ")", ":", "\n", "            ", "if", "IP_tags", "[", "i", "]", "==", "1", ":", "\n", "                ", "word_list", "[", "i", "]", "=", "random_pick_from_distribution", "(", "self", ".", "resources", "[", "\"filler_terms\"", "]", ")", "+", "' '", "+", "word_list", "[", "i", "]", "\n", "", "", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_restart": [[103, 106], ["Speech_Disfluency.random_pick_from_distribution"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_distribution"], ["", "def", "add_restart", "(", "self", ",", "word_list", ")", ":", "\n", "        ", "word_list", "[", "0", "]", "=", "random_pick_from_distribution", "(", "self", ".", "resources", "[", "\"restart_terms\"", "]", ")", "+", "' '", "+", "word_list", "[", "0", "]", "\n", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.find_spans": [[108, 120], ["range", "len", "sentence.count", "len", "sentence.find", "value.split", "sentence.split"], "methods", ["None"], ["", "def", "find_spans", "(", "self", ",", "disfluent_sentence", ",", "spans", ")", ":", "\n", "        ", "checked", "=", "1", "\n", "sentence", "=", "' '", "+", "disfluent_sentence", "+", "' '", "\n", "for", "i", "in", "range", "(", "len", "(", "spans", ")", ")", ":", "\n", "            ", "value", "=", "spans", "[", "i", "]", "[", "2", "]", "\n", "start", "=", "sentence", ".", "count", "(", "' '", ",", "0", ",", "sentence", ".", "find", "(", "' '", "+", "value", "+", "' '", ")", ")", "\n", "lenth", "=", "len", "(", "value", ".", "split", "(", ")", ")", "\n", "spans", "[", "i", "]", "[", "3", "]", "=", "start", "\n", "spans", "[", "i", "]", "[", "4", "]", "=", "start", "+", "lenth", "-", "1", "\n", "if", "' '", ".", "join", "(", "sentence", ".", "split", "(", ")", "[", "spans", "[", "i", "]", "[", "3", "]", ":", "spans", "[", "i", "]", "[", "4", "]", "+", "1", "]", ")", "!=", "spans", "[", "i", "]", "[", "2", "]", ":", "\n", "                ", "checked", "=", "0", "\n", "", "", "return", "spans", ",", "checked", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.aug": [[121, 132], ["Speech_Disfluency.preprocess", "LAUG.aug.Speech_Disfluency.inference.IP_model", "Speech_Disfluency.Speech_Disfluency.protect_slots", "Speech_Disfluency.Speech_Disfluency.add_repairs", "Speech_Disfluency.Speech_Disfluency.add_repeats", "Speech_Disfluency.Speech_Disfluency.add_fillers", "Speech_Disfluency.Speech_Disfluency.add_restart", "Speech_Disfluency.Speech_Disfluency.find_spans"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.preprocess", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.inference.IP_model", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.protect_slots", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_repairs", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_repeats", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_fillers", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.add_restart", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.Speech_Disfluency.find_spans"], ["", "def", "aug", "(", "self", ",", "sentence", ",", "spans", ")", ":", "\n", "        ", "word_list", "=", "preprocess", "(", "sentence", ")", "\n", "IP_tags", "=", "IP_model", "(", "word_list", ")", "\n", "IP_tags", "=", "self", ".", "protect_slots", "(", "word_list", ",", "spans", ",", "IP_tags", ")", "\n", "word_list", "=", "self", ".", "add_repairs", "(", "word_list", ",", "spans", ")", "\n", "word_list", "=", "self", ".", "add_repeats", "(", "word_list", ",", "IP_tags", ")", "\n", "word_list", "=", "self", ".", "add_fillers", "(", "word_list", ",", "IP_tags", ")", "\n", "word_list", "=", "self", ".", "add_restart", "(", "word_list", ")", "\n", "disfluent_sentence", "=", "' '", ".", "join", "(", "word_list", ")", "\n", "new_spans", ",", "checked", "=", "self", ".", "find_spans", "(", "disfluent_sentence", ",", "spans", ")", "\n", "return", "disfluent_sentence", ",", "new_spans", "\n", "# input sentence and span_info ; output the disfluent sentence and new_span_info", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_01": [[9, 15], ["random.random"], "function", ["None"], ["def", "random_01", "(", "possibility", ")", ":", "\n", "    ", "x", "=", "random", ".", "random", "(", ")", "\n", "if", "x", ">=", "possibility", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_list": [[16, 18], ["int", "len", "random.random"], "function", ["None"], ["", "", "def", "random_pick_from_list", "(", "random_list", ")", ":", "\n", "    ", "return", "random_list", "[", "int", "(", "len", "(", "random_list", ")", "*", "random", ".", "random", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.process_distribution_dict": [[19, 26], ["processed_distribution.append"], "function", ["None"], ["", "def", "process_distribution_dict", "(", "distribution_dict", ")", ":", "\n", "    ", "processed_distribution", "=", "[", "]", "\n", "sum", "=", "0", "\n", "for", "key", "in", "distribution_dict", ":", "\n", "        ", "sum", "+=", "distribution_dict", "[", "key", "]", "\n", "processed_distribution", ".", "append", "(", "(", "key", ",", "sum", ")", ")", "\n", "", "return", "processed_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.random_pick_from_distribution": [[27, 37], ["Speech_Disfluency.process_distribution_dict", "random.random"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.process_distribution_dict"], ["", "def", "random_pick_from_distribution", "(", "distribution_dict", ")", ":", "\n", "    ", "processed_distribution", "=", "process_distribution_dict", "(", "distribution_dict", ")", "\n", "x", "=", "random", ".", "random", "(", ")", "*", "processed_distribution", "[", "-", "1", "]", "[", "1", "]", "\n", "for", "item", "in", "processed_distribution", ":", "\n", "        ", "if", "x", ">", "item", "[", "1", "]", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "picked_item", "=", "item", "[", "0", "]", "\n", "break", "\n", "", "", "return", "picked_item", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.Speech_Disfluency.preprocess": [[38, 41], ["sentence.lower().strip().split", "sentence.lower().strip", "sentence.lower"], "function", ["None"], ["", "def", "preprocess", "(", "sentence", ")", ":", "\n", "    ", "word_list", "=", "sentence", ".", "lower", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "return", "word_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.__init__": [[37, 64], ["torch.Module.__init__", "len", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.Embedding.from_pretrained", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "LSTMCRF.BiLSTM_CRF.init_hidden", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "tag_to_ix", ",", "embedding_dim", ",", "hidden_dim", ",", "emb_weights", ")", ":", "\n", "        ", "super", "(", "BiLSTM_CRF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "tag_to_ix", "=", "tag_to_ix", "\n", "self", ".", "tagset_size", "=", "len", "(", "tag_to_ix", ")", "\n", "\n", "#self.word_embeds = nn.Embedding(vocab_size, embedding_dim)", "\n", "self", ".", "word_embeds", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "emb_weights", ")", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "embedding_dim", ",", "hidden_dim", "//", "2", ",", "\n", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "\n", "# Maps the output of the LSTM into tag space.", "\n", "self", ".", "hidden2tag", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "self", ".", "tagset_size", ")", "\n", "\n", "# Matrix of transition parameters.  Entry i,j is the score of", "\n", "# transitioning *to* i *from* j.", "\n", "self", ".", "transitions", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "randn", "(", "self", ".", "tagset_size", ",", "self", ".", "tagset_size", ")", ")", "\n", "\n", "# These two statements enforce the constraint that we never transfer", "\n", "# to the start tag and we never transfer from the stop tag", "\n", "self", ".", "transitions", ".", "data", "[", "tag_to_ix", "[", "START_TAG", "]", ",", ":", "]", "=", "-", "10000", "\n", "self", ".", "transitions", ".", "data", "[", ":", ",", "tag_to_ix", "[", "STOP_TAG", "]", "]", "=", "-", "10000", "\n", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.init_hidden": [[65, 68], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ")", ":", "\n", "        ", "return", "(", "torch", ".", "randn", "(", "2", ",", "1", ",", "self", ".", "hidden_dim", "//", "2", ")", ",", "\n", "torch", ".", "randn", "(", "2", ",", "1", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._forward_alg": [[69, 99], ["torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "LSTMCRF.log_sum_exp", "range", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "feat[].view().expand", "LSTMCRF.BiLSTM_CRF.transitions[].view", "alphas_t.append", "log_sum_exp().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "feat[].view", "LSTMCRF.log_sum_exp"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.log_sum_exp", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.log_sum_exp"], ["", "def", "_forward_alg", "(", "self", ",", "feats", ")", ":", "\n", "# Do the forward algorithm to compute the partition function", "\n", "        ", "init_alphas", "=", "torch", ".", "full", "(", "(", "1", ",", "self", ".", "tagset_size", ")", ",", "-", "10000.", ")", "\n", "# START_TAG has all of the score.", "\n", "init_alphas", "[", "0", "]", "[", "self", ".", "tag_to_ix", "[", "START_TAG", "]", "]", "=", "0.", "\n", "\n", "# Wrap in a variable so that we will get automatic backprop", "\n", "forward_var", "=", "init_alphas", "\n", "\n", "# Iterate through the sentence", "\n", "for", "feat", "in", "feats", ":", "\n", "            ", "alphas_t", "=", "[", "]", "# The forward tensors at this timestep", "\n", "for", "next_tag", "in", "range", "(", "self", ".", "tagset_size", ")", ":", "\n", "# broadcast the emission score: it is the same regardless of", "\n", "# the previous tag", "\n", "                ", "emit_score", "=", "feat", "[", "next_tag", "]", ".", "view", "(", "\n", "1", ",", "-", "1", ")", ".", "expand", "(", "1", ",", "self", ".", "tagset_size", ")", "\n", "# the ith entry of trans_score is the score of transitioning to", "\n", "# next_tag from i", "\n", "trans_score", "=", "self", ".", "transitions", "[", "next_tag", "]", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "# The ith entry of next_tag_var is the value for the", "\n", "# edge (i -> next_tag) before we do log-sum-exp", "\n", "next_tag_var", "=", "forward_var", "+", "trans_score", "+", "emit_score", "\n", "# The forward variable for this tag is log-sum-exp of all the", "\n", "# scores.", "\n", "alphas_t", ".", "append", "(", "log_sum_exp", "(", "next_tag_var", ")", ".", "view", "(", "1", ")", ")", "\n", "", "forward_var", "=", "torch", ".", "cat", "(", "alphas_t", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "", "terminal_var", "=", "forward_var", "+", "self", ".", "transitions", "[", "self", ".", "tag_to_ix", "[", "STOP_TAG", "]", "]", "\n", "alpha", "=", "log_sum_exp", "(", "terminal_var", ")", "\n", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._get_lstm_features": [[100, 107], ["LSTMCRF.BiLSTM_CRF.init_hidden", "LSTMCRF.BiLSTM_CRF.word_embeds().view", "LSTMCRF.BiLSTM_CRF.lstm", "lstm_out.view.view.view", "LSTMCRF.BiLSTM_CRF.hidden2tag", "len", "len", "LSTMCRF.BiLSTM_CRF.word_embeds"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.init_hidden"], ["", "def", "_get_lstm_features", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "embeds", "=", "self", ".", "word_embeds", "(", "sentence", ")", ".", "view", "(", "len", "(", "sentence", ")", ",", "1", ",", "-", "1", ")", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "lstm", "(", "embeds", ",", "self", ".", "hidden", ")", "\n", "lstm_out", "=", "lstm_out", ".", "view", "(", "len", "(", "sentence", ")", ",", "self", ".", "hidden_dim", ")", "\n", "lstm_feats", "=", "self", ".", "hidden2tag", "(", "lstm_out", ")", "\n", "return", "lstm_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._score_sentence": [[108, 117], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "_score_sentence", "(", "self", ",", "feats", ",", "tags", ")", ":", "\n", "# Gives the score of a provided tag sequence", "\n", "        ", "score", "=", "torch", ".", "zeros", "(", "1", ")", "\n", "tags", "=", "torch", ".", "cat", "(", "[", "torch", ".", "tensor", "(", "[", "self", ".", "tag_to_ix", "[", "START_TAG", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "tags", "]", ")", "\n", "for", "i", ",", "feat", "in", "enumerate", "(", "feats", ")", ":", "\n", "            ", "score", "=", "score", "+", "self", ".", "transitions", "[", "tags", "[", "i", "+", "1", "]", ",", "tags", "[", "i", "]", "]", "+", "feat", "[", "tags", "[", "i", "+", "1", "]", "]", "\n", "", "score", "=", "score", "+", "self", ".", "transitions", "[", "self", ".", "tag_to_ix", "[", "STOP_TAG", "]", ",", "tags", "[", "-", "1", "]", "]", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._viterbi_decode": [[118, 161], ["torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "LSTMCRF.argmax", "reversed", "best_path.pop", "best_path.reverse", "range", "backpointers.append", "best_path.append", "LSTMCRF.argmax", "bptrs_t.append", "viterbivars_t.append", "[].view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "_viterbi_decode", "(", "self", ",", "feats", ")", ":", "\n", "        ", "backpointers", "=", "[", "]", "\n", "\n", "# Initialize the viterbi variables in log space", "\n", "init_vvars", "=", "torch", ".", "full", "(", "(", "1", ",", "self", ".", "tagset_size", ")", ",", "-", "10000.", ")", "\n", "init_vvars", "[", "0", "]", "[", "self", ".", "tag_to_ix", "[", "START_TAG", "]", "]", "=", "0", "\n", "\n", "# forward_var at step i holds the viterbi variables for step i-1", "\n", "forward_var", "=", "init_vvars", "\n", "for", "feat", "in", "feats", ":", "\n", "            ", "bptrs_t", "=", "[", "]", "# holds the backpointers for this step", "\n", "viterbivars_t", "=", "[", "]", "# holds the viterbi variables for this step", "\n", "\n", "for", "next_tag", "in", "range", "(", "self", ".", "tagset_size", ")", ":", "\n", "# next_tag_var[i] holds the viterbi variable for tag i at the", "\n", "# previous step, plus the score of transitioning", "\n", "# from tag i to next_tag.", "\n", "# We don't include the emission scores here because the max", "\n", "# does not depend on them (we add them in below)", "\n", "                ", "next_tag_var", "=", "forward_var", "+", "self", ".", "transitions", "[", "next_tag", "]", "\n", "best_tag_id", "=", "argmax", "(", "next_tag_var", ")", "\n", "bptrs_t", ".", "append", "(", "best_tag_id", ")", "\n", "viterbivars_t", ".", "append", "(", "next_tag_var", "[", "0", "]", "[", "best_tag_id", "]", ".", "view", "(", "1", ")", ")", "\n", "# Now add in the emission scores, and assign forward_var to the set", "\n", "# of viterbi variables we just computed", "\n", "", "forward_var", "=", "(", "torch", ".", "cat", "(", "viterbivars_t", ")", "+", "feat", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "backpointers", ".", "append", "(", "bptrs_t", ")", "\n", "\n", "# Transition to STOP_TAG", "\n", "", "terminal_var", "=", "forward_var", "+", "self", ".", "transitions", "[", "self", ".", "tag_to_ix", "[", "STOP_TAG", "]", "]", "\n", "best_tag_id", "=", "argmax", "(", "terminal_var", ")", "\n", "path_score", "=", "terminal_var", "[", "0", "]", "[", "best_tag_id", "]", "\n", "\n", "# Follow the back pointers to decode the best path.", "\n", "best_path", "=", "[", "best_tag_id", "]", "\n", "for", "bptrs_t", "in", "reversed", "(", "backpointers", ")", ":", "\n", "            ", "best_tag_id", "=", "bptrs_t", "[", "best_tag_id", "]", "\n", "best_path", ".", "append", "(", "best_tag_id", ")", "\n", "# Pop off the start tag (we dont want to return that to the caller)", "\n", "", "start", "=", "best_path", ".", "pop", "(", ")", "\n", "assert", "start", "==", "self", ".", "tag_to_ix", "[", "START_TAG", "]", "# Sanity check", "\n", "best_path", ".", "reverse", "(", ")", "\n", "return", "path_score", ",", "best_path", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.neg_log_likelihood": [[162, 167], ["LSTMCRF.BiLSTM_CRF._get_lstm_features", "LSTMCRF.BiLSTM_CRF._forward_alg", "LSTMCRF.BiLSTM_CRF._score_sentence"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._get_lstm_features", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._forward_alg", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._score_sentence"], ["", "def", "neg_log_likelihood", "(", "self", ",", "sentence", ",", "tags", ")", ":", "\n", "        ", "feats", "=", "self", ".", "_get_lstm_features", "(", "sentence", ")", "\n", "forward_score", "=", "self", ".", "_forward_alg", "(", "feats", ")", "\n", "gold_score", "=", "self", ".", "_score_sentence", "(", "feats", ",", "tags", ")", "\n", "return", "forward_score", "-", "gold_score", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF.forward": [[168, 175], ["LSTMCRF.BiLSTM_CRF._get_lstm_features", "LSTMCRF.BiLSTM_CRF._viterbi_decode"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._get_lstm_features", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.BiLSTM_CRF._viterbi_decode"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "# dont confuse this with _forward_alg above.", "\n", "# Get the emission scores from the BiLSTM", "\n", "        ", "lstm_feats", "=", "self", ".", "_get_lstm_features", "(", "sentence", ")", "\n", "\n", "# Find the best path, given the features.", "\n", "score", ",", "tag_seq", "=", "self", ".", "_viterbi_decode", "(", "lstm_feats", ")", "\n", "return", "score", ",", "tag_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax": [[16, 20], ["torch.max", "torch.max", "torch.max", "torch.max", "idx.item"], "function", ["None"], ["def", "argmax", "(", "vec", ")", ":", "\n", "# return the argmax as a python int", "\n", "    ", "_", ",", "idx", "=", "torch", ".", "max", "(", "vec", ",", "1", ")", "\n", "return", "idx", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.log_sum_exp": [[25, 30], ["max_score.view().expand", "torch.log", "torch.log", "torch.log", "torch.log", "max_score.view", "vec.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "LSTMCRF.argmax", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "log_sum_exp", "(", "vec", ")", ":", "\n", "    ", "max_score", "=", "vec", "[", "0", ",", "argmax", "(", "vec", ")", "]", "\n", "max_score_broadcast", "=", "max_score", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand", "(", "1", ",", "vec", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "return", "max_score", "+", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "vec", "-", "max_score_broadcast", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.inference.prepare_sequence": [[12, 20], ["torch.tensor", "idxs.append", "idxs.append"], "function", ["None"], ["def", "prepare_sequence", "(", "seq", ",", "to_ix", ")", ":", "\n", "\t", "idxs", "=", "[", "]", "\n", "for", "w", "in", "seq", ":", "\n", "\t\t", "if", "w", "in", "to_ix", ":", "\n", "\t\t\t", "idxs", ".", "append", "(", "to_ix", "[", "w", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "idxs", ".", "append", "(", "0", ")", "\n", "", "", "return", "torch", ".", "tensor", "(", "idxs", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.inference.IP_model": [[48, 52], ["torch.no_grad", "inference.prepare_sequence", "model"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.train.prepare_sequence"], ["def", "IP_model", "(", "word_list", ")", ":", "\n", "\t", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "precheck_sent", "=", "prepare_sequence", "(", "word_list", ",", "word_to_ix", ")", "\n", "", "return", "model", "(", "precheck_sent", ")", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.train.prepare_sequence": [[13, 21], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "idxs.append", "idxs.append"], "function", ["None"], ["def", "prepare_sequence", "(", "seq", ",", "to_ix", ")", ":", "\n", "\t", "idxs", "=", "[", "]", "\n", "for", "w", "in", "seq", ":", "\n", "\t\t", "if", "w", "in", "to_ix", ":", "\n", "\t\t\t", "idxs", ".", "append", "(", "to_ix", "[", "w", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "idxs", ".", "append", "(", "0", ")", "\n", "", "", "return", "torch", ".", "tensor", "(", "idxs", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate.calculateF1": [[17, 37], ["x[].lower", "x[].lower"], "function", ["None"], ["def", "calculateF1", "(", "predict_golden", ")", ":", "\n", "    ", "TP", ",", "FP", ",", "FN", "=", "0", ",", "0", ",", "0", "\n", "for", "item", "in", "predict_golden", ":", "\n", "        ", "predicts", "=", "item", "[", "'predict'", "]", "\n", "predicts", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ".", "lower", "(", ")", "]", "for", "x", "in", "predicts", "]", "\n", "labels", "=", "item", "[", "'golden'", "]", "\n", "labels", "=", "[", "[", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", ",", "x", "[", "3", "]", ".", "lower", "(", ")", "]", "for", "x", "in", "labels", "]", "\n", "for", "ele", "in", "predicts", ":", "\n", "            ", "if", "ele", "in", "labels", ":", "\n", "                ", "TP", "+=", "1", "\n", "", "else", ":", "\n", "                ", "FP", "+=", "1", "\n", "", "", "for", "ele", "in", "labels", ":", "\n", "            ", "if", "ele", "not", "in", "predicts", ":", "\n", "                ", "FN", "+=", "1", "\n", "# print(TP, FP, FN)", "\n", "", "", "", "precision", "=", "1.0", "*", "TP", "/", "(", "TP", "+", "FP", ")", "\n", "recall", "=", "1.0", "*", "TP", "/", "(", "TP", "+", "FN", ")", "\n", "F1", "=", "2.0", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "if", "precision", "+", "recall", "else", "0.", "\n", "return", "precision", ",", "recall", ",", "F1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate_g.normalize": [[11, 23], ["str", "digit2word.items", "eval", "string.replace.replace"], "function", ["None"], ["def", "normalize", "(", "data", ")", ":", "\n", "    ", "string", "=", "str", "(", "data", ")", "\n", "\n", "digit2word", "=", "{", "\n", "'0'", ":", "'zero'", ",", "'1'", ":", "'one'", ",", "'2'", ":", "'two'", ",", "'3'", ":", "'three'", ",", "'4'", ":", "'four'", ",", "'5'", ":", "'five'", ",", "\n", "'6'", ":", "'six'", ",", "'7'", ":", "'seven'", ",", "'8'", ":", "'eight'", ",", "'9'", ":", "'nine'", ",", "'10'", ":", "'ten'", ",", "'11'", ":", "'eleven'", ",", "\n", "'12'", ":", "'twelve'", "\n", "}", "\n", "\n", "for", "key", ",", "value", "in", "digit2word", ".", "items", "(", ")", ":", "\n", "        ", "string", "=", "string", ".", "replace", "(", "' '", "+", "key", "+", "' '", ",", "' '", "+", "value", "+", "' '", ")", "\n", "", "return", "eval", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate_g.calculateF1gpt": [[25, 36], ["evaluate_g.normalize", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure.get_metric", "print", "LAUG.nlu.gpt.utils.seq2dict", "LAUG.nlu.gpt.utils.seq2dict", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure.", "item[].replace().lower", "item[].replace().split", "item[].replace", "item[].replace"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate_g.normalize", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict"], ["", "def", "calculateF1gpt", "(", "data", ")", ":", "\n", "    ", "data", "=", "normalize", "(", "data", ")", "\n", "dai_f1_metric", "=", "DialogActItemF1Measure", "(", ")", "\n", "\n", "for", "item", "in", "data", ":", "\n", "        ", "predict", "=", "seq2dict", "(", "item", "[", "1", "]", ".", "replace", "(", "'=?'", ",", "'= ?'", ")", ".", "lower", "(", ")", ")", "\n", "target", "=", "seq2dict", "(", "item", "[", "0", "]", ".", "replace", "(", "' \\''", ",", "''", ")", ".", "split", "(", "'&'", ")", "[", "1", "]", ")", "\n", "dai_f1_metric", "(", "[", "predict", "]", ",", "[", "target", "]", ")", "\n", "\n", "", "metric", "=", "dai_f1_metric", ".", "get_metric", "(", "True", ")", "\n", "print", "(", "metric", ")", "\n", "", "def", "calculateF1copy", "(", "data", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate_g.calculateF1copy": [[36, 47], ["evaluate_g.normalize", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure.get_metric", "print", "LAUG.nlu.gpt.utils.seq2dict", "LAUG.nlu.gpt.utils.seq2dict", "LAUG.nlu.milu.dai_f1_measure.DialogActItemF1Measure.", "item[].replace().lower", "item[].replace"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.evaluate_g.normalize", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict"], ["", "def", "calculateF1copy", "(", "data", ")", ":", "\n", "    ", "data", "=", "normalize", "(", "data", ")", "\n", "dai_f1_metric", "=", "DialogActItemF1Measure", "(", ")", "\n", "\n", "for", "item", "in", "data", ":", "\n", "        ", "predict", "=", "seq2dict", "(", "item", "[", "2", "]", ".", "replace", "(", "'i d'", ",", "'id'", ")", ".", "lower", "(", ")", ")", "\n", "target", "=", "seq2dict", "(", "item", "[", "1", "]", ")", "\n", "dai_f1_metric", "(", "[", "predict", "]", ",", "[", "target", "]", ")", "\n", "\n", "", "metric", "=", "dai_f1_metric", ".", "get_metric", "(", "True", ")", "\n", "print", "(", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlu.nlu.NLU.predict": [[8, 22], ["list"], "methods", ["None"], ["def", "predict", "(", "self", ",", "utterance", ",", "context", "=", "list", "(", ")", ")", ":", "\n", "        ", "\"\"\"Predict the dialog act of a natural language utterance.\n        \n        Args:\n            utterance (str):\n                A natural language utterance.\n            context (list of str):\n                Previous utterances.\n\n        Returns:\n            action (list of list):\n                The dialog act of utterance.\n        \"\"\"", "\n", "return", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet.__init__": [[83, 164], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.get_token_index", "copynet.CopyNet.vocab.add_token_to_namespace", "copynet.CopyNet.vocab.get_vocab_size", "copynet.CopyNet._encoder.get_output_dim", "allennlp.modules.token_embedders.Embedding", "torch.nn.modules.linear.Linear", "torch.nn.modules.rnn.LSTMCell", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "allennlp.nn.beam_search.BeamSearch", "initializer", "allennlp.training.metrics.BLEU"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "source_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "attention", ":", "Attention", ",", "\n", "beam_size", ":", "int", ",", "\n", "max_decoding_steps", ":", "int", ",", "\n", "target_embedding_dim", ":", "int", "=", "30", ",", "\n", "copy_token", ":", "str", "=", "\"@COPY@\"", ",", "\n", "source_namespace", ":", "str", "=", "\"source_tokens\"", ",", "\n", "target_namespace", ":", "str", "=", "\"target_tokens\"", ",", "\n", "tensor_based_metric", ":", "Metric", "=", "None", ",", "\n", "token_based_metric", ":", "Metric", "=", "None", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ")", "\n", "self", ".", "_source_namespace", "=", "source_namespace", "\n", "self", ".", "_target_namespace", "=", "target_namespace", "\n", "self", ".", "_src_start_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "START_SYMBOL", ",", "self", ".", "_source_namespace", ")", "\n", "self", ".", "_src_end_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "END_SYMBOL", ",", "self", ".", "_source_namespace", ")", "\n", "self", ".", "_start_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "START_SYMBOL", ",", "self", ".", "_target_namespace", ")", "\n", "self", ".", "_end_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "END_SYMBOL", ",", "self", ".", "_target_namespace", ")", "\n", "self", ".", "_oov_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "self", ".", "vocab", ".", "_oov_token", ",", "self", ".", "_target_namespace", ")", "\n", "self", ".", "_pad_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "vocab", ".", "_padding_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_copy_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "copy_token", ",", "self", ".", "_target_namespace", ")", "\n", "\n", "self", ".", "_tensor_based_metric", "=", "tensor_based_metric", "or", "BLEU", "(", "\n", "exclude_indices", "=", "{", "self", ".", "_pad_index", ",", "self", ".", "_end_index", ",", "self", ".", "_start_index", "}", "\n", ")", "\n", "self", ".", "_token_based_metric", "=", "token_based_metric", "\n", "\n", "self", ".", "_target_vocab_size", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "# Encoding modules.", "\n", "self", ".", "_source_embedder", "=", "source_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "# Decoder output dim needs to be the same as the encoder output dim since we initialize the", "\n", "# hidden state of the decoder with the final hidden state of the encoder.", "\n", "# We arbitrarily set the decoder's input dimension to be the same as the output dimension.", "\n", "self", ".", "encoder_output_dim", "=", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "decoder_output_dim", "=", "self", ".", "encoder_output_dim", "\n", "self", ".", "decoder_input_dim", "=", "self", ".", "decoder_output_dim", "\n", "\n", "# The decoder input will be a function of the embedding of the previous predicted token,", "\n", "# an attended encoder hidden state called the \"attentive read\", and another", "\n", "# weighted sum of the encoder hidden state called the \"selective read\".", "\n", "# While the weights for the attentive read are calculated by an `Attention` module,", "\n", "# the weights for the selective read are simply the predicted probabilities", "\n", "# corresponding to each token in the source sentence that matches the target", "\n", "# token from the previous timestep.", "\n", "self", ".", "_target_embedder", "=", "Embedding", "(", "\n", "num_embeddings", "=", "self", ".", "_target_vocab_size", ",", "embedding_dim", "=", "target_embedding_dim", "\n", ")", "\n", "self", ".", "_attention", "=", "attention", "\n", "self", ".", "_input_projection_layer", "=", "Linear", "(", "\n", "target_embedding_dim", "+", "self", ".", "encoder_output_dim", "*", "2", ",", "self", ".", "decoder_input_dim", "\n", ")", "\n", "\n", "# We then run the projected decoder input through an LSTM cell to produce", "\n", "# the next hidden state.", "\n", "self", ".", "_decoder_cell", "=", "LSTMCell", "(", "self", ".", "decoder_input_dim", ",", "self", ".", "decoder_output_dim", ")", "\n", "\n", "# We create a \"generation\" score for each token in the target vocab", "\n", "# with a linear projection of the decoder hidden state.", "\n", "self", ".", "_output_generation_layer", "=", "Linear", "(", "self", ".", "decoder_output_dim", ",", "self", ".", "_target_vocab_size", ")", "\n", "\n", "# We create a \"copying\" score for each source token by applying a non-linearity", "\n", "# (tanh) to a linear projection of the encoded hidden state for that token,", "\n", "# and then taking the dot product of the result with the decoder hidden state.", "\n", "self", ".", "_output_copying_layer", "=", "Linear", "(", "self", ".", "encoder_output_dim", ",", "self", ".", "decoder_output_dim", ")", "\n", "\n", "# At prediction time, we'll use a beam search to find the best target sequence.", "\n", "self", ".", "_beam_search", "=", "BeamSearch", "(", "\n", "self", ".", "_end_index", ",", "max_steps", "=", "max_decoding_steps", ",", "beam_size", "=", "beam_size", "\n", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet.forward": [[165, 241], ["copynet.CopyNet._encode", "copynet.CopyNet._init_decoder_state", "copynet.CopyNet._forward_loss", "copynet.CopyNet._init_decoder_state", "copynet.CopyNet._forward_beam_search", "copynet.CopyNet.update", "copynet.CopyNet._gather_extended_gold_tokens", "copynet.CopyNet._tensor_based_metric", "copynet.CopyNet._get_predicted_tokens", "copynet.CopyNet._token_based_metric"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._encode", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._init_decoder_state", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._forward_loss", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._init_decoder_state", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._forward_beam_search", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._gather_extended_gold_tokens", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_predicted_tokens"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "source_tokens", ":", "TextFieldTensors", ",", "\n", "source_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "source_to_target", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "target_tokens", ":", "TextFieldTensors", "=", "None", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Make foward pass with decoder logic for producing the entire target sequence.\n\n        # Parameters\n\n        source_tokens : `TextFieldTensors`, required\n            The output of `TextField.as_array()` applied on the source `TextField`. This will be\n            passed through a `TextFieldEmbedder` and then through an encoder.\n        source_token_ids : `torch.Tensor`, required\n            Tensor containing IDs that indicate which source tokens match each other.\n            Has shape: `(batch_size, trimmed_source_length)`.\n        source_to_target : `torch.Tensor`, required\n            Tensor containing vocab index of each source token with respect to the\n            target vocab namespace. Shape: `(batch_size, trimmed_source_length)`.\n        metadata : `List[Dict[str, Any]]`, required\n            Metadata field that contains the original source tokens with key 'source_tokens'\n            and any other meta fields. When 'target_tokens' is also passed, the metadata\n            should also contain the original target tokens with key 'target_tokens'.\n        target_tokens : `TextFieldTensors`, optional (default = None)\n            Output of `Textfield.as_array()` applied on target `TextField`. We assume that the\n            target tokens are also represented as a `TextField` which must contain a \"tokens\"\n            key that uses single ids.\n        target_token_ids : `torch.Tensor`, optional (default = None)\n            A tensor of shape `(batch_size, target_sequence_length)` which indicates which\n            tokens in the target sequence match tokens in the source sequence.\n\n        # Returns\n\n        Dict[str, torch.Tensor]\n        \"\"\"", "\n", "state", "=", "self", ".", "_encode", "(", "source_tokens", ")", "\n", "state", "[", "\"source_token_ids\"", "]", "=", "source_token_ids", "\n", "state", "[", "\"source_to_target\"", "]", "=", "source_to_target", "\n", "\n", "if", "target_tokens", ":", "\n", "            ", "state", "=", "self", ".", "_init_decoder_state", "(", "state", ")", "\n", "output_dict", "=", "self", ".", "_forward_loss", "(", "target_tokens", ",", "target_token_ids", ",", "state", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "output_dict", "[", "\"metadata\"", "]", "=", "metadata", "\n", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "state", "=", "self", ".", "_init_decoder_state", "(", "state", ")", "\n", "predictions", "=", "self", ".", "_forward_beam_search", "(", "state", ")", "\n", "output_dict", ".", "update", "(", "predictions", ")", "\n", "if", "target_tokens", ":", "\n", "                ", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "# shape: (batch_size, beam_size, max_sequence_length)", "\n", "                    ", "top_k_predictions", "=", "output_dict", "[", "\"predictions\"", "]", "\n", "# shape: (batch_size, max_predicted_sequence_length)", "\n", "best_predictions", "=", "top_k_predictions", "[", ":", ",", "0", ",", ":", "]", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "gold_tokens", "=", "self", ".", "_gather_extended_gold_tokens", "(", "\n", "target_tokens", "[", "\"tokens\"", "]", ",", "source_token_ids", ",", "target_token_ids", "\n", ")", "\n", "self", ".", "_tensor_based_metric", "(", "best_predictions", ",", "gold_tokens", ")", "# type: ignore", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                    ", "predicted_tokens", "=", "self", ".", "_get_predicted_tokens", "(", "\n", "output_dict", "[", "\"predictions\"", "]", ",", "metadata", ",", "n_best", "=", "1", "\n", ")", "\n", "self", ".", "_token_based_metric", "(", "# type: ignore", "\n", "predicted_tokens", ",", "[", "x", "[", "\"target_tokens\"", "]", "for", "x", "in", "metadata", "]", "\n", ")", "\n", "\n", "", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._gather_extended_gold_tokens": [[242, 296], ["target_tokens.size", "source_token_ids.size", "source_token_ids.unsqueeze().expand", "target_token_ids.unsqueeze().expand", "matches.sum", "source_token_ids.unsqueeze", "target_token_ids.unsqueeze", "first_match.long", "matches.cumsum"], "methods", ["None"], ["", "def", "_gather_extended_gold_tokens", "(", "\n", "self", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "source_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        Modify the gold target tokens relative to the extended vocabulary.\n\n        For gold targets that are OOV but were copied from the source, the OOV index\n        will be changed to the index of the first occurence in the source sentence,\n        offset by the size of the target vocabulary.\n\n        # Parameters\n\n        target_tokens : `torch.Tensor`\n            Shape: `(batch_size, target_sequence_length)`.\n        source_token_ids : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`.\n        target_token_ids : `torch.Tensor`\n            Shape: `(batch_size, target_sequence_length)`.\n\n        # Returns\n\n        torch.Tensor\n            Modified `target_tokens` with OOV indices replaced by offset index\n            of first match in source sentence.\n        \"\"\"", "\n", "batch_size", ",", "target_sequence_length", "=", "target_tokens", ".", "size", "(", ")", "\n", "trimmed_source_length", "=", "source_token_ids", ".", "size", "(", "1", ")", "\n", "# Only change indices for tokens that were OOV in target vocab but copied from source.", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "oov", "=", "target_tokens", "==", "self", ".", "_oov_index", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "expanded_source_token_ids", "=", "source_token_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "target_sequence_length", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "expanded_target_token_ids", "=", "target_token_ids", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "target_sequence_length", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "matches", "=", "expanded_source_token_ids", "==", "expanded_target_token_ids", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "copied", "=", "matches", ".", "sum", "(", "-", "1", ")", ">", "0", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "mask", "=", "oov", "&", "copied", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "first_match", "=", "(", "(", "matches", ".", "cumsum", "(", "-", "1", ")", "==", "1", ")", "&", "matches", ")", ".", "to", "(", "torch", ".", "uint8", ")", ".", "argmax", "(", "-", "1", ")", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "new_target_tokens", "=", "(", "\n", "target_tokens", "*", "~", "mask", "+", "(", "first_match", ".", "long", "(", ")", "+", "self", ".", "_target_vocab_size", ")", "*", "mask", "\n", ")", "\n", "return", "new_target_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._init_decoder_state": [[297, 317], ["state[].size", "allennlp.nn.util.get_final_encoder_states", "state[].new_zeros", "copynet.CopyNet._encoder.is_bidirectional"], "methods", ["None"], ["", "def", "_init_decoder_state", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Initialize the encoded state to be passed to the first decoding time step.\n        \"\"\"", "\n", "batch_size", ",", "_", "=", "state", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "\n", "\n", "# Initialize the decoder hidden state with the final output of the encoder,", "\n", "# and the decoder context with zeros.", "\n", "# shape: (batch_size, encoder_output_dim)", "\n", "final_encoder_output", "=", "util", ".", "get_final_encoder_states", "(", "\n", "state", "[", "\"encoder_outputs\"", "]", ",", "state", "[", "\"source_mask\"", "]", ",", "self", ".", "_encoder", ".", "is_bidirectional", "(", ")", "\n", ")", "\n", "# shape: (batch_size, decoder_output_dim)", "\n", "state", "[", "\"decoder_hidden\"", "]", "=", "final_encoder_output", "\n", "# shape: (batch_size, decoder_output_dim)", "\n", "state", "[", "\"decoder_context\"", "]", "=", "state", "[", "\"encoder_outputs\"", "]", ".", "new_zeros", "(", "\n", "batch_size", ",", "self", ".", "decoder_output_dim", "\n", ")", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._encode": [[318, 329], ["copynet.CopyNet._source_embedder", "allennlp.nn.util.get_text_field_mask", "copynet.CopyNet._encoder"], "methods", ["None"], ["", "def", "_encode", "(", "self", ",", "source_tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Encode source input sentences.\n        \"\"\"", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_input_dim)", "\n", "embedded_input", "=", "self", ".", "_source_embedder", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "util", ".", "get_text_field_mask", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs", "=", "self", ".", "_encoder", "(", "embedded_input", ",", "source_mask", ")", "\n", "return", "{", "\"source_mask\"", ":", "source_mask", ",", "\"encoder_outputs\"", ":", "encoder_outputs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._decoder_step": [[330, 357], ["copynet.CopyNet._target_embedder", "copynet.CopyNet._attention", "allennlp.nn.util.weighted_sum", "allennlp.nn.util.weighted_sum", "torch.cat", "copynet.CopyNet._input_projection_layer", "copynet.CopyNet._decoder_cell"], "methods", ["None"], ["", "def", "_decoder_step", "(", "\n", "self", ",", "\n", "last_predictions", ":", "torch", ".", "Tensor", ",", "\n", "selective_weights", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# shape: (group_size, max_input_sequence_length, encoder_output_dim)", "\n", "        ", "encoder_outputs_mask", "=", "state", "[", "\"source_mask\"", "]", "\n", "# shape: (group_size, target_embedding_dim)", "\n", "embedded_input", "=", "self", ".", "_target_embedder", "(", "last_predictions", ")", "\n", "# shape: (group_size, max_input_sequence_length)", "\n", "attentive_weights", "=", "self", ".", "_attention", "(", "\n", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"encoder_outputs\"", "]", ",", "encoder_outputs_mask", "\n", ")", "\n", "# shape: (group_size, encoder_output_dim)", "\n", "attentive_read", "=", "util", ".", "weighted_sum", "(", "state", "[", "\"encoder_outputs\"", "]", ",", "attentive_weights", ")", "\n", "# shape: (group_size, encoder_output_dim)", "\n", "selective_read", "=", "util", ".", "weighted_sum", "(", "state", "[", "\"encoder_outputs\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", ",", "selective_weights", ")", "\n", "# shape: (group_size, target_embedding_dim + encoder_output_dim * 2)", "\n", "decoder_input", "=", "torch", ".", "cat", "(", "(", "embedded_input", ",", "attentive_read", ",", "selective_read", ")", ",", "-", "1", ")", "\n", "# shape: (group_size, decoder_input_dim)", "\n", "projected_decoder_input", "=", "self", ".", "_input_projection_layer", "(", "decoder_input", ")", "\n", "\n", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"decoder_context\"", "]", "=", "self", ".", "_decoder_cell", "(", "\n", "projected_decoder_input", ",", "(", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"decoder_context\"", "]", ")", "\n", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_generation_scores": [[358, 360], ["copynet.CopyNet._output_generation_layer"], "methods", ["None"], ["", "def", "_get_generation_scores", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "_output_generation_layer", "(", "state", "[", "\"decoder_hidden\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_copy_scores": [[361, 371], ["copynet.CopyNet._output_copying_layer", "torch.tanh", "torch.tanh.bmm().squeeze", "torch.tanh.bmm", "state[].unsqueeze"], "methods", ["None"], ["", "def", "_get_copy_scores", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "# shape: (batch_size, max_input_sequence_length - 2, encoder_output_dim)", "\n", "        ", "trimmed_encoder_outputs", "=", "state", "[", "\"encoder_outputs\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "# shape: (batch_size, max_input_sequence_length - 2, decoder_output_dim)", "\n", "copy_projection", "=", "self", ".", "_output_copying_layer", "(", "trimmed_encoder_outputs", ")", "\n", "# shape: (batch_size, max_input_sequence_length - 2, decoder_output_dim)", "\n", "copy_projection", "=", "torch", ".", "tanh", "(", "copy_projection", ")", "\n", "# shape: (batch_size, max_input_sequence_length - 2)", "\n", "copy_scores", "=", "copy_projection", ".", "bmm", "(", "state", "[", "\"decoder_hidden\"", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "copy_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_ll_contrib": [[372, 449], ["generation_scores.size", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_softmax", "torch.cat", "allennlp.nn.util.logsumexp", "allennlp.nn.util.masked_log_softmax.gather", "target_to_source.sum", "target_tokens.unsqueeze", "target_to_source.to"], "methods", ["None"], ["", "def", "_get_ll_contrib", "(", "\n", "self", ",", "\n", "generation_scores", ":", "torch", ".", "Tensor", ",", "\n", "generation_scores_mask", ":", "torch", ".", "BoolTensor", ",", "\n", "copy_scores", ":", "torch", ".", "Tensor", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "target_to_source", ":", "torch", ".", "Tensor", ",", "\n", "copy_mask", ":", "torch", ".", "BoolTensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get the log-likelihood contribution from a single timestep.\n\n        # Parameters\n\n        generation_scores : `torch.Tensor`\n            Shape: `(batch_size, target_vocab_size)`\n        generation_scores_mask : `torch.BoolTensor`\n            Shape: `(batch_size, target_vocab_size)`. This is just a tensor of 1's.\n        copy_scores : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`\n        target_tokens : `torch.Tensor`\n            Shape: `(batch_size,)`\n        target_to_source : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`\n        copy_mask : `torch.BoolTensor`\n            Shape: `(batch_size, trimmed_source_length)`\n\n        # Returns\n\n        Tuple[torch.Tensor, torch.Tensor]\n            Shape: `(batch_size,), (batch_size, max_input_sequence_length)`\n        \"\"\"", "\n", "_", ",", "target_size", "=", "generation_scores", ".", "size", "(", ")", "\n", "\n", "# The point of this mask is to just mask out all source token scores", "\n", "# that just represent padding. We apply the mask to the concatenation", "\n", "# of the generation scores and the copy scores to normalize the scores", "\n", "# correctly during the softmax.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "mask", "=", "torch", ".", "cat", "(", "(", "generation_scores_mask", ",", "copy_mask", ")", ",", "dim", "=", "-", "1", ")", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "all_scores", "=", "torch", ".", "cat", "(", "(", "generation_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "# Normalize generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "log_probs", "=", "util", ".", "masked_log_softmax", "(", "all_scores", ",", "mask", ")", "\n", "# Calculate the log probability (`copy_log_probs`) for each token in the source sentence", "\n", "# that matches the current target token. We use the sum of these copy probabilities", "\n", "# for matching tokens in the source sentence to get the total probability", "\n", "# for the target token. We also need to normalize the individual copy probabilities", "\n", "# to create `selective_weights`, which are used in the next timestep to create", "\n", "# a selective read state.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_log_probs", "=", "(", "\n", "log_probs", "[", ":", ",", "target_size", ":", "]", "\n", "+", "(", "\n", "target_to_source", ".", "to", "(", "log_probs", ".", "dtype", ")", "+", "1e-13", "\n", ")", ".", "log", "(", ")", "\n", ")", "\n", "# Since `log_probs[:, target_size]` gives us the raw copy log probabilities,", "\n", "# we use a non-log softmax to get the normalized non-log copy probabilities.", "\n", "selective_weights", "=", "util", ".", "masked_softmax", "(", "log_probs", "[", ":", ",", "target_size", ":", "]", ",", "target_to_source", ")", "# 1e-45 --> 1e-13", "\n", "# This mask ensures that item in the batch has a non-zero generation probabilities", "\n", "# for this timestep only when the gold target token is not OOV or there are no", "\n", "# matching tokens in the source sentence.", "\n", "# shape: (batch_size, 1)", "\n", "gen_mask", "=", "(", "target_tokens", "!=", "self", ".", "_oov_index", ")", "|", "(", "target_to_source", ".", "sum", "(", "-", "1", ")", "==", "0", ")", "\n", "log_gen_mask", "=", "(", "gen_mask", "+", "1e-13", ")", ".", "log", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Now we get the generation score for the gold target token.", "\n", "# shape: (batch_size, 1)", "\n", "generation_log_probs", "=", "log_probs", ".", "gather", "(", "1", ",", "target_tokens", ".", "unsqueeze", "(", "1", ")", ")", "+", "log_gen_mask", "\n", "# ... and add the copy score to get the step log likelihood.", "\n", "# shape: (batch_size, 1 + trimmed_source_length)", "\n", "combined_gen_and_copy", "=", "torch", ".", "cat", "(", "(", "generation_log_probs", ",", "copy_log_probs", ")", ",", "dim", "=", "-", "1", ")", "\n", "# shape: (batch_size,)", "\n", "step_log_likelihood", "=", "util", ".", "logsumexp", "(", "combined_gen_and_copy", ")", "\n", "\n", "return", "step_log_likelihood", ",", "selective_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._forward_loss": [[450, 547], ["target_tokens[].size", "source_mask.new_full", "state[].new_zeros", "state[].new_zeros", "state[].new_full", "range", "torch.cat", "allennlp.nn.util.get_text_field_mask", "copy_mask.size", "copy_mask.size", "copynet.CopyNet._decoder_step", "copynet.CopyNet._get_generation_scores", "copynet.CopyNet._get_copy_scores", "copynet.CopyNet._get_ll_contrib", "step_log_likelihoods.append", "step_log_likelihood.unsqueeze", "log_likelihood.sum", "target_token_ids[].unsqueeze", "state[].new_zeros.sum"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._decoder_step", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_generation_scores", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_copy_scores", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_ll_contrib"], ["", "def", "_forward_loss", "(", "\n", "self", ",", "\n", "target_tokens", ":", "TextFieldTensors", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Calculate the loss against gold targets.\n        \"\"\"", "\n", "batch_size", ",", "target_sequence_length", "=", "target_tokens", "[", "\"tokens\"", "]", ".", "size", "(", ")", "\n", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "state", "[", "\"source_mask\"", "]", "\n", "\n", "# The last input from the target is either padding or the end symbol.", "\n", "# Either way, we don't have to process it.", "\n", "num_decoding_steps", "=", "target_sequence_length", "-", "1", "\n", "# We use this to fill in the copy index when the previous input was copied.", "\n", "# shape: (batch_size,)", "\n", "copy_input_choices", "=", "source_mask", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_copy_index", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_mask", "=", "source_mask", "[", ":", ",", "1", ":", "-", "1", "]", ">", "0", "\n", "# We need to keep track of the probabilities assigned to tokens in the source", "\n", "# sentence that were copied during the previous timestep, since we use", "\n", "# those probabilities as weights when calculating the \"selective read\".", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "selective_weights", "=", "state", "[", "\"decoder_hidden\"", "]", ".", "new_zeros", "(", "copy_mask", ".", "size", "(", ")", ")", "\n", "\n", "# Indicates which tokens in the source sentence match the current target token.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "target_to_source", "=", "state", "[", "\"source_token_ids\"", "]", ".", "new_zeros", "(", "copy_mask", ".", "size", "(", ")", ")", "\n", "\n", "# This is just a tensor of ones which we use repeatedly in `self._get_ll_contrib`,", "\n", "# so we create it once here to avoid doing it over-and-over.", "\n", "generation_scores_mask", "=", "state", "[", "\"decoder_hidden\"", "]", ".", "new_full", "(", "\n", "(", "batch_size", ",", "self", ".", "_target_vocab_size", ")", ",", "fill_value", "=", "1.0", ",", "dtype", "=", "torch", ".", "bool", "\n", ")", "\n", "\n", "step_log_likelihoods", "=", "[", "]", "\n", "for", "timestep", "in", "range", "(", "num_decoding_steps", ")", ":", "\n", "# shape: (batch_size,)", "\n", "            ", "input_choices", "=", "target_tokens", "[", "\"tokens\"", "]", "[", ":", ",", "timestep", "]", "\n", "# If the previous target token was copied, we use the special copy token.", "\n", "# But the end target token will always be THE end token, so we know", "\n", "# it was not copied.", "\n", "if", "timestep", "<", "num_decoding_steps", "-", "1", ":", "\n", "# Get mask tensor indicating which instances were copied.", "\n", "# shape: (batch_size,)", "\n", "                ", "copied", "=", "(", "\n", "(", "input_choices", "==", "self", ".", "_oov_index", ")", "&", "(", "target_to_source", ".", "sum", "(", "-", "1", ")", ">", "0", ")", "\n", ")", ".", "long", "(", ")", "\n", "# shape: (batch_size,)", "\n", "input_choices", "=", "input_choices", "*", "(", "1", "-", "copied", ")", "+", "copy_input_choices", "*", "copied", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "target_to_source", "=", "state", "[", "\"source_token_ids\"", "]", "==", "target_token_ids", "[", "\n", ":", ",", "timestep", "+", "1", "\n", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Update the decoder state by taking a step through the RNN.", "\n", "", "state", "=", "self", ".", "_decoder_step", "(", "input_choices", ",", "selective_weights", ",", "state", ")", "\n", "# Get generation scores for each token in the target vocab.", "\n", "# shape: (batch_size, target_vocab_size)", "\n", "generation_scores", "=", "self", ".", "_get_generation_scores", "(", "state", ")", "\n", "# Get copy scores for each token in the source sentence, excluding the start", "\n", "# and end tokens.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_scores", "=", "self", ".", "_get_copy_scores", "(", "state", ")", "\n", "# shape: (batch_size,)", "\n", "step_target_tokens", "=", "target_tokens", "[", "\"tokens\"", "]", "[", ":", ",", "timestep", "+", "1", "]", "\n", "step_log_likelihood", ",", "selective_weights", "=", "self", ".", "_get_ll_contrib", "(", "\n", "generation_scores", ",", "\n", "generation_scores_mask", ",", "\n", "copy_scores", ",", "\n", "step_target_tokens", ",", "\n", "target_to_source", ",", "\n", "copy_mask", ",", "\n", ")", "\n", "step_log_likelihoods", ".", "append", "(", "step_log_likelihood", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# Gather step log-likelihoods.", "\n", "# shape: (batch_size, num_decoding_steps = target_sequence_length - 1)", "\n", "", "log_likelihoods", "=", "torch", ".", "cat", "(", "step_log_likelihoods", ",", "1", ")", "\n", "# Get target mask to exclude likelihood contributions from timesteps after", "\n", "# the END token.", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "target_mask", "=", "util", ".", "get_text_field_mask", "(", "target_tokens", ")", "\n", "# The first timestep is just the START token, which is not included in the likelihoods.", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "target_mask", "=", "target_mask", "[", ":", ",", "1", ":", "]", "\n", "# Sum of step log-likelihoods.", "\n", "# shape: (batch_size,)", "\n", "log_likelihood", "=", "(", "log_likelihoods", "*", "target_mask", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "# The loss is the negative log-likelihood, averaged over the batch.", "\n", "loss", "=", "-", "log_likelihood", ".", "sum", "(", ")", "/", "batch_size", "\n", "\n", "return", "{", "\"loss\"", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._forward_beam_search": [[548, 566], ["state[].size", "state[].new_full", "copynet.CopyNet._beam_search.search", "state[].new_zeros"], "methods", ["None"], ["", "def", "_forward_beam_search", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "batch_size", ",", "source_length", "=", "state", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "\n", "trimmed_source_length", "=", "source_length", "-", "2", "\n", "# Initialize the copy scores to zero.", "\n", "state", "[", "\"copy_log_probs\"", "]", "=", "(", "\n", "state", "[", "\"decoder_hidden\"", "]", ".", "new_zeros", "(", "(", "batch_size", ",", "trimmed_source_length", ")", ")", "\n", "+", "1e-13", "\n", ")", ".", "log", "(", ")", "\n", "# shape: (batch_size,)", "\n", "start_predictions", "=", "state", "[", "\"source_mask\"", "]", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_start_index", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "\n", "# shape (all_top_k_predictions): (batch_size, beam_size, num_decoding_steps)", "\n", "# shape (log_probabilities): (batch_size, beam_size)", "\n", "all_top_k_predictions", ",", "log_probabilities", "=", "self", ".", "_beam_search", ".", "search", "(", "\n", "start_predictions", ",", "state", ",", "self", ".", "take_search_step", "\n", ")", "\n", "return", "{", "\"predicted_log_probs\"", ":", "log_probabilities", ",", "\"predictions\"", ":", "all_top_k_predictions", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_input_and_selective_weights": [[567, 651], ["state[].size", "only_copied_mask.new_full", "last_predictions.unsqueeze().expand", "source_token_ids.gather", "allennlp.nn.util.masked_softmax", "adjusted_predictions.unsqueeze", "only_copied_mask.unsqueeze", "last_predictions.unsqueeze"], "methods", ["None"], ["", "def", "_get_input_and_selective_weights", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "LongTensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "LongTensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get input choices for the decoder and the selective copy weights.\n\n        The decoder input choices are simply the `last_predictions`, except for\n        target OOV predictions that were copied from source tokens, in which case\n        the prediction will be changed to the COPY symbol in the target namespace.\n\n        The selective weights are just the probabilities assigned to source\n        tokens that were copied, normalized to sum to 1. If no source tokens were copied,\n        there will be all zeros.\n\n        # Parameters\n\n        last_predictions : `torch.LongTensor`\n            Shape: `(group_size,)`\n        state : `Dict[str, torch.Tensor]`\n\n        # Returns\n\n        Tuple[torch.LongTensor, torch.Tensor]\n            `input_choices` (shape `(group_size,)`) and `selective_weights`\n            (shape `(group_size, trimmed_source_length)`).\n        \"\"\"", "\n", "group_size", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "\n", "# This is a mask indicating which last predictions were copied from the", "\n", "# the source AND not in the target vocabulary (OOV).", "\n", "# (group_size,)", "\n", "only_copied_mask", "=", "last_predictions", ">=", "self", ".", "_target_vocab_size", "\n", "\n", "# If the last prediction was in the target vocab or OOV but not copied,", "\n", "# we use that as input, otherwise we use the COPY token.", "\n", "# shape: (group_size,)", "\n", "copy_input_choices", "=", "only_copied_mask", ".", "new_full", "(", "\n", "(", "group_size", ",", ")", ",", "fill_value", "=", "self", ".", "_copy_index", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "\n", "input_choices", "=", "last_predictions", "*", "~", "only_copied_mask", "+", "copy_input_choices", "*", "only_copied_mask", "\n", "\n", "# In order to get the `selective_weights`, we need to find out which predictions", "\n", "# were copied or copied AND generated, which is the case when a prediction appears", "\n", "# in both the source sentence and the target vocab. But whenever a prediction", "\n", "# is in the target vocab (even if it also appeared in the source sentence),", "\n", "# its index will be the corresponding target vocab index, not its index in", "\n", "# the source sentence offset by the target vocab size. So we first", "\n", "# use `state[\"source_to_target\"]` to get an indicator of every source token", "\n", "# that matches the predicted target token.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "expanded_last_predictions", "=", "last_predictions", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "group_size", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_copied_and_generated", "=", "state", "[", "\"source_to_target\"", "]", "==", "expanded_last_predictions", "\n", "\n", "# In order to get indicators for copied source tokens that are OOV with respect", "\n", "# to the target vocab, we'll make use of `state[\"source_token_ids\"]`.", "\n", "# First we adjust predictions relative to the start of the source tokens.", "\n", "# This makes sense because predictions for copied tokens are given by the index of the copied", "\n", "# token in the source sentence, offset by the size of the target vocabulary.", "\n", "# shape: (group_size,)", "\n", "adjusted_predictions", "=", "last_predictions", "-", "self", ".", "_target_vocab_size", "\n", "# The adjusted indices for items that were not copied will be negative numbers,", "\n", "# and therefore invalid. So we zero them out.", "\n", "adjusted_predictions", "=", "adjusted_predictions", "*", "only_copied_mask", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_token_ids", "=", "state", "[", "\"source_token_ids\"", "]", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "adjusted_prediction_ids", "=", "source_token_ids", ".", "gather", "(", "-", "1", ",", "adjusted_predictions", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "# This mask will contain indicators for source tokens that were copied", "\n", "# during the last timestep.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_only_copied", "=", "source_token_ids", "==", "adjusted_prediction_ids", "\n", "# Since we zero'd-out indices for predictions that were not copied,", "\n", "# we need to zero out all entries of this mask corresponding to those predictions.", "\n", "source_only_copied", "=", "source_only_copied", "&", "only_copied_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "mask", "=", "source_only_copied", "|", "source_copied_and_generated", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "selective_weights", "=", "util", ".", "masked_softmax", "(", "state", "[", "\"copy_log_probs\"", "]", ",", "mask", ")", "\n", "\n", "return", "input_choices", ",", "selective_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._gather_final_log_probs": [[652, 764], ["state[].size", "range", "modified_log_probs_list.insert", "torch.cat", "copy_log_probs_to_add.unsqueeze.unsqueeze.unsqueeze", "generation_log_probs.scatter.scatter.gather", "allennlp.nn.util.logsumexp", "generation_log_probs.scatter.scatter.scatter", "modified_log_probs_list.append", "source_to_target_slice.unsqueeze", "torch.cat", "source_to_target_slice.unsqueeze", "allennlp.nn.util.logsumexp.unsqueeze", "torch.cat", "allennlp.nn.util.logsumexp", "left_over_copy_log_probs.unsqueeze", "source_token_ids[].unsqueeze", "source_token_ids[].unsqueeze", "source_previous_occurences.sum", "allennlp.nn.util.logsumexp.unsqueeze"], "methods", ["None"], ["", "def", "_gather_final_log_probs", "(", "\n", "self", ",", "\n", "generation_log_probs", ":", "torch", ".", "Tensor", ",", "\n", "copy_log_probs", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Combine copy probabilities with generation probabilities for matching tokens.\n\n        # Parameters\n\n        generation_log_probs : `torch.Tensor`\n            Shape: `(group_size, target_vocab_size)`\n        copy_log_probs : `torch.Tensor`\n            Shape: `(group_size, trimmed_source_length)`\n        state : `Dict[str, torch.Tensor]`\n\n        # Returns\n\n        torch.Tensor\n            Shape: `(group_size, target_vocab_size + trimmed_source_length)`.\n        \"\"\"", "\n", "_", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "source_token_ids", "=", "state", "[", "\"source_token_ids\"", "]", "\n", "\n", "# shape: [(batch_size, *)]", "\n", "modified_log_probs_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "trimmed_source_length", ")", ":", "\n", "# shape: (group_size,)", "\n", "            ", "copy_log_probs_slice", "=", "copy_log_probs", "[", ":", ",", "i", "]", "\n", "# `source_to_target` is a matrix of shape (group_size, trimmed_source_length)", "\n", "# where element (i, j) is the vocab index of the target token that matches the jth", "\n", "# source token in the ith group, if there is one, or the index of the OOV symbol otherwise.", "\n", "# We'll use this to add copy scores to corresponding generation scores.", "\n", "# shape: (group_size,)", "\n", "source_to_target_slice", "=", "state", "[", "\"source_to_target\"", "]", "[", ":", ",", "i", "]", "\n", "# The OOV index in the source_to_target_slice indicates that the source", "\n", "# token is not in the target vocab, so we don't want to add that copy score", "\n", "# to the OOV token.", "\n", "copy_log_probs_to_add_mask", "=", "source_to_target_slice", "!=", "self", ".", "_oov_index", "\n", "copy_log_probs_to_add", "=", "(", "\n", "copy_log_probs_slice", "\n", "+", "(", "\n", "copy_log_probs_to_add_mask", "\n", "+", "1e-13", "\n", ")", ".", "log", "(", ")", "\n", ")", "\n", "# shape: (batch_size, 1)", "\n", "copy_log_probs_to_add", "=", "copy_log_probs_to_add", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# shape: (batch_size, 1)", "\n", "selected_generation_log_probs", "=", "generation_log_probs", ".", "gather", "(", "\n", "1", ",", "source_to_target_slice", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "combined_scores", "=", "util", ".", "logsumexp", "(", "\n", "torch", ".", "cat", "(", "(", "selected_generation_log_probs", ",", "copy_log_probs_to_add", ")", ",", "dim", "=", "1", ")", "\n", ")", "\n", "generation_log_probs", "=", "generation_log_probs", ".", "scatter", "(", "\n", "-", "1", ",", "source_to_target_slice", ".", "unsqueeze", "(", "-", "1", ")", ",", "combined_scores", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "# We have to combine copy scores for duplicate source tokens so that", "\n", "# we can find the overall most likely source token. So, if this is the first", "\n", "# occurence of this particular source token, we add the log_probs from all other", "\n", "# occurences, otherwise we zero it out since it was already accounted for.", "\n", "if", "i", "<", "(", "trimmed_source_length", "-", "1", ")", ":", "\n", "# Sum copy scores from future occurences of source token.", "\n", "# shape: (group_size, trimmed_source_length - i)", "\n", "                ", "source_future_occurences", "=", "source_token_ids", "[", ":", ",", "(", "i", "+", "1", ")", ":", "]", "==", "source_token_ids", "[", "\n", ":", ",", "i", "\n", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# shape: (group_size, trimmed_source_length - i)", "\n", "future_copy_log_probs", "=", "(", "\n", "copy_log_probs", "[", ":", ",", "(", "i", "+", "1", ")", ":", "]", "\n", "+", "(", "\n", "source_future_occurences", "+", "1e-13", "\n", ")", ".", "log", "(", ")", "\n", ")", "\n", "# shape: (group_size, 1 + trimmed_source_length - i)", "\n", "combined", "=", "torch", ".", "cat", "(", "\n", "(", "copy_log_probs_slice", ".", "unsqueeze", "(", "-", "1", ")", ",", "future_copy_log_probs", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "# shape: (group_size,)", "\n", "copy_log_probs_slice", "=", "util", ".", "logsumexp", "(", "combined", ")", "\n", "", "if", "i", ">", "0", ":", "\n", "# Remove copy log_probs that we have already accounted for.", "\n", "# shape: (group_size, i)", "\n", "                ", "source_previous_occurences", "=", "source_token_ids", "[", ":", ",", "0", ":", "i", "]", "==", "source_token_ids", "[", "\n", ":", ",", "i", "\n", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# shape: (group_size,)", "\n", "duplicate_mask", "=", "source_previous_occurences", ".", "sum", "(", "dim", "=", "-", "1", ")", "==", "0", "\n", "copy_log_probs_slice", "=", "(", "\n", "copy_log_probs_slice", "\n", "+", "(", "duplicate_mask", "+", "1e-13", ")", ".", "log", "(", ")", "\n", ")", "\n", "\n", "# Finally, we zero-out copy scores that we added to the generation scores", "\n", "# above so that we don't double-count them.", "\n", "# shape: (group_size,)", "\n", "", "left_over_copy_log_probs", "=", "(", "\n", "copy_log_probs_slice", "\n", "+", "(", "\n", "~", "copy_log_probs_to_add_mask", "\n", "+", "1e-13", "\n", ")", ".", "log", "(", ")", "\n", ")", "\n", "modified_log_probs_list", ".", "append", "(", "left_over_copy_log_probs", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "", "modified_log_probs_list", ".", "insert", "(", "0", ",", "generation_log_probs", ")", "\n", "\n", "# shape: (group_size, target_vocab_size + trimmed_source_length)", "\n", "modified_log_probs", "=", "torch", ".", "cat", "(", "modified_log_probs_list", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "modified_log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet.take_search_step": [[765, 864], ["state[].size", "copynet.CopyNet._get_input_and_selective_weights", "copynet.CopyNet._decoder_step", "copynet.CopyNet._get_generation_scores", "copynet.CopyNet._get_copy_scores", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax.split", "copynet.CopyNet._gather_final_log_probs", "copynet.CopyNet.new_full", "copynet.CopyNet.size"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_input_and_selective_weights", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._decoder_step", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_generation_scores", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_copy_scores", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._gather_final_log_probs"], ["", "def", "take_search_step", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "Tensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Take step during beam search.\n\n        This function is what gets passed to the `BeamSearch.search` method. It takes\n        predictions from the last timestep and the current state and outputs\n        the log probabilities assigned to tokens for the next timestep, as well as the updated\n        state.\n\n        Since we are predicting tokens out of the extended vocab (target vocab + all unique\n        tokens from the source sentence), this is a little more complicated that just\n        making a forward pass through the model. The output log probs will have\n        shape `(group_size, target_vocab_size + trimmed_source_length)` so that each\n        token in the target vocab and source sentence are assigned a probability.\n\n        Note that copy scores are assigned to each source token based on their position, not unique value.\n        So if a token appears more than once in the source sentence, it will have more than one score.\n        Further, if a source token is also part of the target vocab, its final score\n        will be the sum of the generation and copy scores. Therefore, in order to\n        get the score for all tokens in the extended vocab at this step,\n        we have to combine copy scores for re-occuring source tokens and potentially\n        add them to the generation scores for the matching token in the target vocab, if\n        there is one.\n\n        So we can break down the final log probs output as the concatenation of two\n        matrices, A: `(group_size, target_vocab_size)`, and B: `(group_size, trimmed_source_length)`.\n        Matrix A contains the sum of the generation score and copy scores (possibly 0)\n        for each target token. Matrix B contains left-over copy scores for source tokens\n        that do NOT appear in the target vocab, with zeros everywhere else. But since\n        a source token may appear more than once in the source sentence, we also have to\n        sum the scores for each appearance of each unique source token. So matrix B\n        actually only has non-zero values at the first occurence of each source token\n        that is not in the target vocab.\n\n        # Parameters\n\n        last_predictions : `torch.Tensor`\n            Shape: `(group_size,)`\n\n        state : `Dict[str, torch.Tensor]`\n            Contains all state tensors necessary to produce generation and copy scores\n            for next step.\n\n        Notes\n        -----\n        `group_size` != `batch_size`. In fact, `group_size` = `batch_size * beam_size`.\n        \"\"\"", "\n", "_", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "\n", "# Get input to the decoder RNN and the selective weights. `input_choices`", "\n", "# is the result of replacing target OOV tokens in `last_predictions` with the", "\n", "# copy symbol. `selective_weights` consist of the normalized copy probabilities", "\n", "# assigned to the source tokens that were copied. If no tokens were copied,", "\n", "# there will be all zeros.", "\n", "# shape: (group_size,), (group_size, trimmed_source_length)", "\n", "input_choices", ",", "selective_weights", "=", "self", ".", "_get_input_and_selective_weights", "(", "\n", "last_predictions", ",", "state", "\n", ")", "\n", "# Update the decoder state by taking a step through the RNN.", "\n", "state", "=", "self", ".", "_decoder_step", "(", "input_choices", ",", "selective_weights", ",", "state", ")", "\n", "# Get the un-normalized generation scores for each token in the target vocab.", "\n", "# shape: (group_size, target_vocab_size)", "\n", "generation_scores", "=", "self", ".", "_get_generation_scores", "(", "state", ")", "\n", "# Get the un-normalized copy scores for each token in the source sentence,", "\n", "# excluding the start and end tokens.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "copy_scores", "=", "self", ".", "_get_copy_scores", "(", "state", ")", "\n", "# Concat un-normalized generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "all_scores", "=", "torch", ".", "cat", "(", "(", "generation_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "copy_mask", "=", "state", "[", "\"source_mask\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", ">", "0", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "mask", "=", "torch", ".", "cat", "(", "\n", "(", "\n", "generation_scores", ".", "new_full", "(", "generation_scores", ".", "size", "(", ")", ",", "True", ",", "dtype", "=", "torch", ".", "bool", ")", ",", "\n", "copy_mask", ",", "\n", ")", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "# Normalize generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "log_probs", "=", "util", ".", "masked_log_softmax", "(", "all_scores", ",", "mask", ")", "\n", "# shape: (group_size, target_vocab_size), (group_size, trimmed_source_length)", "\n", "generation_log_probs", ",", "copy_log_probs", "=", "log_probs", ".", "split", "(", "\n", "[", "self", ".", "_target_vocab_size", ",", "trimmed_source_length", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "# Update copy_probs needed for getting the `selective_weights` at the next timestep.", "\n", "state", "[", "\"copy_log_probs\"", "]", "=", "copy_log_probs", "\n", "# We now have normalized generation and copy scores, but to produce the final", "\n", "# score for each token in the extended vocab, we have to go through and add", "\n", "# the copy scores to the generation scores of matching target tokens, and sum", "\n", "# the copy scores of duplicate source tokens.", "\n", "# shape: (group_size, target_vocab_size + trimmed_source_length)", "\n", "final_log_probs", "=", "self", ".", "_gather_final_log_probs", "(", "generation_log_probs", ",", "copy_log_probs", ",", "state", ")", "\n", "\n", "return", "final_log_probs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_predicted_tokens": [[865, 900], ["zip", "isinstance", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "list", "batch_predicted_tokens.append", "predicted_tokens.append", "predicted_tokens.append", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "tokens.append", "copynet.CopyNet.vocab.get_token_from_index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach", "list.index"], "methods", ["None"], ["", "def", "_get_predicted_tokens", "(", "\n", "self", ",", "\n", "predicted_indices", ":", "Union", "[", "torch", ".", "Tensor", ",", "numpy", ".", "ndarray", "]", ",", "\n", "batch_metadata", ":", "List", "[", "Any", "]", ",", "\n", "n_best", ":", "int", "=", "None", ",", "\n", ")", "->", "List", "[", "Union", "[", "List", "[", "List", "[", "str", "]", "]", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert predicted indices into tokens.\n\n        If `n_best = 1`, the result type will be `List[List[str]]`. Otherwise the result\n        type will be `List[List[List[str]]]`.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "predicted_indices", ",", "numpy", ".", "ndarray", ")", ":", "\n", "            ", "predicted_indices", "=", "predicted_indices", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "predicted_tokens", ":", "List", "[", "Union", "[", "List", "[", "List", "[", "str", "]", "]", ",", "List", "[", "str", "]", "]", "]", "=", "[", "]", "\n", "for", "top_k_predictions", ",", "metadata", "in", "zip", "(", "predicted_indices", ",", "batch_metadata", ")", ":", "\n", "            ", "batch_predicted_tokens", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "for", "indices", "in", "top_k_predictions", "[", ":", "n_best", "]", ":", "\n", "                ", "tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "indices", "=", "list", "(", "indices", ")", "\n", "if", "self", ".", "_end_index", "in", "indices", ":", "\n", "                    ", "indices", "=", "indices", "[", ":", "indices", ".", "index", "(", "self", ".", "_end_index", ")", "]", "\n", "", "for", "index", "in", "indices", ":", "\n", "                    ", "if", "index", ">=", "self", ".", "_target_vocab_size", ":", "\n", "                        ", "adjusted_index", "=", "index", "-", "self", ".", "_target_vocab_size", "\n", "token", "=", "metadata", "[", "\"source_tokens\"", "]", "[", "adjusted_index", "]", "\n", "", "else", ":", "\n", "                        ", "token", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "index", ",", "self", ".", "_target_namespace", ")", "\n", "", "tokens", ".", "append", "(", "token", ")", "\n", "", "batch_predicted_tokens", ".", "append", "(", "tokens", ")", "\n", "", "if", "n_best", "==", "1", ":", "\n", "                ", "predicted_tokens", ".", "append", "(", "batch_predicted_tokens", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "predicted_tokens", ".", "append", "(", "batch_predicted_tokens", ")", "\n", "", "", "return", "predicted_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet.make_output_human_readable": [[901, 914], ["copynet.CopyNet._get_predicted_tokens"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet._get_predicted_tokens"], ["", "def", "make_output_human_readable", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Finalize predictions.\n\n        After a beam search, the predicted indices correspond to tokens in the target vocabulary\n        OR tokens in source sentence. Here we gather the actual tokens corresponding to\n        the indices.\n        \"\"\"", "\n", "predicted_tokens", "=", "self", ".", "_get_predicted_tokens", "(", "\n", "output_dict", "[", "\"predictions\"", "]", ",", "output_dict", "[", "\"metadata\"", "]", "\n", ")", "\n", "output_dict", "[", "\"predicted_tokens\"", "]", "=", "predicted_tokens", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.copynet.CopyNet.get_metrics": [[915, 926], ["all_metrics.update", "all_metrics.update", "copynet.CopyNet._tensor_based_metric.get_metric", "copynet.CopyNet._token_based_metric.get_metric"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "all_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "\n", "self", ".", "_tensor_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "# type: ignore", "\n", ")", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "self", ".", "_token_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", ")", "# type: ignore", "\n", "", "", "return", "all_metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.__init__": [[19, 25], ["allennlp.predictors.predictor.Predictor.__init__", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "warnings", ".", "warn", "(", "\n", "\"The 'copynet' predictor has been deprecated in favor of \"", "\n", "\"the 'seq2seq' predictor.\"", ",", "\n", "DeprecationWarning", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.predict": [[27, 29], ["predictor.CopyNetPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "source", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"source_string\"", ":", "source", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.load_line": [[30, 33], ["line.strip().split", "line.strip"], "methods", ["None"], ["", "def", "load_line", "(", "self", ",", "line", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "source", ",", "target", "=", "line", ".", "strip", "(", "\"\\n\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "return", "{", "\"source_string\"", ":", "source", ",", "\"target_string\"", ":", "target", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.dump_line": [[34, 36], ["json.dumps"], "methods", ["None"], ["", "def", "dump_line", "(", "self", ",", "outputs", ":", "JsonDict", ")", "->", "str", ":", "\n", "        ", "return", "json", ".", "dumps", "(", "outputs", "[", "'predictions'", "]", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.predict_json": [[37, 49], ["predictor.CopyNetPredictor._json_to_instance", "predictor.CopyNetPredictor.predict_instance", "enumerate", "predictor.CopyNetPredictor._model.vocab.get_token_from_index", "prediction.index"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor._json_to_instance"], ["", "def", "predict_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "instance", "=", "self", ".", "_json_to_instance", "(", "inputs", ")", "\n", "output_dict", "=", "self", ".", "predict_instance", "(", "instance", ")", "\n", "source_tokens", "=", "output_dict", "[", "'metadata'", "]", "[", "'source_tokens'", "]", "\n", "predictions", "=", "output_dict", "[", "'predictions'", "]", "\n", "for", "idx", ",", "prediction", "in", "enumerate", "(", "predictions", ")", ":", "\n", "            ", "if", "self", ".", "_model", ".", "_end_index", "in", "prediction", ":", "\n", "                ", "prediction", "=", "prediction", "[", ":", "prediction", ".", "index", "(", "self", ".", "_model", ".", "_end_index", ")", "]", "\n", "", "prediction", "=", "[", "self", ".", "_model", ".", "vocab", ".", "get_token_from_index", "(", "index", ",", "self", ".", "_model", ".", "_target_namespace", ")", "if", "index", "<", "self", ".", "_model", ".", "_target_vocab_size", "else", "source_tokens", "[", "\n", "index", "-", "self", ".", "_model", ".", "_target_vocab_size", "]", "for", "index", "in", "prediction", "]", "\n", "predictions", "[", "idx", "]", "=", "' '", ".", "join", "(", "prediction", ")", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor.predict_batch_json": [[50, 63], ["predictor.CopyNetPredictor._batch_json_to_instances", "predictor.CopyNetPredictor.predict_batch_instance", "enumerate", "predictor.CopyNetPredictor._model.vocab.get_token_from_index", "prediction.index"], "methods", ["None"], ["", "def", "predict_batch_json", "(", "self", ",", "inputs", ":", "List", "[", "JsonDict", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "instances", "=", "self", ".", "_batch_json_to_instances", "(", "inputs", ")", "\n", "output_dicts", "=", "self", ".", "predict_batch_instance", "(", "instances", ")", "\n", "for", "output_dict", "in", "output_dicts", ":", "\n", "            ", "source_tokens", "=", "output_dict", "[", "'metadata'", "]", "[", "'source_tokens'", "]", "\n", "predictions", "=", "output_dict", "[", "'predictions'", "]", "\n", "for", "idx", ",", "prediction", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "if", "self", ".", "_model", ".", "_end_index", "in", "prediction", ":", "\n", "                    ", "prediction", "=", "prediction", "[", ":", "prediction", ".", "index", "(", "self", ".", "_model", ".", "_end_index", ")", "]", "\n", "", "prediction", "=", "[", "self", ".", "_model", ".", "vocab", ".", "get_token_from_index", "(", "index", ",", "self", ".", "_model", ".", "_target_namespace", ")", "if", "index", "<", "self", ".", "_model", ".", "_target_vocab_size", "else", "source_tokens", "[", "\n", "index", "-", "self", ".", "_model", ".", "_target_vocab_size", "]", "for", "index", "in", "prediction", "]", "\n", "predictions", "[", "idx", "]", "=", "' '", ".", "join", "(", "prediction", ")", "\n", "", "", "return", "output_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.predictor.CopyNetPredictor._json_to_instance": [[64, 68], ["predictor.CopyNetPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "source", "=", "json_dict", "[", "\"source_string\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "source", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.utils.clear": [[7, 10], ["s.replace.replace"], "function", ["None"], ["for", "span", "in", "span_info", ":", "\n", "        ", "span_words", "=", "span", "[", "2", "]", ".", "split", "(", ")", "\n", "result", "=", "phrase_idx_utt", "(", "span_words", ",", "new_words", ")", "\n", "if", "result", "is", "not", "None", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.utils.seq2dict": [[11, 55], ["s.split", "d[].append", "print", "d[].append", "d[].append", "utils.clear", "utils.clear", "pair.append"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear"], ["            ", "max_start", ",", "max_end", "=", "result", "\n", "new_span_info", ".", "append", "(", "[", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", ")", "\n", "", "", "return", "new_span_info", "\n", "\n", "\n", "", "def", "span2tuple", "(", "span_info", ")", ":", "\n", "    ", "t", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "        ", "t", ".", "append", "(", "(", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", ",", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "0", "]", ",", "span", "[", "1", "]", ",", "span", "[", "2", "]", ")", ")", "\n", "", "return", "t", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader.__init__": [[108, 125], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.WordTokenizer", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "isinstance"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "target_namespace", ":", "str", ",", "\n", "source_tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "target_tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "source_token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_target_namespace", "=", "target_namespace", "\n", "self", ".", "_source_tokenizer", "=", "source_tokenizer", "or", "WordTokenizer", "(", ")", "\n", "self", ".", "_target_tokenizer", "=", "target_tokenizer", "or", "self", ".", "_source_tokenizer", "\n", "self", ".", "_source_token_indexers", "=", "source_token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "if", "\"tokens\"", "not", "in", "self", ".", "_source_token_indexers", "or", "not", "isinstance", "(", "self", ".", "_source_token_indexers", "[", "\"tokens\"", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"CopyNetDatasetReader expects 'source_token_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'tokens'.\"", ")", "\n", "", "self", ".", "_target_token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "{", "\n", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "self", ".", "_target_namespace", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader._read": [[127, 142], ["open", "logger.info", "enumerate", "allennlp.common.file_utils.cached_path", "line.strip.strip.strip", "line.strip.strip.split", "len", "RuntimeError", "reader.CopyNetDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "with", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "for", "line_num", ",", "line", "in", "enumerate", "(", "data_file", ")", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "line_parts", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "line_parts", ")", "!=", "2", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Invalid line format: %s (line number %d)\"", "%", "(", "line", ",", "line_num", "+", "1", ")", ")", "\n", "", "source_sequence", ",", "target_sequence", "=", "line_parts", "\n", "if", "not", "source_sequence", ":", "\n", "                    ", "continue", "\n", "", "yield", "self", ".", "text_to_instance", "(", "source_sequence", ",", "target_sequence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader._tokens_to_ids": [[143, 150], ["out.append", "ids.setdefault", "token.text.lower", "len"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "_tokens_to_ids", "(", "tokens", ":", "List", "[", "Token", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "ids", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "out", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "out", ".", "append", "(", "ids", ".", "setdefault", "(", "token", ".", "text", ".", "lower", "(", ")", ",", "len", "(", "ids", ")", ")", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader.text_to_instance": [[151, 203], ["reader.CopyNetDatasetReader._source_tokenizer.tokenize", "reader.CopyNetDatasetReader.insert", "reader.CopyNetDatasetReader.append", "allennlp.data.fields.TextField", "allennlp.data.fields.NamespaceSwappingField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "reader.CopyNetDatasetReader._target_tokenizer.tokenize", "reader.CopyNetDatasetReader.insert", "reader.CopyNetDatasetReader.append", "allennlp.data.fields.TextField", "reader.CopyNetDatasetReader._tokens_to_ids", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "reader.CopyNetDatasetReader._tokens_to_ids", "allennlp.data.fields.ArrayField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "numpy.array", "numpy.array", "numpy.array", "len", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader._tokens_to_ids", "home.repos.pwc.inspect_result.thu-coai_LAUG.copynet.reader.CopyNetDatasetReader._tokens_to_ids"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "source_string", ":", "str", ",", "target_string", ":", "str", "=", "None", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        Turn raw source string and target string into an ``Instance``.\n\n        Parameters\n        ----------\n        source_string : ``str``, required\n        target_string : ``str``, optional (default = None)\n\n        Returns\n        -------\n        Instance\n            See the above for a description of the fields that the instance will contain.\n        \"\"\"", "\n", "# pylint: disable=arguments-differ", "\n", "tokenized_source", "=", "self", ".", "_source_tokenizer", ".", "tokenize", "(", "source_string", ")", "\n", "tokenized_source", ".", "insert", "(", "0", ",", "Token", "(", "START_SYMBOL", ")", ")", "\n", "tokenized_source", ".", "append", "(", "Token", "(", "END_SYMBOL", ")", ")", "\n", "source_field", "=", "TextField", "(", "tokenized_source", ",", "self", ".", "_source_token_indexers", ")", "\n", "\n", "# For each token in the source sentence, we keep track of the matching token", "\n", "# in the target sentence (which will be the OOV symbol if there is no match).", "\n", "source_to_target_field", "=", "NamespaceSwappingField", "(", "tokenized_source", "[", "1", ":", "-", "1", "]", ",", "self", ".", "_target_namespace", ")", "\n", "\n", "meta_fields", "=", "{", "\"source_tokens\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokenized_source", "[", "1", ":", "-", "1", "]", "]", "}", "\n", "fields_dict", "=", "{", "\n", "\"source_tokens\"", ":", "source_field", ",", "\n", "\"source_to_target\"", ":", "source_to_target_field", ",", "\n", "}", "\n", "\n", "if", "target_string", "is", "not", "None", ":", "\n", "            ", "tokenized_target", "=", "self", ".", "_target_tokenizer", ".", "tokenize", "(", "target_string", ")", "\n", "tokenized_target", ".", "insert", "(", "0", ",", "Token", "(", "START_SYMBOL", ")", ")", "\n", "tokenized_target", ".", "append", "(", "Token", "(", "END_SYMBOL", ")", ")", "\n", "target_field", "=", "TextField", "(", "tokenized_target", ",", "self", ".", "_target_token_indexers", ")", "\n", "\n", "fields_dict", "[", "\"target_tokens\"", "]", "=", "target_field", "\n", "meta_fields", "[", "\"target_tokens\"", "]", "=", "[", "y", ".", "text", "for", "y", "in", "tokenized_target", "[", "1", ":", "-", "1", "]", "]", "\n", "source_and_target_token_ids", "=", "self", ".", "_tokens_to_ids", "(", "tokenized_source", "[", "1", ":", "-", "1", "]", "+", "\n", "tokenized_target", ")", "\n", "source_token_ids", "=", "source_and_target_token_ids", "[", ":", "len", "(", "tokenized_source", ")", "-", "2", "]", "\n", "fields_dict", "[", "\"source_token_ids\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "source_token_ids", ")", ")", "\n", "target_token_ids", "=", "source_and_target_token_ids", "[", "len", "(", "tokenized_source", ")", "-", "2", ":", "]", "\n", "fields_dict", "[", "\"target_token_ids\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "target_token_ids", ")", ")", "\n", "", "else", ":", "\n", "            ", "source_token_ids", "=", "self", ".", "_tokens_to_ids", "(", "tokenized_source", "[", "1", ":", "-", "1", "]", ")", "\n", "fields_dict", "[", "\"source_token_ids\"", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "source_token_ids", ")", ")", "\n", "\n", "", "fields_dict", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "meta_fields", ")", "\n", "\n", "return", "Instance", "(", "fields_dict", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.__init__": [[8, 83], ["torch.nn.Module.__init__", "print", "print", "transformers.BertModel.from_pretrained", "torch.nn.Dropout", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "torch.nn.CrossEntropyLoss", "range", "torch.LongTensor", "range", "torch.LongTensor", "torch.tensor", "torch.tensor", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "intent.split", "dataloader.tokenizer.convert_tokens_to_ids", "dataloader.tokenizer.convert_tokens_to_ids", "jointBERT.JointBERT.intent_prefix.append", "intent.split", "dataloader.tokenizer.convert_tokens_to_ids", "dataloader.tokenizer.convert_tokens_to_ids", "jointBERT.JointBERT.req_intent_prefix.append", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_config", ",", "device", ",", "slot_dim", ",", "intent_dim", ",", "req_dim", ",", "dataloader", ",", "intent_weight", "=", "None", ",", "req_weight", "=", "None", ")", ":", "\n", "        ", "super", "(", "JointBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "slot_num_labels", "=", "slot_dim", "\n", "self", ".", "intent_num_labels", "=", "intent_dim", "\n", "self", ".", "slot_intent_dim", "=", "dataloader", ".", "slot_intent_dim", "\n", "\n", "self", ".", "req_intent_dim", "=", "dataloader", ".", "req_dim", "\n", "self", ".", "req_num_labels", "=", "dataloader", ".", "req_slot_dim", "\n", "#self.req_num_labels = req_dim", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "intent_weight", "=", "intent_weight", "if", "intent_weight", "is", "not", "None", "else", "torch", ".", "tensor", "(", "[", "1.", "]", "*", "intent_dim", ")", "\n", "print", "(", "'JointBERT: self.intent_weight = '", ",", "self", ".", "intent_weight", ")", "\n", "self", ".", "req_weight", "=", "req_weight", "if", "req_weight", "is", "not", "None", "else", "torch", ".", "tensor", "(", "[", "1.", "]", "*", "self", ".", "req_num_labels", ")", "\n", "\n", "print", "(", "model_config", "[", "'pretrained_weights'", "]", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "model_config", "[", "'pretrained_weights'", "]", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "model_config", "[", "'dropout'", "]", ")", "\n", "self", ".", "context", "=", "model_config", "[", "'context'", "]", "\n", "self", ".", "finetune", "=", "model_config", "[", "'finetune'", "]", "\n", "self", ".", "context_grad", "=", "model_config", "[", "'context_grad'", "]", "\n", "self", ".", "hidden_units", "=", "model_config", "[", "'hidden_units'", "]", "\n", "if", "self", ".", "hidden_units", ">", "0", ":", "\n", "            ", "if", "self", ".", "context", ":", "\n", "                ", "self", ".", "intent_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "intent_num_labels", ")", "\n", "self", ".", "req_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "req_num_labels", ")", "\n", "self", ".", "slot_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "slot_num_labels", ")", "#linear input (N,*, input_size)  output(N, *,output_size)  only work for last layer", "\n", "self", ".", "intent_hidden", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "self", ".", "req_hidden", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "self", ".", "slot_hidden", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "intent_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "intent_num_labels", ")", "\n", "self", ".", "slot_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "slot_num_labels", ")", "\n", "self", ".", "req_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_units", ",", "self", ".", "req_num_labels", ")", "\n", "self", ".", "intent_hidden", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "self", ".", "req_hidden", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "self", ".", "slot_hidden", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "hidden_units", ")", "\n", "\n", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "intent_hidden", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "slot_hidden", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "req_hidden", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "context", ":", "\n", "                ", "self", ".", "intent_classifier", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "intent_num_labels", ")", "\n", "self", ".", "slot_classifier", "=", "nn", ".", "Linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "slot_num_labels", ")", "\n", "self", ".", "req_classifier", "=", "nn", ".", "linear", "(", "2", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "req_num_labels", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "intent_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "intent_num_labels", ")", "\n", "self", ".", "req_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "req_num_labels", ")", "\n", "self", ".", "slot_classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "self", ".", "slot_num_labels", ")", "\n", "", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "intent_classifier", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "slot_classifier", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "req_classifier", ".", "weight", ")", "\n", "\n", "self", ".", "intent_loss_fct", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "self", ".", "intent_weight", ")", "\n", "self", ".", "req_loss_fct", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "self", ".", "req_weight", ")", "\n", "self", ".", "slot_loss_fct", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "self", ".", "intent_prefix", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "slot_intent_dim", ")", ":", "\n", "            ", "intent", "=", "dataloader", ".", "id2slotintent", "[", "i", "]", "\n", "domain", ",", "intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "dataloader", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "domain", ")", "\n", "intent_id", "=", "dataloader", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "intent", ")", "\n", "self", ".", "intent_prefix", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "", "self", ".", "intent_prefix", "=", "torch", ".", "LongTensor", "(", "self", ".", "intent_prefix", ")", "\n", "\n", "self", ".", "req_intent_prefix", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "dataloader", ".", "req_dim", ")", ":", "\n", "            ", "intent", "=", "dataloader", ".", "id2req", "[", "i", "]", "\n", "domain", ",", "intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "dataloader", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "domain", ")", "\n", "intent_id", "=", "dataloader", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "intent", ")", "\n", "self", ".", "req_intent_prefix", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "", "self", ".", "req_intent_prefix", "=", "torch", ".", "LongTensor", "(", "self", ".", "req_intent_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.slot_forward": [[84, 134], ["jointBERT.JointBERT.intent_prefix.repeat().to", "torch.zeros().to", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "jointBERT.JointBERT.dropout", "jointBERT.JointBERT.slot_classifier", "jointBERT.JointBERT.bert.eval", "jointBERT.JointBERT.bert", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat().view", "torch.cat", "torch.nn.functional.relu", "jointBERT.JointBERT.intent_prefix.repeat", "torch.zeros", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "repeat_word_seq_tensor[].unsqueeze", "torch.no_grad", "jointBERT.JointBERT.bert", "jointBERT.JointBERT.slot_hidden", "tag_mask_tensor.view", "torch.tensor().to", "torch.no_grad", "jointBERT.JointBERT.bert", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat", "jointBERT.JointBERT.dropout", "jointBERT.JointBERT.view", "tag_seq_tensor.view", "jointBERT.JointBERT.slot_loss_fct", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze", "jointBERT.JointBERT.bert", "torch.nn.functional.relu.size", "torch.tensor", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze"], "methods", ["None"], ["", "def", "slot_forward", "(", "self", ",", "intent_logits", ",", "word_seq_tensor", ",", "word_mask_tensor", ",", "tag_seq_tensor", "=", "None", ",", "tag_mask_tensor", "=", "None", ",", "context_seq_tensor", "=", "None", ",", "context_mask_tensor", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "intent_logits", ".", "shape", "[", "0", "]", "\n", "intent_prefix", "=", "self", ".", "intent_prefix", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "mask_prefix", "=", "torch", ".", "zeros", "(", "intent_prefix", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "slot_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "slot_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "slot_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "repeat_word_seq_tensor", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", ",", "intent_prefix", ",", "repeat_word_seq_tensor", "[", ":", ",", "1", ":", "]", ")", ",", "1", ")", "\n", "slot_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "if", "not", "self", ".", "finetune", ":", "\n", "            ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "slot_word_seq_tensor", ",", "\n", "attention_mask", "=", "slot_word_mask_tensor", ")", "\n", "", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "slot_word_seq_tensor", ",", "\n", "attention_mask", "=", "slot_word_mask_tensor", ")", "\n", "\n", "", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "if", "self", ".", "context", "and", "(", "context_seq_tensor", "is", "not", "None", ")", ":", "\n", "            ", "if", "not", "self", ".", "finetune", "or", "not", "self", ".", "context_grad", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "", "", "else", ":", "\n", "                ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "", "context_max_len", "=", "context_output", ".", "shape", "[", "-", "1", "]", "\n", "context_output", "=", "context_output", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "slot_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "context_max_len", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "\n", "[", "context_output", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "sequence_output", ".", "size", "(", "1", ")", ",", "1", ")", ",", "\n", "sequence_output", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "if", "self", ".", "hidden_units", ">", "0", ":", "\n", "            ", "sequence_output", "=", "nn", ".", "functional", ".", "relu", "(", "self", ".", "slot_hidden", "(", "self", ".", "dropout", "(", "sequence_output", ")", ")", ")", "\n", "\n", "", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "slot_logits", "=", "self", ".", "slot_classifier", "(", "sequence_output", ")", "\n", "output", "=", "(", "slot_logits", ",", ")", "\n", "\n", "if", "tag_seq_tensor", "is", "not", "None", ":", "\n", "            ", "active_tag_loss", "=", "tag_mask_tensor", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "if", "True", "in", "active_tag_loss", ":", "# deal with batch that totally masked", "\n", "                ", "active_tag_logits", "=", "slot_logits", ".", "view", "(", "-", "1", ",", "self", ".", "slot_num_labels", ")", "[", "active_tag_loss", "]", "\n", "active_tag_labels", "=", "tag_seq_tensor", ".", "view", "(", "-", "1", ")", "[", "active_tag_loss", "]", "\n", "slot_loss", "=", "self", ".", "slot_loss_fct", "(", "active_tag_logits", ",", "active_tag_labels", ")", "/", "self", ".", "slot_intent_dim", "\n", "", "else", ":", "\n", "                ", "slot_loss", "=", "torch", ".", "tensor", "(", "0.0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "output", "=", "output", "+", "(", "slot_loss", ",", ")", "\n", "\n", "", "return", "output", "# slot_logits, slot_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.req_forward": [[135, 190], ["jointBERT.JointBERT.req_intent_prefix.repeat().to", "torch.zeros().to", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "jointBERT.JointBERT.dropout", "jointBERT.JointBERT.req_classifier", "jointBERT.JointBERT.bert.eval", "jointBERT.JointBERT.bert", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat().view", "torch.cat", "torch.cat", "torch.nn.functional.relu", "jointBERT.JointBERT.req_intent_prefix.repeat", "torch.zeros", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "repeat_word_seq_tensor[].unsqueeze", "torch.no_grad", "jointBERT.JointBERT.bert", "jointBERT.JointBERT.req_hidden", "req_mask_tensor.view", "torch.tensor().to", "torch.no_grad", "jointBERT.JointBERT.bert", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze().repeat", "jointBERT.JointBERT.dropout", "jointBERT.JointBERT.view", "req_seq_tensor.view", "jointBERT.JointBERT.req_loss_fct", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze", "jointBERT.JointBERT.bert", "torch.cat.size", "torch.tensor", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze", "context_output.unsqueeze().repeat().view.unsqueeze().repeat().view.unsqueeze"], "methods", ["None"], ["", "def", "req_forward", "(", "self", ",", "intent_logits", ",", "word_seq_tensor", ",", "word_mask_tensor", ",", "req_seq_tensor", "=", "None", ",", "req_mask_tensor", "=", "None", ",", "context_seq_tensor", "=", "None", ",", "context_mask_tensor", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "intent_logits", ".", "shape", "[", "0", "]", "\n", "intent_prefix", "=", "self", ".", "req_intent_prefix", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "mask_prefix", "=", "torch", ".", "zeros", "(", "intent_prefix", ".", "shape", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "req_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "req_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "req_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "repeat_word_seq_tensor", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "1", ")", ",", "intent_prefix", ",", "repeat_word_seq_tensor", "[", ":", ",", "1", ":", "]", ")", ",", "1", ")", "\n", "req_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "\n", "if", "not", "self", ".", "finetune", ":", "\n", "            ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "req_word_seq_tensor", ",", "\n", "attention_mask", "=", "req_word_mask_tensor", ")", "\n", "", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "req_word_seq_tensor", ",", "\n", "attention_mask", "=", "req_word_mask_tensor", ")", "\n", "\n", "", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "if", "self", ".", "context", "and", "(", "context_seq_tensor", "is", "not", "None", ")", ":", "\n", "            ", "if", "not", "self", ".", "finetune", "or", "not", "self", ".", "context_grad", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "", "", "else", ":", "\n", "                ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "", "context_max_len", "=", "context_output", ".", "shape", "[", "-", "1", "]", "\n", "context_output", "=", "context_output", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "req_intent_dim", ",", "1", ")", ".", "view", "(", "-", "1", ",", "context_max_len", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "\n", "[", "context_output", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "sequence_output", ".", "size", "(", "1", ")", ",", "1", ")", ",", "\n", "sequence_output", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "pooled_output", "=", "torch", ".", "cat", "(", "[", "context_output", ",", "pooled_output", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "if", "self", ".", "hidden_units", ">", "0", ":", "\n", "            ", "pooled_output", "=", "nn", ".", "functional", ".", "relu", "(", "self", ".", "req_hidden", "(", "self", ".", "dropout", "(", "pooled_output", ")", ")", ")", "\n", "\n", "", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "req_logits", "=", "self", ".", "req_classifier", "(", "pooled_output", ")", "\n", "output", "=", "(", "req_logits", ",", ")", "\n", "\n", "if", "req_seq_tensor", "is", "not", "None", ":", "\n", "            ", "active_req_loss", "=", "req_mask_tensor", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "if", "True", "in", "active_req_loss", ":", "# deal with batch that totally masked", "\n", "                ", "active_req_logits", "=", "req_logits", ".", "view", "(", "-", "1", ",", "self", ".", "req_num_labels", ")", "[", "active_req_loss", "]", "\n", "active_req_labels", "=", "req_seq_tensor", ".", "view", "(", "-", "1", ",", "self", ".", "req_num_labels", ")", "[", "active_req_loss", "]", "\n", "req_loss", "=", "self", ".", "req_loss_fct", "(", "active_req_logits", ",", "active_req_labels", ")", "/", "self", ".", "req_intent_dim", "\n", "", "else", ":", "\n", "                ", "req_loss", "=", "torch", ".", "tensor", "(", "0.0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "output", "=", "output", "+", "(", "req_loss", ",", ")", "\n", "\n", "", "return", "output", "# slot_logits, slot_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.forward": [[191, 250], ["jointBERT.JointBERT.dropout", "jointBERT.JointBERT.intent_classifier", "jointBERT.JointBERT.slot_forward", "jointBERT.JointBERT.req_forward", "jointBERT.JointBERT.bert.eval", "jointBERT.JointBERT.bert", "torch.cat", "torch.cat", "torch.nn.functional.relu", "jointBERT.JointBERT.intent_loss_fct", "torch.no_grad", "jointBERT.JointBERT.bert", "jointBERT.JointBERT.intent_hidden", "torch.no_grad", "jointBERT.JointBERT.bert", "context_output.unsqueeze().repeat", "jointBERT.JointBERT.dropout", "jointBERT.JointBERT.bert", "torch.cat.size", "context_output.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.slot_forward", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.jointBERT.JointBERT.req_forward"], ["", "def", "forward", "(", "self", ",", "word_seq_tensor", ",", "word_mask_tensor", ",", "tag_seq_tensor", "=", "None", ",", "tag_mask_tensor", "=", "None", ",", "\n", "intent_tensor", "=", "None", ",", "req_tensor", "=", "None", ",", "req_mask_tensor", "=", "None", ",", "context_seq_tensor", "=", "None", ",", "context_mask_tensor", "=", "None", ")", ":", "\n", "\n", "        ", "if", "not", "self", ".", "finetune", ":", "\n", "            ", "self", ".", "bert", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "word_seq_tensor", ",", "\n", "attention_mask", "=", "word_mask_tensor", ")", "\n", "", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "input_ids", "=", "word_seq_tensor", ",", "\n", "attention_mask", "=", "word_mask_tensor", ")", "\n", "\n", "", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "\n", "if", "self", ".", "context", "and", "(", "context_seq_tensor", "is", "not", "None", ")", ":", "\n", "            ", "if", "not", "self", ".", "finetune", "or", "not", "self", ".", "context_grad", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "", "", "else", ":", "\n", "                ", "context_output", "=", "self", ".", "bert", "(", "input_ids", "=", "context_seq_tensor", ",", "attention_mask", "=", "context_mask_tensor", ")", "[", "1", "]", "\n", "\n", "", "sequence_output", "=", "torch", ".", "cat", "(", "\n", "[", "context_output", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "sequence_output", ".", "size", "(", "1", ")", ",", "1", ")", ",", "\n", "sequence_output", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "pooled_output", "=", "torch", ".", "cat", "(", "[", "context_output", ",", "pooled_output", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "\n", "", "if", "self", ".", "hidden_units", ">", "0", ":", "\n", "            ", "intent_pooled_output", "=", "nn", ".", "functional", ".", "relu", "(", "self", ".", "intent_hidden", "(", "self", ".", "dropout", "(", "pooled_output", ")", ")", ")", "\n", "#req_pooled_output = nn.functional.relu(self.req_hidden(self.dropout(pooled_output)))", "\n", "\n", "", "outputs", "=", "(", ")", "\n", "\n", "intent_pooled_output", "=", "self", ".", "dropout", "(", "intent_pooled_output", ")", "\n", "intent_logits", "=", "self", ".", "intent_classifier", "(", "intent_pooled_output", ")", "\n", "\n", "slot_output", "=", "self", ".", "slot_forward", "(", "intent_logits", ",", "word_seq_tensor", ",", "word_mask_tensor", ",", "tag_seq_tensor", ",", "tag_mask_tensor", ",", "context_seq_tensor", ",", "context_mask_tensor", ")", "\n", "outputs", "=", "outputs", "+", "(", "slot_output", "[", "0", "]", ",", ")", "#slot_logits", "\n", "\n", "# req_pooled_output = self.dropout(req_pooled_output)", "\n", "# req_logits = self.req_classifier(req_pooled_output)", "\n", "req_output", "=", "self", ".", "req_forward", "(", "intent_logits", ",", "word_seq_tensor", ",", "word_mask_tensor", ",", "req_tensor", ",", "req_mask_tensor", ",", "context_seq_tensor", ",", "context_mask_tensor", ")", "\n", "outputs", "=", "outputs", "+", "(", "intent_logits", ",", ")", "+", "(", "req_output", "[", "0", "]", ",", ")", "# intent logits and req logits", "\n", "\n", "if", "tag_seq_tensor", "is", "not", "None", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "slot_output", "[", "1", "]", ",", ")", "# slot_loss", "\n", "\n", "", "if", "intent_tensor", "is", "not", "None", ":", "\n", "            ", "intent_loss", "=", "self", ".", "intent_loss_fct", "(", "intent_logits", ",", "intent_tensor", ")", "\n", "outputs", "=", "outputs", "+", "(", "intent_loss", ",", ")", "# intent_loss", "\n", "\n", "", "if", "req_tensor", "is", "not", "None", ":", "\n", "#req_loss = self.req_loss_fct(req_logits, req_tensor)", "\n", "            ", "outputs", "=", "outputs", "+", "(", "req_output", "[", "1", "]", ",", ")", "#(req_loss,)", "\n", "\n", "", "return", "outputs", "# slot_logits, intent_logits, req_logits, (slot_loss), (intent_loss),(req_loss)", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.train.set_seed": [[14, 18], ["random.seed", "numpy.random.seed", "torch.manual_seed"], "function", ["None"], ["\t", "idxs", "=", "[", "]", "\n", "for", "w", "in", "seq", ":", "\n", "\t\t", "if", "w", "in", "to_ix", ":", "\n", "\t\t\t", "idxs", ".", "append", "(", "to_ix", "[", "w", "]", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.test.set_seed": [[11, 15], ["random.seed", "numpy.random.seed", "torch.manual_seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.__init__": [[10, 44], ["len", "len", "len", "len", "len", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "transformers.BertTokenizer.from_pretrained", "len", "len", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "intent_vocab", ",", "tag_vocab", ",", "req_vocab", ",", "req_slot_vocab", ",", "slot_intent_vocab", ",", "pretrained_weights", ")", ":", "\n", "        ", "\"\"\"\n        :param intent_vocab: list of all intents\n        :param tag_vocab: list of all tags\n        :param req_vocab: list of all slots of intent request\n        :param pretrained_weights: which bert, e.g. 'bert-base-uncased'\n        \"\"\"", "\n", "self", ".", "intent_vocab", "=", "intent_vocab", "\n", "self", ".", "tag_vocab", "=", "tag_vocab", "\n", "self", ".", "req_vocab", "=", "req_vocab", "\n", "self", ".", "req_slot_vocab", "=", "req_slot_vocab", "\n", "self", ".", "slot_intent_vocab", "=", "slot_intent_vocab", "\n", "\n", "self", ".", "intent_dim", "=", "len", "(", "intent_vocab", ")", "\n", "self", ".", "slot_intent_dim", "=", "len", "(", "slot_intent_vocab", ")", "\n", "self", ".", "tag_dim", "=", "len", "(", "tag_vocab", ")", "\n", "self", ".", "req_dim", "=", "len", "(", "req_vocab", ")", "\n", "self", ".", "req_slot_dim", "=", "len", "(", "req_slot_vocab", ")", "\n", "\n", "self", ".", "id2intent", "=", "dict", "(", "[", "(", "i", ",", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "intent_vocab", ")", "]", ")", "\n", "self", ".", "intent2id", "=", "dict", "(", "[", "(", "x", ",", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "intent_vocab", ")", "]", ")", "\n", "self", ".", "id2slotintent", "=", "dict", "(", "[", "(", "i", ",", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "slot_intent_vocab", ")", "]", ")", "\n", "self", ".", "slotintent2id", "=", "dict", "(", "[", "(", "x", ",", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "slot_intent_vocab", ")", "]", ")", "\n", "self", ".", "id2req", "=", "dict", "(", "[", "(", "i", ",", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "req_vocab", ")", "]", ")", "\n", "self", ".", "req2id", "=", "dict", "(", "[", "(", "x", ",", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "req_vocab", ")", "]", ")", "\n", "self", ".", "id2reqslot", "=", "dict", "(", "[", "(", "i", ",", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "req_slot_vocab", ")", "]", ")", "\n", "self", ".", "reqslot2id", "=", "dict", "(", "[", "(", "x", ",", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "req_slot_vocab", ")", "]", ")", "\n", "self", ".", "id2tag", "=", "dict", "(", "[", "(", "i", ",", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "tag_vocab", ")", "]", ")", "\n", "self", ".", "tag2id", "=", "dict", "(", "[", "(", "x", ",", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "tag_vocab", ")", "]", ")", "\n", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "pretrained_weights", ")", "\n", "self", ".", "data", "=", "{", "}", "\n", "self", ".", "intent_weight", "=", "[", "1", "]", "*", "len", "(", "self", ".", "intent2id", ")", "\n", "self", ".", "req_weight", "=", "[", "1", "]", "*", "len", "(", "self", ".", "reqslot2id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.tag_transfer": [[45, 57], ["enumerate", "len", "range", "tag.split", "dataloader.Dataloader.slotintent2id.keys"], "methods", ["None"], ["", "def", "tag_transfer", "(", "self", ",", "tags", ")", ":", "\n", "        ", "result", "=", "[", "[", "'O'", "]", "*", "len", "(", "tags", ")", "for", "i", "in", "range", "(", "self", ".", "slot_intent_dim", ")", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "            ", "if", "tag", "!=", "'O'", ":", "\n", "                ", "intent", ",", "value", "=", "tag", ".", "split", "(", "'+'", ")", "\n", "prefix", "=", "intent", "[", "0", "]", "\n", "intent", "=", "intent", "[", "2", ":", "]", "\n", "if", "intent", "in", "self", ".", "slotintent2id", ".", "keys", "(", ")", ":", "\n", "                    ", "intent_id", "=", "self", ".", "slotintent2id", "[", "intent", "]", "\n", "result", "[", "intent_id", "]", "[", "i", "]", "=", "prefix", "+", "'-'", "+", "value", "\n", "\n", "", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.req_transfer": [[58, 66], ["req.split", "result.append"], "methods", ["None"], ["", "def", "req_transfer", "(", "self", ",", "reqs", ")", ":", "\n", "        ", "result", "=", "[", "]", "\n", "for", "req", "in", "reqs", ":", "\n", "            ", "temp", "=", "req", ".", "split", "(", "'-'", ")", "\n", "req_intent", "=", "'-'", ".", "join", "(", "temp", "[", ":", "-", "1", "]", ")", "\n", "req_slot", "=", "temp", "[", "-", "1", "]", "\n", "result", ".", "append", "(", "[", "req_intent", ",", "req_slot", "]", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.load_data": [[67, 148], ["print", "print", "print", "print", "max", "sen_len.append", "dataloader.Dataloader.tokenizer.encode", "max", "context_len.append", "d.append", "d.append", "d.append", "d.append", "d.append", "len", "print", "print", "dataloader.Dataloader.intent2id.items", "print", "torch.tensor", "dataloader.Dataloader.req2id.items", "torch.tensor", "sorted", "sorted", "len", "len", "len", "len", "dataloader.Dataloader.bert_tokenize", "dataloader.Dataloader.tag_transfer", "tag_id_seq.append", "dataloader.Dataloader.seq_intent2id", "max", "numpy.log10", "collections.Counter().items", "collections.Counter().items", "dataloader.Dataloader.seq_tag2id", "numpy.log10", "dataloader.Dataloader.req_transfer", "collections.Counter", "collections.Counter", "s.split"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.bert_tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.tag_transfer", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_intent2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_tag2id", "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.req_transfer"], ["", "def", "load_data", "(", "self", ",", "data", ",", "data_key", ",", "cut_sen_len", ",", "use_bert_tokenizer", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        sample representation: [list of words, list of tags, list of intents, original dialog act]\n        :param data_key: train/val/test\n        :param data:\n        :return:\n        \"\"\"", "\n", "self", ".", "data", "[", "data_key", "]", "=", "data", "\n", "max_sen_len", ",", "max_context_len", "=", "0", ",", "0", "\n", "sen_len", "=", "[", "]", "\n", "context_len", "=", "[", "]", "\n", "for", "d", "in", "self", ".", "data", "[", "data_key", "]", ":", "\n", "            ", "max_sen_len", "=", "max", "(", "max_sen_len", ",", "len", "(", "d", "[", "0", "]", ")", ")", "\n", "sen_len", ".", "append", "(", "len", "(", "d", "[", "0", "]", ")", ")", "\n", "# d = (tokens, tags, intents, da2triples(turn[\"dialog_act\"], context(list of str))", "\n", "if", "cut_sen_len", ">", "0", ":", "\n", "                ", "d", "[", "0", "]", "=", "d", "[", "0", "]", "[", ":", "cut_sen_len", "]", "#text", "\n", "d", "[", "1", "]", "=", "d", "[", "1", "]", "[", ":", "cut_sen_len", "]", "#bio tag ", "\n", "\n", "# d[2] intent", "\n", "# d[3] overall label(dialog act)", "\n", "d", "[", "4", "]", "=", "[", "' '", ".", "join", "(", "s", ".", "split", "(", ")", "[", ":", "cut_sen_len", "]", ")", "for", "s", "in", "d", "[", "4", "]", "]", "#d[4] context", "\n", "# d[5] req_slot", "\n", "\n", "", "d", "[", "4", "]", "=", "self", ".", "tokenizer", ".", "encode", "(", "'[CLS] '", "+", "' [SEP] '", ".", "join", "(", "d", "[", "4", "]", ")", ")", "\n", "max_context_len", "=", "max", "(", "max_context_len", ",", "len", "(", "d", "[", "4", "]", ")", ")", "\n", "context_len", ".", "append", "(", "len", "(", "d", "[", "4", "]", ")", ")", "\n", "\n", "if", "use_bert_tokenizer", ":", "\n", "                ", "word_seq", ",", "tag_seq", ",", "new2ori", "=", "self", ".", "bert_tokenize", "(", "d", "[", "0", "]", ",", "d", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "word_seq", "=", "d", "[", "0", "]", "\n", "tag_seq", "=", "self", ".", "tag_transfer", "(", "d", "[", "1", "]", ")", "\n", "new2ori", "=", "None", "\n", "\n", "", "d", ".", "append", "(", "[", "[", "self", ".", "req2id", "[", "req_intent", "]", ",", "self", ".", "reqslot2id", "[", "req_slot", "]", "]", "for", "req_intent", ",", "req_slot", "in", "self", ".", "req_transfer", "(", "d", "[", "5", "]", ")", "]", ")", "\n", "#d.append(self.seq_req2id(d[5])) # \u4fee\u6539req\u7684\u5904\u7406", "\n", "d", ".", "append", "(", "new2ori", ")", "\n", "d", ".", "append", "(", "word_seq", ")", "\n", "\n", "tag_id_seq", "=", "[", "]", "\n", "for", "i", "in", "tag_seq", ":", "\n", "                ", "tag_id_seq", ".", "append", "(", "self", ".", "seq_tag2id", "(", "i", ")", ")", "\n", "\n", "", "d", ".", "append", "(", "tag_id_seq", ")", "\n", "d", ".", "append", "(", "self", ".", "seq_intent2id", "(", "d", "[", "2", "]", ")", ")", "\n", "# ori d = (tokens, tags, intents, da2triples(turn[\"dialog_act\"]), context(token id), new2ori, new_word_seq, tag2id_seq, intent2id_seq)", "\n", "# d = (tokens, tags, intents, da2triples(turn[\"dialog_act\"]), context(token id), reqs, req2id_seq, new2ori, new_word_seq, tag2id_seq, intent2id_seq)", "\n", "\n", "if", "data_key", "==", "'train'", ":", "\n", "                ", "for", "intent_id", "in", "d", "[", "-", "1", "]", ":", "\n", "                    ", "self", ".", "intent_weight", "[", "intent_id", "]", "+=", "1", "\n", "", "for", "req_id", "in", "d", "[", "-", "5", "]", ":", "\n", "                    ", "self", ".", "req_weight", "[", "req_id", "[", "1", "]", "]", "+=", "1", "\n", "\n", "# keys = ['tokens', 'tags', 'intents', 'das', 'context', 'reqs', 'req2id_seq', 'new2ori', 'new_word_seq', 'tag2id_seq', 'intent2id_seq']", "\n", "# assert(len(d) == len(keys))", "\n", "# for key, value in zip(keys, d):", "\n", "#     print(f\"{key} : {value}\")", "\n", "\n", "# print('='*100)", "\n", "\n", "", "", "", "if", "data_key", "==", "'train'", ":", "\n", "            ", "train_size", "=", "len", "(", "self", ".", "data", "[", "'train'", "]", ")", "\n", "print", "(", "'intent weight = '", ",", "self", ".", "intent_weight", ")", "\n", "print", "(", "'train_size = '", ",", "train_size", ")", "\n", "for", "intent", ",", "intent_id", "in", "self", ".", "intent2id", ".", "items", "(", ")", ":", "\n", "                ", "neg_pos", "=", "(", "train_size", "-", "self", ".", "intent_weight", "[", "intent_id", "]", ")", "/", "self", ".", "intent_weight", "[", "intent_id", "]", "\n", "self", ".", "intent_weight", "[", "intent_id", "]", "=", "max", "(", "np", ".", "log10", "(", "neg_pos", ")", ",", "0.1", ")", "\n", "", "print", "(", "'intent weight = '", ",", "self", ".", "intent_weight", ")", "\n", "self", ".", "intent_weight", "=", "torch", ".", "tensor", "(", "self", ".", "intent_weight", ")", "\n", "\n", "for", "req", ",", "req_id", "in", "self", ".", "req2id", ".", "items", "(", ")", ":", "\n", "                ", "neg_pos", "=", "(", "train_size", "-", "self", ".", "req_weight", "[", "req_id", "]", ")", "/", "self", ".", "req_weight", "[", "req_id", "]", "\n", "self", ".", "req_weight", "[", "req_id", "]", "=", "np", ".", "log10", "(", "neg_pos", ")", "\n", "", "self", ".", "req_weight", "=", "torch", ".", "tensor", "(", "self", ".", "req_weight", ")", "\n", "\n", "", "print", "(", "'max sen bert len'", ",", "max_sen_len", ")", "\n", "print", "(", "sorted", "(", "Counter", "(", "sen_len", ")", ".", "items", "(", ")", ")", ")", "\n", "print", "(", "'max context bert len'", ",", "max_context_len", ")", "\n", "print", "(", "sorted", "(", "Counter", "(", "context_len", ")", ".", "items", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.bert_tokenize": [[149, 184], ["dataloader.Dataloader.tokenizer.basic_tokenizer.tokenize", "enumerate", "dataloader.Dataloader.tokenizer.wordpiece_tokenizer.tokenize", "range", "word_seq[].lower", "split_tokens.append", "range", "new_tag_seq.append", "all_tag[].append", "tag.split", "len", "dataloader.Dataloader.slotintent2id.keys"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "def", "bert_tokenize", "(", "self", ",", "word_seq", ",", "tag_seq", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "new_tag_seq", "=", "[", "]", "\n", "new2ori", "=", "{", "}", "\n", "basic_tokens", "=", "self", ".", "tokenizer", ".", "basic_tokenizer", ".", "tokenize", "(", "' '", ".", "join", "(", "word_seq", ")", ")", "\n", "accum", "=", "''", "\n", "i", ",", "j", "=", "0", ",", "0", "\n", "\n", "all_tag", "=", "[", "[", "]", "for", "i", "in", "range", "(", "self", ".", "slot_intent_dim", ")", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "basic_tokens", ")", ":", "\n", "            ", "if", "(", "accum", "+", "token", ")", ".", "lower", "(", ")", "==", "word_seq", "[", "j", "]", ".", "lower", "(", ")", ":", "\n", "                ", "accum", "=", "''", "\n", "", "else", ":", "\n", "                ", "accum", "+=", "token", "\n", "", "for", "sub_token", "in", "self", ".", "tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "basic_tokens", "[", "i", "]", ")", ":", "\n", "                ", "new2ori", "[", "len", "(", "new_tag_seq", ")", "]", "=", "j", "\n", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "tag", "=", "tag_seq", "[", "j", "]", "\n", "for", "k", "in", "range", "(", "self", ".", "slot_intent_dim", ")", ":", "\n", "                    ", "all_tag", "[", "k", "]", ".", "append", "(", "'O'", ")", "\n", "", "if", "tag", "!=", "'O'", ":", "\n", "                    ", "intent", ",", "value", "=", "tag", ".", "split", "(", "'+'", ")", "\n", "prefix", "=", "intent", "[", "0", "]", "\n", "intent", "=", "intent", "[", "2", ":", "]", "\n", "if", "intent", "in", "self", ".", "slotintent2id", ".", "keys", "(", ")", ":", "\n", "                        ", "intent_id", "=", "self", ".", "slotintent2id", "[", "intent", "]", "\n", "all_tag", "[", "intent_id", "]", "[", "-", "1", "]", "=", "prefix", "+", "'-'", "+", "value", "\n", "", "", "new_tag_seq", ".", "append", "(", "tag_seq", "[", "j", "]", ")", "\n", "\n", "", "if", "accum", "==", "''", ":", "\n", "                ", "j", "+=", "1", "\n", "\n", "", "", "return", "split_tokens", ",", "all_tag", ",", "new2ori", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_tag2id": [[185, 187], ["None"], "methods", ["None"], ["", "def", "seq_tag2id", "(", "self", ",", "tags", ")", ":", "\n", "        ", "return", "[", "self", ".", "tag2id", "[", "x", "]", "if", "x", "in", "self", ".", "tag2id", "else", "self", ".", "tag2id", "[", "'O'", "]", "for", "x", "in", "tags", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_id2tag": [[188, 190], ["None"], "methods", ["None"], ["", "def", "seq_id2tag", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "id2tag", "[", "x", "]", "for", "x", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_intent2id": [[191, 193], ["None"], "methods", ["None"], ["", "def", "seq_intent2id", "(", "self", ",", "intents", ")", ":", "\n", "        ", "return", "[", "self", ".", "intent2id", "[", "x", "]", "for", "x", "in", "intents", "if", "x", "in", "self", ".", "intent2id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_id2intent": [[194, 196], ["None"], "methods", ["None"], ["", "def", "seq_id2intent", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "id2intent", "[", "x", "]", "for", "x", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_slotintent2id": [[197, 199], ["None"], "methods", ["None"], ["", "def", "seq_slotintent2id", "(", "self", ",", "intents", ")", ":", "\n", "        ", "return", "[", "self", ".", "slotintent2id", "[", "x", "]", "for", "x", "in", "intents", "if", "x", "in", "self", ".", "slotintent2id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_id2slotintent": [[200, 202], ["None"], "methods", ["None"], ["", "def", "seq_id2slotintent", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "id2slotintent", "[", "x", "]", "for", "x", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_id2req": [[203, 205], ["None"], "methods", ["None"], ["", "def", "seq_id2req", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "id2req", "[", "x", "]", "for", "x", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.seq_req2id": [[206, 208], ["None"], "methods", ["None"], ["", "def", "seq_req2id", "(", "self", ",", "reqs", ")", ":", "\n", "        ", "return", "[", "self", ".", "req2id", "[", "x", "]", "for", "x", "in", "reqs", "if", "x", "in", "self", ".", "req2id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.pad_batch": [[209, 259], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "max", "torch.zeros", "torch.zeros", "range", "max", "dataloader.Dataloader.tokenizer.convert_tokens_to_ids", "len", "torch.LongTensor", "range", "torch.LongTensor", "len", "torch.LongTensor", "torch.LongTensor", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "torch.LongTensor", "len"], "methods", ["None"], ["", "def", "pad_batch", "(", "self", ",", "batch_data", ")", ":", "# TODO\uff1a add req tensor and req_mask tensor", "\n", "#print('enter pad batch')", "\n", "        ", "batch_size", "=", "len", "(", "batch_data", ")", "\n", "#print('batch_size = ',batch_size)", "\n", "max_seq_len", "=", "max", "(", "[", "len", "(", "x", "[", "-", "3", "]", ")", "for", "x", "in", "batch_data", "]", ")", "+", "2", "\n", "#print('max_seq_len = ',max_seq_len)", "\n", "word_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "max_seq_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "word_seq_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "max_seq_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "tag_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", "*", "self", ".", "slot_intent_dim", ",", "max_seq_len", "+", "2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "base_tag_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", "*", "self", ".", "slot_intent_dim", ",", "max_seq_len", "+", "2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "tag_seq_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", "*", "self", ".", "slot_intent_dim", ",", "max_seq_len", "+", "2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "intent_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "intent_dim", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "req_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", "*", "self", ".", "req_dim", ",", "self", ".", "req_slot_dim", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "req_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", "*", "self", ".", "req_dim", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "context_max_seq_len", "=", "max", "(", "[", "len", "(", "x", "[", "4", "]", ")", "for", "x", "in", "batch_data", "]", ")", "#max([len(x[-5]) for x in batch_data])", "\n", "context_mask_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "context_max_seq_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "context_seq_tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "context_max_seq_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "words", "=", "batch_data", "[", "i", "]", "[", "-", "3", "]", "\n", "tags", "=", "batch_data", "[", "i", "]", "[", "-", "2", "]", "\n", "intents", "=", "batch_data", "[", "i", "]", "[", "-", "1", "]", "\n", "reqs", "=", "batch_data", "[", "i", "]", "[", "-", "5", "]", "\n", "words", "=", "[", "'[CLS]'", "]", "+", "words", "+", "[", "'[SEP]'", "]", "\n", "indexed_tokens", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "words", ")", "\n", "sen_len", "=", "len", "(", "words", ")", "\n", "word_seq_tensor", "[", "i", ",", ":", "sen_len", "]", "=", "torch", ".", "LongTensor", "(", "[", "indexed_tokens", "]", ")", "\n", "\n", "flag", "=", "False", "\n", "for", "j", "in", "range", "(", "self", ".", "slot_intent_dim", ")", ":", "\n", "                ", "tag_seq_tensor", "[", "i", "*", "self", ".", "slot_intent_dim", "+", "j", ",", "3", ":", "sen_len", "+", "1", "]", "=", "torch", ".", "LongTensor", "(", "tags", "[", "j", "]", ")", "\n", "base_tag_mask_tensor", "[", "i", "*", "self", ".", "slot_intent_dim", "+", "j", ",", "3", ":", "sen_len", "+", "1", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", "*", "(", "sen_len", "-", "2", ")", ")", "\n", "if", "tags", "[", "j", "]", "!=", "[", "self", ".", "tag2id", "[", "'O'", "]", "]", "*", "len", "(", "tags", "[", "j", "]", ")", ":", "\n", "                    ", "tag_mask_tensor", "[", "i", "*", "self", ".", "slot_intent_dim", "+", "j", ",", "3", ":", "sen_len", "+", "1", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", "*", "(", "sen_len", "-", "2", ")", ")", "\n", "\n", "", "", "word_mask_tensor", "[", "i", ",", ":", "sen_len", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", "*", "sen_len", ")", "\n", "for", "j", "in", "intents", ":", "\n", "                ", "intent_tensor", "[", "i", ",", "j", "]", "=", "1.", "\n", "", "for", "req_intent_id", ",", "req_slot_id", "in", "reqs", ":", "\n", "                ", "req_tensor", "[", "i", "*", "self", ".", "req_dim", "+", "req_intent_id", ",", "req_slot_id", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", "\n", "req_mask_tensor", "[", "i", "*", "self", ".", "req_dim", "+", "req_intent_id", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", "\n", "\n", "", "context_len", "=", "len", "(", "batch_data", "[", "i", "]", "[", "4", "]", ")", "#len(batch_data[i][-5])", "\n", "context_seq_tensor", "[", "i", ",", ":", "context_len", "]", "=", "torch", ".", "LongTensor", "(", "[", "batch_data", "[", "i", "]", "[", "4", "]", "]", ")", "#[batch_data[i][-5]])", "\n", "context_mask_tensor", "[", "i", ",", ":", "context_len", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", "*", "context_len", ")", "\n", "\n", "", "return", "word_seq_tensor", ",", "tag_seq_tensor", ",", "intent_tensor", ",", "req_tensor", ",", "req_mask_tensor", ",", "word_mask_tensor", ",", "tag_mask_tensor", ",", "base_tag_mask_tensor", ",", "context_seq_tensor", ",", "context_mask_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.get_train_batch": [[260, 263], ["random.choices", "dataloader.Dataloader.pad_batch"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.pad_batch"], ["", "def", "get_train_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "batch_data", "=", "random", ".", "choices", "(", "self", ".", "data", "[", "'train'", "]", ",", "k", "=", "batch_size", ")", "\n", "return", "self", ".", "pad_batch", "(", "batch_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.yield_batches": [[264, 269], ["math.ceil", "range", "len", "dataloader.Dataloader.pad_batch", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.jointBERT_new.dataloader.Dataloader.pad_batch"], ["", "def", "yield_batches", "(", "self", ",", "batch_size", ",", "data_key", ")", ":", "\n", "        ", "batch_num", "=", "math", ".", "ceil", "(", "len", "(", "self", ".", "data", "[", "data_key", "]", ")", "/", "batch_size", ")", "\n", "for", "i", "in", "range", "(", "batch_num", ")", ":", "\n", "            ", "batch_data", "=", "self", ".", "data", "[", "data_key", "]", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", "\n", "yield", "self", ".", "pad_batch", "(", "batch_data", ")", ",", "batch_data", ",", "len", "(", "batch_data", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.decode.set_seed": [[10, 15], ["numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "seed", ",", "n_gpu", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.decode.top_k_top_p_filtering": [[17, 46], ["min", "float", "logits.size", "torch.sort", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "torch.softmax", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size x vocabulary size)\n            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "top_k", "=", "min", "(", "top_k", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "torch", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "dim", "=", "1", ",", "index", "=", "sorted_indices", ",", "src", "=", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.decode.sample_sequence": [[48, 91], ["torch.tensor", "context.unsqueeze().repeat.unsqueeze().repeat", "torch.no_grad", "range", "context.unsqueeze().repeat.unsqueeze", "model", "range", "decode.top_k_top_p_filtering", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.cat", "torch.tensor().view", "set", "torch.argmax().unsqueeze", "torch.multinomial", "generated[].tolist", "torch.softmax", "torch.zeros", "torch.full", "torch.tensor", "torch.argmax"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.top_k_top_p_filtering", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "sample_sequence", "(", "model", ",", "length", ",", "context", ",", "num_samples", "=", "1", ",", "temperature", "=", "1", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "repetition_penalty", "=", "1.0", ",", "\n", "is_xlnet", "=", "False", ",", "is_xlm_mlm", "=", "False", ",", "xlm_mask_token", "=", "None", ",", "xlm_lang", "=", "None", ",", "device", "=", "'cpu'", ")", ":", "\n", "    ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "context", "=", "context", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "num_samples", ",", "1", ")", "\n", "generated", "=", "context", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "length", ")", ":", "\n", "\n", "            ", "inputs", "=", "{", "'input_ids'", ":", "generated", "}", "\n", "if", "is_xlnet", ":", "\n", "# XLNet is a direct (predict same token, not next token) and bi-directional model by default", "\n", "# => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "zeros", "(", "(", "1", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "perm_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "perm_mask", "[", ":", ",", ":", ",", "-", "1", "]", "=", "1.0", "# Previous tokens don't see last token", "\n", "target_mapping", "=", "torch", ".", "zeros", "(", "(", "1", ",", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "target_mapping", "[", "0", ",", "0", ",", "-", "1", "]", "=", "1.0", "# predict last token", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", ",", "'perm_mask'", ":", "perm_mask", ",", "'target_mapping'", ":", "target_mapping", "}", "\n", "\n", "", "if", "is_xlm_mlm", "and", "xlm_mask_token", ":", "\n", "# XLM MLM models are direct models (predict same token, not next token)", "\n", "# => need one additional dummy token in the input (will be masked and guessed)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "full", "(", "(", "1", ",", "1", ")", ",", "xlm_mask_token", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", "}", "\n", "\n", "", "if", "xlm_lang", "is", "not", "None", ":", "\n", "                ", "inputs", "[", "\"langs\"", "]", "=", "torch", ".", "tensor", "(", "[", "xlm_lang", "]", "*", "inputs", "[", "\"input_ids\"", "]", ".", "shape", "[", "1", "]", ",", "device", "=", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "", "outputs", "=", "model", "(", "**", "inputs", ")", "# Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "/", "(", "temperature", "if", "temperature", ">", "0", "else", "1.", ")", "\n", "\n", "# repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "                ", "for", "_", "in", "set", "(", "generated", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                    ", "next_token_logits", "[", "i", ",", "_", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "filtered_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "if", "temperature", "==", "0", ":", "# greedy sampling:", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_token", "=", "torch", ".", "multinomial", "(", "torch", ".", "softmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", "\n", "", "generated", "=", "torch", ".", "cat", "(", "(", "generated", ",", "next_token", ")", ",", "dim", "=", "1", ")", "\n", "", "", "return", "generated", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextDataset.__init__": [[47, 82], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "logger.info", "open", "pickle.load", "open", "range", "open", "pickle.dump", "f.read", "tokenizer.convert_tokens_to_ids", "train.TextDataset.examples.append", "str", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "train.TextDataset.examples.append", "tokenizer.build_inputs_with_special_tokens", "tokenizer.tokenize", "len", "str", "line.strip"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "line_list", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "line_list", "[", "0", "]", "\n", "embed", "=", "line_list", "[", "1", ":", "]", "\n", "embed", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "float", "(", "num", ")", "for", "num", "in", "embed", "]", ")", ")", "\n", "word_to_ix", "[", "word", "]", "=", "i", "+", "1", "\n", "weights", ".", "append", "(", "embed", ")", "\n", "\n", "", "weights", "=", "torch", ".", "stack", "(", "weights", ",", "0", ")", ".", "float", "(", ")", "\n", "\n", "\n", "tag_to_ix", "=", "{", "\"O\"", ":", "0", ",", "\"F\"", ":", "1", ",", "\"R\"", ":", "2", ",", "START_TAG", ":", "3", ",", "STOP_TAG", ":", "4", "}", "\n", "\n", "model", "=", "BiLSTM_CRF", "(", "len", "(", "word_to_ix", ")", ",", "tag_to_ix", ",", "EMBEDDING_DIM", ",", "HIDDEN_DIM", ",", "weights", ")", "\n", "model", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ")", "\n", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "0", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n", "precheck_tags", "=", "torch", ".", "tensor", "(", "[", "tag_to_ix", "[", "t", "]", "for", "t", "in", "training_data", "[", "0", "]", "[", "1", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "\n", "", "ep", "=", "0", "\n", "for", "epoch", "in", "range", "(", "30", ")", ":", "\n", "\t", "n", ",", "losses", "=", "0", ",", "0.", "\n", "ep", "+=", "1", "\n", "for", "sentence", ",", "tags", "in", "progressbar", "(", "training_data", ")", ":", "\n", "\t\t", "model", ".", "zero_grad", "(", ")", "\n", "sentence_in", "=", "prepare_sequence", "(", "sentence", ",", "word_to_ix", ")", "\n", "targets", "=", "torch", ".", "tensor", "(", "[", "tag_to_ix", "[", "t", "]", "for", "t", "in", "tags", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "loss", "=", "model", ".", "neg_log_likelihood", "(", "sentence_in", ",", "targets", ")", "\n", "losses", "+=", "loss", "\n", "n", "+=", "1", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextDataset.__len__": [[83, 85], ["len"], "methods", ["None"], ["", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'model/LSTMCRF_'", "+", "str", "(", "ep", ")", "+", "'.bin'", ")", "\n", "print", "(", "'loss:'", "+", "str", "(", "losses", "/", "n", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextDataset.__getitem__": [[86, 88], ["torch.tensor"], "methods", ["None"], ["\t\t", "precheck_sent", "=", "prepare_sequence", "(", "\"okay , i like to do , weight training and cycling .\"", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "1", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextSeqDataset.__init__": [[90, 145], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "logger.info", "open", "pickle.load", "open", "open", "pickle.dump", "line.strip.strip.strip", "line.strip.strip.lower", "code_str.strip.strip.strip", "train.TextSeqDataset.examples.append", "train.TextSeqDataset.masks.append", "train.TextSeqDataset.labels.append", "str", "len", "tokenizer.convert_tokens_to_ids", "len", "tokenizer.convert_tokens_to_ids", "len", "len", "line.strip.strip.lower().split", "line.strip.lower.split", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "line.strip.lower.split", "tokenizer.convert_tokens_to_ids", "len", "str", "line.strip.lower.split", "tokenizer.tokenize", "code_str.strip.strip.split", "len", "args.output_dir.replace", "line.strip.strip.lower", "len", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["precheck_sent", "=", "prepare_sequence", "(", "'i want to go to cambridge .'", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextSeqDataset.__len__": [[146, 148], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.TextSeqDataset.__getitem__": [[149, 151], ["torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.load_and_cache_examples": [[153, 156], ["train.TextSeqDataset"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.set_seed": [[158, 164], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train._rotate_checkpoints": [[166, 193], ["glob.glob", "sorted", "max", "os.path.join", "len", "logger.info", "shutil.rmtree", "ordering_and_checkpoint_path.append", "re.match", "len", "re.match.groups", "ordering_and_checkpoint_path.append", "os.path.getmtime", "int", "re.match.groups"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.mask_tokens": [[195, 216], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "tokenizer.convert_tokens_to_ids", "torch.randint", "tokenizer.get_special_tokens_mask", "torch.tensor", "torch.bernoulli().bool", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli().bool", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.train": [[218, 348], ["torch.utils.data.DataLoader", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "torch.nn.parallel.DistributedDataParallel.resize_token_embeddings", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.trange", "train.set_seed", "SummaryWriter", "max", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.initialize", "len", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "len", "int", "enumerate", "SummaryWriter.close", "logger.info", "inputs.to.to", "labels.to.to", "torch.nn.parallel.DistributedDataParallel.train", "loss.mean.item", "tqdm.trange.close", "len", "ImportError", "torch.distributed.get_world_size", "torch.nn.parallel.DistributedDataParallel.", "torch.nn.parallel.DistributedDataParallel.", "loss.mean.mean", "loss.mean.backward", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "len", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "logger.info", "os.path.join", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "torch.save", "logger.info", "train._rotate_checkpoints", "any", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "train.evaluate", "evaluate.items", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "SummaryWriter.add_scalar", "transformers.get_linear_schedule_with_warmup.get_lr", "float", "float"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train._rotate_checkpoints", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.evaluate": [[350, 409], ["train.load_and_cache_examples", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.eval", "tqdm.tqdm", "torch.exp", "os.path.join", "os.makedirs", "max", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "torch.nn.DataParallel", "len", "inputs.to.to", "masks.to.to", "labels.to.to", "torch.tensor", "open", "logger.info", "sorted", "os.path.exists", "torch.no_grad", "lm_loss.mean().item", "result.keys", "logger.info", "writer.write", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "str", "lm_loss.mean", "str"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.load_and_cache_examples", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.train.main": [[411, 630], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logger.warning", "train.set_seed", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "min", "model_class.from_pretrained", "model_class.from_pretrained.to", "logger.info", "ValueError", "ValueError", "os.path.exists", "os.listdir", "ValueError", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "torch.distributed.barrier", "train.load_and_cache_examples", "train.train", "logger.info", "logger.info", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "model_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained.to", "logger.info", "bool", "torch.distributed.barrier", "torch.distributed.barrier", "os.makedirs", "hasattr", "os.path.join", "list", "logging.getLogger().setLevel", "model_class.from_pretrained", "model_class.from_pretrained.to", "train.evaluate", "dict", "results.update", "torch.distributed.get_rank", "os.path.exists", "torch.cuda.is_available", "os.path.dirname", "logging.getLogger", "len", "checkpoint.split", "checkpoint.find", "checkpoint.split", "sorted", "dict.items", "glob.glob"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.load_and_cache_examples", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear": [[7, 10], ["s.replace.replace"], "function", ["None"], ["for", "span", "in", "span_info", ":", "\n", "        ", "span_words", "=", "span", "[", "2", "]", ".", "split", "(", ")", "\n", "result", "=", "phrase_idx_utt", "(", "span_words", ",", "new_words", ")", "\n", "if", "result", "is", "not", "None", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.seq2dict": [[11, 94], ["s.split", "utils.clear", "d[].append", "d[].append", "d[].append", "type", "type", "type", "utils.clear", "d[].append", "domain.append", "utils.clear", "intent.append", "slot.append", "clear.append"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear", "home.repos.pwc.inspect_result.thu-coai_LAUG.gpt.utils.clear"], ["            ", "max_start", ",", "max_end", "=", "result", "\n", "new_span_info", ".", "append", "(", "[", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", ")", "\n", "", "", "return", "new_span_info", "\n", "\n", "\n", "", "def", "span2tuple", "(", "span_info", ")", ":", "\n", "    ", "t", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "        ", "t", ".", "append", "(", "(", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", ",", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "0", "]", ",", "span", "[", "1", "]", ",", "span", "[", "2", "]", ")", ")", "\n", "", "return", "t", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.__init__": [[46, 255], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "print", "print", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "model_frames.MILU.vocab.get_vocab_size", "allennlp.data.token_indexers.TokenCharactersIndexer", "range", "torch.LongTensor", "torch.LongTensor", "range", "torch.LongTensor", "torch.LongTensor", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.tensor", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "allennlp.modules.TimeDistributed", "LAUG.nlu.milu_new.multilabel_f1_measure.MultiLabelF1Measure", "LAUG.nlu.milu_new.multilabel_f1_measure.MultiLabelF1Measure", "LAUG.nlu.milu_new.dai_f1_measure.DialogActItemF1Measure", "allennlp.common.checks.check_dimensions_match", "initializer", "model_frames.MILU.vocab.get_token_from_index", "model_frames.MILU.split", "model_frames.MILU.vocab.get_token_index", "model_frames.MILU.vocab.get_token_index", "max", "max", "model_frames.MILU.prefix_token_list.append", "model_frames.MILU.prefix_token_character_list.append", "model_frames.MILU.vocab.get_token_from_index", "model_frames.MILU.split", "model_frames.MILU.vocab.get_token_index", "model_frames.MILU.vocab.get_token_index", "max", "max", "model_frames.MILU.req_prefix_token_list.append", "model_frames.MILU.req_prefix_token_character_list.append", "torch.nn.Dropout", "feedforward.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "torch.tensor", "feedforward.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "model_frames.MILU.encoder.get_output_dim", "torch.nn.modules.linear.Linear", "model_frames.MILU.vocab.get_index_to_token_vocabulary", "allennlp.modules.conditional_random_field.allowed_transitions", "allennlp.modules.ConditionalRandomField", "allennlp.training.metrics.SpanBasedF1Measure", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "allennlp.common.checks.check_dimensions_match", "domain.lower", "intent.lower", "allennlp.data.token_indexers.TokenCharactersIndexer.tokens_to_indices", "len", "len", "domain.lower", "intent.lower", "allennlp.data.token_indexers.TokenCharactersIndexer.tokens_to_indices", "len", "len", "allennlp.common.checks.ConfigurationError", "allennlp.modules.attention.LegacyAttention", "torch.tensor", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "encoder.get_output_dim", "feedforward.get_input_dim", "len", "len", "len", "len", "torch.tensor", "model_frames.MILU.vocab.get_index_to_token_vocabulary().items", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "math.log10", "model_frames.MILU.vocab.get_index_to_token_vocabulary().items", "model_frames.MILU.vocab.get_index_to_token_vocabulary().items", "model_frames.MILU.vocab.get_index_to_token_vocabulary", "model_frames.MILU.vocab.get_index_to_token_vocabulary().items", "model_frames.MILU.vocab.get_index_to_token_vocabulary", "model_frames.MILU.vocab.get_index_to_token_vocabulary", "model_frames.MILU.vocab.get_index_to_token_vocabulary", "t.split"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "intent_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "req_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "tag_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "attention", ":", "Attention", "=", "None", ",", "\n", "attention_function", ":", "SimilarityFunction", "=", "None", ",", "\n", "context_for_intent", ":", "bool", "=", "True", ",", "\n", "context_for_req", ":", "bool", "=", "True", ",", "\n", "context_for_tag", ":", "bool", "=", "True", ",", "\n", "attention_for_intent", ":", "bool", "=", "True", ",", "\n", "attention_for_req", ":", "bool", "=", "True", ",", "\n", "attention_for_tag", ":", "bool", "=", "True", ",", "\n", "sequence_label_namespace", ":", "str", "=", "\"tags\"", ",", "\n", "slot_sequence_label_namespace", ":", "str", "=", "\"slot_tags\"", ",", "\n", "intent_label_namespace", ":", "str", "=", "\"intent_labels\"", ",", "\n", "slot_intent_label_namespace", ":", "str", "=", "\"slot_intent_labels\"", ",", "\n", "req_intent_label_namespace", ":", "str", "=", "\"req_intent_labels\"", ",", "\n", "req_slot_label_namespace", ":", "str", "=", "\"req_slot_labels\"", ",", "\n", "req_full_label_namespace", ":", "str", "=", "\"req_full_labels\"", ",", "\n", "feedforward", ":", "Optional", "[", "FeedForward", "]", "=", "None", ",", "\n", "label_encoding", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "include_start_end_transitions", ":", "bool", "=", "True", ",", "\n", "crf_decoding", ":", "bool", "=", "False", ",", "\n", "constrain_crf_decoding", ":", "bool", "=", "None", ",", "\n", "focal_loss_gamma", ":", "float", "=", "None", ",", "\n", "nongeneral_intent_weight", ":", "float", "=", "5.", ",", "\n", "nongeneral_req_weight", ":", "float", "=", "5.", ",", "\n", "num_train_examples", ":", "float", "=", "None", ",", "\n", "calculate_span_f1", ":", "bool", "=", "None", ",", "\n", "dropout", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "verbose_metrics", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "regularizer", ")", "\n", "print", "(", "'vocab = '", ",", "vocab", ")", "\n", "print", "(", "'contruct MILU model!'", ")", "\n", "self", ".", "context_for_intent", "=", "context_for_intent", "\n", "self", ".", "context_for_req", "=", "context_for_req", "\n", "self", ".", "context_for_tag", "=", "context_for_tag", "\n", "self", ".", "attention_for_intent", "=", "attention_for_intent", "\n", "self", ".", "attention_for_req", "=", "attention_for_req", "\n", "self", ".", "attention_for_tag", "=", "attention_for_tag", "\n", "self", ".", "sequence_label_namespace", "=", "sequence_label_namespace", "\n", "self", ".", "slot_sequence_label_namesapce", "=", "slot_sequence_label_namespace", "\n", "self", ".", "intent_label_namespace", "=", "intent_label_namespace", "\n", "self", ".", "req_intent_label_namespace", "=", "req_intent_label_namespace", "\n", "self", ".", "req_slot_label_namespace", "=", "req_slot_label_namespace", "\n", "self", ".", "req_full_label_namespace", "=", "req_full_label_namespace", "\n", "# self.req_label_namespace = req_label_namespace", "\n", "self", ".", "slot_intent_label_namespace", "=", "slot_intent_label_namespace", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "num_tags", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "sequence_label_namespace", ")", "\n", "self", ".", "num_slot_tags", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "slot_sequence_label_namespace", ")", "\n", "self", ".", "num_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "intent_label_namespace", ")", "\n", "self", ".", "num_req_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_intent_label_namespace", ")", "\n", "self", ".", "num_req_slots", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_slot_label_namespace", ")", "\n", "self", ".", "num_req_full", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_full_label_namespace", ")", "\n", "self", ".", "num_slot_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "slot_intent_label_namespace", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "intent_encoder", "=", "intent_encoder", "\n", "self", ".", "req_encoder", "=", "intent_encoder", "\n", "self", ".", "tag_encoder", "=", "intent_encoder", "\n", "self", ".", "_feedforward", "=", "feedforward", "\n", "self", ".", "_verbose_metrics", "=", "verbose_metrics", "\n", "self", ".", "rl", "=", "False", "\n", "\n", "\n", "myIndexer", "=", "TokenCharactersIndexer", "(", "min_padding_length", "=", "3", ")", "\n", "self", ".", "prefix_token_list", "=", "[", "]", "\n", "self", ".", "prefix_token_character_list", "=", "[", "]", "\n", "max_len", "=", "-", "1", "#for padding", "\n", "for", "i", "in", "range", "(", "self", ".", "num_slot_intents", ")", ":", "\n", "            ", "pair", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "i", ",", "self", ".", "slot_intent_label_namespace", ")", "\n", "domain", ",", "intent", "=", "pair", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "#vocab\u90fd\u662f\u5c0f\u5199\u7684 \u8981\u8f6clower", "\n", "intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "intent", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "\n", "characters_list", "=", "myIndexer", ".", "tokens_to_indices", "(", "[", "Token", "(", "domain", ")", ",", "Token", "(", "intent", ")", "]", ",", "self", ".", "vocab", ",", "\"token_characters\"", ")", "[", "\"token_characters\"", "]", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "0", "]", ")", ")", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "1", "]", ")", ")", "\n", "self", ".", "prefix_token_list", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "self", ".", "prefix_token_character_list", ".", "append", "(", "characters_list", ")", "\n", "#padding", "\n", "", "for", "i", "in", "self", ".", "prefix_token_character_list", ":", "\n", "            ", "i", "[", "0", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "0", "]", ")", ")", "\n", "i", "[", "1", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "1", "]", ")", ")", "\n", "\n", "", "self", ".", "prefix_token_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "prefix_token_list", ")", "\n", "self", ".", "prefix_token_character_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "prefix_token_character_list", ")", "\n", "\n", "# req intent prefix", "\n", "self", ".", "req_prefix_token_list", "=", "[", "]", "\n", "self", ".", "req_prefix_token_character_list", "=", "[", "]", "\n", "max_len", "=", "-", "1", "#for padding", "\n", "for", "i", "in", "range", "(", "self", ".", "num_req_intents", ")", ":", "\n", "            ", "pair", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "i", ",", "self", ".", "req_intent_label_namespace", ")", "\n", "domain", ",", "intent", "=", "pair", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "#vocab\u90fd\u662f\u5c0f\u5199\u7684 \u8981\u8f6clower", "\n", "intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "intent", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "\n", "characters_list", "=", "myIndexer", ".", "tokens_to_indices", "(", "[", "Token", "(", "domain", ")", ",", "Token", "(", "intent", ")", "]", ",", "self", ".", "vocab", ",", "\"token_characters\"", ")", "[", "\"token_characters\"", "]", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "0", "]", ")", ")", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "1", "]", ")", ")", "\n", "self", ".", "req_prefix_token_list", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "self", ".", "req_prefix_token_character_list", ".", "append", "(", "characters_list", ")", "\n", "#padding", "\n", "", "for", "i", "in", "self", ".", "req_prefix_token_character_list", ":", "\n", "            ", "i", "[", "0", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "0", "]", ")", ")", "\n", "i", "[", "1", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "1", "]", ")", ")", "\n", "\n", "", "self", ".", "req_prefix_token_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "req_prefix_token_list", ")", "\n", "self", ".", "req_prefix_token_character_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "req_prefix_token_character_list", ")", "\n", "\n", "if", "attention", ":", "\n", "            ", "if", "attention_function", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"You can only specify an attention module or an \"", "\n", "\"attention function, but not both.\"", ")", "\n", "", "self", ".", "attention", "=", "attention", "\n", "", "elif", "attention_function", ":", "\n", "            ", "self", ".", "attention", "=", "LegacyAttention", "(", "attention_function", ")", "\n", "\n", "", "if", "dropout", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "\n", "", "projection_input_dim", "=", "feedforward", ".", "get_output_dim", "(", ")", "if", "self", ".", "_feedforward", "else", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "if", "self", ".", "context_for_intent", ":", "\n", "            ", "projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "", "if", "self", ".", "attention_for_intent", ":", "\n", "            ", "projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "", "self", ".", "intent_projection_layer", "=", "Linear", "(", "projection_input_dim", ",", "self", ".", "num_intents", ")", "\n", "self", ".", "req_projection_layer", "=", "Linear", "(", "projection_input_dim", ",", "self", ".", "num_req_slots", ")", "\n", "\n", "if", "num_train_examples", ":", "\n", "            ", "try", ":", "\n", "                ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "log10", "(", "(", "num_train_examples", "-", "self", ".", "vocab", ".", "_retained_counter", "[", "intent_label_namespace", "]", "[", "t", "]", ")", "/", "\n", "self", ".", "vocab", ".", "_retained_counter", "[", "intent_label_namespace", "]", "[", "t", "]", ")", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "", "except", ":", "\n", "                ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "1.", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "(", "lambda", "t", ":", "nongeneral_intent_weight", "if", "t", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "[", "'request'", ",", "'affirm'", ",", "'negate'", ",", "'request_alts'", ",", "'request_compare'", "]", "else", "1.", ")", "(", "t", ")", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "\n", "", "req_pos_weight", "=", "torch", ".", "tensor", "(", "[", "1.", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "req_slot_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "\n", "self", ".", "intent_loss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "pos_weight", ",", "reduction", "=", "\"none\"", ")", "\n", "self", ".", "req_loss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "req_pos_weight", ",", "reduction", "=", "\"none\"", ")", "\n", "\n", "tag_projection_input_dim", "=", "feedforward", ".", "get_output_dim", "(", ")", "if", "self", ".", "_feedforward", "else", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "if", "self", ".", "context_for_tag", ":", "\n", "            ", "tag_projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "", "if", "self", ".", "attention_for_tag", ":", "\n", "            ", "tag_projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "", "self", ".", "tag_projection_layer", "=", "TimeDistributed", "(", "Linear", "(", "tag_projection_input_dim", ",", "\n", "self", ".", "num_slot_tags", ")", ")", "\n", "\n", "# if  constrain_crf_decoding and calculate_span_f1 are not", "\n", "# provided, (i.e., they're None), set them to True", "\n", "# if label_encoding is provided and False if it isn't.", "\n", "if", "constrain_crf_decoding", "is", "None", ":", "\n", "            ", "constrain_crf_decoding", "=", "label_encoding", "is", "not", "None", "\n", "", "if", "calculate_span_f1", "is", "None", ":", "\n", "            ", "calculate_span_f1", "=", "label_encoding", "is", "not", "None", "\n", "\n", "", "self", ".", "label_encoding", "=", "label_encoding", "\n", "if", "constrain_crf_decoding", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"constrain_crf_decoding is True, but \"", "\n", "\"no label_encoding was specified.\"", ")", "\n", "", "labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "slot_sequence_label_namespace", ")", "\n", "constraints", "=", "allowed_transitions", "(", "label_encoding", ",", "labels", ")", "\n", "", "else", ":", "\n", "            ", "constraints", "=", "None", "\n", "\n", "", "self", ".", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", "if", "crf_decoding", ":", "\n", "            ", "self", ".", "crf", "=", "ConditionalRandomField", "(", "\n", "self", ".", "num_slot_tags", ",", "constraints", ",", "\n", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crf", "=", "None", "\n", "\n", "\n", "", "self", ".", "_intent_f1_metric", "=", "MultiLabelF1Measure", "(", "vocab", ",", "\n", "namespace", "=", "intent_label_namespace", ")", "\n", "self", ".", "_req_f1_metric", "=", "MultiLabelF1Measure", "(", "vocab", ",", "namespace", "=", "req_slot_label_namespace", ")", "\n", "self", ".", "calculate_span_f1", "=", "calculate_span_f1", "\n", "if", "calculate_span_f1", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"calculate_span_f1 is True, but \"", "\n", "\"no label_encoding was specified.\"", ")", "\n", "", "self", ".", "_f1_metric", "=", "SpanBasedF1Measure", "(", "vocab", ",", "\n", "tag_namespace", "=", "slot_sequence_label_namespace", ",", "\n", "label_encoding", "=", "label_encoding", ")", "\n", "", "self", ".", "_dai_f1_metric", "=", "DialogActItemF1Measure", "(", ")", "\n", "\n", "check_dimensions_match", "(", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\"encoder input dim\"", ")", "\n", "if", "feedforward", "is", "not", "None", ":", "\n", "            ", "check_dimensions_match", "(", "encoder", ".", "get_output_dim", "(", ")", ",", "feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"encoder output dim\"", ",", "\"feedforward input dim\"", ")", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.get_tag_encoded_text": [[256, 276], ["model_frames.MILU.prefix_token_list.repeat().cuda", "model_frames.MILU.prefix_token_character_list.repeat().cuda", "model_frames.MILU.text_field_embedder().cuda", "torch.ones().cuda", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "model_frames.MILU.encoder", "model_frames.MILU.tag_encoder", "model_frames.MILU.dropout", "model_frames.MILU.prefix_token_list.repeat", "model_frames.MILU.prefix_token_character_list.repeat", "model_frames.MILU.text_field_embedder", "torch.ones", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze"], "methods", ["None"], ["", "def", "get_tag_encoded_text", "(", "self", ",", "word_seq_tensor", ",", "word_mask_tensor", ")", ":", "\n", "        ", "batch_size", "=", "word_seq_tensor", ".", "shape", "[", "0", "]", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "prefix_token", "=", "self", ".", "prefix_token_list", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix_token_character", "=", "self", ".", "prefix_token_character_list", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix", "=", "{", "\"tokens\"", ":", "prefix_token", ",", "\"token_characters\"", ":", "prefix_token_character", "}", "\n", "intent_prefix", "=", "self", ".", "text_field_embedder", "(", "prefix", ")", ".", "cuda", "(", ")", "\n", "mask_prefix", "=", "torch", ".", "ones", "(", "(", "intent_prefix", ".", "shape", "[", "0", "]", ",", "intent_prefix", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_slot_intents", ",", "1", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_slot_intents", ",", "max_len", ",", "-", "1", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_slot_intents", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "slot_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "intent_prefix", ",", "repeat_word_seq_tensor", ")", ",", "1", ")", "\n", "slot_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "slot_word_seq_tensor", ",", "slot_word_mask_tensor", ")", "\n", "tag_encoded_text", "=", "self", ".", "tag_encoder", "(", "encoded_text", ",", "slot_word_mask_tensor", ")", "if", "self", ".", "tag_encoder", "else", "encoded_text", "\n", "if", "self", ".", "dropout", "and", "self", ".", "tag_encoder", ":", "\n", "            ", "tag_encoded_text", "=", "self", ".", "dropout", "(", "tag_encoded_text", ")", "\n", "\n", "", "return", "tag_encoded_text", ",", "slot_word_mask_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.get_req_encoded_text": [[277, 297], ["model_frames.MILU.req_prefix_token_list.repeat().cuda", "model_frames.MILU.req_prefix_token_character_list.repeat().cuda", "model_frames.MILU.text_field_embedder().cuda", "torch.ones().cuda", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "model_frames.MILU.encoder", "model_frames.MILU.req_encoder", "model_frames.MILU.dropout", "model_frames.MILU.req_prefix_token_list.repeat", "model_frames.MILU.req_prefix_token_character_list.repeat", "model_frames.MILU.text_field_embedder", "torch.ones", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze"], "methods", ["None"], ["", "def", "get_req_encoded_text", "(", "self", ",", "word_seq_tensor", ",", "word_mask_tensor", ")", ":", "\n", "        ", "batch_size", "=", "word_seq_tensor", ".", "shape", "[", "0", "]", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "prefix_token", "=", "self", ".", "req_prefix_token_list", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix_token_character", "=", "self", ".", "req_prefix_token_character_list", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix", "=", "{", "\"tokens\"", ":", "prefix_token", ",", "\"token_characters\"", ":", "prefix_token_character", "}", "\n", "intent_prefix", "=", "self", ".", "text_field_embedder", "(", "prefix", ")", ".", "cuda", "(", ")", "\n", "mask_prefix", "=", "torch", ".", "ones", "(", "(", "intent_prefix", ".", "shape", "[", "0", "]", ",", "intent_prefix", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_req_intents", ",", "max_len", ",", "-", "1", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "req_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "intent_prefix", ",", "repeat_word_seq_tensor", ")", ",", "1", ")", "\n", "req_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "req_word_seq_tensor", ",", "req_word_mask_tensor", ")", "\n", "req_encoded_text", "=", "self", ".", "req_encoder", "(", "encoded_text", ",", "req_word_mask_tensor", ")", "if", "self", ".", "req_encoder", "else", "encoded_text", "\n", "if", "self", ".", "dropout", "and", "self", ".", "req_encoder", ":", "\n", "            ", "req_encoded_text", "=", "self", ".", "dropout", "(", "req_encoded_text", ")", "\n", "\n", "", "return", "req_encoded_text", ",", "req_word_mask_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.forward": [[298, 475], ["model_frames.MILU.text_field_embedder", "allennlp.get_text_field_mask", "model_frames.MILU.encoder", "model_frames.MILU.get_req_encoded_text", "model_frames.MILU.get_tag_encoded_text", "model_frames.MILU.intent_projection_layer", "model_frames.MILU.req_projection_layer", "torch.sigmoid", "torch.sigmoid", "model_frames.MILU.tag_projection_layer", "model_frames.MILU.text_field_embedder", "allennlp.get_text_field_mask", "model_frames.MILU.encoder", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states.unsqueeze().repeat().view", "model_frames.MILU.dropout", "model_frames.MILU.dropout", "model_frames.MILU.intent_encoder", "model_frames.MILU.dropout", "model_frames.MILU.dropout", "model_frames.MILU.intent_encoder.is_bidirectional", "model_frames.MILU.encoder.is_bidirectional", "model_frames.MILU._feedforward", "model_frames.MILU._feedforward", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states", "model_frames.MILU.attention", "allennlp.weighted_sum", "model_frames.MILU.attention", "allennlp.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_frames.MILU.crf.viterbi_tags", "model_frames.MILU.get_predicted_tags", "tokens[].tolist", "model_frames.MILU.decode", "model_frames.MILU._dai_f1_metric", "torch.mean", "model_frames.MILU._intent_f1_metric", "model_frames.MILU.get_req_mask_and_label", "model_frames.MILU.dropout", "model_frames.MILU.dropout", "model_frames.MILU.encoder.is_bidirectional", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states", "allennlp.get_text_field_mask.float", "allennlp.get_text_field_mask.float", "model_frames.MILU.crf", "enumerate", "slot_tag_tensor.view.view.view", "tag_mask_tensor.view.view.view", "allennlp.nn.util.sequence_cross_entropy_with_logits", "model_frames.MILU._f1_metric", "model_frames.MILU.get_rewards", "model_frames.MILU.intent_loss", "torch.mean", "allennlp.get_final_encoder_states.unsqueeze().repeat", "allennlp.get_final_encoder_states.unsqueeze().expand", "allennlp.weighted_sum.unsqueeze().expand", "enumerate", "tag_mask_tensor.view.view.float", "intents.float", "model_frames.MILU.req_loss", "allennlp.get_final_encoder_states.size", "torch.cat.size", "allennlp.get_final_encoder_states.size", "allennlp.weighted_sum.size", "torch.cat.size", "allennlp.weighted_sum.size", "allennlp.get_final_encoder_states.unsqueeze", "allennlp.get_final_encoder_states.unsqueeze", "allennlp.weighted_sum.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_encoded_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_tag_encoded_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_predicted_tags", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_mask_and_label"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# type: ignore", "\n", "context_tokens", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "tags", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "slot_tag_tensor", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "tag_mask_tensor", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "intents", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "reqs", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "full_reqs", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "# pylint: disable=unused-argument", "\n", "**", "kwargs", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n\n        Returns\n        -------\n        \"\"\"", "\n", "if", "self", ".", "context_for_intent", "or", "self", ".", "context_for_tag", "or", "self", ".", "attention_for_intent", "or", "self", ".", "attention_for_tag", ":", "\n", "            ", "embedded_context_input", "=", "self", ".", "text_field_embedder", "(", "context_tokens", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "                ", "embedded_context_input", "=", "self", ".", "dropout", "(", "embedded_context_input", ")", "\n", "\n", "", "context_mask", "=", "util", ".", "get_text_field_mask", "(", "context_tokens", ")", "\n", "encoded_context", "=", "self", ".", "encoder", "(", "embedded_context_input", ",", "context_mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "                ", "encoded_context", "=", "self", ".", "dropout", "(", "encoded_context", ")", "\n", "\n", "", "encoded_context_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "encoded_context", ",", "\n", "context_mask", ",", "\n", "self", ".", "encoder", ".", "is_bidirectional", "(", ")", ")", "\n", "batch_size", ",", "max_len", "=", "encoded_context_summary", ".", "shape", "\n", "req_encoded_context_summary", "=", "encoded_context_summary", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_req_intents", ",", "max_len", ")", "\n", "", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "tokens", ")", "\n", "mask", "=", "util", ".", "get_text_field_mask", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "embedded_text_input", "=", "self", ".", "dropout", "(", "embedded_text_input", ")", "\n", "\n", "", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "encoded_text", "=", "self", ".", "dropout", "(", "encoded_text", ")", "\n", "\n", "", "intent_encoded_text", "=", "self", ".", "intent_encoder", "(", "encoded_text", ",", "mask", ")", "if", "self", ".", "intent_encoder", "else", "encoded_text", "#\u5982\u679cintent\u6709encoder \u5c31\u518dencdoer", "\n", "req_encoded_text", ",", "req_mask", "=", "self", ".", "get_req_encoded_text", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "if", "self", ".", "dropout", "and", "self", ".", "intent_encoder", ":", "\n", "            ", "intent_encoded_text", "=", "self", ".", "dropout", "(", "intent_encoded_text", ")", "\n", "req_encoded_text", "=", "self", ".", "dropout", "(", "req_encoded_text", ")", "\n", "\n", "", "is_bidirectional", "=", "self", ".", "intent_encoder", ".", "is_bidirectional", "(", ")", "if", "self", ".", "intent_encoder", "else", "self", ".", "encoder", ".", "is_bidirectional", "(", ")", "\n", "if", "self", ".", "_feedforward", "is", "not", "None", ":", "\n", "            ", "encoded_summary", "=", "self", ".", "_feedforward", "(", "util", ".", "get_final_encoder_states", "(", "\n", "intent_encoded_text", ",", "\n", "mask", ",", "\n", "is_bidirectional", ")", ")", "\n", "req_encoded_summary", "=", "self", ".", "_feedforward", "(", "util", ".", "get_final_encoder_states", "(", "\n", "req_encoded_text", ",", "\n", "req_mask", ",", "\n", "is_bidirectional", ")", ")", "\n", "", "else", ":", "\n", "            ", "encoded_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "intent_encoded_text", ",", "\n", "mask", ",", "\n", "is_bidirectional", ")", "\n", "req_encoded_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "req_encoded_text", ",", "\n", "req_mask", ",", "\n", "is_bidirectional", ")", "\n", "\n", "", "attend_context", "=", "None", "\n", "\n", "if", "self", ".", "attention_for_intent", "or", "self", ".", "attention_for_tag", ":", "\n", "            ", "attention_weights", "=", "self", ".", "attention", "(", "encoded_summary", ",", "encoded_context", ",", "context_mask", ".", "float", "(", ")", ")", "\n", "attended_context", "=", "util", ".", "weighted_sum", "(", "encoded_context", ",", "attention_weights", ")", "\n", "req_attention_weights", "=", "self", ".", "attention", "(", "req_encoded_summary", ",", "req_encoded_text", ",", "context_mask", ".", "float", "(", ")", ")", "\n", "req_attended_context", "=", "util", ".", "weighted_sum", "(", "req_encoded_text", ",", "attention_weights", ")", "\n", "\n", "", "if", "self", ".", "context_for_intent", ":", "\n", "            ", "encoded_summary", "=", "torch", ".", "cat", "(", "[", "encoded_summary", ",", "encoded_context_summary", "]", ",", "dim", "=", "-", "1", ")", "\n", "req_encoded_summary", "=", "torch", ".", "cat", "(", "[", "req_encoded_summary", ",", "req_encoded_context_summary", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "if", "self", ".", "attention_for_intent", ":", "\n", "            ", "encoded_summary", "=", "torch", ".", "cat", "(", "[", "encoded_summary", ",", "attended_context", "]", ",", "dim", "=", "-", "1", ")", "\n", "req_encoded_summary", "=", "torch", ".", "cat", "(", "[", "req_encoded_summary", ",", "req_attended_context", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "tag_encoded_text", ",", "tag_mask", "=", "self", ".", "get_tag_encoded_text", "(", "embedded_text_input", ",", "mask", ")", "\n", "if", "self", ".", "context_for_tag", ":", "\n", "            ", "tag_encoded_text", "=", "torch", ".", "cat", "(", "[", "tag_encoded_text", ",", "\n", "encoded_context_summary", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "\n", "encoded_context_summary", ".", "size", "(", "0", ")", ",", "\n", "tag_encoded_text", ".", "size", "(", "1", ")", ",", "\n", "encoded_context_summary", ".", "size", "(", "1", ")", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "if", "self", ".", "attention_for_tag", ":", "\n", "            ", "tag_encoded_text", "=", "torch", ".", "cat", "(", "[", "tag_encoded_text", ",", "\n", "attended_context", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "\n", "attended_context", ".", "size", "(", "0", ")", ",", "\n", "tag_encoded_text", ".", "size", "(", "1", ")", ",", "\n", "attended_context", ".", "size", "(", "1", ")", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "intent_logits", "=", "self", ".", "intent_projection_layer", "(", "encoded_summary", ")", "\n", "req_logits", "=", "self", ".", "req_projection_layer", "(", "req_encoded_summary", ")", "\n", "# req_logits = self.req_projection_layer(encoded_summary)", "\n", "\n", "intent_probs", "=", "torch", ".", "sigmoid", "(", "intent_logits", ")", "\n", "req_probs", "=", "torch", ".", "sigmoid", "(", "req_logits", ")", "\n", "predicted_intents", "=", "(", "intent_probs", ">", "0.5", ")", ".", "long", "(", ")", "\n", "predicted_reqs", "=", "(", "req_probs", ">", "0.3", ")", ".", "long", "(", ")", "#change req threshold from 0.5 to 0.3", "\n", "sequence_logits", "=", "self", ".", "tag_projection_layer", "(", "tag_encoded_text", ")", "\n", "\n", "if", "self", ".", "crf", "is", "not", "None", ":", "\n", "            ", "best_paths", "=", "self", ".", "crf", ".", "viterbi_tags", "(", "sequence_logits", ",", "mask", ")", "\n", "# Just get the tags and ignore the score.", "\n", "predicted_tags", "=", "[", "x", "for", "x", ",", "y", "in", "best_paths", "]", "\n", "", "else", ":", "\n", "            ", "predicted_tags", "=", "self", ".", "get_predicted_tags", "(", "sequence_logits", ")", "\n", "#print('predicted_tags = ', predicted_tags)", "\n", "\n", "# output = {\"sequence_logits\": sequence_logits, \"mask\": mask, \"tags\": predicted_tags,", "\n", "# \"intent_logits\": intent_logits, \"intent_probs\": intent_probs, \"intents\": predicted_intents,", "\n", "# \"req_logits\":req_logits, \"req_probs\":req_probs,\"reqs\":predicted_reqs}", "\n", "", "output", "=", "{", "\"tokens\"", ":", "tokens", "[", "'tokens'", "]", ".", "tolist", "(", ")", ",", "\"sequence_logits\"", ":", "sequence_logits", ",", "\"mask\"", ":", "mask", ",", "\"tags\"", ":", "predicted_tags", ",", "\n", "\"intent_logits\"", ":", "intent_logits", ",", "\"intent_probs\"", ":", "intent_probs", ",", "\"intents\"", ":", "predicted_intents", ",", "\n", "\"req_logits\"", ":", "req_logits", ",", "\"req_probs\"", ":", "req_probs", ",", "\"reqs\"", ":", "predicted_reqs", "}", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "crf", "is", "not", "None", ":", "\n", "# Add negative log-likelihood as loss", "\n", "                ", "log_likelihood", "=", "self", ".", "crf", "(", "sequence_logits", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ")", "\n", "output", "[", "\"loss\"", "]", "=", "-", "log_likelihood", "\n", "\n", "# Represent viterbi tags as \"class probabilities\" that we can", "\n", "# feed into the metrics", "\n", "class_probabilities", "=", "sequence_logits", "*", "0.", "\n", "for", "i", ",", "instance_tags", "in", "enumerate", "(", "predicted_tags", ")", ":", "\n", "                    ", "for", "j", ",", "tag_id", "in", "enumerate", "(", "instance_tags", ")", ":", "\n", "                        ", "class_probabilities", "[", "i", ",", "j", ",", "tag_id", "]", "=", "1", "\n", "", "", "", "else", ":", "\n", "                ", "slot_tag_tensor", "=", "slot_tag_tensor", ".", "view", "(", "-", "1", ",", "slot_tag_tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "tag_mask_tensor", "=", "tag_mask_tensor", ".", "view", "(", "-", "1", ",", "slot_tag_tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "loss", "=", "sequence_cross_entropy_with_logits", "(", "sequence_logits", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ")", "#tag,mask", "\n", "class_probabilities", "=", "sequence_logits", "\n", "output", "[", "\"loss\"", "]", "=", "loss", "/", "self", ".", "num_slot_intents", "\n", "\n", "", "if", "self", ".", "calculate_span_f1", ":", "\n", "                 ", "self", ".", "_f1_metric", "(", "class_probabilities", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ".", "float", "(", ")", ")", "#mask.float()) ", "\n", "\n", "", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"words\"", "]", "=", "[", "x", "[", "\"words\"", "]", "for", "x", "in", "metadata", "]", "\n", "\n", "", "if", "tags", "is", "not", "None", "and", "metadata", ":", "\n", "            ", "self", ".", "decode", "(", "output", ")", "\n", "self", ".", "_dai_f1_metric", "(", "output", "[", "\"dialog_act\"", "]", ",", "[", "x", "[", "\"dialog_act\"", "]", "for", "x", "in", "metadata", "]", ")", "\n", "rewards", "=", "self", ".", "get_rewards", "(", "output", "[", "\"dialog_act\"", "]", ",", "[", "x", "[", "\"dialog_act\"", "]", "for", "x", "in", "metadata", "]", ")", "if", "self", ".", "rl", "else", "None", "\n", "\n", "", "if", "intents", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"loss\"", "]", "+=", "torch", ".", "mean", "(", "self", ".", "intent_loss", "(", "intent_logits", ",", "intents", ".", "float", "(", ")", ")", ")", "\n", "self", ".", "_intent_f1_metric", "(", "predicted_intents", ",", "intents", ")", "\n", "\n", "", "if", "full_reqs", "is", "not", "None", ":", "\n", "            ", "req_mask", ",", "req_label", "=", "self", ".", "get_req_mask_and_label", "(", "full_reqs", ")", "\n", "if", "True", "in", "req_mask", ":", "\n", "\n", "                ", "active_req_logits", "=", "req_logits", "[", "req_mask", "]", "\n", "active_req_label", "=", "req_label", "[", "req_mask", "]", "\n", "output", "[", "\"loss\"", "]", "+=", "torch", ".", "mean", "(", "self", ".", "req_loss", "(", "active_req_logits", ",", "active_req_label", ")", ")", "\n", "#self._req_f1_metric(predicted_reqs, reqs)", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.get_req_mask_and_label": [[476, 493], ["torch.zeros", "torch.zeros", "enumerate", "torch.zeros.cuda", "torch.zeros.cuda", "torch.max", "torch.argmax().item", "model_frames.MILU.vocab.get_token_from_index", "model_frames.MILU.split", "model_frames.MILU.vocab.get_token_index", "model_frames.MILU.vocab.get_token_index", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "get_req_mask_and_label", "(", "self", ",", "full_reqs", ")", ":", "\n", "# full_reqs : bc, num_req_full", "\n", "# print('full_reqs.shape = ', full_reqs.shape)", "\n", "        ", "bc", "=", "full_reqs", ".", "shape", "[", "0", "]", "\n", "mask", "=", "torch", ".", "zeros", "(", "(", "bc", "*", "self", ".", "num_req_intents", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "label", "=", "torch", ".", "zeros", "(", "(", "bc", "*", "self", ".", "num_req_intents", ",", "self", ".", "num_req_slots", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "for", "i", ",", "req", "in", "enumerate", "(", "full_reqs", ")", ":", "\n", "            ", "if", "torch", ".", "max", "(", "req", ")", "!=", "0", ":", "\n", "                ", "triple_id", "=", "torch", ".", "argmax", "(", "req", ")", ".", "item", "(", ")", "\n", "triple", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "triple_id", ",", "namespace", "=", "self", ".", "req_full_label_namespace", ")", "\n", "domain", ",", "intent", ",", "slot", "=", "triple", ".", "split", "(", "'-'", ")", "\n", "req_intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", "+", "'-'", "+", "intent", ",", "namespace", "=", "self", ".", "req_intent_label_namespace", ")", "\n", "req_slot_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "slot", ",", "namespace", "=", "self", ".", "req_slot_label_namespace", ")", "\n", "mask", "[", "i", "*", "self", ".", "num_req_intents", "+", "req_intent_id", "]", "=", "torch", ".", "FloatTensor", "(", "[", "1", "]", ")", "\n", "label", "[", "i", "*", "self", ".", "num_req_intents", "+", "req_intent_id", ",", "req_slot_id", "]", "=", "torch", ".", "FloatTensor", "(", "[", "1", "]", ")", "\n", "", "", "mask", "=", "mask", "==", "1", "\n", "return", "mask", ".", "cuda", "(", ")", ",", "label", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.get_predicted_tags": [[495, 512], ["all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.argmax", "all_tags.append", "all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "range", "all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "get_predicted_tags", "(", "self", ",", "sequence_logits", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Does a simple position-wise argmax over each token, converts indices to string labels, and\n        adds a ``\"tags\"`` key to the dictionary with the result.\n        \"\"\"", "\n", "all_predictions", "=", "sequence_logits", "\n", "all_predictions", "=", "all_predictions", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "all_predictions", ".", "ndim", "==", "3", ":", "#\u53bb\u6389prefix", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "[", "i", "]", "[", "2", ":", "]", "for", "i", "in", "range", "(", "all_predictions", ".", "shape", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "[", "2", ":", "]", "]", "\n", "", "all_tags", "=", "[", "]", "\n", "for", "predictions", "in", "predictions_list", ":", "\n", "            ", "tags", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "-", "1", ")", "\n", "all_tags", ".", "append", "(", "tags", ")", "\n", "\n", "", "return", "all_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.decode": [[513, 602], ["range", "len", "enumerate", "enumerate", "output_dict[].append", "model_frames.MILU.vocab.get_token_from_index", "model_frames.MILU.vocab.get_token_from_index", "model_frames.MILU.split", "model_frames.MILU.vocab.get_token_from_index", "len", "allennlp.data.dataset_readers.dataset_utils.span_utils.bio_tags_to_spans", "model_frames.MILU.vocab.get_token_from_index", "instance_intents.nonzero().tolist", "enumerate", "dialog_act[].append", "instance_intents.nonzero", "model_frames.MILU.vocab.get_token_from_index", "dialog_act[].append", "dialog_act[].append"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Converts the tag ids to the actual tags.\n        ``output_dict[\"tags\"]`` is a list of lists of tag_ids,\n        so we use an ugly nested list comprehension.\n        \"\"\"", "\n", "\n", "output_dict", "[", "\"tags\"", "]", "=", "[", "\n", "[", "self", ".", "vocab", ".", "get_token_from_index", "(", "tag", ",", "namespace", "=", "self", ".", "slot_sequence_label_namesapce", ")", "\n", "for", "tag", "in", "instance_tags", "]", "\n", "for", "instance_tags", "in", "output_dict", "[", "\"tags\"", "]", "\n", "]", "\n", "#print('predict tags = ',output_dict['tags'])", "\n", "\n", "output_dict", "[", "\"intents\"", "]", "=", "[", "\n", "[", "self", ".", "vocab", ".", "get_token_from_index", "(", "intent", "[", "0", "]", ",", "namespace", "=", "self", ".", "intent_label_namespace", ")", "\n", "for", "intent", "in", "instance_intents", ".", "nonzero", "(", ")", ".", "tolist", "(", ")", "]", "\n", "for", "instance_intents", "in", "output_dict", "[", "\"intents\"", "]", "\n", "]", "\n", "#print(\"predict intents = \", output_dict['intents'])", "\n", "\n", "# output_dict[\"reqs\"] = [", "\n", "#         [self.vocab.get_token_from_index(req[0], namespace=self.req_label_namespace)", "\n", "#     for req in instance_reqs.nonzero().tolist()]", "\n", "#     for instance_reqs in output_dict['reqs']", "\n", "# ]", "\n", "#print('predict reqs = ',output_dict['reqs'])", "\n", "\n", "#prediction: dialog act", "\n", "#label: metadata", "\n", "output_dict", "[", "\"dialog_act\"", "]", "=", "[", "]", "\n", "batch_size", "=", "len", "(", "output_dict", "[", "\"tags\"", "]", ")", "//", "self", ".", "num_slot_intents", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "dialog_act", "=", "{", "}", "\n", "for", "intent", "in", "output_dict", "[", "\"intents\"", "]", "[", "i", "]", ":", "#add general", "\n", "                ", "domain", ",", "real_intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "if", "real_intent", "in", "GENERAL_TYPE", ":", "\n", "                    ", "pair", "=", "[", "None", ",", "None", "]", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                        ", "dialog_act", "[", "intent", "]", "=", "[", "pair", "]", "\n", "", "else", ":", "\n", "                        ", "dialog_act", "[", "intent", "]", ".", "append", "(", "pair", ")", "\n", "\n", "", "", "", "for", "j", ",", "req", "in", "enumerate", "(", "output_dict", "[", "'reqs'", "]", "[", "i", "*", "self", ".", "num_req_intents", ":", "(", "i", "+", "1", ")", "*", "self", ".", "num_req_intents", "]", ")", ":", "#add requestable", "\n", "                ", "intent", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "j", ",", "self", ".", "req_intent_label_namespace", ")", "\n", "#domain, intent = prefix.split('-')", "\n", "if", "intent", "in", "output_dict", "[", "'intents'", "]", "[", "i", "]", ":", "\n", "                    ", "for", "k", ",", "pick_flag", "in", "enumerate", "(", "req", ")", ":", "\n", "                        ", "if", "pick_flag", ":", "\n", "                            ", "slot", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "k", ",", "self", ".", "req_slot_label_namespace", ")", "\n", "pair", "=", "[", "slot", ",", "None", "]", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                                ", "dialog_act", "[", "intent", "]", "=", "[", "pair", "]", "\n", "", "else", ":", "\n", "                                ", "dialog_act", "[", "intent", "]", ".", "append", "(", "pair", ")", "\n", "\n", "# domain, intent, slot = req.split('-')", "\n", "# temp_intent = domain + '-' + intent", "\n", "# if temp_intent in output_dict['intents'][i]:", "\n", "#     pair = [slot, None]", "\n", "#     if temp_intent not in dialog_act:", "\n", "#         dialog_act[temp_intent] = [pair]", "\n", "#     else:", "\n", "#         dialog_act[temp_intent].append(pair)", "\n", "\n", "", "", "", "", "", "for", "j", ",", "tags", "in", "enumerate", "(", "output_dict", "[", "\"tags\"", "]", "[", "i", "*", "self", ".", "num_slot_intents", ":", "(", "i", "+", "1", ")", "*", "self", ".", "num_slot_intents", "]", ")", ":", "# add informable", "\n", "                ", "seq_len", "=", "len", "(", "output_dict", "[", "\"words\"", "]", "[", "i", "]", ")", "\n", "spans", "=", "bio_tags_to_spans", "(", "tags", "[", ":", "seq_len", "]", ")", "\n", "intent", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "j", ",", "self", ".", "slot_intent_label_namespace", ")", "\n", "for", "span", "in", "spans", ":", "\n", "                    ", "if", "intent", "in", "output_dict", "[", "\"intents\"", "]", "[", "i", "]", ":", "\n", "                        ", "slot", "=", "span", "[", "0", "]", "\n", "value", "=", "\" \"", ".", "join", "(", "output_dict", "[", "\"words\"", "]", "[", "i", "]", "[", "span", "[", "1", "]", "[", "0", "]", ":", "span", "[", "1", "]", "[", "1", "]", "+", "1", "]", ")", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                            ", "dialog_act", "[", "intent", "]", "=", "[", "[", "slot", ",", "value", "]", "]", "\n", "", "else", ":", "\n", "                            ", "dialog_act", "[", "intent", "]", ".", "append", "(", "[", "slot", ",", "value", "]", ")", "\n", "\n", "", "", "", "", "output_dict", "[", "\"dialog_act\"", "]", ".", "append", "(", "dialog_act", ")", "\n", "\n", "# print(' len of tokens = {}, bc = {}'.format(len(output_dict['tokens']), batch_size))", "\n", "# for i in range(batch_size):", "\n", "#     print('text = ',' '.join([self.vocab.get_token_from_index(j,\"tokens\") for j in output_dict['tokens'][i] if self.vocab.get_token_from_index(j,\"tokens\") != '@@PADDING@@']))", "\n", "#     print('intents = ', output_dict['intents'][i])", "\n", "#     print(\"reqs = \",output_dict['reqs'][i])", "\n", "#     print('dialog act = ', output_dict['dialog_act'][i])", "\n", "#     print('='*100)", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model_frames.MILU.get_metrics": [[604, 614], ["model_frames.MILU._intent_f1_metric.get_metric", "metrics_to_return.update", "metrics_to_return.update", "model_frames.MILU._f1_metric.get_metric", "metrics_to_return.update", "model_frames.MILU._dai_f1_metric.get_metric", "model_frames.MILU.items", "model_frames.MILU.items"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics_to_return", "=", "{", "}", "\n", "intent_f1_dict", "=", "self", ".", "_intent_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "metrics_to_return", ".", "update", "(", "{", "\"int_\"", "+", "x", "[", ":", "1", "]", ":", "y", "for", "x", ",", "y", "in", "intent_f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", ")", "\n", "if", "self", ".", "calculate_span_f1", ":", "\n", "            ", "f1_dict", "=", "self", ".", "_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "metrics_to_return", ".", "update", "(", "{", "\"tag_\"", "+", "x", "[", ":", "1", "]", ":", "y", "for", "x", ",", "y", "in", "f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", ")", "\n", "", "metrics_to_return", ".", "update", "(", "self", ".", "_dai_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", ")", "\n", "return", "metrics_to_return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dai_f1_measure.DialogActItemF1Measure.__init__": [[12, 21], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        \"\"\"", "\n", "# These will hold per label span counts.", "\n", "self", ".", "_true_positives", "=", "0", "\n", "self", ".", "_false_positives", "=", "0", "\n", "self", ".", "_false_negatives", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dai_f1_measure.DialogActItemF1Measure.__call__": [[23, 46], ["zip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "predictions", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "gold_labels", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        predictions : ``torch.Tensor``, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : ``torch.Tensor``, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the ``predictions`` tensor without the ``num_classes`` dimension.\n        \"\"\"", "\n", "for", "prediction", ",", "gold_label", "in", "zip", "(", "predictions", ",", "gold_labels", ")", ":", "\n", "            ", "for", "dat", "in", "prediction", ":", "\n", "                ", "for", "sv", "in", "prediction", "[", "dat", "]", ":", "\n", "                    ", "if", "dat", "not", "in", "gold_label", "or", "sv", "not", "in", "gold_label", "[", "dat", "]", ":", "\n", "                        ", "self", ".", "_false_positives", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_true_positives", "+=", "1", "\n", "", "", "", "for", "dat", "in", "gold_label", ":", "\n", "                ", "for", "sv", "in", "gold_label", "[", "dat", "]", ":", "\n", "                    ", "if", "dat", "not", "in", "prediction", "or", "sv", "not", "in", "prediction", "[", "dat", "]", ":", "\n", "                        ", "self", ".", "_false_negatives", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dai_f1_measure.DialogActItemF1Measure.get_metric": [[51, 74], ["dai_f1_measure.DialogActItemF1Measure._compute_metrics", "dai_f1_measure.DialogActItemF1Measure.reset"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure._compute_metrics", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.reset"], ["", "", "", "", "", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns\n        -------\n        A Dict per label containing following the span based metrics:\n        precision : float\n        recall : float\n        f1-measure : float\n\n        Additionally, an ``overall`` key is included, which provides the precision,\n        recall and f1-measure for all spans.\n        \"\"\"", "\n", "# Compute the precision, recall and f1 for all spans jointly.", "\n", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "self", ".", "_true_positives", ",", "\n", "self", ".", "_false_positives", ",", "\n", "self", ".", "_false_negatives", ")", "\n", "metrics", "=", "{", "}", "\n", "metrics", "[", "\"precision\"", "]", "=", "precision", "\n", "metrics", "[", "\"recall\"", "]", "=", "recall", "\n", "metrics", "[", "\"f1-measure\"", "]", "=", "f1_measure", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dai_f1_measure.DialogActItemF1Measure._compute_metrics": [[76, 82], ["float", "float", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_metrics", "(", "true_positives", ":", "int", ",", "false_positives", ":", "int", ",", "false_negatives", ":", "int", ")", ":", "\n", "        ", "precision", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_positives", "+", "1e-13", ")", "\n", "recall", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_negatives", "+", "1e-13", ")", "\n", "f1_measure", "=", "2.", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-13", ")", ")", "\n", "return", "precision", ",", "recall", ",", "f1_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dai_f1_measure.DialogActItemF1Measure.reset": [[84, 88], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_true_positives", "=", "0", "\n", "self", ".", "_false_positives", "=", "0", "\n", "self", ".", "_false_negatives", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.evaluate.evaluate_from_args": [[69, 120], ["logging.getLogger().setLevel", "allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "LAUG.nlu.milu_new.model.eval", "config.pop", "logger.info", "DatasetReader.from_params.read", "config.pop", "allennlp.data.iterators.DataIterator.from_params", "DataIterator.from_params.index_with", "allennlp.training.util.evaluate", "logger.info", "logger.info", "allennlp.training.util.evaluate.items", "logging.getLogger", "logging.getLogger", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "json.loads", "logger.info", "LAUG.nlu.milu_new.model.vocab.extend_from_instances", "LAUG.nlu.milu_new.model.extend_embedder_vocab", "config.pop", "logger.info", "logging.getLogger", "config.pop", "allennlp.common.Params", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], ["\n", "", "from", "LAUG", ".", "util", ".", "dataloader", ".", "module_dataloader", "import", "MultiTurnNLUDataloader", "\n", "from", "LAUG", ".", "util", ".", "dataloader", ".", "dataset_dataloader", "import", "MultiWOZDataloader", "\n", "dataloader", "=", "MultiTurnNLUDataloader", "(", "dataset_dataloader", "=", "MultiWOZDataloader", "(", ")", ")", "\n", "data", "=", "dataloader", ".", "load_data", "(", "data_key", "=", "'test'", ",", "role", "=", "sys", ".", "argv", "[", "3", "]", ")", "[", "'test'", "]", "\n", "predict_golden", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "data", "[", "'utterance'", "]", ")", ")", ")", ":", "\n", "            ", "predict", "=", "model", ".", "predict", "(", "utterance", "=", "data", "[", "'utterance'", "]", "[", "i", "]", ",", "\n", "context", "=", "data", "[", "'context'", "]", "[", "i", "]", ")", "\n", "label", "=", "data", "[", "'dialog_act'", "]", "[", "i", "]", "\n", "predict_golden", ".", "append", "(", "{", "\n", "'predict'", ":", "predict", ",", "\n", "'golden'", ":", "label", "\n", "}", ")", "\n", "\n", "", "precision", ",", "recall", ",", "F1", "=", "calculateF1", "(", "predict_golden", ")", "\n", "print", "(", "'Model {} on {} {} sentences:'", ".", "format", "(", "model_name", ",", "dataset_name", ",", "len", "(", "predict_golden", ")", ")", ")", "\n", "print", "(", "'\\t Precision: %.2f'", "%", "(", "100", "*", "precision", ")", ")", "\n", "print", "(", "'\\t Recall: %.2f'", "%", "(", "100", "*", "recall", ")", ")", "\n", "print", "(", "'\\t F1: %.2f'", "%", "(", "100", "*", "F1", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"currently supported dataset: MultiWOZ\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.train.train_model_from_args": [[56, 66], ["train.train_model_from_file"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.train.train_model_from_file"], ["\n", "tag_to_ix", "=", "{", "\"O\"", ":", "0", ",", "\"F\"", ":", "1", ",", "\"R\"", ":", "2", ",", "START_TAG", ":", "3", ",", "STOP_TAG", ":", "4", "}", "\n", "\n", "model", "=", "BiLSTM_CRF", "(", "len", "(", "word_to_ix", ")", ",", "tag_to_ix", ",", "EMBEDDING_DIM", ",", "HIDDEN_DIM", ",", "weights", ")", "\n", "model", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ")", "\n", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "0", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.train.train_model_from_file": [[68, 100], ["allennlp.common.Params.from_file", "print", "train.train_model"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.train.train_model"], ["print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "\n", "", "ep", "=", "0", "\n", "for", "epoch", "in", "range", "(", "30", ")", ":", "\n", "\t", "n", ",", "losses", "=", "0", ",", "0.", "\n", "ep", "+=", "1", "\n", "for", "sentence", ",", "tags", "in", "progressbar", "(", "training_data", ")", ":", "\n", "\t\t", "model", ".", "zero_grad", "(", ")", "\n", "sentence_in", "=", "prepare_sequence", "(", "sentence", ",", "word_to_ix", ")", "\n", "targets", "=", "torch", ".", "tensor", "(", "[", "tag_to_ix", "[", "t", "]", "for", "t", "in", "tags", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "loss", "=", "model", ".", "neg_log_likelihood", "(", "sentence_in", ",", "targets", ")", "\n", "losses", "+=", "loss", "\n", "n", "+=", "1", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'model/LSTMCRF_'", "+", "str", "(", "ep", ")", "+", "'.bin'", ")", "\n", "print", "(", "'loss:'", "+", "str", "(", "losses", "/", "n", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t\t", "precheck_sent", "=", "prepare_sequence", "(", "\"okay , i like to do , weight training and cycling .\"", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "1", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "precheck_sent", "=", "prepare_sequence", "(", "'i want to go to cambridge .'", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.train.train_model": [[102, 207], ["print", "allennlp.common.util.prepare_environment", "print", "allennlp.training.util.create_serialization_dir", "allennlp.common.util.prepare_global_logging", "params.params.get().get", "allennlp.common.checks.check_for_gpu", "params.to_file", "print", "params.pop_bool", "params.get().get", "params.assert_empty", "allennlp.common.util.cleanup_global_logging", "allennlp.models.archival.archive_model", "allennlp.common.util.dump_metrics", "os.path.join", "print", "allennlp.training.trainer_pieces.TrainerPieces.from_params", "print", "allennlp.training.trainer.Trainer.from_params", "print", "allennlp.training.trainer_base.TrainerBase.from_params", "TrainerBase.from_params.train", "logger.info", "allennlp.training.util.evaluate", "allennlp.training.util.evaluate.items", "os.path.join", "params.params.get", "params.get", "os.path.exists", "logger.info", "os.path.join", "logging.info", "allennlp.models.archival.archive_model"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.__init__": [[46, 255], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "print", "print", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "model.MILU.vocab.get_vocab_size", "allennlp.data.token_indexers.TokenCharactersIndexer", "range", "torch.LongTensor", "torch.LongTensor", "range", "torch.LongTensor", "torch.LongTensor", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.tensor", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss", "allennlp.modules.TimeDistributed", "LAUG.nlu.milu_new.multilabel_f1_measure.MultiLabelF1Measure", "LAUG.nlu.milu_new.multilabel_f1_measure.MultiLabelF1Measure", "LAUG.nlu.milu_new.dai_f1_measure.DialogActItemF1Measure", "allennlp.common.checks.check_dimensions_match", "initializer", "model.MILU.vocab.get_token_from_index", "model.MILU.split", "model.MILU.vocab.get_token_index", "model.MILU.vocab.get_token_index", "max", "max", "model.MILU.prefix_token_list.append", "model.MILU.prefix_token_character_list.append", "model.MILU.vocab.get_token_from_index", "model.MILU.split", "model.MILU.vocab.get_token_index", "model.MILU.vocab.get_token_index", "max", "max", "model.MILU.req_prefix_token_list.append", "model.MILU.req_prefix_token_character_list.append", "torch.nn.Dropout", "feedforward.get_output_dim", "model.MILU.encoder.get_output_dim", "model.MILU.encoder.get_output_dim", "model.MILU.encoder.get_output_dim", "torch.tensor", "feedforward.get_output_dim", "model.MILU.encoder.get_output_dim", "model.MILU.encoder.get_output_dim", "model.MILU.encoder.get_output_dim", "torch.nn.modules.linear.Linear", "model.MILU.vocab.get_index_to_token_vocabulary", "allennlp.modules.conditional_random_field.allowed_transitions", "allennlp.modules.ConditionalRandomField", "allennlp.training.metrics.SpanBasedF1Measure", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "allennlp.common.checks.check_dimensions_match", "domain.lower", "intent.lower", "allennlp.data.token_indexers.TokenCharactersIndexer.tokens_to_indices", "len", "len", "domain.lower", "intent.lower", "allennlp.data.token_indexers.TokenCharactersIndexer.tokens_to_indices", "len", "len", "allennlp.common.checks.ConfigurationError", "allennlp.modules.attention.LegacyAttention", "torch.tensor", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "encoder.get_output_dim", "feedforward.get_input_dim", "len", "len", "len", "len", "torch.tensor", "model.MILU.vocab.get_index_to_token_vocabulary().items", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "math.log10", "model.MILU.vocab.get_index_to_token_vocabulary().items", "model.MILU.vocab.get_index_to_token_vocabulary().items", "model.MILU.vocab.get_index_to_token_vocabulary", "model.MILU.vocab.get_index_to_token_vocabulary().items", "model.MILU.vocab.get_index_to_token_vocabulary", "model.MILU.vocab.get_index_to_token_vocabulary", "model.MILU.vocab.get_index_to_token_vocabulary", "t.split"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "intent_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "req_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "tag_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "attention", ":", "Attention", "=", "None", ",", "\n", "attention_function", ":", "SimilarityFunction", "=", "None", ",", "\n", "context_for_intent", ":", "bool", "=", "True", ",", "\n", "context_for_req", ":", "bool", "=", "True", ",", "\n", "context_for_tag", ":", "bool", "=", "True", ",", "\n", "attention_for_intent", ":", "bool", "=", "True", ",", "\n", "attention_for_req", ":", "bool", "=", "True", ",", "\n", "attention_for_tag", ":", "bool", "=", "True", ",", "\n", "sequence_label_namespace", ":", "str", "=", "\"tags\"", ",", "\n", "slot_sequence_label_namespace", ":", "str", "=", "\"slot_tags\"", ",", "\n", "intent_label_namespace", ":", "str", "=", "\"intent_labels\"", ",", "\n", "slot_intent_label_namespace", ":", "str", "=", "\"slot_intent_labels\"", ",", "\n", "req_intent_label_namespace", ":", "str", "=", "\"req_intent_labels\"", ",", "\n", "req_slot_label_namespace", ":", "str", "=", "\"req_slot_labels\"", ",", "\n", "req_full_label_namespace", ":", "str", "=", "\"req_full_labels\"", ",", "\n", "feedforward", ":", "Optional", "[", "FeedForward", "]", "=", "None", ",", "\n", "label_encoding", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "include_start_end_transitions", ":", "bool", "=", "True", ",", "\n", "crf_decoding", ":", "bool", "=", "False", ",", "\n", "constrain_crf_decoding", ":", "bool", "=", "None", ",", "\n", "focal_loss_gamma", ":", "float", "=", "None", ",", "\n", "nongeneral_intent_weight", ":", "float", "=", "5.", ",", "\n", "nongeneral_req_weight", ":", "float", "=", "5.", ",", "\n", "num_train_examples", ":", "float", "=", "None", ",", "\n", "calculate_span_f1", ":", "bool", "=", "None", ",", "\n", "dropout", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "verbose_metrics", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "regularizer", ")", "\n", "print", "(", "'vocab = '", ",", "vocab", ")", "\n", "print", "(", "'contruct MILU model!'", ")", "\n", "self", ".", "context_for_intent", "=", "context_for_intent", "\n", "self", ".", "context_for_req", "=", "context_for_req", "\n", "self", ".", "context_for_tag", "=", "context_for_tag", "\n", "self", ".", "attention_for_intent", "=", "attention_for_intent", "\n", "self", ".", "attention_for_req", "=", "attention_for_req", "\n", "self", ".", "attention_for_tag", "=", "attention_for_tag", "\n", "self", ".", "sequence_label_namespace", "=", "sequence_label_namespace", "\n", "self", ".", "slot_sequence_label_namesapce", "=", "slot_sequence_label_namespace", "\n", "self", ".", "intent_label_namespace", "=", "intent_label_namespace", "\n", "self", ".", "req_intent_label_namespace", "=", "req_intent_label_namespace", "\n", "self", ".", "req_slot_label_namespace", "=", "req_slot_label_namespace", "\n", "self", ".", "req_full_label_namespace", "=", "req_full_label_namespace", "\n", "# self.req_label_namespace = req_label_namespace", "\n", "self", ".", "slot_intent_label_namespace", "=", "slot_intent_label_namespace", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "num_tags", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "sequence_label_namespace", ")", "\n", "self", ".", "num_slot_tags", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "slot_sequence_label_namespace", ")", "\n", "self", ".", "num_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "intent_label_namespace", ")", "\n", "self", ".", "num_req_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_intent_label_namespace", ")", "\n", "self", ".", "num_req_slots", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_slot_label_namespace", ")", "\n", "self", ".", "num_req_full", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "req_full_label_namespace", ")", "\n", "self", ".", "num_slot_intents", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "slot_intent_label_namespace", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "intent_encoder", "=", "intent_encoder", "\n", "self", ".", "req_encoder", "=", "intent_encoder", "\n", "self", ".", "tag_encoder", "=", "intent_encoder", "\n", "self", ".", "_feedforward", "=", "feedforward", "\n", "self", ".", "_verbose_metrics", "=", "verbose_metrics", "\n", "self", ".", "rl", "=", "False", "\n", "\n", "\n", "myIndexer", "=", "TokenCharactersIndexer", "(", "min_padding_length", "=", "3", ")", "\n", "self", ".", "prefix_token_list", "=", "[", "]", "\n", "self", ".", "prefix_token_character_list", "=", "[", "]", "\n", "max_len", "=", "-", "1", "#for padding", "\n", "for", "i", "in", "range", "(", "self", ".", "num_slot_intents", ")", ":", "\n", "            ", "pair", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "i", ",", "self", ".", "slot_intent_label_namespace", ")", "\n", "domain", ",", "intent", "=", "pair", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "#vocab\u90fd\u662f\u5c0f\u5199\u7684 \u8981\u8f6clower", "\n", "intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "intent", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "\n", "characters_list", "=", "myIndexer", ".", "tokens_to_indices", "(", "[", "Token", "(", "domain", ")", ",", "Token", "(", "intent", ")", "]", ",", "self", ".", "vocab", ",", "\"token_characters\"", ")", "[", "\"token_characters\"", "]", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "0", "]", ")", ")", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "1", "]", ")", ")", "\n", "self", ".", "prefix_token_list", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "self", ".", "prefix_token_character_list", ".", "append", "(", "characters_list", ")", "\n", "#padding", "\n", "", "for", "i", "in", "self", ".", "prefix_token_character_list", ":", "\n", "            ", "i", "[", "0", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "0", "]", ")", ")", "\n", "i", "[", "1", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "1", "]", ")", ")", "\n", "\n", "", "self", ".", "prefix_token_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "prefix_token_list", ")", "\n", "self", ".", "prefix_token_character_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "prefix_token_character_list", ")", "\n", "\n", "# req intent prefix", "\n", "self", ".", "req_prefix_token_list", "=", "[", "]", "\n", "self", ".", "req_prefix_token_character_list", "=", "[", "]", "\n", "max_len", "=", "-", "1", "#for padding", "\n", "for", "i", "in", "range", "(", "self", ".", "num_req_intents", ")", ":", "\n", "            ", "pair", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "i", ",", "self", ".", "req_intent_label_namespace", ")", "\n", "domain", ",", "intent", "=", "pair", ".", "split", "(", "'-'", ")", "\n", "domain_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "#vocab\u90fd\u662f\u5c0f\u5199\u7684 \u8981\u8f6clower", "\n", "intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "intent", ".", "lower", "(", ")", ",", "\"tokens\"", ")", "\n", "characters_list", "=", "myIndexer", ".", "tokens_to_indices", "(", "[", "Token", "(", "domain", ")", ",", "Token", "(", "intent", ")", "]", ",", "self", ".", "vocab", ",", "\"token_characters\"", ")", "[", "\"token_characters\"", "]", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "0", "]", ")", ")", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "characters_list", "[", "1", "]", ")", ")", "\n", "self", ".", "req_prefix_token_list", ".", "append", "(", "[", "domain_id", ",", "intent_id", "]", ")", "\n", "self", ".", "req_prefix_token_character_list", ".", "append", "(", "characters_list", ")", "\n", "#padding", "\n", "", "for", "i", "in", "self", ".", "req_prefix_token_character_list", ":", "\n", "            ", "i", "[", "0", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "0", "]", ")", ")", "\n", "i", "[", "1", "]", "+=", "[", "0", "]", "*", "(", "max_len", "-", "len", "(", "i", "[", "1", "]", ")", ")", "\n", "\n", "", "self", ".", "req_prefix_token_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "req_prefix_token_list", ")", "\n", "self", ".", "req_prefix_token_character_list", "=", "torch", ".", "LongTensor", "(", "self", ".", "req_prefix_token_character_list", ")", "\n", "\n", "if", "attention", ":", "\n", "            ", "if", "attention_function", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"You can only specify an attention module or an \"", "\n", "\"attention function, but not both.\"", ")", "\n", "", "self", ".", "attention", "=", "attention", "\n", "", "elif", "attention_function", ":", "\n", "            ", "self", ".", "attention", "=", "LegacyAttention", "(", "attention_function", ")", "\n", "\n", "", "if", "dropout", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "\n", "", "projection_input_dim", "=", "feedforward", ".", "get_output_dim", "(", ")", "if", "self", ".", "_feedforward", "else", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "if", "self", ".", "context_for_intent", ":", "\n", "            ", "projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "", "if", "self", ".", "attention_for_intent", ":", "\n", "            ", "projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "", "self", ".", "intent_projection_layer", "=", "Linear", "(", "projection_input_dim", ",", "self", ".", "num_intents", ")", "\n", "self", ".", "req_projection_layer", "=", "Linear", "(", "projection_input_dim", ",", "self", ".", "num_req_slots", ")", "\n", "\n", "if", "num_train_examples", ":", "\n", "            ", "try", ":", "\n", "                ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "log10", "(", "(", "num_train_examples", "-", "self", ".", "vocab", ".", "_retained_counter", "[", "intent_label_namespace", "]", "[", "t", "]", ")", "/", "\n", "self", ".", "vocab", ".", "_retained_counter", "[", "intent_label_namespace", "]", "[", "t", "]", ")", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "", "except", ":", "\n", "                ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "1.", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "pos_weight", "=", "torch", ".", "tensor", "(", "[", "(", "lambda", "t", ":", "nongeneral_intent_weight", "if", "t", ".", "split", "(", "'-'", ")", "[", "1", "]", "in", "[", "'request'", ",", "'affirm'", ",", "'negate'", ",", "'request_alts'", ",", "'request_compare'", "]", "else", "1.", ")", "(", "t", ")", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "intent_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "\n", "", "req_pos_weight", "=", "torch", ".", "tensor", "(", "[", "1.", "for", "i", ",", "t", "in", "\n", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "req_slot_label_namespace", ")", ".", "items", "(", ")", "]", ")", "\n", "\n", "self", ".", "intent_loss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "pos_weight", ",", "reduction", "=", "\"none\"", ")", "\n", "self", ".", "req_loss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "pos_weight", "=", "req_pos_weight", ",", "reduction", "=", "\"none\"", ")", "\n", "\n", "tag_projection_input_dim", "=", "feedforward", ".", "get_output_dim", "(", ")", "if", "self", ".", "_feedforward", "else", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "if", "self", ".", "context_for_tag", ":", "\n", "            ", "tag_projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "", "if", "self", ".", "attention_for_tag", ":", "\n", "            ", "tag_projection_input_dim", "+=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "", "self", ".", "tag_projection_layer", "=", "TimeDistributed", "(", "Linear", "(", "tag_projection_input_dim", ",", "\n", "self", ".", "num_slot_tags", ")", ")", "\n", "\n", "# if  constrain_crf_decoding and calculate_span_f1 are not", "\n", "# provided, (i.e., they're None), set them to True", "\n", "# if label_encoding is provided and False if it isn't.", "\n", "if", "constrain_crf_decoding", "is", "None", ":", "\n", "            ", "constrain_crf_decoding", "=", "label_encoding", "is", "not", "None", "\n", "", "if", "calculate_span_f1", "is", "None", ":", "\n", "            ", "calculate_span_f1", "=", "label_encoding", "is", "not", "None", "\n", "\n", "", "self", ".", "label_encoding", "=", "label_encoding", "\n", "if", "constrain_crf_decoding", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"constrain_crf_decoding is True, but \"", "\n", "\"no label_encoding was specified.\"", ")", "\n", "", "labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "slot_sequence_label_namespace", ")", "\n", "constraints", "=", "allowed_transitions", "(", "label_encoding", ",", "labels", ")", "\n", "", "else", ":", "\n", "            ", "constraints", "=", "None", "\n", "\n", "", "self", ".", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", "if", "crf_decoding", ":", "\n", "            ", "self", ".", "crf", "=", "ConditionalRandomField", "(", "\n", "self", ".", "num_slot_tags", ",", "constraints", ",", "\n", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crf", "=", "None", "\n", "\n", "\n", "", "self", ".", "_intent_f1_metric", "=", "MultiLabelF1Measure", "(", "vocab", ",", "\n", "namespace", "=", "intent_label_namespace", ")", "\n", "self", ".", "_req_f1_metric", "=", "MultiLabelF1Measure", "(", "vocab", ",", "namespace", "=", "req_slot_label_namespace", ")", "\n", "self", ".", "calculate_span_f1", "=", "calculate_span_f1", "\n", "if", "calculate_span_f1", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"calculate_span_f1 is True, but \"", "\n", "\"no label_encoding was specified.\"", ")", "\n", "", "self", ".", "_f1_metric", "=", "SpanBasedF1Measure", "(", "vocab", ",", "\n", "tag_namespace", "=", "slot_sequence_label_namespace", ",", "\n", "label_encoding", "=", "label_encoding", ")", "\n", "", "self", ".", "_dai_f1_metric", "=", "DialogActItemF1Measure", "(", ")", "\n", "\n", "check_dimensions_match", "(", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\"encoder input dim\"", ")", "\n", "if", "feedforward", "is", "not", "None", ":", "\n", "            ", "check_dimensions_match", "(", "encoder", ".", "get_output_dim", "(", ")", ",", "feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"encoder output dim\"", ",", "\"feedforward input dim\"", ")", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_tag_encoded_text": [[256, 276], ["model.MILU.prefix_token_list.repeat().cuda", "model.MILU.prefix_token_character_list.repeat().cuda", "model.MILU.text_field_embedder().cuda", "torch.ones().cuda", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "model.MILU.encoder", "model.MILU.tag_encoder", "model.MILU.dropout", "model.MILU.prefix_token_list.repeat", "model.MILU.prefix_token_character_list.repeat", "model.MILU.text_field_embedder", "torch.ones", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze"], "methods", ["None"], ["", "def", "get_tag_encoded_text", "(", "self", ",", "word_seq_tensor", ",", "word_mask_tensor", ")", ":", "\n", "        ", "batch_size", "=", "word_seq_tensor", ".", "shape", "[", "0", "]", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "prefix_token", "=", "self", ".", "prefix_token_list", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix_token_character", "=", "self", ".", "prefix_token_character_list", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix", "=", "{", "\"tokens\"", ":", "prefix_token", ",", "\"token_characters\"", ":", "prefix_token_character", "}", "\n", "intent_prefix", "=", "self", ".", "text_field_embedder", "(", "prefix", ")", ".", "cuda", "(", ")", "\n", "mask_prefix", "=", "torch", ".", "ones", "(", "(", "intent_prefix", ".", "shape", "[", "0", "]", ",", "intent_prefix", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_slot_intents", ",", "1", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_slot_intents", ",", "max_len", ",", "-", "1", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_slot_intents", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "slot_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "intent_prefix", ",", "repeat_word_seq_tensor", ")", ",", "1", ")", "\n", "slot_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "slot_word_seq_tensor", ",", "slot_word_mask_tensor", ")", "\n", "tag_encoded_text", "=", "self", ".", "tag_encoder", "(", "encoded_text", ",", "slot_word_mask_tensor", ")", "if", "self", ".", "tag_encoder", "else", "encoded_text", "\n", "if", "self", ".", "dropout", "and", "self", ".", "tag_encoder", ":", "\n", "            ", "tag_encoded_text", "=", "self", ".", "dropout", "(", "tag_encoded_text", ")", "\n", "\n", "", "return", "tag_encoded_text", ",", "slot_word_mask_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_encoded_text": [[277, 297], ["model.MILU.req_prefix_token_list.repeat().cuda", "model.MILU.req_prefix_token_character_list.repeat().cuda", "model.MILU.text_field_embedder().cuda", "torch.ones().cuda", "word_seq_tensor.unsqueeze().repeat().view", "word_mask_tensor.unsqueeze().repeat().view", "torch.cat", "torch.cat", "model.MILU.encoder", "model.MILU.req_encoder", "model.MILU.dropout", "model.MILU.req_prefix_token_list.repeat", "model.MILU.req_prefix_token_character_list.repeat", "model.MILU.text_field_embedder", "torch.ones", "word_seq_tensor.unsqueeze().repeat", "word_mask_tensor.unsqueeze().repeat", "word_seq_tensor.unsqueeze", "word_mask_tensor.unsqueeze"], "methods", ["None"], ["", "def", "get_req_encoded_text", "(", "self", ",", "word_seq_tensor", ",", "word_mask_tensor", ")", ":", "\n", "        ", "batch_size", "=", "word_seq_tensor", ".", "shape", "[", "0", "]", "\n", "max_len", "=", "word_seq_tensor", ".", "shape", "[", "1", "]", "\n", "prefix_token", "=", "self", ".", "req_prefix_token_list", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix_token_character", "=", "self", ".", "req_prefix_token_character_list", ".", "repeat", "(", "batch_size", ",", "1", ",", "1", ")", ".", "cuda", "(", ")", "\n", "prefix", "=", "{", "\"tokens\"", ":", "prefix_token", ",", "\"token_characters\"", ":", "prefix_token_character", "}", "\n", "intent_prefix", "=", "self", ".", "text_field_embedder", "(", "prefix", ")", ".", "cuda", "(", ")", "\n", "mask_prefix", "=", "torch", ".", "ones", "(", "(", "intent_prefix", ".", "shape", "[", "0", "]", ",", "intent_prefix", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "long", ")", ".", "cuda", "(", ")", "\n", "\n", "repeat_word_seq_tensor", "=", "word_seq_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_req_intents", ",", "max_len", ",", "-", "1", ")", "\n", "repeat_word_mask_tensor", "=", "word_mask_tensor", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ")", ".", "view", "(", "-", "1", ",", "max_len", ")", "\n", "req_word_seq_tensor", "=", "torch", ".", "cat", "(", "(", "intent_prefix", ",", "repeat_word_seq_tensor", ")", ",", "1", ")", "\n", "req_word_mask_tensor", "=", "torch", ".", "cat", "(", "(", "mask_prefix", ",", "repeat_word_mask_tensor", ")", ",", "1", ")", "\n", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "req_word_seq_tensor", ",", "req_word_mask_tensor", ")", "\n", "req_encoded_text", "=", "self", ".", "req_encoder", "(", "encoded_text", ",", "req_word_mask_tensor", ")", "if", "self", ".", "req_encoder", "else", "encoded_text", "\n", "if", "self", ".", "dropout", "and", "self", ".", "req_encoder", ":", "\n", "            ", "req_encoded_text", "=", "self", ".", "dropout", "(", "req_encoded_text", ")", "\n", "\n", "", "return", "req_encoded_text", ",", "req_word_mask_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.forward": [[298, 475], ["model.MILU.text_field_embedder", "allennlp.get_text_field_mask", "model.MILU.encoder", "model.MILU.get_req_encoded_text", "model.MILU.get_tag_encoded_text", "model.MILU.intent_projection_layer", "model.MILU.req_projection_layer", "torch.sigmoid", "torch.sigmoid", "model.MILU.tag_projection_layer", "model.MILU.text_field_embedder", "allennlp.get_text_field_mask", "model.MILU.encoder", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states.unsqueeze().repeat().view", "model.MILU.dropout", "model.MILU.dropout", "model.MILU.intent_encoder", "model.MILU.dropout", "model.MILU.dropout", "model.MILU.intent_encoder.is_bidirectional", "model.MILU.encoder.is_bidirectional", "model.MILU._feedforward", "model.MILU._feedforward", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states", "model.MILU.attention", "allennlp.weighted_sum", "model.MILU.attention", "allennlp.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.MILU.crf.viterbi_tags", "model.MILU.get_predicted_tags", "tokens[].tolist", "model.MILU.decode", "model.MILU._dai_f1_metric", "torch.mean", "model.MILU._intent_f1_metric", "model.MILU.get_req_mask_and_label", "model.MILU.dropout", "model.MILU.dropout", "model.MILU.encoder.is_bidirectional", "allennlp.get_final_encoder_states", "allennlp.get_final_encoder_states", "allennlp.get_text_field_mask.float", "allennlp.get_text_field_mask.float", "model.MILU.crf", "enumerate", "slot_tag_tensor.view.view.view", "tag_mask_tensor.view.view.view", "allennlp.nn.util.sequence_cross_entropy_with_logits", "model.MILU._f1_metric", "model.MILU.get_rewards", "model.MILU.intent_loss", "torch.mean", "allennlp.get_final_encoder_states.unsqueeze().repeat", "allennlp.get_final_encoder_states.unsqueeze().expand", "allennlp.weighted_sum.unsqueeze().expand", "enumerate", "tag_mask_tensor.view.view.float", "intents.float", "model.MILU.req_loss", "allennlp.get_final_encoder_states.size", "torch.cat.size", "allennlp.get_final_encoder_states.size", "allennlp.weighted_sum.size", "torch.cat.size", "allennlp.weighted_sum.size", "allennlp.get_final_encoder_states.unsqueeze", "allennlp.get_final_encoder_states.unsqueeze", "allennlp.weighted_sum.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_encoded_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_tag_encoded_text", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_predicted_tags", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_mask_and_label"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# type: ignore", "\n", "context_tokens", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "tags", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "slot_tag_tensor", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "tag_mask_tensor", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "intents", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "reqs", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "full_reqs", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "# pylint: disable=unused-argument", "\n", "**", "kwargs", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n\n        Returns\n        -------\n        \"\"\"", "\n", "if", "self", ".", "context_for_intent", "or", "self", ".", "context_for_tag", "or", "self", ".", "attention_for_intent", "or", "self", ".", "attention_for_tag", ":", "\n", "            ", "embedded_context_input", "=", "self", ".", "text_field_embedder", "(", "context_tokens", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "                ", "embedded_context_input", "=", "self", ".", "dropout", "(", "embedded_context_input", ")", "\n", "\n", "", "context_mask", "=", "util", ".", "get_text_field_mask", "(", "context_tokens", ")", "\n", "encoded_context", "=", "self", ".", "encoder", "(", "embedded_context_input", ",", "context_mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "                ", "encoded_context", "=", "self", ".", "dropout", "(", "encoded_context", ")", "\n", "\n", "", "encoded_context_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "encoded_context", ",", "\n", "context_mask", ",", "\n", "self", ".", "encoder", ".", "is_bidirectional", "(", ")", ")", "\n", "batch_size", ",", "max_len", "=", "encoded_context_summary", ".", "shape", "\n", "req_encoded_context_summary", "=", "encoded_context_summary", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "num_req_intents", ",", "1", ")", ".", "view", "(", "batch_size", "*", "self", ".", "num_req_intents", ",", "max_len", ")", "\n", "", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "tokens", ")", "\n", "mask", "=", "util", ".", "get_text_field_mask", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "embedded_text_input", "=", "self", ".", "dropout", "(", "embedded_text_input", ")", "\n", "\n", "", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "encoded_text", "=", "self", ".", "dropout", "(", "encoded_text", ")", "\n", "\n", "", "intent_encoded_text", "=", "self", ".", "intent_encoder", "(", "encoded_text", ",", "mask", ")", "if", "self", ".", "intent_encoder", "else", "encoded_text", "#\u5982\u679cintent\u6709encoder \u5c31\u518dencdoer", "\n", "req_encoded_text", ",", "req_mask", "=", "self", ".", "get_req_encoded_text", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "if", "self", ".", "dropout", "and", "self", ".", "intent_encoder", ":", "\n", "            ", "intent_encoded_text", "=", "self", ".", "dropout", "(", "intent_encoded_text", ")", "\n", "req_encoded_text", "=", "self", ".", "dropout", "(", "req_encoded_text", ")", "\n", "\n", "", "is_bidirectional", "=", "self", ".", "intent_encoder", ".", "is_bidirectional", "(", ")", "if", "self", ".", "intent_encoder", "else", "self", ".", "encoder", ".", "is_bidirectional", "(", ")", "\n", "if", "self", ".", "_feedforward", "is", "not", "None", ":", "\n", "            ", "encoded_summary", "=", "self", ".", "_feedforward", "(", "util", ".", "get_final_encoder_states", "(", "\n", "intent_encoded_text", ",", "\n", "mask", ",", "\n", "is_bidirectional", ")", ")", "\n", "req_encoded_summary", "=", "self", ".", "_feedforward", "(", "util", ".", "get_final_encoder_states", "(", "\n", "req_encoded_text", ",", "\n", "req_mask", ",", "\n", "is_bidirectional", ")", ")", "\n", "", "else", ":", "\n", "            ", "encoded_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "intent_encoded_text", ",", "\n", "mask", ",", "\n", "is_bidirectional", ")", "\n", "req_encoded_summary", "=", "util", ".", "get_final_encoder_states", "(", "\n", "req_encoded_text", ",", "\n", "req_mask", ",", "\n", "is_bidirectional", ")", "\n", "\n", "", "attend_context", "=", "None", "\n", "\n", "if", "self", ".", "attention_for_intent", "or", "self", ".", "attention_for_tag", ":", "\n", "            ", "attention_weights", "=", "self", ".", "attention", "(", "encoded_summary", ",", "encoded_context", ",", "context_mask", ".", "float", "(", ")", ")", "\n", "attended_context", "=", "util", ".", "weighted_sum", "(", "encoded_context", ",", "attention_weights", ")", "\n", "req_attention_weights", "=", "self", ".", "attention", "(", "req_encoded_summary", ",", "req_encoded_text", ",", "context_mask", ".", "float", "(", ")", ")", "\n", "req_attended_context", "=", "util", ".", "weighted_sum", "(", "req_encoded_text", ",", "attention_weights", ")", "\n", "\n", "", "if", "self", ".", "context_for_intent", ":", "\n", "            ", "encoded_summary", "=", "torch", ".", "cat", "(", "[", "encoded_summary", ",", "encoded_context_summary", "]", ",", "dim", "=", "-", "1", ")", "\n", "req_encoded_summary", "=", "torch", ".", "cat", "(", "[", "req_encoded_summary", ",", "req_encoded_context_summary", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "if", "self", ".", "attention_for_intent", ":", "\n", "            ", "encoded_summary", "=", "torch", ".", "cat", "(", "[", "encoded_summary", ",", "attended_context", "]", ",", "dim", "=", "-", "1", ")", "\n", "req_encoded_summary", "=", "torch", ".", "cat", "(", "[", "req_encoded_summary", ",", "req_attended_context", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "tag_encoded_text", ",", "tag_mask", "=", "self", ".", "get_tag_encoded_text", "(", "embedded_text_input", ",", "mask", ")", "\n", "if", "self", ".", "context_for_tag", ":", "\n", "            ", "tag_encoded_text", "=", "torch", ".", "cat", "(", "[", "tag_encoded_text", ",", "\n", "encoded_context_summary", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "\n", "encoded_context_summary", ".", "size", "(", "0", ")", ",", "\n", "tag_encoded_text", ".", "size", "(", "1", ")", ",", "\n", "encoded_context_summary", ".", "size", "(", "1", ")", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "if", "self", ".", "attention_for_tag", ":", "\n", "            ", "tag_encoded_text", "=", "torch", ".", "cat", "(", "[", "tag_encoded_text", ",", "\n", "attended_context", ".", "unsqueeze", "(", "dim", "=", "1", ")", ".", "expand", "(", "\n", "attended_context", ".", "size", "(", "0", ")", ",", "\n", "tag_encoded_text", ".", "size", "(", "1", ")", ",", "\n", "attended_context", ".", "size", "(", "1", ")", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "intent_logits", "=", "self", ".", "intent_projection_layer", "(", "encoded_summary", ")", "\n", "req_logits", "=", "self", ".", "req_projection_layer", "(", "req_encoded_summary", ")", "\n", "# req_logits = self.req_projection_layer(encoded_summary)", "\n", "\n", "intent_probs", "=", "torch", ".", "sigmoid", "(", "intent_logits", ")", "\n", "req_probs", "=", "torch", ".", "sigmoid", "(", "req_logits", ")", "\n", "predicted_intents", "=", "(", "intent_probs", ">", "0.5", ")", ".", "long", "(", ")", "\n", "predicted_reqs", "=", "(", "req_probs", ">", "0.3", ")", ".", "long", "(", ")", "#change req threshold from 0.5 to 0.3", "\n", "sequence_logits", "=", "self", ".", "tag_projection_layer", "(", "tag_encoded_text", ")", "\n", "\n", "if", "self", ".", "crf", "is", "not", "None", ":", "\n", "            ", "best_paths", "=", "self", ".", "crf", ".", "viterbi_tags", "(", "sequence_logits", ",", "mask", ")", "\n", "# Just get the tags and ignore the score.", "\n", "predicted_tags", "=", "[", "x", "for", "x", ",", "y", "in", "best_paths", "]", "\n", "", "else", ":", "\n", "            ", "predicted_tags", "=", "self", ".", "get_predicted_tags", "(", "sequence_logits", ")", "\n", "#print('predicted_tags = ', predicted_tags)", "\n", "\n", "# output = {\"sequence_logits\": sequence_logits, \"mask\": mask, \"tags\": predicted_tags,", "\n", "# \"intent_logits\": intent_logits, \"intent_probs\": intent_probs, \"intents\": predicted_intents,", "\n", "# \"req_logits\":req_logits, \"req_probs\":req_probs,\"reqs\":predicted_reqs}", "\n", "", "output", "=", "{", "\"tokens\"", ":", "tokens", "[", "'tokens'", "]", ".", "tolist", "(", ")", ",", "\"sequence_logits\"", ":", "sequence_logits", ",", "\"mask\"", ":", "mask", ",", "\"tags\"", ":", "predicted_tags", ",", "\n", "\"intent_logits\"", ":", "intent_logits", ",", "\"intent_probs\"", ":", "intent_probs", ",", "\"intents\"", ":", "predicted_intents", ",", "\n", "\"req_logits\"", ":", "req_logits", ",", "\"req_probs\"", ":", "req_probs", ",", "\"reqs\"", ":", "predicted_reqs", "}", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "crf", "is", "not", "None", ":", "\n", "# Add negative log-likelihood as loss", "\n", "                ", "log_likelihood", "=", "self", ".", "crf", "(", "sequence_logits", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ")", "\n", "output", "[", "\"loss\"", "]", "=", "-", "log_likelihood", "\n", "\n", "# Represent viterbi tags as \"class probabilities\" that we can", "\n", "# feed into the metrics", "\n", "class_probabilities", "=", "sequence_logits", "*", "0.", "\n", "for", "i", ",", "instance_tags", "in", "enumerate", "(", "predicted_tags", ")", ":", "\n", "                    ", "for", "j", ",", "tag_id", "in", "enumerate", "(", "instance_tags", ")", ":", "\n", "                        ", "class_probabilities", "[", "i", ",", "j", ",", "tag_id", "]", "=", "1", "\n", "", "", "", "else", ":", "\n", "                ", "slot_tag_tensor", "=", "slot_tag_tensor", ".", "view", "(", "-", "1", ",", "slot_tag_tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "tag_mask_tensor", "=", "tag_mask_tensor", ".", "view", "(", "-", "1", ",", "slot_tag_tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "loss", "=", "sequence_cross_entropy_with_logits", "(", "sequence_logits", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ")", "#tag,mask", "\n", "class_probabilities", "=", "sequence_logits", "\n", "output", "[", "\"loss\"", "]", "=", "loss", "/", "self", ".", "num_slot_intents", "\n", "\n", "", "if", "self", ".", "calculate_span_f1", ":", "\n", "                 ", "self", ".", "_f1_metric", "(", "class_probabilities", ",", "slot_tag_tensor", ",", "tag_mask_tensor", ".", "float", "(", ")", ")", "#mask.float()) ", "\n", "\n", "", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"words\"", "]", "=", "[", "x", "[", "\"words\"", "]", "for", "x", "in", "metadata", "]", "\n", "\n", "", "if", "tags", "is", "not", "None", "and", "metadata", ":", "\n", "            ", "self", ".", "decode", "(", "output", ")", "\n", "self", ".", "_dai_f1_metric", "(", "output", "[", "\"dialog_act\"", "]", ",", "[", "x", "[", "\"dialog_act\"", "]", "for", "x", "in", "metadata", "]", ")", "\n", "rewards", "=", "self", ".", "get_rewards", "(", "output", "[", "\"dialog_act\"", "]", ",", "[", "x", "[", "\"dialog_act\"", "]", "for", "x", "in", "metadata", "]", ")", "if", "self", ".", "rl", "else", "None", "\n", "\n", "", "if", "intents", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"loss\"", "]", "+=", "torch", ".", "mean", "(", "self", ".", "intent_loss", "(", "intent_logits", ",", "intents", ".", "float", "(", ")", ")", ")", "\n", "self", ".", "_intent_f1_metric", "(", "predicted_intents", ",", "intents", ")", "\n", "\n", "", "if", "full_reqs", "is", "not", "None", ":", "\n", "            ", "req_mask", ",", "req_label", "=", "self", ".", "get_req_mask_and_label", "(", "full_reqs", ")", "\n", "if", "True", "in", "req_mask", ":", "\n", "\n", "                ", "active_req_logits", "=", "req_logits", "[", "req_mask", "]", "\n", "active_req_label", "=", "req_label", "[", "req_mask", "]", "\n", "output", "[", "\"loss\"", "]", "+=", "torch", ".", "mean", "(", "self", ".", "req_loss", "(", "active_req_logits", ",", "active_req_label", ")", ")", "\n", "#self._req_f1_metric(predicted_reqs, reqs)", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_req_mask_and_label": [[476, 493], ["torch.zeros", "torch.zeros", "enumerate", "torch.zeros.cuda", "torch.zeros.cuda", "torch.max", "torch.argmax().item", "model.MILU.vocab.get_token_from_index", "model.MILU.split", "model.MILU.vocab.get_token_index", "model.MILU.vocab.get_token_index", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "get_req_mask_and_label", "(", "self", ",", "full_reqs", ")", ":", "\n", "# full_reqs : bc, num_req_full", "\n", "# print('full_reqs.shape = ', full_reqs.shape)", "\n", "        ", "bc", "=", "full_reqs", ".", "shape", "[", "0", "]", "\n", "mask", "=", "torch", ".", "zeros", "(", "(", "bc", "*", "self", ".", "num_req_intents", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "label", "=", "torch", ".", "zeros", "(", "(", "bc", "*", "self", ".", "num_req_intents", ",", "self", ".", "num_req_slots", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "for", "i", ",", "req", "in", "enumerate", "(", "full_reqs", ")", ":", "\n", "            ", "if", "torch", ".", "max", "(", "req", ")", "!=", "0", ":", "\n", "                ", "triple_id", "=", "torch", ".", "argmax", "(", "req", ")", ".", "item", "(", ")", "\n", "triple", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "triple_id", ",", "namespace", "=", "self", ".", "req_full_label_namespace", ")", "\n", "domain", ",", "intent", ",", "slot", "=", "triple", ".", "split", "(", "'-'", ")", "\n", "req_intent_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "domain", "+", "'-'", "+", "intent", ",", "namespace", "=", "self", ".", "req_intent_label_namespace", ")", "\n", "req_slot_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "slot", ",", "namespace", "=", "self", ".", "req_slot_label_namespace", ")", "\n", "mask", "[", "i", "*", "self", ".", "num_req_intents", "+", "req_intent_id", "]", "=", "torch", ".", "FloatTensor", "(", "[", "1", "]", ")", "\n", "label", "[", "i", "*", "self", ".", "num_req_intents", "+", "req_intent_id", ",", "req_slot_id", "]", "=", "torch", ".", "FloatTensor", "(", "[", "1", "]", ")", "\n", "", "", "mask", "=", "mask", "==", "1", "\n", "return", "mask", ".", "cuda", "(", ")", ",", "label", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_predicted_tags": [[495, 512], ["all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.argmax", "all_tags.append", "all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "range", "all_predictions.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "get_predicted_tags", "(", "self", ",", "sequence_logits", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Does a simple position-wise argmax over each token, converts indices to string labels, and\n        adds a ``\"tags\"`` key to the dictionary with the result.\n        \"\"\"", "\n", "all_predictions", "=", "sequence_logits", "\n", "all_predictions", "=", "all_predictions", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "all_predictions", ".", "ndim", "==", "3", ":", "#\u53bb\u6389prefix", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "[", "i", "]", "[", "2", ":", "]", "for", "i", "in", "range", "(", "all_predictions", ".", "shape", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "[", "2", ":", "]", "]", "\n", "", "all_tags", "=", "[", "]", "\n", "for", "predictions", "in", "predictions_list", ":", "\n", "            ", "tags", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "-", "1", ")", "\n", "all_tags", ".", "append", "(", "tags", ")", "\n", "\n", "", "return", "all_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.decode": [[513, 602], ["range", "len", "enumerate", "enumerate", "output_dict[].append", "model.MILU.vocab.get_token_from_index", "model.MILU.vocab.get_token_from_index", "model.MILU.split", "model.MILU.vocab.get_token_from_index", "len", "allennlp.data.dataset_readers.dataset_utils.span_utils.bio_tags_to_spans", "model.MILU.vocab.get_token_from_index", "instance_intents.nonzero().tolist", "enumerate", "dialog_act[].append", "instance_intents.nonzero", "model.MILU.vocab.get_token_from_index", "dialog_act[].append", "dialog_act[].append"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Converts the tag ids to the actual tags.\n        ``output_dict[\"tags\"]`` is a list of lists of tag_ids,\n        so we use an ugly nested list comprehension.\n        \"\"\"", "\n", "\n", "output_dict", "[", "\"tags\"", "]", "=", "[", "\n", "[", "self", ".", "vocab", ".", "get_token_from_index", "(", "tag", ",", "namespace", "=", "self", ".", "slot_sequence_label_namesapce", ")", "\n", "for", "tag", "in", "instance_tags", "]", "\n", "for", "instance_tags", "in", "output_dict", "[", "\"tags\"", "]", "\n", "]", "\n", "#print('predict tags = ',output_dict['tags'])", "\n", "\n", "output_dict", "[", "\"intents\"", "]", "=", "[", "\n", "[", "self", ".", "vocab", ".", "get_token_from_index", "(", "intent", "[", "0", "]", ",", "namespace", "=", "self", ".", "intent_label_namespace", ")", "\n", "for", "intent", "in", "instance_intents", ".", "nonzero", "(", ")", ".", "tolist", "(", ")", "]", "\n", "for", "instance_intents", "in", "output_dict", "[", "\"intents\"", "]", "\n", "]", "\n", "#print(\"predict intents = \", output_dict['intents'])", "\n", "\n", "# output_dict[\"reqs\"] = [", "\n", "#         [self.vocab.get_token_from_index(req[0], namespace=self.req_label_namespace)", "\n", "#     for req in instance_reqs.nonzero().tolist()]", "\n", "#     for instance_reqs in output_dict['reqs']", "\n", "# ]", "\n", "#print('predict reqs = ',output_dict['reqs'])", "\n", "\n", "#prediction: dialog act", "\n", "#label: metadata", "\n", "output_dict", "[", "\"dialog_act\"", "]", "=", "[", "]", "\n", "batch_size", "=", "len", "(", "output_dict", "[", "\"tags\"", "]", ")", "//", "self", ".", "num_slot_intents", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "dialog_act", "=", "{", "}", "\n", "for", "intent", "in", "output_dict", "[", "\"intents\"", "]", "[", "i", "]", ":", "#add general", "\n", "                ", "domain", ",", "real_intent", "=", "intent", ".", "split", "(", "'-'", ")", "\n", "if", "domain", "==", "'general'", ":", "\n", "                    ", "pair", "=", "[", "'none'", ",", "'none'", "]", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                        ", "dialog_act", "[", "intent", "]", "=", "[", "pair", "]", "\n", "", "else", ":", "\n", "                        ", "dialog_act", "[", "intent", "]", ".", "append", "(", "pair", ")", "\n", "\n", "", "", "", "for", "j", ",", "req", "in", "enumerate", "(", "output_dict", "[", "'reqs'", "]", "[", "i", "*", "self", ".", "num_req_intents", ":", "(", "i", "+", "1", ")", "*", "self", ".", "num_req_intents", "]", ")", ":", "#add requestable", "\n", "                ", "intent", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "j", ",", "self", ".", "req_intent_label_namespace", ")", "\n", "#domain, intent = prefix.split('-')", "\n", "if", "intent", "in", "output_dict", "[", "'intents'", "]", "[", "i", "]", ":", "\n", "                    ", "for", "k", ",", "pick_flag", "in", "enumerate", "(", "req", ")", ":", "\n", "                        ", "if", "pick_flag", ":", "\n", "                            ", "slot", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "k", ",", "self", ".", "req_slot_label_namespace", ")", "\n", "pair", "=", "[", "slot", ",", "'?'", "]", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                                ", "dialog_act", "[", "intent", "]", "=", "[", "pair", "]", "\n", "", "else", ":", "\n", "                                ", "dialog_act", "[", "intent", "]", ".", "append", "(", "pair", ")", "\n", "\n", "# domain, intent, slot = req.split('-')", "\n", "# temp_intent = domain + '-' + intent", "\n", "# if temp_intent in output_dict['intents'][i]:", "\n", "#     pair = [slot, None]", "\n", "#     if temp_intent not in dialog_act:", "\n", "#         dialog_act[temp_intent] = [pair]", "\n", "#     else:", "\n", "#         dialog_act[temp_intent].append(pair)", "\n", "\n", "", "", "", "", "", "for", "j", ",", "tags", "in", "enumerate", "(", "output_dict", "[", "\"tags\"", "]", "[", "i", "*", "self", ".", "num_slot_intents", ":", "(", "i", "+", "1", ")", "*", "self", ".", "num_slot_intents", "]", ")", ":", "# add informable", "\n", "                ", "seq_len", "=", "len", "(", "output_dict", "[", "\"words\"", "]", "[", "i", "]", ")", "\n", "spans", "=", "bio_tags_to_spans", "(", "tags", "[", ":", "seq_len", "]", ")", "\n", "intent", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "j", ",", "self", ".", "slot_intent_label_namespace", ")", "\n", "for", "span", "in", "spans", ":", "\n", "                    ", "if", "intent", "in", "output_dict", "[", "\"intents\"", "]", "[", "i", "]", ":", "\n", "                        ", "slot", "=", "span", "[", "0", "]", "\n", "value", "=", "\" \"", ".", "join", "(", "output_dict", "[", "\"words\"", "]", "[", "i", "]", "[", "span", "[", "1", "]", "[", "0", "]", ":", "span", "[", "1", "]", "[", "1", "]", "+", "1", "]", ")", "\n", "if", "intent", "not", "in", "dialog_act", ":", "\n", "                            ", "dialog_act", "[", "intent", "]", "=", "[", "[", "slot", ",", "value", "]", "]", "\n", "", "else", ":", "\n", "                            ", "dialog_act", "[", "intent", "]", ".", "append", "(", "[", "slot", ",", "value", "]", ")", "\n", "\n", "", "", "", "", "output_dict", "[", "\"dialog_act\"", "]", ".", "append", "(", "dialog_act", ")", "\n", "\n", "# print(' len of tokens = {}, bc = {}'.format(len(output_dict['tokens']), batch_size))", "\n", "# for i in range(batch_size):", "\n", "#     print('text = ',' '.join([self.vocab.get_token_from_index(j,\"tokens\") for j in output_dict['tokens'][i] if self.vocab.get_token_from_index(j,\"tokens\") != '@@PADDING@@']))", "\n", "#     print('intents = ', output_dict['intents'][i])", "\n", "#     print(\"reqs = \",output_dict['reqs'][i])", "\n", "#     print('dialog act = ', output_dict['dialog_act'][i])", "\n", "#     print('='*100)", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.model.MILU.get_metrics": [[604, 614], ["model.MILU._intent_f1_metric.get_metric", "metrics_to_return.update", "metrics_to_return.update", "model.MILU._f1_metric.get_metric", "metrics_to_return.update", "model.MILU._dai_f1_metric.get_metric", "model.MILU.items", "model.MILU.items"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics_to_return", "=", "{", "}", "\n", "intent_f1_dict", "=", "self", ".", "_intent_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "metrics_to_return", ".", "update", "(", "{", "\"int_\"", "+", "x", "[", ":", "1", "]", ":", "y", "for", "x", ",", "y", "in", "intent_f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", ")", "\n", "if", "self", ".", "calculate_span_f1", ":", "\n", "            ", "f1_dict", "=", "self", ".", "_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "metrics_to_return", ".", "update", "(", "{", "\"tag_\"", "+", "x", "[", ":", "1", "]", ":", "y", "for", "x", ",", "y", "in", "f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", ")", "\n", "", "metrics_to_return", ".", "update", "(", "self", ".", "_dai_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", ")", "\n", "return", "metrics_to_return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.__init__": [[16, 40], ["vocabulary.get_index_to_token_vocabulary", "print", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "namespace", ":", "str", "=", "\"intent_labels\"", ",", "\n", "ignore_classes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "coarse", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        vocabulary : ``Vocabulary``, required.\n            A vocabulary containing the label namespace.\n        namespace : str, required.\n            The vocabulary namespace for labels.\n        ignore_classes : List[str], optional.\n            Labels which will be ignored when computing metrics.\n        \"\"\"", "\n", "self", ".", "_label_vocabulary", "=", "vocabulary", ".", "get_index_to_token_vocabulary", "(", "namespace", ")", "\n", "print", "(", "'self._label_vocabulary'", ")", "\n", "self", ".", "_ignore_classes", ":", "List", "[", "str", "]", "=", "ignore_classes", "or", "[", "]", "\n", "self", ".", "_coarse", "=", "coarse", "\n", "\n", "# These will hold per label span counts.", "\n", "self", ".", "_true_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.__call__": [[41, 83], ["multilabel_f1_measure.MultiLabelF1Measure.unwrap_to_tensors", "torch.ones_like", "predictions.sum", "gold_labels.size", "range", "range", "gold_label.size"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        predictions : ``torch.Tensor``, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : ``torch.Tensor``, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the ``predictions`` tensor without the ``num_classes`` dimension.\n        mask: ``torch.Tensor``, optional (default = None).\n            A masking tensor the same size as ``gold_labels``.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "gold_labels", ")", "\n", "\n", "", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "predictions", ",", "gold_labels", ",", "mask", ")", "\n", "\n", "if", "self", ".", "_coarse", ":", "\n", "            ", "num_positives", "=", "predictions", ".", "sum", "(", ")", "\n", "num_false_positives", "=", "(", "(", "predictions", "-", "gold_labels", ")", ">", "0", ")", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "self", ".", "_false_positives", "[", "\"coarse_overall\"", "]", "+=", "num_false_positives", "\n", "num_true_positives", "=", "num_positives", "-", "num_false_positives", "\n", "self", ".", "_true_positives", "[", "\"coarse_overall\"", "]", "+=", "num_true_positives", "\n", "num_false_negatives", "=", "(", "(", "gold_labels", "-", "predictions", ")", ">", "0", ")", ".", "long", "(", ")", ".", "sum", "(", ")", "\n", "self", ".", "_false_negatives", "[", "\"coarse_overall\"", "]", "+=", "num_false_negatives", "\n", "", "else", ":", "\n", "# Iterate over timesteps in batch.", "\n", "            ", "batch_size", "=", "gold_labels", ".", "size", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "                ", "prediction", "=", "predictions", "[", "i", ",", ":", "]", "\n", "gold_label", "=", "gold_labels", "[", "i", ",", ":", "]", "\n", "for", "label_id", "in", "range", "(", "gold_label", ".", "size", "(", "-", "1", ")", ")", ":", "\n", "                    ", "label", "=", "self", ".", "_label_vocabulary", "[", "label_id", "]", "\n", "if", "prediction", "[", "label_id", "]", "==", "1", "and", "gold_label", "[", "label_id", "]", "==", "1", ":", "\n", "                        ", "self", ".", "_true_positives", "[", "label", "]", "+=", "1", "\n", "", "elif", "prediction", "[", "label_id", "]", "==", "1", "and", "gold_label", "[", "label_id", "]", "==", "0", ":", "\n", "                        ", "self", ".", "_false_positives", "[", "label", "]", "+=", "1", "\n", "", "elif", "prediction", "[", "label_id", "]", "==", "0", "and", "gold_label", "[", "label_id", "]", "==", "1", ":", "\n", "                        ", "self", ".", "_false_negatives", "[", "label", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.get_metric": [[85, 123], ["set", "all_labels.update", "all_labels.update", "all_labels.update", "multilabel_f1_measure.MultiLabelF1Measure._compute_metrics", "multilabel_f1_measure.MultiLabelF1Measure._true_positives.keys", "multilabel_f1_measure.MultiLabelF1Measure._false_positives.keys", "multilabel_f1_measure.MultiLabelF1Measure._false_negatives.keys", "multilabel_f1_measure.MultiLabelF1Measure._compute_metrics", "sum", "sum", "sum", "multilabel_f1_measure.MultiLabelF1Measure.reset", "multilabel_f1_measure.MultiLabelF1Measure._true_positives.values", "multilabel_f1_measure.MultiLabelF1Measure._false_positives.values", "multilabel_f1_measure.MultiLabelF1Measure._false_negatives.values"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure._compute_metrics", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure._compute_metrics", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.reset"], ["", "", "", "", "", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns\n        -------\n        A Dict per label containing following the span based metrics:\n        precision : float\n        recall : float\n        f1-measure : float\n\n        Additionally, an ``overall`` key is included, which provides the precision,\n        recall and f1-measure for all spans.\n        \"\"\"", "\n", "all_labels", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "all_labels", ".", "update", "(", "self", ".", "_true_positives", ".", "keys", "(", ")", ")", "\n", "all_labels", ".", "update", "(", "self", ".", "_false_positives", ".", "keys", "(", ")", ")", "\n", "all_labels", ".", "update", "(", "self", ".", "_false_negatives", ".", "keys", "(", ")", ")", "\n", "all_metrics", "=", "{", "}", "\n", "for", "label", "in", "all_labels", ":", "\n", "            ", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "self", ".", "_true_positives", "[", "label", "]", ",", "\n", "self", ".", "_false_positives", "[", "label", "]", ",", "\n", "self", ".", "_false_negatives", "[", "label", "]", ")", "\n", "precision_key", "=", "\"precision\"", "+", "\"-\"", "+", "label", "\n", "recall_key", "=", "\"recall\"", "+", "\"-\"", "+", "label", "\n", "f1_key", "=", "\"f1-measure\"", "+", "\"-\"", "+", "label", "\n", "all_metrics", "[", "precision_key", "]", "=", "precision", "\n", "all_metrics", "[", "recall_key", "]", "=", "recall", "\n", "all_metrics", "[", "f1_key", "]", "=", "f1_measure", "\n", "\n", "# Compute the precision, recall and f1 for all spans jointly.", "\n", "", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "sum", "(", "self", ".", "_true_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_negatives", ".", "values", "(", ")", ")", ")", "\n", "all_metrics", "[", "\"precision-overall\"", "]", "=", "precision", "\n", "all_metrics", "[", "\"recall-overall\"", "]", "=", "recall", "\n", "all_metrics", "[", "\"f1-measure-overall\"", "]", "=", "f1_measure", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure._compute_metrics": [[124, 130], ["float", "float", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_metrics", "(", "true_positives", ":", "int", ",", "false_positives", ":", "int", ",", "false_negatives", ":", "int", ")", ":", "\n", "        ", "precision", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_positives", ")", "if", "true_positives", "+", "false_positives", ">", "0", "else", "0", "\n", "recall", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_negatives", ")", "if", "true_positives", "+", "false_negatives", ">", "0", "else", "0", "\n", "f1_measure", "=", "2.", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", ")", "if", "precision", "+", "recall", ">", "0", "else", "0", "\n", "return", "precision", ",", "recall", ",", "f1_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.multilabel_f1_measure.MultiLabelF1Measure.reset": [[131, 135], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_true_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", "=", "defaultdict", "(", "int", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.iterator.MiluIterator.transfer_instance_list": [[24, 59], ["enumerate", "iter", "iterator.MiluIterator.vocab.get_vocab_size", "enumerate", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "iter", "range", "numpy.array", "numpy.array", "range", "range", "transfer_tags[].append", "tag.split", "len", "iterator.MiluIterator.vocab.get_token_index", "len"], "methods", ["None"], ["def", "transfer_instance_list", "(", "self", ",", "instance_list", ")", ":", "\n", "        ", "key_list", "=", "[", "'tokens'", ",", "'context_tokens'", ",", "'tags'", ",", "'slot_tags'", ",", "'intents'", ",", "'slot_intents'", "]", "\n", "result", "=", "[", "]", "\n", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "iter", "(", "instance_list", ")", ")", ":", "\n", "                ", "tags", "=", "instance", ".", "fields", "[", "'tags'", "]", "[", "'tags'", "]", "\n", "slot_tags", "=", "instance", ".", "fields", "[", "'slot_tags'", "]", "\n", "slot_intent_dim", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "'slot_intent_labels'", ")", "\n", "transfer_tags", "=", "[", "[", "0", ",", "0", "]", "for", "i", "in", "range", "(", "slot_intent_dim", ")", "]", "\n", "mask", "=", "[", "[", "0", "]", "*", "(", "2", "+", "len", "(", "tags", ")", ")", "for", "i", "in", "range", "(", "slot_intent_dim", ")", "]", "\n", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "iter", "(", "tags", ")", ")", ":", "\n", "                    ", "for", "k", "in", "range", "(", "slot_intent_dim", ")", ":", "\n", "                        ", "transfer_tags", "[", "k", "]", ".", "append", "(", "self", ".", "vocab", ".", "get_token_index", "(", "'O'", ",", "'slot_tags'", ")", ")", "\n", "", "if", "tag", "!=", "'O'", ":", "\n", "                        ", "intent", ",", "value", "=", "tag", ".", "split", "(", "'+'", ")", "\n", "prefix", "=", "intent", "[", "0", "]", "\n", "intent", "=", "intent", "[", "2", ":", "]", "\n", "if", "intent", "in", "self", ".", "vocab", ".", "_token_to_index", "[", "'slot_intent_labels'", "]", ":", "\n", "                            ", "intent_id", "=", "self", ".", "vocab", ".", "_token_to_index", "[", "'slot_intent_labels'", "]", "[", "intent", "]", "\n", "slot_tag", "=", "prefix", "+", "'-'", "+", "value", "\n", "if", "slot_tag", "in", "self", ".", "vocab", ".", "_token_to_index", "[", "'slot_tags'", "]", ":", "\n", "                                ", "slot_tag_id", "=", "self", ".", "vocab", ".", "_token_to_index", "[", "'slot_tags'", "]", "[", "slot_tag", "]", "\n", "transfer_tags", "[", "intent_id", "]", "[", "-", "1", "]", "=", "slot_tag_id", "\n", "# else:", "\n", "#     print(\"slot_tag = {}, \u4e0d\u5728\u8bcd\u8868\u4e2d\".format(slot_tag))", "\n", "#     print(\"tags = \", tags) ", "\n", "#     print(\"tokens = \", instance.fields['tokens'])", "\n", "#     print('='*100)", "\n", "", "mask", "[", "intent_id", "]", "[", "2", ":", "]", "=", "[", "1", "]", "*", "len", "(", "tags", ")", "\n", "\n", "", "", "", "instance_list", "[", "idx", "]", ".", "fields", "[", "'slot_tag_tensor'", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "transfer_tags", ")", ")", "\n", "instance_list", "[", "idx", "]", ".", "fields", "[", "'tag_mask_tensor'", "]", "=", "ArrayField", "(", "np", ".", "array", "(", "mask", ")", ")", "\n", "\n", "", "return", "instance_list", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.iterator.MiluIterator._create_batches": [[61, 76], ["iter.MiluIterator._memory_sized_lists", "iter.MiluIterator.transfer_instance_list", "iter", "collections.deque", "allennlp.common.util.lazy_groups_of", "random.shuffle", "iter.MiluIterator._ensure_batch_is_sufficiently_small", "allennlp.data.dataset.Batch", "allennlp.data.dataset.Batch"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.iterator.MiluIterator.transfer_instance_list"], ["", "def", "_create_batches", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "# First break the dataset into memory-sized lists:", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "instance_list", ")", "\n", "", "self", ".", "transfer_instance_list", "(", "instance_list", ")", "\n", "iterator", "=", "iter", "(", "instance_list", ")", "\n", "excess", ":", "Deque", "[", "Instance", "]", "=", "deque", "(", ")", "\n", "# Then break each memory-sized list into batches.", "\n", "for", "batch_instances", "in", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_batch_size", ")", ":", "\n", "                ", "for", "possibly_smaller_batches", "in", "self", ".", "_ensure_batch_is_sufficiently_small", "(", "batch_instances", ",", "excess", ")", ":", "\n", "                    ", "batch", "=", "Batch", "(", "possibly_smaller_batches", ")", "\n", "yield", "batch", "\n", "", "", "if", "excess", ":", "\n", "                ", "yield", "Batch", "(", "excess", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader_frames.MILUDatasetReader.__init__": [[46, 60], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "print", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "context_size", ":", "int", "=", "0", ",", "\n", "agent", ":", "str", "=", "None", ",", "\n", "random_context_size", ":", "bool", "=", "True", ",", "\n", "token_delimiter", ":", "str", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "print", "(", "'contruct milu dataset reader'", ")", "\n", "self", ".", "_context_size", "=", "context_size", "\n", "self", ".", "_agent", "=", "agent", "\n", "self", ".", "_random_context_size", "=", "random_context_size", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_token_delimiter", "=", "token_delimiter", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader_frames.MILUDatasetReader._read": [[61, 182], ["LAUG.util.file_util.cached_path", "LAUG.util.file_util.cached_path.endswith", "logger.info", "json.load", "print", "zipfile.ZipFile", "zipfile.ZipFile.open", "open", "enumerate", "turn[].lower().split", "enumerate", "turn[].update", "context_tokens_list.append", "os.path.basename", "context_tokens_list.append", "context_tokens_list.append", "intents.append", "slot_intents.append", "dialog_act[].append", "enumerate", "turn[].pop", "turn[].keys", "print", "print", "print", "random.randint", "allennlp.data.tokenizers.Token", "dataset_reader_frames.MILUDatasetReader.text_to_instance", "turn[].lower", "tags.append", "slot_tags.append", "dacts.split", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "turn[].lower().split", "turn[].lower().split", "span[].split", "tags.append", "slot_tags.append", "span[].split", "tags.append", "slot_tags.append", "intents.append", "processed_da.append", "intents.append", "req_intents.append", "req_slots.append", "reqs.append", "processed_da.append", "turn[].lower", "turn[].lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "if", "file_path", ".", "endswith", "(", "\"zip\"", ")", ":", "\n", "            ", "print", "(", "\"file_path = \"", ",", "file_path", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "file_path", ",", "\"r\"", ")", "\n", "data_file", "=", "archive", ".", "open", "(", "os", ".", "path", ".", "basename", "(", "file_path", ")", "[", ":", "-", "4", "]", ")", "\n", "", "else", ":", "\n", "            ", "data_file", "=", "open", "(", "file_path", ",", "\"r\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "\n", "dialogs", "=", "json", ".", "load", "(", "data_file", ")", "\n", "\n", "for", "dial_name", "in", "dialogs", ":", "\n", "            ", "dialog", "=", "dialogs", "[", "dial_name", "]", "[", "\"log\"", "]", "\n", "context_tokens_list", "=", "[", "]", "\n", "for", "i", ",", "turn", "in", "enumerate", "(", "dialog", ")", ":", "\n", "                ", "if", "self", ".", "_agent", "and", "self", ".", "_agent", "==", "\"user\"", "and", "i", "%", "2", "==", "1", ":", "\n", "                    ", "context_tokens_list", ".", "append", "(", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "+", "[", "\"SENT_END\"", "]", ")", "\n", "continue", "\n", "", "if", "self", ".", "_agent", "and", "self", ".", "_agent", "==", "\"system\"", "and", "i", "%", "2", "==", "0", ":", "\n", "                    ", "context_tokens_list", ".", "append", "(", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "+", "[", "\"SENT_END\"", "]", ")", "\n", "continue", "\n", "", "if", "not", "turn", "[", "\"dialog_act\"", "]", ":", "\n", "                    ", "continue", "\n", "\n", "", "tokens", "=", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "slot_intents", "=", "[", "]", "\n", "intents", "=", "[", "]", "\n", "dialog_act", "=", "{", "}", "\n", "for", "dacts", "in", "turn", "[", "\"span_info\"", "]", ":", "#domain-intent ,slot,value,begin_id, end_id", "\n", "                    ", "intents", ".", "append", "(", "dacts", "[", "0", "]", ")", "\n", "slot_intents", ".", "append", "(", "dacts", "[", "0", "]", ")", "\n", "\n", "if", "dacts", "[", "0", "]", "not", "in", "dialog_act", ":", "#domain-intent", "\n", "                        ", "dialog_act", "[", "dacts", "[", "0", "]", "]", "=", "[", "]", "\n", "", "dialog_act", "[", "dacts", "[", "0", "]", "]", ".", "append", "(", "[", "dacts", "[", "1", "]", ",", "\" \"", ".", "join", "(", "tokens", "[", "dacts", "[", "3", "]", ":", "dacts", "[", "4", "]", "+", "1", "]", ")", "]", ")", "\n", "\n", "", "spans", "=", "turn", "[", "\"span_info\"", "]", "\n", "tags", "=", "[", "]", "\n", "slot_tags", "=", "[", "]", "\n", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                    ", "for", "span", "in", "spans", ":", "\n", "                        ", "if", "i", "==", "span", "[", "3", "]", ":", "\n", "                            ", "domain", ",", "intent", "=", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "tags", ".", "append", "(", "\"B-\"", "+", "domain", "+", "'-'", "+", "intent", "+", "\"+\"", "+", "span", "[", "1", "]", ")", "#\u4fee\u6539: B/I-domain-intent-slot", "\n", "slot_tags", ".", "append", "(", "\"B-\"", "+", "span", "[", "1", "]", ")", "\n", "break", "\n", "", "if", "span", "[", "3", "]", "<", "i", "<=", "span", "[", "4", "]", ":", "\n", "                            ", "domain", ",", "intent", "=", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "tags", ".", "append", "(", "\"I-\"", "+", "domain", "+", "'-'", "+", "intent", "+", "\"+\"", "+", "span", "[", "1", "]", ")", "#\u4fee\u6539: B/I-domain-intent-slot", "\n", "slot_tags", ".", "append", "(", "\"I-\"", "+", "span", "[", "1", "]", ")", "\n", "break", "\n", "", "", "else", ":", "\n", "                        ", "tags", ".", "append", "(", "\"O\"", ")", "\n", "slot_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "", "req_intents", "=", "[", "]", "\n", "req_slots", "=", "[", "]", "\n", "reqs", "=", "[", "]", "\n", "# general_intents = []", "\n", "# intents = []", "\n", "# slot_intents = []", "\n", "# intent_cnt = {}", "\n", "for", "dacts", "in", "turn", "[", "\"dialog_act\"", "]", ":", "# dacts: 'domain-intent': [[name, value], ...]", "\n", "                    ", "processed_da", "=", "[", "]", "\n", "for", "i", ",", "dact", "in", "enumerate", "(", "turn", "[", "\"dialog_act\"", "]", "[", "dacts", "]", ")", ":", "# dact: [name, value]", "\n", "                        ", "temp_domain", ",", "temp_intent", "=", "dacts", ".", "split", "(", "'-'", ")", "\n", "if", "temp_intent", "in", "GENERAL_TYPE", "and", "dacts", "not", "in", "intents", ":", "\n", "                            ", "intents", ".", "append", "(", "dacts", ")", "\n", "processed_da", ".", "append", "(", "dact", ")", "\n", "", "elif", "temp_intent", "in", "REQUESTABLE_TYPE", "and", "dacts", "not", "in", "intents", ":", "\n", "                            ", "intents", ".", "append", "(", "dacts", ")", "\n", "req_intents", ".", "append", "(", "dacts", ")", "\n", "req_slots", ".", "append", "(", "dact", "[", "0", "]", ")", "\n", "reqs", ".", "append", "(", "dacts", "+", "'-'", "+", "dact", "[", "0", "]", ")", "# domain - intent - slot", "\n", "processed_da", ".", "append", "(", "dact", ")", "\n", "\n", "# elif temp_intent == 'Inform':", "\n", "#     slot_intents.append(dacts.split('-')[0]+\"-Inform\")", "\n", "#     slot = dact[0] ", "\n", "#     temp_key = dacts + '-' + slot", "\n", "#     if temp_key not in intent_cnt.keys(): ", "\n", "#         intent_cnt[temp_key] = 0", "\n", "\n", "#     if dacts in dialog_act:", "\n", "#         possible_value = [s[1] for s in dialog_act[dacts] if s[0].lower() == slot.lower()] #\u76f8\u540c\u7684slot (\u4e0d\u8003\u8651\u5927\u5c0f\u5199: parking / Parking)", "\n", "#         if len(possible_value): ", "\n", "#             if dacts not in intents: ", "\n", "#                 assert(intent_cnt[temp_key] == 0)", "\n", "#                 intents.append(dacts)", "\n", "\n", "#             processed_da.append([slot, possible_value[intent_cnt[temp_key] if intent_cnt[temp_key] < len(possible_value) else -1]]) #\u6309\u987a\u5e8f\u66ff\u6362\u3000\u5982\u679c\u4e0d\u8db3\u3000\u5c31\u9009\u6700\u540e\u4e00\u4e2a)", "\n", "#             intent_cnt[temp_key] += 1 ", "\n", "\n", "", "", "turn", "[", "'dialog_act'", "]", "[", "dacts", "]", "=", "processed_da", "\n", "\n", "", "negative_keys", "=", "[", "key", "for", "key", "in", "turn", "[", "'dialog_act'", "]", "if", "turn", "[", "'dialog_act'", "]", "[", "key", "]", "==", "[", "]", "]", "\n", "[", "turn", "[", "'dialog_act'", "]", ".", "pop", "(", "key", ")", "for", "key", "in", "negative_keys", "]", "\n", "# print('da label = ', turn['dialog_act'])", "\n", "# print('span label = ', dialog_act)", "\n", "turn", "[", "'dialog_act'", "]", ".", "update", "(", "dialog_act", ")", "#add Informable da", "\n", "if", "turn", "[", "'dialog_act'", "]", ".", "keys", "(", ")", "==", "[", "]", ":", "\n", "                    ", "print", "(", "\"text = \"", ",", "turn", "[", "'text'", "]", ")", "\n", "print", "(", "'overall label = '", ",", "turn", "[", "'dialog_act'", "]", ")", "\n", "print", "(", "'='", "*", "100", ")", "\n", "# print('overall label = ', turn['dialog_act'])", "\n", "# print('='*100)", "\n", "", "num_context", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "_context_size", ")", "if", "self", ".", "_random_context_size", "else", "self", ".", "_context_size", "\n", "if", "len", "(", "context_tokens_list", ")", ">", "0", "and", "num_context", ">", "0", ":", "\n", "                    ", "wrapped_context_tokens", "=", "[", "Token", "(", "token", ")", "for", "context_tokens", "in", "context_tokens_list", "[", "-", "num_context", ":", "]", "for", "token", "in", "context_tokens", "]", "\n", "", "else", ":", "\n", "                    ", "wrapped_context_tokens", "=", "[", "Token", "(", "\"SENT_END\"", ")", "]", "\n", "", "wrapped_tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "tokens", "]", "\n", "context_tokens_list", ".", "append", "(", "tokens", "+", "[", "\"SENT_END\"", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "wrapped_context_tokens", ",", "wrapped_tokens", ",", "tags", ",", "slot_tags", ",", "intents", ",", "slot_intents", ",", "turn", "[", "'dialog_act'", "]", ",", "req_intents", ",", "req_slots", ",", "reqs", ")", "#dialog_act)", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader_frames.MILUDatasetReader.text_to_instance": [[183, 210], ["allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.MetadataField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField"], "methods", ["None"], ["", "", "", "def", "text_to_instance", "(", "self", ",", "context_tokens", ":", "List", "[", "Token", "]", ",", "tokens", ":", "List", "[", "Token", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", ",", "slot_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "intents", ":", "List", "[", "str", "]", "=", "None", ",", "slot_intents", ":", "List", "[", "str", "]", "=", "None", ",", "dialog_act", ":", "Dict", "[", "str", ",", "Any", "]", "=", "None", ",", "req_intents", "=", "None", ",", "req_slots", "=", "None", ",", "reqs", "=", "None", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "# pylint: disable=arguments-differ", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"context_tokens\"", "]", "=", "TextField", "(", "context_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"tokens\"", "]", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "MetadataField", "(", "{", "\"tags\"", ":", "tags", "}", ")", "#SequenceLabelField(tags, fields[\"tokens\"],label_namespace=\"tags\")", "\n", "fields", "[", "\"slot_tags\"", "]", "=", "SequenceLabelField", "(", "slot_tags", ",", "fields", "[", "\"tokens\"", "]", ",", "label_namespace", "=", "'slot_tags'", ")", "#\u9700\u8981slot_tags\u7684vocab", "\n", "", "if", "intents", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"intents\"", "]", "=", "MultiLabelField", "(", "intents", ",", "label_namespace", "=", "\"intent_labels\"", ")", "\n", "fields", "[", "\"slot_intents\"", "]", "=", "MultiLabelField", "(", "slot_intents", ",", "label_namespace", "=", "\"slot_intent_labels\"", ")", "\n", "fields", "[", "'req_intents'", "]", "=", "MultiLabelField", "(", "req_intents", ",", "label_namespace", "=", "\"req_intent_labels\"", ")", "\n", "", "if", "req_slots", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'reqs'", "]", "=", "MultiLabelField", "(", "req_slots", ",", "label_namespace", "=", "'req_slot_labels'", ")", "\n", "fields", "[", "'full_reqs'", "]", "=", "MultiLabelField", "(", "reqs", ",", "label_namespace", "=", "'req_full_labels'", ")", "\n", "", "if", "dialog_act", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", ",", "\n", "'dialog_act'", ":", "dialog_act", "}", ")", "\n", "", "else", ":", "\n", "            ", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", ",", "'dialog_act'", ":", "{", "}", "}", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.__init__": [[46, 60], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "print", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__"], ["def", "__init__", "(", "self", ",", "\n", "context_size", ":", "int", "=", "0", ",", "\n", "agent", ":", "str", "=", "None", ",", "\n", "random_context_size", ":", "bool", "=", "True", ",", "\n", "token_delimiter", ":", "str", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "print", "(", "'contruct milu dataset reader'", ")", "\n", "self", ".", "_context_size", "=", "context_size", "\n", "self", ".", "_agent", "=", "agent", "\n", "self", ".", "_random_context_size", "=", "random_context_size", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_token_delimiter", "=", "token_delimiter", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader._read": [[61, 182], ["LAUG.util.file_util.cached_path", "LAUG.util.file_util.cached_path.endswith", "logger.info", "json.load", "print", "zipfile.ZipFile", "zipfile.ZipFile.open", "open", "enumerate", "turn[].lower().split", "enumerate", "turn[].update", "context_tokens_list.append", "os.path.basename", "context_tokens_list.append", "context_tokens_list.append", "intents.append", "slot_intents.append", "dialog_act[].append", "enumerate", "turn[].pop", "turn[].keys", "print", "print", "print", "random.randint", "allennlp.data.tokenizers.Token", "dataset_reader.MILUDatasetReader.text_to_instance", "turn[].lower", "tags.append", "slot_tags.append", "dacts.split", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "turn[].lower().split", "turn[].lower().split", "span[].split", "tags.append", "slot_tags.append", "span[].split", "tags.append", "slot_tags.append", "intents.append", "processed_da.append", "intents.append", "req_intents.append", "req_slots.append", "reqs.append", "processed_da.append", "turn[].lower", "turn[].lower"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.util.file_util.cached_path", "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "if", "file_path", ".", "endswith", "(", "\"zip\"", ")", ":", "\n", "            ", "print", "(", "\"file_path = \"", ",", "file_path", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "file_path", ",", "\"r\"", ")", "\n", "data_file", "=", "archive", ".", "open", "(", "os", ".", "path", ".", "basename", "(", "file_path", ")", "[", ":", "-", "4", "]", ")", "\n", "", "else", ":", "\n", "            ", "data_file", "=", "open", "(", "file_path", ",", "\"r\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "\n", "dialogs", "=", "json", ".", "load", "(", "data_file", ")", "\n", "\n", "for", "dial_name", "in", "dialogs", ":", "\n", "            ", "dialog", "=", "dialogs", "[", "dial_name", "]", "[", "\"log\"", "]", "\n", "context_tokens_list", "=", "[", "]", "\n", "for", "i", ",", "turn", "in", "enumerate", "(", "dialog", ")", ":", "\n", "                ", "if", "self", ".", "_agent", "and", "self", ".", "_agent", "==", "\"user\"", "and", "i", "%", "2", "==", "1", ":", "\n", "                    ", "context_tokens_list", ".", "append", "(", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "+", "[", "\"SENT_END\"", "]", ")", "\n", "continue", "\n", "", "if", "self", ".", "_agent", "and", "self", ".", "_agent", "==", "\"system\"", "and", "i", "%", "2", "==", "0", ":", "\n", "                    ", "context_tokens_list", ".", "append", "(", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "+", "[", "\"SENT_END\"", "]", ")", "\n", "continue", "\n", "", "if", "not", "turn", "[", "\"dialog_act\"", "]", ":", "\n", "                    ", "continue", "\n", "\n", "", "tokens", "=", "turn", "[", "\"text\"", "]", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "slot_intents", "=", "[", "]", "\n", "intents", "=", "[", "]", "\n", "dialog_act", "=", "{", "}", "\n", "for", "dacts", "in", "turn", "[", "\"span_info\"", "]", ":", "#domain-intent ,slot,value,begin_id, end_id", "\n", "                    ", "intents", ".", "append", "(", "dacts", "[", "0", "]", ")", "\n", "slot_intents", ".", "append", "(", "dacts", "[", "0", "]", ")", "\n", "\n", "if", "dacts", "[", "0", "]", "not", "in", "dialog_act", ":", "#domain-intent", "\n", "                        ", "dialog_act", "[", "dacts", "[", "0", "]", "]", "=", "[", "]", "\n", "", "dialog_act", "[", "dacts", "[", "0", "]", "]", ".", "append", "(", "[", "dacts", "[", "1", "]", ",", "\" \"", ".", "join", "(", "tokens", "[", "dacts", "[", "3", "]", ":", "dacts", "[", "4", "]", "+", "1", "]", ")", "]", ")", "\n", "\n", "", "spans", "=", "turn", "[", "\"span_info\"", "]", "\n", "tags", "=", "[", "]", "\n", "slot_tags", "=", "[", "]", "\n", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                    ", "for", "span", "in", "spans", ":", "\n", "                        ", "if", "i", "==", "span", "[", "3", "]", ":", "\n", "                            ", "domain", ",", "intent", "=", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "tags", ".", "append", "(", "\"B-\"", "+", "domain", "+", "'-'", "+", "intent", "+", "\"+\"", "+", "span", "[", "1", "]", ")", "#\u4fee\u6539: B/I-domain-intent-slot", "\n", "slot_tags", ".", "append", "(", "\"B-\"", "+", "span", "[", "1", "]", ")", "\n", "break", "\n", "", "if", "span", "[", "3", "]", "<", "i", "<=", "span", "[", "4", "]", ":", "\n", "                            ", "domain", ",", "intent", "=", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "tags", ".", "append", "(", "\"I-\"", "+", "domain", "+", "'-'", "+", "intent", "+", "\"+\"", "+", "span", "[", "1", "]", ")", "#\u4fee\u6539: B/I-domain-intent-slot", "\n", "slot_tags", ".", "append", "(", "\"I-\"", "+", "span", "[", "1", "]", ")", "\n", "break", "\n", "", "", "else", ":", "\n", "                        ", "tags", ".", "append", "(", "\"O\"", ")", "\n", "slot_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "", "req_intents", "=", "[", "]", "\n", "req_slots", "=", "[", "]", "\n", "reqs", "=", "[", "]", "\n", "# general_intents = []", "\n", "# intents = []", "\n", "# slot_intents = []", "\n", "# intent_cnt = {}", "\n", "for", "dacts", "in", "turn", "[", "\"dialog_act\"", "]", ":", "# dacts: 'domain-intent': [[name, value], ...]", "\n", "                    ", "processed_da", "=", "[", "]", "\n", "for", "i", ",", "dact", "in", "enumerate", "(", "turn", "[", "\"dialog_act\"", "]", "[", "dacts", "]", ")", ":", "# dact: [name, value]", "\n", "                        ", "temp_domain", ",", "temp_intent", "=", "dacts", ".", "split", "(", "'-'", ")", "\n", "if", "temp_domain", "==", "'general'", "and", "dacts", "not", "in", "intents", ":", "\n", "                            ", "intents", ".", "append", "(", "dacts", ")", "\n", "processed_da", ".", "append", "(", "dact", ")", "\n", "", "elif", "temp_intent", "==", "'Request'", "and", "dacts", "not", "in", "intents", ":", "\n", "                            ", "intents", ".", "append", "(", "dacts", ")", "\n", "req_intents", ".", "append", "(", "dacts", ")", "\n", "req_slots", ".", "append", "(", "dact", "[", "0", "]", ")", "\n", "reqs", ".", "append", "(", "dacts", "+", "'-'", "+", "dact", "[", "0", "]", ")", "# domain - intent - slot", "\n", "processed_da", ".", "append", "(", "dact", ")", "\n", "\n", "# elif temp_intent == 'Inform':", "\n", "#     slot_intents.append(dacts.split('-')[0]+\"-Inform\")", "\n", "#     slot = dact[0] ", "\n", "#     temp_key = dacts + '-' + slot", "\n", "#     if temp_key not in intent_cnt.keys(): ", "\n", "#         intent_cnt[temp_key] = 0", "\n", "\n", "#     if dacts in dialog_act:", "\n", "#         possible_value = [s[1] for s in dialog_act[dacts] if s[0].lower() == slot.lower()] #\u76f8\u540c\u7684slot (\u4e0d\u8003\u8651\u5927\u5c0f\u5199: parking / Parking)", "\n", "#         if len(possible_value): ", "\n", "#             if dacts not in intents: ", "\n", "#                 assert(intent_cnt[temp_key] == 0)", "\n", "#                 intents.append(dacts)", "\n", "\n", "#             processed_da.append([slot, possible_value[intent_cnt[temp_key] if intent_cnt[temp_key] < len(possible_value) else -1]]) #\u6309\u987a\u5e8f\u66ff\u6362\u3000\u5982\u679c\u4e0d\u8db3\u3000\u5c31\u9009\u6700\u540e\u4e00\u4e2a)", "\n", "#             intent_cnt[temp_key] += 1 ", "\n", "\n", "", "", "turn", "[", "'dialog_act'", "]", "[", "dacts", "]", "=", "processed_da", "\n", "\n", "", "negative_keys", "=", "[", "key", "for", "key", "in", "turn", "[", "'dialog_act'", "]", "if", "turn", "[", "'dialog_act'", "]", "[", "key", "]", "==", "[", "]", "]", "\n", "[", "turn", "[", "'dialog_act'", "]", ".", "pop", "(", "key", ")", "for", "key", "in", "negative_keys", "]", "\n", "# print('da label = ', turn['dialog_act'])", "\n", "# print('span label = ', dialog_act)", "\n", "turn", "[", "'dialog_act'", "]", ".", "update", "(", "dialog_act", ")", "#add Informable da", "\n", "if", "turn", "[", "'dialog_act'", "]", ".", "keys", "(", ")", "==", "[", "]", ":", "\n", "                    ", "print", "(", "\"text = \"", ",", "turn", "[", "'text'", "]", ")", "\n", "print", "(", "'overall label = '", ",", "turn", "[", "'dialog_act'", "]", ")", "\n", "print", "(", "'='", "*", "100", ")", "\n", "# print('overall label = ', turn['dialog_act'])", "\n", "# print('='*100)", "\n", "", "num_context", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "_context_size", ")", "if", "self", ".", "_random_context_size", "else", "self", ".", "_context_size", "\n", "if", "len", "(", "context_tokens_list", ")", ">", "0", "and", "num_context", ">", "0", ":", "\n", "                    ", "wrapped_context_tokens", "=", "[", "Token", "(", "token", ")", "for", "context_tokens", "in", "context_tokens_list", "[", "-", "num_context", ":", "]", "for", "token", "in", "context_tokens", "]", "\n", "", "else", ":", "\n", "                    ", "wrapped_context_tokens", "=", "[", "Token", "(", "\"SENT_END\"", ")", "]", "\n", "", "wrapped_tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "tokens", "]", "\n", "context_tokens_list", ".", "append", "(", "tokens", "+", "[", "\"SENT_END\"", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "wrapped_context_tokens", ",", "wrapped_tokens", ",", "tags", ",", "slot_tags", ",", "intents", ",", "slot_intents", ",", "turn", "[", "'dialog_act'", "]", ",", "req_intents", ",", "req_slots", ",", "reqs", ")", "#dialog_act)", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.milu_new.dataset_reader.MILUDatasetReader.text_to_instance": [[183, 210], ["allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.MetadataField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MultiLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField"], "methods", ["None"], ["", "", "", "def", "text_to_instance", "(", "self", ",", "context_tokens", ":", "List", "[", "Token", "]", ",", "tokens", ":", "List", "[", "Token", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", ",", "slot_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "intents", ":", "List", "[", "str", "]", "=", "None", ",", "slot_intents", ":", "List", "[", "str", "]", "=", "None", ",", "dialog_act", ":", "Dict", "[", "str", ",", "Any", "]", "=", "None", ",", "req_intents", "=", "None", ",", "req_slots", "=", "None", ",", "reqs", "=", "None", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "# pylint: disable=arguments-differ", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"context_tokens\"", "]", "=", "TextField", "(", "context_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"tokens\"", "]", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "MetadataField", "(", "{", "\"tags\"", ":", "tags", "}", ")", "#SequenceLabelField(tags, fields[\"tokens\"],label_namespace=\"tags\")", "\n", "fields", "[", "\"slot_tags\"", "]", "=", "SequenceLabelField", "(", "slot_tags", ",", "fields", "[", "\"tokens\"", "]", ",", "label_namespace", "=", "'slot_tags'", ")", "#\u9700\u8981slot_tags\u7684vocab", "\n", "", "if", "intents", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"intents\"", "]", "=", "MultiLabelField", "(", "intents", ",", "label_namespace", "=", "\"intent_labels\"", ")", "\n", "fields", "[", "\"slot_intents\"", "]", "=", "MultiLabelField", "(", "slot_intents", ",", "label_namespace", "=", "\"slot_intent_labels\"", ")", "\n", "fields", "[", "'req_intents'", "]", "=", "MultiLabelField", "(", "req_intents", ",", "label_namespace", "=", "\"req_intent_labels\"", ")", "\n", "", "if", "req_slots", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'reqs'", "]", "=", "MultiLabelField", "(", "req_slots", ",", "label_namespace", "=", "'req_slot_labels'", ")", "\n", "fields", "[", "'full_reqs'", "]", "=", "MultiLabelField", "(", "reqs", ",", "label_namespace", "=", "'req_full_labels'", ")", "\n", "", "if", "dialog_act", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", ",", "\n", "'dialog_act'", ":", "dialog_act", "}", ")", "\n", "", "else", ":", "\n", "            ", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", ",", "'dialog_act'", ":", "{", "}", "}", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.nlg.nlg.NLG.generate": [[8, 19], ["None"], "methods", ["None"], ["def", "generate", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"Generate a natural language utterance conditioned on the dialog act.\n        \n        Args:\n            action (list of list):\n                The dialog action produced by dialog policy module, which is in dialog act format.\n        Returns:\n            utterance (str):\n                A natural langauge utterance.\n        \"\"\"", "\n", "return", "''", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.set_seed": [[10, 15], ["numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "seed", ",", "n_gpu", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.top_k_top_p_filtering": [[17, 46], ["min", "float", "logits.size", "torch.sort", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "torch.softmax", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "filter_value", "=", "-", "float", "(", "'Inf'", ")", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size x vocabulary size)\n            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "top_k", "=", "min", "(", "top_k", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "if", "top_k", ">", "0", ":", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "        ", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", ">", "0.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "torch", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "dim", "=", "1", ",", "index", "=", "sorted_indices", ",", "src", "=", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.sample_sequence": [[48, 91], ["torch.tensor", "context.unsqueeze().repeat.unsqueeze().repeat", "torch.no_grad", "range", "context.unsqueeze().repeat.unsqueeze", "model", "range", "decode.top_k_top_p_filtering", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.cat", "torch.tensor().view", "set", "torch.argmax().unsqueeze", "torch.multinomial", "generated[].tolist", "torch.softmax", "torch.zeros", "torch.full", "torch.tensor", "torch.argmax"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.decode.top_k_top_p_filtering", "home.repos.pwc.inspect_result.thu-coai_LAUG.Speech_Disfluency.LSTMCRF.argmax"], ["", "def", "sample_sequence", "(", "model", ",", "length", ",", "context", ",", "num_samples", "=", "1", ",", "temperature", "=", "1", ",", "top_k", "=", "0", ",", "top_p", "=", "0.0", ",", "repetition_penalty", "=", "1.0", ",", "\n", "is_xlnet", "=", "False", ",", "is_xlm_mlm", "=", "False", ",", "xlm_mask_token", "=", "None", ",", "xlm_lang", "=", "None", ",", "device", "=", "'cpu'", ")", ":", "\n", "    ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "context", "=", "context", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "num_samples", ",", "1", ")", "\n", "generated", "=", "context", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "length", ")", ":", "\n", "\n", "            ", "inputs", "=", "{", "'input_ids'", ":", "generated", "}", "\n", "if", "is_xlnet", ":", "\n", "# XLNet is a direct (predict same token, not next token) and bi-directional model by default", "\n", "# => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "zeros", "(", "(", "1", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "perm_mask", "=", "torch", ".", "zeros", "(", "(", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "perm_mask", "[", ":", ",", ":", ",", "-", "1", "]", "=", "1.0", "# Previous tokens don't see last token", "\n", "target_mapping", "=", "torch", ".", "zeros", "(", "(", "1", ",", "1", ",", "input_ids", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "target_mapping", "[", "0", ",", "0", ",", "-", "1", "]", "=", "1.0", "# predict last token", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", ",", "'perm_mask'", ":", "perm_mask", ",", "'target_mapping'", ":", "target_mapping", "}", "\n", "\n", "", "if", "is_xlm_mlm", "and", "xlm_mask_token", ":", "\n", "# XLM MLM models are direct models (predict same token, not next token)", "\n", "# => need one additional dummy token in the input (will be masked and guessed)", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "(", "generated", ",", "torch", ".", "full", "(", "(", "1", ",", "1", ")", ",", "xlm_mask_token", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ")", ",", "dim", "=", "1", ")", "\n", "inputs", "=", "{", "'input_ids'", ":", "input_ids", "}", "\n", "\n", "", "if", "xlm_lang", "is", "not", "None", ":", "\n", "                ", "inputs", "[", "\"langs\"", "]", "=", "torch", ".", "tensor", "(", "[", "xlm_lang", "]", "*", "inputs", "[", "\"input_ids\"", "]", ".", "shape", "[", "1", "]", ",", "device", "=", "device", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "", "outputs", "=", "model", "(", "**", "inputs", ")", "# Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "/", "(", "temperature", "if", "temperature", ">", "0", "else", "1.", ")", "\n", "\n", "# repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "                ", "for", "_", "in", "set", "(", "generated", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                    ", "next_token_logits", "[", "i", ",", "_", "]", "/=", "repetition_penalty", "\n", "\n", "", "", "filtered_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "if", "temperature", "==", "0", ":", "# greedy sampling:", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "next_token", "=", "torch", ".", "multinomial", "(", "torch", ".", "softmax", "(", "filtered_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", "\n", "", "generated", "=", "torch", ".", "cat", "(", "(", "generated", ",", "next_token", ")", ",", "dim", "=", "1", ")", "\n", "", "", "return", "generated", "\n", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextDataset.__init__": [[47, 82], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "logger.info", "open", "pickle.load", "open", "range", "open", "pickle.dump", "f.read", "tokenizer.convert_tokens_to_ids", "train.TextDataset.examples.append", "str", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "train.TextDataset.examples.append", "tokenizer.build_inputs_with_special_tokens", "tokenizer.tokenize", "len", "str", "line.strip"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["", "line_list", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "line_list", "[", "0", "]", "\n", "embed", "=", "line_list", "[", "1", ":", "]", "\n", "embed", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "float", "(", "num", ")", "for", "num", "in", "embed", "]", ")", ")", "\n", "word_to_ix", "[", "word", "]", "=", "i", "+", "1", "\n", "weights", ".", "append", "(", "embed", ")", "\n", "\n", "", "weights", "=", "torch", ".", "stack", "(", "weights", ",", "0", ")", ".", "float", "(", ")", "\n", "\n", "\n", "tag_to_ix", "=", "{", "\"O\"", ":", "0", ",", "\"F\"", ":", "1", ",", "\"R\"", ":", "2", ",", "START_TAG", ":", "3", ",", "STOP_TAG", ":", "4", "}", "\n", "\n", "model", "=", "BiLSTM_CRF", "(", "len", "(", "word_to_ix", ")", ",", "tag_to_ix", ",", "EMBEDDING_DIM", ",", "HIDDEN_DIM", ",", "weights", ")", "\n", "model", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "0.001", ")", "\n", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\t", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "0", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n", "precheck_tags", "=", "torch", ".", "tensor", "(", "[", "tag_to_ix", "[", "t", "]", "for", "t", "in", "training_data", "[", "0", "]", "[", "1", "]", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "\n", "", "ep", "=", "0", "\n", "for", "epoch", "in", "range", "(", "30", ")", ":", "\n", "\t", "n", ",", "losses", "=", "0", ",", "0.", "\n", "ep", "+=", "1", "\n", "for", "sentence", ",", "tags", "in", "progressbar", "(", "training_data", ")", ":", "\n", "\t\t", "model", ".", "zero_grad", "(", ")", "\n", "sentence_in", "=", "prepare_sequence", "(", "sentence", ",", "word_to_ix", ")", "\n", "targets", "=", "torch", ".", "tensor", "(", "[", "tag_to_ix", "[", "t", "]", "for", "t", "in", "tags", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "loss", "=", "model", ".", "neg_log_likelihood", "(", "sentence_in", ",", "targets", ")", "\n", "losses", "+=", "loss", "\n", "n", "+=", "1", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextDataset.__len__": [[83, 85], ["len"], "methods", ["None"], ["", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "'model/LSTMCRF_'", "+", "str", "(", "ep", ")", "+", "'.bin'", ")", "\n", "print", "(", "'loss:'", "+", "str", "(", "losses", "/", "n", ")", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextDataset.__getitem__": [[86, 88], ["torch.tensor"], "methods", ["None"], ["\t\t", "precheck_sent", "=", "prepare_sequence", "(", "\"okay , i like to do , weight training and cycling .\"", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "\n", "precheck_sent", "=", "prepare_sequence", "(", "training_data", "[", "1", "]", "[", "0", "]", ",", "word_to_ix", ")", "\n"]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__init__": [[90, 145], ["os.path.isfile", "os.path.split", "os.path.join", "os.path.exists", "logger.info", "logger.info", "logger.info", "open", "pickle.load", "open", "open", "pickle.dump", "line.strip.strip.strip", "line.strip.strip.lower", "code_str.strip.strip.strip", "train.TextSeqDataset.examples.append", "train.TextSeqDataset.masks.append", "train.TextSeqDataset.labels.append", "str", "len", "tokenizer.convert_tokens_to_ids", "len", "tokenizer.convert_tokens_to_ids", "len", "len", "line.strip.strip.lower().split", "line.strip.lower.split", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "line.strip.lower.split", "tokenizer.convert_tokens_to_ids", "len", "str", "line.strip.lower.split", "tokenizer.tokenize", "code_str.strip.strip.split", "len", "args.output_dir.replace", "line.strip.strip.lower", "len", "len"], "methods", ["home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize", "home.repos.pwc.inspect_result.thu-coai_LAUG.multiwoz.tokenize_util.tokenize"], ["precheck_sent", "=", "prepare_sequence", "(", "'i want to go to cambridge .'", ".", "split", "(", ")", ",", "word_to_ix", ")", "\n", "print", "(", "model", "(", "precheck_sent", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__len__": [[146, 148], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.TextSeqDataset.__getitem__": [[149, 151], ["torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.load_and_cache_examples": [[153, 156], ["train.TextSeqDataset"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed": [[158, 164], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train._rotate_checkpoints": [[166, 193], ["glob.glob", "sorted", "max", "os.path.join", "len", "logger.info", "shutil.rmtree", "ordering_and_checkpoint_path.append", "re.match", "len", "re.match.groups", "ordering_and_checkpoint_path.append", "os.path.getmtime", "int", "re.match.groups"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.mask_tokens": [[195, 216], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "tokenizer.convert_tokens_to_ids", "torch.randint", "tokenizer.get_special_tokens_mask", "torch.tensor", "torch.bernoulli().bool", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli().bool", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train": [[218, 348], ["torch.utils.data.DataLoader", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "torch.nn.parallel.DistributedDataParallel.resize_token_embeddings", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.trange", "train.set_seed", "SummaryWriter", "max", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.initialize", "len", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "len", "int", "enumerate", "SummaryWriter.close", "logger.info", "inputs.to.to", "labels.to.to", "torch.nn.parallel.DistributedDataParallel.train", "loss.mean.item", "tqdm.trange.close", "len", "ImportError", "torch.distributed.get_world_size", "torch.nn.parallel.DistributedDataParallel.", "torch.nn.parallel.DistributedDataParallel.", "loss.mean.mean", "loss.mean.backward", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "len", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "amp.scale_loss", "scaled_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "SummaryWriter.add_scalar", "SummaryWriter.add_scalar", "logger.info", "os.path.join", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "torch.save", "logger.info", "train._rotate_checkpoints", "any", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "train.evaluate", "evaluate.items", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "SummaryWriter.add_scalar", "transformers.get_linear_schedule_with_warmup.get_lr", "float", "float"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train._rotate_checkpoints", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate": [[350, 409], ["train.load_and_cache_examples", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.eval", "tqdm.tqdm", "torch.exp", "os.path.join", "os.makedirs", "max", "torch.utils.data.SequentialSampler", "torch.utils.data.distributed.DistributedSampler", "torch.nn.DataParallel", "len", "inputs.to.to", "masks.to.to", "labels.to.to", "torch.tensor", "open", "logger.info", "sorted", "os.path.exists", "torch.no_grad", "lm_loss.mean().item", "result.keys", "logger.info", "writer.write", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "str", "lm_loss.mean", "str"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.load_and_cache_examples", "home.repos.pwc.inspect_result.thu-coai_LAUG.util.allennlp_file_utils.Tqdm.tqdm"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.main": [[411, 630], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logger.warning", "train.set_seed", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "min", "model_class.from_pretrained", "model_class.from_pretrained.to", "logger.info", "ValueError", "ValueError", "os.path.exists", "os.listdir", "ValueError", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "torch.distributed.barrier", "train.load_and_cache_examples", "train.train", "logger.info", "logger.info", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "model_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained.to", "logger.info", "bool", "torch.distributed.barrier", "torch.distributed.barrier", "os.makedirs", "hasattr", "os.path.join", "list", "logging.getLogger().setLevel", "model_class.from_pretrained", "model_class.from_pretrained.to", "train.evaluate", "dict", "results.update", "torch.distributed.get_rank", "os.path.exists", "torch.cuda.is_available", "os.path.dirname", "logging.getLogger", "len", "checkpoint.split", "checkpoint.find", "checkpoint.split", "sorted", "dict.items", "glob.glob"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.set_seed", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.load_and_cache_examples", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.train", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.train.evaluate"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.tuple2dict": [[8, 23], ["[].append"], "function", ["None"], ["        ", "span_words", "=", "span", "[", "2", "]", ".", "split", "(", ")", "\n", "result", "=", "phrase_idx_utt", "(", "span_words", ",", "new_words", ")", "\n", "if", "result", "is", "not", "None", ":", "\n", "            ", "max_start", ",", "max_end", "=", "result", "\n", "new_span_info", ".", "append", "(", "[", "span", "[", "0", "]", ",", "span", "[", "1", "]", ",", "' '", ".", "join", "(", "new_words", "[", "max_start", ":", "max_end", "+", "1", "]", ")", ",", "max_start", ",", "max_end", "]", ")", "\n", "", "", "return", "new_span_info", "\n", "\n", "\n", "", "def", "span2tuple", "(", "span_info", ")", ":", "\n", "    ", "t", "=", "[", "]", "\n", "for", "span", "in", "span_info", ":", "\n", "        ", "t", ".", "append", "(", "(", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "1", "]", ",", "span", "[", "0", "]", ".", "split", "(", "'-'", ")", "[", "0", "]", ",", "span", "[", "1", "]", ",", "span", "[", "2", "]", ")", ")", "\n", "", "return", "t", "", "", ""]], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.dict2dict": [[24, 41], ["domint.split", "[].append"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.dict2seq": [[42, 76], ["s.lower"], "function", ["None"], []], "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.tuple2seq": [[77, 81], ["utils.tuple2dict", "utils.dict2seq"], "function", ["home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.tuple2dict", "home.repos.pwc.inspect_result.thu-coai_LAUG.scgpt.utils.dict2seq"], []]}