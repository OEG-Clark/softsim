{"home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Flatten.forward": [[13, 15], ["input.view", "input.size"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "input", ".", "view", "(", "input", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.UnFlatten.forward": [[18, 20], ["input.view", "input.size"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "input", ",", "size", "=", "128", ")", ":", "\n", "        ", "return", "input", ".", "view", "(", "input", ".", "size", "(", "0", ")", ",", "size", ",", "3", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.ScaledDotProductAttention.__init__": [[25, 29], ["torch.nn.Module.__init__", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["def", "__init__", "(", "self", ",", "temperature", ",", "attn_dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "attn_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.ScaledDotProductAttention.forward": [[30, 41], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "models.ScaledDotProductAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "k.transpose", "attn.masked_fill.masked_fill.masked_fill", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ",", "mask", "=", "None", ")", ":", "\n", "\n", "        ", "attn", "=", "torch", ".", "matmul", "(", "q", "/", "self", ".", "temperature", ",", "k", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "attn", "=", "attn", ".", "masked_fill", "(", "mask", "==", "0", ",", "-", "1e9", ")", "\n", "\n", "", "attn", "=", "self", ".", "dropout", "(", "F", ".", "softmax", "(", "attn", ",", "dim", "=", "-", "1", ")", ")", "\n", "output", "=", "torch", ".", "matmul", "(", "attn", ",", "v", ")", "\n", "\n", "return", "output", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Conv_2d.__init__": [[55, 61], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool2d", "torch.nn.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["\t", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "shape", "=", "3", ",", "pooling", "=", "2", ")", ":", "\n", "\t\t", "super", "(", "Conv_2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "input_channels", ",", "output_channels", ",", "shape", ",", "padding", "=", "shape", "//", "2", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "output_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "mp", "=", "nn", ".", "MaxPool2d", "(", "pooling", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Conv_2d.forward": [[62, 65], ["models.Conv_2d.mp", "models.Conv_2d.relu", "models.Conv_2d.bn", "models.Conv_2d.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\t\t", "out", "=", "self", ".", "mp", "(", "self", ".", "relu", "(", "self", ".", "bn", "(", "self", ".", "conv", "(", "x", ")", ")", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Conv_emb.__init__": [[68, 73], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["\t", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ")", ":", "\n", "\t\t", "super", "(", "Conv_emb", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "input_channels", ",", "output_channels", ",", "1", ")", "\n", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "output_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Conv_emb.forward": [[74, 77], ["models.Conv_emb.relu", "models.Conv_emb.bn", "models.Conv_emb.conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\t\t", "out", "=", "self", ".", "relu", "(", "self", ".", "bn", "(", "self", ".", "conv", "(", "x", ")", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.AudioEncoder.__init__": [[79, 100], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "models.Conv_2d", "models.Conv_2d", "models.Conv_2d", "models.Conv_2d", "models.Conv_2d", "models.Conv_2d", "models.Conv_2d", "models.Flatten", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size_w_rep", "=", "128", ")", ":", "\n", "        ", "super", "(", "AudioEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "audio_encoder", "=", "Sequential", "(", "\n", "nn", ".", "BatchNorm2d", "(", "1", ")", ",", "#256x48", "\n", "Conv_2d", "(", "1", ",", "128", ",", "pooling", "=", "2", ")", ",", "#128x24", "\n", "Conv_2d", "(", "128", ",", "128", ",", "pooling", "=", "2", ")", ",", "#64x12", "\n", "Conv_2d", "(", "128", ",", "256", ",", "pooling", "=", "2", ")", ",", "#32x6", "\n", "Conv_2d", "(", "256", ",", "256", ",", "pooling", "=", "2", ")", ",", "#16x3", "\n", "Conv_2d", "(", "256", ",", "256", ",", "pooling", "=", "(", "1", ",", "2", ")", ")", ",", "#8x3", "\n", "Conv_2d", "(", "256", ",", "256", ",", "pooling", "=", "(", "1", ",", "2", ")", ")", ",", "#4x3", "\n", "Conv_2d", "(", "256", ",", "512", ",", "pooling", "=", "2", ")", ",", "#2x1", "\n", "Flatten", "(", ")", "\n", ")", "\n", "self", ".", "fc_audio", "=", "Sequential", "(", "\n", "Linear", "(", "1024", ",", "512", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "Dropout", "(", "0.5", ")", ",", "\n", "Linear", "(", "512", ",", "size_w_rep", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "size_w_rep", ",", "eps", "=", "1e-6", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.AudioEncoder.forward": [[102, 106], ["models.AudioEncoder.audio_encoder", "models.AudioEncoder.fc_audio"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "z", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "z_d", "=", "self", ".", "fc_audio", "(", "z", ")", "\n", "return", "z", ",", "z_d", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.CFEncoder.__init__": [[109, 120], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embedding_dim", ",", "d_model", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "CFEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "\n", "\n", "self", ".", "fc", "=", "Sequential", "(", "\n", "Linear", "(", "embedding_dim", ",", "128", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "Dropout", "(", "0.3", ")", ",", "\n", "Linear", "(", "128", ",", "d_model", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.CFEncoder.forward": [[122, 124], ["models.CFEncoder.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.FCFusion.__init__": [[127, 134], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ")", ":", "\n", "        ", "super", "(", "FCFusion", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "fc", "=", "Sequential", "(", "\n", "Linear", "(", "256", ",", "d_model", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.FCFusion.forward": [[136, 139], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.FCFusion.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "cf_x", ",", "gnr_y", ",", "mask", "=", "None", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "(", "cf_x", ",", "gnr_y", ")", ",", "-", "1", ")", "\n", "return", "self", ".", "fc", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Multiobjective.__init__": [[141, 146], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_out_1", ",", "d_out_2", ")", ":", "\n", "        ", "super", "(", "Multiobjective", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "out_1", "=", "nn", ".", "Linear", "(", "128", ",", "d_out_1", ")", "\n", "self", ".", "out_2", "=", "nn", ".", "Linear", "(", "128", ",", "d_out_2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.Multiobjective.forward": [[147, 151], ["models.Multiobjective.out_1", "models.Multiobjective.out_2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "audio_x", ")", ":", "\n", "        ", "x", "=", "self", ".", "out_1", "(", "audio_x", ")", "\n", "y", "=", "self", ".", "out_2", "(", "audio_x", ")", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagMeanEncoder.__init__": [[154, 164], ["torch.nn.Module.__init__", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding.from_pretrained", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "word_embedding_dim", ",", "d_model", ",", "emb_file", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "TagMeanEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "\n", "self", ".", "embeddings", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "torch", ".", "Tensor", "(", "np", ".", "load", "(", "emb_file", ")", ")", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "word_embedding_dim", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagMeanEncoder.forward": [[165, 174], ["models.TagMeanEncoder.embeddings", "models.TagMeanEncoder.fc", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "models.TagMeanEncoder.dropout", "models.TagMeanEncoder.layer_norm", "models.TagMeanEncoder.sum", "torch.repeat_interleave", "torch.repeat_interleave", "torch.repeat_interleave", "torch.repeat_interleave"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tags", ",", "z_audio", ",", "mask", "=", "None", ")", ":", "\n", "        ", "tag_embeddings", "=", "self", ".", "embeddings", "(", "tags", ")", "\n", "q", "=", "self", ".", "fc", "(", "tag_embeddings", ")", "\n", "# get mean average over non-0 embeddings", "\n", "q", "=", "torch", ".", "mul", "(", "q", ".", "sum", "(", "1", ")", ",", "torch", ".", "repeat_interleave", "(", "1", "/", "(", "(", "mask", "!=", "0", ")", ".", "sum", "(", "-", "1", ")", ".", "float", "(", ")", ")", ",", "repeats", "=", "self", ".", "d_model", ",", "dim", "=", "-", "1", ")", ")", "\n", "#q = torch.mean(q, dim=1)", "\n", "q", "=", "self", ".", "dropout", "(", "q", ")", "\n", "q", "=", "self", ".", "layer_norm", "(", "q", ")", "\n", "return", "q", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagSelfAttentionEncoder.__init__": [[177, 195], ["torch.nn.Module.__init__", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding.from_pretrained", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "models.ScaledDotProductAttention", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.load"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ",", "max_num_tags", ",", "word_embedding_dim", ",", "n_head", ",", "d_model", ",", "d_k", ",", "d_v", ",", "emb_file", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "TagSelfAttentionEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "torch", ".", "Tensor", "(", "np", ".", "load", "(", "emb_file", ")", ")", ",", "freeze", "=", "False", ")", "\n", "\n", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "d_k", "=", "d_k", "\n", "self", ".", "d_v", "=", "d_v", "\n", "\n", "self", ".", "w_qs", "=", "nn", ".", "Linear", "(", "d_model", ",", "n_head", "*", "d_k", ",", "bias", "=", "False", ")", "\n", "self", ".", "w_ks", "=", "nn", ".", "Linear", "(", "d_model", ",", "n_head", "*", "d_k", ",", "bias", "=", "False", ")", "\n", "self", ".", "w_vs", "=", "nn", ".", "Linear", "(", "d_model", ",", "n_head", "*", "d_v", ",", "bias", "=", "False", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "n_head", "*", "d_v", ",", "d_model", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "attention", "=", "ScaledDotProductAttention", "(", "temperature", "=", "d_k", "**", "0.5", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagSelfAttentionEncoder.forward": [[196, 235], ["models.TagSelfAttentionEncoder.embeddings", "models.TagSelfAttentionEncoder.w_qs().view", "models.TagSelfAttentionEncoder.w_ks().view", "models.TagSelfAttentionEncoder.w_vs().view", "models.TagSelfAttentionEncoder.attention", "q.view.view.transpose().contiguous().view", "models.TagSelfAttentionEncoder.dropout", "q.view.view.sum", "models.TagSelfAttentionEncoder.layer_norm", "q.view.view.view", "q.view.view.size", "q.view.view.size", "models.TagSelfAttentionEncoder.size", "models.TagSelfAttentionEncoder.size", "q.view.view.transpose", "models.TagSelfAttentionEncoder.transpose", "models.TagSelfAttentionEncoder.transpose", "mask.unsqueeze.unsqueeze.unsqueeze", "models.TagSelfAttentionEncoder.fc", "models.TagSelfAttentionEncoder.w_qs", "models.TagSelfAttentionEncoder.w_ks", "models.TagSelfAttentionEncoder.w_vs", "q.view.view.transpose().contiguous", "q.view.view.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tags", ",", "q", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "# in self attention q comes from the same source", "\n", "        ", "tag_embeddings", "=", "self", ".", "embeddings", "(", "tags", ")", "\n", "k", "=", "tag_embeddings", "\n", "v", "=", "tag_embeddings", "\n", "q", "=", "tag_embeddings", "\n", "\n", "d_k", ",", "d_v", ",", "n_head", "=", "self", ".", "d_k", ",", "self", ".", "d_v", ",", "self", ".", "n_head", "\n", "sz_b", ",", "len_q", ",", "len_k", ",", "len_v", "=", "q", ".", "size", "(", "0", ")", ",", "q", ".", "size", "(", "1", ")", ",", "k", ".", "size", "(", "1", ")", ",", "v", ".", "size", "(", "1", ")", "\n", "\n", "# No residual for now", "\n", "# residual = q", "\n", "\n", "# Pass through the pre-attention projection: b x lq x (n*dv)", "\n", "# Separate different heads: b x lq x n x dv", "\n", "q", "=", "self", ".", "w_qs", "(", "q", ")", ".", "view", "(", "sz_b", ",", "len_q", ",", "n_head", ",", "d_k", ")", "\n", "k", "=", "self", ".", "w_ks", "(", "k", ")", ".", "view", "(", "sz_b", ",", "len_k", ",", "n_head", ",", "d_k", ")", "\n", "v", "=", "self", ".", "w_vs", "(", "v", ")", ".", "view", "(", "sz_b", ",", "len_v", ",", "n_head", ",", "d_v", ")", "\n", "\n", "# Transpose for attention dot product: b x n x lq x dv", "\n", "q", ",", "k", ",", "v", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ",", "v", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "# For head axis broadcasting.", "\n", "\n", "", "q", ",", "attn", "=", "self", ".", "attention", "(", "q", ",", "k", ",", "v", ",", "mask", "=", "mask", ")", "\n", "\n", "# Transpose to move the head dimension back: b x lq x n x dv", "\n", "# Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "sz_b", ",", "len_q", ",", "-", "1", ")", "\n", "q", "=", "self", ".", "dropout", "(", "self", ".", "fc", "(", "q", ")", ")", "\n", "# q += residual", "\n", "\n", "# we sum all the values that were multiplied with attention weights", "\n", "q", "=", "q", ".", "sum", "(", "1", ")", "\n", "q", "=", "self", ".", "layer_norm", "(", "q", ")", "\n", "q", "=", "q", ".", "view", "(", "-", "1", ",", "q", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "return", "q", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagEncoder.__init__": [[238, 259], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "TagEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tag_encoder", "=", "Sequential", "(", "\n", "Linear", "(", "1000", ",", "512", ")", ",", "\n", "BatchNorm1d", "(", "512", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Dropout", "(", ".25", ")", ",", "\n", "Linear", "(", "512", ",", "512", ")", ",", "\n", "BatchNorm1d", "(", "512", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Dropout", "(", ".25", ")", ",", "\n", "Linear", "(", "512", ",", "1152", ")", ",", "\n", "BatchNorm1d", "(", "1152", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Dropout", "(", ".25", ")", ",", "\n", ")", "\n", "\n", "self", ".", "fc_tag", "=", "Sequential", "(", "\n", "Linear", "(", "1152", ",", "1152", ",", "bias", "=", "False", ")", ",", "\n", "Dropout", "(", ".25", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagEncoder.forward": [[261, 265], ["models.TagEncoder.tag_encoder", "models.TagEncoder.fc_tag"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tags", ")", ":", "\n", "        ", "z", "=", "self", ".", "tag_encoder", "(", "tags", ")", "\n", "z_d", "=", "self", ".", "fc_tag", "(", "z", ")", "\n", "return", "z", ",", "z_d", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagDecoder.__init__": [[268, 282], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.Sigmoid", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "TagDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tag_decoder", "=", "Sequential", "(", "\n", "Linear", "(", "128", ",", "512", ")", ",", "\n", "BatchNorm1d", "(", "512", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Dropout", "(", ".25", ")", ",", "\n", "Linear", "(", "512", ",", "512", ")", ",", "\n", "BatchNorm1d", "(", "512", ")", ",", "\n", "ReLU", "(", ")", ",", "\n", "Linear", "(", "512", ",", "1000", ")", ",", "\n", "BatchNorm1d", "(", "1000", ")", ",", "\n", "Sigmoid", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.models.TagDecoder.forward": [[284, 286], ["models.TagDecoder.tag_decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "z", ")", ":", "\n", "        ", "return", "self", ".", "tag_decoder", "(", "z", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.HDF5Dataset.__init__": [[10, 14], ["h5py.File", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "h5_path", ")", ":", "\n", "        ", "self", ".", "h5_path", "=", "h5_path", "\n", "self", ".", "h_file", "=", "h5py", ".", "File", "(", "h5_path", ",", "'r'", ")", "\n", "self", ".", "length", "=", "len", "(", "self", ".", "h_file", "[", "'dataset'", "]", "[", "'id'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.HDF5Dataset.__getitem__": [[15, 20], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "data", "=", "self", ".", "h_file", "[", "'dataset'", "]", "[", "'data'", "]", "[", "index", "]", "\n", "label", "=", "self", ".", "h_file", "[", "'dataset'", "]", "[", "'label'", "]", "[", "index", "]", "\n", "sound_id", "=", "self", ".", "h_file", "[", "'dataset'", "]", "[", "'id'", "]", "[", "index", "]", "\n", "return", "data", ",", "label", ",", "sound_id", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.HDF5Dataset.__len__": [[21, 23], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.InMemoryDataset.__init__": [[26, 34], ["h5py.File", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "h5_path", ")", ":", "\n", "        ", "self", ".", "h5_path", "=", "h5_path", "\n", "with", "h5py", ".", "File", "(", "h5_path", ",", "'r'", ")", "as", "h_file", ":", "\n", "            ", "self", ".", "data", "=", "h_file", "[", "'dataset'", "]", "[", "'data'", "]", "[", ":", "]", "\n", "self", ".", "cf_data", "=", "h_file", "[", "'dataset'", "]", "[", "'cf_data'", "]", "[", ":", "]", "\n", "self", ".", "label", "=", "h_file", "[", "'dataset'", "]", "[", "'label'", "]", "[", ":", "]", "\n", "self", ".", "id", "=", "h_file", "[", "'dataset'", "]", "[", "'id'", "]", "[", ":", "]", "\n", "self", ".", "length", "=", "len", "(", "self", ".", "id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.InMemoryDataset.__getitem__": [[35, 38], ["numpy.random.randint"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "rnd_index", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "1300", "-", "256", ")", "\n", "return", "self", ".", "data", "[", "index", "]", "[", "rnd_index", ":", "(", "rnd_index", "+", "256", ")", "]", ",", "self", ".", "label", "[", "index", "]", ",", "self", ".", "cf_data", "[", "index", "]", ",", "self", ".", "id", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.data_loader.InMemoryDataset.__len__": [[39, 41], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.__init__": [[24, 49], ["torch.device", "np.load"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "audio_encoder", "=", "None", "\n", "self", ".", "tag_encoder", "=", "None", "\n", "self", ".", "train_dataset_file", "=", "params", "[", "'train_dataset_file'", "]", "\n", "self", ".", "validation_dataset_file", "=", "params", "[", "'validation_dataset_file'", "]", "\n", "self", ".", "contrastive_temperature", "=", "params", "[", "'contrastive_temperature'", "]", "\n", "self", ".", "epochs", "=", "params", "[", "'epochs'", "]", "\n", "self", ".", "batch_size", "=", "params", "[", "'batch_size'", "]", "\n", "self", ".", "learning_rate", "=", "params", "[", "'learning_rate'", "]", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "params", "[", "'device'", "]", ")", "\n", "self", ".", "experiment_name", "=", "params", "[", "'experiment_name'", "]", "\n", "self", ".", "log_interval", "=", "params", "[", "'log_interval'", "]", "\n", "self", ".", "save_model_every", "=", "params", "[", "'save_model_every'", "]", "\n", "# This can be set here, or maybe automaticaly read from embedding_matrix_128.npy", "\n", "# don't forget to add +1 to the size of voc for adding idx 0 for no tag.", "\n", "self", ".", "w2v_emb_file", "=", "'embedding_matrix_128.npy'", "\n", "w2v_embeddings", "=", "np", ".", "load", "(", "self", ".", "w2v_emb_file", ")", "\n", "# We add one (+1) so we also consider no tag", "\n", "self", ".", "size_voc", "=", "w2v_embeddings", ".", "shape", "[", "0", "]", "+", "1", "\n", "self", ".", "aggregation", "=", "params", "[", "'aggregation'", "]", "\n", "self", ".", "num_heads", "=", "params", "[", "'num_heads'", "]", "\n", "self", ".", "encoder_type", "=", "params", "[", "'encoder'", "]", "\n", "self", ".", "save_model_loc", "=", "'/data1/playlists/models/contrastive'", "\n", "self", ".", "curr_min_val", "=", "10", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.init_models": [[50, 53], ["models.AudioEncoder", "models.Multiobjective"], "methods", ["None"], ["", "def", "init_models", "(", "self", ")", ":", "\n", "        ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "128", ")", "\n", "self", ".", "mo_encoder", "=", "Multiobjective", "(", "301", ",", "self", ".", "size_voc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.load_model_checkpoints": [[54, 67], ["pathlib.Path", "max", "trainer_baselines.Trainer.audio_encoder.load_state_dict", "trainer_baselines.Trainer.mo_encoder.load_state_dict", "print", "int", "torch.load", "torch.load", "print", "int", "str", "str", "pathlib.Path.iterdir", "pathlib.Path", "pathlib.Path", "f.stem.split"], "methods", ["None"], ["", "def", "load_model_checkpoints", "(", "self", ")", ":", "\n", "        ", "saved_models_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "try", ":", "\n", "            ", "last_epoch", "=", "max", "(", "[", "int", "(", "f", ".", "stem", ".", "split", "(", "'epoch_'", ")", "[", "-", "1", "]", ")", "for", "f", "in", "saved_models_folder", ".", "iterdir", "(", ")", "]", ")", "\n", "self", ".", "audio_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "self", ".", "mo_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'mo_encoder_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "print", "(", "f'Model checkpoints from epoch {last_epoch} loaded...'", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "last_epoch", "=", "0", "\n", "print", "(", "'No model loaded, training from scratch...'", ")", "\n", "\n", "", "self", ".", "iteration_idx", "=", "last_epoch", "*", "int", "(", "self", ".", "length_val_dataset", "/", "self", ".", "batch_size", ")", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.train": [[68, 111], ["data_loader.InMemoryDataset", "data_loader.InMemoryDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "len", "pathlib.Path", "trainer_baselines.Trainer.init_models", "trainer_baselines.Trainer.load_model_checkpoints", "trainer_baselines.Trainer.audio_encoder.to", "trainer_baselines.Trainer.mo_encoder.to", "torch.optim.Adam", "pathlib.Path.exists", "pathlib.Path.mkdir", "itertools.chain", "SummaryWriter", "range", "trainer_baselines.Trainer.audio_encoder.parameters", "trainer_baselines.Trainer.mo_encoder.parameters", "trainer_baselines.Trainer.train_one_epoch", "trainer_baselines.Trainer.val", "str", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.init_models", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.load_model_checkpoints", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train_one_epoch", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.val"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\" Train models\n\n        \"\"\"", "\n", "# Data loaders", "\n", "loader_params", "=", "{", "\n", "'batch_size'", ":", "self", ".", "batch_size", ",", "\n", "'shuffle'", ":", "True", ",", "\n", "'num_workers'", ":", "2", ",", "\n", "'drop_last'", ":", "True", ",", "\n", "}", "\n", "\n", "dataset_train", "=", "InMemoryDataset", "(", "self", ".", "train_dataset_file", ")", "\n", "#dataset_train = InMemoryDataset(self.validation_dataset_file)", "\n", "dataset_val", "=", "InMemoryDataset", "(", "self", ".", "validation_dataset_file", ")", "\n", "\n", "self", ".", "train_loader", "=", "data", ".", "DataLoader", "(", "dataset_train", ",", "**", "loader_params", ")", "\n", "self", ".", "val_loader", "=", "data", ".", "DataLoader", "(", "dataset_val", ",", "**", "loader_params", ")", "\n", "self", ".", "length_train_dataset", "=", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", "\n", "self", ".", "length_val_dataset", "=", "len", "(", "self", ".", "val_loader", ".", "dataset", ")", "\n", "\n", "# folder for model checkpoints", "\n", "model_checkpoints_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "if", "not", "model_checkpoints_folder", ".", "exists", "(", ")", ":", "\n", "            ", "model_checkpoints_folder", ".", "mkdir", "(", ")", "\n", "\n", "# models", "\n", "", "self", ".", "init_models", "(", ")", "\n", "self", ".", "load_model_checkpoints", "(", ")", "\n", "\n", "self", ".", "audio_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "mo_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# optimizers", "\n", "self", ".", "opt", "=", "torch", ".", "optim", ".", "Adam", "(", "chain", "(", "self", ".", "audio_encoder", ".", "parameters", "(", ")", ",", "self", ".", "mo_encoder", ".", "parameters", "(", ")", ")", ",", "lr", "=", "self", ".", "learning_rate", ",", "weight_decay", "=", "1e-4", ")", "\n", "\n", "# tensorboard", "\n", "with", "SummaryWriter", "(", "log_dir", "=", "str", "(", "Path", "(", "'runs'", ",", "self", ".", "experiment_name", ")", ")", ",", "max_queue", "=", "100", ")", "as", "self", ".", "tb", ":", "\n", "\n", "# Training loop", "\n", "            ", "for", "epoch", "in", "range", "(", "self", ".", "last_epoch", "+", "1", ",", "self", ".", "epochs", "+", "1", ")", ":", "\n", "                ", "self", ".", "train_one_epoch", "(", "epoch", ")", "\n", "self", ".", "val", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.train_one_epoch": [[112, 215], ["trainer_baselines.Trainer.audio_encoder.train", "trainer_baselines.Trainer.mo_encoder.train", "enumerate", "print", "print", "trainer_baselines.Trainer.tb.add_scalar", "torch.utils.data.view().to", "trainer_baselines.Trainer.audio_encoder", "trainer_baselines.Trainer.mo_encoder", "trainer_baselines.Trainer.opt.zero_grad", "pairwise_loss.backward", "trainer_baselines.Trainer.opt.step", "pairwise_loss.item", "trainer_baselines.Trainer.tb.add_scalar", "trainer_baselines.Trainer.tb.add_scalar", "torch.save", "torch.save", "torch.tensor().to", "cf_embeddings.to", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.MSELoss", "torch.nn.MSELoss.", "torch.nn.BCEWithLogitsLoss.item", "torch.nn.MSELoss.item", "trainer_baselines.Trainer.tb.add_scalar", "print", "trainer_baselines.Trainer.audio_encoder.state_dict", "str", "trainer_baselines.Trainer.mo_encoder.state_dict", "str", "torch.utils.data.view", "np.zeros", "curr_labels.append", "pairwise_loss.item", "pathlib.Path", "pathlib.Path", "torch.tensor", "len", "pairwise_loss.item", "len", "len"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train"], ["", "", "", "def", "train_one_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Train one epoch\n\n        \"\"\"", "\n", "self", ".", "audio_encoder", ".", "train", "(", ")", "\n", "self", ".", "mo_encoder", ".", "train", "(", ")", "\n", "\n", "# losses", "\n", "train_pairwise_loss", "=", "0", "\n", "train_pairwise_loss_1", "=", "0", "\n", "train_pairwise_loss_2", "=", "0", "\n", "train_pairwise_loss_3", "=", "0", "\n", "\n", "for", "batch_idx", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "            ", "self", ".", "iteration_idx", "+=", "1", "\n", "\n", "# TODO: REMOVE THAT", "\n", "# tags should already in the tag_idxs form, except for the +1 to indexes to use idx 0 for no tag", "\n", "# We probably want to add some pre-processing in data_loader.py", "\n", "# e.g. select random tags from the 100, or select random sepctrogram chunk", "\n", "\"\"\"\n            tag_idxs = [\n                ([idx+1 for idx, val in enumerate(tag_v) if val]\n                 + self.max_num_tags*[0])[:self.max_num_tags]\n                for tag_v in tags\n            ]\n            \"\"\"", "\n", "\n", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                ", "curr_labels", "=", "[", "]", "\n", "for", "curr_tags", "in", "tags", ":", "\n", "                    ", "non_neg", "=", "[", "i", "for", "i", "in", "curr_tags", "if", "i", "!=", "-", "1", "]", "\n", "new_tags", "=", "np", ".", "zeros", "(", "self", ".", "size_voc", ")", "\n", "new_tags", "[", "non_neg", "]", "=", "1", "\n", "curr_labels", ".", "append", "(", "new_tags", ")", "\n", "", "tags_input", "=", "torch", ".", "tensor", "(", "curr_labels", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                ", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# encode", "\n", "", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "z_cf", ",", "z_tags", "=", "self", ".", "mo_encoder", "(", "z_d_audio", ")", "\n", "\n", "# contrastive loss", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                ", "bceloss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "pairwise_loss_1", "=", "bceloss", "(", "z_tags", ",", "tags_input", ")", "\n", "pairwise_loss", "=", "pairwise_loss_1", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                ", "mseloss", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "pairwise_loss_2", "=", "mseloss", "(", "z_cf", ",", "cf_input", ")", "\n", "pairwise_loss", "=", "pairwise_loss_2", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                ", "pairwise_loss", "=", "pairwise_loss_1", "+", "pairwise_loss_2", "\n", "\n", "# Optimize models", "\n", "", "self", ".", "opt", ".", "zero_grad", "(", ")", "\n", "pairwise_loss", ".", "backward", "(", ")", "\n", "self", ".", "opt", ".", "step", "(", ")", "\n", "\n", "\n", "train_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                ", "train_pairwise_loss_1", "+=", "pairwise_loss_1", ".", "item", "(", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                ", "train_pairwise_loss_2", "+=", "pairwise_loss_2", ".", "item", "(", ")", "\n", "\n", "# write to tensorboard", "\n", "# These are too many data to send to tensorboard, but it can be useful for debugging/developing", "\n", "", "if", "False", ":", "\n", "                ", "self", ".", "tb", ".", "add_scalar", "(", "\"iter/contrastive_pairwise_loss\"", ",", "pairwise_loss", ".", "item", "(", ")", ",", "self", ".", "iteration_idx", ")", "\n", "\n", "# logs per batch", "\n", "", "if", "batch_idx", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "print", "(", "'Train Epoch: {} [{}/{} ({:.0f}%)]\\tPairwise loss: {:.4f})'", ".", "format", "(", "\n", "epoch", ",", "batch_idx", "*", "len", "(", "data", ")", ",", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", ",", "\n", "100.", "*", "batch_idx", "/", "len", "(", "self", ".", "train_loader", ")", ",", "\n", "pairwise_loss", ".", "item", "(", ")", "\n", ")", "\n", ")", "\n", "\n", "# epoch logs", "\n", "", "", "train_pairwise_loss", "=", "train_pairwise_loss", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "train_pairwise_loss_1", "=", "train_pairwise_loss_1", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "train_pairwise_loss_2", "=", "train_pairwise_loss_2", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "", "print", "(", "'====> Epoch: {}  Pairwise loss: {:.8f}'", ".", "format", "(", "epoch", ",", "train_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/sum\"", ",", "train_pairwise_loss", ",", "epoch", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/1\"", ",", "train_pairwise_loss_1", ",", "epoch", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/2\"", ",", "train_pairwise_loss_2", ",", "epoch", ")", "\n", "\n", "", "if", "epoch", "%", "self", ".", "save_model_every", "==", "0", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{epoch}.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "mo_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'mo_encoder_epoch_{epoch}.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_baselines.Trainer.val": [[216, 289], ["trainer_baselines.Trainer.audio_encoder.eval", "trainer_baselines.Trainer.mo_encoder.eval", "print", "print", "trainer_baselines.Trainer.tb.add_scalar", "torch.no_grad", "enumerate", "trainer_baselines.Trainer.tb.add_scalar", "trainer_baselines.Trainer.tb.add_scalar", "torch.utils.data.view().to", "trainer_baselines.Trainer.audio_encoder", "trainer_baselines.Trainer.mo_encoder", "pairwise_loss.item", "math.isinf", "math.isinf", "torch.save", "torch.save", "torch.tensor().to", "cf_embeddings.to", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "torch.nn.MSELoss", "torch.nn.MSELoss.", "torch.nn.BCEWithLogitsLoss.item", "torch.nn.MSELoss.item", "trainer_baselines.Trainer.audio_encoder.state_dict", "str", "trainer_baselines.Trainer.mo_encoder.state_dict", "str", "torch.utils.data.view", "np.zeros", "curr_labels.append", "pathlib.Path", "pathlib.Path", "torch.tensor"], "methods", ["None"], ["", "", "def", "val", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Validation\n\n        \"\"\"", "\n", "# A little bit a code repeat here...", "\n", "self", ".", "audio_encoder", ".", "eval", "(", ")", "\n", "self", ".", "mo_encoder", ".", "eval", "(", ")", "\n", "\n", "val_pairwise_loss", "=", "0", "\n", "val_pairwise_loss_1", "=", "0", "\n", "val_pairwise_loss_2", "=", "0", "\n", "val_pairwise_loss_3", "=", "0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "i", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "val_loader", ")", ":", "\n", "\n", "                ", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                    ", "curr_labels", "=", "[", "]", "\n", "for", "curr_tags", "in", "tags", ":", "\n", "                        ", "non_neg", "=", "[", "i", "for", "i", "in", "curr_tags", "if", "i", "!=", "-", "1", "]", "\n", "new_tags", "=", "np", ".", "zeros", "(", "self", ".", "size_voc", ")", "\n", "new_tags", "[", "non_neg", "]", "=", "1", "\n", "curr_labels", ".", "append", "(", "new_tags", ")", "\n", "", "tags_input", "=", "torch", ".", "tensor", "(", "curr_labels", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                    ", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# encode", "\n", "", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "z_cf", ",", "z_tags", "=", "self", ".", "mo_encoder", "(", "z_d_audio", ")", "\n", "\n", "# contrastive loss", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                    ", "bceloss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "pairwise_loss_1", "=", "bceloss", "(", "z_tags", ",", "tags_input", ")", "\n", "pairwise_loss", "=", "pairwise_loss_1", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", "or", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                    ", "mseloss", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "pairwise_loss_2", "=", "mseloss", "(", "z_cf", ",", "cf_input", ")", "\n", "pairwise_loss", "=", "pairwise_loss_2", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF_gnr\"", ":", "\n", "                    ", "pairwise_loss", "=", "pairwise_loss_1", "+", "pairwise_loss_2", "\n", "\n", "\n", "", "val_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                    ", "val_pairwise_loss_1", "+=", "pairwise_loss_1", ".", "item", "(", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                    ", "val_pairwise_loss_2", "+=", "pairwise_loss_2", ".", "item", "(", ")", "\n", "\n", "", "", "", "val_pairwise_loss", "=", "val_pairwise_loss", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "val_pairwise_loss_1", "=", "val_pairwise_loss_1", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "val_pairwise_loss_2", "=", "val_pairwise_loss_2", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "\n", "", "print", "(", "'====> Val average pairwise loss: {:.4f}'", ".", "format", "(", "val_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/sum\"", ",", "val_pairwise_loss", ",", "epoch", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/1\"", ",", "val_pairwise_loss_1", ",", "epoch", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/2\"", ",", "val_pairwise_loss_2", ",", "epoch", ")", "\n", "", "if", "not", "(", "math", ".", "isinf", "(", "val_pairwise_loss", ")", "or", "math", ".", "isinf", "(", "val_pairwise_loss", ")", ")", ":", "\n", "            ", "if", "val_pairwise_loss", "<", "self", ".", "curr_min_val", ":", "\n", "                ", "self", ".", "curr_min_val", "=", "val_pairwise_loss", "\n", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_best.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "mo_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'mo_encoder_epoch_best.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.encode.return_loaded_model": [[19, 24], ["Model", "Model.load_state_dict", "Model.eval", "torch.load", "torch.device"], "function", ["None"], ["def", "return_loaded_model", "(", "Model", ",", "checkpoint", ")", ":", "\n", "    ", "model", "=", "Model", "(", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "checkpoint", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "model", ".", "eval", "(", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.encode.extract_audio_embedding": [[26, 38], ["torch.no_grad", "utils.extract_spectrogram", "scaler.transform", "torch.unsqueeze().float", "model", "print", "torch.unsqueeze", "torch.unsqueeze", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.extract_spectrogram"], ["", "def", "extract_audio_embedding", "(", "model", ",", "filename", ")", ":", "\n", "    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "x", "=", "extract_spectrogram", "(", "filename", ")", "\n", "x", "=", "scaler", ".", "transform", "(", "x", ")", "\n", "x", "=", "torch", ".", "unsqueeze", "(", "torch", ".", "unsqueeze", "(", "torch", ".", "tensor", "(", "x", ")", ",", "0", ")", ",", "0", ")", ".", "float", "(", ")", "\n", "embedding", ",", "embedding_d", "=", "model", "(", "x", ")", "\n", "return", "embedding", ",", "embedding_d", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "return", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "e", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.encode.extract_audio_embedding_chunks": [[40, 53], ["torch.no_grad", "utils.extract_spectrogram", "librosa.util.frame", "torch.tensor().permute", "torch.unsqueeze", "model", "numpy.asfortranarray", "print", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.extract_spectrogram"], ["", "", "", "def", "extract_audio_embedding_chunks", "(", "model", ",", "filename", ")", ":", "\n", "    ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "x", "=", "extract_spectrogram", "(", "filename", ")", "\n", "x_chunks", "=", "librosa", ".", "util", ".", "frame", "(", "np", ".", "asfortranarray", "(", "x", ")", ",", "frame_length", "=", "256", ",", "hop_length", "=", "256", ",", "axis", "=", "-", "1", ")", "\n", "x_chunks", "=", "torch", ".", "tensor", "(", "x_chunks", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "x_chunks", "=", "torch", ".", "unsqueeze", "(", "x_chunks", ",", "1", ")", "\n", "embedding_chunks", ",", "embedding_d_chunks", "=", "model", "(", "x_chunks", ")", "\n", "return", "embedding_chunks", ",", "embedding_d_chunks", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "return", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "e", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.__init__": [[24, 49], ["torch.device", "np.load"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "audio_encoder", "=", "None", "\n", "self", ".", "tag_encoder", "=", "None", "\n", "self", ".", "train_dataset_file", "=", "params", "[", "'train_dataset_file'", "]", "\n", "self", ".", "validation_dataset_file", "=", "params", "[", "'validation_dataset_file'", "]", "\n", "self", ".", "contrastive_temperature", "=", "params", "[", "'contrastive_temperature'", "]", "\n", "self", ".", "epochs", "=", "params", "[", "'epochs'", "]", "\n", "self", ".", "batch_size", "=", "params", "[", "'batch_size'", "]", "\n", "self", ".", "learning_rate", "=", "params", "[", "'learning_rate'", "]", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "params", "[", "'device'", "]", ")", "\n", "self", ".", "experiment_name", "=", "params", "[", "'experiment_name'", "]", "\n", "self", ".", "log_interval", "=", "params", "[", "'log_interval'", "]", "\n", "self", ".", "save_model_every", "=", "params", "[", "'save_model_every'", "]", "\n", "self", ".", "max_num_tags", "=", "10", "# This may be increased and used in data_loader.py", "\n", "# This can be set here, or maybe automaticaly read from embedding_matrix_128.npy", "\n", "# don't forget to add +1 to the size of voc for adding idx 0 for no tag.", "\n", "self", ".", "w2v_emb_file", "=", "'embedding_matrix_128.npy'", "\n", "w2v_embeddings", "=", "np", ".", "load", "(", "self", ".", "w2v_emb_file", ")", "\n", "# We add one (+1) so we also consider no tag", "\n", "self", ".", "size_voc", "=", "w2v_embeddings", ".", "shape", "[", "0", "]", "+", "1", "\n", "self", ".", "aggregation", "=", "params", "[", "'aggregation'", "]", "\n", "self", ".", "num_heads", "=", "params", "[", "'num_heads'", "]", "\n", "self", ".", "save_model_loc", "=", "'/data1/playlists/models/contrastive'", "\n", "self", ".", "curr_min_val", "=", "10", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.init_models": [[50, 57], ["models.AudioEncoder", "models.CFEncoder", "models.TagMeanEncoder", "models.TagSelfAttentionEncoder"], "methods", ["None"], ["", "def", "init_models", "(", "self", ")", ":", "\n", "        ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "128", ")", "\n", "if", "self", ".", "aggregation", "==", "'mean'", ":", "\n", "            ", "self", ".", "tag_encoder", "=", "TagMeanEncoder", "(", "self", ".", "size_voc", ",", "128", ",", "128", ",", "emb_file", "=", "self", ".", "w2v_emb_file", ")", "\n", "", "elif", "self", ".", "aggregation", "==", "'self'", ":", "\n", "            ", "self", ".", "tag_encoder", "=", "TagSelfAttentionEncoder", "(", "self", ".", "max_num_tags", ",", "128", ",", "self", ".", "num_heads", ",", "128", ",", "128", ",", "128", ",", "self", ".", "w2v_emb_file", ",", "dropout", "=", "0.1", ")", "\n", "", "self", ".", "cf_encoder", "=", "CFEncoder", "(", "301", ",", "128", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.load_model_checkpoints": [[58, 71], ["pathlib.Path", "max", "trainer.Trainer.audio_encoder.load_state_dict", "trainer.Trainer.tag_encoder.load_state_dict", "print", "int", "torch.load", "torch.load", "print", "int", "str", "str", "pathlib.Path.iterdir", "pathlib.Path", "pathlib.Path", "f.stem.split"], "methods", ["None"], ["", "def", "load_model_checkpoints", "(", "self", ")", ":", "\n", "        ", "saved_models_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "try", ":", "\n", "            ", "last_epoch", "=", "max", "(", "[", "int", "(", "f", ".", "stem", ".", "split", "(", "'epoch_'", ")", "[", "-", "1", "]", ")", "for", "f", "in", "saved_models_folder", ".", "iterdir", "(", ")", "]", ")", "\n", "self", ".", "audio_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "self", ".", "tag_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "print", "(", "f'Model checkpoints from epoch {last_epoch} loaded...'", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "last_epoch", "=", "0", "\n", "print", "(", "'No model loaded, training from scratch...'", ")", "\n", "\n", "", "self", ".", "iteration_idx", "=", "last_epoch", "*", "int", "(", "self", ".", "length_val_dataset", "/", "self", ".", "batch_size", ")", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.train": [[72, 121], ["data_loader.InMemoryDataset", "data_loader.InMemoryDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "len", "pathlib.Path", "trainer.Trainer.init_models", "trainer.Trainer.load_model_checkpoints", "trainer.Trainer.audio_encoder.to", "trainer.Trainer.tag_encoder.to", "trainer.Trainer.cf_encoder.to", "torch.optim.Adam", "pathlib.Path.exists", "pathlib.Path.mkdir", "itertools.chain", "SummaryWriter", "range", "trainer.Trainer.audio_encoder.parameters", "trainer.Trainer.cf_encoder.parameters", "trainer.Trainer.tag_encoder.parameters", "trainer.Trainer.train_one_epoch", "trainer.Trainer.val", "str", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.init_models", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.load_model_checkpoints", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train_one_epoch", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.val"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\" Train models\n\n        \"\"\"", "\n", "# Data loaders", "\n", "loader_params", "=", "{", "\n", "'batch_size'", ":", "self", ".", "batch_size", ",", "\n", "'shuffle'", ":", "True", ",", "\n", "'num_workers'", ":", "2", ",", "\n", "'drop_last'", ":", "True", ",", "\n", "}", "\n", "\n", "dataset_train", "=", "InMemoryDataset", "(", "self", ".", "train_dataset_file", ")", "\n", "#dataset_train = InMemoryDataset(self.validation_dataset_file)", "\n", "dataset_val", "=", "InMemoryDataset", "(", "self", ".", "validation_dataset_file", ")", "\n", "\n", "self", ".", "train_loader", "=", "data", ".", "DataLoader", "(", "dataset_train", ",", "**", "loader_params", ")", "\n", "self", ".", "val_loader", "=", "data", ".", "DataLoader", "(", "dataset_val", ",", "**", "loader_params", ")", "\n", "self", ".", "length_train_dataset", "=", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", "\n", "self", ".", "length_val_dataset", "=", "len", "(", "self", ".", "val_loader", ".", "dataset", ")", "\n", "\n", "# folder for model checkpoints", "\n", "model_checkpoints_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "if", "not", "model_checkpoints_folder", ".", "exists", "(", ")", ":", "\n", "            ", "model_checkpoints_folder", ".", "mkdir", "(", ")", "\n", "\n", "# models", "\n", "", "self", ".", "init_models", "(", ")", "\n", "self", ".", "load_model_checkpoints", "(", ")", "\n", "\n", "self", ".", "audio_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "tag_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "cf_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# optimizers", "\n", "#self.audio_opt = optim.SGD(self.audio_encoder.parameters(), lr=self.learning_rate)", "\n", "#self.opt = torch.optim.Adam(chain(self.audio_encoder.parameters(), self.tag_encoder.parameters()), lr=self.learning_rate, weight_decay=1e-4)", "\n", "self", ".", "opt", "=", "torch", ".", "optim", ".", "Adam", "(", "chain", "(", "self", ".", "audio_encoder", ".", "parameters", "(", ")", ",", "\n", "self", ".", "cf_encoder", ".", "parameters", "(", ")", ",", "\n", "self", ".", "tag_encoder", ".", "parameters", "(", ")", ")", ",", "lr", "=", "self", ".", "learning_rate", ",", "weight_decay", "=", "1e-4", ")", "\n", "#self.tag_opt = optim.SGD(self.tag_encoder.parameters(), lr=self.learning_rate)", "\n", "\n", "# tensorboard", "\n", "with", "SummaryWriter", "(", "log_dir", "=", "str", "(", "Path", "(", "'runs'", ",", "self", ".", "experiment_name", ")", ")", ",", "max_queue", "=", "100", ")", "as", "self", ".", "tb", ":", "\n", "\n", "# Training loop", "\n", "            ", "for", "epoch", "in", "range", "(", "self", ".", "last_epoch", "+", "1", ",", "self", ".", "epochs", "+", "1", ")", ":", "\n", "                ", "self", ".", "train_one_epoch", "(", "epoch", ")", "\n", "self", ".", "val", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.train_one_epoch": [[122, 232], ["trainer.Trainer.audio_encoder.train", "trainer.Trainer.tag_encoder.train", "trainer.Trainer.cf_encoder.train", "enumerate", "print", "print", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "torch.tensor().to", "torch.utils.data.view().to", "cf_embeddings.to", "trainer.Trainer.audio_encoder", "trainer.Trainer.tag_encoder", "trainer.Trainer.cf_encoder", "utils.contrastive_loss", "utils.contrastive_loss", "utils.contrastive_loss", "trainer.Trainer.opt.zero_grad", "pairwise_loss.backward", "trainer.Trainer.opt.step", "pairwise_loss.item", "utils.contrastive_loss.item", "utils.contrastive_loss.item", "utils.contrastive_loss.item", "torch.save", "torch.save", "torch.save", "np.zeros", "curr_labels.append", "trainer.Trainer.tb.add_scalar", "print", "trainer.Trainer.audio_encoder.state_dict", "str", "trainer.Trainer.tag_encoder.state_dict", "str", "trainer.Trainer.cf_encoder.state_dict", "str", "torch.tensor", "torch.utils.data.view", "torch.tensor().to.unsqueeze", "pairwise_loss.item", "pathlib.Path", "pathlib.Path", "pathlib.Path", "min", "len", "pairwise_loss.item", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss"], ["", "", "", "def", "train_one_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Train one epoch\n\n        \"\"\"", "\n", "self", ".", "audio_encoder", ".", "train", "(", ")", "\n", "self", ".", "tag_encoder", ".", "train", "(", ")", "\n", "self", ".", "cf_encoder", ".", "train", "(", ")", "\n", "\n", "# losses", "\n", "train_pairwise_loss", "=", "0", "\n", "train_pairwise_loss_1", "=", "0", "\n", "train_pairwise_loss_2", "=", "0", "\n", "train_pairwise_loss_3", "=", "0", "\n", "\n", "for", "batch_idx", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "            ", "self", ".", "iteration_idx", "+=", "1", "\n", "\n", "# TODO: REMOVE THAT", "\n", "# tags should already in the tag_idxs form, except for the +1 to indexes to use idx 0 for no tag", "\n", "# We probably want to add some pre-processing in data_loader.py", "\n", "# e.g. select random tags from the 100, or select random sepctrogram chunk", "\n", "\"\"\"\n            tag_idxs = [\n                ([idx+1 for idx, val in enumerate(tag_v) if val]\n                 + self.max_num_tags*[0])[:self.max_num_tags]\n                for tag_v in tags\n            ]\n\n            \"\"\"", "\n", "curr_labels", "=", "[", "]", "\n", "for", "curr_tags", "in", "tags", ":", "\n", "                ", "non_neg", "=", "[", "i", "+", "1", "for", "i", "in", "curr_tags", "if", "i", "!=", "-", "1", "]", "\n", "new_tags", "=", "np", ".", "zeros", "(", "self", ".", "max_num_tags", ")", "\n", "#new_tags[:len(non_neg)] = np.random.choice(non_neg, min(self.max_num_tags, len(non_neg)), replace=False)", "\n", "new_tags", "[", ":", "min", "(", "len", "(", "non_neg", ")", ",", "10", ")", "]", "=", "non_neg", "[", ":", "10", "]", "\n", "curr_labels", ".", "append", "(", "new_tags", ")", "\n", "", "tags_input", "=", "torch", ".", "tensor", "(", "curr_labels", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#tags_input = tags.to(self.device)", "\n", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# encode", "\n", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "z_tags", ",", "attn", "=", "self", ".", "tag_encoder", "(", "tags_input", ",", "z_d_audio", ",", "mask", "=", "tags_input", ".", "unsqueeze", "(", "1", ")", ")", "\n", "z_cf", "=", "self", ".", "cf_encoder", "(", "cf_input", ")", "\n", "\n", "# contrastive loss", "\n", "pairwise_loss_1", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss_2", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_cf", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss_3", "=", "contrastive_loss", "(", "z_cf", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss", "=", "pairwise_loss_1", "+", "pairwise_loss_2", "+", "pairwise_loss_3", "\n", "\n", "# Optimize models", "\n", "\"\"\"\n            self.audio_opt.zero_grad()\n            self.tag_opt.zero_grad()\n            pairwise_loss.backward()\n            self.audio_opt.step()\n            self.tag_opt.step()\n            \"\"\"", "\n", "self", ".", "opt", ".", "zero_grad", "(", ")", "\n", "pairwise_loss", ".", "backward", "(", ")", "\n", "\"\"\"\n            clip_norm_params = {\n                    'max_norm': 1.,\n                    'norm_type': 2\n            }\n            torch.nn.utils.clip_grad_norm_(self.audio_encoder.parameters(), **clip_norm_params)\n            torch.nn.utils.clip_grad_norm_(self.tag_encoder.parameters(), **clip_norm_params)\n            \"\"\"", "\n", "self", ".", "opt", ".", "step", "(", ")", "\n", "\n", "\n", "train_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "train_pairwise_loss_1", "+=", "pairwise_loss_1", ".", "item", "(", ")", "\n", "train_pairwise_loss_2", "+=", "pairwise_loss_2", ".", "item", "(", ")", "\n", "train_pairwise_loss_3", "+=", "pairwise_loss_3", ".", "item", "(", ")", "\n", "\n", "# write to tensorboard", "\n", "# These are too many data to send to tensorboard, but it can be useful for debugging/developing", "\n", "if", "False", ":", "\n", "                ", "self", ".", "tb", ".", "add_scalar", "(", "\"iter/contrastive_pairwise_loss\"", ",", "pairwise_loss", ".", "item", "(", ")", ",", "self", ".", "iteration_idx", ")", "\n", "\n", "# logs per batch", "\n", "", "if", "batch_idx", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "print", "(", "'Train Epoch: {} [{}/{} ({:.0f}%)]\\tPairwise loss: {:.4f})'", ".", "format", "(", "\n", "epoch", ",", "batch_idx", "*", "len", "(", "data", ")", ",", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", ",", "\n", "100.", "*", "batch_idx", "/", "len", "(", "self", ".", "train_loader", ")", ",", "\n", "pairwise_loss", ".", "item", "(", ")", "\n", ")", "\n", ")", "\n", "\n", "# epoch logs", "\n", "", "", "train_pairwise_loss", "=", "train_pairwise_loss", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "train_pairwise_loss_1", "=", "train_pairwise_loss_1", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "train_pairwise_loss_2", "=", "train_pairwise_loss_2", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "train_pairwise_loss_3", "=", "train_pairwise_loss_3", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "print", "(", "'====> Epoch: {}  Pairwise loss: {:.8f}'", ".", "format", "(", "epoch", ",", "train_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/sum\"", ",", "train_pairwise_loss", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/1\"", ",", "train_pairwise_loss_1", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/2\"", ",", "train_pairwise_loss_2", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/3\"", ",", "train_pairwise_loss_3", ",", "epoch", ")", "\n", "\n", "if", "epoch", "%", "self", ".", "save_model_every", "==", "0", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{epoch}.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "tag_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_{epoch}.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "cf_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'cf_encoder_att_epoch_{epoch}.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer.Trainer.val": [[233, 302], ["trainer.Trainer.audio_encoder.eval", "trainer.Trainer.tag_encoder.eval", "trainer.Trainer.cf_encoder.eval", "print", "print", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "trainer.Trainer.tb.add_scalar", "torch.no_grad", "enumerate", "torch.tensor().to", "torch.utils.data.view().to", "cf_embeddings.to", "trainer.Trainer.audio_encoder", "trainer.Trainer.tag_encoder", "trainer.Trainer.cf_encoder", "utils.contrastive_loss", "utils.contrastive_loss", "utils.contrastive_loss", "pairwise_loss.item", "utils.contrastive_loss.item", "utils.contrastive_loss.item", "utils.contrastive_loss.item", "math.isinf", "math.isinf", "torch.save", "torch.save", "torch.save", "np.zeros", "curr_labels.append", "trainer.Trainer.audio_encoder.state_dict", "str", "trainer.Trainer.tag_encoder.state_dict", "str", "trainer.Trainer.cf_encoder.state_dict", "str", "torch.tensor", "torch.utils.data.view", "torch.tensor().to.unsqueeze", "pathlib.Path", "pathlib.Path", "pathlib.Path", "min", "len"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss"], ["", "", "def", "val", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Validation\n\n        \"\"\"", "\n", "# A little bit a code repeat here...", "\n", "self", ".", "audio_encoder", ".", "eval", "(", ")", "\n", "self", ".", "tag_encoder", ".", "eval", "(", ")", "\n", "self", ".", "cf_encoder", ".", "eval", "(", ")", "\n", "\n", "val_pairwise_loss", "=", "0", "\n", "val_pairwise_loss_1", "=", "0", "\n", "val_pairwise_loss_2", "=", "0", "\n", "val_pairwise_loss_3", "=", "0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "i", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "val_loader", ")", ":", "\n", "\n", "                ", "curr_labels", "=", "[", "]", "\n", "for", "curr_tags", "in", "tags", ":", "\n", "                    ", "non_neg", "=", "[", "i", "+", "1", "for", "i", "in", "curr_tags", "if", "i", "!=", "-", "1", "]", "\n", "new_tags", "=", "np", ".", "zeros", "(", "self", ".", "max_num_tags", ")", "\n", "#new_tags[:len(non_neg)] = np.random.choice(non_neg, min(self.max_num_tags, len(non_neg)), replace=False)", "\n", "new_tags", "[", ":", "min", "(", "len", "(", "non_neg", ")", ",", "10", ")", "]", "=", "non_neg", "[", ":", "10", "]", "\n", "curr_labels", ".", "append", "(", "new_tags", ")", "\n", "", "tags_input", "=", "torch", ".", "tensor", "(", "curr_labels", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# encode", "\n", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "z_tags", ",", "attn", "=", "self", ".", "tag_encoder", "(", "tags_input", ",", "z_d_audio", ",", "mask", "=", "tags_input", ".", "unsqueeze", "(", "1", ")", ")", "\n", "z_cf", "=", "self", ".", "cf_encoder", "(", "cf_input", ")", "\n", "\n", "# pairwise correspondence loss", "\n", "#pairwise_loss = contrastive_loss(z_d_audio, z_tags, self.contrastive_temperature)", "\n", "#pairwise_loss = contrastive_loss(z_d_audio, z_cf, self.contrastive_temperature)", "\n", "#pairwise_loss = contrastive_loss(z_d_audio, z_fusion, self.contrastive_temperature)", "\n", "# contrastive loss", "\n", "pairwise_loss_1", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss_2", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_cf", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss_3", "=", "contrastive_loss", "(", "z_cf", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "pairwise_loss", "=", "pairwise_loss_1", "+", "pairwise_loss_2", "+", "pairwise_loss_3", "\n", "\n", "\n", "val_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "val_pairwise_loss_1", "+=", "pairwise_loss_1", ".", "item", "(", ")", "\n", "val_pairwise_loss_2", "+=", "pairwise_loss_2", ".", "item", "(", ")", "\n", "val_pairwise_loss_3", "+=", "pairwise_loss_3", ".", "item", "(", ")", "\n", "\n", "", "", "val_pairwise_loss", "=", "val_pairwise_loss", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "val_pairwise_loss_1", "=", "val_pairwise_loss_1", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "val_pairwise_loss_2", "=", "val_pairwise_loss_2", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "val_pairwise_loss_3", "=", "val_pairwise_loss_3", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "\n", "print", "(", "'====> Val average pairwise loss: {:.4f}'", ".", "format", "(", "val_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/sum\"", ",", "val_pairwise_loss", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/1\"", ",", "val_pairwise_loss_1", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/2\"", ",", "val_pairwise_loss_2", ",", "epoch", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/3\"", ",", "val_pairwise_loss_3", ",", "epoch", ")", "\n", "if", "not", "(", "math", ".", "isinf", "(", "val_pairwise_loss", ")", "or", "math", ".", "isinf", "(", "val_pairwise_loss", ")", ")", ":", "\n", "            ", "if", "val_pairwise_loss", "<", "self", ".", "curr_min_val", ":", "\n", "                ", "self", ".", "curr_min_val", "=", "val_pairwise_loss", "\n", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_best.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "tag_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_best.pt'", ")", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "cf_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'cf_encoder_att_epoch_best.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.train.main": [[7, 13], ["json.load", "print", "print", "trainer.Trainer", "trainer.Trainer.train", "open", "json.dumps"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train"], ["def", "main", "(", "config_file", ")", ":", "\n", "    ", "params", "=", "json", ".", "load", "(", "open", "(", "config_file", ",", "'rb'", ")", ")", "\n", "print", "(", "\"Training models with params:\"", ")", "\n", "print", "(", "json", ".", "dumps", "(", "params", ",", "separators", "=", "(", "\"\\n\"", ",", "\": \"", ")", ",", "indent", "=", "4", ")", ")", "\n", "trainer", "=", "Trainer", "(", "params", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.baseline_train.main": [[7, 13], ["json.load", "print", "print", "trainer_baselines.Trainer", "trainer_baselines.Trainer.train", "open", "json.dumps"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train"], ["def", "main", "(", "config_file", ")", ":", "\n", "    ", "params", "=", "json", ".", "load", "(", "open", "(", "config_file", ",", "'rb'", ")", ")", "\n", "print", "(", "\"Training models with params:\"", ")", "\n", "print", "(", "json", ".", "dumps", "(", "params", ",", "separators", "=", "(", "\"\\n\"", ",", "\": \"", ")", ",", "indent", "=", "4", ")", ")", "\n", "trainer", "=", "Trainer", "(", "params", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.__init__": [[24, 50], ["torch.device", "np.load"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "audio_encoder", "=", "None", "\n", "self", ".", "tag_encoder", "=", "None", "\n", "self", ".", "train_dataset_file", "=", "params", "[", "'train_dataset_file'", "]", "\n", "self", ".", "validation_dataset_file", "=", "params", "[", "'validation_dataset_file'", "]", "\n", "self", ".", "contrastive_temperature", "=", "params", "[", "'contrastive_temperature'", "]", "\n", "self", ".", "epochs", "=", "params", "[", "'epochs'", "]", "\n", "self", ".", "batch_size", "=", "params", "[", "'batch_size'", "]", "\n", "self", ".", "learning_rate", "=", "params", "[", "'learning_rate'", "]", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "params", "[", "'device'", "]", ")", "\n", "self", ".", "experiment_name", "=", "params", "[", "'experiment_name'", "]", "\n", "self", ".", "log_interval", "=", "params", "[", "'log_interval'", "]", "\n", "self", ".", "save_model_every", "=", "params", "[", "'save_model_every'", "]", "\n", "self", ".", "max_num_tags", "=", "10", "# This may be increased and used in data_loader.py", "\n", "# This can be set here, or maybe automaticaly read from embedding_matrix_128.npy", "\n", "# don't forget to add +1 to the size of voc for adding idx 0 for no tag.", "\n", "self", ".", "w2v_emb_file", "=", "'embedding_matrix_128.npy'", "\n", "w2v_embeddings", "=", "np", ".", "load", "(", "self", ".", "w2v_emb_file", ")", "\n", "# We add one (+1) so we also consider no tag", "\n", "self", ".", "size_voc", "=", "w2v_embeddings", ".", "shape", "[", "0", "]", "+", "1", "\n", "self", ".", "aggregation", "=", "params", "[", "'aggregation'", "]", "\n", "self", ".", "num_heads", "=", "params", "[", "'num_heads'", "]", "\n", "self", ".", "encoder_type", "=", "params", "[", "'encoder'", "]", "\n", "self", ".", "save_model_loc", "=", "'/data1/playlists/models/contrastive'", "\n", "self", ".", "curr_min_val", "=", "10", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.init_models": [[51, 58], ["models.AudioEncoder", "models.CFEncoder", "models.TagMeanEncoder", "models.TagSelfAttentionEncoder"], "methods", ["None"], ["", "def", "init_models", "(", "self", ")", ":", "\n", "        ", "self", ".", "audio_encoder", "=", "AudioEncoder", "(", "128", ")", "\n", "if", "self", ".", "aggregation", "==", "'mean'", ":", "\n", "            ", "self", ".", "tag_encoder", "=", "TagMeanEncoder", "(", "self", ".", "size_voc", ",", "128", ",", "128", ",", "emb_file", "=", "self", ".", "w2v_emb_file", ")", "\n", "", "elif", "self", ".", "aggregation", "==", "'self'", ":", "\n", "            ", "self", ".", "tag_encoder", "=", "TagSelfAttentionEncoder", "(", "self", ".", "max_num_tags", ",", "128", ",", "self", ".", "num_heads", ",", "128", ",", "128", ",", "128", ",", "self", ".", "w2v_emb_file", ",", "dropout", "=", "0.1", ")", "\n", "", "self", ".", "cf_encoder", "=", "CFEncoder", "(", "301", ",", "128", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.load_model_checkpoints": [[59, 72], ["pathlib.Path", "max", "trainer_contrastive.Trainer.audio_encoder.load_state_dict", "trainer_contrastive.Trainer.tag_encoder.load_state_dict", "print", "int", "torch.load", "torch.load", "print", "int", "str", "str", "pathlib.Path.iterdir", "pathlib.Path", "pathlib.Path", "f.stem.split"], "methods", ["None"], ["", "def", "load_model_checkpoints", "(", "self", ")", ":", "\n", "        ", "saved_models_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "try", ":", "\n", "            ", "last_epoch", "=", "max", "(", "[", "int", "(", "f", ".", "stem", ".", "split", "(", "'epoch_'", ")", "[", "-", "1", "]", ")", "for", "f", "in", "saved_models_folder", ".", "iterdir", "(", ")", "]", ")", "\n", "self", ".", "audio_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "self", ".", "tag_encoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_{last_epoch}.pt'", ")", ")", ")", ")", "\n", "print", "(", "f'Model checkpoints from epoch {last_epoch} loaded...'", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "last_epoch", "=", "0", "\n", "print", "(", "'No model loaded, training from scratch...'", ")", "\n", "\n", "", "self", ".", "iteration_idx", "=", "last_epoch", "*", "int", "(", "self", ".", "length_val_dataset", "/", "self", ".", "batch_size", ")", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train": [[73, 122], ["data_loader.InMemoryDataset", "data_loader.InMemoryDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "len", "pathlib.Path", "trainer_contrastive.Trainer.init_models", "trainer_contrastive.Trainer.load_model_checkpoints", "trainer_contrastive.Trainer.audio_encoder.to", "pathlib.Path.exists", "pathlib.Path.mkdir", "trainer_contrastive.Trainer.tag_encoder.to", "trainer_contrastive.Trainer.cf_encoder.to", "torch.optim.Adam", "torch.optim.Adam", "SummaryWriter", "range", "itertools.chain", "itertools.chain", "trainer_contrastive.Trainer.train_one_epoch", "trainer_contrastive.Trainer.val", "trainer_contrastive.Trainer.audio_encoder.parameters", "trainer_contrastive.Trainer.tag_encoder.parameters", "trainer_contrastive.Trainer.audio_encoder.parameters", "trainer_contrastive.Trainer.cf_encoder.parameters", "str", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.init_models", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.load_model_checkpoints", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train_one_epoch", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.val"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\" Train models\n\n        \"\"\"", "\n", "# Data loaders", "\n", "loader_params", "=", "{", "\n", "'batch_size'", ":", "self", ".", "batch_size", ",", "\n", "'shuffle'", ":", "True", ",", "\n", "'num_workers'", ":", "2", ",", "\n", "'drop_last'", ":", "True", ",", "\n", "}", "\n", "\n", "dataset_train", "=", "InMemoryDataset", "(", "self", ".", "train_dataset_file", ")", "\n", "#dataset_train = InMemoryDataset(self.validation_dataset_file)", "\n", "dataset_val", "=", "InMemoryDataset", "(", "self", ".", "validation_dataset_file", ")", "\n", "\n", "self", ".", "train_loader", "=", "data", ".", "DataLoader", "(", "dataset_train", ",", "**", "loader_params", ")", "\n", "self", ".", "val_loader", "=", "data", ".", "DataLoader", "(", "dataset_val", ",", "**", "loader_params", ")", "\n", "self", ".", "length_train_dataset", "=", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", "\n", "self", ".", "length_val_dataset", "=", "len", "(", "self", ".", "val_loader", ".", "dataset", ")", "\n", "\n", "# folder for model checkpoints", "\n", "model_checkpoints_folder", "=", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ")", "\n", "if", "not", "model_checkpoints_folder", ".", "exists", "(", ")", ":", "\n", "            ", "model_checkpoints_folder", ".", "mkdir", "(", ")", "\n", "\n", "# models", "\n", "", "self", ".", "init_models", "(", ")", "\n", "self", ".", "load_model_checkpoints", "(", ")", "\n", "\n", "self", ".", "audio_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "tag_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "cf_encoder", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# optimizers", "\n", "", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "opt", "=", "torch", ".", "optim", ".", "Adam", "(", "chain", "(", "self", ".", "audio_encoder", ".", "parameters", "(", ")", ",", "self", ".", "tag_encoder", ".", "parameters", "(", ")", ")", ",", "lr", "=", "self", ".", "learning_rate", ",", "weight_decay", "=", "1e-4", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "opt", "=", "torch", ".", "optim", ".", "Adam", "(", "chain", "(", "self", ".", "audio_encoder", ".", "parameters", "(", ")", ",", "self", ".", "cf_encoder", ".", "parameters", "(", ")", ")", ",", "lr", "=", "self", ".", "learning_rate", ",", "weight_decay", "=", "1e-4", ")", "\n", "\n", "# tensorboard", "\n", "", "with", "SummaryWriter", "(", "log_dir", "=", "str", "(", "Path", "(", "'runs'", ",", "self", ".", "experiment_name", ")", ")", ",", "max_queue", "=", "100", ")", "as", "self", ".", "tb", ":", "\n", "\n", "# Training loop", "\n", "            ", "for", "epoch", "in", "range", "(", "self", ".", "last_epoch", "+", "1", ",", "self", ".", "epochs", "+", "1", ")", ":", "\n", "                ", "self", ".", "train_one_epoch", "(", "epoch", ")", "\n", "self", ".", "val", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train_one_epoch": [[123, 210], ["trainer_contrastive.Trainer.audio_encoder.train", "enumerate", "print", "print", "trainer_contrastive.Trainer.tb.add_scalar", "trainer_contrastive.Trainer.tag_encoder.train", "trainer_contrastive.Trainer.cf_encoder.train", "torch.utils.data.view().to", "trainer_contrastive.Trainer.audio_encoder", "trainer_contrastive.Trainer.opt.zero_grad", "utils.contrastive_loss.backward", "trainer_contrastive.Trainer.opt.step", "utils.contrastive_loss.item", "torch.save", "torch.tensor().to", "trainer_contrastive.Trainer.tag_encoder", "utils.contrastive_loss", "cf_embeddings.to", "trainer_contrastive.Trainer.cf_encoder", "utils.contrastive_loss", "trainer_contrastive.Trainer.tb.add_scalar", "print", "trainer_contrastive.Trainer.audio_encoder.state_dict", "str", "torch.save", "torch.save", "torch.utils.data.view", "np.zeros", "curr_labels.append", "utils.contrastive_loss.item", "pathlib.Path", "trainer_contrastive.Trainer.tag_encoder.state_dict", "str", "trainer_contrastive.Trainer.cf_encoder.state_dict", "str", "torch.tensor", "torch.tensor().to.unsqueeze", "len", "utils.contrastive_loss.item", "pathlib.Path", "pathlib.Path", "min", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss"], ["", "", "", "def", "train_one_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Train one epoch\n\n        \"\"\"", "\n", "self", ".", "audio_encoder", ".", "train", "(", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "tag_encoder", ".", "train", "(", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "cf_encoder", ".", "train", "(", ")", "\n", "\n", "# losses", "\n", "", "train_pairwise_loss", "=", "0", "\n", "\n", "for", "batch_idx", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "            ", "self", ".", "iteration_idx", "+=", "1", "\n", "\n", "# TODO: REMOVE THAT", "\n", "# tags should already in the tag_idxs form, except for the +1 to indexes to use idx 0 for no tag", "\n", "# We probably want to add some pre-processing in data_loader.py", "\n", "# e.g. select random tags from the 100, or select random sepctrogram chunk", "\n", "\"\"\"\n            tag_idxs = [\n                ([idx+1 for idx, val in enumerate(tag_v) if val]\n                 + self.max_num_tags*[0])[:self.max_num_tags]\n                for tag_v in tags\n            ]\n\n            \"\"\"", "\n", "# encode", "\n", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                ", "curr_labels", "=", "[", "]", "\n", "for", "curr_tags", "in", "tags", ":", "\n", "                    ", "non_neg", "=", "[", "i", "+", "1", "for", "i", "in", "curr_tags", "if", "i", "!=", "-", "1", "]", "\n", "new_tags", "=", "np", ".", "zeros", "(", "self", ".", "max_num_tags", ")", "\n", "#new_tags[:len(non_neg)] = np.random.choice(non_neg, min(self.max_num_tags, len(non_neg)), replace=False)", "\n", "new_tags", "[", ":", "min", "(", "len", "(", "non_neg", ")", ",", "10", ")", "]", "=", "non_neg", "[", ":", "10", "]", "\n", "curr_labels", ".", "append", "(", "new_tags", ")", "\n", "", "tags_input", "=", "torch", ".", "tensor", "(", "curr_labels", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#tags_input = tags.to(self.device)", "\n", "z_tags", ",", "attn", "=", "self", ".", "tag_encoder", "(", "tags_input", ",", "z_d_audio", ",", "mask", "=", "tags_input", ".", "unsqueeze", "(", "1", ")", ")", "\n", "pairwise_loss", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                ", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "z_cf", "=", "self", ".", "cf_encoder", "(", "cf_input", ")", "\n", "\n", "# contrastive loss", "\n", "pairwise_loss", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_cf", ",", "self", ".", "contrastive_temperature", ")", "\n", "\n", "# Optimize models", "\n", "", "self", ".", "opt", ".", "zero_grad", "(", ")", "\n", "pairwise_loss", ".", "backward", "(", ")", "\n", "self", ".", "opt", ".", "step", "(", ")", "\n", "\n", "\n", "train_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "\n", "# write to tensorboard", "\n", "# These are too many data to send to tensorboard, but it can be useful for debugging/developing", "\n", "if", "False", ":", "\n", "                ", "self", ".", "tb", ".", "add_scalar", "(", "\"iter/contrastive_pairwise_loss\"", ",", "pairwise_loss", ".", "item", "(", ")", ",", "self", ".", "iteration_idx", ")", "\n", "\n", "# logs per batch", "\n", "", "if", "batch_idx", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "print", "(", "'Train Epoch: {} [{}/{} ({:.0f}%)]\\tPairwise loss: {:.4f})'", ".", "format", "(", "\n", "epoch", ",", "batch_idx", "*", "len", "(", "data", ")", ",", "len", "(", "self", ".", "train_loader", ".", "dataset", ")", ",", "\n", "100.", "*", "batch_idx", "/", "len", "(", "self", ".", "train_loader", ")", ",", "\n", "pairwise_loss", ".", "item", "(", ")", "\n", ")", "\n", ")", "\n", "\n", "# epoch logs", "\n", "", "", "train_pairwise_loss", "=", "train_pairwise_loss", "/", "self", ".", "length_train_dataset", "*", "self", ".", "batch_size", "\n", "print", "(", "'====> Epoch: {}  Pairwise loss: {:.8f}'", ".", "format", "(", "epoch", ",", "train_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/train/sum\"", ",", "train_pairwise_loss", ",", "epoch", ")", "\n", "\n", "if", "epoch", "%", "self", ".", "save_model_every", "==", "0", ":", "\n", "            ", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_{epoch}.pt'", ")", ")", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                ", "torch", ".", "save", "(", "self", ".", "tag_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_{epoch}.pt'", ")", ")", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                ", "torch", ".", "save", "(", "self", ".", "cf_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'cf_encoder_att_epoch_{epoch}.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.val": [[211, 268], ["trainer_contrastive.Trainer.audio_encoder.eval", "print", "print", "trainer_contrastive.Trainer.tb.add_scalar", "trainer_contrastive.Trainer.tag_encoder.eval", "trainer_contrastive.Trainer.cf_encoder.eval", "torch.no_grad", "enumerate", "torch.utils.data.view().to", "cf_embeddings.to", "trainer_contrastive.Trainer.audio_encoder", "utils.contrastive_loss.item", "math.isinf", "math.isinf", "torch.save", "trainer_contrastive.Trainer.tag_encoder", "utils.contrastive_loss", "trainer_contrastive.Trainer.cf_encoder", "utils.contrastive_loss", "trainer_contrastive.Trainer.audio_encoder.state_dict", "str", "torch.save", "torch.save", "torch.utils.data.view", "pathlib.Path", "trainer_contrastive.Trainer.tag_encoder.state_dict", "str", "trainer_contrastive.Trainer.cf_encoder.state_dict", "str", "tags_input.unsqueeze", "pathlib.Path", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss"], ["", "", "", "def", "val", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\" Validation\n\n        \"\"\"", "\n", "# A little bit a code repeat here...", "\n", "self", ".", "audio_encoder", ".", "eval", "(", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "            ", "self", ".", "tag_encoder", ".", "eval", "(", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "            ", "self", ".", "cf_encoder", ".", "eval", "(", ")", "\n", "\n", "", "val_pairwise_loss", "=", "0", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "i", ",", "(", "data", ",", "tags", ",", "cf_embeddings", ",", "sound_ids", ")", "in", "enumerate", "(", "self", ".", "val_loader", ")", ":", "\n", "                ", "\"\"\"\n                curr_labels = []\n                for curr_tags in tags:\n                    non_neg = [i+1 for i in curr_tags if i != -1]\n                    new_tags = np.zeros(self.max_num_tags)\n                    #new_tags[:len(non_neg)] = np.random.choice(non_neg, min(self.max_num_tags, len(non_neg)), replace=False)\n                    new_tags[:min(len(non_neg), 10)] = non_neg[:10]\n                    curr_labels.append(new_tags)\n                tags_input = torch.tensor(curr_labels, dtype=torch.long).to(self.device)\n                \"\"\"", "\n", "\n", "x", "=", "data", ".", "view", "(", "-", "1", ",", "1", ",", "48", ",", "256", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "cf_input", "=", "cf_embeddings", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# encode", "\n", "z_audio", ",", "z_d_audio", "=", "self", ".", "audio_encoder", "(", "x", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                    ", "z_tags", ",", "attn", "=", "self", ".", "tag_encoder", "(", "tags_input", ",", "z_d_audio", ",", "mask", "=", "tags_input", ".", "unsqueeze", "(", "1", ")", ")", "\n", "# pairwise correspondence loss", "\n", "pairwise_loss", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_tags", ",", "self", ".", "contrastive_temperature", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                    ", "z_cf", "=", "self", ".", "cf_encoder", "(", "cf_input", ")", "\n", "# pairwise correspondence loss", "\n", "pairwise_loss", "=", "contrastive_loss", "(", "z_d_audio", ",", "z_cf", ",", "self", ".", "contrastive_temperature", ")", "\n", "\n", "", "val_pairwise_loss", "+=", "pairwise_loss", ".", "item", "(", ")", "\n", "\n", "", "", "val_pairwise_loss", "=", "val_pairwise_loss", "/", "self", ".", "length_val_dataset", "*", "self", ".", "batch_size", "\n", "\n", "print", "(", "'====> Val average pairwise loss: {:.4f}'", ".", "format", "(", "val_pairwise_loss", ")", ")", "\n", "print", "(", "'\\n\\n'", ")", "\n", "\n", "# tensorboard", "\n", "self", ".", "tb", ".", "add_scalar", "(", "\"contrastive_pairwise_loss/val/sum\"", ",", "val_pairwise_loss", ",", "epoch", ")", "\n", "if", "not", "(", "math", ".", "isinf", "(", "val_pairwise_loss", ")", "or", "math", ".", "isinf", "(", "val_pairwise_loss", ")", ")", ":", "\n", "            ", "if", "val_pairwise_loss", "<", "self", ".", "curr_min_val", ":", "\n", "                ", "self", ".", "curr_min_val", "=", "val_pairwise_loss", "\n", "torch", ".", "save", "(", "self", ".", "audio_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'audio_encoder_epoch_best.pt'", ")", ")", ")", "\n", "if", "self", ".", "encoder_type", "==", "\"gnr\"", ":", "\n", "                    ", "torch", ".", "save", "(", "self", ".", "tag_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'tag_encoder_att_epoch_best.pt'", ")", ")", ")", "\n", "", "if", "self", ".", "encoder_type", "==", "\"MF\"", ":", "\n", "                    ", "torch", ".", "save", "(", "self", ".", "cf_encoder", ".", "state_dict", "(", ")", ",", "str", "(", "Path", "(", "self", ".", "save_model_loc", ",", "self", ".", "experiment_name", ",", "f'cf_encoder_att_epoch_best.pt'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.__init__": [[25, 32], ["print"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "valmax", ",", "maxbar", ",", "title", ")", ":", "\n", "        ", "if", "valmax", "==", "0", ":", "valmax", "=", "1", "\n", "if", "maxbar", ">", "200", ":", "maxbar", "=", "200", "\n", "self", ".", "valmax", "=", "valmax", "\n", "self", ".", "maxbar", "=", "maxbar", "\n", "self", ".", "title", "=", "title", "\n", "print", "(", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.ProgressBar.update": [[33, 57], ["round", "int", "sys.stdout.write", "sys.stdout.flush", "float", "float", "float"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "avg_loss", "=", "0", ")", ":", "\n", "# format", "\n", "        ", "if", "val", ">", "self", ".", "valmax", ":", "val", "=", "self", ".", "valmax", "\n", "\n", "# process", "\n", "perc", "=", "round", "(", "(", "float", "(", "val", ")", "/", "float", "(", "self", ".", "valmax", ")", ")", "*", "100", ")", "\n", "scale", "=", "100.0", "/", "float", "(", "self", ".", "maxbar", ")", "\n", "bar", "=", "int", "(", "perc", "/", "scale", ")", "\n", "\n", "# render", "\n", "if", "avg_loss", ":", "\n", "# out = '\\r %20s [%s%s] %3d / %3d  cost: %.2f  r_loss: %.0f  l_loss: %.4f  clf_loss: %.4f' % (", "\n", "            ", "out", "=", "'\\r %20s [%s%s] %3d / %3d  loss: %.5f'", "%", "(", "\n", "self", ".", "title", ",", "\n", "'='", "*", "bar", ",", "' '", "*", "(", "self", ".", "maxbar", "-", "bar", ")", ",", "\n", "val", ",", "\n", "self", ".", "valmax", ",", "\n", "avg_loss", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "'\\r %20s [%s%s] %3d / %3d '", "%", "(", "self", ".", "title", ",", "'='", "*", "bar", ",", "' '", "*", "(", "self", ".", "maxbar", "-", "bar", ")", ",", "val", ",", "self", ".", "valmax", ")", "\n", "\n", "", "sys", ".", "stdout", ".", "write", "(", "out", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.logme": [[11, 19], ["functools.wraps", "print", "print", "print", "f", "f.__name__.upper"], "function", ["None"], ["def", "logme", "(", "f", ")", ":", "\n", "    ", "@", "functools", ".", "wraps", "(", "f", ")", "\n", "def", "wrapped", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "print", "(", "'\\n-----------------\\n'", ")", "\n", "print", "(", "'   MODEL: {}'", ".", "format", "(", "f", ".", "__name__", ".", "upper", "(", ")", ")", ")", "\n", "print", "(", "'\\n-----------------\\n'", ")", "\n", "return", "f", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "return", "wrapped", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.pad": [[59, 65], ["numpy.zeros", "numpy.random.rand().astype", "numpy.random.rand"], "function", ["None"], ["", "", "def", "pad", "(", "l", ",", "sr", ")", ":", "\n", "# 0-Pad 10 sec at fs hz and add little noise", "\n", "    ", "z", "=", "np", ".", "zeros", "(", "10", "*", "sr", ",", "dtype", "=", "'float32'", ")", "\n", "z", "[", ":", "l", ".", "size", "]", "=", "l", "\n", "z", "=", "z", "+", "5", "*", "1e-4", "*", "np", ".", "random", ".", "rand", "(", "z", ".", "size", ")", ".", "astype", "(", "'float32'", ")", "\n", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.compute_spectrogram": [[67, 81], ["librosa.feature.melspectrogram", "numpy.log", "librosa.load", "utils.pad", "sf.read", "librosa.core.resample", "numpy.finfo"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.melspectrogram", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.pad"], ["", "def", "compute_spectrogram", "(", "filename", ",", "sr", "=", "22000", ",", "n_mels", "=", "96", ")", ":", "\n", "# zero pad and compute log mel spec", "\n", "    ", "try", ":", "\n", "        ", "audio", ",", "sr", "=", "librosa", ".", "load", "(", "filename", ",", "sr", "=", "sr", ",", "res_type", "=", "'kaiser_fast'", ")", "\n", "", "except", ":", "\n", "        ", "audio", ",", "o_sr", "=", "sf", ".", "read", "(", "filename", ")", "\n", "audio", "=", "librosa", ".", "core", ".", "resample", "(", "audio", ",", "o_sr", ",", "sr", ")", "\n", "", "try", ":", "\n", "        ", "x", "=", "pad", "(", "audio", ",", "sr", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "x", "=", "audio", "\n", "", "audio_rep", "=", "librosa", ".", "feature", ".", "melspectrogram", "(", "y", "=", "x", ",", "sr", "=", "sr", ",", "hop_length", "=", "512", ",", "n_fft", "=", "1024", ",", "n_mels", "=", "n_mels", ",", "power", "=", "1.", ")", "\n", "audio_rep", "=", "np", ".", "log", "(", "audio_rep", "+", "np", ".", "finfo", "(", "np", ".", "float32", ")", ".", "eps", ")", "\n", "return", "audio_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.return_spectrogram_max_nrg_frame": [[83, 87], ["librosa.util.frame", "numpy.argmax", "numpy.asfortranarray", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "return_spectrogram_max_nrg_frame", "(", "spectrogram", ")", ":", "\n", "    ", "frames", "=", "librosa", ".", "util", ".", "frame", "(", "np", ".", "asfortranarray", "(", "spectrogram", ")", ",", "frame_length", "=", "96", ",", "hop_length", "=", "12", ")", "\n", "idx_max_nrg", "=", "np", ".", "argmax", "(", "np", ".", "sum", "(", "np", ".", "sum", "(", "frames", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", ")", "\n", "return", "frames", "[", ":", ",", ":", ",", "idx_max_nrg", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.return_spectrogram_3_max_nrg_frames": [[89, 93], ["librosa.util.frame", "numpy.asfortranarray", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "return_spectrogram_3_max_nrg_frames", "(", "spectrogram", ")", ":", "\n", "    ", "frames", "=", "librosa", ".", "util", ".", "frame", "(", "np", ".", "asfortranarray", "(", "spectrogram", ")", ",", "frame_length", "=", "96", ",", "hop_length", "=", "12", ")", "\n", "idxes_max_nrg", "=", "(", "-", "np", ".", "sum", "(", "np", ".", "sum", "(", "frames", ",", "axis", "=", "0", ")", ",", "axis", "=", "0", ")", ")", ".", "argsort", "(", ")", "[", ":", "3", "]", "\n", "return", "frames", "[", ":", ",", ":", ",", "idxes_max_nrg", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.spectrogram_to_audio": [[95, 99], ["numpy.exp", "librosa.feature.inverse.mel_to_audio", "librosa.output.write_wav"], "function", ["None"], ["", "def", "spectrogram_to_audio", "(", "filename", ",", "y", ",", "sr", "=", "22000", ")", ":", "\n", "    ", "y", "=", "np", ".", "exp", "(", "y", ")", "\n", "x", "=", "librosa", ".", "feature", ".", "inverse", ".", "mel_to_audio", "(", "y", ",", "sr", "=", "sr", ",", "n_fft", "=", "1024", ",", "hop_length", "=", "512", ",", "power", "=", "1.", ")", "\n", "librosa", ".", "output", ".", "write_wav", "(", "filename", ",", "x", ",", "sr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.extract_spectrogram": [[101, 107], ["utils.cut_audio", "utils.melspectrogram"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.cut_audio", "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.melspectrogram"], ["", "def", "extract_spectrogram", "(", "filename", ",", "sr", "=", "16000", ",", "n_mels", "=", "48", ")", ":", "\n", "    ", "audio", "=", "cut_audio", "(", "filename", ",", "sampleRate", "=", "sr", ",", "segment_duration", "=", "29.1", ")", "\n", "frames", "=", "melspectrogram", "(", "audio", ",", "sampleRate", "=", "sr", ",", "frameSize", "=", "512", ",", "hopSize", "=", "256", ",", "numberBands", "=", "[", "48", "]", ",", "\n", "warpingFormula", "=", "'slaneyMel'", ",", "window", "=", "'hann'", ",", "normalize", "=", "'unit_tri'", ")", "\n", "\n", "return", "frames", "[", "'mel_48_db'", "]", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.melspectrogram": [[109, 148], ["es.Windowing", "es.Spectrum", "es.UnaryOperator", "es.UnaryOperator", "es.UnaryOperator", "essentia.Pool", "es.FrameGenerator", "es.MelBands", "es.Spectrum.", "es.Windowing.", "essentia.Pool.add", "essentia.Pool.add", "essentia.Pool.add", "es.UnaryOperator.", "es.UnaryOperator.", "es.UnaryOperator.", "str", "str", "str"], "function", ["None"], ["", "def", "melspectrogram", "(", "audio", ",", "sampleRate", "=", "44100", ",", "frameSize", "=", "2048", ",", "hopSize", "=", "1024", ",", "\n", "window", "=", "'blackmanharris62'", ",", "zeroPadding", "=", "0", ",", "center", "=", "True", ",", "\n", "numberBands", "=", "[", "128", ",", "96", ",", "48", ",", "32", ",", "24", ",", "16", ",", "8", "]", ",", "\n", "lowFrequencyBound", "=", "0", ",", "highFrequencyBound", "=", "None", ",", "\n", "weighting", "=", "'linear'", ",", "warpingFormula", "=", "'slaneyMel'", ",", "normalize", "=", "'unit_tri'", ")", ":", "\n", "\n", "    ", "if", "highFrequencyBound", "is", "None", ":", "\n", "        ", "highFrequencyBound", "=", "sampleRate", "/", "2", "\n", "\n", "", "windowing", "=", "es", ".", "Windowing", "(", "type", "=", "window", ",", "normalized", "=", "False", ",", "zeroPadding", "=", "zeroPadding", ")", "\n", "spectrum", "=", "es", ".", "Spectrum", "(", ")", "\n", "melbands", "=", "{", "}", "\n", "for", "nBands", "in", "numberBands", ":", "\n", "        ", "melbands", "[", "nBands", "]", "=", "es", ".", "MelBands", "(", "numberBands", "=", "nBands", ",", "\n", "sampleRate", "=", "sampleRate", ",", "\n", "lowFrequencyBound", "=", "lowFrequencyBound", ",", "\n", "highFrequencyBound", "=", "highFrequencyBound", ",", "\n", "inputSize", "=", "(", "frameSize", "+", "zeroPadding", ")", "//", "2", "+", "1", ",", "\n", "weighting", "=", "weighting", ",", "\n", "normalize", "=", "normalize", ",", "\n", "warpingFormula", "=", "warpingFormula", ",", "\n", "type", "=", "'power'", ")", "\n", "", "norm10k", "=", "es", ".", "UnaryOperator", "(", "type", "=", "'identity'", ",", "shift", "=", "1", ",", "scale", "=", "10000", ")", "\n", "log10", "=", "es", ".", "UnaryOperator", "(", "type", "=", "'log10'", ")", "\n", "amp2db", "=", "es", ".", "UnaryOperator", "(", "type", "=", "'lin2db'", ",", "scale", "=", "2", ")", "\n", "\n", "results", "=", "essentia", ".", "Pool", "(", ")", "\n", "\n", "for", "frame", "in", "es", ".", "FrameGenerator", "(", "audio", ",", "frameSize", "=", "frameSize", ",", "hopSize", "=", "hopSize", ",", "\n", "startFromZero", "=", "not", "center", ")", ":", "\n", "        ", "spectrumFrame", "=", "spectrum", "(", "windowing", "(", "frame", ")", ")", "\n", "\n", "for", "nBands", "in", "numberBands", ":", "\n", "            ", "melFrame", "=", "melbands", "[", "nBands", "]", "(", "spectrumFrame", ")", "\n", "results", ".", "add", "(", "'mel_'", "+", "str", "(", "nBands", ")", "+", "'_db'", ",", "amp2db", "(", "melFrame", ")", ")", "\n", "results", ".", "add", "(", "'mel_'", "+", "str", "(", "nBands", ")", "+", "'_log1+10kx'", ",", "log10", "(", "norm10k", "(", "melFrame", ")", ")", ")", "\n", "results", ".", "add", "(", "'mel_'", "+", "str", "(", "nBands", ")", ",", "melFrame", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.cut_audio": [[150, 166], ["es.MonoLoader", "round", "len", "ValueError", "len", "len"], "function", ["None"], ["", "def", "cut_audio", "(", "filename", ",", "sampleRate", "=", "44100", ",", "segment_duration", "=", "None", ")", ":", "\n", "\n", "    ", "audio", "=", "es", ".", "MonoLoader", "(", "filename", "=", "filename", ",", "sampleRate", "=", "sampleRate", ")", "(", ")", "\n", "\n", "if", "segment_duration", ":", "\n", "        ", "segment_duration", "=", "round", "(", "segment_duration", "*", "sampleRate", ")", "\n", "segment_start", "=", "(", "len", "(", "audio", ")", "-", "segment_duration", ")", "//", "2", "\n", "segment_end", "=", "segment_start", "+", "segment_duration", "\n", "", "else", ":", "\n", "        ", "segment_start", "=", "0", "\n", "segment_end", "=", "len", "(", "audio", ")", "\n", "\n", "", "if", "segment_start", "<", "0", "or", "segment_end", ">", "len", "(", "audio", ")", ":", "\n", "      ", "raise", "ValueError", "(", "'Segment duration is larger than the input audio duration'", ")", "\n", "\n", "", "return", "audio", "[", "segment_start", ":", "segment_end", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.kullback_leibler": [[168, 179], ["y.add().log", "y_hat.add().log", "y.add", "y_hat.add"], "function", ["None"], ["", "def", "kullback_leibler", "(", "y_hat", ",", "y", ")", ":", "\n", "    ", "\"\"\"Generalized Kullback Leibler divergence.\n    :param y_hat: The predicted distribution.\n    :type y_hat: torch.Tensor\n    :param y: The true distribution.\n    :type y: torch.Tensor\n    :return: The generalized Kullback Leibler divergence\\\n             between predicted and true distributions.\n    :rtype: torch.Tensor\n    \"\"\"", "\n", "return", "(", "y", "*", "(", "y", ".", "add", "(", "1e-5", ")", ".", "log", "(", ")", "-", "y_hat", ".", "add", "(", "1e-5", ")", ".", "log", "(", ")", ")", "+", "(", "y_hat", "-", "y", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.embeddings_to_cosine_similarity_matrix": [[181, 189], ["torch.matmul", "torch.norm", "z.t", "torch.norm.unsqueeze", "torch.norm.unsqueeze"], "function", ["None"], ["", "def", "embeddings_to_cosine_similarity_matrix", "(", "z", ")", ":", "\n", "    ", "\"\"\"Converts a a tensor of n embeddings to an (n, n) tensor of similarities.\n    \"\"\"", "\n", "cosine_similarity", "=", "torch", ".", "matmul", "(", "z", ",", "z", ".", "t", "(", ")", ")", "\n", "embedding_norms", "=", "torch", ".", "norm", "(", "z", ",", "p", "=", "2", ",", "dim", "=", "1", ")", "\n", "embedding_norms_mat", "=", "embedding_norms", ".", "unsqueeze", "(", "0", ")", "*", "embedding_norms", ".", "unsqueeze", "(", "1", ")", "\n", "cosine_similarity", "=", "cosine_similarity", "/", "(", "embedding_norms_mat", ")", "\n", "return", "cosine_similarity", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.contrastive_loss": [[191, 210], ["torch.cat", "utils.embeddings_to_cosine_similarity_matrix", "int", "torch.exp", "torch.exp.sum", "torch.cat", "torch.log().neg().mean", "s[].diag", "s[].diag", "torch.log().neg", "torch.eye().cuda", "torch.eye", "torch.log", "torch.eye", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.utils.embeddings_to_cosine_similarity_matrix"], ["", "def", "contrastive_loss", "(", "z_audio", ",", "z_tag", ",", "t", "=", "1", ")", ":", "\n", "    ", "\"\"\"Computes contrastive loss following the paper:\n        A Simple Framework for Contrastive Learning of Visual Representations\n        https://arxiv.org/pdf/2002.05709v1.pdf\n        TODO: make it robust to NaN (with low values of t it happens).\n        e.g Cast to double float for exp calculation.\n    \"\"\"", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z_audio", ",", "z_tag", ")", ",", "dim", "=", "0", ")", "\n", "s", "=", "embeddings_to_cosine_similarity_matrix", "(", "z", ")", "\n", "N", "=", "int", "(", "s", ".", "shape", "[", "0", "]", "/", "2", ")", "\n", "s", "=", "torch", ".", "exp", "(", "s", "/", "t", ")", "\n", "try", ":", "\n", "        ", "s", "=", "s", "*", "(", "1", "-", "torch", ".", "eye", "(", "len", "(", "s", ")", ",", "len", "(", "s", ")", ")", ".", "cuda", "(", ")", ")", "\n", "# s[range(len(s)), range(len(s))] = torch.zeros((len(s),)).cuda()", "\n", "", "except", "AssertionError", ":", "\n", "        ", "s", "=", "s", "*", "(", "1", "-", "torch", ".", "eye", "(", "len", "(", "s", ")", ",", "len", "(", "s", ")", ")", ")", "\n", "", "denom", "=", "s", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "num", "=", "torch", ".", "cat", "(", "(", "s", "[", ":", "N", ",", "N", ":", "]", ".", "diag", "(", ")", ",", "s", "[", "N", ":", ",", ":", "N", "]", ".", "diag", "(", ")", ")", ",", "dim", "=", "0", ")", "\n", "return", "torch", ".", "log", "(", "(", "num", "/", "denom", ")", "+", "1e-5", ")", ".", "neg", "(", ")", ".", "mean", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.single_train.main": [[7, 13], ["json.load", "print", "print", "trainer_contrastive.Trainer", "trainer_contrastive.Trainer.train", "open", "json.dumps"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.None.trainer_contrastive.Trainer.train"], ["def", "main", "(", "config_file", ")", ":", "\n", "    ", "params", "=", "json", ".", "load", "(", "open", "(", "config_file", ",", "'rb'", ")", ")", "\n", "print", "(", "\"Training models with params:\"", ")", "\n", "print", "(", "json", ".", "dumps", "(", "params", ",", "separators", "=", "(", "\"\\n\"", ",", "\": \"", ")", ",", "indent", "=", "4", ")", ")", "\n", "trainer", "=", "Trainer", "(", "params", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.scripts.create_dataset.train_mf": [[49, 60], ["lightfm.LightFM", "model.fit.fit", "model.fit.get_user_representations", "model.fit.get_item_representations", "numpy.concatenate", "numpy.concatenate", "numpy.reshape", "numpy.ones"], "function", ["None"], ["def", "train_mf", "(", "impl_train_data", ",", "dims", "=", "200", ",", "epochs", "=", "50", ",", "max_sampled", "=", "10", ",", "lr", "=", "0.05", ")", ":", "\n", "\n", "    ", "model", "=", "LightFM", "(", "loss", "=", "'warp'", ",", "no_components", "=", "dims", ",", "max_sampled", "=", "max_sampled", ",", "learning_rate", "=", "lr", ",", "random_state", "=", "42", ")", "\n", "model", "=", "model", ".", "fit", "(", "impl_train_data", ",", "epochs", "=", "epochs", ",", "num_threads", "=", "24", ")", "\n", "\n", "user_biases", ",", "user_embeddings", "=", "model", ".", "get_user_representations", "(", ")", "\n", "item_biases", ",", "item_embeddings", "=", "model", ".", "get_item_representations", "(", ")", "\n", "item_vec", "=", "np", ".", "concatenate", "(", "(", "item_embeddings", ",", "np", ".", "reshape", "(", "item_biases", ",", "(", "1", ",", "-", "1", ")", ")", ".", "T", ")", ",", "axis", "=", "1", ")", "\n", "user_vec", "=", "np", ".", "concatenate", "(", "(", "user_embeddings", ",", "np", ".", "ones", "(", "(", "1", ",", "user_biases", ".", "shape", "[", "0", "]", ")", ")", ".", "T", ")", ",", "axis", "=", "1", ")", "\n", "\n", "return", "user_vec", ",", "item_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.scripts.create_dataset.load_cf_data": [[61, 87], ["json.load", "scipy.sparse.coo_matrix", "json.dump", "json.dump", "create_dataset.train_mf", "numpy.save", "open", "playlists_ids.append", "open", "open", "open", "os.path.join", "os.path.join", "os.path.join", "str", "cols.append", "rows.append", "data.append", "playlists_test[].append", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.andrebola_contrastive-mir-learning.scripts.create_dataset.train_mf"], ["", "def", "load_cf_data", "(", "train_file", ",", "tracks_ids", ")", ":", "\n", "    ", "train_playlists", "=", "json", ".", "load", "(", "open", "(", "train_file", ",", "encoding", "=", "'utf-8'", ")", ")", "\n", "\n", "rows", "=", "[", "]", "\n", "cols", "=", "[", "]", "\n", "data", "=", "[", "]", "\n", "playlists_ids", "=", "[", "]", "\n", "playlists_test", "=", "{", "}", "\n", "for", "playlist", "in", "train_playlists", ":", "\n", "        ", "for", "track", "in", "playlist", "[", "'songs'", "]", ":", "\n", "            ", "if", "str", "(", "track", ")", "in", "tracks_ids", ":", "\n", "                ", "cols", ".", "append", "(", "tracks_ids", "[", "str", "(", "track", ")", "]", ")", "\n", "rows", ".", "append", "(", "len", "(", "playlists_ids", ")", ")", "\n", "data", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "if", "str", "(", "playlist", "[", "'id'", "]", ")", "not", "in", "playlists_test", ":", "\n", "                    ", "playlists_test", "[", "str", "(", "playlist", "[", "'id'", "]", ")", "]", "=", "[", "]", "\n", "", "playlists_test", "[", "str", "(", "playlist", "[", "'id'", "]", ")", "]", ".", "append", "(", "str", "(", "track", ")", ")", "\n", "", "", "playlists_ids", ".", "append", "(", "playlist", "[", "'id'", "]", ")", "\n", "", "train_coo", "=", "sparse", ".", "coo_matrix", "(", "(", "data", ",", "(", "rows", ",", "cols", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "json", ".", "dump", "(", "playlists_test", ",", "open", "(", "os", ".", "path", ".", "join", "(", "SAVE_DATASET_LOCATION", ",", "'test_playlists.json'", ")", ",", "'w'", ")", ")", "\n", "json", ".", "dump", "(", "playlists_ids", ",", "open", "(", "os", ".", "path", ".", "join", "(", "SAVE_DATASET_LOCATION", ",", "'all_playlists_ids.json'", ")", ",", "'w'", ")", ")", "\n", "user_vec", ",", "item_vec", "=", "train_mf", "(", "train_coo", ",", "dims", "=", "CF_DIMS", ",", "epochs", "=", "CF_EPOCHS", ",", "max_sampled", "=", "CF_MAX_SAMPLED", ",", "lr", "=", "CF_LR", ")", "\n", "np", ".", "save", "(", "open", "(", "os", ".", "path", ".", "join", "(", "SAVE_DATASET_LOCATION", ",", "'playlists_cf_vec.npy'", ")", ",", "'wb'", ")", ",", "user_vec", ")", "\n", "\n", "return", "item_vec", "\n", "\n"]]}