{"home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.parse_sequence_example": [[27, 91], ["tensorflow.parse_single_sequence_example", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenSequenceFeature", "tensorflow.FixedLenSequenceFeature", "tensorflow.FixedLenSequenceFeature", "tensorflow.FixedLenSequenceFeature", "tensorflow.FixedLenSequenceFeature"], "function", ["None"], ["def", "parse_sequence_example", "(", "serialized", ",", "image_feature", ",", "caption_feature", ")", ":", "\n", "  ", "context", ",", "sequence", "=", "tf", ".", "parse_single_sequence_example", "(", "\n", "serialized", ",", "\n", "context_features", "=", "{", "\n", "\"image/data_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "string", ")", ",", "\n", "\"image/image_id_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/order_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/story_id_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/album_id_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_id_0\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\n", "\"image/data_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "string", ")", ",", "\n", "\"image/image_id_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/order_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/story_id_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/album_id_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_id_1\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\n", "\"image/data_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "string", ")", ",", "\n", "\"image/image_id_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/order_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/story_id_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/album_id_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_id_2\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\n", "\"image/data_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "string", ")", ",", "\n", "\"image/image_id_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/order_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/story_id_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/album_id_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_id_3\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\n", "\"image/data_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "string", ")", ",", "\n", "\"image/image_id_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/order_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/story_id_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/album_id_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_id_4\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", ",", "\n", "sequence_features", "=", "{", "\n", "\"image/caption_ids_0\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_ids_1\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_ids_2\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_ids_3\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "\"image/caption_ids_4\"", ":", "tf", ".", "FixedLenSequenceFeature", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", ",", "\n", "}", ")", "\n", "\n", "encoded_image_0", "=", "context", "[", "\"image/data_0\"", "]", "\n", "caption_0", "=", "sequence", "[", "\"image/caption_ids_0\"", "]", "\n", "\n", "encoded_image_1", "=", "context", "[", "\"image/data_1\"", "]", "\n", "caption_1", "=", "sequence", "[", "\"image/caption_ids_1\"", "]", "\n", "\n", "encoded_image_2", "=", "context", "[", "\"image/data_2\"", "]", "\n", "caption_2", "=", "sequence", "[", "\"image/caption_ids_2\"", "]", "\n", "\n", "encoded_image_3", "=", "context", "[", "\"image/data_3\"", "]", "\n", "caption_3", "=", "sequence", "[", "\"image/caption_ids_3\"", "]", "\n", "\n", "encoded_image_4", "=", "context", "[", "\"image/data_4\"", "]", "\n", "caption_4", "=", "sequence", "[", "\"image/caption_ids_4\"", "]", "\n", "\n", "\n", "return", "encoded_image_0", ",", "caption_0", ",", "encoded_image_1", ",", "caption_1", ",", "encoded_image_2", ",", "caption_2", ",", "encoded_image_3", ",", "caption_3", ",", "encoded_image_4", ",", "caption_4", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.prefetch_input_data": [[93, 160], ["file_pattern.split", "range", "tensorflow.train.queue_runner.add_queue_runner", "tensorflow.summary.scalar", "data_files.extend", "tensorflow.logging.fatal", "tensorflow.logging.info", "tensorflow.train.string_input_producer", "tensorflow.FIFOQueue", "tensorflow.train.string_input_producer", "tensorflow.FIFOQueue", "reader.read", "enqueue_ops.append", "tensorflow.train.queue_runner.QueueRunner", "tensorflow.gfile.Glob", "len", "tf.FIFOQueue.enqueue", "tensorflow.cast", "tf.FIFOQueue.size"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.size"], ["", "def", "prefetch_input_data", "(", "reader", ",", "\n", "file_pattern", ",", "\n", "is_training", ",", "\n", "batch_size", ",", "\n", "values_per_shard", ",", "\n", "input_queue_capacity_factor", "=", "15", ",", "\n", "num_reader_threads", "=", "1", ",", "\n", "shard_queue_name", "=", "\"filename_queue\"", ",", "\n", "value_queue_name", "=", "\"input_queue\"", ")", ":", "\n", "  ", "\"\"\"Prefetches string values from disk into an input queue.\n\n  In training the capacity of the queue is important because a larger queue\n  means better mixing of training examples between shards. The minimum number of\n  values kept in the queue is values_per_shard * input_queue_capacity_factor,\n  where input_queue_memory factor should be chosen to trade-off better mixing\n  with memory usage.\n\n  Args:\n    reader: Instance of tf.ReaderBase.\n    file_pattern: Comma-separated list of file patterns (e.g.\n        /tmp/train_data-?????-of-00100).\n    is_training: Boolean; whether prefetching for training or eval.\n    batch_size: Model batch size used to determine queue capacity.\n    values_per_shard: Approximate number of values per shard.\n    input_queue_capacity_factor: Minimum number of values to keep in the queue\n      in multiples of values_per_shard. See comments above.\n    num_reader_threads: Number of reader threads to fill the queue.\n    shard_queue_name: Name for the shards filename queue.\n    value_queue_name: Name for the values input queue.\n\n  Returns:\n    A Queue containing prefetched string values.\n  \"\"\"", "\n", "data_files", "=", "[", "]", "\n", "for", "pattern", "in", "file_pattern", ".", "split", "(", "\",\"", ")", ":", "\n", "    ", "data_files", ".", "extend", "(", "tf", ".", "gfile", ".", "Glob", "(", "pattern", ")", ")", "\n", "", "if", "not", "data_files", ":", "\n", "    ", "tf", ".", "logging", ".", "fatal", "(", "\"Found no input files matching %s\"", ",", "file_pattern", ")", "\n", "", "else", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"Prefetching values from %d files matching %s\"", ",", "\n", "len", "(", "data_files", ")", ",", "file_pattern", ")", "\n", "\n", "", "if", "is_training", ":", "\n", "    ", "filename_queue", "=", "tf", ".", "train", ".", "string_input_producer", "(", "data_files", ",", "shuffle", "=", "False", ",", "capacity", "=", "15", ",", "name", "=", "shard_queue_name", ")", "\n", "capacity", "=", "values_per_shard", "*", "batch_size", "\n", "values_queue", "=", "tf", ".", "FIFOQueue", "(", "capacity", "=", "capacity", ",", "dtypes", "=", "[", "tf", ".", "string", "]", ",", "name", "=", "\"fifoTrain_\"", "+", "value_queue_name", ")", "\n", "", "else", ":", "\n", "    ", "filename_queue", "=", "tf", ".", "train", ".", "string_input_producer", "(", "\n", "data_files", ",", "shuffle", "=", "False", ",", "capacity", "=", "1", ",", "name", "=", "shard_queue_name", ")", "\n", "capacity", "=", "values_per_shard", "*", "batch_size", "\n", "values_queue", "=", "tf", ".", "FIFOQueue", "(", "capacity", "=", "capacity", ",", "dtypes", "=", "[", "tf", ".", "string", "]", ",", "name", "=", "\"fifoEval_\"", "+", "value_queue_name", ")", "\n", "\n", "", "enqueue_ops", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_reader_threads", ")", ":", "\n", "    ", "_", ",", "value", "=", "reader", ".", "read", "(", "filename_queue", ")", "\n", "enqueue_ops", ".", "append", "(", "values_queue", ".", "enqueue", "(", "[", "value", "]", ")", ")", "\n", "\n", "\n", "", "tf", ".", "train", ".", "queue_runner", ".", "add_queue_runner", "(", "tf", ".", "train", ".", "queue_runner", ".", "QueueRunner", "(", "\n", "values_queue", ",", "enqueue_ops", ")", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "\n", "\"queue/%s/fraction_of_%d_full\"", "%", "(", "values_queue", ".", "name", ",", "capacity", ")", ",", "\n", "tf", ".", "cast", "(", "values_queue", ".", "size", "(", ")", ",", "tf", ".", "float32", ")", "*", "(", "1.", "/", "capacity", ")", ")", "\n", "\n", "return", "values_queue", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.batch_with_dynamic_pad": [[162, 301], ["tensorflow.train.batch_join", "tensorflow.expand_dims", "tensorflow.slice", "tensorflow.slice", "tensorflow.ones", "tensorflow.expand_dims", "tensorflow.slice", "tensorflow.slice", "tensorflow.ones", "tensorflow.expand_dims", "tensorflow.slice", "tensorflow.slice", "tensorflow.ones", "tensorflow.expand_dims", "tensorflow.slice", "tensorflow.slice", "tensorflow.ones", "tensorflow.expand_dims", "tensorflow.slice", "tensorflow.slice", "tensorflow.ones", "enqueue_list.append", "tensorflow.add", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.add", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.add", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.add", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.add", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.shape", "tensorflow.subtract", "tensorflow.shape", "tensorflow.subtract", "tensorflow.shape", "tensorflow.subtract", "tensorflow.shape", "tensorflow.subtract", "tensorflow.shape", "tensorflow.subtract", "tensorflow.reduce_sum", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.reduce_mean"], "function", ["None"], ["", "def", "batch_with_dynamic_pad", "(", "images_and_captions", ",", "\n", "batch_size", ",", "\n", "queue_capacity", ",", "\n", "add_summaries", "=", "True", ")", ":", "\n", "  ", "\"\"\"Batches input images and captions.\n\n  This function splits the caption into an input sequence and a target sequence,\n  where the target sequence is the input sequence right-shifted by 1. Input and\n  target sequences are batched and padded up to the maximum length of sequences\n  in the batch. A mask is created to distinguish real words from padding words.\n\n  Example:\n    Actual captions in the batch ('-' denotes padded character):\n      [\n        [ 1 2 3 4 5 ],\n        [ 1 2 3 4 - ],\n        [ 1 2 3 - - ],\n      ]\n\n    input_seqs:\n      [\n        [ 1 2 3 4 ],\n        [ 1 2 3 - ],\n        [ 1 2 - - ],\n      ]\n\n    target_seqs:\n      [\n        [ 2 3 4 5 ],\n        [ 2 3 4 - ],\n        [ 2 3 - - ],\n      ]\n\n    mask:\n      [\n        [ 1 1 1 1 ],\n        [ 1 1 1 0 ],\n        [ 1 1 0 0 ],\n      ]\n\n  Args:\n    images_and_captions: A list of pairs [image, caption], where image is a\n      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n      any length. Each pair will be processed and added to the queue in a\n      separate thread.\n    batch_size: Batch size.\n    queue_capacity: Queue capacity.\n    add_summaries: If true, add caption length summaries.\n\n  Returns:\n    images: A Tensor of shape [batch_size, height, width, channels].\n    input_seqs: An int32 Tensor of shape [batch_size, padded_length * 5].\n    target_seqs: An int32 Tensor of shape [batch_size, padded_length * 5].\n    mask: An int32 0/1 Tensor of shape [batch_size, padded_length * 5].  \n  \n    tf.subtract = entre dos tensores x & y: Returns x - y element-wise.\n    tf.expand_dims = Inserts a dimension of 1 into a tensor's shape. \n        Given a tensor input, this operation inserts a dimension of 1 at the dimension index axis of input's shape. \n        The dimension index axis starts at zero; if you specify a negative number for axis it is counted backward from the end.\n        This operation is useful if you want to add a batch dimension to a single element. \n        For example, if you have a single image of shape [height, width, channels], \n        you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels]. \n    tf.slice = Extracts a slice from a tensor. \n        This operation extracts a slice of size size from a tensor input starting at the location specified by begin. \n        The slice size is represented as a tensor shape, where size[i] is the number of elements of the 'i'th dimension \n        of input that you want to slice. The starting location (begin) for the slice is represented as an offset in \n        each dimension of input. In other words, begin[i] is the offset into the 'i'th dimension of input that you want to slice from.\n    tf.ones = Creates a tensor with all elements set to 1.\n  \"\"\"", "\n", "\n", "enqueue_list", "=", "[", "]", "\n", "for", "image_0", ",", "caption_0", ",", "image_1", ",", "caption_1", ",", "image_2", ",", "caption_2", ",", "image_3", ",", "caption_3", ",", "image_4", ",", "caption_4", "in", "images_and_captions", ":", "\n", "\n", "    ", "caption_length_0", "=", "tf", ".", "shape", "(", "caption_0", ")", "[", "0", "]", "\n", "input_length_0", "=", "tf", ".", "expand_dims", "(", "tf", ".", "subtract", "(", "caption_length_0", ",", "1", ")", ",", "0", ")", "\n", "input_seq_0", "=", "tf", ".", "slice", "(", "caption_0", ",", "[", "0", "]", ",", "input_length_0", ")", "\n", "target_seq_0", "=", "tf", ".", "slice", "(", "caption_0", ",", "[", "1", "]", ",", "input_length_0", ")", "\n", "indicator_0", "=", "tf", ".", "ones", "(", "input_length_0", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "caption_length_1", "=", "tf", ".", "shape", "(", "caption_1", ")", "[", "0", "]", "\n", "input_length_1", "=", "tf", ".", "expand_dims", "(", "tf", ".", "subtract", "(", "caption_length_1", ",", "1", ")", ",", "0", ")", "\n", "input_seq_1", "=", "tf", ".", "slice", "(", "caption_1", ",", "[", "0", "]", ",", "input_length_1", ")", "\n", "target_seq_1", "=", "tf", ".", "slice", "(", "caption_1", ",", "[", "1", "]", ",", "input_length_1", ")", "\n", "indicator_1", "=", "tf", ".", "ones", "(", "input_length_1", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "caption_length_2", "=", "tf", ".", "shape", "(", "caption_2", ")", "[", "0", "]", "\n", "input_length_2", "=", "tf", ".", "expand_dims", "(", "tf", ".", "subtract", "(", "caption_length_2", ",", "1", ")", ",", "0", ")", "\n", "input_seq_2", "=", "tf", ".", "slice", "(", "caption_2", ",", "[", "0", "]", ",", "input_length_2", ")", "\n", "target_seq_2", "=", "tf", ".", "slice", "(", "caption_2", ",", "[", "1", "]", ",", "input_length_2", ")", "\n", "indicator_2", "=", "tf", ".", "ones", "(", "input_length_2", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "caption_length_3", "=", "tf", ".", "shape", "(", "caption_3", ")", "[", "0", "]", "\n", "input_length_3", "=", "tf", ".", "expand_dims", "(", "tf", ".", "subtract", "(", "caption_length_3", ",", "1", ")", ",", "0", ")", "\n", "input_seq_3", "=", "tf", ".", "slice", "(", "caption_3", ",", "[", "0", "]", ",", "input_length_3", ")", "\n", "target_seq_3", "=", "tf", ".", "slice", "(", "caption_3", ",", "[", "1", "]", ",", "input_length_3", ")", "\n", "indicator_3", "=", "tf", ".", "ones", "(", "input_length_3", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "caption_length_4", "=", "tf", ".", "shape", "(", "caption_4", ")", "[", "0", "]", "\n", "input_length_4", "=", "tf", ".", "expand_dims", "(", "tf", ".", "subtract", "(", "caption_length_4", ",", "1", ")", ",", "0", ")", "\n", "input_seq_4", "=", "tf", ".", "slice", "(", "caption_4", ",", "[", "0", "]", ",", "input_length_4", ")", "\n", "target_seq_4", "=", "tf", ".", "slice", "(", "caption_4", ",", "[", "1", "]", ",", "input_length_4", ")", "\n", "indicator_4", "=", "tf", ".", "ones", "(", "input_length_4", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "enqueue_list", ".", "append", "(", "[", "image_0", ",", "input_seq_0", ",", "target_seq_0", ",", "indicator_0", ",", "image_1", ",", "input_seq_1", ",", "target_seq_1", ",", "indicator_1", ",", "image_2", ",", "input_seq_2", ",", "target_seq_2", ",", "indicator_2", ",", "image_3", ",", "input_seq_3", ",", "target_seq_3", ",", "indicator_3", ",", "image_4", ",", "input_seq_4", ",", "target_seq_4", ",", "indicator_4", "]", ")", "\n", "\n", "", "images_0", ",", "input_seqs_0", ",", "target_seqs_0", ",", "mask_0", ",", "images_1", ",", "input_seqs_1", ",", "target_seqs_1", ",", "mask_1", ",", "images_2", ",", "input_seqs_2", ",", "target_seqs_2", ",", "mask_2", ",", "images_3", ",", "input_seqs_3", ",", "target_seqs_3", ",", "mask_3", ",", "images_4", ",", "input_seqs_4", ",", "target_seqs_4", ",", "mask_4", "=", "tf", ".", "train", ".", "batch_join", "(", "\n", "enqueue_list", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "capacity", "=", "queue_capacity", ",", "\n", "dynamic_pad", "=", "True", ",", "\n", "name", "=", "\"batch_and_pad\"", ")", "\n", "\n", "if", "add_summaries", ":", "\n", "    ", "lengths_0", "=", "tf", ".", "add", "(", "tf", ".", "reduce_sum", "(", "mask_0", ",", "1", ")", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_min\"", ",", "tf", ".", "reduce_min", "(", "lengths_0", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_max\"", ",", "tf", ".", "reduce_max", "(", "lengths_0", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_mean\"", ",", "tf", ".", "reduce_mean", "(", "lengths_0", ")", ")", "\n", "\n", "lengths_1", "=", "tf", ".", "add", "(", "tf", ".", "reduce_sum", "(", "mask_1", ",", "1", ")", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_min\"", ",", "tf", ".", "reduce_min", "(", "lengths_1", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_max\"", ",", "tf", ".", "reduce_max", "(", "lengths_1", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_mean\"", ",", "tf", ".", "reduce_mean", "(", "lengths_1", ")", ")", "\n", "\n", "lengths_2", "=", "tf", ".", "add", "(", "tf", ".", "reduce_sum", "(", "mask_2", ",", "1", ")", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_min\"", ",", "tf", ".", "reduce_min", "(", "lengths_2", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_max\"", ",", "tf", ".", "reduce_max", "(", "lengths_2", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_mean\"", ",", "tf", ".", "reduce_mean", "(", "lengths_2", ")", ")", "\n", "\n", "lengths_3", "=", "tf", ".", "add", "(", "tf", ".", "reduce_sum", "(", "mask_3", ",", "1", ")", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_min\"", ",", "tf", ".", "reduce_min", "(", "lengths_3", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_max\"", ",", "tf", ".", "reduce_max", "(", "lengths_3", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_mean\"", ",", "tf", ".", "reduce_mean", "(", "lengths_3", ")", ")", "\n", "\n", "lengths_4", "=", "tf", ".", "add", "(", "tf", ".", "reduce_sum", "(", "mask_4", ",", "1", ")", ",", "1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_min\"", ",", "tf", ".", "reduce_min", "(", "lengths_4", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_max\"", ",", "tf", ".", "reduce_max", "(", "lengths_4", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"caption_length/batch_mean\"", ",", "tf", ".", "reduce_mean", "(", "lengths_4", ")", ")", "\n", "\n", "", "return", "images_0", ",", "input_seqs_0", ",", "target_seqs_0", ",", "mask_0", ",", "images_1", ",", "input_seqs_1", ",", "target_seqs_1", ",", "mask_1", ",", "images_2", ",", "input_seqs_2", ",", "target_seqs_2", ",", "mask_2", ",", "images_3", ",", "input_seqs_3", ",", "target_seqs_3", ",", "mask_3", ",", "images_4", ",", "input_seqs_4", ",", "target_seqs_4", ",", "mask_4", "\n", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.vocabulary.Vocabulary.__init__": [[29, 64], ["tensorflow.logging.info", "dict", "tensorflow.logging.info", "tensorflow.gfile.Exists", "tensorflow.logging.fatal", "tensorflow.gfile.GFile", "list", "list.append", "f.readlines", "line.split", "len", "enumerate"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "start_word", "=", "\"<S>\"", ",", "end_word", "=", "\"</S>\"", ",", "unk_word", "=", "\"<UNK>\"", ")", ":", "\n", "    ", "\"\"\"Initializes the vocabulary.\n\n    Args:\n      vocab_file: File containing the vocabulary, where the words are the first\n        whitespace-separated token on each line (other tokens are ignored) and\n        the word ids are the corresponding line numbers.\n      start_word: Special word denoting sentence start.\n      end_word: Special word denoting sentence end.\n      unk_word: Special word denoting unknown words.\n    \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "vocab_file", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "fatal", "(", "\"Vocab file %s not found.\"", ",", "vocab_file", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Initializing vocabulary from file: %s\"", ",", "vocab_file", ")", "\n", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "mode", "=", "\"r\"", ")", "as", "f", ":", "\n", "      ", "reverse_vocab", "=", "list", "(", "f", ".", "readlines", "(", ")", ")", "\n", "\n", "", "reverse_vocab", "=", "[", "line", ".", "split", "(", ")", "[", "0", "]", "for", "line", "in", "reverse_vocab", "]", "\n", "assert", "start_word", "in", "reverse_vocab", "\n", "assert", "end_word", "in", "reverse_vocab", "\n", "if", "unk_word", "not", "in", "reverse_vocab", ":", "\n", "      ", "reverse_vocab", ".", "append", "(", "unk_word", ")", "\n", "\n", "", "vocab", "=", "dict", "(", "[", "(", "x", ",", "y", ")", "for", "(", "y", ",", "x", ")", "in", "enumerate", "(", "reverse_vocab", ")", "]", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Created vocabulary with %d words\"", "%", "len", "(", "vocab", ")", ")", "\n", "\n", "self", ".", "vocab", "=", "vocab", "# vocab[word] = id", "\n", "self", ".", "reverse_vocab", "=", "reverse_vocab", "# reverse_vocab[id] = word", "\n", "\n", "# Save special word ids.", "\n", "self", ".", "start_id", "=", "vocab", "[", "start_word", "]", "\n", "self", ".", "end_id", "=", "vocab", "[", "end_word", "]", "\n", "self", ".", "unk_id", "=", "vocab", "[", "unk_word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.vocabulary.Vocabulary.word_to_id": [[65, 71], ["None"], "methods", ["None"], ["", "def", "word_to_id", "(", "self", ",", "word", ")", ":", "\n", "    ", "\"\"\"Returns the integer word id of a word string.\"\"\"", "\n", "if", "word", "in", "self", ".", "vocab", ":", "\n", "      ", "return", "self", ".", "vocab", "[", "word", "]", "\n", "", "else", ":", "\n", "      ", "return", "self", ".", "unk_id", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.vocabulary.Vocabulary.id_to_word": [[72, 78], ["len"], "methods", ["None"], ["", "", "def", "id_to_word", "(", "self", ",", "word_id", ")", ":", "\n", "    ", "\"\"\"Returns the word string of an integer word id.\"\"\"", "\n", "if", "word_id", ">=", "len", "(", "self", ".", "reverse_vocab", ")", ":", "\n", "      ", "return", "self", ".", "reverse_vocab", "[", "self", ".", "unk_id", "]", "\n", "", "else", ":", "\n", "      ", "return", "self", ".", "reverse_vocab", "[", "word_id", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.evaluate_model": [[56, 121], ["sess.run", "summary_writer.add_summary", "int", "time.time", "range", "math.exp", "tensorflow.logging.info", "tensorflow.Summary", "tf.Summary.value.add", "summary_writer.add_summary", "summary_writer.flush", "tensorflow.logging.info", "math.ceil", "sess.run", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "time.time", "tensorflow.logging.info"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run"], ["def", "evaluate_model", "(", "sess", ",", "model", ",", "global_step", ",", "summary_writer", ",", "summary_op", ")", ":", "\n", "  ", "\"\"\"Computes perplexity-per-word over the evaluation dataset.\n\n  Summaries and perplexity-per-word are written out to the eval directory.\n\n  Args:\n    sess: Session object.\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    global_step: Integer; global step of the model checkpoint.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"", "\n", "# Log model summaries on a single batch.", "\n", "summary_str", "=", "sess", ".", "run", "(", "summary_op", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary_str", ",", "global_step", ")", "\n", "\n", "# Compute perplexity over the entire dataset.", "\n", "num_eval_batches", "=", "int", "(", "\n", "math", ".", "ceil", "(", "FLAGS", ".", "num_eval_examples", "/", "model", ".", "config", ".", "batch_size", ")", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "sum_losses", "=", "0.", "\n", "sum_weights", "=", "0.", "\n", "for", "i", "in", "range", "(", "num_eval_batches", ")", ":", "\n", "    ", "cross_entropy_losses_0", ",", "cross_entropy_losses_1", ",", "cross_entropy_losses_2", ",", "cross_entropy_losses_3", ",", "cross_entropy_losses_4", ",", "weights_0", ",", "weights_1", ",", "weights_2", ",", "weights_3", ",", "weights_4", "=", "sess", ".", "run", "(", "[", "\n", "model", ".", "target_cross_entropy_losses_0", ",", "\n", "model", ".", "target_cross_entropy_losses_1", ",", "\n", "model", ".", "target_cross_entropy_losses_2", ",", "\n", "model", ".", "target_cross_entropy_losses_3", ",", "\n", "model", ".", "target_cross_entropy_losses_4", ",", "\n", "model", ".", "target_cross_entropy_loss_weights_0", ",", "\n", "model", ".", "target_cross_entropy_loss_weights_1", ",", "\n", "model", ".", "target_cross_entropy_loss_weights_2", ",", "\n", "model", ".", "target_cross_entropy_loss_weights_3", ",", "\n", "model", ".", "target_cross_entropy_loss_weights_4", "\n", "]", ")", "\n", "sum_losses", "+=", "np", ".", "sum", "(", "cross_entropy_losses_0", "*", "weights_0", ")", "\n", "sum_losses", "+=", "np", ".", "sum", "(", "cross_entropy_losses_1", "*", "weights_1", ")", "\n", "sum_losses", "+=", "np", ".", "sum", "(", "cross_entropy_losses_2", "*", "weights_2", ")", "\n", "sum_losses", "+=", "np", ".", "sum", "(", "cross_entropy_losses_3", "*", "weights_3", ")", "\n", "sum_losses", "+=", "np", ".", "sum", "(", "cross_entropy_losses_4", "*", "weights_4", ")", "\n", "sum_weights", "+=", "np", ".", "sum", "(", "weights_0", ")", "\n", "sum_weights", "+=", "np", ".", "sum", "(", "weights_1", ")", "\n", "sum_weights", "+=", "np", ".", "sum", "(", "weights_2", ")", "\n", "sum_weights", "+=", "np", ".", "sum", "(", "weights_3", ")", "\n", "sum_weights", "+=", "np", ".", "sum", "(", "weights_4", ")", "\n", "if", "not", "i", "%", "100", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Computed losses for %d of %d batches.\"", ",", "i", "+", "1", ",", "\n", "num_eval_batches", ")", "\n", "", "", "eval_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "sum_losses", "/", "sum_weights", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Perplexity = %f (%.2g sec)\"", ",", "perplexity", ",", "eval_time", ")", "\n", "\n", "# Log perplexity to the FileWriter.", "\n", "summary", "=", "tf", ".", "Summary", "(", ")", "\n", "value", "=", "summary", ".", "value", ".", "add", "(", ")", "\n", "value", ".", "simple_value", "=", "perplexity", "\n", "value", ".", "tag", "=", "\"Perplexity\"", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "global_step", ")", "\n", "\n", "# Write the Events file to the eval directory.", "\n", "summary_writer", ".", "flush", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Finished processing evaluation at global step %d.\"", ",", "\n", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run_once": [[123, 168], ["tensorflow.train.latest_checkpoint", "tensorflow.logging.info", "tensorflow.Session", "tensorflow.logging.info", "saver.restore", "tensorflow.train.global_step", "tensorflow.logging.info", "tensorflow.train.Coordinator", "tensorflow.train.start_queue_runners", "tf.train.Coordinator.request_stop", "tf.train.Coordinator.join", "os.path.basename", "tensorflow.logging.info", "evaluate.evaluate_model", "tensorflow.logging.error", "tf.train.Coordinator.request_stop"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.evaluate_model"], ["", "def", "run_once", "(", "model", ",", "saver", ",", "summary_writer", ",", "summary_op", ")", ":", "\n", "  ", "\"\"\"Evaluates the latest model checkpoint.\n\n  Args:\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    saver: Instance of tf.train.Saver for restoring model Variables.\n    summary_writer: Instance of FileWriter.\n    summary_op: Op for generating model summaries.\n  \"\"\"", "\n", "model_path", "=", "tf", ".", "train", ".", "latest_checkpoint", "(", "FLAGS", ".", "checkpoint_dir", ")", "\n", "if", "not", "model_path", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"Skipping evaluation. No checkpoint found in: %s\"", ",", "\n", "FLAGS", ".", "checkpoint_dir", ")", "\n", "return", "\n", "\n", "", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "# Load model from checkpoint.", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"Loading model from checkpoint: %s\"", ",", "model_path", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "model_path", ")", "\n", "global_step", "=", "tf", ".", "train", ".", "global_step", "(", "sess", ",", "model", ".", "global_step", ".", "name", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Successfully loaded %s at global step = %d.\"", ",", "\n", "os", ".", "path", ".", "basename", "(", "model_path", ")", ",", "global_step", ")", "\n", "if", "global_step", "<", "FLAGS", ".", "min_global_step", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Skipping evaluation. Global step = %d < %d\"", ",", "global_step", ",", "\n", "FLAGS", ".", "min_global_step", ")", "\n", "return", "\n", "\n", "# Start the queue runners.", "\n", "", "coord", "=", "tf", ".", "train", ".", "Coordinator", "(", ")", "\n", "threads", "=", "tf", ".", "train", ".", "start_queue_runners", "(", "coord", "=", "coord", ")", "\n", "\n", "# Run evaluation on the latest checkpoint.", "\n", "try", ":", "\n", "      ", "evaluate_model", "(", "\n", "sess", "=", "sess", ",", "\n", "model", "=", "model", ",", "\n", "global_step", "=", "global_step", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "summary_op", "=", "summary_op", ")", "\n", "", "except", "Exception", "as", "e", ":", "# pylint: disable=broad-except", "\n", "      ", "tf", ".", "logging", ".", "error", "(", "\"Evaluation failed.\"", ")", "\n", "coord", ".", "request_stop", "(", "e", ")", "\n", "\n", "", "coord", ".", "request_stop", "(", ")", "\n", "coord", ".", "join", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run": [[170, 204], ["tensorflow.Graph", "tensorflow.gfile.IsDirectory", "tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "tf.Graph.as_default", "configuration.ModelConfig", "show_and_tell_model.ShowAndTellModel", "show_and_tell_model.ShowAndTellModel.build", "tensorflow.train.Saver", "tensorflow.summary.merge_all", "tensorflow.summary.FileWriter", "tf.Graph.finalize", "time.time", "tensorflow.logging.info", "evaluate.run_once", "time.time", "time.sleep", "time.strftime", "time.localtime"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run_once"], ["", "", "def", "run", "(", ")", ":", "\n", "  ", "\"\"\"Runs evaluation in a loop, and logs summaries to TensorBoard.\"\"\"", "\n", "# Create the evaluation directory if it doesn't exist.", "\n", "eval_dir", "=", "FLAGS", ".", "eval_dir", "\n", "if", "not", "tf", ".", "gfile", ".", "IsDirectory", "(", "eval_dir", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"Creating eval directory: %s\"", ",", "eval_dir", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "eval_dir", ")", "\n", "\n", "", "g", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "g", ".", "as_default", "(", ")", ":", "\n", "# Build the model for evaluation.", "\n", "    ", "model_config", "=", "configuration", ".", "ModelConfig", "(", ")", "\n", "model_config", ".", "input_file_pattern", "=", "FLAGS", ".", "input_file_pattern", "\n", "model", "=", "show_and_tell_model", ".", "ShowAndTellModel", "(", "model_config", ",", "mode", "=", "\"eval\"", ")", "\n", "model", ".", "build", "(", ")", "\n", "\n", "# Create the Saver to restore model Variables.", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "# Create the summary operation and the summary writer.", "\n", "summary_op", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "eval_dir", ")", "\n", "\n", "g", ".", "finalize", "(", ")", "\n", "\n", "# Run a new evaluation run every eval_interval_secs.", "\n", "while", "True", ":", "\n", "      ", "start", "=", "time", ".", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Starting evaluation at \"", "+", "time", ".", "strftime", "(", "\n", "\"%Y-%m-%d-%H:%M:%S\"", ",", "time", ".", "localtime", "(", ")", ")", ")", "\n", "run_once", "(", "model", ",", "saver", ",", "summary_writer", ",", "summary_op", ")", "\n", "time_to_next_eval", "=", "start", "+", "FLAGS", ".", "eval_interval_secs", "-", "time", ".", "time", "(", ")", "\n", "if", "time_to_next_eval", ">", "0", ":", "\n", "        ", "time", ".", "sleep", "(", "time_to_next_eval", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.main": [[206, 211], ["evaluate.run"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run"], ["", "", "", "", "def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "assert", "FLAGS", ".", "input_file_pattern", ",", "\"--input_file_pattern is required\"", "\n", "assert", "FLAGS", ".", "checkpoint_dir", ",", "\"--checkpoint_dir is required\"", "\n", "assert", "FLAGS", ".", "eval_dir", ",", "\"--eval_dir is required\"", "\n", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.run_inference.main": [[42, 82], ["tensorflow.Graph", "tf.Graph.finalize", "vocabulary.Vocabulary", "FLAGS.input_files.split", "tensorflow.logging.info", "tf.Graph.as_default", "inference_wrapper.InferenceWrapper", "inference_wrapper.InferenceWrapper.build_graph_from_config", "filenames.extend", "len", "tensorflow.Session", "model.build_graph_from_config.", "caption_generator.CaptionGenerator", "caption_generator.CaptionGenerator.beam_search", "enumerate", "configuration.ModelConfig", "tensorflow.gfile.Glob", "print", "enumerate", "tensorflow.gfile.GFile", "f.read", "images.append", "print", "os.path.basename", "vocabulary.Vocabulary.id_to_word", "math.exp"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.build_graph_from_config", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.vocabulary.Vocabulary.id_to_word"], ["def", "main", "(", "_", ")", ":", "\n", "# Build the inference graph.", "\n", "  ", "g", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "g", ".", "as_default", "(", ")", ":", "\n", "    ", "model", "=", "inference_wrapper", ".", "InferenceWrapper", "(", ")", "\n", "restore_fn", "=", "model", ".", "build_graph_from_config", "(", "configuration", ".", "ModelConfig", "(", ")", ",", "FLAGS", ".", "checkpoint_path", ")", "\n", "", "g", ".", "finalize", "(", ")", "\n", "\n", "# Create the vocabulary.", "\n", "vocab", "=", "vocabulary", ".", "Vocabulary", "(", "FLAGS", ".", "vocab_file", ")", "\n", "\n", "filenames", "=", "[", "]", "\n", "for", "file_pattern", "in", "FLAGS", ".", "input_files", ".", "split", "(", "\",\"", ")", ":", "\n", "    ", "filenames", ".", "extend", "(", "tf", ".", "gfile", ".", "Glob", "(", "file_pattern", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"Running caption generation on %d files matching %s\"", ",", "\n", "len", "(", "filenames", ")", ",", "FLAGS", ".", "input_files", ")", "\n", "\n", "with", "tf", ".", "Session", "(", "graph", "=", "g", ")", "as", "sess", ":", "\n", "# Load the model from checkpoint.", "\n", "    ", "restore_fn", "(", "sess", ")", "\n", "\n", "# Prepare the caption generator. Here we are implicitly using the default", "\n", "# beam search parameters. See caption_generator.py for a description of the", "\n", "# available beam search parameters.", "\n", "generator", "=", "caption_generator", ".", "CaptionGenerator", "(", "model", ",", "vocab", ")", "\n", "\n", "images", "=", "[", "]", "\n", "for", "filename", "in", "filenames", ":", "\n", "      ", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "image", "=", "f", ".", "read", "(", ")", "\n", "images", ".", "append", "(", "image", ")", "\n", "", "", "captions", "=", "generator", ".", "beam_search", "(", "sess", ",", "images", ",", "vocab", ")", "\n", "for", "i", ",", "image", "in", "enumerate", "(", "images", ")", ":", "\n", "      ", "print", "(", "\"Captions for image %s:\"", "%", "os", ".", "path", ".", "basename", "(", "filenames", "[", "i", "]", ")", ")", "\n", "for", "j", ",", "caption", "in", "enumerate", "(", "captions", "[", "i", "]", ")", ":", "\n", "# Ignore begin and end words.", "\n", "        ", "sentence", "=", "[", "vocab", ".", "id_to_word", "(", "w", ")", "for", "w", "in", "caption", ".", "sentence", "[", "1", ":", "-", "1", "]", "]", "\n", "sentence", "=", "\" \"", ".", "join", "(", "sentence", ")", "\n", "print", "(", "\"  %d) %s (p=%f)\"", "%", "(", "j", ",", "sentence", ",", "math", ".", "exp", "(", "caption", ".", "logprob", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3": [[30, 115], ["tensorflow.contrib.layers.l2_regularizer", "tensorflow.variable_scope", "scope.reuse_variables", "end_points.values", "slim.arg_scope", "tensorflow.contrib.layers.summaries.summarize_activation", "slim.arg_scope", "tensorflow.contrib.slim.python.slim.nets.inception_v3.inception_v3_base", "tensorflow.variable_scope", "slim.flatten.get_shape", "slim.avg_pool2d", "slim.dropout", "slim.flatten", "tensorflow.truncated_normal_initializer"], "function", ["None"], ["def", "inception_v3", "(", "images", ",", "\n", "trainable", "=", "True", ",", "\n", "is_training", "=", "True", ",", "\n", "weight_decay", "=", "0.00004", ",", "\n", "stddev", "=", "0.1", ",", "\n", "dropout_keep_prob", "=", "0.8", ",", "\n", "use_batch_norm", "=", "True", ",", "\n", "batch_norm_params", "=", "None", ",", "\n", "add_summaries", "=", "True", ",", "\n", "scope", "=", "\"InceptionV3\"", ")", ":", "\n", "  ", "\"\"\"Builds an Inception V3 subgraph for image embeddings.\n\n  Args:\n    images: A float32 Tensor of shape [batch, height, width, channels].\n    trainable: Whether the inception submodel should be trainable or not.\n    is_training: Boolean indicating training mode or not.\n    weight_decay: Coefficient for weight regularization.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    dropout_keep_prob: Dropout keep probability.\n    use_batch_norm: Whether to use batch normalization.\n    batch_norm_params: Parameters for batch normalization. See\n      tf.contrib.layers.batch_norm for details.\n    add_summaries: Whether to add activation summaries.\n    scope: Optional Variable scope.\n\n  Returns:\n    end_points: A dictionary of activations from inception_v3 layers.\n  \"\"\"", "\n", "# Only consider the inception model to be in training mode if it's trainable.", "\n", "is_inception_model_training", "=", "trainable", "and", "is_training", "\n", "\n", "if", "use_batch_norm", ":", "\n", "# Default parameters for batch normalization.", "\n", "    ", "if", "not", "batch_norm_params", ":", "\n", "      ", "batch_norm_params", "=", "{", "\n", "\"is_training\"", ":", "is_inception_model_training", ",", "\n", "\"trainable\"", ":", "trainable", ",", "\n", "# Decay for the moving averages.", "\n", "\"decay\"", ":", "0.9997", ",", "\n", "# Epsilon to prevent 0s in variance.", "\n", "\"epsilon\"", ":", "0.001", ",", "\n", "# Collection containing the moving mean and moving variance.", "\n", "\"variables_collections\"", ":", "{", "\n", "\"beta\"", ":", "None", ",", "\n", "\"gamma\"", ":", "None", ",", "\n", "\"moving_mean\"", ":", "[", "\"moving_vars\"", "]", ",", "\n", "\"moving_variance\"", ":", "[", "\"moving_vars\"", "]", ",", "\n", "}", "\n", "}", "\n", "", "", "else", ":", "\n", "    ", "batch_norm_params", "=", "None", "\n", "\n", "", "if", "trainable", ":", "\n", "    ", "weights_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "weight_decay", ")", "\n", "", "else", ":", "\n", "    ", "weights_regularizer", "=", "None", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "[", "images", "]", ")", "as", "scope", ":", "\n", "    ", "with", "slim", ".", "arg_scope", "(", "\n", "[", "slim", ".", "conv2d", ",", "slim", ".", "fully_connected", "]", ",", "\n", "weights_regularizer", "=", "weights_regularizer", ",", "\n", "trainable", "=", "trainable", ")", ":", "\n", "      ", "with", "slim", ".", "arg_scope", "(", "\n", "[", "slim", ".", "conv2d", "]", ",", "\n", "weights_initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "stddev", ")", ",", "\n", "activation_fn", "=", "tf", ".", "nn", ".", "relu", ",", "\n", "normalizer_fn", "=", "slim", ".", "batch_norm", ",", "\n", "normalizer_params", "=", "batch_norm_params", ")", ":", "\n", "        ", "net", ",", "end_points", "=", "inception_v3_base", "(", "images", ",", "scope", "=", "scope", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"logits\"", ")", ":", "\n", "          ", "shape", "=", "net", ".", "get_shape", "(", ")", "\n", "net", "=", "slim", ".", "avg_pool2d", "(", "net", ",", "shape", "[", "1", ":", "3", "]", ",", "padding", "=", "\"VALID\"", ",", "scope", "=", "\"pool\"", ")", "\n", "net", "=", "slim", ".", "dropout", "(", "\n", "net", ",", "\n", "keep_prob", "=", "dropout_keep_prob", ",", "\n", "is_training", "=", "is_inception_model_training", ",", "\n", "scope", "=", "\"dropout\"", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ",", "scope", "=", "\"flatten\"", ")", "\n", "", "", "", "scope", ".", "reuse_variables", "(", ")", "\n", "# Add summaries.", "\n", "", "if", "add_summaries", ":", "\n", "    ", "for", "v", "in", "end_points", ".", "values", "(", ")", ":", "\n", "      ", "tf", ".", "contrib", ".", "layers", ".", "summaries", ".", "summarize_activation", "(", "v", ")", "\n", "\n", "", "", "return", "net", "\n", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.train.main": [[43, 110], ["configuration.ModelConfig", "configuration.TrainingConfig", "tensorflow.Graph", "tensorflow.contrib.slim.learning.train", "tensorflow.gfile.IsDirectory", "tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "tf.Graph.as_default", "show_and_tell_model.ShowAndTellModel", "show_and_tell_model.ShowAndTellModel.build", "tensorflow.contrib.layers.optimize_loss", "tensorflow.train.Saver", "tensorflow.constant", "tensorflow.constant", "int", "tensorflow.train.exponential_decay"], "function", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build"], ["def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "assert", "FLAGS", ".", "input_file_pattern", ",", "\"--input_file_pattern is required\"", "\n", "assert", "FLAGS", ".", "train_dir", ",", "\"--train_dir is required\"", "\n", "\n", "model_config", "=", "configuration", ".", "ModelConfig", "(", ")", "\n", "model_config", ".", "input_file_pattern", "=", "FLAGS", ".", "input_file_pattern", "\n", "model_config", ".", "inception_checkpoint_file", "=", "FLAGS", ".", "inception_checkpoint_file", "\n", "training_config", "=", "configuration", ".", "TrainingConfig", "(", ")", "\n", "\n", "# Create training directory.", "\n", "train_dir", "=", "FLAGS", ".", "train_dir", "\n", "if", "not", "tf", ".", "gfile", ".", "IsDirectory", "(", "train_dir", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"Creating training directory: %s\"", ",", "train_dir", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "train_dir", ")", "\n", "\n", "# Build the TensorFlow graph.", "\n", "", "g", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "g", ".", "as_default", "(", ")", ":", "\n", "# Build the model.", "\n", "    ", "model", "=", "show_and_tell_model", ".", "ShowAndTellModel", "(", "\n", "model_config", ",", "mode", "=", "\"train\"", ",", "train_inception", "=", "FLAGS", ".", "train_inception", ")", "\n", "model", ".", "build", "(", ")", "\n", "\n", "# Set up the learning rate.", "\n", "learning_rate_decay_fn", "=", "None", "\n", "if", "FLAGS", ".", "train_inception", ":", "\n", "      ", "learning_rate", "=", "tf", ".", "constant", "(", "training_config", ".", "train_inception_learning_rate", ")", "\n", "", "else", ":", "\n", "      ", "learning_rate", "=", "tf", ".", "constant", "(", "training_config", ".", "initial_learning_rate", ")", "\n", "if", "training_config", ".", "learning_rate_decay_factor", ">", "0", ":", "\n", "        ", "num_batches_per_epoch", "=", "(", "training_config", ".", "num_examples_per_epoch", "/", "\n", "model_config", ".", "batch_size", ")", "\n", "decay_steps", "=", "int", "(", "num_batches_per_epoch", "*", "\n", "training_config", ".", "num_epochs_per_decay", ")", "\n", "\n", "def", "_learning_rate_decay_fn", "(", "learning_rate", ",", "global_step", ")", ":", "\n", "          ", "return", "tf", ".", "train", ".", "exponential_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "decay_steps", "=", "decay_steps", ",", "\n", "decay_rate", "=", "training_config", ".", "learning_rate_decay_factor", ",", "\n", "staircase", "=", "True", ")", "\n", "\n", "", "learning_rate_decay_fn", "=", "_learning_rate_decay_fn", "\n", "\n", "# Set up the training ops.", "\n", "", "", "train_op", "=", "tf", ".", "contrib", ".", "layers", ".", "optimize_loss", "(", "\n", "loss", "=", "model", ".", "total_loss", ",", "\n", "global_step", "=", "model", ".", "global_step", ",", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "optimizer", "=", "training_config", ".", "optimizer", ",", "\n", "clip_gradients", "=", "training_config", ".", "clip_gradients", ",", "\n", "learning_rate_decay_fn", "=", "learning_rate_decay_fn", ")", "\n", "\n", "# Set up the Saver for saving and restoring model checkpoints.", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "training_config", ".", "max_checkpoints_to_keep", ")", "\n", "\n", "# Run training.", "\n", "", "tf", ".", "contrib", ".", "slim", ".", "learning", ".", "train", "(", "\n", "train_op", ",", "\n", "train_dir", ",", "\n", "log_every_n_steps", "=", "FLAGS", ".", "log_every_n_steps", ",", "\n", "graph", "=", "g", ",", "\n", "global_step", "=", "model", ".", "global_step", ",", "\n", "number_of_steps", "=", "FLAGS", ".", "number_of_steps", ",", "\n", "init_fn", "=", "model", ".", "init_fn", ",", "\n", "saver", "=", "saver", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.__init__": [[59, 61], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.build_model": [[62, 72], ["tensorflow.logging.fatal"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "model_config", ")", ":", "\n", "    ", "\"\"\"Builds the model for inference.\n\n    Args:\n      model_config: Object containing configuration for building the model.\n\n    Returns:\n      model: The model object.\n    \"\"\"", "\n", "tf", ".", "logging", ".", "fatal", "(", "\"Please implement build_model in subclass\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase._create_restore_fn": [[73, 101], ["tensorflow.gfile.IsDirectory", "tensorflow.train.latest_checkpoint", "tensorflow.logging.info", "saver.restore", "tensorflow.logging.info", "ValueError", "os.path.basename"], "methods", ["None"], ["", "def", "_create_restore_fn", "(", "self", ",", "checkpoint_path", ",", "saver", ")", ":", "\n", "    ", "\"\"\"Creates a function that restores a model from checkpoint.\n\n    Args:\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n      saver: Saver for restoring variables from the checkpoint file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n\n    Raises:\n      ValueError: If checkpoint_path does not refer to a checkpoint file or a\n        directory containing a checkpoint file.\n    \"\"\"", "\n", "if", "tf", ".", "gfile", ".", "IsDirectory", "(", "checkpoint_path", ")", ":", "\n", "      ", "checkpoint_path", "=", "tf", ".", "train", ".", "latest_checkpoint", "(", "checkpoint_path", ")", "\n", "if", "not", "checkpoint_path", ":", "\n", "        ", "raise", "ValueError", "(", "\"No checkpoint file found in: %s\"", "%", "checkpoint_path", ")", "\n", "\n", "", "", "def", "_restore_fn", "(", "sess", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Loading model from checkpoint: %s\"", ",", "checkpoint_path", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "checkpoint_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Successfully loaded checkpoint: %s\"", ",", "\n", "os", ".", "path", ".", "basename", "(", "checkpoint_path", ")", ")", "\n", "\n", "", "return", "_restore_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.build_graph_from_config": [[102, 120], ["tensorflow.logging.info", "inference_wrapper_base.InferenceWrapperBase.build_model", "tensorflow.train.Saver", "inference_wrapper_base.InferenceWrapperBase._create_restore_fn"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_model", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase._create_restore_fn"], ["", "def", "build_graph_from_config", "(", "self", ",", "model_config", ",", "checkpoint_path", ")", ":", "\n", "    ", "\"\"\"Builds the inference graph from a configuration object.\n\n    Args:\n      model_config: Object containing configuration for building the model.\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n    \"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Building model.\"", ")", "\n", "self", ".", "build_model", "(", "model_config", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "\n", "return", "self", ".", "_create_restore_fn", "(", "checkpoint_path", ",", "saver", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.build_graph_from_proto": [[121, 150], ["tensorflow.logging.info", "tensorflow.GraphDef", "tensorflow.import_graph_def", "tensorflow.logging.info", "tensorflow.train.SaverDef", "tensorflow.train.Saver", "inference_wrapper_base.InferenceWrapperBase._create_restore_fn", "tensorflow.gfile.FastGFile", "tensorflow.GraphDef.ParseFromString", "tensorflow.gfile.FastGFile", "tensorflow.train.SaverDef.ParseFromString", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase._create_restore_fn"], ["", "def", "build_graph_from_proto", "(", "self", ",", "graph_def_file", ",", "saver_def_file", ",", "\n", "checkpoint_path", ")", ":", "\n", "    ", "\"\"\"Builds the inference graph from serialized GraphDef and SaverDef protos.\n\n    Args:\n      graph_def_file: File containing a serialized GraphDef proto.\n      saver_def_file: File containing a serialized SaverDef proto.\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n    \"\"\"", "\n", "# Load the Graph.", "\n", "tf", ".", "logging", ".", "info", "(", "\"Loading GraphDef from file: %s\"", ",", "graph_def_file", ")", "\n", "graph_def", "=", "tf", ".", "GraphDef", "(", ")", "\n", "with", "tf", ".", "gfile", ".", "FastGFile", "(", "graph_def_file", ",", "\"rb\"", ")", "as", "f", ":", "\n", "      ", "graph_def", ".", "ParseFromString", "(", "f", ".", "read", "(", ")", ")", "\n", "", "tf", ".", "import_graph_def", "(", "graph_def", ",", "name", "=", "\"\"", ")", "\n", "\n", "# Load the Saver.", "\n", "tf", ".", "logging", ".", "info", "(", "\"Loading SaverDef from file: %s\"", ",", "saver_def_file", ")", "\n", "saver_def", "=", "tf", ".", "train", ".", "SaverDef", "(", ")", "\n", "with", "tf", ".", "gfile", ".", "FastGFile", "(", "saver_def_file", ",", "\"rb\"", ")", "as", "f", ":", "\n", "      ", "saver_def", ".", "ParseFromString", "(", "f", ".", "read", "(", ")", ")", "\n", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "saver_def", "=", "saver_def", ")", "\n", "\n", "return", "self", ".", "_create_restore_fn", "(", "checkpoint_path", ",", "saver", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.feed_image": [[151, 164], ["tensorflow.logging.fatal"], "methods", ["None"], ["", "def", "feed_image", "(", "self", ",", "sess", ",", "encoded_images", ")", ":", "\n", "    ", "\"\"\"Feeds an image and returns the initial model state.\n\n    See comments at the top of file.\n\n    Args:\n      sess: TensorFlow Session object.\n      encoded_image: An encoded image string.\n\n    Returns:\n      state: A numpy array of shape [1, state_size].\n    \"\"\"", "\n", "tf", ".", "logging", ".", "fatal", "(", "\"Please implement feed_image in subclass\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper_base.InferenceWrapperBase.inference_step": [[165, 181], ["tensorflow.logging.fatal"], "methods", ["None"], ["", "def", "inference_step", "(", "self", ",", "sess", ",", "input_feed", ",", "state_feed", ",", "order", ")", ":", "\n", "    ", "\"\"\"Runs one step of inference.\n\n    Args:\n      sess: TensorFlow Session object.\n      input_feed: A numpy array of shape [batch_size].\n      state_feed: A numpy array of shape [batch_size, state_size].\n\n    Returns:\n      softmax_output: A numpy array of shape [batch_size, vocab_size].\n      new_state: A numpy array of shape [batch_size, state_size].\n      metadata: Optional. If not None, a string containing metadata about the\n        current inference step (e.g. serialized numpy array containing\n        activations from a particular model layer.).\n    \"\"\"", "\n", "tf", ".", "logging", ".", "fatal", "(", "\"Please implement inference_step in subclass\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.Caption.__init__": [[32, 48], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sentence", ",", "state", ",", "logprob", ",", "score", ",", "metadata", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initializes the Caption.\n\n    Args:\n      sentence: List of word ids in the caption.\n      state: Model state after generating the previous word.\n      logprob: Log-probability of the caption.\n      score: Score of the caption.\n      metadata: Optional metadata associated with the partial sentence. If not\n        None, a list of strings with the same length as 'sentence'.\n    \"\"\"", "\n", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "state", "=", "state", "\n", "self", ".", "logprob", "=", "logprob", "\n", "self", ".", "score", "=", "score", "\n", "self", ".", "metadata", "=", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.Caption.__cmp__": [[49, 58], ["isinstance"], "methods", ["None"], ["", "def", "__cmp__", "(", "self", ",", "other", ")", ":", "\n", "    ", "\"\"\"Compares Captions by score.\"\"\"", "\n", "assert", "isinstance", "(", "other", ",", "Caption", ")", "\n", "if", "self", ".", "score", "==", "other", ".", "score", ":", "\n", "      ", "return", "0", "\n", "", "elif", "self", ".", "score", "<", "other", ".", "score", ":", "\n", "      ", "return", "-", "1", "\n", "", "else", ":", "\n", "      ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.Caption.__lt__": [[60, 63], ["isinstance"], "methods", ["None"], ["", "", "def", "__lt__", "(", "self", ",", "other", ")", ":", "\n", "    ", "assert", "isinstance", "(", "other", ",", "Caption", ")", "\n", "return", "self", ".", "score", "<", "other", ".", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.Caption.__eq__": [[65, 68], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "    ", "assert", "isinstance", "(", "other", ",", "Caption", ")", "\n", "return", "self", ".", "score", "==", "other", ".", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.__init__": [[73, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n", ")", ":", "\n", "    ", "self", ".", "_n", "=", "n", "\n", "self", ".", "_data", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.size": [[77, 80], ["len"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "    ", "assert", "self", ".", "_data", "is", "not", "None", "\n", "return", "len", "(", "self", ".", "_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push": [[81, 88], ["len", "heapq.heappush", "heapq.heappushpop"], "methods", ["None"], ["", "def", "push", "(", "self", ",", "x", ")", ":", "\n", "    ", "\"\"\"Pushes a new element.\"\"\"", "\n", "assert", "self", ".", "_data", "is", "not", "None", "\n", "if", "len", "(", "self", ".", "_data", ")", "<", "self", ".", "_n", ":", "\n", "      ", "heapq", ".", "heappush", "(", "self", ".", "_data", ",", "x", ")", "\n", "", "else", ":", "\n", "      ", "heapq", ".", "heappushpop", "(", "self", ".", "_data", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.extract": [[89, 106], ["data.sort"], "methods", ["None"], ["", "", "def", "extract", "(", "self", ",", "sort", "=", "False", ")", ":", "\n", "    ", "\"\"\"Extracts all elements from the TopN. This is a destructive operation.\n\n    The only method that can be called immediately after extract() is reset().\n\n    Args:\n      sort: Whether to return the elements in descending sorted order.\n\n    Returns:\n      A list of data; the top n elements provided to the set.\n    \"\"\"", "\n", "assert", "self", ".", "_data", "is", "not", "None", "\n", "data", "=", "self", ".", "_data", "\n", "self", ".", "_data", "=", "None", "\n", "if", "sort", ":", "\n", "      ", "data", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.reset": [[107, 110], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the TopN to an empty state.\"\"\"", "\n", "self", ".", "_data", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.__init__": [[115, 141], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "model", ",", "\n", "vocab", ",", "\n", "beam_size", "=", "3", ",", "\n", "max_caption_length", "=", "20", ",", "\n", "length_normalization_factor", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"Initializes the generator.\n\n    Args:\n      model: Object encapsulating a trained image-to-text model. Must have\n        methods feed_image() and inference_step(). For example, an instance of\n        InferenceWrapperBase.\n      vocab: A Vocabulary object.\n      beam_size: Beam size to use when generating captions.\n      max_caption_length: The maximum caption length before stopping the search.\n      length_normalization_factor: If != 0, a number x such that captions are\n        scored by logprob/length^x, rather than logprob. This changes the\n        relative scores of captions depending on their lengths. For example, if\n        x > 0 then longer captions will be favored.\n    \"\"\"", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "model", "=", "model", "\n", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "max_caption_length", "=", "max_caption_length", "\n", "self", ".", "length_normalization_factor", "=", "length_normalization_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner": [[142, 188], ["range", "complete_captions.extract", "partial_captions.extract", "partial_captions.reset", "numpy.array", "numpy.array", "caption_generator.CaptionGenerator.model.inference_step", "enumerate", "complete_captions.size", "list", "list.sort", "partial_captions.size", "enumerate", "math.log", "caption_generator.Caption", "complete_captions.push", "caption_generator.Caption", "partial_captions.push", "len"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.extract", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.extract", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.reset", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.inference_step", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.size", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.size", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push"], ["", "def", "beam_search_inner", "(", "self", ",", "sess", ",", "partial_captions", ",", "complete_captions", ",", "order", ",", "prev_captions", ",", "vocab", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "self", ".", "max_caption_length", "-", "1", ")", ":", "\n", "      ", "partial_captions_list", "=", "partial_captions", ".", "extract", "(", ")", "\n", "partial_captions", ".", "reset", "(", ")", "\n", "input_feed", "=", "np", ".", "array", "(", "[", "c", ".", "sentence", "[", "-", "1", "]", "for", "c", "in", "partial_captions_list", "]", ")", "\n", "state_feed", "=", "np", ".", "array", "(", "[", "c", ".", "state", "for", "c", "in", "partial_captions_list", "]", ")", "\n", "\n", "softmax", ",", "new_states", ",", "metadata", "=", "self", ".", "model", ".", "inference_step", "(", "sess", ",", "input_feed", ",", "state_feed", ",", "order", ")", "\n", "\n", "for", "i", ",", "partial_caption", "in", "enumerate", "(", "partial_captions_list", ")", ":", "\n", "        ", "word_probabilities", "=", "softmax", "[", "i", "]", "\n", "state", "=", "new_states", "[", "i", "]", "\n", "# For this partial caption, get the beam_size most probable next words.", "\n", "words_and_probs", "=", "list", "(", "enumerate", "(", "word_probabilities", ")", ")", "\n", "words_and_probs", ".", "sort", "(", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "words_and_probs", "=", "words_and_probs", "[", "0", ":", "self", ".", "beam_size", "]", "\n", "# Each next word gives a new partial caption.", "\n", "for", "w", ",", "p", "in", "words_and_probs", ":", "\n", "          ", "if", "p", "<", "1e-12", ":", "\n", "            ", "continue", "# Avoid log(0).", "\n", "", "sentence", "=", "partial_caption", ".", "sentence", "+", "[", "w", "]", "\n", "logprob", "=", "partial_caption", ".", "logprob", "+", "math", ".", "log", "(", "p", ")", "\n", "score", "=", "logprob", "\n", "if", "metadata", ":", "\n", "            ", "metadata_list", "=", "partial_caption", ".", "metadata", "+", "[", "metadata", "[", "i", "]", "]", "\n", "", "else", ":", "\n", "            ", "metadata_list", "=", "None", "\n", "", "if", "w", "==", "self", ".", "vocab", ".", "end_id", ":", "\n", "            ", "if", "self", ".", "length_normalization_factor", ">", "0", ":", "\n", "              ", "score", "/=", "len", "(", "sentence", ")", "**", "self", ".", "length_normalization_factor", "\n", "", "beam", "=", "Caption", "(", "sentence", ",", "state", ",", "logprob", ",", "score", ",", "metadata_list", ")", "\n", "complete_captions", ".", "push", "(", "beam", ")", "\n", "", "else", ":", "\n", "            ", "beam", "=", "Caption", "(", "sentence", ",", "state", ",", "logprob", ",", "score", ",", "metadata_list", ")", "\n", "partial_captions", ".", "push", "(", "beam", ")", "\n", "", "", "", "if", "partial_captions", ".", "size", "(", ")", "==", "0", ":", "\n", "# We have run out of partial candidates; happens when beam_size = 1.", "\n", "        ", "break", "\n", "\n", "# If we have no complete captions then fall back to the partial captions.", "\n", "# But never output a mixture of complete and partial captions because a", "\n", "# partial caption could have a higher score than all the complete captions.", "\n", "", "", "if", "not", "complete_captions", ".", "size", "(", ")", ":", "\n", "      ", "complete_captions", "=", "partial_captions", "\n", "\n", "", "return", "complete_captions", ".", "extract", "(", "sort", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search": [[190, 232], ["caption_generator.CaptionGenerator.model.feed_image", "caption_generator.Caption", "caption_generator.Caption", "caption_generator.Caption", "caption_generator.Caption", "caption_generator.Caption", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN.push", "caption_generator.TopN.push", "caption_generator.TopN.push", "caption_generator.TopN.push", "caption_generator.TopN.push", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.TopN", "caption_generator.CaptionGenerator.beam_search_inner", "caption_generator.CaptionGenerator.beam_search_inner", "caption_generator.CaptionGenerator.beam_search_inner", "caption_generator.CaptionGenerator.beam_search_inner", "caption_generator.CaptionGenerator.beam_search_inner", "captions.append", "captions.append", "captions.append", "captions.append", "captions.append"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.feed_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.TopN.push", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.caption_generator.CaptionGenerator.beam_search_inner"], ["", "def", "beam_search", "(", "self", ",", "sess", ",", "encoded_images", ",", "vocab", ")", ":", "\n", "    ", "initial_state_0", ",", "initial_state_1", ",", "initial_state_2", ",", "initial_state_3", ",", "initial_state_4", "=", "self", ".", "model", ".", "feed_image", "(", "sess", ",", "encoded_images", ")", "\n", "\n", "initial_beam_0", "=", "Caption", "(", "sentence", "=", "[", "self", ".", "vocab", ".", "start_id", "]", ",", "state", "=", "initial_state_0", "[", "0", "]", ",", "logprob", "=", "0.0", ",", "score", "=", "0.0", ",", "metadata", "=", "[", "\"\"", "]", ")", "\n", "initial_beam_1", "=", "Caption", "(", "sentence", "=", "[", "self", ".", "vocab", ".", "start_id", "]", ",", "state", "=", "initial_state_1", "[", "0", "]", ",", "logprob", "=", "0.0", ",", "score", "=", "0.0", ",", "metadata", "=", "[", "\"\"", "]", ")", "\n", "initial_beam_2", "=", "Caption", "(", "sentence", "=", "[", "self", ".", "vocab", ".", "start_id", "]", ",", "state", "=", "initial_state_2", "[", "0", "]", ",", "logprob", "=", "0.0", ",", "score", "=", "0.0", ",", "metadata", "=", "[", "\"\"", "]", ")", "\n", "initial_beam_3", "=", "Caption", "(", "sentence", "=", "[", "self", ".", "vocab", ".", "start_id", "]", ",", "state", "=", "initial_state_3", "[", "0", "]", ",", "logprob", "=", "0.0", ",", "score", "=", "0.0", ",", "metadata", "=", "[", "\"\"", "]", ")", "\n", "initial_beam_4", "=", "Caption", "(", "sentence", "=", "[", "self", ".", "vocab", ".", "start_id", "]", ",", "state", "=", "initial_state_4", "[", "0", "]", ",", "logprob", "=", "0.0", ",", "score", "=", "0.0", ",", "metadata", "=", "[", "\"\"", "]", ")", "\n", "\n", "partial_captions_0", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "partial_captions_1", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "partial_captions_2", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "partial_captions_3", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "partial_captions_4", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "\n", "partial_captions_0", ".", "push", "(", "initial_beam_0", ")", "\n", "partial_captions_1", ".", "push", "(", "initial_beam_1", ")", "\n", "partial_captions_2", ".", "push", "(", "initial_beam_2", ")", "\n", "partial_captions_3", ".", "push", "(", "initial_beam_3", ")", "\n", "partial_captions_4", ".", "push", "(", "initial_beam_4", ")", "\n", "\n", "complete_captions_0", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "complete_captions_1", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "complete_captions_2", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "complete_captions_3", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "complete_captions_4", "=", "TopN", "(", "self", ".", "beam_size", ")", "\n", "\n", "# Run beam search.", "\n", "captions", "=", "[", "]", "\n", "captions_0", "=", "self", ".", "beam_search_inner", "(", "sess", ",", "partial_captions_0", ",", "complete_captions_0", ",", "0", ",", "[", "]", ",", "vocab", ")", "\n", "captions_1", "=", "self", ".", "beam_search_inner", "(", "sess", ",", "partial_captions_1", ",", "complete_captions_1", ",", "1", ",", "captions_0", ",", "vocab", ")", "\n", "captions_2", "=", "self", ".", "beam_search_inner", "(", "sess", ",", "partial_captions_2", ",", "complete_captions_2", ",", "2", ",", "captions_0", "+", "captions_1", ",", "vocab", ")", "\n", "captions_3", "=", "self", ".", "beam_search_inner", "(", "sess", ",", "partial_captions_3", ",", "complete_captions_3", ",", "3", ",", "captions_0", "+", "captions_1", "+", "captions_2", ",", "vocab", ")", "\n", "captions_4", "=", "self", ".", "beam_search_inner", "(", "sess", ",", "partial_captions_4", ",", "complete_captions_4", ",", "4", ",", "captions_0", "+", "captions_1", "+", "captions_2", "+", "captions_3", ",", "vocab", ")", "\n", "\n", "captions", ".", "append", "(", "captions_0", ")", "\n", "captions", ".", "append", "(", "captions_1", ")", "\n", "captions", ".", "append", "(", "captions_2", ")", "\n", "captions", ".", "append", "(", "captions_3", ")", "\n", "captions", ".", "append", "(", "captions_4", ")", "\n", "\n", "return", "captions", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_processing.distort_image": [[26, 60], ["tensorflow.name_scope", "tensorflow.image.random_flip_left_right", "tensorflow.name_scope", "tensorflow.clip_by_value", "tensorflow.image.random_brightness", "tensorflow.image.random_saturation", "tensorflow.image.random_hue", "tensorflow.image.random_contrast", "tensorflow.image.random_brightness", "tensorflow.image.random_contrast", "tensorflow.image.random_saturation", "tensorflow.image.random_hue"], "function", ["None"], ["def", "distort_image", "(", "image", ",", "thread_id", ")", ":", "\n", "  ", "\"\"\"Perform random distortions on an image.\n\n  Args:\n    image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n    thread_id: Preprocessing thread id used to select the ordering of color\n      distortions. There should be a multiple of 2 preprocessing threads.\n\n  Returns:\n    distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n      [0, 1].\n  \"\"\"", "\n", "# Randomly flip horizontally.", "\n", "with", "tf", ".", "name_scope", "(", "\"flip_horizontal\"", ",", "values", "=", "[", "image", "]", ")", ":", "\n", "    ", "image", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "image", ")", "\n", "\n", "# Randomly distort the colors based on thread id.", "\n", "", "color_ordering", "=", "thread_id", "%", "2", "\n", "with", "tf", ".", "name_scope", "(", "\"distort_color\"", ",", "values", "=", "[", "image", "]", ")", ":", "\n", "    ", "if", "color_ordering", "==", "0", ":", "\n", "      ", "image", "=", "tf", ".", "image", ".", "random_brightness", "(", "image", ",", "max_delta", "=", "32.", "/", "255.", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_saturation", "(", "image", ",", "lower", "=", "0.5", ",", "upper", "=", "1.5", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_hue", "(", "image", ",", "max_delta", "=", "0.032", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_contrast", "(", "image", ",", "lower", "=", "0.5", ",", "upper", "=", "1.5", ")", "\n", "", "elif", "color_ordering", "==", "1", ":", "\n", "      ", "image", "=", "tf", ".", "image", ".", "random_brightness", "(", "image", ",", "max_delta", "=", "32.", "/", "255.", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_contrast", "(", "image", ",", "lower", "=", "0.5", ",", "upper", "=", "1.5", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_saturation", "(", "image", ",", "lower", "=", "0.5", ",", "upper", "=", "1.5", ")", "\n", "image", "=", "tf", ".", "image", ".", "random_hue", "(", "image", ",", "max_delta", "=", "0.032", ")", "\n", "\n", "# The random_* ops do not necessarily clamp.", "\n", "", "image", "=", "tf", ".", "clip_by_value", "(", "image", ",", "0.0", ",", "1.0", ")", "\n", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_processing.process_image": [[62, 134], ["tensorflow.image.convert_image_dtype", "image_processing.process_image.image_summary"], "function", ["None"], ["", "def", "process_image", "(", "encoded_image", ",", "\n", "is_training", ",", "\n", "height", ",", "\n", "width", ",", "\n", "resize_height", "=", "346", ",", "\n", "resize_width", "=", "346", ",", "\n", "thread_id", "=", "0", ",", "\n", "image_format", "=", "\"jpeg\"", ")", ":", "\n", "  ", "\"\"\"Decode an image, resize and apply random distortions.\n\n  In training, images are distorted slightly differently depending on thread_id.\n\n  Args:\n    encoded_image: String Tensor containing the image.\n    is_training: Boolean; whether preprocessing for training or eval.\n    height: Height of the output image.\n    width: Width of the output image.\n    resize_height: If > 0, resize height before crop to final dimensions.\n    resize_width: If > 0, resize width before crop to final dimensions.\n    thread_id: Preprocessing thread id used to select the ordering of color\n      distortions. There should be a multiple of 2 preprocessing threads.\n    image_format: \"jpeg\" or \"png\".\n\n  Returns:\n    A float32 Tensor of shape [height, width, 3] with values in [-1, 1].\n\n  Raises:\n    ValueError: If image_format is invalid.\n  \"\"\"", "\n", "# Helper function to log an image summary to the visualizer. Summaries are", "\n", "# only logged in thread 0.", "\n", "def", "image_summary", "(", "name", ",", "image", ")", ":", "\n", "    ", "if", "not", "thread_id", ":", "\n", "      ", "tf", ".", "summary", ".", "image", "(", "name", ",", "tf", ".", "expand_dims", "(", "image", ",", "0", ")", ")", "\n", "\n", "# Decode image into a float32 Tensor of shape [?, ?, 3] with values in [0, 1).", "\n", "", "", "with", "tf", ".", "name_scope", "(", "\"decode\"", ",", "values", "=", "[", "encoded_image", "]", ")", ":", "\n", "    ", "if", "image_format", "==", "\"jpeg\"", ":", "\n", "      ", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "encoded_image", ",", "channels", "=", "3", ")", "\n", "", "elif", "image_format", "==", "\"png\"", ":", "\n", "      ", "image", "=", "tf", ".", "image", ".", "decode_png", "(", "encoded_image", ",", "channels", "=", "3", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Invalid image format: %s\"", "%", "image_format", ")", "\n", "", "", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "image", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "image_summary", "(", "\"original_image\"", ",", "image", ")", "\n", "\n", "# Resize image.", "\n", "assert", "(", "resize_height", ">", "0", ")", "==", "(", "resize_width", ">", "0", ")", "\n", "if", "resize_height", ":", "\n", "    ", "image", "=", "tf", ".", "image", ".", "resize_images", "(", "image", ",", "\n", "size", "=", "[", "resize_height", ",", "resize_width", "]", ",", "\n", "method", "=", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", ")", "\n", "\n", "# Crop to final dimensions.", "\n", "", "if", "is_training", ":", "\n", "    ", "image", "=", "tf", ".", "random_crop", "(", "image", ",", "[", "height", ",", "width", ",", "3", "]", ")", "\n", "", "else", ":", "\n", "# Central crop, assuming resize_height > height, resize_width > width.", "\n", "    ", "image", "=", "tf", ".", "image", ".", "resize_image_with_crop_or_pad", "(", "image", ",", "height", ",", "width", ")", "\n", "\n", "", "image_summary", "(", "\"resized_image\"", ",", "image", ")", "\n", "\n", "# Randomly distort the image.", "\n", "if", "is_training", ":", "\n", "    ", "image", "=", "distort_image", "(", "image", ",", "thread_id", ")", "\n", "\n", "", "image_summary", "(", "\"final_image\"", ",", "image", ")", "\n", "\n", "# Rescale to [-1,1] instead of [0, 1]", "\n", "image", "=", "tf", ".", "subtract", "(", "image", ",", "0.5", ")", "\n", "image", "=", "tf", ".", "multiply", "(", "image", ",", "2.0", ")", "\n", "return", "image", "\n", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.__init__": [[29, 31], ["inference_wrapper_base.InferenceWrapperBase.__init__"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.configuration.TrainingConfig.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "InferenceWrapper", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.build_model": [[32, 36], ["show_and_tell_model.ShowAndTellModel", "show_and_tell_model.ShowAndTellModel.build"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build"], ["", "def", "build_model", "(", "self", ",", "model_config", ")", ":", "\n", "    ", "model", "=", "show_and_tell_model", ".", "ShowAndTellModel", "(", "model_config", ",", "mode", "=", "\"inference\"", ")", "\n", "model", ".", "build", "(", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.feed_image": [[37, 40], ["sess.run"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run"], ["", "def", "feed_image", "(", "self", ",", "sess", ",", "encoded_images", ")", ":", "\n", "    ", "initial_state_0", ",", "initial_state_1", ",", "initial_state_2", ",", "initial_state_3", ",", "initial_state_4", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"decoder/decoder_0/initial_state_0:0\"", ",", "\"decoder/decoder_1/initial_state_1:0\"", ",", "\"decoder/decoder_2/initial_state_2:0\"", ",", "\"decoder/decoder_3/initial_state_3:0\"", ",", "\"decoder/decoder_4/initial_state_4:0\"", "]", ",", "feed_dict", "=", "{", "\"image_feed_0:0\"", ":", "encoded_images", "[", "0", "]", ",", "\"image_feed_1:0\"", ":", "encoded_images", "[", "1", "]", ",", "\"image_feed_2:0\"", ":", "encoded_images", "[", "2", "]", ",", "\"image_feed_3:0\"", ":", "encoded_images", "[", "3", "]", ",", "\"image_feed_4:0\"", ":", "encoded_images", "[", "4", "]", "}", ")", "\n", "return", "initial_state_0", ",", "initial_state_1", ",", "initial_state_2", ",", "initial_state_3", ",", "initial_state_4", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inference_wrapper.InferenceWrapper.inference_step": [[41, 53], ["sess.run", "sess.run", "sess.run", "sess.run", "sess.run"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.evaluate.run"], ["", "def", "inference_step", "(", "self", ",", "sess", ",", "input_feed", ",", "state_feed", ",", "order", ")", ":", "\n", "    ", "if", "order", "==", "0", ":", "\n", "      ", "softmax_output", ",", "state_output", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"softmax_0:0\"", ",", "\"decoder/decoder_0/state_0:0\"", "]", ",", "feed_dict", "=", "{", "\"input_feed_0:0\"", ":", "input_feed", ",", "\"decoder/decoder_0/state_feed_0:0\"", ":", "state_feed", ",", "}", ")", "\n", "", "elif", "order", "==", "1", ":", "\n", "      ", "softmax_output", ",", "state_output", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"softmax_1:0\"", ",", "\"decoder/decoder_1/state_1:0\"", "]", ",", "feed_dict", "=", "{", "\"input_feed_1:0\"", ":", "input_feed", ",", "\"decoder/decoder_1/state_feed_1:0\"", ":", "state_feed", ",", "}", ")", "\n", "", "elif", "order", "==", "2", ":", "\n", "      ", "softmax_output", ",", "state_output", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"softmax_2:0\"", ",", "\"decoder/decoder_2/state_2:0\"", "]", ",", "feed_dict", "=", "{", "\"input_feed_2:0\"", ":", "input_feed", ",", "\"decoder/decoder_2/state_feed_2:0\"", ":", "state_feed", ",", "}", ")", "\n", "", "elif", "order", "==", "3", ":", "\n", "      ", "softmax_output", ",", "state_output", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"softmax_3:0\"", ",", "\"decoder/decoder_3/state_3:0\"", "]", ",", "feed_dict", "=", "{", "\"input_feed_3:0\"", ":", "input_feed", ",", "\"decoder/decoder_3/state_feed_3:0\"", ":", "state_feed", ",", "}", ")", "\n", "", "else", ":", "\n", "      ", "softmax_output", ",", "state_output", "=", "sess", ".", "run", "(", "fetches", "=", "[", "\"softmax_4:0\"", ",", "\"decoder/decoder_4/state_4:0\"", "]", ",", "feed_dict", "=", "{", "\"input_feed_4:0\"", ":", "input_feed", ",", "\"decoder/decoder_4/state_feed_4:0\"", ":", "state_feed", ",", "}", ")", "\n", "", "return", "softmax_output", ",", "state_output", ",", "None", "", "", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.__init__": [[34, 114], ["tensorflow.TFRecordReader", "tensorflow.random_uniform_initializer"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "config", ",", "mode", ",", "train_inception", "=", "False", ")", ":", "\n", "    ", "assert", "mode", "in", "[", "\"train\"", ",", "\"eval\"", ",", "\"inference\"", "]", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "train_inception", "=", "train_inception", "\n", "\n", "# Reader for the input data.", "\n", "self", ".", "reader", "=", "tf", ".", "TFRecordReader", "(", ")", "\n", "\n", "self", ".", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "\n", "minval", "=", "-", "self", ".", "config", ".", "initializer_scale", ",", "\n", "maxval", "=", "self", ".", "config", ".", "initializer_scale", ")", "\n", "\n", "# Each one is a float32 Tensor with shape [batch_size, height, width, channels].", "\n", "self", ".", "images_0", "=", "None", "\n", "self", ".", "images_1", "=", "None", "\n", "self", ".", "images_2", "=", "None", "\n", "self", ".", "images_3", "=", "None", "\n", "self", ".", "images_4", "=", "None", "\n", "\n", "# Each one is an int32 Tensor with shape [batch_size, padded_length].", "\n", "self", ".", "input_seqs_0", "=", "None", "\n", "self", ".", "input_seqs_1", "=", "None", "\n", "self", ".", "input_seqs_2", "=", "None", "\n", "self", ".", "input_seqs_3", "=", "None", "\n", "self", ".", "input_seqs_4", "=", "None", "\n", "\n", "# Each one is an int32 Tensor with shape [batch_size, padded_length].", "\n", "self", ".", "target_seqs_0", "=", "None", "\n", "self", ".", "target_seqs_1", "=", "None", "\n", "self", ".", "target_seqs_2", "=", "None", "\n", "self", ".", "target_seqs_3", "=", "None", "\n", "self", ".", "target_seqs_4", "=", "None", "\n", "\n", "# Each one is an int32 0/1 Tensor with shape [batch_size, padded_length].", "\n", "self", ".", "input_mask_0", "=", "None", "\n", "self", ".", "input_mask_1", "=", "None", "\n", "self", ".", "input_mask_2", "=", "None", "\n", "self", ".", "input_mask_3", "=", "None", "\n", "self", ".", "input_mask_4", "=", "None", "\n", "\n", "# Each one is a float32 Tensor with shape [batch_size, embedding_size].", "\n", "self", ".", "image_embeddings_0", "=", "None", "\n", "self", ".", "image_embeddings_1", "=", "None", "\n", "self", ".", "image_embeddings_2", "=", "None", "\n", "self", ".", "image_embeddings_3", "=", "None", "\n", "self", ".", "image_embeddings_4", "=", "None", "\n", "\n", "# Each one is a float32 Tensor with shape [batch_size, padded_length, embedding_size].", "\n", "self", ".", "seq_embeddings_0", "=", "None", "\n", "self", ".", "seq_embeddings_1", "=", "None", "\n", "self", ".", "seq_embeddings_2", "=", "None", "\n", "self", ".", "seq_embeddings_3", "=", "None", "\n", "self", ".", "seq_embeddings_4", "=", "None", "\n", "\n", "# Each one is a float32 scalar Tensor; the total loss for the trainer to optimize.", "\n", "self", ".", "total_loss", "=", "None", "\n", "\n", "# Each one is a float32 Tensor with shape [batch_size * padded_length].", "\n", "self", ".", "target_cross_entropy_losses_0", "=", "None", "\n", "self", ".", "target_cross_entropy_losses_1", "=", "None", "\n", "self", ".", "target_cross_entropy_losses_2", "=", "None", "\n", "self", ".", "target_cross_entropy_losses_3", "=", "None", "\n", "self", ".", "target_cross_entropy_losses_4", "=", "None", "\n", "\n", "# Each one is a float32 Tensor with shape [batch_size * padded_length].", "\n", "self", ".", "target_cross_entropy_loss_weights_0", "=", "None", "\n", "self", ".", "target_cross_entropy_loss_weights_1", "=", "None", "\n", "self", ".", "target_cross_entropy_loss_weights_2", "=", "None", "\n", "self", ".", "target_cross_entropy_loss_weights_3", "=", "None", "\n", "self", ".", "target_cross_entropy_loss_weights_4", "=", "None", "\n", "\n", "# Collection of variables from the inception submodel.", "\n", "self", ".", "inception_variables", "=", "[", "]", "\n", "\n", "# Function to restore the inception submodel from checkpoint.", "\n", "self", ".", "init_fn", "=", "None", "\n", "\n", "# Global step Tensor.", "\n", "self", ".", "global_step", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training": [[115, 117], ["None"], "methods", ["None"], ["", "def", "is_training", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "mode", "==", "\"train\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image": [[118, 125], ["im2txt.ops.image_processing.process_image", "show_and_tell_model.ShowAndTellModel.is_training"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training"], ["", "def", "process_image", "(", "self", ",", "encoded_image", ",", "thread_id", "=", "0", ")", ":", "\n", "    ", "return", "image_processing", ".", "process_image", "(", "encoded_image", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "\n", "height", "=", "self", ".", "config", ".", "image_height", ",", "\n", "width", "=", "self", ".", "config", ".", "image_width", ",", "\n", "thread_id", "=", "thread_id", ",", "\n", "image_format", "=", "self", ".", "config", ".", "image_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_inputs": [[126, 226], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "im2txt.ops.inputs.prefetch_input_data", "range", "im2txt.ops.inputs.batch_with_dynamic_pad", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "im2txt.ops.inputs.prefetch_input_data.dequeue", "im2txt.ops.inputs.parse_sequence_example", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "show_and_tell_model.ShowAndTellModel.process_image", "images_and_captions.append", "show_and_tell_model.ShowAndTellModel.is_training"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.prefetch_input_data", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.batch_with_dynamic_pad", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.inputs.parse_sequence_example", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.process_image", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training"], ["", "def", "build_inputs", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "# In inference mode, images and inputs are fed via placeholders.", "\n", "      ", "image_feed_0", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"image_feed_0\"", ")", "\n", "image_feed_1", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"image_feed_1\"", ")", "\n", "image_feed_2", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"image_feed_2\"", ")", "\n", "image_feed_3", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"image_feed_3\"", ")", "\n", "image_feed_4", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"image_feed_4\"", ")", "\n", "\n", "input_feed_0", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int64", ",", "shape", "=", "[", "None", "]", ",", "name", "=", "\"input_feed_0\"", ")", "\n", "input_feed_1", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int64", ",", "shape", "=", "[", "None", "]", ",", "name", "=", "\"input_feed_1\"", ")", "\n", "input_feed_2", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int64", ",", "shape", "=", "[", "None", "]", ",", "name", "=", "\"input_feed_2\"", ")", "\n", "input_feed_3", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int64", ",", "shape", "=", "[", "None", "]", ",", "name", "=", "\"input_feed_3\"", ")", "\n", "input_feed_4", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int64", ",", "shape", "=", "[", "None", "]", ",", "name", "=", "\"input_feed_4\"", ")", "\n", "\n", "# Process each image and insert batch dimensions.", "\n", "images_0", "=", "tf", ".", "expand_dims", "(", "self", ".", "process_image", "(", "image_feed_0", ")", ",", "0", ")", "\n", "images_1", "=", "tf", ".", "expand_dims", "(", "self", ".", "process_image", "(", "image_feed_1", ")", ",", "0", ")", "\n", "images_2", "=", "tf", ".", "expand_dims", "(", "self", ".", "process_image", "(", "image_feed_2", ")", ",", "0", ")", "\n", "images_3", "=", "tf", ".", "expand_dims", "(", "self", ".", "process_image", "(", "image_feed_3", ")", ",", "0", ")", "\n", "images_4", "=", "tf", ".", "expand_dims", "(", "self", ".", "process_image", "(", "image_feed_4", ")", ",", "0", ")", "\n", "\n", "input_seqs_0", "=", "tf", ".", "expand_dims", "(", "input_feed_0", ",", "1", ")", "\n", "input_seqs_1", "=", "tf", ".", "expand_dims", "(", "input_feed_1", ",", "1", ")", "\n", "input_seqs_2", "=", "tf", ".", "expand_dims", "(", "input_feed_2", ",", "1", ")", "\n", "input_seqs_3", "=", "tf", ".", "expand_dims", "(", "input_feed_3", ",", "1", ")", "\n", "input_seqs_4", "=", "tf", ".", "expand_dims", "(", "input_feed_4", ",", "1", ")", "\n", "\n", "# No target sequences or input mask in inference mode.", "\n", "target_seqs_0", "=", "None", "\n", "target_seqs_1", "=", "None", "\n", "target_seqs_2", "=", "None", "\n", "target_seqs_3", "=", "None", "\n", "target_seqs_4", "=", "None", "\n", "\n", "input_mask_0", "=", "None", "\n", "input_mask_1", "=", "None", "\n", "input_mask_2", "=", "None", "\n", "input_mask_3", "=", "None", "\n", "input_mask_4", "=", "None", "\n", "\n", "", "else", ":", "\n", "# Prefetch serialized SequenceExample protos.", "\n", "      ", "input_queue", "=", "input_ops", ".", "prefetch_input_data", "(", "\n", "self", ".", "reader", ",", "\n", "self", ".", "config", ".", "input_file_pattern", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "\n", "batch_size", "=", "self", ".", "config", ".", "batch_size", ",", "\n", "values_per_shard", "=", "self", ".", "config", ".", "values_per_input_shard", ",", "\n", "input_queue_capacity_factor", "=", "self", ".", "config", ".", "input_queue_capacity_factor", ",", "\n", "num_reader_threads", "=", "self", ".", "config", ".", "num_input_reader_threads", ")", "\n", "\n", "self", ".", "input_queue", "=", "input_queue", "\n", "\n", "assert", "self", ".", "config", ".", "num_preprocess_threads", "%", "2", "==", "0", "\n", "\n", "# Image processing and random distortion. Split across multiple threads", "\n", "# with each thread applying a slightly different distortion.", "\n", "images_and_captions", "=", "[", "]", "\n", "for", "thread_id", "in", "range", "(", "self", ".", "config", ".", "num_preprocess_threads", ")", ":", "\n", "        ", "serialized_sequence_example", "=", "input_queue", ".", "dequeue", "(", ")", "\n", "encoded_image_0", ",", "caption_0", ",", "encoded_image_1", ",", "caption_1", ",", "encoded_image_2", ",", "caption_2", ",", "encoded_image_3", ",", "caption_3", ",", "encoded_image_4", ",", "caption_4", "=", "input_ops", ".", "parse_sequence_example", "(", "serialized_sequence_example", ",", "image_feature", "=", "self", ".", "config", ".", "image_feature_name", ",", "caption_feature", "=", "self", ".", "config", ".", "caption_feature_name", ")", "\n", "image_0", "=", "self", ".", "process_image", "(", "encoded_image_0", ",", "thread_id", "=", "thread_id", ")", "\n", "image_1", "=", "self", ".", "process_image", "(", "encoded_image_1", ",", "thread_id", "=", "thread_id", ")", "\n", "image_2", "=", "self", ".", "process_image", "(", "encoded_image_2", ",", "thread_id", "=", "thread_id", ")", "\n", "image_3", "=", "self", ".", "process_image", "(", "encoded_image_3", ",", "thread_id", "=", "thread_id", ")", "\n", "image_4", "=", "self", ".", "process_image", "(", "encoded_image_4", ",", "thread_id", "=", "thread_id", ")", "\n", "images_and_captions", ".", "append", "(", "[", "image_0", ",", "caption_0", ",", "image_1", ",", "caption_1", ",", "image_2", ",", "caption_2", ",", "image_3", ",", "caption_3", ",", "image_4", ",", "caption_4", "]", ")", "\n", "\n", "", "self", ".", "images_and_captions", "=", "images_and_captions", "\n", "\n", "queue_capacity", "=", "(", "2", "*", "self", ".", "config", ".", "num_preprocess_threads", "*", "self", ".", "config", ".", "batch_size", ")", "#200", "\n", "images_0", ",", "input_seqs_0", ",", "target_seqs_0", ",", "input_mask_0", ",", "images_1", ",", "input_seqs_1", ",", "target_seqs_1", ",", "input_mask_1", ",", "images_2", ",", "input_seqs_2", ",", "target_seqs_2", ",", "input_mask_2", ",", "images_3", ",", "input_seqs_3", ",", "target_seqs_3", ",", "input_mask_3", ",", "images_4", ",", "input_seqs_4", ",", "target_seqs_4", ",", "input_mask_4", "=", "(", "\n", "input_ops", ".", "batch_with_dynamic_pad", "(", "images_and_captions", ",", "\n", "batch_size", "=", "self", ".", "config", ".", "batch_size", ",", "\n", "queue_capacity", "=", "queue_capacity", ")", ")", "\n", "", "self", ".", "images_0", "=", "images_0", "\n", "self", ".", "input_seqs_0", "=", "input_seqs_0", "\n", "self", ".", "target_seqs_0", "=", "target_seqs_0", "\n", "self", ".", "input_mask_0", "=", "input_mask_0", "\n", "\n", "self", ".", "images_1", "=", "images_1", "\n", "self", ".", "input_seqs_1", "=", "input_seqs_1", "\n", "self", ".", "target_seqs_1", "=", "target_seqs_1", "\n", "self", ".", "input_mask_1", "=", "input_mask_1", "\n", "\n", "self", ".", "images_2", "=", "images_2", "\n", "self", ".", "input_seqs_2", "=", "input_seqs_2", "\n", "self", ".", "target_seqs_2", "=", "target_seqs_2", "\n", "self", ".", "input_mask_2", "=", "input_mask_2", "\n", "\n", "self", ".", "images_3", "=", "images_3", "\n", "self", ".", "input_seqs_3", "=", "input_seqs_3", "\n", "self", ".", "target_seqs_3", "=", "target_seqs_3", "\n", "self", ".", "input_mask_3", "=", "input_mask_3", "\n", "\n", "self", ".", "images_4", "=", "images_4", "\n", "self", ".", "input_seqs_4", "=", "input_seqs_4", "\n", "self", ".", "target_seqs_4", "=", "target_seqs_4", "\n", "self", ".", "input_mask_4", "=", "input_mask_4", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_image_embeddings": [[227, 315], ["im2txt.ops.image_embedding.inception_v3", "tensorflow.get_collection", "im2txt.ops.image_embedding.inception_v3", "tensorflow.get_collection", "im2txt.ops.image_embedding.inception_v3", "tensorflow.get_collection", "im2txt.ops.image_embedding.inception_v3", "tensorflow.get_collection", "im2txt.ops.image_embedding.inception_v3", "tensorflow.get_collection", "tensorflow.constant", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "scope.reuse_variables", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "tensorflow.contrib.layers.fully_connected", "show_and_tell_model.ShowAndTellModel.is_training", "show_and_tell_model.ShowAndTellModel.is_training", "show_and_tell_model.ShowAndTellModel.is_training", "show_and_tell_model.ShowAndTellModel.is_training", "show_and_tell_model.ShowAndTellModel.is_training"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.image_embedding.inception_v3", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.is_training"], ["", "def", "build_image_embeddings", "(", "self", ")", ":", "\n", "# Get image representation via Inception V3 model", "\n", "    ", "inception_output_0", "=", "image_embedding", ".", "inception_v3", "(", "\n", "self", ".", "images_0", ",", "\n", "trainable", "=", "self", ".", "train_inception", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "self", ".", "inception_variables", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "\n", "inception_output_1", "=", "image_embedding", ".", "inception_v3", "(", "\n", "self", ".", "images_1", ",", "\n", "trainable", "=", "self", ".", "train_inception", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "self", ".", "inception_variables", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "\n", "inception_output_2", "=", "image_embedding", ".", "inception_v3", "(", "\n", "self", ".", "images_2", ",", "\n", "trainable", "=", "self", ".", "train_inception", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "self", ".", "inception_variables", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "\n", "inception_output_3", "=", "image_embedding", ".", "inception_v3", "(", "\n", "self", ".", "images_3", ",", "\n", "trainable", "=", "self", ".", "train_inception", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "self", ".", "inception_variables", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "\n", "inception_output_4", "=", "image_embedding", ".", "inception_v3", "(", "\n", "self", ".", "images_4", ",", "\n", "trainable", "=", "self", ".", "train_inception", ",", "\n", "is_training", "=", "self", ".", "is_training", "(", ")", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "self", ".", "inception_variables", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "\"InceptionV3\"", ")", "\n", "\n", "# Map inception output into embedding space.", "\n", "with", "tf", ".", "variable_scope", "(", "\"image_embedding\"", ")", "as", "scope", ":", "\n", "      ", "image_embeddings_0", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "inception_output_0", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "embedding_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "biases_initializer", "=", "None", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "scope", ".", "reuse_variables", "(", ")", "\n", "\n", "image_embeddings_1", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "inception_output_1", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "embedding_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "biases_initializer", "=", "None", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "image_embeddings_2", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "inception_output_2", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "embedding_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "biases_initializer", "=", "None", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "image_embeddings_3", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "inception_output_3", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "embedding_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "biases_initializer", "=", "None", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "image_embeddings_4", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "inception_output_4", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "embedding_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "biases_initializer", "=", "None", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "", "tf", ".", "constant", "(", "self", ".", "config", ".", "embedding_size", ",", "name", "=", "\"embedding_size\"", ")", "\n", "\n", "self", ".", "image_embeddings_0", "=", "image_embeddings_0", "\n", "self", ".", "image_embeddings_1", "=", "image_embeddings_1", "\n", "self", ".", "image_embeddings_2", "=", "image_embeddings_2", "\n", "self", ".", "image_embeddings_3", "=", "image_embeddings_3", "\n", "self", ".", "image_embeddings_4", "=", "image_embeddings_4", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_seq_embeddings": [[316, 357], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup"], "methods", ["None"], ["", "def", "build_seq_embeddings", "(", "self", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"seq_embedding_0\"", ")", ":", "\n", "      ", "embedding_map_0", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"map_0\"", ",", "\n", "shape", "=", "[", "self", ".", "config", ".", "vocab_size", ",", "self", ".", "config", ".", "embedding_size", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ")", "\n", "seq_embeddings_0", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_map_0", ",", "self", ".", "input_seqs_0", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"seq_embedding_1\"", ")", ":", "\n", "      ", "embedding_map_1", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"map_1\"", ",", "\n", "shape", "=", "[", "self", ".", "config", ".", "vocab_size", ",", "self", ".", "config", ".", "embedding_size", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ")", "\n", "seq_embeddings_1", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_map_1", ",", "self", ".", "input_seqs_1", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"seq_embedding_2\"", ")", ":", "\n", "      ", "embedding_map_2", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"map_2\"", ",", "\n", "shape", "=", "[", "self", ".", "config", ".", "vocab_size", ",", "self", ".", "config", ".", "embedding_size", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ")", "\n", "seq_embeddings_2", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_map_2", ",", "self", ".", "input_seqs_2", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"seq_embedding_3\"", ")", ":", "\n", "      ", "embedding_map_3", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"map_3\"", ",", "\n", "shape", "=", "[", "self", ".", "config", ".", "vocab_size", ",", "self", ".", "config", ".", "embedding_size", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ")", "\n", "seq_embeddings_3", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_map_3", ",", "self", ".", "input_seqs_3", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"seq_embedding_4\"", ")", ":", "\n", "      ", "embedding_map_4", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"map_4\"", ",", "\n", "shape", "=", "[", "self", ".", "config", ".", "vocab_size", ",", "self", ".", "config", ".", "embedding_size", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ")", "\n", "seq_embeddings_4", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_map_4", ",", "self", ".", "input_seqs_4", ")", "\n", "\n", "", "self", ".", "seq_embeddings_0", "=", "seq_embeddings_0", "\n", "self", ".", "seq_embeddings_1", "=", "seq_embeddings_1", "\n", "self", ".", "seq_embeddings_2", "=", "seq_embeddings_2", "\n", "self", ".", "seq_embeddings_3", "=", "seq_embeddings_3", "\n", "self", ".", "seq_embeddings_4", "=", "seq_embeddings_4", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_model": [[359, 664], ["tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.contrib.rnn.DropoutWrapper", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.zero_state", "tensorflow.contrib.rnn.DropoutWrapper.", "encoder_scope.reuse_variables", "tensorflow.stack", "tensorflow.nn.dynamic_rnn", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.reshape", "tensorflow.to_float", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.div", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.div", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.div", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.div", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.div", "tensorflow.losses.add_loss", "tensorflow.losses.add_loss", "tensorflow.losses.add_loss", "tensorflow.losses.add_loss", "tensorflow.losses.add_loss", "tensorflow.losses.get_total_loss", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.trainable_variables", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.", "decoder_0_scope.reuse_variables", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.", "decoder_1_scope.reuse_variables", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.", "decoder_2_scope.reuse_variables", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.", "decoder_3_scope.reuse_variables", "tensorflow.variable_scope", "tensorflow.contrib.rnn.DropoutWrapper.", "decoder_4_scope.reuse_variables", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.summary.histogram", "tensorflow.concat", "tensorflow.placeholder", "tensorflow.split", "tensorflow.contrib.rnn.DropoutWrapper.", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.placeholder", "tensorflow.split", "tensorflow.contrib.rnn.DropoutWrapper.", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.placeholder", "tensorflow.split", "tensorflow.contrib.rnn.DropoutWrapper.", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.placeholder", "tensorflow.split", "tensorflow.contrib.rnn.DropoutWrapper.", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.concat", "tensorflow.placeholder", "tensorflow.split", "tensorflow.contrib.rnn.DropoutWrapper.", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "show_and_tell_model.ShowAndTellModel.image_embeddings_0.get_shape", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "sum", "sum", "sum", "sum", "sum"], "methods", ["None"], ["", "def", "build_model", "(", "self", ")", ":", "\n", "# Define cell", "\n", "    ", "lstm_cell_enc", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "lstm_cell_dec_0", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "lstm_cell_dec_1", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "lstm_cell_dec_2", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "lstm_cell_dec_3", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "lstm_cell_dec_4", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", "=", "self", ".", "config", ".", "num_lstm_units", ",", "state_is_tuple", "=", "True", ")", "\n", "\n", "# Adds dropout when training", "\n", "if", "self", ".", "mode", "==", "\"train\"", ":", "\n", "      ", "lstm_cell_enc", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_enc", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "lstm_cell_dec_0", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_dec_0", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "lstm_cell_dec_1", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_dec_1", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "lstm_cell_dec_2", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_dec_2", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "lstm_cell_dec_3", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_dec_3", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "lstm_cell_dec_4", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "lstm_cell_dec_4", ",", "input_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ",", "output_keep_prob", "=", "self", ".", "config", ".", "lstm_dropout_keep_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "encoder_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "      ", "zero_state", "=", "lstm_cell_enc", ".", "zero_state", "(", "batch_size", "=", "self", ".", "image_embeddings_0", ".", "get_shape", "(", ")", "[", "0", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "_", ",", "initial_state_enc", "=", "lstm_cell_enc", "(", "self", ".", "image_embeddings_0", ",", "zero_state", ")", "\n", "\n", "# Allow the LSTM variables to be reused.", "\n", "encoder_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "encoder_inputs", "=", "tf", ".", "stack", "(", "[", "self", ".", "image_embeddings_1", ",", "self", ".", "image_embeddings_2", ",", "self", ".", "image_embeddings_3", ",", "self", ".", "image_embeddings_4", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Run the batch of sequence embeddings through the LSTM.", "\n", "encoder_outputs", ",", "encoder_states", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_enc", ",", "\n", "inputs", "=", "encoder_inputs", ",", "\n", "initial_state", "=", "initial_state_enc", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "encoder_scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"decoder\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_scope", ":", "\n", "# Multiple decoders", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"decoder_0\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_0_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "        ", "_", ",", "initial_state_0", "=", "lstm_cell_dec_0", "(", "self", ".", "image_embeddings_0", ",", "encoder_states", ")", "\n", "\n", "# Run the batch of sequence embeddings through the LSTM.", "\n", "decoder_0_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "          ", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "initial_state_0", ",", "name", "=", "\"initial_state_0\"", ")", "\n", "\n", "# Placeholder for feeding a batch of concatenated states.", "\n", "state_feed_0", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "None", ",", "sum", "(", "lstm_cell_dec_0", ".", "state_size", ")", "]", ",", "name", "=", "\"state_feed_0\"", ")", "\n", "state_tuple_0", "=", "tf", ".", "split", "(", "value", "=", "state_feed_0", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "\n", "# Run a single LSTM step.", "\n", "decoder_outputs_0", ",", "state_tuple_0", "=", "lstm_cell_dec_0", "(", "inputs", "=", "tf", ".", "squeeze", "(", "self", ".", "seq_embeddings_0", ",", "axis", "=", "[", "1", "]", ")", ",", "state", "=", "state_tuple_0", ")", "\n", "\n", "# Concatentate the resulting state.", "\n", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "state_tuple_0", ",", "name", "=", "\"state_0\"", ")", "\n", "\n", "", "else", ":", "\n", "          ", "decoder_outputs_0", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_dec_0", ",", "\n", "inputs", "=", "self", ".", "seq_embeddings_0", ",", "\n", "initial_state", "=", "initial_state_0", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "decoder_0_scope", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"decoder_1\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_1_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "        ", "_", ",", "initial_state_1", "=", "lstm_cell_dec_1", "(", "self", ".", "image_embeddings_1", ",", "encoder_states", ")", "\n", "\n", "# Allow the LSTM variables to be reused.", "\n", "decoder_1_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "          ", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "initial_state_1", ",", "name", "=", "\"initial_state_1\"", ")", "\n", "\n", "# Placeholder for feeding a batch of concatenated states.", "\n", "state_feed_1", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "\n", "shape", "=", "[", "None", ",", "sum", "(", "lstm_cell_dec_1", ".", "state_size", ")", "]", ",", "\n", "name", "=", "\"state_feed_1\"", ")", "\n", "state_tuple_1", "=", "tf", ".", "split", "(", "value", "=", "state_feed_1", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "\n", "# Run a single LSTM step.", "\n", "decoder_outputs_1", ",", "state_tuple_1", "=", "lstm_cell_dec_1", "(", "\n", "inputs", "=", "tf", ".", "squeeze", "(", "self", ".", "seq_embeddings_1", ",", "axis", "=", "[", "1", "]", ")", ",", "\n", "state", "=", "state_tuple_1", ")", "\n", "\n", "# Concatentate the resulting state.", "\n", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "state_tuple_1", ",", "name", "=", "\"state_1\"", ")", "\n", "\n", "", "else", ":", "\n", "          ", "decoder_outputs_1", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_dec_1", ",", "\n", "inputs", "=", "self", ".", "seq_embeddings_1", ",", "\n", "initial_state", "=", "initial_state_1", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "decoder_1_scope", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"decoder_2\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_2_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "        ", "_", ",", "initial_state_2", "=", "lstm_cell_dec_2", "(", "self", ".", "image_embeddings_2", ",", "encoder_states", ")", "\n", "\n", "# Allow the LSTM variables to be reused.", "\n", "decoder_2_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "          ", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "initial_state_2", ",", "name", "=", "\"initial_state_2\"", ")", "\n", "\n", "# Placeholder for feeding a batch of concatenated states.", "\n", "state_feed_2", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "\n", "shape", "=", "[", "None", ",", "sum", "(", "lstm_cell_dec_2", ".", "state_size", ")", "]", ",", "\n", "name", "=", "\"state_feed_2\"", ")", "\n", "state_tuple_2", "=", "tf", ".", "split", "(", "value", "=", "state_feed_2", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "\n", "# Run a single LSTM step.", "\n", "decoder_outputs_2", ",", "state_tuple_2", "=", "lstm_cell_dec_2", "(", "\n", "inputs", "=", "tf", ".", "squeeze", "(", "self", ".", "seq_embeddings_2", ",", "axis", "=", "[", "1", "]", ")", ",", "\n", "state", "=", "state_tuple_2", ")", "\n", "\n", "# Concatentate the resulting state.", "\n", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "state_tuple_2", ",", "name", "=", "\"state_2\"", ")", "\n", "\n", "", "else", ":", "\n", "          ", "decoder_outputs_2", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_dec_2", ",", "\n", "inputs", "=", "self", ".", "seq_embeddings_2", ",", "\n", "initial_state", "=", "initial_state_2", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "decoder_2_scope", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"decoder_3\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_3_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "        ", "_", ",", "initial_state_3", "=", "lstm_cell_dec_3", "(", "self", ".", "image_embeddings_3", ",", "encoder_states", ")", "\n", "\n", "# Allow the LSTM variables to be reused.", "\n", "decoder_3_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "          ", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "initial_state_3", ",", "name", "=", "\"initial_state_3\"", ")", "\n", "\n", "# Placeholder for feeding a batch of concatenated states.", "\n", "state_feed_3", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "\n", "shape", "=", "[", "None", ",", "sum", "(", "lstm_cell_dec_3", ".", "state_size", ")", "]", ",", "\n", "name", "=", "\"state_feed_3\"", ")", "\n", "state_tuple_3", "=", "tf", ".", "split", "(", "value", "=", "state_feed_3", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "\n", "# Run a single LSTM step.", "\n", "decoder_outputs_3", ",", "state_tuple_3", "=", "lstm_cell_dec_3", "(", "\n", "inputs", "=", "tf", ".", "squeeze", "(", "self", ".", "seq_embeddings_3", ",", "axis", "=", "[", "1", "]", ")", ",", "\n", "state", "=", "state_tuple_3", ")", "\n", "\n", "# Concatentate the resulting state.", "\n", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "state_tuple_3", ",", "name", "=", "\"state_3\"", ")", "\n", "\n", "", "else", ":", "\n", "          ", "decoder_outputs_3", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_dec_3", ",", "\n", "inputs", "=", "self", ".", "seq_embeddings_3", ",", "\n", "initial_state", "=", "initial_state_3", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "decoder_3_scope", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"decoder_4\"", ",", "initializer", "=", "self", ".", "initializer", ")", "as", "decoder_4_scope", ":", "\n", "# Feed the image embeddings to set the initial LSTM state.", "\n", "        ", "_", ",", "initial_state_4", "=", "lstm_cell_dec_4", "(", "self", ".", "image_embeddings_4", ",", "encoder_states", ")", "\n", "\n", "# Allow the LSTM variables to be reused.", "\n", "decoder_4_scope", ".", "reuse_variables", "(", ")", "\n", "\n", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "          ", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "initial_state_4", ",", "name", "=", "\"initial_state_4\"", ")", "\n", "\n", "# Placeholder for feeding a batch of concatenated states.", "\n", "state_feed_4", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "\n", "shape", "=", "[", "None", ",", "sum", "(", "lstm_cell_dec_4", ".", "state_size", ")", "]", ",", "\n", "name", "=", "\"state_feed_4\"", ")", "\n", "state_tuple_4", "=", "tf", ".", "split", "(", "value", "=", "state_feed_4", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "\n", "# Run a single LSTM step.", "\n", "decoder_outputs_4", ",", "state_tuple_4", "=", "lstm_cell_dec_4", "(", "\n", "inputs", "=", "tf", ".", "squeeze", "(", "self", ".", "seq_embeddings_4", ",", "axis", "=", "[", "1", "]", ")", ",", "\n", "state", "=", "state_tuple_4", ")", "\n", "\n", "# Concatentate the resulting state.", "\n", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "state_tuple_4", ",", "name", "=", "\"state_4\"", ")", "\n", "\n", "", "else", ":", "\n", "          ", "decoder_outputs_4", ",", "_", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", "=", "lstm_cell_dec_4", ",", "\n", "inputs", "=", "self", ".", "seq_embeddings_4", ",", "\n", "initial_state", "=", "initial_state_4", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "scope", "=", "decoder_4_scope", ")", "\n", "\n", "# Stack batches vertically.", "\n", "", "", "", "decoder_outputs_0", "=", "tf", ".", "reshape", "(", "decoder_outputs_0", ",", "[", "-", "1", ",", "lstm_cell_dec_0", ".", "output_size", "]", ")", "\n", "decoder_outputs_1", "=", "tf", ".", "reshape", "(", "decoder_outputs_1", ",", "[", "-", "1", ",", "lstm_cell_dec_1", ".", "output_size", "]", ")", "\n", "decoder_outputs_2", "=", "tf", ".", "reshape", "(", "decoder_outputs_2", ",", "[", "-", "1", ",", "lstm_cell_dec_2", ".", "output_size", "]", ")", "\n", "decoder_outputs_3", "=", "tf", ".", "reshape", "(", "decoder_outputs_3", ",", "[", "-", "1", ",", "lstm_cell_dec_3", ".", "output_size", "]", ")", "\n", "decoder_outputs_4", "=", "tf", ".", "reshape", "(", "decoder_outputs_4", ",", "[", "-", "1", ",", "lstm_cell_dec_4", ".", "output_size", "]", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"logits_0\"", ")", "as", "logits_0_scope", ":", "\n", "      ", "logits_0", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "decoder_outputs_0", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "vocab_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "scope", "=", "logits_0_scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"logits_1\"", ")", "as", "logits_1_scope", ":", "\n", "      ", "logits_1", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "decoder_outputs_1", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "vocab_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "scope", "=", "logits_1_scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"logits_2\"", ")", "as", "logits_2_scope", ":", "\n", "      ", "logits_2", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "decoder_outputs_2", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "vocab_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "scope", "=", "logits_2_scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"logits_3\"", ")", "as", "logits_3_scope", ":", "\n", "      ", "logits_3", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "decoder_outputs_3", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "vocab_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "scope", "=", "logits_3_scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"logits_4\"", ")", "as", "logits_4_scope", ":", "\n", "      ", "logits_4", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "inputs", "=", "decoder_outputs_4", ",", "\n", "num_outputs", "=", "self", ".", "config", ".", "vocab_size", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "initializer", ",", "\n", "scope", "=", "logits_4_scope", ")", "\n", "\n", "", "if", "self", ".", "mode", "==", "\"inference\"", ":", "\n", "      ", "tf", ".", "nn", ".", "softmax", "(", "logits_0", ",", "name", "=", "\"softmax_0\"", ")", "\n", "tf", ".", "nn", ".", "softmax", "(", "logits_1", ",", "name", "=", "\"softmax_1\"", ")", "\n", "tf", ".", "nn", ".", "softmax", "(", "logits_2", ",", "name", "=", "\"softmax_2\"", ")", "\n", "tf", ".", "nn", ".", "softmax", "(", "logits_3", ",", "name", "=", "\"softmax_3\"", ")", "\n", "tf", ".", "nn", ".", "softmax", "(", "logits_4", ",", "name", "=", "\"softmax_4\"", ")", "\n", "\n", "", "else", ":", "\n", "      ", "targets_0", "=", "tf", ".", "reshape", "(", "self", ".", "target_seqs_0", ",", "[", "-", "1", "]", ")", "\n", "weights_0", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "self", ".", "input_mask_0", ",", "[", "-", "1", "]", ")", ")", "\n", "\n", "targets_1", "=", "tf", ".", "reshape", "(", "self", ".", "target_seqs_1", ",", "[", "-", "1", "]", ")", "\n", "weights_1", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "self", ".", "input_mask_1", ",", "[", "-", "1", "]", ")", ")", "\n", "\n", "targets_2", "=", "tf", ".", "reshape", "(", "self", ".", "target_seqs_2", ",", "[", "-", "1", "]", ")", "\n", "weights_2", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "self", ".", "input_mask_2", ",", "[", "-", "1", "]", ")", ")", "\n", "\n", "targets_3", "=", "tf", ".", "reshape", "(", "self", ".", "target_seqs_3", ",", "[", "-", "1", "]", ")", "\n", "weights_3", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "self", ".", "input_mask_3", ",", "[", "-", "1", "]", ")", ")", "\n", "\n", "targets_4", "=", "tf", ".", "reshape", "(", "self", ".", "target_seqs_4", ",", "[", "-", "1", "]", ")", "\n", "weights_4", "=", "tf", ".", "to_float", "(", "tf", ".", "reshape", "(", "self", ".", "input_mask_4", ",", "[", "-", "1", "]", ")", ")", "\n", "\n", "# Compute losses.", "\n", "losses_0", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "targets_0", ",", "logits", "=", "logits_0", ")", "\n", "batch_loss_0", "=", "tf", ".", "div", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "losses_0", ",", "weights_0", ")", ")", ",", "tf", ".", "reduce_sum", "(", "weights_0", ")", ",", "name", "=", "\"batch_loss_0\"", ")", "\n", "\n", "losses_1", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "targets_1", ",", "logits", "=", "logits_1", ")", "\n", "batch_loss_1", "=", "tf", ".", "div", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "losses_1", ",", "weights_1", ")", ")", ",", "tf", ".", "reduce_sum", "(", "weights_1", ")", ",", "name", "=", "\"batch_loss_1\"", ")", "\n", "\n", "losses_2", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "targets_2", ",", "logits", "=", "logits_2", ")", "\n", "batch_loss_2", "=", "tf", ".", "div", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "losses_2", ",", "weights_2", ")", ")", ",", "tf", ".", "reduce_sum", "(", "weights_2", ")", ",", "name", "=", "\"batch_loss_2\"", ")", "\n", "\n", "losses_3", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "targets_3", ",", "logits", "=", "logits_3", ")", "\n", "batch_loss_3", "=", "tf", ".", "div", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "losses_3", ",", "weights_3", ")", ")", ",", "tf", ".", "reduce_sum", "(", "weights_3", ")", ",", "name", "=", "\"batch_loss_3\"", ")", "\n", "\n", "losses_4", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "targets_4", ",", "logits", "=", "logits_4", ")", "\n", "batch_loss_4", "=", "tf", ".", "div", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "losses_4", ",", "weights_4", ")", ")", ",", "tf", ".", "reduce_sum", "(", "weights_4", ")", ",", "name", "=", "\"batch_loss_4\"", ")", "\n", "\n", "tf", ".", "losses", ".", "add_loss", "(", "batch_loss_0", ")", "\n", "tf", ".", "losses", ".", "add_loss", "(", "batch_loss_1", ")", "\n", "tf", ".", "losses", ".", "add_loss", "(", "batch_loss_2", ")", "\n", "tf", ".", "losses", ".", "add_loss", "(", "batch_loss_3", ")", "\n", "tf", ".", "losses", ".", "add_loss", "(", "batch_loss_4", ")", "\n", "\n", "total_loss", "=", "tf", ".", "losses", ".", "get_total_loss", "(", ")", "\n", "\n", "# Add summaries.", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/batch_loss_0\"", ",", "batch_loss_0", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/batch_loss_1\"", ",", "batch_loss_1", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/batch_loss_2\"", ",", "batch_loss_2", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/batch_loss_3\"", ",", "batch_loss_3", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/batch_loss_4\"", ",", "batch_loss_4", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"losses/total_loss\"", ",", "total_loss", ")", "\n", "\n", "for", "var", "in", "tf", ".", "trainable_variables", "(", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "histogram", "(", "\"parameters/\"", "+", "var", ".", "op", ".", "name", ",", "var", ")", "\n", "\n", "", "self", ".", "total_loss", "=", "total_loss", "\n", "self", ".", "target_cross_entropy_losses_0", "=", "losses_0", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_losses_1", "=", "losses_1", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_losses_2", "=", "losses_2", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_losses_3", "=", "losses_3", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_losses_4", "=", "losses_4", "# Used in evaluation.", "\n", "\n", "self", ".", "target_cross_entropy_loss_weights_0", "=", "weights_0", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_loss_weights_1", "=", "weights_1", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_loss_weights_2", "=", "weights_2", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_loss_weights_3", "=", "weights_3", "# Used in evaluation.", "\n", "self", ".", "target_cross_entropy_loss_weights_4", "=", "weights_4", "# Used in evaluation.", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.setup_inception_initializer": [[665, 675], ["tensorflow.train.Saver", "tensorflow.logging.info", "tensorflow.train.Saver.restore"], "methods", ["None"], ["", "", "def", "setup_inception_initializer", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "mode", "!=", "\"inference\"", ":", "\n", "      ", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "self", ".", "inception_variables", ")", "\n", "\n", "def", "restore_fn", "(", "sess", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Restoring Inception variables from checkpoint file %s\"", ",", "\n", "self", ".", "config", ".", "inception_checkpoint_file", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "self", ".", "config", ".", "inception_checkpoint_file", ")", "\n", "\n", "", "self", ".", "init_fn", "=", "restore_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.setup_global_step": [[676, 679], ["tensorflow.Variable"], "methods", ["None"], ["", "", "def", "setup_global_step", "(", "self", ")", ":", "\n", "    ", "global_step", "=", "tf", ".", "Variable", "(", "initial_value", "=", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ",", "collections", "=", "[", "tf", ".", "GraphKeys", ".", "GLOBAL_STEP", ",", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", "]", ")", "\n", "self", ".", "global_step", "=", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build": [[680, 687], ["show_and_tell_model.ShowAndTellModel.build_inputs", "show_and_tell_model.ShowAndTellModel.build_image_embeddings", "show_and_tell_model.ShowAndTellModel.build_seq_embeddings", "show_and_tell_model.ShowAndTellModel.build_model", "show_and_tell_model.ShowAndTellModel.setup_inception_initializer", "show_and_tell_model.ShowAndTellModel.setup_global_step"], "methods", ["home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_inputs", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_image_embeddings", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_seq_embeddings", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.build_model", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.setup_inception_initializer", "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.show_and_tell_model.ShowAndTellModel.setup_global_step"], ["", "def", "build", "(", "self", ")", ":", "\n", "    ", "self", ".", "build_inputs", "(", ")", "\n", "self", ".", "build_image_embeddings", "(", ")", "\n", "self", ".", "build_seq_embeddings", "(", ")", "\n", "self", ".", "build_model", "(", ")", "\n", "self", ".", "setup_inception_initializer", "(", ")", "\n", "self", ".", "setup_global_step", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.configuration.ModelConfig.__init__": [[26, 80], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "\"\"\"Sets the default model hyperparameters.\"\"\"", "\n", "# File pattern of sharded TFRecord file containing SequenceExample protos.", "\n", "# Must be provided in training and evaluation modes.", "\n", "self", ".", "input_file_pattern", "=", "None", "\n", "\n", "# Image format (\"jpeg\" or \"png\").", "\n", "self", ".", "image_format", "=", "\"jpeg\"", "\n", "\n", "# Approximate number of values per input shard. Used to ensure sufficient", "\n", "# mixing between shards in training.", "\n", "self", ".", "values_per_input_shard", "=", "350", "\n", "# Minimum number of shards to keep in the input queue.", "\n", "self", ".", "input_queue_capacity_factor", "=", "2", "\n", "# Number of threads for prefetching SequenceExample protos.", "\n", "# creo que pueden ser 5", "\n", "self", ".", "num_input_reader_threads", "=", "10", "\n", "\n", "# Name of the SequenceExample context feature containing image data.", "\n", "self", ".", "image_feature_name", "=", "\"image/data\"", "\n", "# Name of the SequenceExample feature list containing integer captions.", "\n", "self", ".", "caption_feature_name", "=", "\"image/caption_ids\"", "\n", "\n", "# Number of unique words in the vocab (plus 1, for <UNK>).", "\n", "# The default value is larger than the expected actual vocab size to allow", "\n", "# for differences between tokenizer versions used in preprocessing. There is", "\n", "# no harm in using a value greater than the actual vocab size, but using a", "\n", "# value less than the actual vocab size will result in an error.", "\n", "self", ".", "vocab_size", "=", "11840", "\n", "\n", "# Number of threads for image preprocessing. Should be a multiple of 2.", "\n", "self", ".", "num_preprocess_threads", "=", "10", "\n", "\n", "# Batch size.", "\n", "self", ".", "batch_size", "=", "10", "\n", "\n", "# File containing an Inception v3 checkpoint to initialize the variables", "\n", "# of the Inception model. Must be provided when starting training for the", "\n", "# first time.", "\n", "self", ".", "inception_checkpoint_file", "=", "None", "\n", "\n", "# Dimensions of Inception v3 input images.", "\n", "self", ".", "image_height", "=", "299", "\n", "self", ".", "image_width", "=", "299", "\n", "\n", "# Scale used to initialize model variables.", "\n", "self", ".", "initializer_scale", "=", "0.08", "\n", "\n", "# LSTM input and output dimensionality, respectively.", "\n", "self", ".", "embedding_size", "=", "512", "\n", "self", ".", "num_lstm_units", "=", "512", "\n", "\n", "# If < 1.0, the dropout keep probability applied to LSTM variables.", "\n", "self", ".", "lstm_dropout_keep_prob", "=", "0.7", "\n", "\n"]], "home.repos.pwc.inspect_result.dianaglzrico_neural-visual-storyteller.None.configuration.TrainingConfig.__init__": [[85, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "\"\"\"Sets the default training hyperparameters.\"\"\"", "\n", "# Number of examples per epoch of training data.", "\n", "self", ".", "num_examples_per_epoch", "=", "39650", "\n", "\n", "# Optimizer for training the model.", "\n", "self", ".", "optimizer", "=", "\"SGD\"", "\n", "\n", "# Learning rate for the initial phase of training.", "\n", "self", ".", "initial_learning_rate", "=", "2.0", "\n", "self", ".", "learning_rate_decay_factor", "=", "0.5", "\n", "self", ".", "num_epochs_per_decay", "=", "8.0", "\n", "\n", "# Learning rate when fine tuning the Inception v3 parameters.", "\n", "self", ".", "train_inception_learning_rate", "=", "0.0005", "\n", "\n", "# If not None, clip gradients to this value.", "\n", "self", ".", "clip_gradients", "=", "5.0", "\n", "\n", "# How many model checkpoints to keep.", "\n", "self", ".", "max_checkpoints_to_keep", "=", "5", "\n", "", "", ""]]}