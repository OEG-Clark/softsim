{"home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.load_args": [[18, 96], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "load_args", "(", "default_config", "=", "None", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Lipreading Project'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config-filename\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"Model configuration with ini format\"", ",", "\n", ")", "\n", "# -- ", "\n", "parser", ".", "add_argument", "(", "\"--data-filename\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The filename for sequence.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--landmarks-filename\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The filename for tracked landmarks.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dst-filename\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"The filename of the saved mouth patches or embedding.\"", ",", "\n", ")", "\n", "# -- for benchmark evaluation", "\n", "parser", ".", "add_argument", "(", "\"--data-dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The directory for sequence.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--landmarks-dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The directory for tracked landmarks.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--labels-filename\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The filename for labels.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dst-dir\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"The directory of saved mouth patches or embeddings.\"", ",", "\n", ")", "\n", "# -- feature extraction", "\n", "parser", ".", "add_argument", "(", "\n", "\"--feats-position\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "choices", "=", "[", "\"\"", ",", "\"mouth\"", ",", "\"resnet\"", ",", "\"conformer\"", "]", ",", "\n", "help", "=", "\"Specify the position for feature extraction.\"", ",", "\n", ")", "\n", "# -- utils", "\n", "parser", ".", "add_argument", "(", "\n", "\"--video-ext\"", ",", "\n", "default", "=", "\".mp4\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The extension for video files.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--landmarks-ext\"", ",", "\n", "default", "=", "\".pkl\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The extension for landmarks file.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gpu-idx\"", ",", "\n", "default", "=", "-", "1", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Inference in GPU when gpu_idx >= 0 or in CPU when gpu_idx < 0.\"", ",", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.benchmark_inference": [[100, 148], ["lipreading.utils.AverageMeter", "lipreading.utils.AverageMeter", "enumerate", "os.path.join", "os.path.join", "lipreader", "isinstance", "print", "isinstance", "line.split", "print", "lipreading.utils.AverageMeter.update", "lipreading.utils.AverageMeter.update", "print", "print", "lipreading.utils.save2avi", "print", "lipreading.utils.save2npz", "line.split", "metrics.measures.get_wer", "len", "metrics.measures.get_cer", "len", "os.path.join", "os.path.join", "groundtruth.split", "lipreader.cpu().detach().numpy", "len", "os.path.join", "os.path.join", "lipreader.cpu().detach", "lipreader.cpu"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.update", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.update", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2avi", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2npz", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_wer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_cer"], ["def", "benchmark_inference", "(", "lipreader", ",", "data_dir", ",", "landmarks_dir", ",", "lines", ",", "dst_dir", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"benchmark_inference.\n\n    :param lipreader: LipreadingPipeline object, contains the function for \\\n        facial tracking[option], facial pre-processing and lipreading inference.\n    :param data_dir: str, the directory for tracked landmarks.\n    :param landmarks_dir: str, the directory for tracked landmarks.\n    :param lines: List, the list of basename for each sequence.\n    :param dst_dir: str, The directory of the saved mouth patch or embeddings.\n    \"\"\"", "\n", "wer", "=", "AverageMeter", "(", ")", "\n", "cer", "=", "AverageMeter", "(", ")", "\n", "for", "idx", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "basename", ",", "groundtruth", "=", "line", ".", "split", "(", ")", "[", "0", "]", ",", "\" \"", ".", "join", "(", "line", ".", "split", "(", ")", "[", "1", ":", "]", ")", "\n", "data_filename", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "basename", "+", "args", ".", "video_ext", ")", "\n", "landmarks_filename", "=", "os", ".", "path", ".", "join", "(", "landmarks_dir", ",", "basename", "+", "args", ".", "landmarks_ext", ")", "\n", "output", "=", "lipreader", "(", "data_filename", ",", "landmarks_filename", ")", "\n", "if", "isinstance", "(", "output", ",", "str", ")", ":", "\n", "            ", "print", "(", "f\"hyp: {output}\"", ")", "\n", "if", "groundtruth", "is", "not", "None", ":", "\n", "                ", "print", "(", "f\"ref: {groundtruth}\"", ")", "\n", "wer", ".", "update", "(", "get_wer", "(", "output", ",", "groundtruth", ")", ",", "len", "(", "groundtruth", ".", "split", "(", ")", ")", ")", "\n", "cer", ".", "update", "(", "get_cer", "(", "output", ",", "groundtruth", ")", ",", "len", "(", "groundtruth", ")", ")", "\n", "print", "(", "\n", "f\"progress: {idx+1}/{len(lines)}\\tcur WER: {wer.val*100:.1f}\\t\"", "\n", "f\"cur CER: {cer.val*100:.1f}\\t\"", "\n", "f\"avg WER: {wer.avg*100:.1f}\\tavg CER: {cer.avg*100:.1f}\"", "\n", ")", "\n", "", "", "elif", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "            ", "print", "(", "\n", "f\"filename: {basename} has been saved to \"", "\n", "f\"{os.path.join(dst_dir, basename+'.avi')}.\"", "\n", ")", "\n", "save2avi", "(", "\n", "os", ".", "path", ".", "join", "(", "dst_dir", ",", "basename", "+", "\".avi\"", ")", ",", "\n", "data", "=", "output", "[", "0", "]", ",", "\n", "fps", "=", "output", "[", "1", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\n", "f\"filename: {basename} has been saved to \"", "\n", "f\"{os.path.join(dst_dir, basename+'.npz')}.\"", "\n", ")", "\n", "save2npz", "(", "\n", "os", ".", "path", ".", "join", "(", "dst_dir", ",", "basename", "+", "\".npz\"", ")", ",", "\n", "data", "=", "output", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.one_step_inference": [[150, 171], ["lipreader", "isinstance", "print", "isinstance", "print", "lipreading.utils.save2avi", "print", "lipreading.utils.save2npz", "lipreader.cpu().detach().numpy", "lipreader.cpu().detach", "lipreader.cpu"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2avi", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2npz"], ["", "def", "one_step_inference", "(", "lipreader", ",", "data_filename", ",", "landmarks_filename", ",", "dst_filename", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"one_step_inference.\n\n    :param lipreader: LipreadingPipeline object, contains the function for \\\n        facial tracking[option], facial pre-processing and lipreading inference.\n    :param data_filename: str, the filename for tracked landmarks.\n    :param landmarks_filename: str, the filename for tracked landmarks.\n    :param dst_filename: str, the filename of the saved mouth patch or embedding.\n    \"\"\"", "\n", "output", "=", "lipreader", "(", "data_filename", ",", "landmarks_filename", ")", "\n", "if", "isinstance", "(", "output", ",", "str", ")", ":", "\n", "        ", "print", "(", "f\"hyp: {output}\"", ")", "\n", "", "elif", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "        ", "assert", "dst_filename", "[", "-", "4", ":", "]", "==", "\".avi\"", ",", "f\"the ext of {dst_filename} should be .avi\"", "\n", "print", "(", "f\"mouth patch is saved to {dst_filename}.\"", ")", "\n", "save2avi", "(", "dst_filename", ",", "data", "=", "output", "[", "0", "]", ",", "fps", "=", "output", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "assert", "dst_filename", "[", "-", "4", ":", "]", "==", "\".npz\"", ",", "f\"the ext of {dst_filename} should be .npz\"", "\n", "print", "(", "f\"embedding is saved to {dst_filename}.\"", ")", "\n", "save2npz", "(", "dst_filename", ",", "data", "=", "output", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.main": [[173, 206], ["lipreading.subroutines.LipreadingPipeline", "torch.cuda.is_available", "torch.device", "main.one_step_inference", "os.path.isdir", "os.path.isfile", "main.benchmark_inference", "open().read().splitlines", "open().read", "open"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.one_step_inference", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.None.main.benchmark_inference"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# -- pick device for inference.", "\n", "    ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "gpu_idx", ">=", "0", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "f\"cuda:{args.gpu_idx}\"", ")", "\n", "", "else", ":", "\n", "        ", "device", "=", "\"cpu\"", "\n", "\n", "", "lipreader", "=", "LipreadingPipeline", "(", "\n", "config_filename", "=", "args", ".", "config_filename", ",", "\n", "feats_position", "=", "args", ".", "feats_position", ",", "\n", "device", "=", "device", ",", "\n", "face_track", "=", "not", "args", ".", "landmarks_filename", "and", "not", "args", ".", "landmarks_dir", ",", "\n", ")", "\n", "\n", "if", "args", ".", "data_filename", "is", "not", "None", ":", "\n", "        ", "one_step_inference", "(", "\n", "lipreader", ",", "\n", "args", ".", "data_filename", ",", "\n", "args", ".", "landmarks_filename", ",", "\n", "args", ".", "dst_filename", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "assert", "os", ".", "path", ".", "isdir", "(", "args", ".", "data_dir", ")", ",", "f\"{args.data_dir} is not a directory.\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "args", ".", "labels_filename", ")", ",", "f\"{args.labels_filename} does not exist.\"", "\n", "benchmark_inference", "(", "\n", "lipreader", ",", "\n", "args", ".", "data_dir", ",", "\n", "args", ".", "landmarks_dir", ",", "\n", "open", "(", "args", ".", "labels_filename", ")", ".", "read", "(", ")", ".", "splitlines", "(", ")", ",", "\n", "args", ".", "dst_dir", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.linear_interpolate": [[11, 24], ["range", "float"], "function", ["None"], ["def", "linear_interpolate", "(", "landmarks", ",", "start_idx", ",", "stop_idx", ")", ":", "\n", "    ", "\"\"\"linear_interpolate.\n\n    :param landmarks: ndarray, input landmarks to be interpolated.\n    :param start_idx: int, the start index for linear interpolation.\n    :param stop_idx: int, the stop for linear interpolation.\n    \"\"\"", "\n", "start_landmarks", "=", "landmarks", "[", "start_idx", "]", "\n", "stop_landmarks", "=", "landmarks", "[", "stop_idx", "]", "\n", "delta", "=", "stop_landmarks", "-", "start_landmarks", "\n", "for", "idx", "in", "range", "(", "1", ",", "stop_idx", "-", "start_idx", ")", ":", "\n", "        ", "landmarks", "[", "start_idx", "+", "idx", "]", "=", "start_landmarks", "+", "idx", "/", "float", "(", "stop_idx", "-", "start_idx", ")", "*", "delta", "\n", "", "return", "landmarks", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.warp_img": [[26, 39], ["skimage.transform.estimate_transform", "skimage.transform.warp", "warped.astype.astype"], "function", ["None"], ["", "def", "warp_img", "(", "src", ",", "dst", ",", "img", ",", "std_size", ")", ":", "\n", "    ", "\"\"\"warp_img.\n\n    :param src: ndarray, source coordinates.\n    :param dst: ndarray, destination coordinates. \n    :param img: ndarray, an input image.\n    :param std_size: tuple (rows, cols), shape of the output image generated.\n    \"\"\"", "\n", "tform", "=", "tf", ".", "estimate_transform", "(", "'similarity'", ",", "src", ",", "dst", ")", "# find the transformation matrix", "\n", "warped", "=", "tf", ".", "warp", "(", "img", ",", "inverse_map", "=", "tform", ".", "inverse", ",", "output_shape", "=", "std_size", ")", "# wrap the frame image", "\n", "warped", "=", "warped", "*", "255", "# note output from wrap is double image (value range [0,1])", "\n", "warped", "=", "warped", ".", "astype", "(", "'uint8'", ")", "\n", "return", "warped", ",", "tform", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.apply_transform": [[41, 53], ["skimage.transform.warp", "warped.astype.astype"], "function", ["None"], ["", "def", "apply_transform", "(", "transform", ",", "img", ",", "std_size", ")", ":", "\n", "    ", "\"\"\"apply_transform.\n\n    :param transform: Transform object, containing the transformation parameters \\\n                      and providing access to forward and inverse transformation functions.\n    :param img: ndarray, an input image.\n    :param std_size: tuple (rows, cols), shape of the output image generated.\n    \"\"\"", "\n", "warped", "=", "tf", ".", "warp", "(", "img", ",", "inverse_map", "=", "transform", ".", "inverse", ",", "output_shape", "=", "std_size", ")", "\n", "warped", "=", "warped", "*", "255", "# note output from wrap is double image (value range [0,1])", "\n", "warped", "=", "warped", ".", "astype", "(", "'uint8'", ")", "\n", "return", "warped", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.cut_patch": [[55, 87], ["numpy.mean", "numpy.copy", "Exception", "Exception", "Exception", "Exception", "int", "int", "int", "int", "round", "round", "round", "round", "round", "round", "round", "round"], "function", ["None"], ["", "def", "cut_patch", "(", "img", ",", "landmarks", ",", "height", ",", "width", ",", "threshold", "=", "5", ")", ":", "\n", "    ", "\"\"\"cut_patch.\n\n    :param img: ndarray, an input image.\n    :param landmarks: ndarray, the corresponding landmarks for the input image.\n    :param height: int, the distance from the centre to the side of of a bounding box.\n    :param width: int, the distance from the centre to the side of of a bounding box.\n    :param threshold: int, the threshold from the centre of a bounding box to the side of image.\n    \"\"\"", "\n", "center_x", ",", "center_y", "=", "np", ".", "mean", "(", "landmarks", ",", "axis", "=", "0", ")", "\n", "\n", "if", "center_y", "-", "height", "<", "0", ":", "\n", "        ", "center_y", "=", "height", "\n", "", "if", "center_y", "-", "height", "<", "0", "-", "threshold", ":", "\n", "        ", "raise", "Exception", "(", "'too much bias in height'", ")", "\n", "", "if", "center_x", "-", "width", "<", "0", ":", "\n", "        ", "center_x", "=", "width", "\n", "", "if", "center_x", "-", "width", "<", "0", "-", "threshold", ":", "\n", "        ", "raise", "Exception", "(", "'too much bias in width'", ")", "\n", "\n", "", "if", "center_y", "+", "height", ">", "img", ".", "shape", "[", "0", "]", ":", "\n", "        ", "center_y", "=", "img", ".", "shape", "[", "0", "]", "-", "height", "\n", "", "if", "center_y", "+", "height", ">", "img", ".", "shape", "[", "0", "]", "+", "threshold", ":", "\n", "        ", "raise", "Exception", "(", "'too much bias in height'", ")", "\n", "", "if", "center_x", "+", "width", ">", "img", ".", "shape", "[", "1", "]", ":", "\n", "        ", "center_x", "=", "img", ".", "shape", "[", "1", "]", "-", "width", "\n", "", "if", "center_x", "+", "width", ">", "img", ".", "shape", "[", "1", "]", "+", "threshold", ":", "\n", "        ", "raise", "Exception", "(", "'too much bias in width'", ")", "\n", "\n", "", "cutted_img", "=", "np", ".", "copy", "(", "img", "[", "int", "(", "round", "(", "center_y", ")", "-", "round", "(", "height", ")", ")", ":", "int", "(", "round", "(", "center_y", ")", "+", "round", "(", "height", ")", ")", ",", "\n", "int", "(", "round", "(", "center_x", ")", "-", "round", "(", "width", ")", ")", ":", "int", "(", "round", "(", "center_x", ")", "+", "round", "(", "width", ")", ")", "]", ")", "\n", "return", "cutted_img", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.convert_bgr2gray": [[89, 95], ["numpy.stack", "cv2.cvtColor"], "function", ["None"], ["", "def", "convert_bgr2gray", "(", "sequence", ")", ":", "\n", "    ", "\"\"\"convert_bgr2gray.\n\n    :param sequence: ndarray, the RGB image sequence.\n    \"\"\"", "\n", "return", "np", ".", "stack", "(", "[", "cv2", ".", "cvtColor", "(", "_", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", "for", "_", "in", "sequence", "]", ",", "axis", "=", "0", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Compose.__init__": [[25, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "preprocess", ")", ":", "\n", "        ", "self", ".", "preprocess", "=", "preprocess", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Compose.__call__": [[28, 32], ["t"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "preprocess", ":", "\n", "            ", "img", "=", "t", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Compose.__repr__": [[33, 40], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "for", "t", "in", "self", ".", "preprocess", ":", "\n", "            ", "format_string", "+=", "'\\n'", "\n", "format_string", "+=", "'    {0}'", ".", "format", "(", "t", ")", "\n", "", "format_string", "+=", "'\\n)'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Normalize.__init__": [[46, 49], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "        ", "self", ".", "mean", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Normalize.__call__": [[50, 58], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"", "\n", "return", "(", "img", "-", "self", ".", "mean", ")", "/", "self", ".", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Normalize.__repr__": [[59, 61], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(mean={0}, std={1})'", ".", "format", "(", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.NormalizeUtterance.__call__": [[66, 70], ["numpy.mean", "numpy.std", "numpy.std"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "signal", ")", ":", "\n", "        ", "signal_std", "=", "0.", "if", "np", ".", "std", "(", "signal", ")", "==", "0.", "else", "np", ".", "std", "(", "signal", ")", "\n", "signal_mean", "=", "np", ".", "mean", "(", "signal", ")", "\n", "return", "(", "signal", "-", "signal_mean", ")", "/", "signal_std", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.CenterCrop.__init__": [[77, 79], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "crop_size", ")", ":", "\n", "        ", "self", ".", "crop_size", "=", "crop_size", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.CenterCrop.__call__": [[80, 92], ["int", "int", "round", "round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (numpy.ndarray): Images to be cropped.\n        Returns:\n            numpy.ndarray: Cropped image.\n        \"\"\"", "\n", "frames", ",", "h", ",", "w", "=", "img", ".", "shape", "\n", "th", ",", "tw", "=", "self", ".", "crop_size", "\n", "delta_w", "=", "int", "(", "round", "(", "(", "w", "-", "tw", ")", ")", "/", "2.", ")", "\n", "delta_h", "=", "int", "(", "round", "(", "(", "h", "-", "th", ")", ")", "/", "2.", ")", "\n", "return", "img", "[", ":", ",", "delta_h", ":", "delta_h", "+", "th", ",", "delta_w", ":", "delta_w", "+", "tw", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.CenterCrop.__repr__": [[93, 95], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(size={0})'", ".", "format", "(", "self", ".", "crop_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.AddNoise.__init__": [[101, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "noise", ",", "snr_target", "=", "None", ",", "snr_levels", "=", "[", "-", "5", ",", "0", ",", "5", ",", "10", ",", "15", ",", "20", ",", "9999", "]", ")", ":", "\n", "        ", "assert", "noise", ".", "dtype", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ",", "\"noise only supports float data type\"", "\n", "self", ".", "noise", "=", "noise", "\n", "self", ".", "snr_levels", "=", "snr_levels", "\n", "self", ".", "snr_target", "=", "snr_target", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.AddNoise.get_power": [[107, 111], ["clip.copy", "numpy.sum", "len"], "methods", ["None"], ["", "def", "get_power", "(", "self", ",", "clip", ")", ":", "\n", "        ", "clip2", "=", "clip", ".", "copy", "(", ")", "\n", "clip2", "=", "clip2", "**", "2", "\n", "return", "np", ".", "sum", "(", "clip2", ")", "/", "(", "len", "(", "clip2", ")", "*", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.AddNoise.__call__": [[112, 127], ["random.choice", "random.randint", "transform.AddNoise.get_power", "transform.AddNoise.get_power", "len", "len", "len", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.AddNoise.get_power", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.AddNoise.get_power"], ["", "def", "__call__", "(", "self", ",", "signal", ")", ":", "\n", "        ", "assert", "signal", ".", "dtype", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ",", "\"signal only supports float32 data type\"", "\n", "snr_target", "=", "random", ".", "choice", "(", "self", ".", "snr_levels", ")", "if", "not", "self", ".", "snr_target", "else", "self", ".", "snr_target", "\n", "if", "snr_target", "==", "9999", ":", "\n", "            ", "return", "signal", "\n", "", "else", ":", "\n", "# -- get noise", "\n", "            ", "start_idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "noise", ")", "-", "len", "(", "signal", ")", ")", "\n", "noise_clip", "=", "self", ".", "noise", "[", "start_idx", ":", "start_idx", "+", "len", "(", "signal", ")", "]", "\n", "\n", "sig_power", "=", "self", ".", "get_power", "(", "signal", ")", "\n", "noise_clip_power", "=", "self", ".", "get_power", "(", "noise_clip", ")", "\n", "factor", "=", "(", "sig_power", "/", "noise_clip_power", ")", "/", "(", "10", "**", "(", "snr_target", "/", "10.0", ")", ")", "\n", "desired_signal", "=", "(", "signal", "+", "noise_clip", "*", "np", ".", "sqrt", "(", "factor", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "return", "desired_signal", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Identity.__init__": [[132, 134], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", ")", ":", "\n", "        ", "pass", "\n", "", "def", "__call__", "(", "self", ",", "array", ")", ":", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.Identity.__call__": [[134, 136], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "array", ")", ":", "\n", "        ", "return", "array", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.SpeedRate.__init__": [[142, 149], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "speed_rate", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param speed_rate: float, the speed rate between the frame rate of \\\n            the input video and the frame rate used for training.\n        \"\"\"", "\n", "self", ".", "_speed_rate", "=", "speed_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.SpeedRate.__call__": [[150, 168], ["int", "numpy.arange", "numpy.linspace", "list", "ValueError", "map"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (numpy.ndarray): sequence to be sampled.\n        Returns:\n            numpy.ndarray: sampled sequence.\n        \"\"\"", "\n", "if", "self", ".", "_speed_rate", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"speed_rate should be greater than zero.\"", ")", "\n", "", "if", "self", ".", "_speed_rate", "==", "1.", ":", "\n", "            ", "return", "x", "\n", "", "old_length", "=", "x", ".", "shape", "[", "0", "]", "\n", "new_length", "=", "int", "(", "old_length", "/", "self", ".", "_speed_rate", ")", "\n", "old_indices", "=", "np", ".", "arange", "(", "old_length", ")", "\n", "new_indices", "=", "np", ".", "linspace", "(", "start", "=", "0", ",", "stop", "=", "old_length", ",", "num", "=", "new_length", ",", "endpoint", "=", "False", ")", "\n", "new_indices", "=", "list", "(", "map", "(", "int", ",", "new_indices", ")", ")", "\n", "x", "=", "x", "[", "new_indices", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.ExpandDims.__init__": [[173, 175], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", ")", ":", "\n", "        ", "\"\"\"__init__.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.transform.ExpandDims.__call__": [[176, 182], ["numpy.expand_dims"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"__call__.\n\n        :param x: numpy.ndarray, Expand the shape of an array.\n        \"\"\"", "\n", "return", "np", ".", "expand_dims", "(", "x", ",", "axis", "=", "1", ")", "if", "x", ".", "ndim", "==", "1", "else", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.__init__": [[20, 68], ["numpy.load", "numpy.load", "os.path.join", "os.path.join", "dataloader.AVSRDataLoader.get_audio_transform", "os.path.dirname", "os.path.dirname", "dataloader.AVSRDataLoader.get_video_transform"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.get_audio_transform", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.get_video_transform"], ["def", "__init__", "(", "\n", "self", ",", "\n", "modality", ",", "\n", "disable_transform", "=", "False", ",", "\n", "speed_rate", "=", "1.0", ",", "\n", "mean_face_path", "=", "\"20words_mean_face.npy\"", ",", "\n", "noise_file_path", "=", "\"babbleNoise_resample_16K.npy\"", ",", "\n", "crop_width", "=", "96", ",", "\n", "crop_height", "=", "96", ",", "\n", "start_idx", "=", "48", ",", "\n", "stop_idx", "=", "68", ",", "\n", "window_margin", "=", "12", ",", "\n", "convert_gray", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param modality: str, the modality type of the input.\n        :param disable_transform: bool, disable transform if set as True.\n        :param speed_rate: float, the speed rate between the frame rate of \\\n            the input video and the frame rate used for training.\n        :param mean_face_path: str, the file path of the reference face.\n        :param noise_file_path: str, the file path of the noisy background.\n        :param crop_width: int, the width of the cropped patch.\n        :param crop_height: int, the height of the cropped patch.\n        :param start_idx: int, the starting index for cropping the bounding box.\n        :param stop_idx: int, the ending inex for cropping the bounding box.\n        :param window_margin: int, the window size for smoothing landmarks.\n        :param convert_gray: bool, save as grayscale if set as True.\n        \"\"\"", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "disable_transform", "=", "disable_transform", "\n", "self", ".", "_reference", "=", "np", ".", "load", "(", "\n", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "mean_face_path", ")", "\n", ")", "\n", "self", ".", "_noise", "=", "np", ".", "load", "(", "\n", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "noise_file_path", ")", "\n", ")", "\n", "self", ".", "_crop_width", "=", "crop_width", "\n", "self", ".", "_crop_height", "=", "crop_height", "\n", "self", ".", "_start_idx", "=", "start_idx", "\n", "self", ".", "_stop_idx", "=", "stop_idx", "\n", "self", ".", "_window_margin", "=", "window_margin", "\n", "self", ".", "_convert_gray", "=", "convert_gray", "\n", "\n", "if", "self", ".", "modality", "==", "\"audio\"", ":", "\n", "            ", "self", ".", "transform", "=", "self", ".", "get_audio_transform", "(", "self", ".", "_noise", ")", "\n", "", "elif", "self", ".", "modality", "==", "\"video\"", ":", "\n", "            ", "self", ".", "transform", "=", "self", ".", "get_video_transform", "(", "speed_rate", "=", "speed_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.get_audio_transform": [[70, 83], ["transform.Compose", "transform.AddNoise", "transform.NormalizeUtterance", "transform.ExpandDims"], "methods", ["None"], ["", "", "def", "get_audio_transform", "(", "self", ",", "noise_data", ")", ":", "\n", "        ", "\"\"\"get_audio_transform.\n\n        :param noise_data: numpy.ndarray, the noisy data to be injected to data.\n        \"\"\"", "\n", "\n", "return", "Compose", "(", "[", "\n", "AddNoise", "(", "\n", "noise", "=", "noise_data", ",", "\n", "snr_target", "=", "9999", ",", "\n", ")", ",", "\n", "NormalizeUtterance", "(", ")", ",", "\n", "ExpandDims", "(", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.get_video_transform": [[86, 101], ["transform.Compose", "transform.Normalize", "transform.CenterCrop", "transform.Normalize", "transform.SpeedRate", "transform.Identity"], "methods", ["None"], ["", "def", "get_video_transform", "(", "self", ",", "speed_rate", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"get_video_transform.\n\n        :param speed_rate: float, the speed rate between the frame rate of \\\n            the input video and the frame rate used for training.\n        \"\"\"", "\n", "crop_size", "=", "(", "88", ",", "88", ")", "\n", "(", "mean", ",", "std", ")", "=", "(", "0.421", ",", "0.165", ")", "\n", "return", "Compose", "(", "[", "\n", "Normalize", "(", "0.0", ",", "255.0", ")", ",", "\n", "CenterCrop", "(", "crop_size", ")", ",", "\n", "Normalize", "(", "mean", ",", "std", ")", ",", "\n", "SpeedRate", "(", "\n", "speed_rate", "=", "speed_rate", ",", "\n", ")", "if", "speed_rate", "!=", "1.0", "else", "Identity", "(", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.preprocess": [[104, 127], ["isinstance", "dataloader.AVSRDataLoader.landmarks_interpolate", "dataloader.AVSRDataLoader.crop_patch", "open", "pickle.load", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.landmarks_interpolate", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.crop_patch"], ["", "def", "preprocess", "(", "self", ",", "video_pathname", ",", "landmarks_pathname", ")", ":", "\n", "        ", "\"\"\"preprocess.\n\n        :param video_pathname: str, the filename for the video.\n        :param landmarks_pathname: str, the filename for the landmarks.\n        \"\"\"", "\n", "# -- Step 1, extract landmarks from pkl files.", "\n", "if", "isinstance", "(", "landmarks_pathname", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "landmarks_pathname", ",", "\"rb\"", ")", "as", "pkl_file", ":", "\n", "                ", "landmarks", "=", "pickle", ".", "load", "(", "pkl_file", ")", "\n", "", "", "else", ":", "\n", "            ", "landmarks", "=", "landmarks_pathname", "\n", "# -- Step 2, pre-process landmarks: interpolate frames that not being detected.", "\n", "", "preprocessed_landmarks", "=", "self", ".", "landmarks_interpolate", "(", "landmarks", ")", "\n", "# -- Step 3, exclude corner cases:", "\n", "#   -- 1) no landmark in all frames", "\n", "#   -- 2) number of frames is less than window length.", "\n", "if", "not", "preprocessed_landmarks", "or", "len", "(", "preprocessed_landmarks", ")", "<", "self", ".", "_window_margin", ":", "return", "\n", "# -- Step 4, affine transformation and crop patch ", "\n", "sequence", ",", "transformed_frame", ",", "transformed_landmarks", "=", "self", ".", "crop_patch", "(", "video_pathname", ",", "preprocessed_landmarks", ")", "\n", "assert", "sequence", "is", "not", "None", ",", "\"cannot crop from {}.\"", ".", "format", "(", "filename", ")", "\n", "return", "sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.landmarks_interpolate": [[129, 151], ["range", "len", "len", "len", "enumerate", "preprocess.linear_interpolate", "enumerate", "enumerate", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.linear_interpolate"], ["", "def", "landmarks_interpolate", "(", "self", ",", "landmarks", ")", ":", "\n", "        ", "\"\"\"landmarks_interpolate.\n\n        :param landmarks: List, the raw landmark (in-place)\n\n        \"\"\"", "\n", "valid_frames_idx", "=", "[", "idx", "for", "idx", ",", "_", "in", "enumerate", "(", "landmarks", ")", "if", "_", "is", "not", "None", "]", "\n", "if", "not", "valid_frames_idx", ":", "\n", "            ", "return", "None", "\n", "", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "valid_frames_idx", ")", ")", ":", "\n", "            ", "if", "valid_frames_idx", "[", "idx", "]", "-", "valid_frames_idx", "[", "idx", "-", "1", "]", "==", "1", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "landmarks", "=", "linear_interpolate", "(", "landmarks", ",", "valid_frames_idx", "[", "idx", "-", "1", "]", ",", "valid_frames_idx", "[", "idx", "]", ")", "\n", "", "", "valid_frames_idx", "=", "[", "idx", "for", "idx", ",", "_", "in", "enumerate", "(", "landmarks", ")", "if", "_", "is", "not", "None", "]", "\n", "# -- Corner case: keep frames at the beginning or at the end failed to be detected.", "\n", "if", "valid_frames_idx", ":", "\n", "            ", "landmarks", "[", ":", "valid_frames_idx", "[", "0", "]", "]", "=", "[", "landmarks", "[", "valid_frames_idx", "[", "0", "]", "]", "]", "*", "valid_frames_idx", "[", "0", "]", "\n", "landmarks", "[", "valid_frames_idx", "[", "-", "1", "]", ":", "]", "=", "[", "landmarks", "[", "valid_frames_idx", "[", "-", "1", "]", "]", "]", "*", "(", "len", "(", "landmarks", ")", "-", "valid_frames_idx", "[", "-", "1", "]", ")", "\n", "", "valid_frames_idx", "=", "[", "idx", "for", "idx", ",", "_", "in", "enumerate", "(", "landmarks", ")", "if", "_", "is", "not", "None", "]", "\n", "assert", "len", "(", "valid_frames_idx", ")", "==", "len", "(", "landmarks", ")", ",", "\"not every frame has landmark\"", "\n", "return", "landmarks", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.crop_patch": [[153, 191], ["utils.load_video", "min", "numpy.mean", "dataloader.AVSRDataLoader.affine_transform", "sequence.append", "sequence_frame.append", "sequence_landmarks.append", "numpy.array", "numpy.array", "numpy.array", "utils.load_video.__next__", "landmarks[].mean", "numpy.mean.mean", "preprocess.cut_patch", "len", "range"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_video", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.affine_transform", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.preprocess.cut_patch"], ["", "def", "crop_patch", "(", "self", ",", "video_pathname", ",", "landmarks", ")", ":", "\n", "        ", "\"\"\"crop_patch.\n\n        :param video_pathname: str, the filename for the processed video.\n        :param landmarks: List, the interpolated landmarks.\n        \"\"\"", "\n", "\n", "frame_idx", "=", "0", "\n", "frame_gen", "=", "load_video", "(", "video_pathname", ")", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "frame", "=", "frame_gen", ".", "__next__", "(", ")", "## -- BGR", "\n", "", "except", "StopIteration", ":", "\n", "                ", "break", "\n", "", "if", "frame_idx", "==", "0", ":", "\n", "                ", "sequence", "=", "[", "]", "\n", "sequence_frame", "=", "[", "]", "\n", "sequence_landmarks", "=", "[", "]", "\n", "\n", "", "window_margin", "=", "min", "(", "self", ".", "_window_margin", "//", "2", ",", "frame_idx", ",", "len", "(", "landmarks", ")", "-", "1", "-", "frame_idx", ")", "\n", "smoothed_landmarks", "=", "np", ".", "mean", "(", "[", "landmarks", "[", "x", "]", "for", "x", "in", "range", "(", "frame_idx", "-", "window_margin", ",", "frame_idx", "+", "window_margin", "+", "1", ")", "]", ",", "axis", "=", "0", ")", "\n", "smoothed_landmarks", "+=", "landmarks", "[", "frame_idx", "]", ".", "mean", "(", "axis", "=", "0", ")", "-", "smoothed_landmarks", ".", "mean", "(", "axis", "=", "0", ")", "\n", "transformed_frame", ",", "transformed_landmarks", "=", "self", ".", "affine_transform", "(", "\n", "frame", ",", "\n", "smoothed_landmarks", ",", "\n", "self", ".", "_reference", ",", "\n", "grayscale", "=", "self", ".", "_convert_gray", ",", "\n", ")", "\n", "sequence", ".", "append", "(", "cut_patch", "(", "transformed_frame", ",", "\n", "transformed_landmarks", "[", "self", ".", "_start_idx", ":", "self", ".", "_stop_idx", "]", ",", "\n", "self", ".", "_crop_height", "//", "2", ",", "\n", "self", ".", "_crop_width", "//", "2", ",", ")", ")", "\n", "\n", "sequence_frame", ".", "append", "(", "transformed_frame", ")", "\n", "sequence_landmarks", ".", "append", "(", "transformed_landmarks", ")", "\n", "frame_idx", "+=", "1", "\n", "\n", "", "return", "np", ".", "array", "(", "sequence", ")", ",", "np", ".", "array", "(", "sequence_frame", ")", ",", "np", ".", "array", "(", "sequence_landmarks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.affine_transform": [[193, 240], ["numpy.vstack", "cv2.warpAffine", "cv2.cvtColor", "cv2.estimateAffinePartial2D", "numpy.matmul", "transform[].transpose", "numpy.vstack", "transform[].transpose"], "methods", ["None"], ["", "def", "affine_transform", "(", "\n", "self", ",", "\n", "frame", ",", "\n", "landmarks", ",", "\n", "reference", ",", "\n", "grayscale", "=", "False", ",", "\n", "target_size", "=", "(", "256", ",", "256", ")", ",", "\n", "reference_size", "=", "(", "256", ",", "256", ")", ",", "\n", "stable_points", "=", "(", "28", ",", "33", ",", "36", ",", "39", ",", "42", ",", "45", ",", "48", ",", "54", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ",", "\n", "border_mode", "=", "cv2", ".", "BORDER_CONSTANT", ",", "\n", "border_value", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"affine_transform.\n\n        :param frame: numpy.array, the input sequence.\n        :param landmarks: List, the tracked landmarks.\n        :param reference: numpy.array, the neutral reference frame.\n        :param grayscale: bool, save as grayscale if set as True.\n        :param target_size: tuple, size of the output image.\n        :param reference_size: tuple, size of the neural reference frame.\n        :param stable_points: tuple, landmark idx for the stable points.\n        :param interpolation: interpolation method to be used.\n        :param border_mode: Pixel extrapolation method .\n        :param border_value: Value used in case of a constant border. By default, it is 0.\n        \"\"\"", "\n", "# Prepare everything", "\n", "if", "grayscale", "and", "frame", ".", "ndim", "==", "3", ":", "\n", "            ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", "\n", "", "stable_reference", "=", "np", ".", "vstack", "(", "[", "reference", "[", "x", "]", "for", "x", "in", "stable_points", "]", ")", "\n", "stable_reference", "[", ":", ",", "0", "]", "-=", "(", "reference_size", "[", "0", "]", "-", "target_size", "[", "0", "]", ")", "/", "2.0", "\n", "stable_reference", "[", ":", ",", "1", "]", "-=", "(", "reference_size", "[", "1", "]", "-", "target_size", "[", "1", "]", ")", "/", "2.0", "\n", "\n", "# Warp the face patch and the landmarks", "\n", "transform", "=", "cv2", ".", "estimateAffinePartial2D", "(", "np", ".", "vstack", "(", "[", "landmarks", "[", "x", "]", "for", "x", "in", "stable_points", "]", ")", ",", "\n", "stable_reference", ",", "method", "=", "cv2", ".", "LMEDS", ")", "[", "0", "]", "\n", "transformed_frame", "=", "cv2", ".", "warpAffine", "(", "\n", "frame", ",", "\n", "transform", ",", "\n", "dsize", "=", "(", "target_size", "[", "0", "]", ",", "target_size", "[", "1", "]", ")", ",", "\n", "flags", "=", "interpolation", ",", "\n", "borderMode", "=", "border_mode", ",", "\n", "borderValue", "=", "border_value", ",", "\n", ")", "\n", "transformed_landmarks", "=", "np", ".", "matmul", "(", "landmarks", ",", "transform", "[", ":", ",", ":", "2", "]", ".", "transpose", "(", ")", ")", "+", "transform", "[", ":", ",", "2", "]", ".", "transpose", "(", ")", "\n", "\n", "return", "transformed_frame", ",", "transformed_landmarks", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.load_audio": [[242, 249], ["utils.load_audio"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_audio"], ["", "def", "load_audio", "(", "self", ",", "data_filename", ")", ":", "\n", "        ", "\"\"\"load_audio.\n\n        :param data_filename: str, the filename of input sequence.\n        \"\"\"", "\n", "sequence", "=", "load_audio", "(", "data_filename", ",", "specified_sr", "=", "16000", ",", "int_16", "=", "False", ")", "\n", "return", "sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.load_video": [[251, 263], ["dataloader.AVSRDataLoader.preprocess"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.preprocess"], ["", "def", "load_video", "(", "self", ",", "data_filename", ",", "landmarks_filename", "=", "None", ")", ":", "\n", "        ", "\"\"\"load_video.\n\n        :param data_filename: str, the filename of input sequence.\n        :param landmarks_filename: str, the filename of landmarks.\n        \"\"\"", "\n", "assert", "landmarks_filename", "is", "not", "None", "\n", "sequence", "=", "self", ".", "preprocess", "(", "\n", "video_pathname", "=", "data_filename", ",", "\n", "landmarks_pathname", "=", "landmarks_filename", ",", "\n", ")", "\n", "return", "sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.load_data": [[265, 282], ["dataloader.AVSRDataLoader.load_audio", "dataloader.AVSRDataLoader.transform", "dataloader.AVSRDataLoader.load_video"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_audio", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_video"], ["", "def", "load_data", "(", "self", ",", "data_filename", ",", "landmarks_filename", "=", "None", ")", ":", "\n", "        ", "\"\"\"load_data.\n\n        :param data_filename: str, the filename of input sequence.\n        :param landmarks_filename: str, the filename of landmarks.\n        \"\"\"", "\n", "if", "self", ".", "modality", "==", "\"audio\"", ":", "\n", "            ", "raw_sequence", "=", "self", ".", "load_audio", "(", "data_filename", ")", "\n", "", "elif", "self", ".", "modality", "==", "\"video\"", ":", "\n", "            ", "raw_sequence", "=", "self", ".", "load_video", "(", "\n", "data_filename", ",", "\n", "landmarks_filename", "=", "landmarks_filename", ",", "\n", ")", "\n", "", "if", "self", ".", "disable_transform", ":", "\n", "            ", "return", "raw_sequence", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "transform", "(", "raw_sequence", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_video": [[12, 25], ["cv2.VideoCapture", "cv2.VideoCapture.isOpened", "cv2.VideoCapture.release", "cv2.VideoCapture.read"], "function", ["None"], ["def", "load_video", "(", "filename", ")", ":", "\n", "    ", "\"\"\"load_video.\n\n    :param filename: str, the fileanme for a video sequence.\n    \"\"\"", "\n", "cap", "=", "cv2", ".", "VideoCapture", "(", "filename", ")", "\n", "while", "(", "cap", ".", "isOpened", "(", ")", ")", ":", "\n", "        ", "ret", ",", "frame", "=", "cap", ".", "read", "(", ")", "# BGR                                            ", "\n", "if", "ret", ":", "\n", "            ", "yield", "frame", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "cap", ".", "release", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.load_audio": [[27, 51], ["audio_filename.endswith", "numpy.array", "numpy.load", "audio_filename.endswith", "sys.exit", "numpy.clip", "librosa.load", "numpy.round", "numpy.load", "librosa.resample"], "function", ["None"], ["", "def", "load_audio", "(", "audio_filename", ",", "specified_sr", "=", "16000", ",", "int_16", "=", "True", ")", ":", "\n", "    ", "\"\"\"load_audio.\n\n    :param audio_filename: str, the filename for an audio waveform.\n    :param specified_sr: int, expected sampling rate, the default value is 16KHz.\n    :param int_16: boolean, return 16-bit PCM if set it as True.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "if", "audio_filename", ".", "endswith", "(", "'npy'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "\n", "", "elif", "audio_filename", ".", "endswith", "(", "'npz'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "[", "'data'", "]", "\n", "", "else", ":", "\n", "            ", "import", "librosa", "\n", "audio", ",", "sr", "=", "librosa", ".", "load", "(", "audio_filename", ",", "sr", "=", "None", ")", "\n", "audio", "=", "librosa", ".", "resample", "(", "audio", ",", "sr", ",", "specified_sr", ")", "if", "sr", "!=", "specified_sr", "else", "audio", "\n", "", "", "except", "IOError", ":", "\n", "        ", "sys", ".", "exit", "(", ")", "\n", "", "if", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "float32", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "1.", ")", "*", "(", "65535.", "/", "2.", ")", "+", "32767.", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "audio", "=", "np", ".", "array", "(", "np", ".", "clip", "(", "np", ".", "round", "(", "audio", ")", ",", "-", "2", "**", "15", ",", "2", "**", "15", "-", "1", ")", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "", "if", "not", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "int16", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "32767.", ")", "*", "2", "/", "65535.", "+", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "audio", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.save2npz": [[53, 63], ["numpy.savez_compressed", "os.path.exists", "os.makedirs", "os.path.dirname", "os.path.dirname"], "function", ["None"], ["", "def", "save2npz", "(", "filename", ",", "data", "=", "None", ")", ":", "\n", "    ", "\"\"\"save2npz.\n\n    :param filename: str, the fileanme where the data will be saved.\n    :param data: ndarray, arrays to save to the file.\n    \"\"\"", "\n", "assert", "data", "is", "not", "None", ",", "\"data is {}\"", ".", "format", "(", "data", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ")", "\n", "", "np", ".", "savez_compressed", "(", "filename", ",", "data", "=", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.get_video_properties": [[66, 79], ["cv2.VideoCapture", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "cv2.VideoCapture.get", "cv2.VideoCapture.release"], "function", ["None"], ["", "def", "get_video_properties", "(", "filename", ")", ":", "\n", "    ", "\"\"\"get_video_properties.\n\n    :param filename: str, the fileanme for a video sequence.\n    \"\"\"", "\n", "vid_properties", "=", "{", "}", "\n", "vid", "=", "cv2", ".", "VideoCapture", "(", "filename", ")", "\n", "vid_properties", "[", "\"width\"", "]", "=", "vid", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_WIDTH", ")", "\n", "vid_properties", "[", "\"height\"", "]", "=", "vid", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_HEIGHT", ")", "\n", "vid_properties", "[", "\"fps\"", "]", "=", "vid", ".", "get", "(", "cv2", ".", "CAP_PROP_FPS", ")", "\n", "vid_properties", "[", "\"frames\"", "]", "=", "vid", ".", "get", "(", "cv2", ".", "CAP_PROP_FRAME_COUNT", ")", "\n", "vid", ".", "release", "(", ")", "\n", "return", "vid_properties", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.face_tracker.FaceTracker.__init__": [[16, 31], ["ibug.face_detection.RetinaFacePredictor", "ibug.face_alignment.FANPredictor", "ibug.face_detection.RetinaFacePredictor.get_model"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "device", "=", "\"cuda:0\"", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param device: str, contain the device on which a torch.Tensor is or will be allocated.\n        \"\"\"", "\n", "\n", "# Create a RetinaFace detector using Resnet50 backbone.", "\n", "self", ".", "face_detector", "=", "RetinaFacePredictor", "(", "\n", "device", "=", "device", ",", "\n", "threshold", "=", "0.8", ",", "\n", "model", "=", "RetinaFacePredictor", ".", "get_model", "(", "'resnet50'", ")", "\n", ")", "\n", "# Create FAN for alignmentm, default model is '2dfan2'", "\n", "alignment_weights", "=", "None", "\n", "self", ".", "landmark_detector", "=", "FANPredictor", "(", "device", "=", "device", ",", "model", "=", "alignment_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.face_tracker.FaceTracker.tracker": [[32, 55], ["collections.defaultdict", "utils.extract_opencv_generator", "utils.get_landmarks", "face_tracker.FaceTracker.face_detector", "face_tracker.FaceTracker.landmark_detector", "face_info[].append", "face_info[].append", "face_info[].append", "utils.extract_opencv_generator.__next__"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.utils.extract_opencv_generator", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.utils.get_landmarks"], ["", "def", "tracker", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"tracker.\n\n        :param filename: str, the filename for the video\n        \"\"\"", "\n", "\n", "face_info", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "frame_gen", "=", "extract_opencv_generator", "(", "filename", ")", "\n", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "frame", "=", "frame_gen", ".", "__next__", "(", ")", "\n", "", "except", "StopIteration", ":", "\n", "                ", "break", "\n", "# -- face detection", "\n", "", "detected_faces", "=", "self", ".", "face_detector", "(", "frame", ",", "rgb", "=", "False", ")", "\n", "# -- face alignment", "\n", "landmarks", ",", "scores", "=", "self", ".", "landmark_detector", "(", "frame", ",", "detected_faces", ",", "rgb", "=", "False", ")", "\n", "face_info", "[", "'bbox'", "]", ".", "append", "(", "detected_faces", ")", "\n", "face_info", "[", "'landmarks'", "]", ".", "append", "(", "landmarks", ")", "\n", "face_info", "[", "'landmarks_scores'", "]", ".", "append", "(", "scores", ")", "\n", "\n", "", "return", "get_landmarks", "(", "face_info", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.utils.extract_opencv_generator": [[9, 23], ["cv2.VideoCapture", "cv2.VideoCapture.isOpened", "cv2.VideoCapture.release", "cv2.VideoCapture.read"], "function", ["None"], ["import", "numpy", "as", "np", "\n", "\n", "\n", "def", "load_video", "(", "filename", ")", ":", "\n", "    ", "\"\"\"load_video.\n\n    :param filename: str, the fileanme for a video sequence.\n    \"\"\"", "\n", "cap", "=", "cv2", ".", "VideoCapture", "(", "filename", ")", "\n", "while", "(", "cap", ".", "isOpened", "(", ")", ")", ":", "\n", "        ", "ret", ",", "frame", "=", "cap", ".", "read", "(", ")", "# BGR                                            ", "\n", "if", "ret", ":", "\n", "            ", "yield", "frame", "\n", "", "else", ":", "\n", "            ", "break", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.utils.get_landmarks": [[25, 55], ["range", "len", "len", "len", "range", "len", "landmark_scores[].min"], "function", ["None"], ["\n", "\n", "", "def", "load_audio", "(", "audio_filename", ",", "specified_sr", "=", "16000", ",", "int_16", "=", "True", ")", ":", "\n", "    ", "\"\"\"load_audio.\n\n    :param audio_filename: str, the filename for an audio waveform.\n    :param specified_sr: int, expected sampling rate, the default value is 16KHz.\n    :param int_16: boolean, return 16-bit PCM if set it as True.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "if", "audio_filename", ".", "endswith", "(", "'npy'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "\n", "", "elif", "audio_filename", ".", "endswith", "(", "'npz'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "[", "'data'", "]", "\n", "", "else", ":", "\n", "            ", "import", "librosa", "\n", "audio", ",", "sr", "=", "librosa", ".", "load", "(", "audio_filename", ",", "sr", "=", "None", ")", "\n", "audio", "=", "librosa", ".", "resample", "(", "audio", ",", "sr", ",", "specified_sr", ")", "if", "sr", "!=", "specified_sr", "else", "audio", "\n", "", "", "except", "IOError", ":", "\n", "        ", "sys", ".", "exit", "(", ")", "\n", "", "if", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "float32", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "1.", ")", "*", "(", "65535.", "/", "2.", ")", "+", "32767.", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "audio", "=", "np", ".", "array", "(", "np", ".", "clip", "(", "np", ".", "round", "(", "audio", ")", ",", "-", "2", "**", "15", ",", "2", "**", "15", "-", "1", ")", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "", "if", "not", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "int16", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "32767.", ")", "*", "2", "/", "65535.", "+", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "audio", "\n", "\n", "\n", "", "def", "save2npz", "(", "filename", ",", "data", "=", "None", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.subroutines.LipreadingPipeline.__init__": [[18, 64], ["dataloader.dataloader.AVSRDataLoader", "os.path.isfile", "configparser.ConfigParser", "configparser.ConfigParser.read", "configparser.ConfigParser.getfloat", "configparser.ConfigParser.getfloat", "configparser.ConfigParser.get", "dataloader.dataloader.AVSRDataLoader", "lipreading.model.Lipreading", "FaceTracker"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "config_filename", ",", "\n", "feats_position", "=", "None", ",", "\n", "face_track", "=", "False", ",", "\n", "device", "=", "\"cpu\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param config_filename: str, the filename of the configuration.\n        :param feats_position: str, the layer position for feature extraction.\n        :param face_track: str, face tracker will be used if set it as True.\n        :param device: str, contain the device on which a torch.Tensor is or will be allocated.\n        \"\"\"", "\n", "if", "feats_position", "==", "\"mouth\"", ":", "\n", "            ", "self", ".", "modality", "=", "\"video\"", "\n", "self", ".", "dataloader", "=", "AVSRDataLoader", "(", "\n", "modality", "=", "self", ".", "modality", ",", "\n", "disable_transform", "=", "True", ",", "\n", ")", "\n", "self", ".", "model", "=", "None", "\n", "", "else", ":", "\n", "            ", "assert", "os", ".", "path", ".", "isfile", "(", "config_filename", ")", ",", "f\"config_filename: {config_filename} does not exist.\"", "\n", "config", "=", "ConfigParser", "(", ")", "\n", "config", ".", "read", "(", "config_filename", ")", "\n", "self", ".", "input_v_fps", "=", "config", ".", "getfloat", "(", "\"input\"", ",", "\"v_fps\"", ")", "\n", "self", ".", "model_v_fps", "=", "config", ".", "getfloat", "(", "\"model\"", ",", "\"v_fps\"", ")", "\n", "self", ".", "modality", "=", "config", ".", "get", "(", "\"input\"", ",", "\"modality\"", ")", "\n", "\n", "self", ".", "dataloader", "=", "AVSRDataLoader", "(", "\n", "modality", "=", "self", ".", "modality", ",", "\n", "speed_rate", "=", "self", ".", "input_v_fps", "/", "self", ".", "model_v_fps", ",", "\n", ")", "\n", "self", ".", "model", "=", "Lipreading", "(", "\n", "config", ",", "\n", "feats_position", "=", "feats_position", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "\n", "", "if", "face_track", "and", "self", ".", "modality", "==", "\"video\"", ":", "\n", "            ", "from", "tracker", ".", "face_tracker", "import", "FaceTracker", "\n", "self", ".", "face_tracker", "=", "FaceTracker", "(", "device", "=", "\"cuda:0\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "face_tracker", "=", "None", "\n", "\n", "", "self", ".", "feats_position", "=", "feats_position", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.subroutines.LipreadingPipeline.__call__": [[66, 108], ["os.path.isfile", "subroutines.LipreadingPipeline.dataloader.load_data", "os.path.isfile", "subroutines.LipreadingPipeline.model.predict", "time.time", "subroutines.LipreadingPipeline.face_tracker.tracker", "print", "subroutines.LipreadingPipeline.model.extract_feats", "dataloader.utils.get_video_properties", "len", "time.time"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.dataloader.AVSRDataLoader.load_data", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.tracker.face_tracker.FaceTracker.tracker", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.extract_feats", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.dataloader.utils.get_video_properties"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "data_filename", ",", "\n", "landmarks_filename", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__call__.\n\n        :param data_filename: str, the filename of the input sequence.\n        :param landmarks_filename: str, the filename of the corresponding landmarks.\n        \"\"\"", "\n", "# Step 1, track face in the input video or read landmarks from the file.", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "data_filename", ")", ",", "f\"data_filename: {data_filename} does not exist.\"", "\n", "\n", "if", "self", ".", "modality", "==", "\"audio\"", ":", "\n", "            ", "landmarks", "=", "None", "\n", "", "else", ":", "\n", "            ", "if", "os", ".", "path", ".", "isfile", "(", "landmarks_filename", ")", ":", "\n", "                ", "landmarks", "=", "landmarks_filename", "\n", "", "else", ":", "\n", "                ", "assert", "self", ".", "face_tracker", "is", "not", "None", ",", "\"face tracker is not enabled.\"", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "landmarks", "=", "self", ".", "face_tracker", ".", "tracker", "(", "data_filename", ")", "\n", "print", "(", "f\"face tracking speed: {len(landmarks)/(time.time()-end):.2f} fps.\"", ")", "\n", "\n", "# Step 2, extract mouth patches from segments.", "\n", "", "", "sequence", "=", "self", ".", "dataloader", ".", "load_data", "(", "\n", "data_filename", ",", "\n", "landmarks", ",", "\n", ")", "\n", "\n", "# Step 3, perform inference or extract mouth ROIs or speech representations.", "\n", "if", "self", ".", "feats_position", ":", "\n", "            ", "if", "self", ".", "feats_position", "==", "\"mouth\"", ":", "\n", "                ", "assert", "self", ".", "modality", "==", "\"video\"", ",", "\"input modality should be `video`.\"", "\n", "vid_fps", "=", "get_video_properties", "(", "data_filename", ")", "[", "\"fps\"", "]", "\n", "output", "=", "(", "sequence", ",", "vid_fps", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "self", ".", "model", ".", "extract_feats", "(", "sequence", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "model", ".", "predict", "(", "sequence", ")", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.__init__": [[25, 42], ["super().__init__", "model.Lipreading.load_model", "model.Lipreading.get_beam_search", "model.Lipreading.model.to().eval", "model.Lipreading.beam_search.to().eval", "model.Lipreading.model.to", "model.Lipreading.beam_search.to"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.load_model", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.get_beam_search"], ["def", "__init__", "(", "self", ",", "config", ",", "feats_position", "=", "\"resnet\"", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param config: ConfigParser class, contains model's configuration.\n        :param feats_position: str, the position to extract features.\n        :param device: str, contain the device on which a torch.Tensor is or will be allocated.\n        \"\"\"", "\n", "super", "(", "Lipreading", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "feats_position", "=", "feats_position", "\n", "\n", "self", ".", "load_model", "(", "config", ")", "\n", "self", ".", "get_beam_search", "(", "config", ")", "\n", "\n", "self", ".", "model", ".", "to", "(", "device", "=", "self", ".", "device", ")", ".", "eval", "(", ")", "\n", "self", ".", "beam_search", ".", "to", "(", "device", "=", "self", ".", "device", ")", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.load_model": [[44, 69], ["config.get", "config.get", "os.path.isfile", "os.path.isfile", "isinstance", "argparse.Namespace", "espnet.nets.pytorch_backend.e2e_asr_transformer.E2E", "model.Lipreading.model.load_state_dict", "print", "open", "json.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict"], ["", "def", "load_model", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"load_model.\n\n        :param config: ConfigParser class, the configuration parser.\n        \"\"\"", "\n", "model_path", "=", "config", ".", "get", "(", "\"model\"", ",", "\"model_path\"", ")", "\n", "model_conf", "=", "config", ".", "get", "(", "\"model\"", ",", "\"model_conf\"", ")", "\n", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "model_path", ")", ",", "f\"model_path: {model_path} does not exist.\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "model_conf", ")", ",", "f\"model_conf: {model_conf} does not exist.\"", "\n", "\n", "with", "open", "(", "model_conf", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "confs", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "isinstance", "(", "confs", ",", "dict", ")", ":", "\n", "            ", "args", "=", "confs", "\n", "", "else", ":", "\n", "            ", "idim", ",", "odim", ",", "args", "=", "confs", "\n", "self", ".", "odim", "=", "odim", "\n", "", "self", ".", "train_args", "=", "argparse", ".", "Namespace", "(", "**", "args", ")", "\n", "self", ".", "char_list", "=", "self", ".", "train_args", ".", "char_list", "\n", "self", ".", "model", "=", "E2E", "(", "odim", ",", "self", ".", "train_args", ")", "\n", "\n", "# -- load a pre-trained model", "\n", "self", ".", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "print", "(", "f\"load a pre-trained model from: {model_path}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.get_beam_search": [[71, 130], ["config.get", "config.get", "config.getfloat", "config.getfloat", "config.getfloat", "config.getfloat", "config.getfloat", "config.getint", "model.Lipreading.model.scorers", "espnet.nets.scorers.length_bonus.LengthBonus", "dict", "espnet.nets.batch_beam_search.BatchBeamSearch", "espnet.asr.asr_utils.get_model_conf", "getattr", "espnet.nets.lm_interface.dynamic_import_lm", "espnet.nets.lm_interface.dynamic_import_lm.", "espnet.asr.asr_utils.torch_load", "espnet.nets.lm_interface.dynamic_import_lm.eval", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.scorers", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.get_model_conf", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.lm_interface.dynamic_import_lm", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_load"], ["", "def", "get_beam_search", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"get_beam_search.\n\n        :param config: ConfigParser Objects, the main configuration parser.\n        \"\"\"", "\n", "\n", "rnnlm", "=", "config", ".", "get", "(", "\"model\"", ",", "\"rnnlm\"", ")", "\n", "rnnlm_conf", "=", "config", ".", "get", "(", "\"model\"", ",", "\"rnnlm_conf\"", ")", "\n", "\n", "penalty", "=", "config", ".", "getfloat", "(", "\"decode\"", ",", "\"penalty\"", ")", "\n", "maxlenratio", "=", "config", ".", "getfloat", "(", "\"decode\"", ",", "\"maxlenratio\"", ")", "\n", "minlenratio", "=", "config", ".", "getfloat", "(", "\"decode\"", ",", "\"minlenratio\"", ")", "\n", "ctc_weight", "=", "config", ".", "getfloat", "(", "\"decode\"", ",", "\"ctc_weight\"", ")", "\n", "lm_weight", "=", "config", ".", "getfloat", "(", "\"decode\"", ",", "\"lm_weight\"", ")", "\n", "beam_size", "=", "config", ".", "getint", "(", "\"decode\"", ",", "\"beam_size\"", ")", "\n", "\n", "sos", "=", "self", ".", "odim", "-", "1", "\n", "eos", "=", "self", ".", "odim", "-", "1", "\n", "scorers", "=", "self", ".", "model", ".", "scorers", "(", ")", "\n", "\n", "if", "not", "rnnlm", ":", "\n", "            ", "lm", "=", "None", "\n", "", "else", ":", "\n", "            ", "lm_args", "=", "get_model_conf", "(", "rnnlm", ",", "rnnlm_conf", ")", "\n", "lm_model_module", "=", "getattr", "(", "lm_args", ",", "\"model_module\"", ",", "\"default\"", ")", "\n", "lm_class", "=", "dynamic_import_lm", "(", "lm_model_module", ",", "lm_args", ".", "backend", ")", "\n", "lm", "=", "lm_class", "(", "len", "(", "self", ".", "train_args", ".", "char_list", ")", ",", "lm_args", ")", "\n", "torch_load", "(", "rnnlm", ",", "lm", ")", "\n", "lm", ".", "eval", "(", ")", "\n", "\n", "", "scorers", "[", "\"lm\"", "]", "=", "lm", "\n", "scorers", "[", "\"length_bonus\"", "]", "=", "LengthBonus", "(", "len", "(", "self", ".", "train_args", ".", "char_list", ")", ")", "\n", "weights", "=", "dict", "(", "\n", "decoder", "=", "1.0", "-", "ctc_weight", ",", "\n", "ctc", "=", "ctc_weight", ",", "\n", "lm", "=", "lm_weight", ",", "\n", "length_bonus", "=", "penalty", ",", "\n", ")", "\n", "\n", "# -- decoding config", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "nbest", "=", "1", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "scorers", "=", "scorers", "\n", "self", ".", "sos", "=", "sos", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "ctc_weight", "=", "ctc_weight", "\n", "self", ".", "maxlenratio", "=", "maxlenratio", "\n", "self", ".", "minlenratio", "=", "minlenratio", "\n", "\n", "self", ".", "beam_search", "=", "BatchBeamSearch", "(", "\n", "beam_size", "=", "self", ".", "beam_size", ",", "\n", "vocab_size", "=", "len", "(", "self", ".", "train_args", ".", "char_list", ")", ",", "\n", "weights", "=", "self", ".", "weights", ",", "\n", "scorers", "=", "self", ".", "scorers", ",", "\n", "sos", "=", "self", ".", "sos", ",", "\n", "eos", "=", "self", ".", "eos", ",", "\n", "token_list", "=", "self", ".", "train_args", ".", "char_list", ",", "\n", "pre_beam_score_key", "=", "None", "if", "self", ".", "ctc_weight", "==", "1.0", "else", "\"decoder\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.predict": [[133, 153], ["espnet.asr.asr_utils.add_results_to_json.replace", "torch.no_grad", "torch.FloatTensor().to", "model.Lipreading.model.encode", "model.Lipreading.beam_search", "espnet.asr.asr_utils.add_results_to_json", "torch.as_tensor().to", "h.asdict", "torch.FloatTensor", "torch.as_tensor", "min", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.encode", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.beam_search", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.add_results_to_json", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.Hypothesis.asdict"], ["", "def", "predict", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"predict.\n\n        :param sequence: ndarray, the raw sequence saved in a format of numpy array.\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "sequence", "=", "(", "torch", ".", "FloatTensor", "(", "sequence", ")", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "enc_feats", "=", "self", ".", "model", ".", "encode", "(", "torch", ".", "as_tensor", "(", "sequence", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", ")", "\n", "nbest_hyps", "=", "self", ".", "beam_search", "(", "\n", "x", "=", "enc_feats", ",", "\n", "maxlenratio", "=", "self", ".", "maxlenratio", ",", "\n", "minlenratio", "=", "self", ".", "minlenratio", "\n", ")", "\n", "nbest_hyps", "=", "[", "\n", "h", ".", "asdict", "(", ")", "for", "h", "in", "nbest_hyps", "[", ":", "min", "(", "len", "(", "nbest_hyps", ")", ",", "self", ".", "nbest", ")", "]", "\n", "]", "\n", "\n", "transcription", "=", "add_results_to_json", "(", "nbest_hyps", ",", "self", ".", "char_list", ")", "\n", "\n", "", "return", "transcription", ".", "replace", "(", "\"<eos>\"", ",", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.model.Lipreading.extract_feats": [[155, 169], ["torch.FloatTensor().to", "model.Lipreading.model.encode", "torch.FloatTensor", "torch.as_tensor().to", "model.Lipreading.model.encode", "torch.as_tensor().to", "torch.as_tensor", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.encode", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.encode"], ["", "def", "extract_feats", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "\"\"\"extract_feats.\n\n        :param sequence: ndarray, the raw sequence saved in a format of numpy array.\n        \"\"\"", "\n", "sequence", "=", "(", "torch", ".", "FloatTensor", "(", "sequence", ")", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "if", "self", ".", "feats_position", "==", "\"resnet\"", ":", "\n", "            ", "feats", "=", "self", ".", "model", ".", "encode", "(", "\n", "torch", ".", "as_tensor", "(", "sequence", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", ",", "\n", "extract_resnet_feats", "=", "True", ",", "\n", ")", "\n", "", "elif", "self", ".", "feats_position", "==", "\"conformer\"", ":", "\n", "            ", "feats", "=", "self", ".", "model", ".", "encode", "(", "torch", ".", "as_tensor", "(", "sequence", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", ")", "\n", "", "return", "feats", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.__init__": [[14, 16], ["utils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.reset"], ["\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.reset": [[17, 22], ["None"], "methods", ["None"], ["cap", "=", "cv2", ".", "VideoCapture", "(", "filename", ")", "\n", "while", "(", "cap", ".", "isOpened", "(", ")", ")", ":", "\n", "        ", "ret", ",", "frame", "=", "cap", ".", "read", "(", ")", "# BGR                                            ", "\n", "if", "ret", ":", "\n", "            ", "yield", "frame", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.AverageMeter.update": [[23, 28], ["None"], "methods", ["None"], ["            ", "break", "\n", "", "", "cap", ".", "release", "(", ")", "\n", "\n", "\n", "", "def", "load_audio", "(", "audio_filename", ",", "specified_sr", "=", "16000", ",", "int_16", "=", "True", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2npz": [[30, 39], ["os.makedirs", "numpy.savez_compressed", "os.path.dirname"], "function", ["None"], ["\n", "try", ":", "\n", "        ", "if", "audio_filename", ".", "endswith", "(", "'npy'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "\n", "", "elif", "audio_filename", ".", "endswith", "(", "'npz'", ")", ":", "\n", "            ", "audio", "=", "np", ".", "load", "(", "audio_filename", ")", "[", "'data'", "]", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lipreading.utils.save2avi": [[41, 55], ["os.makedirs", "cv2.VideoWriter_fourcc", "cv2.VideoWriter", "cv2.VideoWriter.release", "os.path.dirname", "cv2.VideoWriter.write"], "function", ["None"], ["audio", ",", "sr", "=", "librosa", ".", "load", "(", "audio_filename", ",", "sr", "=", "None", ")", "\n", "audio", "=", "librosa", ".", "resample", "(", "audio", ",", "sr", ",", "specified_sr", ")", "if", "sr", "!=", "specified_sr", "else", "audio", "\n", "", "", "except", "IOError", ":", "\n", "        ", "sys", ".", "exit", "(", ")", "\n", "", "if", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "float32", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "1.", ")", "*", "(", "65535.", "/", "2.", ")", "+", "32767.", ")", ".", "astype", "(", "np", ".", "int16", ")", "\n", "audio", "=", "np", ".", "array", "(", "np", ".", "clip", "(", "np", ".", "round", "(", "audio", ")", ",", "-", "2", "**", "15", ",", "2", "**", "15", "-", "1", ")", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "", "if", "not", "int_16", "and", "audio", ".", "dtype", "==", "np", ".", "int16", ":", "\n", "        ", "audio", "=", "(", "(", "audio", "-", "32767.", ")", "*", "2", "/", "65535.", "+", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "audio", "\n", "\n", "\n", "", "def", "save2npz", "(", "filename", ",", "data", "=", "None", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_wer": [[8, 10], ["measures.get_er", "s.split", "ref.split"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_er"], ["def", "get_wer", "(", "s", ",", "ref", ")", ":", "\n", "    ", "return", "get_er", "(", "s", ".", "split", "(", "\" \"", ")", ",", "ref", ".", "split", "(", "\" \"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_cer": [[11, 13], ["measures.get_er", "list", "list"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_er"], ["", "def", "get_cer", "(", "s", ",", "ref", ")", ":", "\n", "    ", "return", "get_er", "(", "list", "(", "s", ")", ",", "list", "(", "ref", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.metrics.measures.get_er": [[14, 41], ["numpy.zeros", "range", "range", "range", "range", "len", "len", "len", "len", "min", "len", "len", "len"], "function", ["None"], ["", "def", "get_er", "(", "s", ",", "ref", ")", ":", "\n", "    ", "\"\"\"\n        FROM wikipedia levenshtein distance\n        s: list of words/char in sentence to measure\n        ref: list of words/char in reference\n    \"\"\"", "\n", "\n", "costs", "=", "np", ".", "zeros", "(", "(", "len", "(", "s", ")", "+", "1", ",", "len", "(", "ref", ")", "+", "1", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "s", ")", "+", "1", ")", ":", "\n", "        ", "costs", "[", "i", ",", "0", "]", "=", "i", "\n", "", "for", "j", "in", "range", "(", "len", "(", "ref", ")", "+", "1", ")", ":", "\n", "        ", "costs", "[", "0", ",", "j", "]", "=", "j", "\n", "\n", "", "for", "j", "in", "range", "(", "1", ",", "len", "(", "ref", ")", "+", "1", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "1", ",", "len", "(", "s", ")", "+", "1", ")", ":", "\n", "            ", "cost", "=", "None", "\n", "if", "s", "[", "i", "-", "1", "]", "==", "ref", "[", "j", "-", "1", "]", ":", "\n", "                ", "cost", "=", "0", "\n", "", "else", ":", "\n", "                ", "cost", "=", "1", "\n", "", "costs", "[", "i", ",", "j", "]", "=", "min", "(", "\n", "costs", "[", "i", "-", "1", ",", "j", "]", "+", "1", ",", "\n", "costs", "[", "i", ",", "j", "-", "1", "]", "+", "1", ",", "\n", "costs", "[", "i", "-", "1", ",", "j", "-", "1", "]", "+", "cost", "\n", ")", "\n", "\n", "", "", "return", "costs", "[", "-", "1", ",", "-", "1", "]", "/", "len", "(", "ref", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.fill_missing_args.fill_missing_args": [[10, 47], ["callable", "add_arguments().parse_known_args", "vars", "vars.items", "argparse.Namespace", "isinstance", "vars", "add_arguments", "logging.info", "argparse.ArgumentParser", "str"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.add_arguments"], ["def", "fill_missing_args", "(", "args", ",", "add_arguments", ")", ":", "\n", "    ", "\"\"\"Fill missing arguments in args.\n\n    Args:\n        args (Namespace or None): Namesapce containing hyperparameters.\n        add_arguments (function): Function to add arguments.\n\n    Returns:\n        Namespace: Arguments whose missing ones are filled with default value.\n\n    Examples:\n        >>> from argparse import Namespace\n        >>> from espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2\n        >>> args = Namespace()\n        >>> fill_missing_args(args, Tacotron2.add_arguments_fn)\n        Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype='location', ...)\n\n    \"\"\"", "\n", "# check argument type", "\n", "assert", "isinstance", "(", "args", ",", "argparse", ".", "Namespace", ")", "or", "args", "is", "None", "\n", "assert", "callable", "(", "add_arguments", ")", "\n", "\n", "# get default arguments", "\n", "default_args", ",", "_", "=", "add_arguments", "(", "argparse", ".", "ArgumentParser", "(", ")", ")", ".", "parse_known_args", "(", ")", "\n", "\n", "# convert to dict", "\n", "args", "=", "{", "}", "if", "args", "is", "None", "else", "vars", "(", "args", ")", "\n", "default_args", "=", "vars", "(", "default_args", ")", "\n", "\n", "for", "key", ",", "value", "in", "default_args", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", "not", "in", "args", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "'attribute \"%s\" does not exist. use default %s.'", "%", "(", "key", ",", "str", "(", "value", ")", ")", "\n", ")", "\n", "args", "[", "key", "]", "=", "value", "\n", "\n", "", "", "return", "argparse", ".", "Namespace", "(", "**", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.strtobool": [[8, 11], ["bool", "distutils.util.strtobool"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.strtobool"], ["def", "strtobool", "(", "x", ")", ":", "\n", "# distutils.util.strtobool returns integer, but it's confusing,", "\n", "    ", "return", "bool", "(", "dist_strtobool", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.get_commandline_args": [[13, 46], ["all", "arg.replace", "arg.replace"], "function", ["None"], ["", "def", "get_commandline_args", "(", ")", ":", "\n", "    ", "extra_chars", "=", "[", "\n", "\" \"", ",", "\n", "\";\"", ",", "\n", "\"&\"", ",", "\n", "\"(\"", ",", "\n", "\")\"", ",", "\n", "\"|\"", ",", "\n", "\"^\"", ",", "\n", "\"<\"", ",", "\n", "\">\"", ",", "\n", "\"?\"", ",", "\n", "\"*\"", ",", "\n", "\"[\"", ",", "\n", "\"]\"", ",", "\n", "\"$\"", ",", "\n", "\"`\"", ",", "\n", "'\"'", ",", "\n", "\"\\\\\"", ",", "\n", "\"!\"", ",", "\n", "\"{\"", ",", "\n", "\"}\"", ",", "\n", "]", "\n", "\n", "# Escape the extra characters for shell", "\n", "argv", "=", "[", "\n", "arg", ".", "replace", "(", "\"'\"", ",", "\"'\\\\''\"", ")", "\n", "if", "all", "(", "char", "not", "in", "arg", "for", "char", "in", "extra_chars", ")", "\n", "else", "\"'\"", "+", "arg", ".", "replace", "(", "\"'\"", ",", "\"'\\\\''\"", ")", "+", "\"'\"", "\n", "for", "arg", "in", "sys", ".", "argv", "\n", "]", "\n", "\n", "return", "sys", ".", "executable", "+", "\" \"", "+", "\" \"", ".", "join", "(", "argv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.is_scipy_wav_style": [[48, 55], ["isinstance", "isinstance", "isinstance", "len"], "function", ["None"], ["", "def", "is_scipy_wav_style", "(", "value", ")", ":", "\n", "# If Tuple[int, numpy.ndarray] or not", "\n", "    ", "return", "(", "\n", "isinstance", "(", "value", ",", "Sequence", ")", "\n", "and", "len", "(", "value", ")", "==", "2", "\n", "and", "isinstance", "(", "value", "[", "0", "]", ",", "int", ")", "\n", "and", "isinstance", "(", "value", "[", "1", "]", ",", "numpy", ".", "ndarray", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.assert_scipy_wav_style": [[58, 65], ["cli_utils.is_scipy_wav_style", "type", "isinstance", "type", "str", "type"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.cli_utils.is_scipy_wav_style"], ["", "def", "assert_scipy_wav_style", "(", "value", ")", ":", "\n", "    ", "assert", "is_scipy_wav_style", "(", "\n", "value", "\n", ")", ",", "\"Must be Tuple[int, numpy.ndarray], but got {}\"", ".", "format", "(", "\n", "type", "(", "value", ")", "\n", "if", "not", "isinstance", "(", "value", ",", "Sequence", ")", "\n", "else", "\"{}[{}]\"", ".", "format", "(", "type", "(", "value", ")", ",", "\", \"", ".", "join", "(", "str", "(", "type", "(", "v", ")", ")", "for", "v", "in", "value", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.dynamic_import.dynamic_import": [[4, 24], ["dict", "import_path.split", "importlib.import_module", "getattr", "ValueError", "set"], "function", ["None"], ["def", "dynamic_import", "(", "import_path", ",", "alias", "=", "dict", "(", ")", ")", ":", "\n", "    ", "\"\"\"dynamic import module and class\n\n    :param str import_path: syntax 'module_name:class_name'\n        e.g., 'espnet.transform.add_deltas:AddDeltas'\n    :param dict alias: shortcut for registered class\n    :return: imported class\n    \"\"\"", "\n", "if", "import_path", "not", "in", "alias", "and", "\":\"", "not", "in", "import_path", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"import_path should be one of {} or \"", "\n", "'include \":\", e.g. \"espnet.transform.add_deltas:AddDeltas\" : '", "\n", "\"{}\"", ".", "format", "(", "set", "(", "alias", ")", ",", "import_path", ")", "\n", ")", "\n", "", "if", "\":\"", "not", "in", "import_path", ":", "\n", "        ", "import_path", "=", "alias", "[", "import_path", "]", "\n", "\n", "", "module_name", ",", "objname", "=", "import_path", ".", "split", "(", "\":\"", ")", "\n", "m", "=", "importlib", ".", "import_module", "(", "module_name", ")", "\n", "return", "getattr", "(", "m", ",", "objname", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.CompareValueTrigger.__init__": [[29, 37], ["training.util.get_trigger", "asr_utils.CompareValueTrigger._init_summary"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.CompareValueTrigger._init_summary"], ["def", "__init__", "(", "self", ",", "key", ",", "compare_fn", ",", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ")", ":", "\n", "        ", "from", "chainer", "import", "training", "\n", "\n", "self", ".", "_key", "=", "key", "\n", "self", ".", "_best_value", "=", "None", "\n", "self", ".", "_interval_trigger", "=", "training", ".", "util", ".", "get_trigger", "(", "trigger", ")", "\n", "self", ".", "_init_summary", "(", ")", "\n", "self", ".", "_compare_fn", "=", "compare_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.CompareValueTrigger.__call__": [[38, 62], ["summary.compute_mean", "float", "asr_utils.CompareValueTrigger._init_summary", "summary.add", "asr_utils.CompareValueTrigger._interval_trigger", "asr_utils.CompareValueTrigger._compare_fn"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.CompareValueTrigger._init_summary"], ["", "def", "__call__", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "\"\"\"Get value related to the key and compare with current value.\"\"\"", "\n", "observation", "=", "trainer", ".", "observation", "\n", "summary", "=", "self", ".", "_summary", "\n", "key", "=", "self", ".", "_key", "\n", "if", "key", "in", "observation", ":", "\n", "            ", "summary", ".", "add", "(", "{", "key", ":", "observation", "[", "key", "]", "}", ")", "\n", "\n", "", "if", "not", "self", ".", "_interval_trigger", "(", "trainer", ")", ":", "\n", "            ", "return", "False", "\n", "\n", "", "stats", "=", "summary", ".", "compute_mean", "(", ")", "\n", "value", "=", "float", "(", "stats", "[", "key", "]", ")", "# copy to CPU", "\n", "self", ".", "_init_summary", "(", ")", "\n", "\n", "if", "self", ".", "_best_value", "is", "None", ":", "\n", "# initialize best value", "\n", "            ", "self", ".", "_best_value", "=", "value", "\n", "return", "False", "\n", "", "elif", "self", ".", "_compare_fn", "(", "self", ".", "_best_value", ",", "value", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "_best_value", "=", "value", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.CompareValueTrigger._init_summary": [[63, 67], ["chainer.reporter.DictSummary"], "methods", ["None"], ["", "", "def", "_init_summary", "(", "self", ")", ":", "\n", "        ", "import", "chainer", "\n", "\n", "self", ".", "_summary", "=", "chainer", ".", "reporter", ".", "DictSummary", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.restore_snapshot": [[506, 524], ["training.make_extension", "asr_utils._restore_snapshot"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._restore_snapshot"], ["", "", "", "def", "restore_snapshot", "(", "model", ",", "snapshot", ",", "load_fn", "=", "None", ")", ":", "\n", "    ", "\"\"\"Extension to restore snapshot.\n\n    Returns:\n        An extension function.\n\n    \"\"\"", "\n", "import", "chainer", "\n", "from", "chainer", "import", "training", "\n", "\n", "if", "load_fn", "is", "None", ":", "\n", "        ", "load_fn", "=", "chainer", ".", "serializers", ".", "load_npz", "\n", "\n", "", "@", "training", ".", "make_extension", "(", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ")", "\n", "def", "restore_snapshot", "(", "trainer", ")", ":", "\n", "        ", "_restore_snapshot", "(", "model", ",", "snapshot", ",", "load_fn", ")", "\n", "\n", "", "return", "restore_snapshot", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._restore_snapshot": [[526, 534], ["load_fn", "logging.info", "str"], "function", ["None"], ["", "def", "_restore_snapshot", "(", "model", ",", "snapshot", ",", "load_fn", "=", "None", ")", ":", "\n", "    ", "if", "load_fn", "is", "None", ":", "\n", "        ", "import", "chainer", "\n", "\n", "load_fn", "=", "chainer", ".", "serializers", ".", "load_npz", "\n", "\n", "", "load_fn", "(", "snapshot", ",", "model", ")", "\n", "logging", ".", "info", "(", "\"restored from \"", "+", "str", "(", "snapshot", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.adadelta_eps_decay": [[536, 553], ["training.make_extension", "asr_utils._adadelta_eps_decay"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._adadelta_eps_decay"], ["", "def", "adadelta_eps_decay", "(", "eps_decay", ")", ":", "\n", "    ", "\"\"\"Extension to perform adadelta eps decay.\n\n    Args:\n        eps_decay (float): Decay rate of eps.\n\n    Returns:\n        An extension function.\n\n    \"\"\"", "\n", "from", "chainer", "import", "training", "\n", "\n", "@", "training", ".", "make_extension", "(", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ")", "\n", "def", "adadelta_eps_decay", "(", "trainer", ")", ":", "\n", "        ", "_adadelta_eps_decay", "(", "trainer", ",", "eps_decay", ")", "\n", "\n", "", "return", "adadelta_eps_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._adadelta_eps_decay": [[555, 567], ["trainer.updater.get_optimizer", "hasattr", "setattr", "logging.info", "logging.info", "str", "str"], "function", ["None"], ["", "def", "_adadelta_eps_decay", "(", "trainer", ",", "eps_decay", ")", ":", "\n", "    ", "optimizer", "=", "trainer", ".", "updater", ".", "get_optimizer", "(", "\"main\"", ")", "\n", "# for chainer", "\n", "if", "hasattr", "(", "optimizer", ",", "\"eps\"", ")", ":", "\n", "        ", "current_eps", "=", "optimizer", ".", "eps", "\n", "setattr", "(", "optimizer", ",", "\"eps\"", ",", "current_eps", "*", "eps_decay", ")", "\n", "logging", ".", "info", "(", "\"adadelta eps decayed to \"", "+", "str", "(", "optimizer", ".", "eps", ")", ")", "\n", "# pytorch", "\n", "", "else", ":", "\n", "        ", "for", "p", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "p", "[", "\"eps\"", "]", "*=", "eps_decay", "\n", "logging", ".", "info", "(", "\"adadelta eps decayed to \"", "+", "str", "(", "p", "[", "\"eps\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.adam_lr_decay": [[569, 586], ["training.make_extension", "asr_utils._adam_lr_decay"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._adam_lr_decay"], ["", "", "", "def", "adam_lr_decay", "(", "eps_decay", ")", ":", "\n", "    ", "\"\"\"Extension to perform adam lr decay.\n\n    Args:\n        eps_decay (float): Decay rate of lr.\n\n    Returns:\n        An extension function.\n\n    \"\"\"", "\n", "from", "chainer", "import", "training", "\n", "\n", "@", "training", ".", "make_extension", "(", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ")", "\n", "def", "adam_lr_decay", "(", "trainer", ")", ":", "\n", "        ", "_adam_lr_decay", "(", "trainer", ",", "eps_decay", ")", "\n", "\n", "", "return", "adam_lr_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._adam_lr_decay": [[588, 600], ["trainer.updater.get_optimizer", "hasattr", "setattr", "logging.info", "logging.info", "str", "str"], "function", ["None"], ["", "def", "_adam_lr_decay", "(", "trainer", ",", "eps_decay", ")", ":", "\n", "    ", "optimizer", "=", "trainer", ".", "updater", ".", "get_optimizer", "(", "\"main\"", ")", "\n", "# for chainer", "\n", "if", "hasattr", "(", "optimizer", ",", "\"lr\"", ")", ":", "\n", "        ", "current_lr", "=", "optimizer", ".", "lr", "\n", "setattr", "(", "optimizer", ",", "\"lr\"", ",", "current_lr", "*", "eps_decay", ")", "\n", "logging", ".", "info", "(", "\"adam lr decayed to \"", "+", "str", "(", "optimizer", ".", "lr", ")", ")", "\n", "# pytorch", "\n", "", "else", ":", "\n", "        ", "for", "p", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "p", "[", "\"lr\"", "]", "*=", "eps_decay", "\n", "logging", ".", "info", "(", "\"adam lr decayed to \"", "+", "str", "(", "p", "[", "\"lr\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_snapshot": [[602, 616], ["extension.make_extension", "asr_utils._torch_snapshot_object", "filename.format"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._torch_snapshot_object"], ["", "", "", "def", "torch_snapshot", "(", "savefun", "=", "torch", ".", "save", ",", "filename", "=", "\"snapshot.ep.{.updater.epoch}\"", ")", ":", "\n", "    ", "\"\"\"Extension to take snapshot of the trainer for pytorch.\n\n    Returns:\n        An extension function.\n\n    \"\"\"", "\n", "from", "chainer", ".", "training", "import", "extension", "\n", "\n", "@", "extension", ".", "make_extension", "(", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ",", "priority", "=", "-", "100", ")", "\n", "def", "torch_snapshot", "(", "trainer", ")", ":", "\n", "        ", "_torch_snapshot_object", "(", "trainer", ",", "trainer", ",", "filename", ".", "format", "(", "trainer", ")", ",", "savefun", ")", "\n", "\n", "", "return", "torch_snapshot", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils._torch_snapshot_object": [[618, 652], ["DictionarySerializer", "DictionarySerializer.save", "hasattr", "filename.format", "tempfile.mkdtemp", "os.path.join", "hasattr", "hasattr", "trainer.updater.get_optimizer().state_dict", "savefun", "shutil.move", "shutil.rmtree", "trainer.updater.model.model.module.state_dict", "trainer.updater.model.model.state_dict", "trainer.updater.model.module.state_dict", "trainer.updater.model.state_dict", "os.path.join", "trainer.updater.get_optimizer"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict"], ["", "def", "_torch_snapshot_object", "(", "trainer", ",", "target", ",", "filename", ",", "savefun", ")", ":", "\n", "    ", "from", "chainer", ".", "serializers", "import", "DictionarySerializer", "\n", "\n", "# make snapshot_dict dictionary", "\n", "s", "=", "DictionarySerializer", "(", ")", "\n", "s", ".", "save", "(", "trainer", ")", "\n", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ",", "\"model\"", ")", ":", "\n", "# (for TTS)", "\n", "        ", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ".", "model", ",", "\"module\"", ")", ":", "\n", "            ", "model_state_dict", "=", "trainer", ".", "updater", ".", "model", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "            ", "model_state_dict", "=", "trainer", ".", "updater", ".", "model", ".", "model", ".", "state_dict", "(", ")", "\n", "", "", "else", ":", "\n", "# (for ASR)", "\n", "        ", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ",", "\"module\"", ")", ":", "\n", "            ", "model_state_dict", "=", "trainer", ".", "updater", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "            ", "model_state_dict", "=", "trainer", ".", "updater", ".", "model", ".", "state_dict", "(", ")", "\n", "", "", "snapshot_dict", "=", "{", "\n", "\"trainer\"", ":", "s", ".", "target", ",", "\n", "\"model\"", ":", "model_state_dict", ",", "\n", "\"optimizer\"", ":", "trainer", ".", "updater", ".", "get_optimizer", "(", "\"main\"", ")", ".", "state_dict", "(", ")", ",", "\n", "}", "\n", "\n", "# save snapshot dictionary", "\n", "fn", "=", "filename", ".", "format", "(", "trainer", ")", "\n", "prefix", "=", "\"tmp\"", "+", "fn", "\n", "tmpdir", "=", "tempfile", ".", "mkdtemp", "(", "prefix", "=", "prefix", ",", "dir", "=", "trainer", ".", "out", ")", "\n", "tmppath", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "fn", ")", "\n", "try", ":", "\n", "        ", "savefun", "(", "snapshot_dict", ",", "tmppath", ")", "\n", "shutil", ".", "move", "(", "tmppath", ",", "os", ".", "path", ".", "join", "(", "trainer", ".", "out", ",", "fn", ")", ")", "\n", "", "finally", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "tmpdir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.add_gradient_noise": [[654, 675], ["model.parameters", "param.grad.size", "torch.randn().to", "torch.randn"], "function", ["None"], ["", "", "def", "add_gradient_noise", "(", "model", ",", "iteration", ",", "duration", "=", "100", ",", "eta", "=", "1.0", ",", "scale_factor", "=", "0.55", ")", ":", "\n", "    ", "\"\"\"Adds noise from a standard normal distribution to the gradients.\n\n    The standard deviation (`sigma`) is controlled by the three hyper-parameters below.\n    `sigma` goes to zero (no noise) with more iterations.\n\n    Args:\n        model (torch.nn.model): Model.\n        iteration (int): Number of iterations.\n        duration (int) {100, 1000}:\n            Number of durations to control the interval of the `sigma` change.\n        eta (float) {0.01, 0.3, 1.0}: The magnitude of `sigma`.\n        scale_factor (float) {0.55}: The scale of `sigma`.\n    \"\"\"", "\n", "interval", "=", "(", "iteration", "//", "duration", ")", "+", "1", "\n", "sigma", "=", "eta", "/", "interval", "**", "scale_factor", "\n", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "            ", "_shape", "=", "param", ".", "grad", ".", "size", "(", ")", "\n", "noise", "=", "sigma", "*", "torch", ".", "randn", "(", "_shape", ")", ".", "to", "(", "param", ".", "device", ")", "\n", "param", ".", "grad", "+=", "noise", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.get_model_conf": [[678, 704], ["isinstance", "open", "logging.info", "json.load", "argparse.Namespace", "os.path.dirname", "argparse.Namespace"], "function", ["None"], ["", "", "", "def", "get_model_conf", "(", "model_path", ",", "conf_path", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get model config information by reading a model config file (model.json).\n\n    Args:\n        model_path (str): Model path.\n        conf_path (str): Optional model config path.\n\n    Returns:\n        list[int, int, dict[str, Any]]: Config information loaded from json file.\n\n    \"\"\"", "\n", "if", "conf_path", "is", "None", ":", "\n", "        ", "model_conf", "=", "os", ".", "path", ".", "dirname", "(", "model_path", ")", "+", "\"/model.json\"", "\n", "", "else", ":", "\n", "        ", "model_conf", "=", "conf_path", "\n", "", "with", "open", "(", "model_conf", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "logging", ".", "info", "(", "\"reading a config file from \"", "+", "model_conf", ")", "\n", "confs", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "isinstance", "(", "confs", ",", "dict", ")", ":", "\n", "# for lm", "\n", "        ", "args", "=", "confs", "\n", "return", "argparse", ".", "Namespace", "(", "**", "args", ")", "\n", "", "else", ":", "\n", "# for asr, tts, mt", "\n", "        ", "idim", ",", "odim", ",", "args", "=", "confs", "\n", "return", "idim", ",", "odim", ",", "argparse", ".", "Namespace", "(", "**", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.chainer_load": [[706, 720], ["os.path.basename", "chainer.serializers.load_npz", "chainer.serializers.load_npz"], "function", ["None"], ["", "", "def", "chainer_load", "(", "path", ",", "model", ")", ":", "\n", "    ", "\"\"\"Load chainer model parameters.\n\n    Args:\n        path (str): Model path or snapshot file path to be loaded.\n        model (chainer.Chain): Chainer model.\n\n    \"\"\"", "\n", "import", "chainer", "\n", "\n", "if", "\"snapshot\"", "in", "os", ".", "path", ".", "basename", "(", "path", ")", ":", "\n", "        ", "chainer", ".", "serializers", ".", "load_npz", "(", "path", ",", "model", ",", "path", "=", "\"updater/model:main/\"", ")", "\n", "", "else", ":", "\n", "        ", "chainer", ".", "serializers", ".", "load_npz", "(", "path", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_save": [[722, 734], ["hasattr", "torch.save", "torch.save", "model.module.state_dict", "model.state_dict"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict"], ["", "", "def", "torch_save", "(", "path", ",", "model", ")", ":", "\n", "    ", "\"\"\"Save torch model states.\n\n    Args:\n        path (str): Model path to be saved.\n        model (torch.nn.Module): Torch model.\n\n    \"\"\"", "\n", "if", "hasattr", "(", "model", ",", "\"module\"", ")", ":", "\n", "        ", "torch", ".", "save", "(", "model", ".", "module", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.snapshot_object": [[736, 758], ["extension.make_extension", "asr_utils.torch_save", "os.path.join", "filename.format"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_save"], ["", "", "def", "snapshot_object", "(", "target", ",", "filename", ")", ":", "\n", "    ", "\"\"\"Returns a trainer extension to take snapshots of a given object.\n\n    Args:\n        target (model): Object to serialize.\n        filename (str): Name of the file into which the object is serialized.It can\n            be a format string, where the trainer object is passed to\n            the :meth: `str.format` method. For example,\n            ``'snapshot_{.updater.iteration}'`` is converted to\n            ``'snapshot_10000'`` at the 10,000th iteration.\n\n    Returns:\n        An extension function.\n\n    \"\"\"", "\n", "from", "chainer", ".", "training", "import", "extension", "\n", "\n", "@", "extension", ".", "make_extension", "(", "trigger", "=", "(", "1", ",", "\"epoch\"", ")", ",", "priority", "=", "-", "100", ")", "\n", "def", "snapshot_object", "(", "trainer", ")", ":", "\n", "        ", "torch_save", "(", "os", ".", "path", ".", "join", "(", "trainer", ".", "out", ",", "filename", ".", "format", "(", "trainer", ")", ")", ",", "target", ")", "\n", "\n", "", "return", "snapshot_object", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_load": [[760, 781], ["hasattr", "os.path.basename", "torch.load", "model.module.load_state_dict", "model.load_state_dict", "torch.load"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict"], ["", "def", "torch_load", "(", "path", ",", "model", ")", ":", "\n", "    ", "\"\"\"Load torch model states.\n\n    Args:\n        path (str): Model path or snapshot file path to be loaded.\n        model (torch.nn.Module): Torch model.\n\n    \"\"\"", "\n", "if", "\"snapshot\"", "in", "os", ".", "path", ".", "basename", "(", "path", ")", ":", "\n", "        ", "model_state_dict", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "[", "\n", "\"model\"", "\n", "]", "\n", "", "else", ":", "\n", "        ", "model_state_dict", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "\n", "", "if", "hasattr", "(", "model", ",", "\"module\"", ")", ":", "\n", "        ", "model", ".", "module", ".", "load_state_dict", "(", "model_state_dict", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "model_state_dict", ")", "\n", "\n", "", "del", "model_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.torch_resume": [[783, 819], ["torch.load", "NpzDeserializer", "NpzDeserializer.load", "hasattr", "trainer.updater.get_optimizer().load_state_dict", "hasattr", "hasattr", "trainer.updater.model.model.module.load_state_dict", "trainer.updater.model.model.load_state_dict", "trainer.updater.model.module.load_state_dict", "trainer.updater.model.load_state_dict", "trainer.updater.get_optimizer"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict"], ["", "def", "torch_resume", "(", "snapshot_path", ",", "trainer", ")", ":", "\n", "    ", "\"\"\"Resume from snapshot for pytorch.\n\n    Args:\n        snapshot_path (str): Snapshot file path.\n        trainer (chainer.training.Trainer): Chainer's trainer instance.\n\n    \"\"\"", "\n", "from", "chainer", ".", "serializers", "import", "NpzDeserializer", "\n", "\n", "# load snapshot", "\n", "snapshot_dict", "=", "torch", ".", "load", "(", "snapshot_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "\n", "# restore trainer states", "\n", "d", "=", "NpzDeserializer", "(", "snapshot_dict", "[", "\"trainer\"", "]", ")", "\n", "d", ".", "load", "(", "trainer", ")", "\n", "\n", "# restore model states", "\n", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ",", "\"model\"", ")", ":", "\n", "# (for TTS model)", "\n", "        ", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ".", "model", ",", "\"module\"", ")", ":", "\n", "            ", "trainer", ".", "updater", ".", "model", ".", "model", ".", "module", ".", "load_state_dict", "(", "snapshot_dict", "[", "\"model\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "trainer", ".", "updater", ".", "model", ".", "model", ".", "load_state_dict", "(", "snapshot_dict", "[", "\"model\"", "]", ")", "\n", "", "", "else", ":", "\n", "# (for ASR model)", "\n", "        ", "if", "hasattr", "(", "trainer", ".", "updater", ".", "model", ",", "\"module\"", ")", ":", "\n", "            ", "trainer", ".", "updater", ".", "model", ".", "module", ".", "load_state_dict", "(", "snapshot_dict", "[", "\"model\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "trainer", ".", "updater", ".", "model", ".", "load_state_dict", "(", "snapshot_dict", "[", "\"model\"", "]", ")", "\n", "\n", "# retore optimizer states", "\n", "", "", "trainer", ".", "updater", ".", "get_optimizer", "(", "\"main\"", ")", ".", "load_state_dict", "(", "snapshot_dict", "[", "\"optimizer\"", "]", ")", "\n", "\n", "# delete opened snapshot", "\n", "del", "snapshot_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.parse_hypothesis": [[822, 844], ["list", "float", "map", "str"], "function", ["None"], ["", "def", "parse_hypothesis", "(", "hyp", ",", "char_list", ")", ":", "\n", "    ", "\"\"\"Parse hypothesis.\n\n    Args:\n        hyp (list[dict[str, Any]]): Recognition hypothesis.\n        char_list (list[str]): List of characters.\n\n    Returns:\n        tuple(str, str, str, float)\n\n    \"\"\"", "\n", "# remove sos and get results", "\n", "tokenid_as_list", "=", "list", "(", "map", "(", "int", ",", "hyp", "[", "\"yseq\"", "]", "[", "1", ":", "]", ")", ")", "\n", "token_as_list", "=", "[", "char_list", "[", "idx", "]", "for", "idx", "in", "tokenid_as_list", "]", "\n", "score", "=", "float", "(", "hyp", "[", "\"score\"", "]", ")", "\n", "\n", "# convert to string", "\n", "tokenid", "=", "\" \"", ".", "join", "(", "[", "str", "(", "idx", ")", "for", "idx", "in", "tokenid_as_list", "]", ")", "\n", "token", "=", "\" \"", ".", "join", "(", "token_as_list", ")", "\n", "text", "=", "\"\"", ".", "join", "(", "token_as_list", ")", ".", "replace", "(", "\"<space>\"", ",", "\" \"", ")", "\n", "\n", "return", "text", ",", "token", ",", "tokenid", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.add_results_to_json": [[846, 860], ["asr_utils.parse_hypothesis", "len"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.parse_hypothesis"], ["", "def", "add_results_to_json", "(", "nbest_hyps", ",", "char_list", ")", ":", "\n", "    ", "\"\"\"Add N-best results to json.\n    Args:\n        js (dict[str, Any]): Groundtruth utterance dict.\n        nbest_hyps_sd (list[dict[str, Any]]):\n            List of hypothesis for multi_speakers: nutts x nspkrs.\n        char_list (list[str]): List of characters.\n    Returns:\n        str: 1-best result\n    \"\"\"", "\n", "assert", "len", "(", "nbest_hyps", ")", "==", "1", ",", "\"only 1-best result is supported.\"", "\n", "# parse hypothesis", "\n", "rec_text", ",", "rec_token", ",", "rec_tokenid", ",", "score", "=", "parse_hypothesis", "(", "nbest_hyps", "[", "0", "]", ",", "char_list", ")", "\n", "return", "rec_text", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.plot_spectrogram": [[862, 939], ["numpy.abs", "plt.imshow", "plt.colorbar().set_label", "plt.tick_params", "plt.axis", "plt.xlabel", "plt.ylabel", "numpy.log10", "ValueError", "plt.colorbar", "numpy.finfo"], "function", ["None"], ["", "def", "plot_spectrogram", "(", "\n", "plt", ",", "\n", "spec", ",", "\n", "mode", "=", "\"db\"", ",", "\n", "fs", "=", "None", ",", "\n", "frame_shift", "=", "None", ",", "\n", "bottom", "=", "True", ",", "\n", "left", "=", "True", ",", "\n", "right", "=", "True", ",", "\n", "top", "=", "False", ",", "\n", "labelbottom", "=", "True", ",", "\n", "labelleft", "=", "True", ",", "\n", "labelright", "=", "True", ",", "\n", "labeltop", "=", "False", ",", "\n", "cmap", "=", "\"inferno\"", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Plot spectrogram using matplotlib.\n\n    Args:\n        plt (matplotlib.pyplot): pyplot object.\n        spec (numpy.ndarray): Input stft (Freq, Time)\n        mode (str): db or linear.\n        fs (int): Sample frequency. To convert y-axis to kHz unit.\n        frame_shift (int): The frame shift of stft. To convert x-axis to second unit.\n        bottom (bool):Whether to draw the respective ticks.\n        left (bool):\n        right (bool):\n        top (bool):\n        labelbottom (bool):Whether to draw the respective tick labels.\n        labelleft (bool):\n        labelright (bool):\n        labeltop (bool):\n        cmap (str): Colormap defined in matplotlib.\n\n    \"\"\"", "\n", "spec", "=", "np", ".", "abs", "(", "spec", ")", "\n", "if", "mode", "==", "\"db\"", ":", "\n", "        ", "x", "=", "20", "*", "np", ".", "log10", "(", "spec", "+", "np", ".", "finfo", "(", "spec", ".", "dtype", ")", ".", "eps", ")", "\n", "", "elif", "mode", "==", "\"linear\"", ":", "\n", "        ", "x", "=", "spec", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "mode", ")", "\n", "\n", "", "if", "fs", "is", "not", "None", ":", "\n", "        ", "ytop", "=", "fs", "/", "2000", "\n", "ylabel", "=", "\"kHz\"", "\n", "", "else", ":", "\n", "        ", "ytop", "=", "x", ".", "shape", "[", "0", "]", "\n", "ylabel", "=", "\"bin\"", "\n", "\n", "", "if", "frame_shift", "is", "not", "None", "and", "fs", "is", "not", "None", ":", "\n", "        ", "xtop", "=", "x", ".", "shape", "[", "1", "]", "*", "frame_shift", "/", "fs", "\n", "xlabel", "=", "\"s\"", "\n", "", "else", ":", "\n", "        ", "xtop", "=", "x", ".", "shape", "[", "1", "]", "\n", "xlabel", "=", "\"frame\"", "\n", "\n", "", "extent", "=", "(", "0", ",", "xtop", ",", "0", ",", "ytop", ")", "\n", "plt", ".", "imshow", "(", "x", "[", ":", ":", "-", "1", "]", ",", "cmap", "=", "cmap", ",", "extent", "=", "extent", ")", "\n", "\n", "if", "labelbottom", ":", "\n", "        ", "plt", ".", "xlabel", "(", "\"time [{}]\"", ".", "format", "(", "xlabel", ")", ")", "\n", "", "if", "labelleft", ":", "\n", "        ", "plt", ".", "ylabel", "(", "\"freq [{}]\"", ".", "format", "(", "ylabel", ")", ")", "\n", "", "plt", ".", "colorbar", "(", ")", ".", "set_label", "(", "\"{}\"", ".", "format", "(", "mode", ")", ")", "\n", "\n", "plt", ".", "tick_params", "(", "\n", "bottom", "=", "bottom", ",", "\n", "left", "=", "left", ",", "\n", "right", "=", "right", ",", "\n", "top", "=", "top", ",", "\n", "labelbottom", "=", "labelbottom", ",", "\n", "labelleft", "=", "labelleft", ",", "\n", "labelright", "=", "labelright", ",", "\n", "labeltop", "=", "labeltop", ",", "\n", ")", "\n", "plt", ".", "axis", "(", "\"auto\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.asr.asr_utils.format_mulenc_args": [[942, 991], ["default_dict.keys", "isinstance", "logging.warning", "vars", "len", "logging.warning", "vars", "logging.warning", "vars", "vars", "vars", "vars", "vars", "range", "vars", "vars", "vars", "vars", "range", "vars"], "function", ["None"], ["", "def", "format_mulenc_args", "(", "args", ")", ":", "\n", "    ", "\"\"\"Format args for multi-encoder setup.\n\n    It deals with following situations:  (when args.num_encs=2):\n    1. args.elayers = None -> args.elayers = [4, 4];\n    2. args.elayers = 4 -> args.elayers = [4, 4];\n    3. args.elayers = [4, 4, 4] -> args.elayers = [4, 4].\n\n    \"\"\"", "\n", "# default values when None is assigned.", "\n", "default_dict", "=", "{", "\n", "\"etype\"", ":", "\"blstmp\"", ",", "\n", "\"elayers\"", ":", "4", ",", "\n", "\"eunits\"", ":", "300", ",", "\n", "\"subsample\"", ":", "\"1\"", ",", "\n", "\"dropout_rate\"", ":", "0.0", ",", "\n", "\"atype\"", ":", "\"dot\"", ",", "\n", "\"adim\"", ":", "320", ",", "\n", "\"awin\"", ":", "5", ",", "\n", "\"aheads\"", ":", "4", ",", "\n", "\"aconv_chans\"", ":", "-", "1", ",", "\n", "\"aconv_filts\"", ":", "100", ",", "\n", "}", "\n", "for", "k", "in", "default_dict", ".", "keys", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "vars", "(", "args", ")", "[", "k", "]", ",", "list", ")", ":", "\n", "            ", "if", "len", "(", "vars", "(", "args", ")", "[", "k", "]", ")", "!=", "args", ".", "num_encs", ":", "\n", "                ", "logging", ".", "warning", "(", "\n", "\"Length mismatch {}: Convert {} to {}.\"", ".", "format", "(", "\n", "k", ",", "vars", "(", "args", ")", "[", "k", "]", ",", "vars", "(", "args", ")", "[", "k", "]", "[", ":", "args", ".", "num_encs", "]", "\n", ")", "\n", ")", "\n", "", "vars", "(", "args", ")", "[", "k", "]", "=", "vars", "(", "args", ")", "[", "k", "]", "[", ":", "args", ".", "num_encs", "]", "\n", "", "else", ":", "\n", "            ", "if", "not", "vars", "(", "args", ")", "[", "k", "]", ":", "\n", "# assign default value if it is None", "\n", "                ", "vars", "(", "args", ")", "[", "k", "]", "=", "default_dict", "[", "k", "]", "\n", "logging", ".", "warning", "(", "\n", "\"{} is not specified, use default value {}.\"", ".", "format", "(", "\n", "k", ",", "default_dict", "[", "k", "]", "\n", ")", "\n", ")", "\n", "# duplicate", "\n", "", "logging", ".", "warning", "(", "\n", "\"Type mismatch {}: Convert {} to {}.\"", ".", "format", "(", "\n", "k", ",", "vars", "(", "args", ")", "[", "k", "]", ",", "[", "vars", "(", "args", ")", "[", "k", "]", "for", "_", "in", "range", "(", "args", ".", "num_encs", ")", "]", "\n", ")", "\n", ")", "\n", "vars", "(", "args", ")", "[", "k", "]", "=", "[", "vars", "(", "args", ")", "[", "k", "]", "for", "_", "in", "range", "(", "args", ".", "num_encs", ")", "]", "\n", "", "", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.Hypothesis.asdict": [[27, 33], ["beam_search.Hypothesis._replace()._asdict", "beam_search.Hypothesis._replace", "beam_search.Hypothesis.yseq.tolist", "float", "float", "beam_search.Hypothesis.scores.items"], "methods", ["None"], ["def", "asdict", "(", "self", ")", "->", "dict", ":", "\n", "        ", "\"\"\"Convert data to JSON-friendly dict.\"\"\"", "\n", "return", "self", ".", "_replace", "(", "\n", "yseq", "=", "self", ".", "yseq", ".", "tolist", "(", ")", ",", "\n", "score", "=", "float", "(", "self", ".", "score", ")", ",", "\n", "scores", "=", "{", "k", ":", "float", "(", "v", ")", "for", "k", ",", "v", "in", "self", ".", "scores", ".", "items", "(", ")", "}", ",", "\n", ")", ".", "_asdict", "(", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.__init__": [[39, 111], ["super().__init__", "dict", "dict", "dict", "torch.nn.ModuleDict", "scorers.items", "int", "weights.get", "isinstance", "isinstance", "isinstance", "KeyError", "len", "type"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "scorers", ":", "Dict", "[", "str", ",", "ScorerInterface", "]", ",", "\n", "weights", ":", "Dict", "[", "str", ",", "float", "]", ",", "\n", "beam_size", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "sos", ":", "int", ",", "\n", "eos", ":", "int", ",", "\n", "token_list", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "pre_beam_ratio", ":", "float", "=", "1.5", ",", "\n", "pre_beam_score_key", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize beam search.\n\n        Args:\n            scorers (dict[str, ScorerInterface]): Dict of decoder modules\n                e.g., Decoder, CTCPrefixScorer, LM\n                The scorer will be ignored if it is `None`\n            weights (dict[str, float]): Dict of weights for each scorers\n                The scorer will be ignored if its weight is 0\n            beam_size (int): The number of hypotheses kept during search\n            vocab_size (int): The number of vocabulary\n            sos (int): Start of sequence id\n            eos (int): End of sequence id\n            token_list (list[str]): List of tokens for debug log\n            pre_beam_score_key (str): key of scores to perform pre-beam search\n            pre_beam_ratio (float): beam size in the pre-beam search\n                will be `int(pre_beam_ratio * beam_size)`\n\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# set scorers", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "scorers", "=", "dict", "(", ")", "\n", "self", ".", "full_scorers", "=", "dict", "(", ")", "\n", "self", ".", "part_scorers", "=", "dict", "(", ")", "\n", "# this module dict is required for recursive cast", "\n", "# `self.to(device, dtype)` in `recog.py`", "\n", "self", ".", "nn_dict", "=", "torch", ".", "nn", ".", "ModuleDict", "(", ")", "\n", "for", "k", ",", "v", "in", "scorers", ".", "items", "(", ")", ":", "\n", "            ", "w", "=", "weights", ".", "get", "(", "k", ",", "0", ")", "\n", "if", "w", "==", "0", "or", "v", "is", "None", ":", "\n", "                ", "continue", "\n", "", "assert", "isinstance", "(", "\n", "v", ",", "ScorerInterface", "\n", ")", ",", "f\"{k} ({type(v)}) does not implement ScorerInterface\"", "\n", "self", ".", "scorers", "[", "k", "]", "=", "v", "\n", "if", "isinstance", "(", "v", ",", "PartialScorerInterface", ")", ":", "\n", "                ", "self", ".", "part_scorers", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "                ", "self", ".", "full_scorers", "[", "k", "]", "=", "v", "\n", "", "if", "isinstance", "(", "v", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "                ", "self", ".", "nn_dict", "[", "k", "]", "=", "v", "\n", "\n", "# set configurations", "\n", "", "", "self", ".", "sos", "=", "sos", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "token_list", "=", "token_list", "\n", "self", ".", "pre_beam_size", "=", "int", "(", "pre_beam_ratio", "*", "beam_size", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "n_vocab", "=", "vocab_size", "\n", "if", "(", "\n", "pre_beam_score_key", "is", "not", "None", "\n", "and", "pre_beam_score_key", "!=", "\"full\"", "\n", "and", "pre_beam_score_key", "not", "in", "self", ".", "full_scorers", "\n", ")", ":", "\n", "            ", "raise", "KeyError", "(", "f\"{pre_beam_score_key} is not found in {self.full_scorers}\"", ")", "\n", "", "self", ".", "pre_beam_score_key", "=", "pre_beam_score_key", "\n", "self", ".", "do_pre_beam", "=", "(", "\n", "self", ".", "pre_beam_score_key", "is", "not", "None", "\n", "and", "self", ".", "pre_beam_size", "<", "self", ".", "n_vocab", "\n", "and", "len", "(", "self", ".", "part_scorers", ")", ">", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.init_hyp": [[113, 134], ["dict", "dict", "beam_search.BeamSearch.scorers.items", "d.init_state", "beam_search.Hypothesis", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.init_state"], ["", "def", "init_hyp", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "List", "[", "Hypothesis", "]", ":", "\n", "        ", "\"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"", "\n", "init_states", "=", "dict", "(", ")", "\n", "init_scores", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "scorers", ".", "items", "(", ")", ":", "\n", "            ", "init_states", "[", "k", "]", "=", "d", ".", "init_state", "(", "x", ")", "\n", "init_scores", "[", "k", "]", "=", "0.0", "\n", "", "return", "[", "\n", "Hypothesis", "(", "\n", "score", "=", "0.0", ",", "\n", "scores", "=", "init_scores", ",", "\n", "states", "=", "init_states", ",", "\n", "yseq", "=", "torch", ".", "tensor", "(", "[", "self", ".", "sos", "]", ",", "device", "=", "x", ".", "device", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.append_token": [[137, 151], ["torch.tensor", "torch.cat"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "append_token", "(", "xs", ":", "torch", ".", "Tensor", ",", "x", ":", "int", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Append new token to prefix tokens.\n\n        Args:\n            xs (torch.Tensor): The prefix token\n            x (int): The new token to append\n\n        Returns:\n            torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device\n\n        \"\"\"", "\n", "x", "=", "torch", ".", "tensor", "(", "[", "x", "]", ",", "dtype", "=", "xs", ".", "dtype", ",", "device", "=", "xs", ".", "device", ")", "\n", "return", "torch", ".", "cat", "(", "(", "xs", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.score_full": [[152, 174], ["dict", "dict", "beam_search.BeamSearch.full_scorers.items", "d.score"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.score"], ["", "def", "score_full", "(", "\n", "self", ",", "hyp", ":", "Hypothesis", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"", "\n", "scores", "=", "dict", "(", ")", "\n", "states", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "full_scorers", ".", "items", "(", ")", ":", "\n", "            ", "scores", "[", "k", "]", ",", "states", "[", "k", "]", "=", "d", ".", "score", "(", "hyp", ".", "yseq", ",", "hyp", ".", "states", "[", "k", "]", ",", "x", ")", "\n", "", "return", "scores", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.score_partial": [[175, 198], ["dict", "dict", "beam_search.BeamSearch.part_scorers.items", "d.score_partial"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.score_partial"], ["", "def", "score_partial", "(", "\n", "self", ",", "hyp", ":", "Hypothesis", ",", "ids", ":", "torch", ".", "Tensor", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new hypothesis by `self.part_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 1D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.part_scorers`\n                and tensor score values of shape: `(len(ids),)`,\n                and state dict that has string keys\n                and state values of `self.part_scorers`\n\n        \"\"\"", "\n", "scores", "=", "dict", "(", ")", "\n", "states", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "part_scorers", ".", "items", "(", ")", ":", "\n", "            ", "scores", "[", "k", "]", ",", "states", "[", "k", "]", "=", "d", ".", "score_partial", "(", "hyp", ".", "yseq", ",", "ids", ",", "hyp", ".", "states", "[", "k", "]", ",", "x", ")", "\n", "", "return", "scores", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.beam": [[199, 227], ["weighted_scores.size", "ids.size", "float", "weighted_scores.topk", "weighted_scores[].topk", "weighted_scores.topk"], "methods", ["None"], ["", "def", "beam", "(", "\n", "self", ",", "weighted_scores", ":", "torch", ".", "Tensor", ",", "ids", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"Compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n            Its shape is `(self.n_vocab,)`.\n            ids (torch.Tensor): The partial token ids to compute topk\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                The topk full token ids and partial token ids.\n                Their shapes are `(self.beam_size,)`\n\n        \"\"\"", "\n", "# no pre beam performed", "\n", "if", "weighted_scores", ".", "size", "(", "0", ")", "==", "ids", ".", "size", "(", "0", ")", ":", "\n", "            ", "top_ids", "=", "weighted_scores", ".", "topk", "(", "self", ".", "beam_size", ")", "[", "1", "]", "\n", "return", "top_ids", ",", "top_ids", "\n", "\n", "# mask pruned in pre-beam not to select in topk", "\n", "", "tmp", "=", "weighted_scores", "[", "ids", "]", "\n", "weighted_scores", "[", ":", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "weighted_scores", "[", "ids", "]", "=", "tmp", "\n", "top_ids", "=", "weighted_scores", ".", "topk", "(", "self", ".", "beam_size", ")", "[", "1", "]", "\n", "local_ids", "=", "weighted_scores", "[", "ids", "]", ".", "topk", "(", "self", ".", "beam_size", ")", "[", "1", "]", "\n", "return", "top_ids", ",", "local_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.merge_scores": [[228, 259], ["dict", "next_full_scores.items", "next_part_scores.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "merge_scores", "(", "\n", "prev_scores", ":", "Dict", "[", "str", ",", "float", "]", ",", "\n", "next_full_scores", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "full_idx", ":", "int", ",", "\n", "next_part_scores", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "part_idx", ":", "int", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"Merge scores for new hypothesis.\n\n        Args:\n            prev_scores (Dict[str, float]):\n                The previous hypothesis scores by `self.scorers`\n            next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers`\n            full_idx (int): The next token id for `next_full_scores`\n            next_part_scores (Dict[str, torch.Tensor]):\n                scores of partial tokens by `self.part_scorers`\n            part_idx (int): The new token id for `next_part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are scalar tensors by the scorers.\n\n        \"\"\"", "\n", "new_scores", "=", "dict", "(", ")", "\n", "for", "k", ",", "v", "in", "next_full_scores", ".", "items", "(", ")", ":", "\n", "            ", "new_scores", "[", "k", "]", "=", "prev_scores", "[", "k", "]", "+", "v", "[", "full_idx", "]", "\n", "", "for", "k", ",", "v", "in", "next_part_scores", ".", "items", "(", ")", ":", "\n", "            ", "new_scores", "[", "k", "]", "=", "prev_scores", "[", "k", "]", "+", "v", "[", "part_idx", "]", "\n", "", "return", "new_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.merge_states": [[260, 280], ["dict", "states.items", "beam_search.BeamSearch.part_scorers.items", "d.select_state"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state"], ["", "def", "merge_states", "(", "self", ",", "states", ":", "Any", ",", "part_states", ":", "Any", ",", "part_idx", ":", "int", ")", "->", "Any", ":", "\n", "        ", "\"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"", "\n", "new_states", "=", "dict", "(", ")", "\n", "for", "k", ",", "v", "in", "states", ".", "items", "(", ")", ":", "\n", "            ", "new_states", "[", "k", "]", "=", "v", "\n", "", "for", "k", ",", "d", "in", "self", ".", "part_scorers", ".", "items", "(", ")", ":", "\n", "            ", "new_states", "[", "k", "]", "=", "d", ".", "select_state", "(", "part_states", "[", "k", "]", ",", "part_idx", ")", "\n", "", "return", "new_states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.search": [[281, 335], ["torch.arange", "torch.zeros", "beam_search.BeamSearch.score_full", "beam_search.BeamSearch.score_partial", "zip", "best_hyps.append", "sorted", "torch.topk", "beam_search.BeamSearch.beam", "beam_search.Hypothesis", "min", "len", "beam_search.BeamSearch.append_token", "beam_search.BeamSearch.merge_scores", "beam_search.BeamSearch.merge_states"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.score_full", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.score_partial", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.beam", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.append_token", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.merge_scores", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.merge_states"], ["", "def", "search", "(", "\n", "self", ",", "running_hyps", ":", "List", "[", "Hypothesis", "]", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "List", "[", "Hypothesis", "]", ":", "\n", "        ", "\"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (List[Hypothesis]): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            List[Hypotheses]: Best sorted hypotheses\n\n        \"\"\"", "\n", "best_hyps", "=", "[", "]", "\n", "part_ids", "=", "torch", ".", "arange", "(", "self", ".", "n_vocab", ",", "device", "=", "x", ".", "device", ")", "# no pre-beam", "\n", "for", "hyp", "in", "running_hyps", ":", "\n", "# scoring", "\n", "            ", "weighted_scores", "=", "torch", ".", "zeros", "(", "self", ".", "n_vocab", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "scores", ",", "states", "=", "self", ".", "score_full", "(", "hyp", ",", "x", ")", "\n", "for", "k", "in", "self", ".", "full_scorers", ":", "\n", "                ", "weighted_scores", "+=", "self", ".", "weights", "[", "k", "]", "*", "scores", "[", "k", "]", "\n", "# partial scoring", "\n", "", "if", "self", ".", "do_pre_beam", ":", "\n", "                ", "pre_beam_scores", "=", "(", "\n", "weighted_scores", "\n", "if", "self", ".", "pre_beam_score_key", "==", "\"full\"", "\n", "else", "scores", "[", "self", ".", "pre_beam_score_key", "]", "\n", ")", "\n", "part_ids", "=", "torch", ".", "topk", "(", "pre_beam_scores", ",", "self", ".", "pre_beam_size", ")", "[", "1", "]", "\n", "", "part_scores", ",", "part_states", "=", "self", ".", "score_partial", "(", "hyp", ",", "part_ids", ",", "x", ")", "\n", "for", "k", "in", "self", ".", "part_scorers", ":", "\n", "                ", "weighted_scores", "[", "part_ids", "]", "+=", "self", ".", "weights", "[", "k", "]", "*", "part_scores", "[", "k", "]", "\n", "# add previous hyp score", "\n", "", "weighted_scores", "+=", "hyp", ".", "score", "\n", "\n", "# update hyps", "\n", "for", "j", ",", "part_j", "in", "zip", "(", "*", "self", ".", "beam", "(", "weighted_scores", ",", "part_ids", ")", ")", ":", "\n", "# will be (2 x beam at most)", "\n", "                ", "best_hyps", ".", "append", "(", "\n", "Hypothesis", "(", "\n", "score", "=", "weighted_scores", "[", "j", "]", ",", "\n", "yseq", "=", "self", ".", "append_token", "(", "hyp", ".", "yseq", ",", "j", ")", ",", "\n", "scores", "=", "self", ".", "merge_scores", "(", "\n", "hyp", ".", "scores", ",", "scores", ",", "j", ",", "part_scores", ",", "part_j", "\n", ")", ",", "\n", "states", "=", "self", ".", "merge_states", "(", "states", ",", "part_states", ",", "part_j", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "# sort and prune 2 x beam -> beam", "\n", "", "best_hyps", "=", "sorted", "(", "best_hyps", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "[", "\n", ":", "min", "(", "len", "(", "best_hyps", ")", ",", "self", ".", "beam_size", ")", "\n", "]", "\n", "", "return", "best_hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.forward": [[336, 413], ["int", "logging.info", "logging.info", "logging.info", "beam_search.BeamSearch.init_hyp", "range", "sorted", "beam_search.BeamSearch.scores.items", "logging.info", "logging.info", "logging.info", "logging.debug", "beam_search.BeamSearch.search", "beam_search.BeamSearch.post_process", "len", "logging.warning", "logging.info", "logging.info", "max", "x.size", "str", "str", "str", "espnet.nets.e2e_asr_common.end_detect", "logging.info", "len", "logging.info", "logging.debug", "beam_search.BeamSearch.forward", "int", "int", "str", "max", "len", "h.asdict", "len", "x.size", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.init_hyp", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.search", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.post_process", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.end_detect", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.forward", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.Hypothesis.asdict"], ["", "def", "forward", "(", "\n", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "maxlenratio", ":", "float", "=", "0.0", ",", "minlenratio", ":", "float", "=", "0.0", "\n", ")", "->", "List", "[", "Hypothesis", "]", ":", "\n", "        ", "\"\"\"Perform beam search.\n\n        Args:\n            x (torch.Tensor): Encoded speech feature (T, D)\n            maxlenratio (float): Input length ratio to obtain max output length.\n                If maxlenratio=0.0 (default), it uses a end-detect function\n                to automatically find maximum hypothesis lengths\n                If maxlenratio<0.0, its absolute value is interpreted\n                as a constant max output length.\n            minlenratio (float): Input length ratio to obtain min output length.\n\n        Returns:\n            list[Hypothesis]: N-best decoding results\n\n        \"\"\"", "\n", "# set length bounds", "\n", "if", "maxlenratio", "==", "0", ":", "\n", "            ", "maxlen", "=", "x", ".", "shape", "[", "0", "]", "\n", "", "elif", "maxlenratio", "<", "0", ":", "\n", "            ", "maxlen", "=", "-", "1", "*", "int", "(", "maxlenratio", ")", "\n", "", "else", ":", "\n", "            ", "maxlen", "=", "max", "(", "1", ",", "int", "(", "maxlenratio", "*", "x", ".", "size", "(", "0", ")", ")", ")", "\n", "", "minlen", "=", "int", "(", "minlenratio", "*", "x", ".", "size", "(", "0", ")", ")", "\n", "logging", ".", "info", "(", "\"decoder input length: \"", "+", "str", "(", "x", ".", "shape", "[", "0", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"max output length: \"", "+", "str", "(", "maxlen", ")", ")", "\n", "logging", ".", "info", "(", "\"min output length: \"", "+", "str", "(", "minlen", ")", ")", "\n", "\n", "# main loop of prefix search", "\n", "running_hyps", "=", "self", ".", "init_hyp", "(", "x", ")", "\n", "ended_hyps", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "maxlen", ")", ":", "\n", "            ", "logging", ".", "debug", "(", "\"position \"", "+", "str", "(", "i", ")", ")", "\n", "best", "=", "self", ".", "search", "(", "running_hyps", ",", "x", ")", "\n", "# post process of one iteration", "\n", "running_hyps", "=", "self", ".", "post_process", "(", "i", ",", "maxlen", ",", "maxlenratio", ",", "best", ",", "ended_hyps", ")", "\n", "# end detection", "\n", "if", "maxlenratio", "==", "0.0", "and", "end_detect", "(", "[", "h", ".", "asdict", "(", ")", "for", "h", "in", "ended_hyps", "]", ",", "i", ")", ":", "\n", "                ", "logging", ".", "info", "(", "f\"end detected at {i}\"", ")", "\n", "break", "\n", "", "if", "len", "(", "running_hyps", ")", "==", "0", ":", "\n", "                ", "logging", ".", "info", "(", "\"no hypothesis. Finish decoding.\"", ")", "\n", "break", "\n", "", "else", ":", "\n", "                ", "logging", ".", "debug", "(", "f\"remained hypotheses: {len(running_hyps)}\"", ")", "\n", "\n", "", "", "nbest_hyps", "=", "sorted", "(", "ended_hyps", ",", "key", "=", "lambda", "x", ":", "x", ".", "score", ",", "reverse", "=", "True", ")", "\n", "# check the number of hypotheses reaching to eos", "\n", "if", "len", "(", "nbest_hyps", ")", "==", "0", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"there is no N-best results, perform recognition \"", "\n", "\"again with smaller minlenratio.\"", "\n", ")", "\n", "return", "(", "\n", "[", "]", "\n", "if", "minlenratio", "<", "0.1", "\n", "else", "self", ".", "forward", "(", "x", ",", "maxlenratio", ",", "max", "(", "0.0", ",", "minlenratio", "-", "0.1", ")", ")", "\n", ")", "\n", "\n", "# report the best result", "\n", "", "best", "=", "nbest_hyps", "[", "0", "]", "\n", "for", "k", ",", "v", "in", "best", ".", "scores", ".", "items", "(", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "f\"{v:6.2f} * {self.weights[k]:3} = {v * self.weights[k]:6.2f} for {k}\"", "\n", ")", "\n", "", "logging", ".", "info", "(", "f\"total log probability: {best.score:.2f}\"", ")", "\n", "logging", ".", "info", "(", "f\"normalized log probability: {best.score / len(best.yseq):.2f}\"", ")", "\n", "logging", ".", "info", "(", "f\"total number of ended hypotheses: {len(nbest_hyps)}\"", ")", "\n", "if", "self", ".", "token_list", "is", "not", "None", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "\"best hypo: \"", "\n", "+", "\"\"", ".", "join", "(", "[", "self", ".", "token_list", "[", "x", "]", "for", "x", "in", "best", ".", "yseq", "[", "1", ":", "-", "1", "]", "]", ")", "\n", "+", "\"\\n\"", "\n", ")", "\n", "", "return", "nbest_hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.post_process": [[414, 463], ["logging.debug", "logging.debug", "logging.info", "h._replace", "itertools.chain", "ended_hyps.append", "remained_hyps.append", "len", "beam_search.BeamSearch.full_scorers.items", "beam_search.BeamSearch.part_scorers.items", "d.final_score", "hyp._replace._replace._replace", "beam_search.BeamSearch.append_token"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.final_score", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.append_token"], ["", "def", "post_process", "(", "\n", "self", ",", "\n", "i", ":", "int", ",", "\n", "maxlen", ":", "int", ",", "\n", "maxlenratio", ":", "float", ",", "\n", "running_hyps", ":", "List", "[", "Hypothesis", "]", ",", "\n", "ended_hyps", ":", "List", "[", "Hypothesis", "]", ",", "\n", ")", "->", "List", "[", "Hypothesis", "]", ":", "\n", "        ", "\"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (List[Hypothesis]): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            List[Hypothesis]: The new running hypotheses.\n\n        \"\"\"", "\n", "logging", ".", "debug", "(", "f\"the number of running hypotheses: {len(running_hyps)}\"", ")", "\n", "if", "self", ".", "token_list", "is", "not", "None", ":", "\n", "            ", "logging", ".", "debug", "(", "\n", "\"best hypo: \"", "\n", "+", "\"\"", ".", "join", "(", "[", "self", ".", "token_list", "[", "x", "]", "for", "x", "in", "running_hyps", "[", "0", "]", ".", "yseq", "[", "1", ":", "]", "]", ")", "\n", ")", "\n", "# add eos in the final loop to avoid that there are no ended hyps", "\n", "", "if", "i", "==", "maxlen", "-", "1", ":", "\n", "            ", "logging", ".", "info", "(", "\"adding <eos> in the last position in the loop\"", ")", "\n", "running_hyps", "=", "[", "\n", "h", ".", "_replace", "(", "yseq", "=", "self", ".", "append_token", "(", "h", ".", "yseq", ",", "self", ".", "eos", ")", ")", "\n", "for", "h", "in", "running_hyps", "\n", "]", "\n", "\n", "# add ended hypotheses to a final list, and removed them from current hypotheses", "\n", "# (this will be a problem, number of hyps < beam)", "\n", "", "remained_hyps", "=", "[", "]", "\n", "for", "hyp", "in", "running_hyps", ":", "\n", "            ", "if", "hyp", ".", "yseq", "[", "-", "1", "]", "==", "self", ".", "eos", ":", "\n", "# e.g., Word LM needs to add final <eos> score", "\n", "                ", "for", "k", ",", "d", "in", "chain", "(", "self", ".", "full_scorers", ".", "items", "(", ")", ",", "self", ".", "part_scorers", ".", "items", "(", ")", ")", ":", "\n", "                    ", "s", "=", "d", ".", "final_score", "(", "hyp", ".", "states", "[", "k", "]", ")", "\n", "hyp", ".", "scores", "[", "k", "]", "+=", "s", "\n", "hyp", "=", "hyp", ".", "_replace", "(", "score", "=", "hyp", ".", "score", "+", "self", ".", "weights", "[", "k", "]", "*", "s", ")", "\n", "", "ended_hyps", ".", "append", "(", "hyp", ")", "\n", "", "else", ":", "\n", "                ", "remained_hyps", ".", "append", "(", "hyp", ")", "\n", "", "", "return", "remained_hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.beam_search": [[465, 517], ["beam_search.BeamSearch.forward", "h.asdict", "beam_search.BeamSearch"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.forward", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.Hypothesis.asdict"], ["", "", "def", "beam_search", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "sos", ":", "int", ",", "\n", "eos", ":", "int", ",", "\n", "beam_size", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "scorers", ":", "Dict", "[", "str", ",", "ScorerInterface", "]", ",", "\n", "weights", ":", "Dict", "[", "str", ",", "float", "]", ",", "\n", "token_list", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "maxlenratio", ":", "float", "=", "0.0", ",", "\n", "minlenratio", ":", "float", "=", "0.0", ",", "\n", "pre_beam_ratio", ":", "float", "=", "1.5", ",", "\n", "pre_beam_score_key", ":", "str", "=", "\"full\"", ",", "\n", ")", "->", "list", ":", "\n", "    ", "\"\"\"Perform beam search with scorers.\n\n    Args:\n        x (torch.Tensor): Encoded speech feature (T, D)\n        sos (int): Start of sequence id\n        eos (int): End of sequence id\n        beam_size (int): The number of hypotheses kept during search\n        vocab_size (int): The number of vocabulary\n        scorers (dict[str, ScorerInterface]): Dict of decoder modules\n            e.g., Decoder, CTCPrefixScorer, LM\n            The scorer will be ignored if it is `None`\n        weights (dict[str, float]): Dict of weights for each scorers\n            The scorer will be ignored if its weight is 0\n        token_list (list[str]): List of tokens for debug log\n        maxlenratio (float): Input length ratio to obtain max output length.\n            If maxlenratio=0.0 (default), it uses a end-detect function\n            to automatically find maximum hypothesis lengths\n        minlenratio (float): Input length ratio to obtain min output length.\n        pre_beam_score_key (str): key of scores to perform pre-beam search\n        pre_beam_ratio (float): beam size in the pre-beam search\n            will be `int(pre_beam_ratio * beam_size)`\n\n    Returns:\n        list: N-best decoding results\n\n    \"\"\"", "\n", "ret", "=", "BeamSearch", "(", "\n", "scorers", ",", "\n", "weights", ",", "\n", "beam_size", "=", "beam_size", ",", "\n", "vocab_size", "=", "vocab_size", ",", "\n", "pre_beam_ratio", "=", "pre_beam_ratio", ",", "\n", "pre_beam_score_key", "=", "pre_beam_score_key", ",", "\n", "sos", "=", "sos", ",", "\n", "eos", "=", "eos", ",", "\n", "token_list", "=", "token_list", ",", "\n", ")", ".", "forward", "(", "x", "=", "x", ",", "maxlenratio", "=", "maxlenratio", ",", "minlenratio", "=", "minlenratio", ")", "\n", "return", "[", "h", ".", "asdict", "(", ")", "for", "h", "in", "ret", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.ScorerInterface.init_state": [[29, 39], ["None"], "methods", ["None"], ["def", "init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "Any", ":", "\n", "        ", "\"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.ScorerInterface.select_state": [[40, 53], ["None"], "methods", ["None"], ["", "def", "select_state", "(", "self", ",", "state", ":", "Any", ",", "i", ":", "int", ",", "new_id", ":", "int", "=", "None", ")", "->", "Any", ":", "\n", "        ", "\"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label index to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"", "\n", "return", "None", "if", "state", "is", "None", "else", "state", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.ScorerInterface.score": [[54, 71], ["None"], "methods", ["None"], ["", "def", "score", "(", "\n", "self", ",", "y", ":", "torch", ".", "Tensor", ",", "state", ":", "Any", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Any", "]", ":", "\n", "        ", "\"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                scores for next token that has a shape of `(n_vocab)`\n                and next state for ys\n\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.ScorerInterface.final_score": [[72, 83], ["None"], "methods", ["None"], ["", "def", "final_score", "(", "self", ",", "state", ":", "Any", ")", "->", "float", ":", "\n", "        ", "\"\"\"Score eos (optional).\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        \"\"\"", "\n", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.BatchScorerInterface.batch_init_state": [[88, 98], ["scorer_interface.BatchScorerInterface.init_state"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.init_state"], ["def", "batch_init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "Any", ":", "\n", "        ", "\"\"\"Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "return", "self", ".", "init_state", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.BatchScorerInterface.batch_score": [[99, 129], ["warnings.warn", "list", "list", "enumerate", "torch.cat().view", "zip", "scorer_interface.BatchScorerInterface.score", "list.append", "torch.cat().view.append", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.score"], ["", "def", "batch_score", "(", "\n", "self", ",", "ys", ":", "torch", ".", "Tensor", ",", "states", ":", "List", "[", "Any", "]", ",", "xs", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "List", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"", "\n", "warnings", ".", "warn", "(", "\n", "\"{} batch score is implemented through for loop not parallelized\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", "\n", ")", "\n", ")", "\n", "scores", "=", "list", "(", ")", "\n", "outstates", "=", "list", "(", ")", "\n", "for", "i", ",", "(", "y", ",", "state", ",", "x", ")", "in", "enumerate", "(", "zip", "(", "ys", ",", "states", ",", "xs", ")", ")", ":", "\n", "            ", "score", ",", "outstate", "=", "self", ".", "score", "(", "y", ",", "state", ",", "x", ")", "\n", "outstates", ".", "append", "(", "outstate", ")", "\n", "scores", ".", "append", "(", "score", ")", "\n", "", "scores", "=", "torch", ".", "cat", "(", "scores", ",", "0", ")", ".", "view", "(", "ys", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "return", "scores", ",", "outstates", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.PartialScorerInterface.score_partial": [[144, 162], ["None"], "methods", ["None"], ["def", "score_partial", "(", "\n", "self", ",", "y", ":", "torch", ".", "Tensor", ",", "next_tokens", ":", "torch", ".", "Tensor", ",", "state", ":", "Any", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Any", "]", ":", "\n", "        ", "\"\"\"Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.scorer_interface.BatchPartialScorerInterface.batch_score_partial": [[167, 189], ["None"], "methods", ["None"], ["def", "batch_score_partial", "(", "\n", "self", ",", "\n", "ys", ":", "torch", ".", "Tensor", ",", "\n", "next_tokens", ":", "torch", ".", "Tensor", ",", "\n", "states", ":", "List", "[", "Any", "]", ",", "\n", "xs", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Any", "]", ":", "\n", "        ", "\"\"\"Score new token (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            next_tokens (torch.Tensor): torch.int64 tokens to score (n_batch, n_token).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for ys that has a shape `(n_batch, n_vocab)`\n                and next states for ys\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchHypothesis.__len__": [[26, 29], ["len"], "methods", ["None"], ["def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"Return a batch size.\"\"\"", "\n", "return", "len", "(", "self", ".", "length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.batchfy": [[34, 46], ["batch_beam_search.BatchHypothesis", "len", "batch_beam_search.BatchHypothesis", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["None"], ["def", "batchfy", "(", "self", ",", "hyps", ":", "List", "[", "Hypothesis", "]", ")", "->", "BatchHypothesis", ":", "\n", "        ", "\"\"\"Convert list to batch.\"\"\"", "\n", "if", "len", "(", "hyps", ")", "==", "0", ":", "\n", "            ", "return", "BatchHypothesis", "(", ")", "\n", "", "return", "BatchHypothesis", "(", "\n", "yseq", "=", "pad_sequence", "(", "\n", "[", "h", ".", "yseq", "for", "h", "in", "hyps", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "self", ".", "eos", "\n", ")", ",", "\n", "length", "=", "torch", ".", "tensor", "(", "[", "len", "(", "h", ".", "yseq", ")", "for", "h", "in", "hyps", "]", ",", "dtype", "=", "torch", ".", "int64", ")", ",", "\n", "score", "=", "torch", ".", "tensor", "(", "[", "h", ".", "score", "for", "h", "in", "hyps", "]", ")", ",", "\n", "scores", "=", "{", "k", ":", "torch", ".", "tensor", "(", "[", "h", ".", "scores", "[", "k", "]", "for", "h", "in", "hyps", "]", ")", "for", "k", "in", "self", ".", "scorers", "}", ",", "\n", "states", "=", "{", "k", ":", "[", "h", ".", "states", "[", "k", "]", "for", "h", "in", "hyps", "]", "for", "k", "in", "self", ".", "scorers", "}", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch._batch_select": [[48, 57], ["batch_beam_search.BatchHypothesis", "hyps.scores.items", "batch_beam_search.BatchBeamSearch.scorers[].select_state", "hyps.states.items"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state"], ["", "def", "_batch_select", "(", "self", ",", "hyps", ":", "BatchHypothesis", ",", "ids", ":", "List", "[", "int", "]", ")", "->", "BatchHypothesis", ":", "\n", "        ", "return", "BatchHypothesis", "(", "\n", "yseq", "=", "hyps", ".", "yseq", "[", "ids", "]", ",", "\n", "score", "=", "hyps", ".", "score", "[", "ids", "]", ",", "\n", "length", "=", "hyps", ".", "length", "[", "ids", "]", ",", "\n", "scores", "=", "{", "k", ":", "v", "[", "ids", "]", "for", "k", ",", "v", "in", "hyps", ".", "scores", ".", "items", "(", ")", "}", ",", "\n", "states", "=", "{", "\n", "k", ":", "[", "self", ".", "scorers", "[", "k", "]", ".", "select_state", "(", "v", ",", "i", ")", "for", "i", "in", "ids", "]", "\n", "for", "k", ",", "v", "in", "hyps", ".", "states", ".", "items", "(", ")", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch._select": [[60, 67], ["espnet.nets.beam_search.Hypothesis", "batch_beam_search.BatchBeamSearch.scorers[].select_state", "hyps.scores.items", "hyps.states.items"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state"], ["", "def", "_select", "(", "self", ",", "hyps", ":", "BatchHypothesis", ",", "i", ":", "int", ")", "->", "Hypothesis", ":", "\n", "        ", "return", "Hypothesis", "(", "\n", "yseq", "=", "hyps", ".", "yseq", "[", "i", ",", ":", "hyps", ".", "length", "[", "i", "]", "]", ",", "\n", "score", "=", "hyps", ".", "score", "[", "i", "]", ",", "\n", "scores", "=", "{", "k", ":", "v", "[", "i", "]", "for", "k", ",", "v", "in", "hyps", ".", "scores", ".", "items", "(", ")", "}", ",", "\n", "states", "=", "{", "\n", "k", ":", "self", ".", "scorers", "[", "k", "]", ".", "select_state", "(", "v", ",", "i", ")", "for", "k", ",", "v", "in", "hyps", ".", "states", ".", "items", "(", ")", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.unbatchfy": [[70, 83], ["espnet.nets.beam_search.Hypothesis", "range", "len", "v.select_state", "batch_beam_search.BatchBeamSearch.scorers.items"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state"], ["", "def", "unbatchfy", "(", "self", ",", "batch_hyps", ":", "BatchHypothesis", ")", "->", "List", "[", "Hypothesis", "]", ":", "\n", "        ", "\"\"\"Revert batch to list.\"\"\"", "\n", "return", "[", "\n", "Hypothesis", "(", "\n", "yseq", "=", "batch_hyps", ".", "yseq", "[", "i", "]", "[", ":", "batch_hyps", ".", "length", "[", "i", "]", "]", ",", "\n", "score", "=", "batch_hyps", ".", "score", "[", "i", "]", ",", "\n", "scores", "=", "{", "k", ":", "batch_hyps", ".", "scores", "[", "k", "]", "[", "i", "]", "for", "k", "in", "self", ".", "scorers", "}", ",", "\n", "states", "=", "{", "\n", "k", ":", "v", ".", "select_state", "(", "batch_hyps", ".", "states", "[", "k", "]", ",", "i", ")", "\n", "for", "k", ",", "v", "in", "self", ".", "scorers", ".", "items", "(", ")", "\n", "}", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch_hyps", ".", "length", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.batch_beam": [[85, 110], ["torch.div", "weighted_scores.view().topk", "weighted_scores.view"], "methods", ["None"], ["", "def", "batch_beam", "(", "\n", "self", ",", "weighted_scores", ":", "torch", ".", "Tensor", ",", "ids", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"Batch-compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n                Its shape is `(n_beam, self.vocab_size)`.\n            ids (torch.Tensor): The partial token ids to compute topk.\n                Its shape is `(n_beam, self.pre_beam_size)`.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n                The topk full (prev_hyp, new_token) ids\n                and partial (prev_hyp, new_token) ids.\n                Their shapes are all `(self.beam_size,)`\n\n        \"\"\"", "\n", "top_ids", "=", "weighted_scores", ".", "view", "(", "-", "1", ")", ".", "topk", "(", "self", ".", "beam_size", ")", "[", "1", "]", "\n", "# Because of the flatten above, `top_ids` is organized as:", "\n", "# [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK],", "\n", "# where V is `self.n_vocab` and K is `self.beam_size`", "\n", "prev_hyp_ids", "=", "torch", ".", "div", "(", "top_ids", ",", "self", ".", "n_vocab", ",", "rounding_mode", "=", "'trunc'", ")", "\n", "new_token_ids", "=", "top_ids", "%", "self", ".", "n_vocab", "\n", "return", "prev_hyp_ids", ",", "new_token_ids", ",", "prev_hyp_ids", ",", "new_token_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.init_hyp": [[111, 133], ["dict", "dict", "batch_beam_search.BatchBeamSearch.scorers.items", "batch_beam_search.BatchBeamSearch.batchfy", "d.batch_init_state", "espnet.nets.beam_search.Hypothesis", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.batchfy", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.batch_init_state"], ["", "def", "init_hyp", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "BatchHypothesis", ":", "\n", "        ", "\"\"\"Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        \"\"\"", "\n", "init_states", "=", "dict", "(", ")", "\n", "init_scores", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "scorers", ".", "items", "(", ")", ":", "\n", "            ", "init_states", "[", "k", "]", "=", "d", ".", "batch_init_state", "(", "x", ")", "\n", "init_scores", "[", "k", "]", "=", "0.0", "\n", "", "return", "self", ".", "batchfy", "(", "\n", "[", "\n", "Hypothesis", "(", "\n", "score", "=", "0.0", ",", "\n", "scores", "=", "init_scores", ",", "\n", "states", "=", "init_states", ",", "\n", "yseq", "=", "torch", ".", "tensor", "(", "[", "self", ".", "sos", "]", ",", "device", "=", "x", ".", "device", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.score_full": [[137, 159], ["dict", "dict", "batch_beam_search.BatchBeamSearch.full_scorers.items", "d.batch_score"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.batch_score"], ["", "def", "score_full", "(", "\n", "self", ",", "hyp", ":", "BatchHypothesis", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"", "\n", "scores", "=", "dict", "(", ")", "\n", "states", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "full_scorers", ".", "items", "(", ")", ":", "\n", "            ", "scores", "[", "k", "]", ",", "states", "[", "k", "]", "=", "d", ".", "batch_score", "(", "hyp", ".", "yseq", ",", "hyp", ".", "states", "[", "k", "]", ",", "x", ")", "\n", "", "return", "scores", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.score_partial": [[160, 185], ["dict", "dict", "batch_beam_search.BatchBeamSearch.part_scorers.items", "d.batch_score_partial"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.batch_score_partial"], ["", "def", "score_partial", "(", "\n", "self", ",", "hyp", ":", "BatchHypothesis", ",", "ids", ":", "torch", ".", "Tensor", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 2D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        \"\"\"", "\n", "scores", "=", "dict", "(", ")", "\n", "states", "=", "dict", "(", ")", "\n", "for", "k", ",", "d", "in", "self", ".", "part_scorers", ".", "items", "(", ")", ":", "\n", "            ", "scores", "[", "k", "]", ",", "states", "[", "k", "]", "=", "d", ".", "batch_score_partial", "(", "\n", "hyp", ".", "yseq", ",", "ids", ",", "hyp", ".", "states", "[", "k", "]", ",", "x", "\n", ")", "\n", "", "return", "scores", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.merge_states": [[186, 206], ["dict", "states.items", "part_states.items"], "methods", ["None"], ["", "def", "merge_states", "(", "self", ",", "states", ":", "Any", ",", "part_states", ":", "Any", ",", "part_idx", ":", "int", ")", "->", "Any", ":", "\n", "        ", "\"\"\"Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        \"\"\"", "\n", "new_states", "=", "dict", "(", ")", "\n", "for", "k", ",", "v", "in", "states", ".", "items", "(", ")", ":", "\n", "            ", "new_states", "[", "k", "]", "=", "v", "\n", "", "for", "k", ",", "v", "in", "part_states", ".", "items", "(", ")", ":", "\n", "            ", "new_states", "[", "k", "]", "=", "v", "\n", "", "return", "new_states", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.search": [[207, 285], ["len", "torch.zeros", "batch_beam_search.BatchBeamSearch.score_full", "batch_beam_search.BatchBeamSearch.score_partial", "running_hyps.score.to().unsqueeze", "batch_beam_search.BatchBeamSearch.unbatchfy", "zip", "batch_beam_search.BatchBeamSearch.batchfy", "x.expand", "best_hyps.append", "torch.topk", "running_hyps.score.to", "batch_beam_search.BatchBeamSearch.batch_beam", "espnet.nets.beam_search.Hypothesis", "batch_beam_search.BatchBeamSearch.append_token", "batch_beam_search.BatchBeamSearch.merge_scores", "batch_beam_search.BatchBeamSearch.merge_states", "batch_beam_search.BatchBeamSearch.full_scorers[].select_state", "batch_beam_search.BatchBeamSearch.part_scorers[].select_state", "scores.items", "part_scores.items", "states.items", "part_states.items"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.score_full", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.score_partial", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.unbatchfy", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.batchfy", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.batch_beam", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.append_token", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.beam_search.BeamSearch.merge_scores", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.merge_states", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state"], ["", "def", "search", "(", "self", ",", "running_hyps", ":", "BatchHypothesis", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "BatchHypothesis", ":", "\n", "        ", "\"\"\"Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (BatchHypothesis): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            BatchHypothesis: Best sorted hypotheses\n\n        \"\"\"", "\n", "n_batch", "=", "len", "(", "running_hyps", ")", "\n", "part_ids", "=", "None", "# no pre-beam", "\n", "# batch scoring", "\n", "weighted_scores", "=", "torch", ".", "zeros", "(", "\n", "n_batch", ",", "self", ".", "n_vocab", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", "\n", ")", "\n", "scores", ",", "states", "=", "self", ".", "score_full", "(", "running_hyps", ",", "x", ".", "expand", "(", "n_batch", ",", "*", "x", ".", "shape", ")", ")", "\n", "for", "k", "in", "self", ".", "full_scorers", ":", "\n", "            ", "weighted_scores", "+=", "self", ".", "weights", "[", "k", "]", "*", "scores", "[", "k", "]", "\n", "# partial scoring", "\n", "", "if", "self", ".", "do_pre_beam", ":", "\n", "            ", "pre_beam_scores", "=", "(", "\n", "weighted_scores", "\n", "if", "self", ".", "pre_beam_score_key", "==", "\"full\"", "\n", "else", "scores", "[", "self", ".", "pre_beam_score_key", "]", "\n", ")", "\n", "part_ids", "=", "torch", ".", "topk", "(", "pre_beam_scores", ",", "self", ".", "pre_beam_size", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "\n", "# NOTE(takaaki-hori): Unlike BeamSearch, we assume that score_partial returns", "\n", "# full-size score matrices, which has non-zero scores for part_ids and zeros", "\n", "# for others.", "\n", "", "part_scores", ",", "part_states", "=", "self", ".", "score_partial", "(", "running_hyps", ",", "part_ids", ",", "x", ")", "\n", "for", "k", "in", "self", ".", "part_scorers", ":", "\n", "            ", "weighted_scores", "+=", "self", ".", "weights", "[", "k", "]", "*", "part_scores", "[", "k", "]", "\n", "# add previous hyp scores", "\n", "", "weighted_scores", "+=", "running_hyps", ".", "score", ".", "to", "(", "\n", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# TODO(karita): do not use list. use batch instead", "\n", "# see also https://github.com/espnet/espnet/pull/1402#discussion_r354561029", "\n", "# update hyps", "\n", "best_hyps", "=", "[", "]", "\n", "prev_hyps", "=", "self", ".", "unbatchfy", "(", "running_hyps", ")", "\n", "for", "(", "\n", "full_prev_hyp_id", ",", "\n", "full_new_token_id", ",", "\n", "part_prev_hyp_id", ",", "\n", "part_new_token_id", ",", "\n", ")", "in", "zip", "(", "*", "self", ".", "batch_beam", "(", "weighted_scores", ",", "part_ids", ")", ")", ":", "\n", "            ", "prev_hyp", "=", "prev_hyps", "[", "full_prev_hyp_id", "]", "\n", "best_hyps", ".", "append", "(", "\n", "Hypothesis", "(", "\n", "score", "=", "weighted_scores", "[", "full_prev_hyp_id", ",", "full_new_token_id", "]", ",", "\n", "yseq", "=", "self", ".", "append_token", "(", "prev_hyp", ".", "yseq", ",", "full_new_token_id", ")", ",", "\n", "scores", "=", "self", ".", "merge_scores", "(", "\n", "prev_hyp", ".", "scores", ",", "\n", "{", "k", ":", "v", "[", "full_prev_hyp_id", "]", "for", "k", ",", "v", "in", "scores", ".", "items", "(", ")", "}", ",", "\n", "full_new_token_id", ",", "\n", "{", "k", ":", "v", "[", "part_prev_hyp_id", "]", "for", "k", ",", "v", "in", "part_scores", ".", "items", "(", ")", "}", ",", "\n", "part_new_token_id", ",", "\n", ")", ",", "\n", "states", "=", "self", ".", "merge_states", "(", "\n", "{", "\n", "k", ":", "self", ".", "full_scorers", "[", "k", "]", ".", "select_state", "(", "v", ",", "full_prev_hyp_id", ")", "\n", "for", "k", ",", "v", "in", "states", ".", "items", "(", ")", "\n", "}", ",", "\n", "{", "\n", "k", ":", "self", ".", "part_scorers", "[", "k", "]", ".", "select_state", "(", "\n", "v", ",", "part_prev_hyp_id", ",", "part_new_token_id", "\n", ")", "\n", "for", "k", ",", "v", "in", "part_states", ".", "items", "(", ")", "\n", "}", ",", "\n", "part_new_token_id", ",", "\n", ")", ",", "\n", ")", "\n", ")", "\n", "", "return", "self", ".", "batchfy", "(", "best_hyps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch.post_process": [[286, 349], ["logging.debug", "torch.nonzero().view", "torch.nonzero().view", "batch_beam_search.BatchBeamSearch._batch_select", "logging.debug", "logging.info", "torch.cat", "running_hyps.yseq.resize_as_", "batch_beam_search.BatchBeamSearch._select", "ended_hyps.append", "torch.nonzero", "torch.nonzero", "torch.full", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch._batch_select", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.batch_beam_search.BatchBeamSearch._select"], ["", "def", "post_process", "(", "\n", "self", ",", "\n", "i", ":", "int", ",", "\n", "maxlen", ":", "int", ",", "\n", "maxlenratio", ":", "float", ",", "\n", "running_hyps", ":", "BatchHypothesis", ",", "\n", "ended_hyps", ":", "List", "[", "Hypothesis", "]", ",", "\n", ")", "->", "BatchHypothesis", ":", "\n", "        ", "\"\"\"Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (BatchHypothesis): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            BatchHypothesis: The new running hypotheses.\n\n        \"\"\"", "\n", "n_batch", "=", "running_hyps", ".", "yseq", ".", "shape", "[", "0", "]", "\n", "logging", ".", "debug", "(", "f\"the number of running hypothes: {n_batch}\"", ")", "\n", "if", "self", ".", "token_list", "is", "not", "None", ":", "\n", "            ", "logging", ".", "debug", "(", "\n", "\"best hypo: \"", "\n", "+", "\"\"", ".", "join", "(", "\n", "[", "\n", "self", ".", "token_list", "[", "x", "]", "\n", "for", "x", "in", "running_hyps", ".", "yseq", "[", "0", ",", "1", ":", "running_hyps", ".", "length", "[", "0", "]", "]", "\n", "]", "\n", ")", "\n", ")", "\n", "# add eos in the final loop to avoid that there are no ended hyps", "\n", "", "if", "i", "==", "maxlen", "-", "1", ":", "\n", "            ", "logging", ".", "info", "(", "\"adding <eos> in the last position in the loop\"", ")", "\n", "yseq_eos", "=", "torch", ".", "cat", "(", "\n", "(", "\n", "running_hyps", ".", "yseq", ",", "\n", "torch", ".", "full", "(", "\n", "(", "n_batch", ",", "1", ")", ",", "\n", "self", ".", "eos", ",", "\n", "device", "=", "running_hyps", ".", "yseq", ".", "device", ",", "\n", "dtype", "=", "torch", ".", "int64", ",", "\n", ")", ",", "\n", ")", ",", "\n", "1", ",", "\n", ")", "\n", "running_hyps", ".", "yseq", ".", "resize_as_", "(", "yseq_eos", ")", "\n", "running_hyps", ".", "yseq", "[", ":", "]", "=", "yseq_eos", "\n", "running_hyps", ".", "length", "[", ":", "]", "=", "yseq_eos", ".", "shape", "[", "1", "]", "\n", "\n", "# add ended hypotheses to a final list, and removed them from current hypotheses", "\n", "# (this will be a probmlem, number of hyps < beam)", "\n", "", "is_eos", "=", "(", "\n", "running_hyps", ".", "yseq", "[", "torch", ".", "arange", "(", "n_batch", ")", ",", "running_hyps", ".", "length", "-", "1", "]", "\n", "==", "self", ".", "eos", "\n", ")", "\n", "for", "b", "in", "torch", ".", "nonzero", "(", "is_eos", ",", "as_tuple", "=", "False", ")", ".", "view", "(", "-", "1", ")", ":", "\n", "            ", "hyp", "=", "self", ".", "_select", "(", "running_hyps", ",", "b", ")", "\n", "ended_hyps", ".", "append", "(", "hyp", ")", "\n", "", "remained_ids", "=", "torch", ".", "nonzero", "(", "is_eos", "==", "0", ",", "as_tuple", "=", "False", ")", ".", "view", "(", "-", "1", ")", "\n", "return", "self", ".", "_batch_select", "(", "running_hyps", ",", "remained_ids", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScoreTH.__init__": [[23, 68], ["x.size", "x.size", "x.size", "enumerate", "x.transpose", "xn[].unsqueeze().expand", "torch.stack", "torch.arange", "torch.device", "torch.device", "torch.as_tensor", "torch.arange", "xn[].unsqueeze", "x.get_device"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "x", ",", "xlens", ",", "blank", ",", "eos", ",", "margin", "=", "0", ")", ":", "\n", "        ", "\"\"\"Construct CTC prefix scorer\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        :param torch.Tensor xlens: input lengths (B,)\n        :param int blank: blank label id\n        :param int eos: end-of-sequence id\n        :param int margin: margin parameter for windowing (0 means no windowing)\n        \"\"\"", "\n", "# In the comment lines,", "\n", "# we assume T: input_length, B: batch size, W: beam width, O: output dim.", "\n", "self", ".", "logzero", "=", "-", "10000000000.0", "\n", "self", ".", "blank", "=", "blank", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "batch", "=", "x", ".", "size", "(", "0", ")", "\n", "self", ".", "input_length", "=", "x", ".", "size", "(", "1", ")", "\n", "self", ".", "odim", "=", "x", ".", "size", "(", "2", ")", "\n", "self", ".", "dtype", "=", "x", ".", "dtype", "\n", "self", ".", "device", "=", "(", "\n", "torch", ".", "device", "(", "\"cuda:%d\"", "%", "x", ".", "get_device", "(", ")", ")", "\n", "if", "x", ".", "is_cuda", "\n", "else", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", ")", "\n", "# Pad the rest of posteriors in the batch", "\n", "# TODO(takaaki-hori): need a better way without for-loops", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "xlens", ")", ":", "\n", "            ", "if", "l", "<", "self", ".", "input_length", ":", "\n", "                ", "x", "[", "i", ",", "l", ":", ",", ":", "]", "=", "self", ".", "logzero", "\n", "x", "[", "i", ",", "l", ":", ",", "blank", "]", "=", "0", "\n", "# Reshape input x", "\n", "", "", "xn", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# (B, T, O) -> (T, B, O)", "\n", "xb", "=", "xn", "[", ":", ",", ":", ",", "self", ".", "blank", "]", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "odim", ")", "\n", "self", ".", "x", "=", "torch", ".", "stack", "(", "[", "xn", ",", "xb", "]", ")", "# (2, T, B, O)", "\n", "self", ".", "end_frames", "=", "torch", ".", "as_tensor", "(", "xlens", ")", "-", "1", "\n", "\n", "# Setup CTC windowing", "\n", "self", ".", "margin", "=", "margin", "\n", "if", "margin", ">", "0", ":", "\n", "            ", "self", ".", "frame_ids", "=", "torch", ".", "arange", "(", "\n", "self", ".", "input_length", ",", "dtype", "=", "self", ".", "dtype", ",", "device", "=", "self", ".", "device", "\n", ")", "\n", "# Base indices for index conversion", "\n", "", "self", ".", "idx_bh", "=", "None", "\n", "self", ".", "idx_b", "=", "torch", ".", "arange", "(", "self", ".", "batch", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "idx_bo", "=", "(", "self", ".", "idx_b", "*", "self", ".", "odim", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScoreTH.__call__": [[69, 189], ["len", "torch.full", "torch.logsumexp", "torch.logsumexp.unsqueeze().repeat", "range", "range", "len", "scoring_ids.size", "torch.full", "torch.cumsum().unsqueeze", "r_prev.view.view.view", "torch.full", "torch.arange", "torch.index_select().view", "ctc_prefix_score.CTCPrefixScoreTH.x.unsqueeze().repeat().view", "range", "range", "torch.matmul", "max", "max", "min", "min", "max", "torch.stack().view", "torch.cat", "torch.full", "torch.logsumexp", "range", "torch.logsumexp", "torch.arange().view", "torch.logsumexp.unsqueeze", "int", "int", "max", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cumsum", "len", "torch.index_select", "ctc_prefix_score.CTCPrefixScoreTH.x.unsqueeze().repeat", "torch.matmul.min().cpu", "torch.matmul.max().cpu", "torch.stack", "log_phi[].unsqueeze", "torch.arange", "ctc_prefix_score.CTCPrefixScoreTH.idx_bo.repeat().view", "ctc_prefix_score.CTCPrefixScoreTH.x.view", "r[].unsqueeze", "r[].unsqueeze", "ctc_prefix_score.CTCPrefixScoreTH.x.unsqueeze", "torch.matmul.min", "torch.matmul.max", "ctc_prefix_score.CTCPrefixScoreTH.idx_bo.repeat"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat"], ["", "def", "__call__", "(", "self", ",", "y", ",", "state", ",", "scoring_ids", "=", "None", ",", "att_w", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute CTC prefix scores for next labels\n\n        :param list y: prefix label sequences\n        :param tuple state: previous CTC state\n        :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O)\n        :param torch.Tensor att_w: attention weights to decide CTC window\n        :return new_state, ctc_local_scores (BW, O)\n        \"\"\"", "\n", "output_length", "=", "len", "(", "y", "[", "0", "]", ")", "-", "1", "# ignore sos", "\n", "last_ids", "=", "[", "yi", "[", "-", "1", "]", "for", "yi", "in", "y", "]", "# last output label ids", "\n", "n_bh", "=", "len", "(", "last_ids", ")", "# batch * hyps", "\n", "n_hyps", "=", "n_bh", "//", "self", ".", "batch", "# assuming each utterance has the same # of hyps", "\n", "self", ".", "scoring_num", "=", "scoring_ids", ".", "size", "(", "-", "1", ")", "if", "scoring_ids", "is", "not", "None", "else", "0", "\n", "# prepare state info", "\n", "if", "state", "is", "None", ":", "\n", "            ", "r_prev", "=", "torch", ".", "full", "(", "\n", "(", "self", ".", "input_length", ",", "2", ",", "self", ".", "batch", ",", "n_hyps", ")", ",", "\n", "self", ".", "logzero", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "r_prev", "[", ":", ",", "1", "]", "=", "torch", ".", "cumsum", "(", "self", ".", "x", "[", "0", ",", ":", ",", ":", ",", "self", ".", "blank", "]", ",", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "r_prev", "=", "r_prev", ".", "view", "(", "-", "1", ",", "2", ",", "n_bh", ")", "\n", "s_prev", "=", "0.0", "\n", "f_min_prev", "=", "0", "\n", "f_max_prev", "=", "1", "\n", "", "else", ":", "\n", "            ", "r_prev", ",", "s_prev", ",", "f_min_prev", ",", "f_max_prev", "=", "state", "\n", "\n", "# select input dimensions for scoring", "\n", "", "if", "self", ".", "scoring_num", ">", "0", ":", "\n", "            ", "scoring_idmap", "=", "torch", ".", "full", "(", "\n", "(", "n_bh", ",", "self", ".", "odim", ")", ",", "-", "1", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", "\n", ")", "\n", "snum", "=", "self", ".", "scoring_num", "\n", "if", "self", ".", "idx_bh", "is", "None", "or", "n_bh", ">", "len", "(", "self", ".", "idx_bh", ")", ":", "\n", "                ", "self", ".", "idx_bh", "=", "torch", ".", "arange", "(", "n_bh", ",", "device", "=", "self", ".", "device", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "", "scoring_idmap", "[", "self", ".", "idx_bh", "[", ":", "n_bh", "]", ",", "scoring_ids", "]", "=", "torch", ".", "arange", "(", "\n", "snum", ",", "device", "=", "self", ".", "device", "\n", ")", "\n", "scoring_idx", "=", "(", "\n", "scoring_ids", "+", "self", ".", "idx_bo", ".", "repeat", "(", "1", ",", "n_hyps", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", ".", "view", "(", "-", "1", ")", "\n", "x_", "=", "torch", ".", "index_select", "(", "\n", "self", ".", "x", ".", "view", "(", "2", ",", "-", "1", ",", "self", ".", "batch", "*", "self", ".", "odim", ")", ",", "2", ",", "scoring_idx", "\n", ")", ".", "view", "(", "2", ",", "-", "1", ",", "n_bh", ",", "snum", ")", "\n", "", "else", ":", "\n", "            ", "scoring_ids", "=", "None", "\n", "scoring_idmap", "=", "None", "\n", "snum", "=", "self", ".", "odim", "\n", "x_", "=", "self", ".", "x", ".", "unsqueeze", "(", "3", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "n_hyps", ",", "1", ")", ".", "view", "(", "2", ",", "-", "1", ",", "n_bh", ",", "snum", ")", "\n", "\n", "# new CTC forward probs are prepared as a (T x 2 x BW x S) tensor", "\n", "# that corresponds to r_t^n(h) and r_t^b(h) in a batch.", "\n", "", "r", "=", "torch", ".", "full", "(", "\n", "(", "self", ".", "input_length", ",", "2", ",", "n_bh", ",", "snum", ")", ",", "\n", "self", ".", "logzero", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "if", "output_length", "==", "0", ":", "\n", "            ", "r", "[", "0", ",", "0", "]", "=", "x_", "[", "0", ",", "0", "]", "\n", "\n", "", "r_sum", "=", "torch", ".", "logsumexp", "(", "r_prev", ",", "1", ")", "\n", "log_phi", "=", "r_sum", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "snum", ")", "\n", "if", "scoring_ids", "is", "not", "None", ":", "\n", "            ", "for", "idx", "in", "range", "(", "n_bh", ")", ":", "\n", "                ", "pos", "=", "scoring_idmap", "[", "idx", ",", "last_ids", "[", "idx", "]", "]", "\n", "if", "pos", ">=", "0", ":", "\n", "                    ", "log_phi", "[", ":", ",", "idx", ",", "pos", "]", "=", "r_prev", "[", ":", ",", "1", ",", "idx", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "idx", "in", "range", "(", "n_bh", ")", ":", "\n", "                ", "log_phi", "[", ":", ",", "idx", ",", "last_ids", "[", "idx", "]", "]", "=", "r_prev", "[", ":", ",", "1", ",", "idx", "]", "\n", "\n", "# decide start and end frames based on attention weights", "\n", "", "", "if", "att_w", "is", "not", "None", "and", "self", ".", "margin", ">", "0", ":", "\n", "            ", "f_arg", "=", "torch", ".", "matmul", "(", "att_w", ",", "self", ".", "frame_ids", ")", "\n", "f_min", "=", "max", "(", "int", "(", "f_arg", ".", "min", "(", ")", ".", "cpu", "(", ")", ")", ",", "f_min_prev", ")", "\n", "f_max", "=", "max", "(", "int", "(", "f_arg", ".", "max", "(", ")", ".", "cpu", "(", ")", ")", ",", "f_max_prev", ")", "\n", "start", "=", "min", "(", "f_max_prev", ",", "max", "(", "f_min", "-", "self", ".", "margin", ",", "output_length", ",", "1", ")", ")", "\n", "end", "=", "min", "(", "f_max", "+", "self", ".", "margin", ",", "self", ".", "input_length", ")", "\n", "", "else", ":", "\n", "            ", "f_min", "=", "f_max", "=", "0", "\n", "start", "=", "max", "(", "output_length", ",", "1", ")", "\n", "end", "=", "self", ".", "input_length", "\n", "\n", "# compute forward probabilities log(r_t^n(h)) and log(r_t^b(h))", "\n", "", "for", "t", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "            ", "rp", "=", "r", "[", "t", "-", "1", "]", "\n", "rr", "=", "torch", ".", "stack", "(", "[", "rp", "[", "0", "]", ",", "log_phi", "[", "t", "-", "1", "]", ",", "rp", "[", "0", "]", ",", "rp", "[", "1", "]", "]", ")", ".", "view", "(", "\n", "2", ",", "2", ",", "n_bh", ",", "snum", "\n", ")", "\n", "r", "[", "t", "]", "=", "torch", ".", "logsumexp", "(", "rr", ",", "1", ")", "+", "x_", "[", ":", ",", "t", "]", "\n", "\n", "# compute log prefix probabilities log(psi)", "\n", "", "log_phi_x", "=", "torch", ".", "cat", "(", "(", "log_phi", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "log_phi", "[", ":", "-", "1", "]", ")", ",", "dim", "=", "0", ")", "+", "x_", "[", "0", "]", "\n", "if", "scoring_ids", "is", "not", "None", ":", "\n", "            ", "log_psi", "=", "torch", ".", "full", "(", "\n", "(", "n_bh", ",", "self", ".", "odim", ")", ",", "self", ".", "logzero", ",", "dtype", "=", "self", ".", "dtype", ",", "device", "=", "self", ".", "device", "\n", ")", "\n", "log_psi_", "=", "torch", ".", "logsumexp", "(", "\n", "torch", ".", "cat", "(", "(", "log_phi_x", "[", "start", ":", "end", "]", ",", "r", "[", "start", "-", "1", ",", "0", "]", ".", "unsqueeze", "(", "0", ")", ")", ",", "dim", "=", "0", ")", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "for", "si", "in", "range", "(", "n_bh", ")", ":", "\n", "                ", "log_psi", "[", "si", ",", "scoring_ids", "[", "si", "]", "]", "=", "log_psi_", "[", "si", "]", "\n", "", "", "else", ":", "\n", "            ", "log_psi", "=", "torch", ".", "logsumexp", "(", "\n", "torch", ".", "cat", "(", "(", "log_phi_x", "[", "start", ":", "end", "]", ",", "r", "[", "start", "-", "1", ",", "0", "]", ".", "unsqueeze", "(", "0", ")", ")", ",", "dim", "=", "0", ")", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "\n", "", "for", "si", "in", "range", "(", "n_bh", ")", ":", "\n", "            ", "log_psi", "[", "si", ",", "self", ".", "eos", "]", "=", "r_sum", "[", "self", ".", "end_frames", "[", "si", "//", "n_hyps", "]", ",", "si", "]", "\n", "\n", "# exclude blank probs", "\n", "", "log_psi", "[", ":", ",", "self", ".", "blank", "]", "=", "self", ".", "logzero", "\n", "\n", "return", "(", "log_psi", "-", "s_prev", ")", ",", "(", "r", ",", "log_psi", ",", "f_min", ",", "f_max", ",", "scoring_idmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScoreTH.index_select_state": [[190, 222], ["len", "torch.index_select", "s_new.view().repeat().view.view().repeat().view.view().repeat().view", "torch.index_select().view", "s.view", "torch.fmod().view", "s_new.view().repeat().view.view().repeat().view.view().repeat", "torch.index_select", "torch.fmod", "r.view", "s_new.view().repeat().view.view().repeat().view.view"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat"], ["", "def", "index_select_state", "(", "self", ",", "state", ",", "best_ids", ")", ":", "\n", "        ", "\"\"\"Select CTC states according to best ids\n\n        :param state    : CTC state\n        :param best_ids : index numbers selected by beam pruning (B, W)\n        :return selected_state\n        \"\"\"", "\n", "r", ",", "s", ",", "f_min", ",", "f_max", ",", "scoring_idmap", "=", "state", "\n", "# convert ids to BHO space", "\n", "n_bh", "=", "len", "(", "s", ")", "\n", "n_hyps", "=", "n_bh", "//", "self", ".", "batch", "\n", "vidx", "=", "(", "best_ids", "+", "(", "self", ".", "idx_b", "*", "(", "n_hyps", "*", "self", ".", "odim", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "# select hypothesis scores", "\n", "s_new", "=", "torch", ".", "index_select", "(", "s", ".", "view", "(", "-", "1", ")", ",", "0", ",", "vidx", ")", "\n", "s_new", "=", "s_new", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "odim", ")", ".", "view", "(", "n_bh", ",", "self", ".", "odim", ")", "\n", "# convert ids to BHS space (S: scoring_num)", "\n", "if", "scoring_idmap", "is", "not", "None", ":", "\n", "            ", "snum", "=", "self", ".", "scoring_num", "\n", "hyp_idx", "=", "(", "best_ids", "//", "self", ".", "odim", "+", "(", "self", ".", "idx_b", "*", "n_hyps", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "view", "(", "\n", "-", "1", "\n", ")", "\n", "label_ids", "=", "torch", ".", "fmod", "(", "best_ids", ",", "self", ".", "odim", ")", ".", "view", "(", "-", "1", ")", "\n", "score_idx", "=", "scoring_idmap", "[", "hyp_idx", ",", "label_ids", "]", "\n", "score_idx", "[", "score_idx", "==", "-", "1", "]", "=", "0", "\n", "vidx", "=", "score_idx", "+", "hyp_idx", "*", "snum", "\n", "", "else", ":", "\n", "            ", "snum", "=", "self", ".", "odim", "\n", "# select forward probabilities", "\n", "", "r_new", "=", "torch", ".", "index_select", "(", "r", ".", "view", "(", "-", "1", ",", "2", ",", "n_bh", "*", "snum", ")", ",", "2", ",", "vidx", ")", ".", "view", "(", "\n", "-", "1", ",", "2", ",", "n_bh", "\n", ")", "\n", "return", "r_new", ",", "s_new", ",", "f_min", ",", "f_max", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScoreTH.extend_prob": [[223, 244], ["enumerate", "x.transpose", "xn[].unsqueeze().expand", "torch.stack", "x.size", "x.size", "torch.as_tensor", "xn[].unsqueeze"], "methods", ["None"], ["", "def", "extend_prob", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Extend CTC prob.\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        \"\"\"", "\n", "\n", "if", "self", ".", "x", ".", "shape", "[", "1", "]", "<", "x", ".", "shape", "[", "1", "]", ":", "# self.x (2,T,B,O); x (B,T,O)", "\n", "# Pad the rest of posteriors in the batch", "\n", "# TODO(takaaki-hori): need a better way without for-loops", "\n", "            ", "xlens", "=", "[", "x", ".", "size", "(", "1", ")", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "xlens", ")", ":", "\n", "                ", "if", "l", "<", "self", ".", "input_length", ":", "\n", "                    ", "x", "[", "i", ",", "l", ":", ",", ":", "]", "=", "self", ".", "logzero", "\n", "x", "[", "i", ",", "l", ":", ",", "self", ".", "blank", "]", "=", "0", "\n", "", "", "tmp_x", "=", "self", ".", "x", "\n", "xn", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# (B, T, O) -> (T, B, O)", "\n", "xb", "=", "xn", "[", ":", ",", ":", ",", "self", ".", "blank", "]", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "odim", ")", "\n", "self", ".", "x", "=", "torch", ".", "stack", "(", "[", "xn", ",", "xb", "]", ")", "# (2, T, B, O)", "\n", "self", ".", "x", "[", ":", ",", ":", "tmp_x", ".", "shape", "[", "1", "]", ",", ":", ",", ":", "]", "=", "tmp_x", "\n", "self", ".", "input_length", "=", "x", ".", "size", "(", "1", ")", "\n", "self", ".", "end_frames", "=", "torch", ".", "as_tensor", "(", "xlens", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScoreTH.extend_state": [[245, 271], ["torch.full", "max", "six.moves.range"], "methods", ["None"], ["", "", "def", "extend_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Compute CTC prefix state.\n\n\n        :param state    : CTC state\n        :return ctc_state\n        \"\"\"", "\n", "\n", "if", "state", "is", "None", ":", "\n", "# nothing to do", "\n", "            ", "return", "state", "\n", "", "else", ":", "\n", "            ", "r_prev", ",", "s_prev", ",", "f_min_prev", ",", "f_max_prev", "=", "state", "\n", "\n", "r_prev_new", "=", "torch", ".", "full", "(", "\n", "(", "self", ".", "input_length", ",", "2", ")", ",", "\n", "self", ".", "logzero", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "start", "=", "max", "(", "r_prev", ".", "shape", "[", "0", "]", ",", "1", ")", "\n", "r_prev_new", "[", "0", ":", "start", "]", "=", "r_prev", "\n", "for", "t", "in", "six", ".", "moves", ".", "range", "(", "start", ",", "self", ".", "input_length", ")", ":", "\n", "                ", "r_prev_new", "[", "t", ",", "1", "]", "=", "r_prev_new", "[", "t", "-", "1", ",", "1", "]", "+", "self", ".", "x", "[", "0", ",", "t", ",", ":", ",", "self", ".", "blank", "]", "\n", "\n", "", "return", "(", "r_prev_new", ",", "s_prev", ",", "f_min_prev", ",", "f_max_prev", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScore.__init__": [[282, 289], ["len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "x", ",", "blank", ",", "eos", ",", "xp", ")", ":", "\n", "        ", "self", ".", "xp", "=", "xp", "\n", "self", ".", "logzero", "=", "-", "10000000000.0", "\n", "self", ".", "blank", "=", "blank", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "input_length", "=", "len", "(", "x", ")", "\n", "self", ".", "x", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScore.initial_state": [[290, 303], ["ctc_prefix_score.CTCPrefixScore.xp.full", "six.moves.range"], "methods", ["None"], ["", "def", "initial_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Obtain an initial CTC state\n\n        :return: CTC state\n        \"\"\"", "\n", "# initial CTC state is made of a frame x 2 tensor that corresponds to", "\n", "# r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent", "\n", "# superscripts n and b (non-blank and blank), respectively.", "\n", "r", "=", "self", ".", "xp", ".", "full", "(", "(", "self", ".", "input_length", ",", "2", ")", ",", "self", ".", "logzero", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "r", "[", "0", ",", "1", "]", "=", "self", ".", "x", "[", "0", ",", "self", ".", "blank", "]", "\n", "for", "i", "in", "six", ".", "moves", ".", "range", "(", "1", ",", "self", ".", "input_length", ")", ":", "\n", "            ", "r", "[", "i", ",", "1", "]", "=", "r", "[", "i", "-", "1", ",", "1", "]", "+", "self", ".", "x", "[", "i", ",", "self", ".", "blank", "]", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScore.__call__": [[304, 360], ["ctc_prefix_score.CTCPrefixScore.xp.ndarray", "ctc_prefix_score.CTCPrefixScore.xp.logaddexp", "max", "six.moves.range", "len", "ctc_prefix_score.CTCPrefixScore.xp.ndarray", "six.moves.range", "ctc_prefix_score.CTCPrefixScore.xp.logaddexp", "ctc_prefix_score.CTCPrefixScore.xp.where", "len", "ctc_prefix_score.CTCPrefixScore.xp.where", "len", "ctc_prefix_score.CTCPrefixScore.xp.rollaxis", "len", "len", "ctc_prefix_score.CTCPrefixScore.xp.logaddexp", "ctc_prefix_score.CTCPrefixScore.xp.logaddexp", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "y", ",", "cs", ",", "r_prev", ")", ":", "\n", "        ", "\"\"\"Compute CTC prefix scores for next labels\n\n        :param y     : prefix label sequence\n        :param cs    : array of next labels\n        :param r_prev: previous CTC state\n        :return ctc_scores, ctc_states\n        \"\"\"", "\n", "# initialize CTC states", "\n", "output_length", "=", "len", "(", "y", ")", "-", "1", "# ignore sos", "\n", "# new CTC states are prepared as a frame x (n or b) x n_labels tensor", "\n", "# that corresponds to r_t^n(h) and r_t^b(h).", "\n", "r", "=", "self", ".", "xp", ".", "ndarray", "(", "(", "self", ".", "input_length", ",", "2", ",", "len", "(", "cs", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "xs", "=", "self", ".", "x", "[", ":", ",", "cs", "]", "\n", "if", "output_length", "==", "0", ":", "\n", "            ", "r", "[", "0", ",", "0", "]", "=", "xs", "[", "0", "]", "\n", "r", "[", "0", ",", "1", "]", "=", "self", ".", "logzero", "\n", "", "else", ":", "\n", "            ", "r", "[", "output_length", "-", "1", "]", "=", "self", ".", "logzero", "\n", "\n", "# prepare forward probabilities for the last label", "\n", "", "r_sum", "=", "self", ".", "xp", ".", "logaddexp", "(", "\n", "r_prev", "[", ":", ",", "0", "]", ",", "r_prev", "[", ":", ",", "1", "]", "\n", ")", "# log(r_t^n(g) + r_t^b(g))", "\n", "last", "=", "y", "[", "-", "1", "]", "\n", "if", "output_length", ">", "0", "and", "last", "in", "cs", ":", "\n", "            ", "log_phi", "=", "self", ".", "xp", ".", "ndarray", "(", "(", "self", ".", "input_length", ",", "len", "(", "cs", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "six", ".", "moves", ".", "range", "(", "len", "(", "cs", ")", ")", ":", "\n", "                ", "log_phi", "[", ":", ",", "i", "]", "=", "r_sum", "if", "cs", "[", "i", "]", "!=", "last", "else", "r_prev", "[", ":", ",", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "log_phi", "=", "r_sum", "\n", "\n", "# compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),", "\n", "# and log prefix probabilities log(psi)", "\n", "", "start", "=", "max", "(", "output_length", ",", "1", ")", "\n", "log_psi", "=", "r", "[", "start", "-", "1", ",", "0", "]", "\n", "for", "t", "in", "six", ".", "moves", ".", "range", "(", "start", ",", "self", ".", "input_length", ")", ":", "\n", "            ", "r", "[", "t", ",", "0", "]", "=", "self", ".", "xp", ".", "logaddexp", "(", "r", "[", "t", "-", "1", ",", "0", "]", ",", "log_phi", "[", "t", "-", "1", "]", ")", "+", "xs", "[", "t", "]", "\n", "r", "[", "t", ",", "1", "]", "=", "(", "\n", "self", ".", "xp", ".", "logaddexp", "(", "r", "[", "t", "-", "1", ",", "0", "]", ",", "r", "[", "t", "-", "1", ",", "1", "]", ")", "+", "self", ".", "x", "[", "t", ",", "self", ".", "blank", "]", "\n", ")", "\n", "log_psi", "=", "self", ".", "xp", ".", "logaddexp", "(", "log_psi", ",", "log_phi", "[", "t", "-", "1", "]", "+", "xs", "[", "t", "]", ")", "\n", "\n", "# get P(...eos|X) that ends with the prefix itself", "\n", "", "eos_pos", "=", "self", ".", "xp", ".", "where", "(", "cs", "==", "self", ".", "eos", ")", "[", "0", "]", "\n", "if", "len", "(", "eos_pos", ")", ">", "0", ":", "\n", "            ", "log_psi", "[", "eos_pos", "]", "=", "r_sum", "[", "-", "1", "]", "# log(r_T^n(g) + r_T^b(g))", "\n", "\n", "# exclude blank probs", "\n", "", "blank_pos", "=", "self", ".", "xp", ".", "where", "(", "cs", "==", "self", ".", "blank", ")", "[", "0", "]", "\n", "if", "len", "(", "blank_pos", ")", ">", "0", ":", "\n", "            ", "log_psi", "[", "blank_pos", "]", "=", "self", ".", "logzero", "\n", "\n", "# return the log prefix probability and CTC states, where the label axis", "\n", "# of the CTC states is moved to the first axis to slice it easily", "\n", "", "return", "log_psi", ",", "self", ".", "xp", ".", "rollaxis", "(", "r", ",", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.lm_interface.LMInterface.add_arguments": [[13, 17], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_arguments", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to command line argument parser.\"\"\"", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.lm_interface.LMInterface.build": [[18, 39], ["argparse.Namespace", "espnet.utils.fill_missing_args.fill_missing_args", "espnet.utils.fill_missing_args.fill_missing_args", "cls", "get_parser"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.fill_missing_args.fill_missing_args", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.fill_missing_args.fill_missing_args"], ["", "@", "classmethod", "\n", "def", "build", "(", "cls", ",", "n_vocab", ":", "int", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Initialize this class with python-level args.\n\n        Args:\n            idim (int): The number of vocabulary.\n\n        Returns:\n            LMinterface: A new instance of LMInterface.\n\n        \"\"\"", "\n", "# local import to avoid cyclic import in lm_train", "\n", "from", "espnet", ".", "bin", ".", "lm_train", "import", "get_parser", "\n", "\n", "def", "wrap", "(", "parser", ")", ":", "\n", "            ", "return", "get_parser", "(", "parser", ",", "required", "=", "False", ")", "\n", "\n", "", "args", "=", "argparse", ".", "Namespace", "(", "**", "kwargs", ")", "\n", "args", "=", "fill_missing_args", "(", "args", ",", "wrap", ")", "\n", "args", "=", "fill_missing_args", "(", "args", ",", "cls", ".", "add_arguments", ")", "\n", "return", "cls", "(", "n_vocab", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.lm_interface.LMInterface.forward": [[40, 59], ["NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "t", ")", ":", "\n", "        ", "\"\"\"Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"forward method is not implemented\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.lm_interface.dynamic_import_lm": [[71, 87], ["espnet.utils.dynamic_import.dynamic_import", "issubclass", "predefined_lms.get", "dict"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.utils.dynamic_import.dynamic_import"], ["def", "dynamic_import_lm", "(", "module", ",", "backend", ")", ":", "\n", "    ", "\"\"\"Import LM class dynamically.\n\n    Args:\n        module (str): module_name:class_name or alias in `predefined_lms`\n        backend (str): NN backend. e.g., pytorch, chainer\n\n    Returns:\n        type: LM class\n\n    \"\"\"", "\n", "model_class", "=", "dynamic_import", "(", "module", ",", "predefined_lms", ".", "get", "(", "backend", ",", "dict", "(", ")", ")", ")", "\n", "assert", "issubclass", "(", "\n", "model_class", ",", "LMInterface", "\n", ")", ",", "f\"{module} does not implement LMInterface\"", "\n", "return", "model_class", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.__init__": [[111, 128], ["object.__init__", "e2e_asr_common.ErrorCalculator.char_list.index", "e2e_asr_common.ErrorCalculator.char_list.index"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "char_list", ",", "sym_space", ",", "sym_blank", ",", "report_cer", "=", "False", ",", "report_wer", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an ErrorCalculator object.\"\"\"", "\n", "super", "(", "ErrorCalculator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "report_cer", "=", "report_cer", "\n", "self", ".", "report_wer", "=", "report_wer", "\n", "\n", "self", ".", "char_list", "=", "char_list", "\n", "self", ".", "space", "=", "sym_space", "\n", "self", ".", "blank", "=", "sym_blank", "\n", "self", ".", "idx_blank", "=", "self", ".", "char_list", ".", "index", "(", "self", ".", "blank", ")", "\n", "if", "self", ".", "space", "in", "self", ".", "char_list", ":", "\n", "            ", "self", ".", "idx_space", "=", "self", ".", "char_list", ".", "index", "(", "self", ".", "space", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "idx_space", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.__call__": [[129, 153], ["e2e_asr_common.ErrorCalculator.convert_to_char", "e2e_asr_common.ErrorCalculator.calculate_cer_ctc", "e2e_asr_common.ErrorCalculator.calculate_cer", "e2e_asr_common.ErrorCalculator.calculate_wer"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.convert_to_char", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_cer_ctc", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_cer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_wer"], ["", "", "def", "__call__", "(", "self", ",", "ys_hat", ",", "ys_pad", ",", "is_ctc", "=", "False", ")", ":", "\n", "        ", "\"\"\"Calculate sentence-level WER/CER score.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :param bool is_ctc: calculate CER score for CTC\n        :return: sentence-level WER score\n        :rtype float\n        :return: sentence-level CER score\n        :rtype float\n        \"\"\"", "\n", "cer", ",", "wer", "=", "None", ",", "None", "\n", "if", "is_ctc", ":", "\n", "            ", "return", "self", ".", "calculate_cer_ctc", "(", "ys_hat", ",", "ys_pad", ")", "\n", "", "elif", "not", "self", ".", "report_cer", "and", "not", "self", ".", "report_wer", ":", "\n", "            ", "return", "cer", ",", "wer", "\n", "\n", "", "seqs_hat", ",", "seqs_true", "=", "self", ".", "convert_to_char", "(", "ys_hat", ",", "ys_pad", ")", "\n", "if", "self", ".", "report_cer", ":", "\n", "            ", "cer", "=", "self", ".", "calculate_cer", "(", "seqs_hat", ",", "seqs_true", ")", "\n", "\n", "", "if", "self", ".", "report_wer", ":", "\n", "            ", "wer", "=", "self", ".", "calculate_wer", "(", "seqs_hat", ",", "seqs_true", ")", "\n", "", "return", "cer", ",", "wer", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_cer_ctc": [[154, 187], ["enumerate", "int", "int", "len", "cers.append", "char_ref_lens.append", "float", "sum", "itertools.groupby", "seq_hat.append", "seq_true.append", "editdistance.eval", "len", "sum", "int", "int"], "methods", ["None"], ["", "def", "calculate_cer_ctc", "(", "self", ",", "ys_hat", ",", "ys_pad", ")", ":", "\n", "        ", "\"\"\"Calculate sentence-level CER score for CTC.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :return: average sentence-level CER score\n        :rtype float\n        \"\"\"", "\n", "import", "editdistance", "\n", "\n", "cers", ",", "char_ref_lens", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "y", "in", "enumerate", "(", "ys_hat", ")", ":", "\n", "            ", "y_hat", "=", "[", "x", "[", "0", "]", "for", "x", "in", "groupby", "(", "y", ")", "]", "\n", "y_true", "=", "ys_pad", "[", "i", "]", "\n", "seq_hat", ",", "seq_true", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", "in", "y_hat", ":", "\n", "                ", "idx", "=", "int", "(", "idx", ")", "\n", "if", "idx", "!=", "-", "1", "and", "idx", "!=", "self", ".", "idx_blank", "and", "idx", "!=", "self", ".", "idx_space", ":", "\n", "                    ", "seq_hat", ".", "append", "(", "self", ".", "char_list", "[", "int", "(", "idx", ")", "]", ")", "\n", "\n", "", "", "for", "idx", "in", "y_true", ":", "\n", "                ", "idx", "=", "int", "(", "idx", ")", "\n", "if", "idx", "!=", "-", "1", "and", "idx", "!=", "self", ".", "idx_blank", "and", "idx", "!=", "self", ".", "idx_space", ":", "\n", "                    ", "seq_true", ".", "append", "(", "self", ".", "char_list", "[", "int", "(", "idx", ")", "]", ")", "\n", "\n", "", "", "hyp_chars", "=", "\"\"", ".", "join", "(", "seq_hat", ")", "\n", "ref_chars", "=", "\"\"", ".", "join", "(", "seq_true", ")", "\n", "if", "len", "(", "ref_chars", ")", ">", "0", ":", "\n", "                ", "cers", ".", "append", "(", "editdistance", ".", "eval", "(", "hyp_chars", ",", "ref_chars", ")", ")", "\n", "char_ref_lens", ".", "append", "(", "len", "(", "ref_chars", ")", ")", "\n", "\n", "", "", "cer_ctc", "=", "float", "(", "sum", "(", "cers", ")", ")", "/", "sum", "(", "char_ref_lens", ")", "if", "cers", "else", "None", "\n", "return", "cer_ctc", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.convert_to_char": [[188, 212], ["enumerate", "seq_hat_text.replace.replace.replace", "seqs_hat.append", "seqs_true.append", "numpy.where", "len", "len", "int", "int", "int"], "methods", ["None"], ["", "def", "convert_to_char", "(", "self", ",", "ys_hat", ",", "ys_pad", ")", ":", "\n", "        ", "\"\"\"Convert index to character.\n\n        :param torch.Tensor seqs_hat: prediction (batch, seqlen)\n        :param torch.Tensor seqs_true: reference (batch, seqlen)\n        :return: token list of prediction\n        :rtype list\n        :return: token list of reference\n        :rtype list\n        \"\"\"", "\n", "seqs_hat", ",", "seqs_true", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "y_hat", "in", "enumerate", "(", "ys_hat", ")", ":", "\n", "            ", "y_true", "=", "ys_pad", "[", "i", "]", "\n", "eos_true", "=", "np", ".", "where", "(", "y_true", "==", "-", "1", ")", "[", "0", "]", "\n", "ymax", "=", "eos_true", "[", "0", "]", "if", "len", "(", "eos_true", ")", ">", "0", "else", "len", "(", "y_true", ")", "\n", "# NOTE: padding index (-1) in y_true is used to pad y_hat", "\n", "seq_hat", "=", "[", "self", ".", "char_list", "[", "int", "(", "idx", ")", "]", "for", "idx", "in", "y_hat", "[", ":", "ymax", "]", "]", "\n", "seq_true", "=", "[", "self", ".", "char_list", "[", "int", "(", "idx", ")", "]", "for", "idx", "in", "y_true", "if", "int", "(", "idx", ")", "!=", "-", "1", "]", "\n", "seq_hat_text", "=", "\"\"", ".", "join", "(", "seq_hat", ")", ".", "replace", "(", "self", ".", "space", ",", "\" \"", ")", "\n", "seq_hat_text", "=", "seq_hat_text", ".", "replace", "(", "self", ".", "blank", ",", "\"\"", ")", "\n", "seq_true_text", "=", "\"\"", ".", "join", "(", "seq_true", ")", ".", "replace", "(", "self", ".", "space", ",", "\" \"", ")", "\n", "seqs_hat", ".", "append", "(", "seq_hat_text", ")", "\n", "seqs_true", ".", "append", "(", "seq_true_text", ")", "\n", "", "return", "seqs_hat", ",", "seqs_true", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_cer": [[213, 231], ["enumerate", "seq_hat_text.replace", "seq_true_text.replace", "char_eds.append", "char_ref_lens.append", "float", "sum", "editdistance.eval", "len", "sum"], "methods", ["None"], ["", "def", "calculate_cer", "(", "self", ",", "seqs_hat", ",", "seqs_true", ")", ":", "\n", "        ", "\"\"\"Calculate sentence-level CER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level CER score\n        :rtype float\n        \"\"\"", "\n", "import", "editdistance", "\n", "\n", "char_eds", ",", "char_ref_lens", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "seq_hat_text", "in", "enumerate", "(", "seqs_hat", ")", ":", "\n", "            ", "seq_true_text", "=", "seqs_true", "[", "i", "]", "\n", "hyp_chars", "=", "seq_hat_text", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "ref_chars", "=", "seq_true_text", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "char_eds", ".", "append", "(", "editdistance", ".", "eval", "(", "hyp_chars", ",", "ref_chars", ")", ")", "\n", "char_ref_lens", ".", "append", "(", "len", "(", "ref_chars", ")", ")", "\n", "", "return", "float", "(", "sum", "(", "char_eds", ")", ")", "/", "sum", "(", "char_ref_lens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.ErrorCalculator.calculate_wer": [[232, 250], ["enumerate", "seq_hat_text.split", "seq_true_text.split", "word_eds.append", "word_ref_lens.append", "float", "sum", "editdistance.eval", "len", "sum"], "methods", ["None"], ["", "def", "calculate_wer", "(", "self", ",", "seqs_hat", ",", "seqs_true", ")", ":", "\n", "        ", "\"\"\"Calculate sentence-level WER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level WER score\n        :rtype float\n        \"\"\"", "\n", "import", "editdistance", "\n", "\n", "word_eds", ",", "word_ref_lens", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "seq_hat_text", "in", "enumerate", "(", "seqs_hat", ")", ":", "\n", "            ", "seq_true_text", "=", "seqs_true", "[", "i", "]", "\n", "hyp_words", "=", "seq_hat_text", ".", "split", "(", ")", "\n", "ref_words", "=", "seq_true_text", ".", "split", "(", ")", "\n", "word_eds", ".", "append", "(", "editdistance", ".", "eval", "(", "hyp_words", ",", "ref_words", ")", ")", "\n", "word_ref_lens", ".", "append", "(", "len", "(", "ref_words", ")", ")", "\n", "", "return", "float", "(", "sum", "(", "word_eds", ")", ")", "/", "sum", "(", "word_ref_lens", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.end_detect": [[18, 49], ["numpy.log", "six.moves.range", "len", "sorted", "numpy.exp", "len", "sorted", "len"], "function", ["None"], ["def", "end_detect", "(", "ended_hyps", ",", "i", ",", "M", "=", "3", ",", "D_end", "=", "np", ".", "log", "(", "1", "*", "np", ".", "exp", "(", "-", "10", ")", ")", ")", ":", "\n", "    ", "\"\"\"End detection.\n\n    described in Eq. (50) of S. Watanabe et al\n    \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition\"\n\n    :param ended_hyps:\n    :param i:\n    :param M:\n    :param D_end:\n    :return:\n    \"\"\"", "\n", "if", "len", "(", "ended_hyps", ")", "==", "0", ":", "\n", "        ", "return", "False", "\n", "", "count", "=", "0", "\n", "best_hyp", "=", "sorted", "(", "ended_hyps", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"score\"", "]", ",", "reverse", "=", "True", ")", "[", "0", "]", "\n", "for", "m", "in", "six", ".", "moves", ".", "range", "(", "M", ")", ":", "\n", "# get ended_hyps with their length is i - m", "\n", "        ", "hyp_length", "=", "i", "-", "m", "\n", "hyps_same_length", "=", "[", "x", "for", "x", "in", "ended_hyps", "if", "len", "(", "x", "[", "\"yseq\"", "]", ")", "==", "hyp_length", "]", "\n", "if", "len", "(", "hyps_same_length", ")", ">", "0", ":", "\n", "            ", "best_hyp_same_length", "=", "sorted", "(", "\n", "hyps_same_length", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"score\"", "]", ",", "reverse", "=", "True", "\n", ")", "[", "0", "]", "\n", "if", "best_hyp_same_length", "[", "\"score\"", "]", "-", "best_hyp", "[", "\"score\"", "]", "<", "D_end", ":", "\n", "                ", "count", "+=", "1", "\n", "\n", "", "", "", "if", "count", "==", "M", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.label_smoothing_dist": [[52, 84], ["numpy.zeros", "trans_json.items", "len", "logging.error", "sys.exit", "open", "numpy.array", "np.zeros.astype", "numpy.sum", "json.load", "len", "int", "[].split"], "function", ["None"], ["", "", "def", "label_smoothing_dist", "(", "odim", ",", "lsm_type", ",", "transcript", "=", "None", ",", "blank", "=", "0", ")", ":", "\n", "    ", "\"\"\"Obtain label distribution for loss smoothing.\n\n    :param odim:\n    :param lsm_type:\n    :param blank:\n    :param transcript:\n    :return:\n    \"\"\"", "\n", "if", "transcript", "is", "not", "None", ":", "\n", "        ", "with", "open", "(", "transcript", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "trans_json", "=", "json", ".", "load", "(", "f", ")", "[", "\"utts\"", "]", "\n", "\n", "", "", "if", "lsm_type", "==", "\"unigram\"", ":", "\n", "        ", "assert", "transcript", "is", "not", "None", ",", "(", "\n", "\"transcript is required for %s label smoothing\"", "%", "lsm_type", "\n", ")", "\n", "labelcount", "=", "np", ".", "zeros", "(", "odim", ")", "\n", "for", "k", ",", "v", "in", "trans_json", ".", "items", "(", ")", ":", "\n", "            ", "ids", "=", "np", ".", "array", "(", "[", "int", "(", "n", ")", "for", "n", "in", "v", "[", "\"output\"", "]", "[", "0", "]", "[", "\"tokenid\"", "]", ".", "split", "(", ")", "]", ")", "\n", "# to avoid an error when there is no text in an uttrance", "\n", "if", "len", "(", "ids", ")", ">", "0", ":", "\n", "                ", "labelcount", "[", "ids", "]", "+=", "1", "\n", "", "", "labelcount", "[", "odim", "-", "1", "]", "=", "len", "(", "transcript", ")", "# count <eos>", "\n", "labelcount", "[", "labelcount", "==", "0", "]", "=", "1", "# flooring", "\n", "labelcount", "[", "blank", "]", "=", "0", "# remove counts for blank", "\n", "labeldist", "=", "labelcount", ".", "astype", "(", "np", ".", "float32", ")", "/", "np", ".", "sum", "(", "labelcount", ")", "\n", "", "else", ":", "\n", "        ", "logging", ".", "error", "(", "\"Error: unexpected label smoothing type: %s\"", "%", "lsm_type", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "return", "labeldist", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.e2e_asr_common.get_vgg2l_odim": [[86, 98], ["numpy.ceil", "numpy.ceil", "int", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "get_vgg2l_odim", "(", "idim", ",", "in_channel", "=", "3", ",", "out_channel", "=", "128", ")", ":", "\n", "    ", "\"\"\"Return the output size of the VGG frontend.\n\n    :param in_channel: input channel size\n    :param out_channel: output channel size\n    :return: output size\n    :rtype int\n    \"\"\"", "\n", "idim", "=", "idim", "/", "in_channel", "\n", "idim", "=", "np", ".", "ceil", "(", "np", ".", "array", "(", "idim", ",", "dtype", "=", "np", ".", "float32", ")", "/", "2", ")", "# 1st max pooling", "\n", "idim", "=", "np", ".", "ceil", "(", "np", ".", "array", "(", "idim", ",", "dtype", "=", "np", ".", "float32", ")", "/", "2", ")", "# 2nd max pooling", "\n", "return", "int", "(", "idim", ")", "*", "out_channel", "# numer of channels", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.length_bonus.LengthBonus.__init__": [[14, 22], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "n_vocab", ":", "int", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The number of tokens in vocabulary for beam search\n\n        \"\"\"", "\n", "self", ".", "n", "=", "n_vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.length_bonus.LengthBonus.score": [[23, 38], ["torch.tensor().expand", "torch.tensor"], "methods", ["None"], ["", "def", "score", "(", "self", ",", "y", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and None\n\n        \"\"\"", "\n", "return", "torch", ".", "tensor", "(", "[", "1.0", "]", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "expand", "(", "self", ".", "n", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.length_bonus.LengthBonus.batch_score": [[39, 61], ["torch.tensor().expand", "torch.tensor"], "methods", ["None"], ["", "def", "batch_score", "(", "\n", "self", ",", "ys", ":", "torch", ".", "Tensor", ",", "states", ":", "List", "[", "Any", "]", ",", "xs", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "List", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"", "\n", "return", "(", "\n", "torch", ".", "tensor", "(", "[", "1.0", "]", ",", "device", "=", "xs", ".", "device", ",", "dtype", "=", "xs", ".", "dtype", ")", ".", "expand", "(", "\n", "ys", ".", "shape", "[", "0", "]", ",", "self", ".", "n", "\n", ")", ",", "\n", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.__init__": [[14, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ctc", ":", "torch", ".", "nn", ".", "Module", ",", "eos", ":", "int", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        Args:\n            ctc (torch.nn.Module): The CTC implementation.\n                For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`\n            eos (int): The end-of-sequence id.\n\n        \"\"\"", "\n", "self", ".", "ctc", "=", "ctc", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "impl", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.init_state": [[27, 40], ["ctc.CTCPrefixScorer.ctc.log_softmax().detach().squeeze().cpu().numpy", "espnet.nets.ctc_prefix_score.CTCPrefixScore", "ctc.CTCPrefixScorer.impl.initial_state", "ctc.CTCPrefixScorer.ctc.log_softmax().detach().squeeze().cpu", "ctc.CTCPrefixScorer.ctc.log_softmax().detach().squeeze", "ctc.CTCPrefixScorer.ctc.log_softmax().detach", "ctc.CTCPrefixScorer.ctc.log_softmax", "x.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.nets.ctc_prefix_score.CTCPrefixScore.initial_state", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", ".", "detach", "(", ")", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# TODO(karita): use CTCPrefixScoreTH", "\n", "self", ".", "impl", "=", "CTCPrefixScore", "(", "logp", ",", "0", ",", "self", ".", "eos", ",", "np", ")", "\n", "return", "0", ",", "self", ".", "impl", ".", "initial_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.select_state": [[41, 65], ["type", "len", "log_psi[].expand", "log_psi.size"], "methods", ["None"], ["", "def", "select_state", "(", "self", ",", "state", ",", "i", ",", "new_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label id to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"", "\n", "if", "type", "(", "state", ")", "==", "tuple", ":", "\n", "            ", "if", "len", "(", "state", ")", "==", "2", ":", "# for CTCPrefixScore", "\n", "                ", "sc", ",", "st", "=", "state", "\n", "return", "sc", "[", "i", "]", ",", "st", "[", "i", "]", "\n", "", "else", ":", "# for CTCPrefixScoreTH (need new_id > 0)", "\n", "                ", "r", ",", "log_psi", ",", "f_min", ",", "f_max", ",", "scoring_idmap", "=", "state", "\n", "s", "=", "log_psi", "[", "i", ",", "new_id", "]", ".", "expand", "(", "log_psi", ".", "size", "(", "1", ")", ")", "\n", "if", "scoring_idmap", "is", "not", "None", ":", "\n", "                    ", "return", "r", "[", ":", ",", ":", ",", "i", ",", "scoring_idmap", "[", "i", ",", "new_id", "]", "]", ",", "s", ",", "f_min", ",", "f_max", "\n", "", "else", ":", "\n", "                    ", "return", "r", "[", ":", ",", ":", ",", "i", ",", "new_id", "]", ",", "s", ",", "f_min", ",", "f_max", "\n", "", "", "", "return", "None", "if", "state", "is", "None", "else", "state", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.score_partial": [[66, 87], ["ctc.CTCPrefixScorer.impl", "torch.as_tensor", "y.cpu", "ids.cpu"], "methods", ["None"], ["", "def", "score_partial", "(", "self", ",", "y", ",", "ids", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"", "\n", "prev_score", ",", "state", "=", "state", "\n", "presub_score", ",", "new_st", "=", "self", ".", "impl", "(", "y", ".", "cpu", "(", ")", ",", "ids", ".", "cpu", "(", ")", ",", "state", ")", "\n", "tscore", "=", "torch", ".", "as_tensor", "(", "\n", "presub_score", "-", "prev_score", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", "\n", ")", "\n", "return", "tscore", ",", "(", "presub_score", ",", "new_st", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.batch_init_state": [[88, 101], ["ctc.CTCPrefixScorer.ctc.log_softmax", "torch.tensor", "espnet.nets.ctc_prefix_score.CTCPrefixScoreTH", "x.unsqueeze", "ctc.CTCPrefixScorer.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "batch_init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", "# assuming batch_size = 1", "\n", "xlen", "=", "torch", ".", "tensor", "(", "[", "logp", ".", "size", "(", "1", ")", "]", ")", "\n", "self", ".", "impl", "=", "CTCPrefixScoreTH", "(", "logp", ",", "xlen", ",", "0", ",", "self", ".", "eos", ")", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.batch_score_partial": [[102, 128], ["ctc.CTCPrefixScorer.impl", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "batch_score_partial", "(", "self", ",", "y", ",", "ids", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            ids (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"", "\n", "batch_state", "=", "(", "\n", "(", "\n", "torch", ".", "stack", "(", "[", "s", "[", "0", "]", "for", "s", "in", "state", "]", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "stack", "(", "[", "s", "[", "1", "]", "for", "s", "in", "state", "]", ")", ",", "\n", "state", "[", "0", "]", "[", "2", "]", ",", "\n", "state", "[", "0", "]", "[", "3", "]", ",", "\n", ")", "\n", "if", "state", "[", "0", "]", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "return", "self", ".", "impl", "(", "y", ",", "batch_state", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.extend_prob": [[129, 141], ["ctc.CTCPrefixScorer.ctc.log_softmax", "ctc.CTCPrefixScorer.impl.extend_prob", "x.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.extend_prob"], ["", "def", "extend_prob", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Extend probs for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", "\n", "self", ".", "impl", ".", "extend_prob", "(", "logp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.extend_state": [[142, 159], ["new_state.append", "ctc.CTCPrefixScorer.impl.extend_state"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.scorers.ctc.CTCPrefixScorer.extend_state"], ["", "def", "extend_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Extend state for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            state: The states of hyps\n\n        Returns: exteded state\n\n        \"\"\"", "\n", "new_state", "=", "[", "]", "\n", "for", "s", "in", "state", ":", "\n", "            ", "new_state", ".", "append", "(", "self", ".", "impl", ".", "extend_state", "(", "s", ")", ")", "\n", "\n", "", "return", "new_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device": [[12, 32], ["isinstance", "x.to", "isinstance", "next", "TypeError", "m.parameters", "type"], "function", ["None"], ["def", "to_device", "(", "m", ",", "x", ")", ":", "\n", "    ", "\"\"\"Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    \"\"\"", "\n", "if", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "        ", "device", "=", "next", "(", "m", ".", "parameters", "(", ")", ")", ".", "device", "\n", "", "elif", "isinstance", "(", "m", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "device", "=", "m", ".", "device", "\n", "", "else", ":", "\n", "        ", "raise", "TypeError", "(", "\n", "\"Expected torch.nn.Module or torch.tensor, \"", "f\"bot got: {type(m)}\"", "\n", ")", "\n", "", "return", "x", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.pad_list": [[34, 62], ["len", "max", "xs[].new().fill_", "range", "x.size", "xs[].new", "xs[].size", "xs[].size"], "function", ["None"], ["", "def", "pad_list", "(", "xs", ",", "pad_value", ")", ":", "\n", "    ", "\"\"\"Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    \"\"\"", "\n", "n_batch", "=", "len", "(", "xs", ")", "\n", "max_len", "=", "max", "(", "x", ".", "size", "(", "0", ")", "for", "x", "in", "xs", ")", "\n", "pad", "=", "xs", "[", "0", "]", ".", "new", "(", "n_batch", ",", "max_len", ",", "*", "xs", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", ")", ".", "fill_", "(", "pad_value", ")", "\n", "\n", "for", "i", "in", "range", "(", "n_batch", ")", ":", "\n", "        ", "pad", "[", "i", ",", ":", "xs", "[", "i", "]", ".", "size", "(", "0", ")", "]", "=", "xs", "[", "i", "]", "\n", "\n", "", "return", "pad", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.make_pad_mask": [[64, 181], ["int", "torch.arange", "torch.arange.unsqueeze().expand", "seq_range.unsqueeze().expand.new().unsqueeze", "ValueError", "isinstance", "lengths.tolist.tolist", "len", "tuple", "mask[].expand_as().to", "int", "xs.size", "int", "torch.arange.unsqueeze", "seq_range.unsqueeze().expand.new", "xs.size", "xs.size", "max", "max", "xs.dim", "mask[].expand_as", "slice", "range", "xs.dim"], "function", ["None"], ["", "def", "make_pad_mask", "(", "lengths", ",", "xs", "=", "None", ",", "length_dim", "=", "-", "1", ",", "maxlen", "=", "None", ")", ":", "\n", "    ", "\"\"\"Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    \"\"\"", "\n", "if", "length_dim", "==", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"length_dim cannot be 0: {}\"", ".", "format", "(", "length_dim", ")", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "lengths", ",", "list", ")", ":", "\n", "        ", "lengths", "=", "lengths", ".", "tolist", "(", ")", "\n", "", "bs", "=", "int", "(", "len", "(", "lengths", ")", ")", "\n", "if", "maxlen", "is", "None", ":", "\n", "        ", "if", "xs", "is", "None", ":", "\n", "            ", "maxlen", "=", "int", "(", "max", "(", "lengths", ")", ")", "\n", "", "else", ":", "\n", "            ", "maxlen", "=", "xs", ".", "size", "(", "length_dim", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "xs", "is", "None", "\n", "assert", "maxlen", ">=", "int", "(", "max", "(", "lengths", ")", ")", "\n", "\n", "", "seq_range", "=", "torch", ".", "arange", "(", "0", ",", "maxlen", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "seq_range_expand", "=", "seq_range", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bs", ",", "maxlen", ")", "\n", "seq_length_expand", "=", "seq_range_expand", ".", "new", "(", "lengths", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "mask", "=", "seq_range_expand", ">=", "seq_length_expand", "\n", "\n", "if", "xs", "is", "not", "None", ":", "\n", "        ", "assert", "xs", ".", "size", "(", "0", ")", "==", "bs", ",", "(", "xs", ".", "size", "(", "0", ")", ",", "bs", ")", "\n", "\n", "if", "length_dim", "<", "0", ":", "\n", "            ", "length_dim", "=", "xs", ".", "dim", "(", ")", "+", "length_dim", "\n", "# ind = (:, None, ..., None, :, , None, ..., None)", "\n", "", "ind", "=", "tuple", "(", "\n", "slice", "(", "None", ")", "if", "i", "in", "(", "0", ",", "length_dim", ")", "else", "None", "for", "i", "in", "range", "(", "xs", ".", "dim", "(", ")", ")", "\n", ")", "\n", "mask", "=", "mask", "[", "ind", "]", ".", "expand_as", "(", "xs", ")", ".", "to", "(", "xs", ".", "device", ")", "\n", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.make_non_pad_mask": [[183, 270], ["nets_utils.make_pad_mask"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.make_pad_mask"], ["", "def", "make_non_pad_mask", "(", "lengths", ",", "xs", "=", "None", ",", "length_dim", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    \"\"\"", "\n", "return", "~", "make_pad_mask", "(", "lengths", ",", "xs", ",", "length_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.mask_by_length": [[272, 301], ["xs.data.new().fill_", "enumerate", "xs.size", "len", "xs.data.new", "xs.size"], "function", ["None"], ["", "def", "mask_by_length", "(", "xs", ",", "lengths", ",", "fill", "=", "0", ")", ":", "\n", "    ", "\"\"\"Mask tensor according to length.\n\n    Args:\n        xs (Tensor): Batch of input tensor (B, `*`).\n        lengths (LongTensor or List): Batch of lengths (B,).\n        fill (int or float): Value to fill masked part.\n\n    Returns:\n        Tensor: Batch of masked input tensor (B, `*`).\n\n    Examples:\n        >>> x = torch.arange(5).repeat(3, 1) + 1\n        >>> x\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5]])\n        >>> lengths = [5, 3, 2]\n        >>> mask_by_length(x, lengths)\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 0, 0],\n                [1, 2, 0, 0, 0]])\n\n    \"\"\"", "\n", "assert", "xs", ".", "size", "(", "0", ")", "==", "len", "(", "lengths", ")", "\n", "ret", "=", "xs", ".", "data", ".", "new", "(", "*", "xs", ".", "size", "(", ")", ")", ".", "fill_", "(", "fill", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "        ", "ret", "[", "i", ",", ":", "l", "]", "=", "xs", "[", "i", ",", ":", "l", "]", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.th_accuracy": [[303, 324], ["pad_outputs.view().argmax", "torch.sum", "torch.sum", "float", "float", "pad_outputs.view", "pad_outputs.view().argmax.masked_select", "pad_targets.masked_select", "pad_targets.size", "pad_targets.size", "pad_outputs.size"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax"], ["", "def", "th_accuracy", "(", "pad_outputs", ",", "pad_targets", ",", "ignore_label", ")", ":", "\n", "    ", "\"\"\"Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    \"\"\"", "\n", "pad_pred", "=", "pad_outputs", ".", "view", "(", "\n", "pad_targets", ".", "size", "(", "0", ")", ",", "pad_targets", ".", "size", "(", "1", ")", ",", "pad_outputs", ".", "size", "(", "1", ")", "\n", ")", ".", "argmax", "(", "2", ")", "\n", "mask", "=", "pad_targets", "!=", "ignore_label", "\n", "numerator", "=", "torch", ".", "sum", "(", "\n", "pad_pred", ".", "masked_select", "(", "mask", ")", "==", "pad_targets", ".", "masked_select", "(", "mask", ")", "\n", ")", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ")", "\n", "return", "float", "(", "numerator", ")", "/", "float", "(", "denominator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_torch_tensor": [[326, 392], ["isinstance", "isinstance", "ComplexTensor", "torch.from_numpy", "ComplexTensor", "isinstance", "ValueError", "type", "isinstance", "list", "ValueError", "ValueError"], "function", ["None"], ["", "def", "to_torch_tensor", "(", "x", ")", ":", "\n", "    ", "\"\"\"Change to torch.Tensor or ComplexTensor from numpy.ndarray.\n\n    Args:\n        x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\n\n    Returns:\n        Tensor or ComplexTensor: Type converted inputs.\n\n    Examples:\n        >>> xs = np.ones(3, dtype=np.float32)\n        >>> xs = to_torch_tensor(xs)\n        tensor([1., 1., 1.])\n        >>> xs = torch.ones(3, 4, 5)\n        >>> assert to_torch_tensor(xs) is xs\n        >>> xs = {'real': xs, 'imag': xs}\n        >>> to_torch_tensor(xs)\n        ComplexTensor(\n        Real:\n        tensor([1., 1., 1.])\n        Imag;\n        tensor([1., 1., 1.])\n        )\n\n    \"\"\"", "\n", "# If numpy, change to torch tensor", "\n", "if", "isinstance", "(", "x", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "if", "x", ".", "dtype", ".", "kind", "==", "\"c\"", ":", "\n", "# Dynamically importing because torch_complex requires python3", "\n", "            ", "from", "torch_complex", ".", "tensor", "import", "ComplexTensor", "\n", "\n", "return", "ComplexTensor", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "from_numpy", "(", "x", ")", "\n", "\n", "# If {'real': ..., 'imag': ...}, convert to ComplexTensor", "\n", "", "", "elif", "isinstance", "(", "x", ",", "dict", ")", ":", "\n", "# Dynamically importing because torch_complex requires python3", "\n", "        ", "from", "torch_complex", ".", "tensor", "import", "ComplexTensor", "\n", "\n", "if", "\"real\"", "not", "in", "x", "or", "\"imag\"", "not", "in", "x", ":", "\n", "            ", "raise", "ValueError", "(", "\"has 'real' and 'imag' keys: {}\"", ".", "format", "(", "list", "(", "x", ")", ")", ")", "\n", "# Relative importing because of using python3 syntax", "\n", "", "return", "ComplexTensor", "(", "x", "[", "\"real\"", "]", ",", "x", "[", "\"imag\"", "]", ")", "\n", "\n", "# If torch.Tensor, as it is", "\n", "", "elif", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "x", "\n", "\n", "", "else", ":", "\n", "        ", "error", "=", "(", "\n", "\"x must be numpy.ndarray, torch.Tensor or a dict like \"", "\n", "\"{{'real': torch.Tensor, 'imag': torch.Tensor}}, \"", "\n", "\"but got {}\"", ".", "format", "(", "type", "(", "x", ")", ")", "\n", ")", "\n", "try", ":", "\n", "            ", "from", "torch_complex", ".", "tensor", "import", "ComplexTensor", "\n", "", "except", "Exception", ":", "\n", "# If PY2", "\n", "            ", "raise", "ValueError", "(", "error", ")", "\n", "", "else", ":", "\n", "# If PY3", "\n", "            ", "if", "isinstance", "(", "x", ",", "ComplexTensor", ")", ":", "\n", "                ", "return", "x", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "error", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.get_subsample": [[394, 473], ["numpy.array", "numpy.ones", "logging.warning", "logging.info", "numpy.ones", "logging.info", "train_args.etype.endswith", "train_args.subsample.split", "range", "logging.warning", "numpy.ones", "logging.info", "train_args.etype.startswith", "min", "int", "train_args.etype.endswith", "train_args.subsample.split", "range", "logging.warning", "range", "ValueError", "str", "len", "train_args.etype.startswith", "min", "int", "numpy.ones", "logging.info", "subsample_list.append", "str", "len", "train_args.etype[].endswith", "train_args.subsample[].split", "range", "logging.warning", "str", "train_args.etype[].startswith", "min", "int", "len", "str"], "function", ["None"], ["", "", "", "", "def", "get_subsample", "(", "train_args", ",", "mode", ",", "arch", ")", ":", "\n", "    ", "\"\"\"Parse the subsampling factors from the args for the specified `mode` and `arch`.\n\n    Args:\n        train_args: argument Namespace containing options.\n        mode: one of ('asr', 'mt', 'st')\n        arch: one of ('rnn', 'rnn-t', 'rnn_mix', 'rnn_mulenc', 'transformer')\n\n    Returns:\n        np.ndarray / List[np.ndarray]: subsampling factors.\n    \"\"\"", "\n", "if", "arch", "==", "\"transformer\"", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "1", "]", ")", "\n", "\n", "", "elif", "mode", "==", "\"mt\"", "and", "arch", "==", "\"rnn\"", ":", "\n", "# +1 means input (+1) and layers outputs (train_args.elayer)", "\n", "        ", "subsample", "=", "np", ".", "ones", "(", "train_args", ".", "elayers", "+", "1", ",", "dtype", "=", "np", ".", "int", ")", "\n", "logging", ".", "warning", "(", "\"Subsampling is not performed for machine translation.\"", ")", "\n", "logging", ".", "info", "(", "\"subsample: \"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "subsample", "]", ")", ")", "\n", "return", "subsample", "\n", "\n", "", "elif", "(", "\n", "(", "mode", "==", "\"asr\"", "and", "arch", "in", "(", "\"rnn\"", ",", "\"rnn-t\"", ")", ")", "\n", "or", "(", "mode", "==", "\"mt\"", "and", "arch", "==", "\"rnn\"", ")", "\n", "or", "(", "mode", "==", "\"st\"", "and", "arch", "==", "\"rnn\"", ")", "\n", ")", ":", "\n", "        ", "subsample", "=", "np", ".", "ones", "(", "train_args", ".", "elayers", "+", "1", ",", "dtype", "=", "np", ".", "int", ")", "\n", "if", "train_args", ".", "etype", ".", "endswith", "(", "\"p\"", ")", "and", "not", "train_args", ".", "etype", ".", "startswith", "(", "\"vgg\"", ")", ":", "\n", "            ", "ss", "=", "train_args", ".", "subsample", ".", "split", "(", "\"_\"", ")", "\n", "for", "j", "in", "range", "(", "min", "(", "train_args", ".", "elayers", "+", "1", ",", "len", "(", "ss", ")", ")", ")", ":", "\n", "                ", "subsample", "[", "j", "]", "=", "int", "(", "ss", "[", "j", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"Subsampling is not performed for vgg*. \"", "\n", "\"It is performed in max pooling layers at CNN.\"", "\n", ")", "\n", "", "logging", ".", "info", "(", "\"subsample: \"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "subsample", "]", ")", ")", "\n", "return", "subsample", "\n", "\n", "", "elif", "mode", "==", "\"asr\"", "and", "arch", "==", "\"rnn_mix\"", ":", "\n", "        ", "subsample", "=", "np", ".", "ones", "(", "\n", "train_args", ".", "elayers_sd", "+", "train_args", ".", "elayers", "+", "1", ",", "dtype", "=", "np", ".", "int", "\n", ")", "\n", "if", "train_args", ".", "etype", ".", "endswith", "(", "\"p\"", ")", "and", "not", "train_args", ".", "etype", ".", "startswith", "(", "\"vgg\"", ")", ":", "\n", "            ", "ss", "=", "train_args", ".", "subsample", ".", "split", "(", "\"_\"", ")", "\n", "for", "j", "in", "range", "(", "\n", "min", "(", "train_args", ".", "elayers_sd", "+", "train_args", ".", "elayers", "+", "1", ",", "len", "(", "ss", ")", ")", "\n", ")", ":", "\n", "                ", "subsample", "[", "j", "]", "=", "int", "(", "ss", "[", "j", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"Subsampling is not performed for vgg*. \"", "\n", "\"It is performed in max pooling layers at CNN.\"", "\n", ")", "\n", "", "logging", ".", "info", "(", "\"subsample: \"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "subsample", "]", ")", ")", "\n", "return", "subsample", "\n", "\n", "", "elif", "mode", "==", "\"asr\"", "and", "arch", "==", "\"rnn_mulenc\"", ":", "\n", "        ", "subsample_list", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "train_args", ".", "num_encs", ")", ":", "\n", "            ", "subsample", "=", "np", ".", "ones", "(", "train_args", ".", "elayers", "[", "idx", "]", "+", "1", ",", "dtype", "=", "np", ".", "int", ")", "\n", "if", "train_args", ".", "etype", "[", "idx", "]", ".", "endswith", "(", "\"p\"", ")", "and", "not", "train_args", ".", "etype", "[", "\n", "idx", "\n", "]", ".", "startswith", "(", "\"vgg\"", ")", ":", "\n", "                ", "ss", "=", "train_args", ".", "subsample", "[", "idx", "]", ".", "split", "(", "\"_\"", ")", "\n", "for", "j", "in", "range", "(", "min", "(", "train_args", ".", "elayers", "[", "idx", "]", "+", "1", ",", "len", "(", "ss", ")", ")", ")", ":", "\n", "                    ", "subsample", "[", "j", "]", "=", "int", "(", "ss", "[", "j", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "logging", ".", "warning", "(", "\n", "\"Encoder %d: Subsampling is not performed for vgg*. \"", "\n", "\"It is performed in max pooling layers at CNN.\"", ",", "\n", "idx", "+", "1", ",", "\n", ")", "\n", "", "logging", ".", "info", "(", "\"subsample: \"", "+", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "subsample", "]", ")", ")", "\n", "subsample_list", ".", "append", "(", "subsample", ")", "\n", "", "return", "subsample_list", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid options: mode={}, arch={}\"", ".", "format", "(", "mode", ",", "arch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.rename_state_dict": [[475, 487], ["len", "logging.warning", "state_dict.pop", "k.replace", "k.startswith"], "function", ["None"], ["", "", "def", "rename_state_dict", "(", "\n", "old_prefix", ":", "str", ",", "new_prefix", ":", "str", ",", "state_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", ":", "\n", "    ", "\"\"\"Replace keys of old prefix with new prefix in state dict.\"\"\"", "\n", "# need this list not to break the dict iterator", "\n", "old_keys", "=", "[", "k", "for", "k", "in", "state_dict", "if", "k", ".", "startswith", "(", "old_prefix", ")", "]", "\n", "if", "len", "(", "old_keys", ")", ">", "0", ":", "\n", "        ", "logging", ".", "warning", "(", "f\"Rename: {old_prefix} -> {new_prefix}\"", ")", "\n", "", "for", "k", "in", "old_keys", ":", "\n", "        ", "v", "=", "state_dict", ".", "pop", "(", "k", ")", "\n", "new_k", "=", "k", ".", "replace", "(", "old_prefix", ",", "new_prefix", ")", "\n", "state_dict", "[", "new_k", "]", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.get_activation": [[489, 503], ["None"], "function", ["None"], ["", "", "def", "get_activation", "(", "act", ")", ":", "\n", "    ", "\"\"\"Return activation function.\"\"\"", "\n", "# Lazy load to avoid unused import", "\n", "from", "espnet", ".", "nets", ".", "pytorch_backend", ".", "conformer", ".", "swish", "import", "Swish", "\n", "\n", "activation_funcs", "=", "{", "\n", "\"hardtanh\"", ":", "torch", ".", "nn", ".", "Hardtanh", ",", "\n", "\"tanh\"", ":", "torch", ".", "nn", ".", "Tanh", ",", "\n", "\"relu\"", ":", "torch", ".", "nn", ".", "ReLU", ",", "\n", "\"selu\"", ":", "torch", ".", "nn", ".", "SELU", ",", "\n", "\"swish\"", ":", "Swish", ",", "\n", "}", "\n", "\n", "return", "activation_funcs", "[", "act", "]", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.add_arguments": [[45, 202], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_arguments", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments.\"\"\"", "\n", "group", "=", "parser", ".", "add_argument_group", "(", "\"transformer model setting\"", ")", "\n", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-init\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"pytorch\"", ",", "\n", "choices", "=", "[", "\n", "\"pytorch\"", ",", "\n", "\"xavier_uniform\"", ",", "\n", "\"xavier_normal\"", ",", "\n", "\"kaiming_uniform\"", ",", "\n", "\"kaiming_normal\"", ",", "\n", "]", ",", "\n", "help", "=", "\"how to initialize transformer parameters\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-input-layer\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"conv2d\"", ",", "\n", "choices", "=", "[", "\"conv3d\"", ",", "\"conv2d\"", ",", "\"conv1d\"", ",", "\"linear\"", ",", "\"embed\"", "]", ",", "\n", "help", "=", "\"transformer input layer type\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-encoder-attn-layer-type\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"mha\"", ",", "\n", "choices", "=", "[", "\"mha\"", ",", "\"rel_mha\"", ",", "\"legacy_rel_mha\"", "]", ",", "\n", "help", "=", "\"transformer encoder attention layer type\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-attn-dropout-rate\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"dropout in transformer attention. use --dropout-rate if None is set\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-lr\"", ",", "\n", "default", "=", "10.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Initial value of learning rate\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-warmup-steps\"", ",", "\n", "default", "=", "25000", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"optimizer warmup steps\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--transformer-length-normalized-loss\"", ",", "\n", "default", "=", "True", ",", "\n", "type", "=", "strtobool", ",", "\n", "help", "=", "\"normalize loss by length\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--dropout-rate\"", ",", "\n", "default", "=", "0.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for the encoder\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--macaron-style\"", ",", "\n", "default", "=", "False", ",", "\n", "type", "=", "strtobool", ",", "\n", "help", "=", "\"Whether to use macaron style for positionwise layer\"", ",", "\n", ")", "\n", "# -- input", "\n", "group", ".", "add_argument", "(", "\n", "\"--a-upsample-ratio\"", ",", "\n", "default", "=", "1", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Upsample rate for audio\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--relu-type\"", ",", "\n", "default", "=", "\"swish\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"the type of activation layer\"", ",", "\n", ")", "\n", "# Encoder", "\n", "group", ".", "add_argument", "(", "\n", "\"--elayers\"", ",", "\n", "default", "=", "4", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of encoder layers (for shared recognition part \"", "\n", "\"in multi-speaker asr mode)\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--eunits\"", ",", "\n", "\"-u\"", ",", "\n", "default", "=", "300", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of encoder hidden units\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--use-cnn-module\"", ",", "\n", "default", "=", "False", ",", "\n", "type", "=", "strtobool", ",", "\n", "help", "=", "\"Use convolution module or not\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--cnn-module-kernel\"", ",", "\n", "default", "=", "31", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Kernel size of convolution module.\"", ",", "\n", ")", "\n", "# Attention", "\n", "group", ".", "add_argument", "(", "\n", "\"--adim\"", ",", "\n", "default", "=", "320", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of attention transformation dimensions\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--aheads\"", ",", "\n", "default", "=", "4", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of heads for multi head attention\"", ",", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--zero-triu\"", ",", "\n", "default", "=", "False", ",", "\n", "type", "=", "strtobool", ",", "\n", "help", "=", "\"If true, zero the uppper triangular part of attention matrix.\"", ",", "\n", ")", "\n", "# Relative positional encoding", "\n", "group", ".", "add_argument", "(", "\n", "\"--rel-pos-type\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"legacy\"", ",", "\n", "choices", "=", "[", "\"legacy\"", ",", "\"latest\"", "]", ",", "\n", "help", "=", "\"Whether to use the latest relative positional encoding or the legacy one.\"", "\n", "\"The legacy relative positional encoding will be deprecated in the future.\"", "\n", "\"More Details can be found in https://github.com/espnet/espnet/pull/2816.\"", ",", "\n", ")", "\n", "# Decoder", "\n", "group", ".", "add_argument", "(", "\n", "\"--dlayers\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "\"Number of decoder layers\"", "\n", ")", "\n", "group", ".", "add_argument", "(", "\n", "\"--dunits\"", ",", "default", "=", "320", ",", "type", "=", "int", ",", "help", "=", "\"Number of decoder hidden units\"", "\n", ")", "\n", "# -- pretrain", "\n", "group", ".", "add_argument", "(", "\"--pretrain-dataset\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'pre-trained dataset for encoder'", "\n", ")", "\n", "# -- custom name", "\n", "group", ".", "add_argument", "(", "\"--custom-pretrain-name\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'pre-trained model for encoder'", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.attention_plot_class": [[203, 207], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "attention_plot_class", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return PlotAttentionReport.\"\"\"", "\n", "return", "PlotAttentionReport", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.__init__": [[208, 297], ["torch.nn.Module.__init__", "getattr", "espnet.nets.pytorch_backend.transformer.encoder.Encoder", "espnet.nets.pytorch_backend.nets_utils.get_subsample", "espnet.nets.pytorch_backend.transformer.label_smoothing_loss.LabelSmoothingLoss", "logging.warning", "espnet.nets.pytorch_backend.transformer.decoder.Decoder", "espnet.nets.pytorch_backend.ctc.CTC", "espnet.nets.e2e_asr_common.ErrorCalculator", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.get_subsample"], ["", "def", "__init__", "(", "self", ",", "odim", ",", "args", ",", "ignore_id", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Construct an E2E object.\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        \"\"\"", "\n", "torch", ".", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "if", "args", ".", "transformer_attn_dropout_rate", "is", "None", ":", "\n", "            ", "args", ".", "transformer_attn_dropout_rate", "=", "args", ".", "dropout_rate", "\n", "# Check the relative positional encoding type", "\n", "", "self", ".", "rel_pos_type", "=", "getattr", "(", "args", ",", "\"rel_pos_type\"", ",", "None", ")", "\n", "if", "self", ".", "rel_pos_type", "is", "None", "and", "args", ".", "transformer_encoder_attn_layer_type", "==", "\"rel_mha\"", ":", "\n", "            ", "args", ".", "transformer_encoder_attn_layer_type", "=", "\"legacy_rel_mha\"", "\n", "logging", ".", "warning", "(", "\n", "\"Using legacy_rel_pos and it will be deprecated in the future.\"", "\n", ")", "\n", "\n", "", "idim", "=", "80", "\n", "\n", "self", ".", "encoder", "=", "Encoder", "(", "\n", "idim", "=", "idim", ",", "\n", "attention_dim", "=", "args", ".", "adim", ",", "\n", "attention_heads", "=", "args", ".", "aheads", ",", "\n", "linear_units", "=", "args", ".", "eunits", ",", "\n", "num_blocks", "=", "args", ".", "elayers", ",", "\n", "input_layer", "=", "args", ".", "transformer_input_layer", ",", "\n", "dropout_rate", "=", "args", ".", "dropout_rate", ",", "\n", "positional_dropout_rate", "=", "args", ".", "dropout_rate", ",", "\n", "attention_dropout_rate", "=", "args", ".", "transformer_attn_dropout_rate", ",", "\n", "encoder_attn_layer_type", "=", "args", ".", "transformer_encoder_attn_layer_type", ",", "\n", "macaron_style", "=", "args", ".", "macaron_style", ",", "\n", "use_cnn_module", "=", "args", ".", "use_cnn_module", ",", "\n", "cnn_module_kernel", "=", "args", ".", "cnn_module_kernel", ",", "\n", "zero_triu", "=", "getattr", "(", "args", ",", "\"zero_triu\"", ",", "False", ")", ",", "\n", "a_upsample_ratio", "=", "args", ".", "a_upsample_ratio", ",", "\n", "relu_type", "=", "getattr", "(", "args", ",", "\"relu_type\"", ",", "\"swish\"", ")", ",", "\n", ")", "\n", "\n", "self", ".", "transformer_input_layer", "=", "args", ".", "transformer_input_layer", "\n", "self", ".", "a_upsample_ratio", "=", "args", ".", "a_upsample_ratio", "\n", "\n", "if", "args", ".", "mtlalpha", "<", "1", ":", "\n", "            ", "self", ".", "decoder", "=", "Decoder", "(", "\n", "odim", "=", "odim", ",", "\n", "attention_dim", "=", "args", ".", "adim", ",", "\n", "attention_heads", "=", "args", ".", "aheads", ",", "\n", "linear_units", "=", "args", ".", "dunits", ",", "\n", "num_blocks", "=", "args", ".", "dlayers", ",", "\n", "dropout_rate", "=", "args", ".", "dropout_rate", ",", "\n", "positional_dropout_rate", "=", "args", ".", "dropout_rate", ",", "\n", "self_attention_dropout_rate", "=", "args", ".", "transformer_attn_dropout_rate", ",", "\n", "src_attention_dropout_rate", "=", "args", ".", "transformer_attn_dropout_rate", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "decoder", "=", "None", "\n", "", "self", ".", "blank", "=", "0", "\n", "self", ".", "sos", "=", "odim", "-", "1", "\n", "self", ".", "eos", "=", "odim", "-", "1", "\n", "self", ".", "odim", "=", "odim", "\n", "self", ".", "ignore_id", "=", "ignore_id", "\n", "self", ".", "subsample", "=", "get_subsample", "(", "args", ",", "mode", "=", "\"asr\"", ",", "arch", "=", "\"transformer\"", ")", "\n", "\n", "# self.lsm_weight = a", "\n", "self", ".", "criterion", "=", "LabelSmoothingLoss", "(", "\n", "self", ".", "odim", ",", "\n", "self", ".", "ignore_id", ",", "\n", "args", ".", "lsm_weight", ",", "\n", "args", ".", "transformer_length_normalized_loss", ",", "\n", ")", "\n", "\n", "self", ".", "adim", "=", "args", ".", "adim", "\n", "self", ".", "mtlalpha", "=", "args", ".", "mtlalpha", "\n", "if", "args", ".", "mtlalpha", ">", "0.0", ":", "\n", "            ", "self", ".", "ctc", "=", "CTC", "(", "\n", "odim", ",", "args", ".", "adim", ",", "args", ".", "dropout_rate", ",", "ctc_type", "=", "args", ".", "ctc_type", ",", "reduce", "=", "True", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "ctc", "=", "None", "\n", "\n", "", "if", "args", ".", "report_cer", "or", "args", ".", "report_wer", ":", "\n", "            ", "self", ".", "error_calculator", "=", "ErrorCalculator", "(", "\n", "args", ".", "char_list", ",", "\n", "args", ".", "sym_space", ",", "\n", "args", ".", "sym_blank", ",", "\n", "args", ".", "report_cer", ",", "\n", "args", ".", "report_wer", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "error_calculator", "=", "None", "\n", "", "self", ".", "rnnlm", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.scorers": [[298, 301], ["dict", "espnet.nets.scorers.ctc.CTCPrefixScorer"], "methods", ["None"], ["", "def", "scorers", "(", "self", ")", ":", "\n", "        ", "\"\"\"Scorers.\"\"\"", "\n", "return", "dict", "(", "decoder", "=", "self", ".", "decoder", ",", "ctc", "=", "CTCPrefixScorer", "(", "self", ".", "ctc", ",", "self", ".", "eos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.e2e_asr_transformer.E2E.encode": [[302, 321], ["e2e_asr_transformer.E2E.eval", "torch.as_tensor().unsqueeze", "e2e_asr_transformer.E2E.encoder", "e2e_asr_transformer.E2E.squeeze", "e2e_asr_transformer.E2E.encoder", "enc_output.squeeze", "torch.as_tensor"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "x", ",", "extract_resnet_feats", "=", "False", ")", ":", "\n", "        ", "\"\"\"Encode acoustic features.\n\n        :param ndarray x: source acoustic feature (T, D)\n        :return: encoder outputs\n        :rtype: torch.Tensor\n        \"\"\"", "\n", "self", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "as_tensor", "(", "x", ")", ".", "unsqueeze", "(", "0", ")", "\n", "if", "extract_resnet_feats", ":", "\n", "            ", "resnet_feats", "=", "self", ".", "encoder", "(", "\n", "x", ",", "\n", "None", ",", "\n", "extract_resnet_feats", "=", "extract_resnet_feats", ",", "\n", ")", "\n", "return", "resnet_feats", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "enc_output", ",", "_", "=", "self", ".", "encoder", "(", "x", ",", "None", ")", "\n", "return", "enc_output", ".", "squeeze", "(", "0", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.__init__": [[22, 60], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "distutils.version.LooseVersion", "distutils.version.LooseVersion", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "torch.nn.CTCLoss", "warp_ctc.CTCLoss", "ValueError"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["\n", "self", ".", "ctc", "=", "ctc", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "impl", "=", "None", "\n", "\n", "", "def", "init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", ".", "detach", "(", ")", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# TODO(karita): use CTCPrefixScoreTH", "\n", "self", ".", "impl", "=", "CTCPrefixScore", "(", "logp", ",", "0", ",", "self", ".", "eos", ",", "np", ")", "\n", "return", "0", ",", "self", ".", "impl", ".", "initial_state", "(", ")", "\n", "\n", "", "def", "select_state", "(", "self", ",", "state", ",", "i", ",", "new_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n            new_id (int): New label id to select a state if necessary\n\n        Returns:\n            state: pruned state\n\n        \"\"\"", "\n", "if", "type", "(", "state", ")", "==", "tuple", ":", "\n", "            ", "if", "len", "(", "state", ")", "==", "2", ":", "# for CTCPrefixScore", "\n", "                ", "sc", ",", "st", "=", "state", "\n", "return", "sc", "[", "i", "]", ",", "st", "[", "i", "]", "\n", "", "else", ":", "# for CTCPrefixScoreTH (need new_id > 0)", "\n", "                ", "r", ",", "log_psi", ",", "f_min", ",", "f_max", ",", "scoring_idmap", "=", "state", "\n", "s", "=", "log_psi", "[", "i", ",", "new_id", "]", ".", "expand", "(", "log_psi", ".", "size", "(", "1", ")", ")", "\n", "if", "scoring_idmap", "is", "not", "None", ":", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.loss_fn": [[61, 79], ["th_pred.log_softmax.log_softmax.log_softmax", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "torch.backends.cudnn.flags", "ctc.CTC.ctc_loss", "th_pred.log_softmax.log_softmax.size", "ctc.CTC.ctc_loss", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "ctc.CTC.ctc_loss", "t.tolist"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["                    ", "return", "r", "[", ":", ",", ":", ",", "i", ",", "scoring_idmap", "[", "i", ",", "new_id", "]", "]", ",", "s", ",", "f_min", ",", "f_max", "\n", "", "else", ":", "\n", "                    ", "return", "r", "[", ":", ",", ":", ",", "i", ",", "new_id", "]", ",", "s", ",", "f_min", ",", "f_max", "\n", "", "", "", "return", "None", "if", "state", "is", "None", "else", "state", "[", "i", "]", "\n", "\n", "", "def", "score_partial", "(", "self", ",", "y", ",", "ids", ",", "state", ",", "x", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.forward": [[80, 148], ["ctc.CTC.ctc_lo", "logging.info", "logging.info", "ctc.CTC.dropout", "ys_hat.to.to.transpose", "espnet.nets.pytorch_backend.nets_utils.to_device", "torch.from_numpy.long", "torch.from_numpy.long", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ctc.CTC.loss_fn", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.cat().cpu().int", "torch.cat().cpu().int", "torch.cat().cpu().int", "torch.cat().cpu().int", "espnet.nets.pytorch_backend.nets_utils.to_device().to", "ctc.CTC.loss.sum", "logging.info", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.fromiter", "numpy.fromiter", "ys_hat.to.to.to", "espnet.nets.pytorch_backend.nets_utils.to_device", "torch.cat().cpu", "torch.cat().cpu", "torch.cat().cpu", "torch.cat().cpu", "espnet.nets.pytorch_backend.nets_utils.to_device", "str().split", "str().split", "str", "len", "x.size", "ctc.CTC.loss_fn", "float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "str", "str"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.loss_fn", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.loss_fn"], ["\n", "prev_score", ",", "state", "=", "state", "\n", "presub_score", ",", "new_st", "=", "self", ".", "impl", "(", "y", ".", "cpu", "(", ")", ",", "ids", ".", "cpu", "(", ")", ",", "state", ")", "\n", "tscore", "=", "torch", ".", "as_tensor", "(", "\n", "presub_score", "-", "prev_score", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", "\n", ")", "\n", "return", "tscore", ",", "(", "presub_score", ",", "new_st", ")", "\n", "\n", "", "def", "batch_init_state", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", "# assuming batch_size = 1", "\n", "xlen", "=", "torch", ".", "tensor", "(", "[", "logp", ".", "size", "(", "1", ")", "]", ")", "\n", "self", ".", "impl", "=", "CTCPrefixScoreTH", "(", "logp", ",", "xlen", ",", "0", ",", "self", ".", "eos", ")", "\n", "return", "None", "\n", "\n", "", "def", "batch_score_partial", "(", "self", ",", "y", ",", "ids", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            ids (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        \"\"\"", "\n", "batch_state", "=", "(", "\n", "(", "\n", "torch", ".", "stack", "(", "[", "s", "[", "0", "]", "for", "s", "in", "state", "]", ",", "dim", "=", "2", ")", ",", "\n", "torch", ".", "stack", "(", "[", "s", "[", "1", "]", "for", "s", "in", "state", "]", ")", ",", "\n", "state", "[", "0", "]", "[", "2", "]", ",", "\n", "state", "[", "0", "]", "[", "3", "]", ",", "\n", ")", "\n", "if", "state", "[", "0", "]", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "return", "self", ".", "impl", "(", "y", ",", "batch_state", ",", "ids", ")", "\n", "\n", "", "def", "extend_prob", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Extend probs for decoding.\n\n        This extension is for streaming decoding\n        as in Eq (14) in https://arxiv.org/abs/2006.14941\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        \"\"\"", "\n", "logp", "=", "self", ".", "ctc", ".", "log_softmax", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", "\n", "self", ".", "impl", ".", "extend_prob", "(", "logp", ")", "\n", "\n", "", "def", "extend_state", "(", "self", ",", "state", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.softmax": [[149, 158], ["torch.softmax", "torch.softmax", "ctc.CTC.ctc_lo"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.softmax"], ["\n", "new_state", "=", "[", "]", "\n", "for", "s", "in", "state", ":", "\n", "            ", "new_state", ".", "append", "(", "self", ".", "impl", ".", "extend_state", "(", "s", ")", ")", "\n", "\n", "", "return", "new_state", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax": [[159, 167], ["torch.log_softmax", "torch.log_softmax", "ctc.CTC.ctc_lo"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax": [[168, 176], ["torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "ctc.CTC.ctc_lo"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.argmax"], []], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.forced_align": [[177, 241], ["ctc.CTC.log_softmax", "lpz.squeeze.squeeze.squeeze", "ctc.CTC.forced_align.interpolate_blank"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], []], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.ctc_for": [[243, 283], ["getattr", "ctc.CTC", "torch.nn.ModuleList", "torch.nn.ModuleList", "ValueError", "ctc.CTC", "torch.nn.ModuleList.append", "range", "ctc.CTC", "torch.nn.ModuleList.append"], "function", ["None"], []], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.positionwise_feed_forward.PositionwiseFeedForward.__init__": [[21, 27], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "idim", ",", "hidden_units", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"Construct an PositionwiseFeedForward object.\"\"\"", "\n", "super", "(", "PositionwiseFeedForward", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "torch", ".", "nn", ".", "Linear", "(", "idim", ",", "hidden_units", ")", "\n", "self", ".", "w_2", "=", "torch", ".", "nn", ".", "Linear", "(", "hidden_units", ",", "idim", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.positionwise_feed_forward.PositionwiseFeedForward.forward": [[28, 31], ["positionwise_feed_forward.PositionwiseFeedForward.w_2", "positionwise_feed_forward.PositionwiseFeedForward.dropout", "torch.relu", "positionwise_feed_forward.PositionwiseFeedForward.w_1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward funciton.\"\"\"", "\n", "return", "self", ".", "w_2", "(", "self", ".", "dropout", "(", "torch", ".", "relu", "(", "self", ".", "w_1", "(", "x", ")", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder.Encoder.__init__": [[82, 229], ["super().__init__", "encoder.Encoder._register_load_state_dict_pre_hook", "espnet.nets.pytorch_backend.transformer.repeat.repeat", "espnet.nets.pytorch_backend.backbones.conv1d_extractor.Conv1dResNet", "torch.nn.Sequential", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.backbones.conv3d_extractor.Conv3dResNet", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.ReLU", "pos_enc_class", "espnet.nets.pytorch_backend.transformer.subsampling.Conv2dSubsampling", "espnet.nets.pytorch_backend.transformer.encoder_layer.EncoderLayer", "pos_enc_class", "VGG2L", "NotImplementedError", "ValueError", "encoder_attn_layer", "positionwise_layer", "torch.nn.Sequential", "isinstance", "convolution_layer", "torch.nn.Embedding", "pos_enc_class", "torch.nn.Sequential", "pos_enc_class", "torch.nn.Sequential", "torch.nn.Linear", "pos_enc_class", "torch.nn.Sequential", "ValueError", "pos_enc_class"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat"], ["def", "__init__", "(", "\n", "self", ",", "\n", "idim", ",", "\n", "attention_dim", "=", "256", ",", "\n", "attention_heads", "=", "4", ",", "\n", "linear_units", "=", "2048", ",", "\n", "num_blocks", "=", "6", ",", "\n", "dropout_rate", "=", "0.1", ",", "\n", "positional_dropout_rate", "=", "0.1", ",", "\n", "attention_dropout_rate", "=", "0.0", ",", "\n", "input_layer", "=", "\"conv2d\"", ",", "\n", "pos_enc_class", "=", "PositionalEncoding", ",", "\n", "normalize_before", "=", "True", ",", "\n", "concat_after", "=", "False", ",", "\n", "positionwise_layer_type", "=", "\"linear\"", ",", "\n", "positionwise_conv_kernel_size", "=", "1", ",", "\n", "macaron_style", "=", "False", ",", "\n", "encoder_attn_layer_type", "=", "\"mha\"", ",", "\n", "use_cnn_module", "=", "False", ",", "\n", "zero_triu", "=", "False", ",", "\n", "cnn_module_kernel", "=", "31", ",", "\n", "padding_idx", "=", "-", "1", ",", "\n", "relu_type", "=", "\"prelu\"", ",", "\n", "a_upsample_ratio", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an Encoder object.\"\"\"", "\n", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_register_load_state_dict_pre_hook", "(", "_pre_hook", ")", "\n", "\n", "if", "encoder_attn_layer_type", "==", "\"rel_mha\"", ":", "\n", "            ", "pos_enc_class", "=", "RelPositionalEncoding", "\n", "", "elif", "encoder_attn_layer_type", "==", "\"legacy_rel_mha\"", ":", "\n", "            ", "pos_enc_class", "=", "LegacyRelPositionalEncoding", "\n", "# -- frontend module.", "\n", "", "if", "input_layer", "==", "\"conv1d\"", ":", "\n", "            ", "self", ".", "frontend", "=", "Conv1dResNet", "(", "\n", "relu_type", "=", "relu_type", ",", "\n", "a_upsample_ratio", "=", "a_upsample_ratio", ",", "\n", ")", "\n", "", "elif", "input_layer", "==", "\"conv3d\"", ":", "\n", "            ", "self", ".", "frontend", "=", "Conv3dResNet", "(", "relu_type", "=", "relu_type", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "frontend", "=", "None", "\n", "# -- backend module.", "\n", "", "if", "input_layer", "==", "\"linear\"", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "idim", ",", "attention_dim", ")", ",", "\n", "torch", ".", "nn", ".", "LayerNorm", "(", "attention_dim", ")", ",", "\n", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "input_layer", "==", "\"conv2d\"", ":", "\n", "            ", "self", ".", "embed", "=", "Conv2dSubsampling", "(", "\n", "idim", ",", "\n", "attention_dim", ",", "\n", "dropout_rate", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "input_layer", "==", "\"vgg2l\"", ":", "\n", "            ", "self", ".", "embed", "=", "VGG2L", "(", "idim", ",", "attention_dim", ")", "\n", "", "elif", "input_layer", "==", "\"embed\"", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Embedding", "(", "idim", ",", "attention_dim", ",", "padding_idx", "=", "padding_idx", ")", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "isinstance", "(", "input_layer", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "input_layer", ",", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "input_layer", "in", "[", "\"conv1d\"", ",", "\"conv3d\"", "]", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "512", ",", "attention_dim", ")", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", "\n", ")", "\n", "", "elif", "input_layer", "is", "None", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown input_layer: \"", "+", "input_layer", ")", "\n", "", "self", ".", "normalize_before", "=", "normalize_before", "\n", "if", "positionwise_layer_type", "==", "\"linear\"", ":", "\n", "            ", "positionwise_layer", "=", "PositionwiseFeedForward", "\n", "positionwise_layer_args", "=", "(", "attention_dim", ",", "linear_units", ",", "dropout_rate", ")", "\n", "", "elif", "positionwise_layer_type", "==", "\"conv1d\"", ":", "\n", "            ", "positionwise_layer", "=", "MultiLayeredConv1d", "\n", "positionwise_layer_args", "=", "(", "\n", "attention_dim", ",", "\n", "linear_units", ",", "\n", "positionwise_conv_kernel_size", ",", "\n", "dropout_rate", ",", "\n", ")", "\n", "", "elif", "positionwise_layer_type", "==", "\"conv1d-linear\"", ":", "\n", "            ", "positionwise_layer", "=", "Conv1dLinear", "\n", "positionwise_layer_args", "=", "(", "\n", "attention_dim", ",", "\n", "linear_units", ",", "\n", "positionwise_conv_kernel_size", ",", "\n", "dropout_rate", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Support only linear or conv1d.\"", ")", "\n", "\n", "", "if", "encoder_attn_layer_type", "==", "\"mha\"", ":", "\n", "            ", "encoder_attn_layer", "=", "MultiHeadedAttention", "\n", "encoder_attn_layer_args", "=", "(", "\n", "attention_heads", ",", "\n", "attention_dim", ",", "\n", "attention_dropout_rate", ",", "\n", ")", "\n", "", "elif", "encoder_attn_layer_type", "==", "\"legacy_rel_mha\"", ":", "\n", "            ", "encoder_attn_layer", "=", "LegacyRelPositionMultiHeadedAttention", "\n", "encoder_attn_layer_args", "=", "(", "\n", "attention_heads", ",", "\n", "attention_dim", ",", "\n", "attention_dropout_rate", ",", "\n", ")", "\n", "", "elif", "encoder_attn_layer_type", "==", "\"rel_mha\"", ":", "\n", "            ", "encoder_attn_layer", "=", "RelPositionMultiHeadedAttention", "\n", "encoder_attn_layer_args", "=", "(", "\n", "attention_heads", ",", "\n", "attention_dim", ",", "\n", "attention_dropout_rate", ",", "\n", "zero_triu", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown encoder_attn_layer: \"", "+", "encoder_attn_layer", ")", "\n", "\n", "", "convolution_layer", "=", "ConvolutionModule", "\n", "convolution_layer_args", "=", "(", "attention_dim", ",", "cnn_module_kernel", ")", "\n", "\n", "self", ".", "encoders", "=", "repeat", "(", "\n", "num_blocks", ",", "\n", "lambda", ":", "EncoderLayer", "(", "\n", "attention_dim", ",", "\n", "encoder_attn_layer", "(", "*", "encoder_attn_layer_args", ")", ",", "\n", "positionwise_layer", "(", "*", "positionwise_layer_args", ")", ",", "\n", "convolution_layer", "(", "*", "convolution_layer_args", ")", "if", "use_cnn_module", "else", "None", ",", "\n", "dropout_rate", ",", "\n", "normalize_before", ",", "\n", "concat_after", ",", "\n", "macaron_style", ",", "\n", ")", ",", "\n", ")", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "self", ".", "after_norm", "=", "LayerNorm", "(", "attention_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder.Encoder.forward": [[230, 258], ["isinstance", "isinstance", "encoder.Encoder.encoders", "isinstance", "encoder.Encoder.frontend", "encoder.Encoder.embed", "encoder.Encoder.embed", "encoder.Encoder.after_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "xs", ",", "masks", ",", "extract_resnet_feats", "=", "False", ")", ":", "\n", "        ", "\"\"\"Encode input sequence.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :param str extract_features: the position for feature extraction\n        :return: position embedded tensor and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "frontend", ",", "(", "Conv1dResNet", ",", "Conv3dResNet", ")", ")", ":", "\n", "            ", "xs", "=", "self", ".", "frontend", "(", "xs", ")", "\n", "", "if", "extract_resnet_feats", ":", "\n", "            ", "return", "xs", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "embed", ",", "Conv2dSubsampling", ")", ":", "\n", "            ", "xs", ",", "masks", "=", "self", ".", "embed", "(", "xs", ",", "masks", ")", "\n", "", "else", ":", "\n", "            ", "xs", "=", "self", ".", "embed", "(", "xs", ")", "\n", "\n", "", "xs", ",", "masks", "=", "self", ".", "encoders", "(", "xs", ",", "masks", ")", "\n", "\n", "if", "isinstance", "(", "xs", ",", "tuple", ")", ":", "\n", "            ", "xs", "=", "xs", "[", "0", "]", "\n", "\n", "", "if", "self", ".", "normalize_before", ":", "\n", "            ", "xs", "=", "self", ".", "after_norm", "(", "xs", ")", "\n", "\n", "", "return", "xs", ",", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder.Encoder.forward_one_step": [[259, 284], ["isinstance", "isinstance", "zip", "encoder.Encoder.frontend", "encoder.Encoder.embed", "encoder.Encoder.embed", "e", "new_cache.append", "encoder.Encoder.after_norm", "range", "len"], "methods", ["None"], ["", "def", "forward_one_step", "(", "self", ",", "xs", ",", "masks", ",", "cache", "=", "None", ")", ":", "\n", "        ", "\"\"\"Encode input frame.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :param List[torch.Tensor] cache: cache tensors\n        :return: position embedded tensor, mask and new cache\n        :rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "frontend", ",", "(", "Conv1dResNet", ",", "Conv3dResNet", ")", ")", ":", "\n", "            ", "xs", "=", "self", ".", "frontend", "(", "xs", ")", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "embed", ",", "Conv2dSubsampling", ")", ":", "\n", "            ", "xs", ",", "masks", "=", "self", ".", "embed", "(", "xs", ",", "masks", ")", "\n", "", "else", ":", "\n", "            ", "xs", "=", "self", ".", "embed", "(", "xs", ")", "\n", "", "if", "cache", "is", "None", ":", "\n", "            ", "cache", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "self", ".", "encoders", ")", ")", "]", "\n", "", "new_cache", "=", "[", "]", "\n", "for", "c", ",", "e", "in", "zip", "(", "cache", ",", "self", ".", "encoders", ")", ":", "\n", "            ", "xs", ",", "masks", "=", "e", "(", "xs", ",", "masks", ",", "cache", "=", "c", ")", "\n", "new_cache", ".", "append", "(", "xs", ")", "\n", "", "if", "self", ".", "normalize_before", ":", "\n", "            ", "xs", "=", "self", ".", "after_norm", "(", "xs", ")", "\n", "", "return", "xs", ",", "masks", ",", "new_cache", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder._pre_hook": [[39, 52], ["espnet.nets.pytorch_backend.nets_utils.rename_state_dict", "espnet.nets.pytorch_backend.nets_utils.rename_state_dict"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.rename_state_dict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.rename_state_dict"], ["def", "_pre_hook", "(", "\n", "state_dict", ",", "\n", "prefix", ",", "\n", "local_metadata", ",", "\n", "strict", ",", "\n", "missing_keys", ",", "\n", "unexpected_keys", ",", "\n", "error_msgs", ",", "\n", ")", ":", "\n", "# https://github.com/espnet/espnet/commit/21d70286c354c66c0350e65dc098d2ee236faccc#diff-bffb1396f038b317b2b64dd96e6d3563", "\n", "    ", "rename_state_dict", "(", "prefix", "+", "\"input_layer.\"", ",", "prefix", "+", "\"embed.\"", ",", "state_dict", ")", "\n", "# https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563", "\n", "rename_state_dict", "(", "prefix", "+", "\"norm.\"", ",", "prefix", "+", "\"after_norm.\"", ",", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder_layer.EncoderLayer.__init__": [[40, 74], ["torch.nn.Module.__init__", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "torch.nn.Dropout", "copy.deepcopy", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "size", ",", "\n", "self_attn", ",", "\n", "feed_forward", ",", "\n", "conv_module", ",", "\n", "dropout_rate", ",", "\n", "normalize_before", "=", "True", ",", "\n", "concat_after", "=", "False", ",", "\n", "macaron_style", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an EncoderLayer object.\"\"\"", "\n", "super", "(", "EncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "self_attn", "\n", "self", ".", "feed_forward", "=", "feed_forward", "\n", "self", ".", "ff_scale", "=", "1.0", "\n", "self", ".", "conv_module", "=", "conv_module", "\n", "self", ".", "macaron_style", "=", "macaron_style", "\n", "self", ".", "norm_ff", "=", "LayerNorm", "(", "size", ")", "# for the FNN module", "\n", "self", ".", "norm_mha", "=", "LayerNorm", "(", "size", ")", "# for the MHA module", "\n", "if", "self", ".", "macaron_style", ":", "\n", "            ", "self", ".", "feed_forward_macaron", "=", "copy", ".", "deepcopy", "(", "feed_forward", ")", "\n", "self", ".", "ff_scale", "=", "0.5", "\n", "# for another FNN module in macaron style", "\n", "self", ".", "norm_ff_macaron", "=", "LayerNorm", "(", "size", ")", "\n", "", "if", "self", ".", "conv_module", "is", "not", "None", ":", "\n", "            ", "self", ".", "norm_conv", "=", "LayerNorm", "(", "size", ")", "# for the CNN module", "\n", "self", ".", "norm_final", "=", "LayerNorm", "(", "size", ")", "# for the final output of the block", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "normalize_before", "=", "normalize_before", "\n", "self", ".", "concat_after", "=", "concat_after", "\n", "if", "self", ".", "concat_after", ":", "\n", "            ", "self", ".", "concat_linear", "=", "nn", ".", "Linear", "(", "size", "+", "size", ",", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.encoder_layer.EncoderLayer.forward": [[75, 150], ["isinstance", "encoder_layer.EncoderLayer.norm_mha", "encoder_layer.EncoderLayer.self_attn", "encoder_layer.EncoderLayer.self_attn", "torch.cat", "encoder_layer.EncoderLayer.norm_mha", "encoder_layer.EncoderLayer.norm_ff", "encoder_layer.EncoderLayer.norm_ff", "encoder_layer.EncoderLayer.norm_final", "torch.cat", "encoder_layer.EncoderLayer.norm_ff_macaron", "encoder_layer.EncoderLayer.norm_ff_macaron", "encoder_layer.EncoderLayer.concat_linear", "encoder_layer.EncoderLayer.dropout", "encoder_layer.EncoderLayer.norm_conv", "encoder_layer.EncoderLayer.dropout", "encoder_layer.EncoderLayer.norm_conv", "encoder_layer.EncoderLayer.dropout", "encoder_layer.EncoderLayer.dropout", "encoder_layer.EncoderLayer.conv_module", "encoder_layer.EncoderLayer.feed_forward", "encoder_layer.EncoderLayer.feed_forward_macaron"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_input", ",", "mask", ",", "cache", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute encoded features.\n\n        :param torch.Tensor x_input: encoded source features (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"", "\n", "if", "isinstance", "(", "x_input", ",", "tuple", ")", ":", "\n", "            ", "x", ",", "pos_emb", "=", "x_input", "[", "0", "]", ",", "x_input", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "x", ",", "pos_emb", "=", "x_input", ",", "None", "\n", "\n", "# whether to use macaron style", "\n", "", "if", "self", ".", "macaron_style", ":", "\n", "            ", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "                ", "x", "=", "self", ".", "norm_ff_macaron", "(", "x", ")", "\n", "", "x", "=", "residual", "+", "self", ".", "ff_scale", "*", "self", ".", "dropout", "(", "self", ".", "feed_forward_macaron", "(", "x", ")", ")", "\n", "if", "not", "self", ".", "normalize_before", ":", "\n", "                ", "x", "=", "self", ".", "norm_ff_macaron", "(", "x", ")", "\n", "\n", "# multi-headed self-attention module", "\n", "", "", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm_mha", "(", "x", ")", "\n", "\n", "", "if", "cache", "is", "None", ":", "\n", "            ", "x_q", "=", "x", "\n", "", "else", ":", "\n", "            ", "assert", "cache", ".", "shape", "==", "(", "x", ".", "shape", "[", "0", "]", ",", "x", ".", "shape", "[", "1", "]", "-", "1", ",", "self", ".", "size", ")", "\n", "x_q", "=", "x", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "residual", "=", "residual", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "mask", "=", "None", "if", "mask", "is", "None", "else", "mask", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "", "if", "pos_emb", "is", "not", "None", ":", "\n", "            ", "x_att", "=", "self", ".", "self_attn", "(", "x_q", ",", "x", ",", "x", ",", "pos_emb", ",", "mask", ")", "\n", "", "else", ":", "\n", "            ", "x_att", "=", "self", ".", "self_attn", "(", "x_q", ",", "x", ",", "x", ",", "mask", ")", "\n", "\n", "", "if", "self", ".", "concat_after", ":", "\n", "            ", "x_concat", "=", "torch", ".", "cat", "(", "(", "x", ",", "x_att", ")", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "residual", "+", "self", ".", "concat_linear", "(", "x_concat", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "residual", "+", "self", ".", "dropout", "(", "x_att", ")", "\n", "", "if", "not", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm_mha", "(", "x", ")", "\n", "\n", "# convolution module", "\n", "", "if", "self", ".", "conv_module", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "                ", "x", "=", "self", ".", "norm_conv", "(", "x", ")", "\n", "", "x", "=", "residual", "+", "self", ".", "dropout", "(", "self", ".", "conv_module", "(", "x", ")", ")", "\n", "if", "not", "self", ".", "normalize_before", ":", "\n", "                ", "x", "=", "self", ".", "norm_conv", "(", "x", ")", "\n", "\n", "# feed forward module", "\n", "", "", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm_ff", "(", "x", ")", "\n", "", "x", "=", "residual", "+", "self", ".", "ff_scale", "*", "self", ".", "dropout", "(", "self", ".", "feed_forward", "(", "x", ")", ")", "\n", "if", "not", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm_ff", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "conv_module", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "norm_final", "(", "x", ")", "\n", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "cache", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "if", "pos_emb", "is", "not", "None", ":", "\n", "            ", "return", "(", "x", ",", "pos_emb", ")", ",", "mask", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "mask", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.raw_embeddings.VideoEmbedding.__init__": [[16, 25], ["super().__init__", "espnet.nets.pytorch_backend.backbones.conv3d_extractor.Conv3dResNet", "torch.nn.Sequential", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "idim", ",", "odim", ",", "dropout_rate", ",", "pos_enc_class", ",", "backbone_type", "=", "\"resnet\"", ",", "relu_type", "=", "\"prelu\"", ")", ":", "\n", "        ", "super", "(", "VideoEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "trunk", "=", "Conv3dResNet", "(", "\n", "backbone_type", "=", "backbone_type", ",", "\n", "relu_type", "=", "relu_type", "\n", ")", "\n", "self", ".", "out", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "idim", ",", "odim", ")", ",", "\n", "pos_enc_class", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.raw_embeddings.VideoEmbedding.forward": [[27, 42], ["raw_embeddings.VideoEmbedding.trunk", "raw_embeddings.VideoEmbedding.out"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ",", "extract_feats", "=", "None", ")", ":", "\n", "        ", "\"\"\"video embedding for x\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :param str extract_features: the position for feature extraction\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"", "\n", "x_resnet", ",", "x_mask", "=", "self", ".", "trunk", "(", "x", ",", "x_mask", ")", "\n", "x", "=", "self", ".", "out", "(", "x_resnet", ")", "\n", "if", "extract_feats", ":", "\n", "            ", "return", "x", ",", "x_mask", ",", "x_resnet", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "x_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.raw_embeddings.AudioEmbedding.__init__": [[52, 61], ["super().__init__", "espnet.nets.pytorch_backend.backbones.conv1d_extractor.Conv1dResNet", "torch.nn.Sequential", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "idim", ",", "odim", ",", "dropout_rate", ",", "pos_enc_class", ",", "relu_type", "=", "\"prelu\"", ",", "a_upsample_ratio", "=", "1", ")", ":", "\n", "        ", "super", "(", "AudioEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "trunk", "=", "Conv1dResNet", "(", "\n", "relu_type", "=", "relu_type", ",", "\n", "a_upsample_ratio", "=", "a_upsample_ratio", ",", "\n", ")", "\n", "self", ".", "out", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "idim", ",", "odim", ")", ",", "\n", "pos_enc_class", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.raw_embeddings.AudioEmbedding.forward": [[63, 78], ["raw_embeddings.AudioEmbedding.trunk", "raw_embeddings.AudioEmbedding.out"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ",", "extract_feats", "=", "None", ")", ":", "\n", "        ", "\"\"\"audio embedding for x\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :param str extract_features: the position for feature extraction\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n        \"\"\"", "\n", "x_resnet", ",", "x_mask", "=", "self", ".", "trunk", "(", "x", ",", "x_mask", ")", "\n", "x", "=", "self", ".", "out", "(", "x_resnet", ")", "\n", "if", "extract_feats", ":", "\n", "            ", "return", "x", ",", "x_mask", ",", "x_resnet", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "x_mask", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.add_sos_eos.add_sos_eos": [[12, 32], ["ys_pad.new", "ys_pad.new", "torch.cat", "torch.cat", "pad_list", "pad_list"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.pad_list", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.pad_list"], ["def", "add_sos_eos", "(", "ys_pad", ",", "sos", ",", "eos", ",", "ignore_id", ")", ":", "\n", "    ", "\"\"\"Add <sos> and <eos> labels.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int sos: index of <sos>\n    :param int eos: index of <eeos>\n    :param int ignore_id: index of padding\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    \"\"\"", "\n", "from", "espnet", ".", "nets", ".", "pytorch_backend", ".", "nets_utils", "import", "pad_list", "\n", "\n", "_sos", "=", "ys_pad", ".", "new", "(", "[", "sos", "]", ")", "\n", "_eos", "=", "ys_pad", ".", "new", "(", "[", "eos", "]", ")", "\n", "ys", "=", "[", "y", "[", "y", "!=", "ignore_id", "]", "for", "y", "in", "ys_pad", "]", "# parse padded ys", "\n", "ys_in", "=", "[", "torch", ".", "cat", "(", "[", "_sos", ",", "y", "]", ",", "dim", "=", "0", ")", "for", "y", "in", "ys", "]", "\n", "ys_out", "=", "[", "torch", ".", "cat", "(", "[", "y", ",", "_eos", "]", ",", "dim", "=", "0", ")", "for", "y", "in", "ys", "]", "\n", "return", "pad_list", "(", "ys_in", ",", "eos", ")", ",", "pad_list", "(", "ys_out", ",", "ignore_id", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.convolution.ConvolutionModule.__init__": [[22, 45], ["torch.nn.Module.__init__", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.Conv1d", "convolution.Swish"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "channels", ",", "kernel_size", ",", "bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Construct an ConvolutionModule object.\"\"\"", "\n", "super", "(", "ConvolutionModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# kernerl_size should be a odd number for 'SAME' padding", "\n", "assert", "(", "kernel_size", "-", "1", ")", "%", "2", "==", "0", "\n", "\n", "self", ".", "pointwise_cov1", "=", "nn", ".", "Conv1d", "(", "\n", "channels", ",", "2", "*", "channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "bias", ",", "\n", ")", "\n", "self", ".", "depthwise_conv", "=", "nn", ".", "Conv1d", "(", "\n", "channels", ",", "\n", "channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "\n", "groups", "=", "channels", ",", "\n", "bias", "=", "bias", ",", "\n", ")", "\n", "self", ".", "norm", "=", "nn", ".", "BatchNorm1d", "(", "channels", ")", "\n", "self", ".", "pointwise_cov2", "=", "nn", ".", "Conv1d", "(", "\n", "channels", ",", "channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "bias", ",", "\n", ")", "\n", "self", ".", "activation", "=", "Swish", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.convolution.ConvolutionModule.forward": [[46, 66], ["convolution.ConvolutionModule.transpose", "convolution.ConvolutionModule.pointwise_cov1", "torch.nn.functional.glu", "convolution.ConvolutionModule.depthwise_conv", "convolution.ConvolutionModule.activation", "convolution.ConvolutionModule.pointwise_cov2", "convolution.ConvolutionModule.transpose", "convolution.ConvolutionModule.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Compute covolution module.\n\n        :param torch.Tensor x: (batch, time, size)\n        :return torch.Tensor: convoluted `value` (batch, time, d_model)\n        \"\"\"", "\n", "# exchange the temporal dimension and the feature dimension", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# GLU mechanism", "\n", "x", "=", "self", ".", "pointwise_cov1", "(", "x", ")", "# (batch, 2*channel, dim)", "\n", "x", "=", "nn", ".", "functional", ".", "glu", "(", "x", ",", "dim", "=", "1", ")", "# (batch, channel, dim)", "\n", "\n", "# 1D Depthwise Conv", "\n", "x", "=", "self", ".", "depthwise_conv", "(", "x", ")", "\n", "x", "=", "self", ".", "activation", "(", "self", ".", "norm", "(", "x", ")", ")", "\n", "\n", "x", "=", "self", ".", "pointwise_cov2", "(", "x", ")", "\n", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.convolution.Swish.forward": [[71, 74], ["torch.sigmoid"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Return Swich activation function.\"\"\"", "\n", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.__init__": [[61, 123], ["torch.nn.Module.__init__", "decoder.Decoder._register_load_state_dict_pre_hook", "espnet.nets.pytorch_backend.transformer.repeat.repeat", "torch.nn.Sequential", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "torch.nn.Linear", "torch.nn.Embedding", "pos_enc_class", "torch.nn.Sequential", "isinstance", "espnet.nets.pytorch_backend.transformer.decoder_layer.DecoderLayer", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.nn.ReLU", "pos_enc_class", "torch.nn.Sequential", "NotImplementedError", "espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention", "espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention", "espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.PositionwiseFeedForward", "pos_enc_class"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat"], ["def", "__init__", "(", "\n", "self", ",", "\n", "odim", ",", "\n", "attention_dim", "=", "256", ",", "\n", "attention_heads", "=", "4", ",", "\n", "linear_units", "=", "2048", ",", "\n", "num_blocks", "=", "6", ",", "\n", "dropout_rate", "=", "0.1", ",", "\n", "positional_dropout_rate", "=", "0.1", ",", "\n", "self_attention_dropout_rate", "=", "0.0", ",", "\n", "src_attention_dropout_rate", "=", "0.0", ",", "\n", "input_layer", "=", "\"embed\"", ",", "\n", "use_output_layer", "=", "True", ",", "\n", "pos_enc_class", "=", "PositionalEncoding", ",", "\n", "normalize_before", "=", "True", ",", "\n", "concat_after", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an Decoder object.\"\"\"", "\n", "torch", ".", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "_register_load_state_dict_pre_hook", "(", "_pre_hook", ")", "\n", "if", "input_layer", "==", "\"embed\"", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Embedding", "(", "odim", ",", "attention_dim", ")", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "input_layer", "==", "\"linear\"", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "odim", ",", "attention_dim", ")", ",", "\n", "torch", ".", "nn", ".", "LayerNorm", "(", "attention_dim", ")", ",", "\n", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", ",", "\n", ")", "\n", "", "elif", "isinstance", "(", "input_layer", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "self", ".", "embed", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "input_layer", ",", "pos_enc_class", "(", "attention_dim", ",", "positional_dropout_rate", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"only `embed` or torch.nn.Module is supported.\"", ")", "\n", "", "self", ".", "normalize_before", "=", "normalize_before", "\n", "self", ".", "decoders", "=", "repeat", "(", "\n", "num_blocks", ",", "\n", "lambda", ":", "DecoderLayer", "(", "\n", "attention_dim", ",", "\n", "MultiHeadedAttention", "(", "\n", "attention_heads", ",", "attention_dim", ",", "self_attention_dropout_rate", "\n", ")", ",", "\n", "MultiHeadedAttention", "(", "\n", "attention_heads", ",", "attention_dim", ",", "src_attention_dropout_rate", "\n", ")", ",", "\n", "PositionwiseFeedForward", "(", "attention_dim", ",", "linear_units", ",", "dropout_rate", ")", ",", "\n", "dropout_rate", ",", "\n", "normalize_before", ",", "\n", "concat_after", ",", "\n", ")", ",", "\n", ")", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "self", ".", "after_norm", "=", "LayerNorm", "(", "attention_dim", ")", "\n", "", "if", "use_output_layer", ":", "\n", "            ", "self", ".", "output_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "attention_dim", ",", "odim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_layer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward": [[124, 154], ["decoder.Decoder.embed", "decoder.Decoder.decoders", "decoder.Decoder.after_norm", "decoder.Decoder.output_layer"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "tgt", ",", "tgt_mask", ",", "memory", ",", "memory_mask", ")", ":", "\n", "        ", "\"\"\"Forward decoder.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n                                 if input_layer == \"embed\"\n                                 input tensor (batch, maxlen_out, #mels)\n                                 in the other cases\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param torch.Tensor memory_mask: encoded memory mask,  (batch, maxlen_in)\n                                         dtype=torch.uint8 in PyTorch 1.2-\n                                         dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :return x: decoded token score before softmax (batch, maxlen_out, token)\n                   if use_output_layer is True,\n                   final block outputs (batch, maxlen_out, attention_dim)\n                   in the other cases\n        :rtype: torch.Tensor\n        :return tgt_mask: score mask before softmax (batch, maxlen_out)\n        :rtype: torch.Tensor\n        \"\"\"", "\n", "x", "=", "self", ".", "embed", "(", "tgt", ")", "\n", "x", ",", "tgt_mask", ",", "memory", ",", "memory_mask", "=", "self", ".", "decoders", "(", "\n", "x", ",", "tgt_mask", ",", "memory", ",", "memory_mask", "\n", ")", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "after_norm", "(", "x", ")", "\n", "", "if", "self", ".", "output_layer", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "output_layer", "(", "x", ")", "\n", "", "return", "x", ",", "tgt_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward_one_step": [[155, 186], ["decoder.Decoder.embed", "zip", "decoder", "new_cache.append", "decoder.Decoder.after_norm", "torch.log_softmax", "len", "decoder.Decoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "forward_one_step", "(", "self", ",", "tgt", ",", "tgt_mask", ",", "memory", ",", "memory_mask", "=", "None", ",", "cache", "=", "None", ")", ":", "\n", "        ", "\"\"\"Forward one step.\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param List[torch.Tensor] cache:\n            cached output list of (batch, max_time_out-1, size)\n        :return y, cache: NN output value and cache per `self.decoders`.\n            `y.shape` is (batch, maxlen_out, token)\n        :rtype: Tuple[torch.Tensor, List[torch.Tensor]]\n        \"\"\"", "\n", "x", "=", "self", ".", "embed", "(", "tgt", ")", "\n", "if", "cache", "is", "None", ":", "\n", "            ", "cache", "=", "[", "None", "]", "*", "len", "(", "self", ".", "decoders", ")", "\n", "", "new_cache", "=", "[", "]", "\n", "for", "c", ",", "decoder", "in", "zip", "(", "cache", ",", "self", ".", "decoders", ")", ":", "\n", "            ", "x", ",", "tgt_mask", ",", "memory", ",", "memory_mask", "=", "decoder", "(", "\n", "x", ",", "tgt_mask", ",", "memory", ",", "memory_mask", ",", "cache", "=", "c", "\n", ")", "\n", "new_cache", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize_before", ":", "\n", "            ", "y", "=", "self", ".", "after_norm", "(", "x", "[", ":", ",", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "y", "=", "x", "[", ":", ",", "-", "1", "]", "\n", "", "if", "self", ".", "output_layer", "is", "not", "None", ":", "\n", "            ", "y", "=", "torch", ".", "log_softmax", "(", "self", ".", "output_layer", "(", "y", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "return", "y", ",", "new_cache", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.score": [[188, 195], ["espnet.nets.pytorch_backend.transformer.mask.subsequent_mask().unsqueeze", "decoder.Decoder.forward_one_step", "ys.unsqueeze", "x.unsqueeze", "logp.squeeze", "espnet.nets.pytorch_backend.transformer.mask.subsequent_mask", "len"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward_one_step", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.subsequent_mask"], ["", "def", "score", "(", "self", ",", "ys", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score.\"\"\"", "\n", "ys_mask", "=", "subsequent_mask", "(", "len", "(", "ys", ")", ",", "device", "=", "x", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "logp", ",", "state", "=", "self", ".", "forward_one_step", "(", "\n", "ys", ".", "unsqueeze", "(", "0", ")", ",", "ys_mask", ",", "x", ".", "unsqueeze", "(", "0", ")", ",", "cache", "=", "state", "\n", ")", "\n", "return", "logp", ".", "squeeze", "(", "0", ")", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.batch_score": [[197, 230], ["len", "len", "espnet.nets.pytorch_backend.transformer.mask.subsequent_mask().unsqueeze", "decoder.Decoder.forward_one_step", "torch.stack", "espnet.nets.pytorch_backend.transformer.mask.subsequent_mask", "range", "range", "ys.size", "range", "range"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward_one_step", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.subsequent_mask"], ["", "def", "batch_score", "(", "\n", "self", ",", "ys", ":", "torch", ".", "Tensor", ",", "states", ":", "List", "[", "Any", "]", ",", "xs", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "List", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new token batch (required).\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n        \"\"\"", "\n", "# merge states", "\n", "n_batch", "=", "len", "(", "ys", ")", "\n", "n_layers", "=", "len", "(", "self", ".", "decoders", ")", "\n", "if", "states", "[", "0", "]", "is", "None", ":", "\n", "            ", "batch_state", "=", "None", "\n", "", "else", ":", "\n", "# transpose state of [batch, layer] into [layer, batch]", "\n", "            ", "batch_state", "=", "[", "\n", "torch", ".", "stack", "(", "[", "states", "[", "b", "]", "[", "l", "]", "for", "b", "in", "range", "(", "n_batch", ")", "]", ")", "\n", "for", "l", "in", "range", "(", "n_layers", ")", "\n", "]", "\n", "\n", "# batch decoding", "\n", "", "ys_mask", "=", "subsequent_mask", "(", "ys", ".", "size", "(", "-", "1", ")", ",", "device", "=", "xs", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "logp", ",", "states", "=", "self", ".", "forward_one_step", "(", "ys", ",", "ys_mask", ",", "xs", ",", "cache", "=", "batch_state", ")", "\n", "\n", "# transpose state of [layer, batch] into [batch, layer]", "\n", "state_list", "=", "[", "[", "states", "[", "l", "]", "[", "b", "]", "for", "l", "in", "range", "(", "n_layers", ")", "]", "for", "b", "in", "range", "(", "n_batch", ")", "]", "\n", "return", "logp", ",", "state_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder._pre_hook": [[28, 39], ["espnet.nets.pytorch_backend.nets_utils.rename_state_dict"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.rename_state_dict"], ["def", "_pre_hook", "(", "\n", "state_dict", ",", "\n", "prefix", ",", "\n", "local_metadata", ",", "\n", "strict", ",", "\n", "missing_keys", ",", "\n", "unexpected_keys", ",", "\n", "error_msgs", ",", "\n", ")", ":", "\n", "# https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563", "\n", "    ", "rename_state_dict", "(", "prefix", "+", "\"output_norm.\"", ",", "prefix", "+", "\"after_norm.\"", ",", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.PositionalEncoding.__init__": [[44, 54], ["super().__init__", "math.sqrt", "torch.nn.Dropout", "embedding.PositionalEncoding.extend_pe", "embedding.PositionalEncoding._register_load_state_dict_pre_hook", "torch.tensor().expand", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout_rate", ",", "max_len", "=", "5000", ",", "reverse", "=", "False", ")", ":", "\n", "        ", "\"\"\"Construct an PositionalEncoding object.\"\"\"", "\n", "super", "(", "PositionalEncoding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "reverse", "=", "reverse", "\n", "self", ".", "xscale", "=", "math", ".", "sqrt", "(", "self", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", ")", "\n", "self", ".", "pe", "=", "None", "\n", "self", ".", "extend_pe", "(", "torch", ".", "tensor", "(", "0.0", ")", ".", "expand", "(", "1", ",", "max_len", ")", ")", "\n", "self", ".", "_register_load_state_dict_pre_hook", "(", "_pre_hook", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.PositionalEncoding.extend_pe": [[55, 77], ["torch.zeros", "torch.exp", "torch.sin", "torch.cos", "pe.unsqueeze.unsqueeze.unsqueeze", "pe.unsqueeze.unsqueeze.to", "x.size", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "embedding.PositionalEncoding.pe.size", "x.size", "torch.arange", "embedding.PositionalEncoding.pe.to", "torch.arange", "torch.arange", "x.size", "math.log", "x.size"], "methods", ["None"], ["", "def", "extend_pe", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Reset the positional encodings.\"\"\"", "\n", "if", "self", ".", "pe", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "pe", ".", "size", "(", "1", ")", ">=", "x", ".", "size", "(", "1", ")", ":", "\n", "                ", "if", "self", ".", "pe", ".", "dtype", "!=", "x", ".", "dtype", "or", "self", ".", "pe", ".", "device", "!=", "x", ".", "device", ":", "\n", "                    ", "self", ".", "pe", "=", "self", ".", "pe", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "", "return", "\n", "", "", "pe", "=", "torch", ".", "zeros", "(", "x", ".", "size", "(", "1", ")", ",", "self", ".", "d_model", ")", "\n", "if", "self", ".", "reverse", ":", "\n", "            ", "position", "=", "torch", ".", "arange", "(", "\n", "x", ".", "size", "(", "1", ")", "-", "1", ",", "-", "1", ",", "-", "1.0", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "position", "=", "torch", ".", "arange", "(", "0", ",", "x", ".", "size", "(", "1", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "div_term", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "arange", "(", "0", ",", "self", ".", "d_model", ",", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "self", ".", "d_model", ")", "\n", ")", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "pe", "=", "pe", ".", "to", "(", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.PositionalEncoding.forward": [[78, 88], ["embedding.PositionalEncoding.extend_pe", "embedding.PositionalEncoding.dropout", "x.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"", "\n", "self", ".", "extend_pe", "(", "x", ")", "\n", "x", "=", "x", "*", "self", ".", "xscale", "+", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "return", "self", ".", "dropout", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.ScaledPositionalEncoding.__init__": [[99, 103], ["embedding.PositionalEncoding.__init__", "torch.nn.Parameter", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout_rate", ",", "max_len", "=", "5000", ")", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "d_model", "=", "d_model", ",", "dropout_rate", "=", "dropout_rate", ",", "max_len", "=", "max_len", ")", "\n", "self", ".", "alpha", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "1.0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.ScaledPositionalEncoding.reset_parameters": [[104, 107], ["torch.tensor"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset parameters.\"\"\"", "\n", "self", ".", "alpha", ".", "data", "=", "torch", ".", "tensor", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.ScaledPositionalEncoding.forward": [[108, 118], ["embedding.ScaledPositionalEncoding.extend_pe", "embedding.ScaledPositionalEncoding.dropout", "x.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"", "\n", "self", ".", "extend_pe", "(", "x", ")", "\n", "x", "=", "x", "+", "self", ".", "alpha", "*", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "return", "self", ".", "dropout", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.LegacyRelPositionalEncoding.__init__": [[130, 137], ["embedding.PositionalEncoding.__init__"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout_rate", ",", "max_len", "=", "5000", ")", ":", "\n", "        ", "\"\"\"Initialize class.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "d_model", "=", "d_model", ",", "\n", "dropout_rate", "=", "dropout_rate", ",", "\n", "max_len", "=", "max_len", ",", "\n", "reverse", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.LegacyRelPositionalEncoding.forward": [[139, 151], ["embedding.LegacyRelPositionalEncoding.extend_pe", "embedding.LegacyRelPositionalEncoding.dropout", "embedding.LegacyRelPositionalEncoding.dropout", "x.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Compute positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n            torch.Tensor: Positional embedding tensor (1, time, `*`).\n        \"\"\"", "\n", "self", ".", "extend_pe", "(", "x", ")", "\n", "x", "=", "x", "*", "self", ".", "xscale", "\n", "pos_emb", "=", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "return", "self", ".", "dropout", "(", "x", ")", ",", "self", ".", "dropout", "(", "pos_emb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.__init__": [[163, 171], ["super().__init__", "math.sqrt", "torch.nn.Dropout", "embedding.RelPositionalEncoding.extend_pe", "torch.tensor().expand", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout_rate", ",", "max_len", "=", "5000", ")", ":", "\n", "        ", "\"\"\"Construct an PositionalEncoding object.\"\"\"", "\n", "super", "(", "RelPositionalEncoding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "xscale", "=", "math", ".", "sqrt", "(", "self", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", ")", "\n", "self", ".", "pe", "=", "None", "\n", "self", ".", "extend_pe", "(", "torch", ".", "tensor", "(", "0.0", ")", ".", "expand", "(", "1", ",", "max_len", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe": [[172, 203], ["torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.exp", "torch.sin", "torch.cos", "torch.sin", "torch.cos", "torch.flip().unsqueeze", "pe_negative[].unsqueeze", "torch.cat", "torch.cat.to", "x.size", "x.size", "embedding.RelPositionalEncoding.pe.size", "torch.arange", "torch.arange", "torch.flip", "embedding.RelPositionalEncoding.pe.to", "x.size", "x.size", "math.log"], "methods", ["None"], ["", "def", "extend_pe", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Reset the positional encodings.\"\"\"", "\n", "if", "self", ".", "pe", "is", "not", "None", ":", "\n", "# self.pe contains both positive and negative parts", "\n", "# the length of self.pe is 2 * input_len - 1", "\n", "            ", "if", "self", ".", "pe", ".", "size", "(", "1", ")", ">=", "x", ".", "size", "(", "1", ")", "*", "2", "-", "1", ":", "\n", "                ", "if", "self", ".", "pe", ".", "dtype", "!=", "x", ".", "dtype", "or", "self", ".", "pe", ".", "device", "!=", "x", ".", "device", ":", "\n", "                    ", "self", ".", "pe", "=", "self", ".", "pe", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "", "return", "\n", "# Suppose `i` means to the position of query vecotr and `j` means the", "\n", "# position of key vector. We use position relative positions when keys", "\n", "# are to the left (i>j) and negative relative positions otherwise (i<j).", "\n", "", "", "pe_positive", "=", "torch", ".", "zeros", "(", "x", ".", "size", "(", "1", ")", ",", "self", ".", "d_model", ")", "\n", "pe_negative", "=", "torch", ".", "zeros", "(", "x", ".", "size", "(", "1", ")", ",", "self", ".", "d_model", ")", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "x", ".", "size", "(", "1", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "arange", "(", "0", ",", "self", ".", "d_model", ",", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "self", ".", "d_model", ")", "\n", ")", "\n", "pe_positive", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe_positive", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "pe_negative", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "-", "1", "*", "position", "*", "div_term", ")", "\n", "pe_negative", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "-", "1", "*", "position", "*", "div_term", ")", "\n", "\n", "# Reserve the order of positive indices and concat both positive and", "\n", "# negative indices. This is used to support the shifting trick", "\n", "# as in https://arxiv.org/abs/1901.02860", "\n", "pe_positive", "=", "torch", ".", "flip", "(", "pe_positive", ",", "[", "0", "]", ")", ".", "unsqueeze", "(", "0", ")", "\n", "pe_negative", "=", "pe_negative", "[", "1", ":", "]", ".", "unsqueeze", "(", "0", ")", "\n", "pe", "=", "torch", ".", "cat", "(", "[", "pe_positive", ",", "pe_negative", "]", ",", "dim", "=", "1", ")", "\n", "self", ".", "pe", "=", "pe", ".", "to", "(", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.forward": [[204, 218], ["embedding.RelPositionalEncoding.extend_pe", "embedding.RelPositionalEncoding.dropout", "embedding.RelPositionalEncoding.dropout", "x.size", "x.size", "embedding.RelPositionalEncoding.pe.size", "embedding.RelPositionalEncoding.pe.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding.RelPositionalEncoding.extend_pe"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Add positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, time, `*`).\n        Returns:\n            torch.Tensor: Encoded tensor (batch, time, `*`).\n        \"\"\"", "\n", "self", ".", "extend_pe", "(", "x", ")", "\n", "x", "=", "x", "*", "self", ".", "xscale", "\n", "pos_emb", "=", "self", ".", "pe", "[", "\n", ":", ",", "\n", "self", ".", "pe", ".", "size", "(", "1", ")", "//", "2", "-", "x", ".", "size", "(", "1", ")", "+", "1", ":", "self", ".", "pe", ".", "size", "(", "1", ")", "//", "2", "+", "x", ".", "size", "(", "1", ")", ",", "\n", "]", "\n", "return", "self", ".", "dropout", "(", "x", ")", ",", "self", ".", "dropout", "(", "pos_emb", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.embedding._pre_hook": [[14, 31], ["state_dict.pop"], "function", ["None"], ["def", "_pre_hook", "(", "\n", "state_dict", ",", "\n", "prefix", ",", "\n", "local_metadata", ",", "\n", "strict", ",", "\n", "missing_keys", ",", "\n", "unexpected_keys", ",", "\n", "error_msgs", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Perform pre-hook in load_state_dict for backward compatibility.\n    Note:\n        We saved self.pe until v.0.5.2 but we have omitted it later.\n        Therefore, we remove the item \"pe\" from `state_dict` for backward compatibility.\n    \"\"\"", "\n", "k", "=", "prefix", "+", "\"pe\"", "\n", "if", "k", "in", "state_dict", ":", "\n", "        ", "state_dict", ".", "pop", "(", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.__init__": [[24, 37], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "n_head", ",", "n_feat", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"Construct an MultiHeadedAttention object.\"\"\"", "\n", "super", "(", "MultiHeadedAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "n_feat", "%", "n_head", "==", "0", "\n", "# We assume d_v always equals d_k", "\n", "self", ".", "d_k", "=", "n_feat", "//", "n_head", "\n", "self", ".", "h", "=", "n_head", "\n", "self", ".", "linear_q", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ")", "\n", "self", ".", "linear_k", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ")", "\n", "self", ".", "linear_v", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ")", "\n", "self", ".", "attn", "=", "None", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_qkv": [[38, 58], ["query.size", "attention.MultiHeadedAttention.linear_q().view", "attention.MultiHeadedAttention.linear_k().view", "attention.MultiHeadedAttention.linear_v().view", "q.transpose.transpose.transpose", "k.transpose.transpose.transpose", "v.transpose.transpose.transpose", "attention.MultiHeadedAttention.linear_q", "attention.MultiHeadedAttention.linear_k", "attention.MultiHeadedAttention.linear_v"], "methods", ["None"], ["", "def", "forward_qkv", "(", "self", ",", "query", ",", "key", ",", "value", ")", ":", "\n", "        ", "\"\"\"Transform query, key and value.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n        Returns:\n            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n        \"\"\"", "\n", "n_batch", "=", "query", ".", "size", "(", "0", ")", "\n", "q", "=", "self", ".", "linear_q", "(", "query", ")", ".", "view", "(", "n_batch", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", "\n", "k", "=", "self", ".", "linear_k", "(", "key", ")", ".", "view", "(", "n_batch", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", "\n", "v", "=", "self", ".", "linear_v", "(", "value", ")", ".", "view", "(", "n_batch", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, head, time1, d_k)", "\n", "k", "=", "k", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, head, time2, d_k)", "\n", "v", "=", "v", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, head, time2, d_k)", "\n", "\n", "return", "q", ",", "k", ",", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_attention": [[59, 91], ["value.size", "attention.MultiHeadedAttention.dropout", "torch.matmul", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attention.MultiHeadedAttention.linear_out", "mask.unsqueeze().eq.unsqueeze().eq.unsqueeze().eq", "float", "scores.masked_fill.masked_fill.masked_fill", "torch.softmax().masked_fill", "torch.softmax", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attention.MultiHeadedAttention.linear_out", "mask.unsqueeze().eq.unsqueeze().eq.unsqueeze", "numpy.finfo", "torch.softmax", "x.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.tensor().numpy", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.softmax"], ["", "def", "forward_attention", "(", "self", ",", "value", ",", "scores", ",", "mask", ",", "rtn_attn", "=", "False", ")", ":", "\n", "        ", "\"\"\"Compute attention context vector.\n        Args:\n            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Transformed value (#batch, time1, d_model)\n                weighted by the attention score (#batch, time1, time2).\n        \"\"\"", "\n", "n_batch", "=", "value", ".", "size", "(", "0", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "eq", "(", "0", ")", "# (batch, 1, *, time2)", "\n", "min_value", "=", "float", "(", "\n", "numpy", ".", "finfo", "(", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "scores", ".", "dtype", ")", ".", "numpy", "(", ")", ".", "dtype", ")", ".", "min", "\n", ")", "\n", "scores", "=", "scores", ".", "masked_fill", "(", "mask", ",", "min_value", ")", "\n", "self", ".", "attn", "=", "torch", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", ".", "masked_fill", "(", "\n", "mask", ",", "0.0", "\n", ")", "# (batch, head, time1, time2)", "\n", "", "else", ":", "\n", "            ", "self", ".", "attn", "=", "torch", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (batch, head, time1, time2)", "\n", "\n", "", "p_attn", "=", "self", ".", "dropout", "(", "self", ".", "attn", ")", "\n", "x", "=", "torch", ".", "matmul", "(", "p_attn", ",", "value", ")", "# (batch, head, time1, d_k)", "\n", "x", "=", "(", "\n", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "n_batch", ",", "-", "1", ",", "self", ".", "h", "*", "self", ".", "d_k", ")", "\n", ")", "# (batch, time1, d_model)", "\n", "if", "rtn_attn", ":", "\n", "            ", "return", "self", ".", "linear_out", "(", "x", ")", ",", "self", ".", "attn", "\n", "", "return", "self", ".", "linear_out", "(", "x", ")", "# (batch, time1, d_model)", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward": [[92, 107], ["attention.MultiHeadedAttention.forward_qkv", "attention.MultiHeadedAttention.forward_attention", "torch.matmul", "math.sqrt", "k.transpose"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_qkv", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_attention"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "mask", ",", "rtn_attn", "=", "False", ")", ":", "\n", "        ", "\"\"\"Compute scaled dot product attention.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n            rtn_attn (boolean): Flag of return attention score\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "forward_qkv", "(", "query", ",", "key", ",", "value", ")", "\n", "scores", "=", "torch", ".", "matmul", "(", "q", ",", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "d_k", ")", "\n", "return", "self", ".", "forward_attention", "(", "v", ",", "scores", ",", "mask", ",", "rtn_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.LegacyRelPositionMultiHeadedAttention.__init__": [[120, 132], ["attention.MultiHeadedAttention.__init__", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "n_head", ",", "n_feat", ",", "dropout_rate", ",", "zero_triu", "=", "False", ")", ":", "\n", "        ", "\"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "n_head", ",", "n_feat", ",", "dropout_rate", ")", "\n", "self", ".", "zero_triu", "=", "zero_triu", "\n", "# linear transformation for positional encoding", "\n", "self", ".", "linear_pos", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ",", "bias", "=", "False", ")", "\n", "# these two learnable bias are used in matrix c and matrix d", "\n", "# as described in https://arxiv.org/abs/1901.02860 Section 3.3", "\n", "self", ".", "pos_bias_u", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "h", ",", "self", ".", "d_k", ")", ")", "\n", "self", ".", "pos_bias_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "h", ",", "self", ".", "d_k", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "pos_bias_u", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "pos_bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.LegacyRelPositionMultiHeadedAttention.rel_shift": [[133, 151], ["torch.zeros", "torch.cat", "x_padded.view.view.view", "x_padded[].view_as", "x_padded[].view_as.size", "torch.ones", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size", "torch.tril", "x_padded[].view_as.size", "x_padded[].view_as.size", "x_padded[].view_as.size"], "methods", ["None"], ["", "def", "rel_shift", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"", "\n", "zero_pad", "=", "torch", ".", "zeros", "(", "(", "*", "x", ".", "size", "(", ")", "[", ":", "3", "]", ",", "1", ")", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "x_padded", "=", "torch", ".", "cat", "(", "[", "zero_pad", ",", "x", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "x_padded", "=", "x_padded", ".", "view", "(", "*", "x", ".", "size", "(", ")", "[", ":", "2", "]", ",", "x", ".", "size", "(", "3", ")", "+", "1", ",", "x", ".", "size", "(", "2", ")", ")", "\n", "x", "=", "x_padded", "[", ":", ",", ":", ",", "1", ":", "]", ".", "view_as", "(", "x", ")", "\n", "\n", "if", "self", ".", "zero_triu", ":", "\n", "            ", "ones", "=", "torch", ".", "ones", "(", "(", "x", ".", "size", "(", "2", ")", ",", "x", ".", "size", "(", "3", ")", ")", ")", "\n", "x", "=", "x", "*", "torch", ".", "tril", "(", "ones", ",", "x", ".", "size", "(", "3", ")", "-", "x", ".", "size", "(", "2", ")", ")", "[", "None", ",", "None", ",", ":", ",", ":", "]", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.LegacyRelPositionMultiHeadedAttention.forward": [[152, 192], ["attention.LegacyRelPositionMultiHeadedAttention.forward_qkv", "q.transpose.transpose.transpose", "pos_emb.size", "attention.LegacyRelPositionMultiHeadedAttention.linear_pos().view", "p.transpose.transpose.transpose", "torch.matmul", "torch.matmul", "attention.LegacyRelPositionMultiHeadedAttention.rel_shift", "attention.LegacyRelPositionMultiHeadedAttention.forward_attention", "k.transpose", "p.transpose.transpose.transpose", "math.sqrt", "attention.LegacyRelPositionMultiHeadedAttention.linear_pos"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_qkv", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.RelPositionMultiHeadedAttention.rel_shift", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_attention"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "pos_emb", ",", "mask", ")", ":", "\n", "        ", "\"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor (#batch, time1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "forward_qkv", "(", "query", ",", "key", ",", "value", ")", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, time1, head, d_k)", "\n", "\n", "n_batch_pos", "=", "pos_emb", ".", "size", "(", "0", ")", "\n", "p", "=", "self", ".", "linear_pos", "(", "pos_emb", ")", ".", "view", "(", "n_batch_pos", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", "\n", "p", "=", "p", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, head, time1, d_k)", "\n", "\n", "# (batch, head, time1, d_k)", "\n", "q_with_bias_u", "=", "(", "q", "+", "self", ".", "pos_bias_u", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# (batch, head, time1, d_k)", "\n", "q_with_bias_v", "=", "(", "q", "+", "self", ".", "pos_bias_v", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# compute attention score", "\n", "# first compute matrix a and matrix c", "\n", "# as described in https://arxiv.org/abs/1901.02860 Section 3.3", "\n", "# (batch, head, time1, time2)", "\n", "matrix_ac", "=", "torch", ".", "matmul", "(", "q_with_bias_u", ",", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "\n", "# compute matrix b and matrix d", "\n", "# (batch, head, time1, time1)", "\n", "matrix_bd", "=", "torch", ".", "matmul", "(", "q_with_bias_v", ",", "p", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "matrix_bd", "=", "self", ".", "rel_shift", "(", "matrix_bd", ")", "\n", "\n", "scores", "=", "(", "matrix_ac", "+", "matrix_bd", ")", "/", "math", ".", "sqrt", "(", "\n", "self", ".", "d_k", "\n", ")", "# (batch, head, time1, time2)", "\n", "\n", "return", "self", ".", "forward_attention", "(", "v", ",", "scores", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.RelPositionMultiHeadedAttention.__init__": [[205, 217], ["attention.MultiHeadedAttention.__init__", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "n_head", ",", "n_feat", ",", "dropout_rate", ",", "zero_triu", "=", "False", ")", ":", "\n", "        ", "\"\"\"Construct an RelPositionMultiHeadedAttention object.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "n_head", ",", "n_feat", ",", "dropout_rate", ")", "\n", "self", ".", "zero_triu", "=", "zero_triu", "\n", "# linear transformation for positional encoding", "\n", "self", ".", "linear_pos", "=", "nn", ".", "Linear", "(", "n_feat", ",", "n_feat", ",", "bias", "=", "False", ")", "\n", "# these two learnable bias are used in matrix c and matrix d", "\n", "# as described in https://arxiv.org/abs/1901.02860 Section 3.3", "\n", "self", ".", "pos_bias_u", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "h", ",", "self", ".", "d_k", ")", ")", "\n", "self", ".", "pos_bias_v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "h", ",", "self", ".", "d_k", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "pos_bias_u", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "pos_bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.RelPositionMultiHeadedAttention.rel_shift": [[218, 239], ["torch.zeros", "torch.cat", "x_padded.view.view.view", "x.size", "x_padded[].view_as", "torch.ones", "x.size", "x.size", "x.size", "x.size", "torch.tril", "x.size", "x.size", "x.size", "x.size"], "methods", ["None"], ["", "def", "rel_shift", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Compute relative positional encoding.\n        Args:\n            x (torch.Tensor): Input tensor (batch, head, time1, 2*time1-1).\n            time1 means the length of query vector.\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"", "\n", "zero_pad", "=", "torch", ".", "zeros", "(", "(", "*", "x", ".", "size", "(", ")", "[", ":", "3", "]", ",", "1", ")", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "x_padded", "=", "torch", ".", "cat", "(", "[", "zero_pad", ",", "x", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "x_padded", "=", "x_padded", ".", "view", "(", "*", "x", ".", "size", "(", ")", "[", ":", "2", "]", ",", "x", ".", "size", "(", "3", ")", "+", "1", ",", "x", ".", "size", "(", "2", ")", ")", "\n", "x", "=", "x_padded", "[", ":", ",", ":", ",", "1", ":", "]", ".", "view_as", "(", "x", ")", "[", "\n", ":", ",", ":", ",", ":", ",", ":", "x", ".", "size", "(", "-", "1", ")", "//", "2", "+", "1", "\n", "]", "# only keep the positions from 0 to time2", "\n", "\n", "if", "self", ".", "zero_triu", ":", "\n", "            ", "ones", "=", "torch", ".", "ones", "(", "(", "x", ".", "size", "(", "2", ")", ",", "x", ".", "size", "(", "3", ")", ")", ",", "device", "=", "x", ".", "device", ")", "\n", "x", "=", "x", "*", "torch", ".", "tril", "(", "ones", ",", "x", ".", "size", "(", "3", ")", "-", "x", ".", "size", "(", "2", ")", ")", "[", "None", ",", "None", ",", ":", ",", ":", "]", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.RelPositionMultiHeadedAttention.forward": [[240, 281], ["attention.RelPositionMultiHeadedAttention.forward_qkv", "q.transpose.transpose.transpose", "pos_emb.size", "attention.RelPositionMultiHeadedAttention.linear_pos().view", "p.transpose.transpose.transpose", "torch.matmul", "torch.matmul", "attention.RelPositionMultiHeadedAttention.rel_shift", "attention.RelPositionMultiHeadedAttention.forward_attention", "k.transpose", "p.transpose.transpose.transpose", "math.sqrt", "attention.RelPositionMultiHeadedAttention.linear_pos"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_qkv", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.RelPositionMultiHeadedAttention.rel_shift", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.attention.MultiHeadedAttention.forward_attention"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "pos_emb", ",", "mask", ")", ":", "\n", "        ", "\"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n        Args:\n            query (torch.Tensor): Query tensor (#batch, time1, size).\n            key (torch.Tensor): Key tensor (#batch, time2, size).\n            value (torch.Tensor): Value tensor (#batch, time2, size).\n            pos_emb (torch.Tensor): Positional embedding tensor\n                (#batch, 2*time1-1, size).\n            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n                (#batch, time1, time2).\n        Returns:\n            torch.Tensor: Output tensor (#batch, time1, d_model).\n        \"\"\"", "\n", "q", ",", "k", ",", "v", "=", "self", ".", "forward_qkv", "(", "query", ",", "key", ",", "value", ")", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, time1, head, d_k)", "\n", "\n", "n_batch_pos", "=", "pos_emb", ".", "size", "(", "0", ")", "\n", "p", "=", "self", ".", "linear_pos", "(", "pos_emb", ")", ".", "view", "(", "n_batch_pos", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", "\n", "p", "=", "p", ".", "transpose", "(", "1", ",", "2", ")", "# (batch, head, 2*time1-1, d_k)", "\n", "\n", "# (batch, head, time1, d_k)", "\n", "q_with_bias_u", "=", "(", "q", "+", "self", ".", "pos_bias_u", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# (batch, head, time1, d_k)", "\n", "q_with_bias_v", "=", "(", "q", "+", "self", ".", "pos_bias_v", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# compute attention score", "\n", "# first compute matrix a and matrix c", "\n", "# as described in https://arxiv.org/abs/1901.02860 Section 3.3", "\n", "# (batch, head, time1, time2)", "\n", "matrix_ac", "=", "torch", ".", "matmul", "(", "q_with_bias_u", ",", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "\n", "# compute matrix b and matrix d", "\n", "# (batch, head, time1, 2*time1-1)", "\n", "matrix_bd", "=", "torch", ".", "matmul", "(", "q_with_bias_v", ",", "p", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "\n", "matrix_bd", "=", "self", ".", "rel_shift", "(", "matrix_bd", ")", "\n", "\n", "scores", "=", "(", "matrix_ac", "+", "matrix_bd", ")", "/", "math", ".", "sqrt", "(", "\n", "self", ".", "d_k", "\n", ")", "# (batch, head, time1, time2)", "\n", "\n", "return", "self", ".", "forward_attention", "(", "v", ",", "scores", ",", "mask", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.layer_norm.LayerNorm.__init__": [[19, 23], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "nout", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Construct an LayerNorm object.\"\"\"", "\n", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", "nout", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.layer_norm.LayerNorm.forward": [[24, 34], ["super().forward().transpose", "super().forward", "super().forward", "x.transpose"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.forward", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.forward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Apply layer normalization.\n\n        :param torch.Tensor x: input tensor\n        :return: layer normalized tensor\n        :rtype torch.Tensor\n        \"\"\"", "\n", "if", "self", ".", "dim", "==", "-", "1", ":", "\n", "            ", "return", "super", "(", "LayerNorm", ",", "self", ")", ".", "forward", "(", "x", ")", "\n", "", "return", "super", "(", "LayerNorm", ",", "self", ")", ".", "forward", "(", "x", ".", "transpose", "(", "1", ",", "-", "1", ")", ")", ".", "transpose", "(", "1", ",", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.plotfn": [[106, 112], ["plot.plot_multi_head_attention"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.plot_multi_head_attention"], ["    ", "def", "plotfn", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", "[", "\"ikey\"", "]", "=", "self", ".", "ikey", "\n", "kwargs", "[", "\"iaxis\"", "]", "=", "self", ".", "iaxis", "\n", "kwargs", "[", "\"okey\"", "]", "=", "self", ".", "okey", "\n", "kwargs", "[", "\"oaxis\"", "]", "=", "self", ".", "oaxis", "\n", "plot_multi_head_attention", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.__call__": [[113, 117], ["plot.PlotAttentionReport.get_attention_weights", "plot.PlotAttentionReport.plotfn"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.get_attention_weights", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.plotfn"], ["", "def", "__call__", "(", "self", ",", "trainer", ")", ":", "\n", "        ", "attn_dict", "=", "self", ".", "get_attention_weights", "(", ")", "\n", "suffix", "=", "\"ep.{.updater.epoch}.png\"", ".", "format", "(", "trainer", ")", "\n", "self", ".", "plotfn", "(", "self", ".", "data", ",", "attn_dict", ",", "self", ".", "outdir", ",", "suffix", ",", "savefig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.get_attention_weights": [[118, 125], ["plot.PlotAttentionReport.converter", "isinstance", "plot.PlotAttentionReport.att_vis_fn", "isinstance", "plot.PlotAttentionReport.transform", "plot.PlotAttentionReport.att_vis_fn"], "methods", ["None"], ["", "def", "get_attention_weights", "(", "self", ")", ":", "\n", "        ", "batch", "=", "self", ".", "converter", "(", "[", "self", ".", "transform", "(", "self", ".", "data", ")", "]", ",", "self", ".", "device", ")", "\n", "if", "isinstance", "(", "batch", ",", "tuple", ")", ":", "\n", "            ", "att_ws", "=", "self", ".", "att_vis_fn", "(", "*", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "            ", "att_ws", "=", "self", ".", "att_vis_fn", "(", "**", "batch", ")", "\n", "", "return", "att_ws", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.log_attentions": [[126, 135], ["plot.PlotAttentionReport.get_attention_weights", "plot.PlotAttentionReport.plotfn", "logger.add_figure", "matplotlib.clf", "basename"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.get_attention_weights", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.PlotAttentionReport.plotfn"], ["", "def", "log_attentions", "(", "self", ",", "logger", ",", "step", ")", ":", "\n", "        ", "def", "log_fig", "(", "plot", ",", "filename", ")", ":", "\n", "            ", "from", "os", ".", "path", "import", "basename", "\n", "\n", "logger", ".", "add_figure", "(", "basename", "(", "filename", ")", ",", "plot", ",", "step", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n", "", "attn_dict", "=", "self", ".", "get_attention_weights", "(", ")", "\n", "self", ".", "plotfn", "(", "self", ".", "data", ",", "attn_dict", ",", "self", ".", "outdir", ",", "\"\"", ",", "log_fig", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot._plot_and_save_attention": [[15, 46], ["os.path.dirname", "matplotlib.figaspect", "matplotlib.Figure", "plt.Figure.subplots", "zip", "plt.Figure.tight_layout", "os.path.exists", "os.makedirs", "len", "len", "ax.imshow", "ax.set_xlabel", "ax.set_ylabel", "ax.xaxis.set_major_locator", "ax.yaxis.set_major_locator", "len", "aw.astype", "MaxNLocator", "MaxNLocator", "ax.set_xticks", "ax.set_xticks", "ax.set_xticklabels", "ax.set_yticks", "ax.set_yticks", "ax.set_yticklabels", "numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["def", "_plot_and_save_attention", "(", "att_w", ",", "filename", ",", "xtokens", "=", "None", ",", "ytokens", "=", "None", ")", ":", "\n", "# dynamically import matplotlib due to not found error", "\n", "    ", "from", "matplotlib", ".", "ticker", "import", "MaxNLocator", "\n", "import", "os", "\n", "\n", "d", "=", "os", ".", "path", ".", "dirname", "(", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "d", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "d", ")", "\n", "", "w", ",", "h", "=", "plt", ".", "figaspect", "(", "1.0", "/", "len", "(", "att_w", ")", ")", "\n", "fig", "=", "plt", ".", "Figure", "(", "figsize", "=", "(", "w", "*", "2", ",", "h", "*", "2", ")", ")", "\n", "axes", "=", "fig", ".", "subplots", "(", "1", ",", "len", "(", "att_w", ")", ")", "\n", "if", "len", "(", "att_w", ")", "==", "1", ":", "\n", "        ", "axes", "=", "[", "axes", "]", "\n", "", "for", "ax", ",", "aw", "in", "zip", "(", "axes", ",", "att_w", ")", ":", "\n", "# plt.subplot(1, len(att_w), h)", "\n", "        ", "ax", ".", "imshow", "(", "aw", ".", "astype", "(", "numpy", ".", "float32", ")", ",", "aspect", "=", "\"auto\"", ")", "\n", "ax", ".", "set_xlabel", "(", "\"Input\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"Output\"", ")", "\n", "ax", ".", "xaxis", ".", "set_major_locator", "(", "MaxNLocator", "(", "integer", "=", "True", ")", ")", "\n", "ax", ".", "yaxis", ".", "set_major_locator", "(", "MaxNLocator", "(", "integer", "=", "True", ")", ")", "\n", "# Labels for major ticks", "\n", "if", "xtokens", "is", "not", "None", ":", "\n", "            ", "ax", ".", "set_xticks", "(", "numpy", ".", "linspace", "(", "0", ",", "len", "(", "xtokens", ")", "-", "1", ",", "len", "(", "xtokens", ")", ")", ")", "\n", "ax", ".", "set_xticks", "(", "numpy", ".", "linspace", "(", "0", ",", "len", "(", "xtokens", ")", "-", "1", ",", "1", ")", ",", "minor", "=", "True", ")", "\n", "ax", ".", "set_xticklabels", "(", "xtokens", "+", "[", "\"\"", "]", ",", "rotation", "=", "40", ")", "\n", "", "if", "ytokens", "is", "not", "None", ":", "\n", "            ", "ax", ".", "set_yticks", "(", "numpy", ".", "linspace", "(", "0", ",", "len", "(", "ytokens", ")", "-", "1", ",", "len", "(", "ytokens", ")", ")", ")", "\n", "ax", ".", "set_yticks", "(", "numpy", ".", "linspace", "(", "0", ",", "len", "(", "ytokens", ")", "-", "1", ",", "1", ")", ",", "minor", "=", "True", ")", "\n", "ax", ".", "set_yticklabels", "(", "ytokens", "+", "[", "\"\"", "]", ")", "\n", "", "", "fig", ".", "tight_layout", "(", ")", "\n", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.savefig": [[48, 51], ["plot.savefig", "matplotlib.clf"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.savefig"], ["", "def", "savefig", "(", "plot", ",", "filename", ")", ":", "\n", "    ", "plot", ".", "savefig", "(", "filename", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot.plot_multi_head_attention": [[53, 103], ["attn_dict.items", "enumerate", "int", "int", "plot._plot_and_save_attention", "savefn", "[].keys", "[].split", "logging.warning", "[].keys", "[].keys", "[].split", "[].split"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.plot._plot_and_save_attention"], ["", "def", "plot_multi_head_attention", "(", "\n", "data", ",", "\n", "attn_dict", ",", "\n", "outdir", ",", "\n", "suffix", "=", "\"png\"", ",", "\n", "savefn", "=", "savefig", ",", "\n", "ikey", "=", "\"input\"", ",", "\n", "iaxis", "=", "0", ",", "\n", "okey", "=", "\"output\"", ",", "\n", "oaxis", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Plot multi head attentions.\n\n    :param dict data: utts info from json file\n    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.\n        values should be torch.Tensor (head, input_length, output_length)\n    :param str outdir: dir to save fig\n    :param str suffix: filename suffix including image type (e.g., png)\n    :param savefn: function to save\n\n    \"\"\"", "\n", "for", "name", ",", "att_ws", "in", "attn_dict", ".", "items", "(", ")", ":", "\n", "        ", "for", "idx", ",", "att_w", "in", "enumerate", "(", "att_ws", ")", ":", "\n", "            ", "filename", "=", "\"%s/%s.%s.%s\"", "%", "(", "outdir", ",", "data", "[", "idx", "]", "[", "0", "]", ",", "name", ",", "suffix", ")", "\n", "dec_len", "=", "int", "(", "data", "[", "idx", "]", "[", "1", "]", "[", "okey", "]", "[", "oaxis", "]", "[", "\"shape\"", "]", "[", "0", "]", ")", "\n", "enc_len", "=", "int", "(", "data", "[", "idx", "]", "[", "1", "]", "[", "ikey", "]", "[", "iaxis", "]", "[", "\"shape\"", "]", "[", "0", "]", ")", "\n", "xtokens", ",", "ytokens", "=", "None", ",", "None", "\n", "if", "\"encoder\"", "in", "name", ":", "\n", "                ", "att_w", "=", "att_w", "[", ":", ",", ":", "enc_len", ",", ":", "enc_len", "]", "\n", "# for MT", "\n", "if", "\"token\"", "in", "data", "[", "idx", "]", "[", "1", "]", "[", "ikey", "]", "[", "iaxis", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "xtokens", "=", "data", "[", "idx", "]", "[", "1", "]", "[", "ikey", "]", "[", "iaxis", "]", "[", "\"token\"", "]", ".", "split", "(", ")", "\n", "ytokens", "=", "xtokens", "[", ":", "]", "\n", "", "", "elif", "\"decoder\"", "in", "name", ":", "\n", "                ", "if", "\"self\"", "in", "name", ":", "\n", "                    ", "att_w", "=", "att_w", "[", ":", ",", ":", "dec_len", "+", "1", ",", ":", "dec_len", "+", "1", "]", "# +1 for <sos>", "\n", "", "else", ":", "\n", "                    ", "att_w", "=", "att_w", "[", ":", ",", ":", "dec_len", "+", "1", ",", ":", "enc_len", "]", "# +1 for <sos>", "\n", "# for MT", "\n", "if", "\"token\"", "in", "data", "[", "idx", "]", "[", "1", "]", "[", "ikey", "]", "[", "iaxis", "]", ".", "keys", "(", ")", ":", "\n", "                        ", "xtokens", "=", "data", "[", "idx", "]", "[", "1", "]", "[", "ikey", "]", "[", "iaxis", "]", "[", "\"token\"", "]", ".", "split", "(", ")", "\n", "# for ASR/ST/MT", "\n", "", "", "if", "\"token\"", "in", "data", "[", "idx", "]", "[", "1", "]", "[", "okey", "]", "[", "oaxis", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "ytokens", "=", "[", "\"<sos>\"", "]", "+", "data", "[", "idx", "]", "[", "1", "]", "[", "okey", "]", "[", "oaxis", "]", "[", "\"token\"", "]", ".", "split", "(", ")", "\n", "if", "\"self\"", "in", "name", ":", "\n", "                        ", "xtokens", "=", "ytokens", "[", ":", "]", "\n", "", "", "", "else", ":", "\n", "                ", "logging", ".", "warning", "(", "\"unknown name for shaping attention\"", ")", "\n", "", "fig", "=", "_plot_and_save_attention", "(", "att_w", ",", "filename", ",", "xtokens", ",", "ytokens", ")", "\n", "savefn", "(", "fig", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder_layer.DecoderLayer.__init__": [[32, 57], ["torch.nn.Module.__init__", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "espnet.nets.pytorch_backend.transformer.layer_norm.LayerNorm", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "size", ",", "\n", "self_attn", ",", "\n", "src_attn", ",", "\n", "feed_forward", ",", "\n", "dropout_rate", ",", "\n", "normalize_before", "=", "True", ",", "\n", "concat_after", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an DecoderLayer object.\"\"\"", "\n", "super", "(", "DecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "self_attn", "=", "self_attn", "\n", "self", ".", "src_attn", "=", "src_attn", "\n", "self", ".", "feed_forward", "=", "feed_forward", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "size", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "size", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "self", ".", "normalize_before", "=", "normalize_before", "\n", "self", ".", "concat_after", "=", "concat_after", "\n", "if", "self", ".", "concat_after", ":", "\n", "            ", "self", ".", "concat_linear1", "=", "nn", ".", "Linear", "(", "size", "+", "size", ",", "size", ")", "\n", "self", ".", "concat_linear2", "=", "nn", ".", "Linear", "(", "size", "+", "size", ",", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder_layer.DecoderLayer.forward": [[58, 122], ["decoder_layer.DecoderLayer.norm1", "torch.cat", "decoder_layer.DecoderLayer.norm1", "decoder_layer.DecoderLayer.norm2", "torch.cat", "decoder_layer.DecoderLayer.norm2", "decoder_layer.DecoderLayer.norm3", "decoder_layer.DecoderLayer.dropout", "decoder_layer.DecoderLayer.norm3", "torch.cat", "decoder_layer.DecoderLayer.concat_linear1", "decoder_layer.DecoderLayer.dropout", "decoder_layer.DecoderLayer.concat_linear2", "decoder_layer.DecoderLayer.dropout", "decoder_layer.DecoderLayer.feed_forward", "decoder_layer.DecoderLayer.self_attn", "decoder_layer.DecoderLayer.self_attn", "decoder_layer.DecoderLayer.src_attn", "decoder_layer.DecoderLayer.src_attn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "tgt", ",", "tgt_mask", ",", "memory", ",", "memory_mask", ",", "cache", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute decoded features.\n        Args:\n            tgt (torch.Tensor):\n                decoded previous target features (batch, max_time_out, size)\n            tgt_mask (torch.Tensor): mask for x (batch, max_time_out)\n            memory (torch.Tensor): encoded source features (batch, max_time_in, size)\n            memory_mask (torch.Tensor): mask for memory (batch, max_time_in)\n            cache (torch.Tensor): cached output (batch, max_time_out-1, size)\n        \"\"\"", "\n", "residual", "=", "tgt", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "\n", "", "if", "cache", "is", "None", ":", "\n", "            ", "tgt_q", "=", "tgt", "\n", "tgt_q_mask", "=", "tgt_mask", "\n", "", "else", ":", "\n", "# compute only the last frame query keeping dim: max_time_out -> 1", "\n", "            ", "assert", "cache", ".", "shape", "==", "(", "\n", "tgt", ".", "shape", "[", "0", "]", ",", "\n", "tgt", ".", "shape", "[", "1", "]", "-", "1", ",", "\n", "self", ".", "size", ",", "\n", ")", ",", "f\"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}\"", "\n", "tgt_q", "=", "tgt", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "residual", "=", "residual", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "tgt_q_mask", "=", "None", "\n", "if", "tgt_mask", "is", "not", "None", ":", "\n", "                ", "tgt_q_mask", "=", "tgt_mask", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "\n", "", "", "if", "self", ".", "concat_after", ":", "\n", "            ", "tgt_concat", "=", "torch", ".", "cat", "(", "\n", "(", "tgt_q", ",", "self", ".", "self_attn", "(", "tgt_q", ",", "tgt", ",", "tgt", ",", "tgt_q_mask", ")", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "x", "=", "residual", "+", "self", ".", "concat_linear1", "(", "tgt_concat", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "residual", "+", "self", ".", "dropout", "(", "self", ".", "self_attn", "(", "tgt_q", ",", "tgt", ",", "tgt", ",", "tgt_q_mask", ")", ")", "\n", "", "if", "not", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm1", "(", "x", ")", "\n", "\n", "", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm2", "(", "x", ")", "\n", "", "if", "self", ".", "concat_after", ":", "\n", "            ", "x_concat", "=", "torch", ".", "cat", "(", "\n", "(", "x", ",", "self", ".", "src_attn", "(", "x", ",", "memory", ",", "memory", ",", "memory_mask", ")", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "x", "=", "residual", "+", "self", ".", "concat_linear2", "(", "x_concat", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "residual", "+", "self", ".", "dropout", "(", "self", ".", "src_attn", "(", "x", ",", "memory", ",", "memory", ",", "memory_mask", ")", ")", "\n", "", "if", "not", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm2", "(", "x", ")", "\n", "\n", "", "residual", "=", "x", "\n", "if", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm3", "(", "x", ")", "\n", "", "x", "=", "residual", "+", "self", ".", "dropout", "(", "self", ".", "feed_forward", "(", "x", ")", ")", "\n", "if", "not", "self", ".", "normalize_before", ":", "\n", "            ", "x", "=", "self", ".", "norm3", "(", "x", ")", "\n", "\n", "", "if", "cache", "is", "not", "None", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "[", "cache", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "return", "x", ",", "tgt_mask", ",", "memory", ",", "memory_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.subsampling.Conv2dSubsampling.__init__": [[22, 33], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "idim", ",", "odim", ",", "dropout_rate", ",", "pos_enc_class", ")", ":", "\n", "        ", "\"\"\"Construct an Conv2dSubsampling object.\"\"\"", "\n", "super", "(", "Conv2dSubsampling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Conv2d", "(", "1", ",", "odim", ",", "3", ",", "2", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Conv2d", "(", "odim", ",", "odim", ",", "3", ",", "2", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "self", ".", "out", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "odim", "*", "(", "(", "(", "idim", "-", "1", ")", "//", "2", "-", "1", ")", "//", "2", ")", ",", "odim", ")", ",", "pos_enc_class", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.subsampling.Conv2dSubsampling.forward": [[35, 53], ["subsampling.Conv2dSubsampling.unsqueeze", "subsampling.Conv2dSubsampling.conv", "subsampling.Conv2dSubsampling.size", "subsampling.Conv2dSubsampling.out", "subsampling.Conv2dSubsampling.transpose().contiguous().view", "subsampling.Conv2dSubsampling.transpose().contiguous", "subsampling.Conv2dSubsampling.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n               or Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]\n        \"\"\"", "\n", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# (b, c, t, f)", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "b", ",", "c", ",", "t", ",", "f", "=", "x", ".", "size", "(", ")", "\n", "# if RelPositionalEncoding, x: Tuple[torch.Tensor, torch.Tensor]", "\n", "# else x: torch.Tensor", "\n", "x", "=", "self", ".", "out", "(", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", ",", "t", ",", "c", "*", "f", ")", ")", "\n", "if", "x_mask", "is", "None", ":", "\n", "            ", "return", "x", ",", "None", "\n", "", "return", "x", ",", "x_mask", "[", ":", ",", ":", ",", ":", "-", "2", ":", "2", "]", "[", ":", ",", ":", ",", ":", "-", "2", ":", "2", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.__init__": [[15, 23], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model_size", ",", "factor", ",", "warmup", ",", "optimizer", ")", ":", "\n", "        ", "\"\"\"Construct an NoamOpt object.\"\"\"", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "_step", "=", "0", "\n", "self", ".", "warmup", "=", "warmup", "\n", "self", ".", "factor", "=", "factor", "\n", "self", ".", "model_size", "=", "model_size", "\n", "self", ".", "_rate", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.param_groups": [[24, 28], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "param_groups", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return param_groups.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "param_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.step": [[29, 37], ["optimizer.NoamOpt.rate", "optimizer.NoamOpt.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.rate", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update parameters and rate.\"\"\"", "\n", "self", ".", "_step", "+=", "1", "\n", "rate", "=", "self", ".", "rate", "(", ")", "\n", "for", "p", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "p", "[", "\"lr\"", "]", "=", "rate", "\n", "", "self", ".", "_rate", "=", "rate", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.rate": [[38, 46], ["min"], "methods", ["None"], ["", "def", "rate", "(", "self", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implement `lrate` above.\"\"\"", "\n", "if", "step", "is", "None", ":", "\n", "            ", "step", "=", "self", ".", "_step", "\n", "", "return", "(", "\n", "self", ".", "factor", "\n", "*", "self", ".", "model_size", "**", "(", "-", "0.5", ")", "\n", "*", "min", "(", "step", "**", "(", "-", "0.5", ")", ",", "step", "*", "self", ".", "warmup", "**", "(", "-", "1.5", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.zero_grad": [[48, 51], ["optimizer.NoamOpt.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset gradient.\"\"\"", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.state_dict": [[52, 61], ["optimizer.NoamOpt.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return state_dict.\"\"\"", "\n", "return", "{", "\n", "\"_step\"", ":", "self", ".", "_step", ",", "\n", "\"warmup\"", ":", "self", ".", "warmup", ",", "\n", "\"factor\"", ":", "self", ".", "factor", ",", "\n", "\"model_size\"", ":", "self", ".", "model_size", ",", "\n", "\"_rate\"", ":", "self", ".", "_rate", ",", "\n", "\"optimizer\"", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.NoamOpt.load_state_dict": [[63, 70], ["state_dict.items", "optimizer.NoamOpt.optimizer.load_state_dict", "setattr"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load state_dict.\"\"\"", "\n", "for", "key", ",", "value", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "==", "\"optimizer\"", ":", "\n", "                ", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "\"optimizer\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.optimizer.get_std_opt": [[72, 76], ["torch.optim.Adam", "optimizer.NoamOpt", "model.parameters"], "function", ["None"], ["", "", "", "", "def", "get_std_opt", "(", "model", ",", "d_model", ",", "warmup", ",", "factor", ")", ":", "\n", "    ", "\"\"\"Get standard NoamOpt.\"\"\"", "\n", "base", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "0", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "eps", "=", "1e-9", ")", "\n", "return", "NoamOpt", "(", "d_model", ",", "factor", ",", "warmup", ",", "base", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.subsequent_mask": [[20, 39], ["torch.ones", "torch.tril().type", "torch.ones", "torch.tril", "torch.tril"], "function", ["None"], ["def", "subsequent_mask", "(", "size", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "datatype", ")", ":", "\n", "    ", "\"\"\"Create mask for subsequent steps (1, size, size).\n\n    :param int size: size of mask\n    :param str device: \"cpu\" or \"cuda\" or torch.Tensor.device\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    >>> subsequent_mask(3)\n    [[1, 0, 0],\n     [1, 1, 0],\n     [1, 1, 1]]\n    \"\"\"", "\n", "if", "is_torch_1_2", "and", "dtype", "==", "torch", ".", "bool", ":", "\n", "# torch=1.2 doesn't support tril for bool tensor", "\n", "        ", "ret", "=", "torch", ".", "ones", "(", "size", ",", "size", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "return", "torch", ".", "tril", "(", "ret", ",", "out", "=", "ret", ")", ".", "type", "(", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "ret", "=", "torch", ".", "ones", "(", "size", ",", "size", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "\n", "return", "torch", ".", "tril", "(", "ret", ",", "out", "=", "ret", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.target_mask": [[41, 52], ["subsequent_mask().unsqueeze", "ys_mask.unsqueeze", "mask.subsequent_mask", "ys_mask.size"], "function", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.subsequent_mask"], ["", "", "def", "target_mask", "(", "ys_in_pad", ",", "ignore_id", ")", ":", "\n", "    ", "\"\"\"Create mask for decoder self-attention.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int ignore_id: index of padding\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    \"\"\"", "\n", "ys_mask", "=", "ys_in_pad", "!=", "ignore_id", "\n", "m", "=", "subsequent_mask", "(", "ys_mask", ".", "size", "(", "-", "1", ")", ",", "device", "=", "ys_mask", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "ys_mask", ".", "unsqueeze", "(", "-", "2", ")", "&", "m", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.MultiSequential.forward": [[15, 20], ["m"], "methods", ["None"], ["def", "forward", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Repeat.\"\"\"", "\n", "for", "m", "in", "self", ":", "\n", "            ", "args", "=", "m", "(", "*", "args", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.repeat.repeat": [[22, 31], ["repeat.MultiSequential", "fn", "range"], "function", ["None"], ["", "", "def", "repeat", "(", "N", ",", "fn", ")", ":", "\n", "    ", "\"\"\"Repeat module N times.\n\n    :param int N: repeat time\n    :param function fn: function to generate module\n    :return: repeated modules\n    :rtype: MultiSequential\n    \"\"\"", "\n", "return", "MultiSequential", "(", "*", "[", "fn", "(", ")", "for", "_", "in", "range", "(", "N", ")", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.multi_layer_conv.MultiLayeredConv1d.__init__": [[25, 51], ["super().__init__", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "in_chans", ",", "hidden_chans", ",", "kernel_size", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"Initialize MultiLayeredConv1d module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        \"\"\"", "\n", "super", "(", "MultiLayeredConv1d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_chans", ",", "\n", "hidden_chans", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "\n", ")", "\n", "self", ".", "w_2", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "hidden_chans", ",", "\n", "in_chans", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.multi_layer_conv.MultiLayeredConv1d.forward": [[52, 64], ["torch.relu().transpose", "multi_layer_conv.MultiLayeredConv1d.w_2().transpose", "torch.relu", "multi_layer_conv.MultiLayeredConv1d.w_2", "multi_layer_conv.MultiLayeredConv1d.w_1", "multi_layer_conv.MultiLayeredConv1d.dropout().transpose", "torch.relu().transpose.transpose", "multi_layer_conv.MultiLayeredConv1d.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        \"\"\"", "\n", "x", "=", "torch", ".", "relu", "(", "self", ".", "w_1", "(", "x", ".", "transpose", "(", "-", "1", ",", "1", ")", ")", ")", ".", "transpose", "(", "-", "1", ",", "1", ")", "\n", "return", "self", ".", "w_2", "(", "self", ".", "dropout", "(", "x", ")", ".", "transpose", "(", "-", "1", ",", "1", ")", ")", ".", "transpose", "(", "-", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.multi_layer_conv.Conv1dLinear.__init__": [[73, 93], ["super().__init__", "torch.nn.Conv1d", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "in_chans", ",", "hidden_chans", ",", "kernel_size", ",", "dropout_rate", ")", ":", "\n", "        ", "\"\"\"Initialize Conv1dLinear module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        \"\"\"", "\n", "super", "(", "Conv1dLinear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_chans", ",", "\n", "hidden_chans", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "\n", ")", "\n", "self", ".", "w_2", "=", "torch", ".", "nn", ".", "Linear", "(", "hidden_chans", ",", "in_chans", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.multi_layer_conv.Conv1dLinear.forward": [[94, 106], ["torch.relu().transpose", "multi_layer_conv.Conv1dLinear.w_2", "multi_layer_conv.Conv1dLinear.dropout", "torch.relu", "multi_layer_conv.Conv1dLinear.w_1", "torch.relu().transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        \"\"\"", "\n", "x", "=", "torch", ".", "relu", "(", "self", ".", "w_1", "(", "x", ".", "transpose", "(", "-", "1", ",", "1", ")", ")", ")", ".", "transpose", "(", "-", "1", ",", "1", ")", "\n", "return", "self", ".", "w_2", "(", "self", ".", "dropout", "(", "x", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.label_smoothing_loss.LabelSmoothingLoss.__init__": [[23, 40], ["torch.nn.KLDivLoss", "torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "size", ",", "\n", "padding_idx", ",", "\n", "smoothing", ",", "\n", "normalize_length", "=", "False", ",", "\n", "criterion", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"none\"", ")", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Construct an LabelSmoothingLoss object.\"\"\"", "\n", "super", "(", "LabelSmoothingLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "criterion", "=", "criterion", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "confidence", "=", "1.0", "-", "smoothing", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "true_dist", "=", "None", "\n", "self", ".", "normalize_length", "=", "normalize_length", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.label_smoothing_loss.LabelSmoothingLoss.forward": [[41, 64], ["x.view.view.size", "x.view.view.view", "target.masked_fill.masked_fill.view", "label_smoothing_loss.LabelSmoothingLoss.criterion", "x.view.view.size", "torch.no_grad", "x.view.view.clone", "x.view.clone.fill_", "target.masked_fill.masked_fill.masked_fill", "x.view.clone.scatter_", "torch.log_softmax", "label_smoothing_loss.LabelSmoothingLoss.masked_fill().sum", "len", "ignore.sum().item", "target.masked_fill.masked_fill.unsqueeze", "label_smoothing_loss.LabelSmoothingLoss.masked_fill", "ignore.sum", "ignore.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "\"\"\"Compute loss between x and target.\n\n        :param torch.Tensor x: prediction (batch, seqlen, class)\n        :param torch.Tensor target:\n            target signal masked with self.padding_id (batch, seqlen)\n        :return: scalar float value\n        :rtype torch.Tensor\n        \"\"\"", "\n", "assert", "x", ".", "size", "(", "2", ")", "==", "self", ".", "size", "\n", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "size", ")", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "true_dist", "=", "x", ".", "clone", "(", ")", "\n", "true_dist", ".", "fill_", "(", "self", ".", "smoothing", "/", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "ignore", "=", "target", "==", "self", ".", "padding_idx", "# (B,)", "\n", "total", "=", "len", "(", "target", ")", "-", "ignore", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "target", "=", "target", ".", "masked_fill", "(", "ignore", ",", "0", ")", "# avoid -1 index", "\n", "true_dist", ".", "scatter_", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "\n", "", "kl", "=", "self", ".", "criterion", "(", "torch", ".", "log_softmax", "(", "x", ",", "dim", "=", "1", ")", ",", "true_dist", ")", "\n", "denom", "=", "total", "if", "self", ".", "normalize_length", "else", "batch_size", "\n", "return", "kl", ".", "masked_fill", "(", "ignore", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", ".", "sum", "(", ")", "/", "denom", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv3d_extractor.Conv3dResNet.__init__": [[29, 82], ["super().__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "espnet.nets.pytorch_backend.backbones.modules.resnet.ResNet", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "espnet.nets.pytorch_backend.backbones.modules.shufflenetv2.ShuffleNetV2", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.PReLU", "torch.PReLU", "torch.PReLU", "espnet.nets.pytorch_backend.transformer.convolution.Swish"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "backbone_type", "=", "\"resnet\"", ",", "relu_type", "=", "\"swish\"", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param backbone_type: str, the type of a visual front-end.\n        :param relu_type: str, activation function used in an audio front-end.\n        \"\"\"", "\n", "super", "(", "Conv3dResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "backbone_type", "=", "backbone_type", "\n", "\n", "if", "self", ".", "backbone_type", "==", "\"resnet\"", ":", "\n", "            ", "self", ".", "frontend_nout", "=", "64", "\n", "self", ".", "trunk", "=", "ResNet", "(", "\n", "BasicBlock", ",", "\n", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "relu_type", "=", "relu_type", ",", "\n", ")", "\n", "", "elif", "self", ".", "backbone_type", "==", "\"shufflenet\"", ":", "\n", "            ", "shufflenet", "=", "ShuffleNetV2", "(", "\n", "input_size", "=", "96", ",", "\n", "width_mult", "=", "1.0", "\n", ")", "\n", "self", ".", "trunk", "=", "nn", ".", "Sequential", "(", "\n", "shufflenet", ".", "features", ",", "\n", "shufflenet", ".", "conv_last", ",", "\n", "shufflenet", ".", "globalpool", ",", "\n", ")", "\n", "self", ".", "frontend_nout", "=", "24", "\n", "self", ".", "stage_out_channels", "=", "shufflenet", ".", "stage_out_channels", "[", "-", "1", "]", "\n", "\n", "# -- frontend3D", "\n", "", "if", "relu_type", "==", "'relu'", ":", "\n", "            ", "frontend_relu", "=", "nn", ".", "ReLU", "(", "True", ")", "\n", "", "elif", "relu_type", "==", "'prelu'", ":", "\n", "            ", "frontend_relu", "=", "nn", ".", "PReLU", "(", "self", ".", "frontend_nout", ")", "\n", "", "elif", "relu_type", "==", "'swish'", ":", "\n", "            ", "frontend_relu", "=", "Swish", "(", ")", "\n", "\n", "", "self", ".", "frontend3D", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv3d", "(", "\n", "in_channels", "=", "1", ",", "\n", "out_channels", "=", "self", ".", "frontend_nout", ",", "\n", "kernel_size", "=", "(", "5", ",", "7", ",", "7", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "2", ",", "3", ",", "3", ")", ",", "\n", "bias", "=", "False", "\n", ")", ",", "\n", "nn", ".", "BatchNorm3d", "(", "self", ".", "frontend_nout", ")", ",", "\n", "frontend_relu", ",", "\n", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv3d_extractor.Conv3dResNet.forward": [[86, 102], ["xs_pad.view.view.unsqueeze", "xs_pad.view.view.size", "conv3d_extractor.Conv3dResNet.frontend3D", "conv3d_extractor.threeD_to_2D_tensor", "conv3d_extractor.Conv3dResNet.trunk", "xs_pad.view.view.view", "xs_pad.view.view.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv3d_extractor.threeD_to_2D_tensor"], ["", "def", "forward", "(", "self", ",", "xs_pad", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param xs_pad: torch.Tensor, batch of padded input sequences.\n        \"\"\"", "\n", "# -- include Channel dimension", "\n", "xs_pad", "=", "xs_pad", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "B", ",", "C", ",", "T", ",", "H", ",", "W", "=", "xs_pad", ".", "size", "(", ")", "\n", "xs_pad", "=", "self", ".", "frontend3D", "(", "xs_pad", ")", "\n", "Tnew", "=", "xs_pad", ".", "shape", "[", "2", "]", "# outpu should be B x C2 x Tnew x H x W", "\n", "xs_pad", "=", "threeD_to_2D_tensor", "(", "xs_pad", ")", "\n", "xs_pad", "=", "self", ".", "trunk", "(", "xs_pad", ")", "\n", "xs_pad", "=", "xs_pad", ".", "view", "(", "B", ",", "Tnew", ",", "xs_pad", ".", "size", "(", "1", ")", ")", "\n", "\n", "return", "xs_pad", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv3d_extractor.threeD_to_2D_tensor": [[19, 23], ["x.transpose.transpose", "x.transpose.reshape"], "function", ["None"], ["def", "threeD_to_2D_tensor", "(", "x", ")", ":", "\n", "    ", "n_batch", ",", "n_channels", ",", "s_time", ",", "sx", ",", "sy", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", ".", "reshape", "(", "n_batch", "*", "s_time", ",", "n_channels", ",", "sx", ",", "sy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv1d_extractor.Conv1dResNet.__init__": [[17, 33], ["super().__init__", "espnet.nets.pytorch_backend.backbones.modules.resnet1d.ResNet1D"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "self", ",", "relu_type", "=", "\"swish\"", ",", "a_upsample_ratio", "=", "1", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param relu_type: str, Activation function used in an audio front-end.\n        :param a_upsample_ratio: int, The ratio related to the \\\n            temporal resolution of output features of the frontend. \\\n            a_upsample_ratio=1 produce features with a fps of 25.\n        \"\"\"", "\n", "\n", "super", "(", "Conv1dResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "a_upsample_ratio", "=", "a_upsample_ratio", "\n", "self", ".", "trunk", "=", "ResNet1D", "(", "\n", "BasicBlock1D", ",", "\n", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "relu_type", "=", "relu_type", ",", "\n", "a_upsample_ratio", "=", "a_upsample_ratio", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.backbones.conv1d_extractor.Conv1dResNet.forward": [[35, 47], ["xs_pad.transpose.transpose.size", "xs_pad.transpose.transpose.transpose", "conv1d_extractor.Conv1dResNet.trunk", "xs_pad.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "xs_pad", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param xs_pad: torch.Tensor, batch of padded input sequences (B, Tmax, idim)\n        \"\"\"", "\n", "B", ",", "T", ",", "C", "=", "xs_pad", ".", "size", "(", ")", "\n", "xs_pad", "=", "xs_pad", "[", ":", ",", ":", "T", "//", "640", "*", "640", ",", ":", "]", "\n", "xs_pad", "=", "xs_pad", ".", "transpose", "(", "1", ",", "2", ")", "\n", "xs_pad", "=", "self", ".", "trunk", "(", "xs_pad", ")", "\n", "# -- from B x C x T to B x T x C", "\n", "xs_pad", "=", "xs_pad", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "xs_pad", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.InvertedResidual.__init__": [[43, 89], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inp", ",", "oup", ",", "stride", ",", "benchmodel", ")", ":", "\n", "        ", "super", "(", "InvertedResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "benchmodel", "=", "benchmodel", "\n", "self", ".", "stride", "=", "stride", "\n", "assert", "stride", "in", "[", "1", ",", "2", "]", "\n", "\n", "oup_inc", "=", "oup", "//", "2", "\n", "\n", "if", "self", ".", "benchmodel", "==", "1", ":", "\n", "#assert inp == oup_inc", "\n", "            ", "self", ".", "banch2", "=", "nn", ".", "Sequential", "(", "\n", "# pw", "\n", "nn", ".", "Conv2d", "(", "oup_inc", ",", "oup_inc", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "# dw", "\n", "nn", ".", "Conv2d", "(", "oup_inc", ",", "oup_inc", ",", "3", ",", "stride", ",", "1", ",", "groups", "=", "oup_inc", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "# pw-linear", "\n", "nn", ".", "Conv2d", "(", "oup_inc", ",", "oup_inc", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "banch1", "=", "nn", ".", "Sequential", "(", "\n", "# dw", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "inp", ",", "3", ",", "stride", ",", "1", ",", "groups", "=", "inp", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "inp", ")", ",", "\n", "# pw-linear", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "oup_inc", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n", "\n", "self", ".", "banch2", "=", "nn", ".", "Sequential", "(", "\n", "# pw", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "oup_inc", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "# dw", "\n", "nn", ".", "Conv2d", "(", "oup_inc", ",", "oup_inc", ",", "3", ",", "stride", ",", "1", ",", "groups", "=", "oup_inc", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "# pw-linear", "\n", "nn", ".", "Conv2d", "(", "oup_inc", ",", "oup_inc", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup_inc", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.InvertedResidual._concat": [[91, 95], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_concat", "(", "x", ",", "out", ")", ":", "\n", "# concatenate along channel axis", "\n", "        ", "return", "torch", ".", "cat", "(", "(", "x", ",", "out", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.InvertedResidual.forward": [[96, 105], ["shufflenetv2.channel_shuffle", "shufflenetv2.InvertedResidual._concat", "shufflenetv2.InvertedResidual.banch2", "shufflenetv2.InvertedResidual._concat", "shufflenetv2.InvertedResidual.banch1", "shufflenetv2.InvertedResidual.banch2"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.channel_shuffle", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.InvertedResidual._concat", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.InvertedResidual._concat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "1", "==", "self", ".", "benchmodel", ":", "\n", "            ", "x1", "=", "x", "[", ":", ",", ":", "(", "x", ".", "shape", "[", "1", "]", "//", "2", ")", ",", ":", ",", ":", "]", "\n", "x2", "=", "x", "[", ":", ",", "(", "x", ".", "shape", "[", "1", "]", "//", "2", ")", ":", ",", ":", ",", ":", "]", "\n", "out", "=", "self", ".", "_concat", "(", "x1", ",", "self", ".", "banch2", "(", "x2", ")", ")", "\n", "", "elif", "2", "==", "self", ".", "benchmodel", ":", "\n", "            ", "out", "=", "self", ".", "_concat", "(", "self", ".", "banch1", "(", "x", ")", ",", "self", ".", "banch2", "(", "x", ")", ")", "\n", "\n", "", "return", "channel_shuffle", "(", "out", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.ShuffleNetV2.__init__": [[108, 156], ["torch.Module.__init__", "shufflenetv2.conv_bn", "torch.MaxPool2d", "torch.MaxPool2d", "torch.MaxPool2d", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "shufflenetv2.conv_1x1_bn", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "range", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Linear", "torch.Linear", "torch.Linear", "int", "shufflenetv2.ShuffleNetV2.features.append", "shufflenetv2.ShuffleNetV2.features.append", "ValueError", "shufflenetv2.InvertedResidual", "shufflenetv2.InvertedResidual"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.conv_bn", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.conv_1x1_bn"], ["    ", "def", "__init__", "(", "self", ",", "n_class", "=", "1000", ",", "input_size", "=", "224", ",", "width_mult", "=", "2.", ")", ":", "\n", "        ", "super", "(", "ShuffleNetV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "input_size", "%", "32", "==", "0", ",", "\"Input size needs to be divisible by 32\"", "\n", "\n", "self", ".", "stage_repeats", "=", "[", "4", ",", "8", ",", "4", "]", "\n", "# index 0 is invalid and should never be called.", "\n", "# only used for indexing convenience.", "\n", "if", "width_mult", "==", "0.5", ":", "\n", "            ", "self", ".", "stage_out_channels", "=", "[", "-", "1", ",", "24", ",", "48", ",", "96", ",", "192", ",", "1024", "]", "\n", "", "elif", "width_mult", "==", "1.0", ":", "\n", "            ", "self", ".", "stage_out_channels", "=", "[", "-", "1", ",", "24", ",", "116", ",", "232", ",", "464", ",", "1024", "]", "\n", "", "elif", "width_mult", "==", "1.5", ":", "\n", "            ", "self", ".", "stage_out_channels", "=", "[", "-", "1", ",", "24", ",", "176", ",", "352", ",", "704", ",", "1024", "]", "\n", "", "elif", "width_mult", "==", "2.0", ":", "\n", "            ", "self", ".", "stage_out_channels", "=", "[", "-", "1", ",", "24", ",", "244", ",", "488", ",", "976", ",", "2048", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"\"\"Width multiplier should be in [0.5, 1.0, 1.5, 2.0]. Current value: {}\"\"\"", ".", "format", "(", "width_mult", ")", ")", "\n", "\n", "# building first layer", "\n", "", "input_channel", "=", "self", ".", "stage_out_channels", "[", "1", "]", "\n", "self", ".", "conv1", "=", "conv_bn", "(", "3", ",", "input_channel", ",", "2", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "\n", "self", ".", "features", "=", "[", "]", "\n", "# building inverted residual blocks", "\n", "for", "idxstage", "in", "range", "(", "len", "(", "self", ".", "stage_repeats", ")", ")", ":", "\n", "            ", "numrepeat", "=", "self", ".", "stage_repeats", "[", "idxstage", "]", "\n", "output_channel", "=", "self", ".", "stage_out_channels", "[", "idxstage", "+", "2", "]", "\n", "for", "i", "in", "range", "(", "numrepeat", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "#inp, oup, stride, benchmodel):", "\n", "                    ", "self", ".", "features", ".", "append", "(", "InvertedResidual", "(", "input_channel", ",", "output_channel", ",", "2", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "features", ".", "append", "(", "InvertedResidual", "(", "input_channel", ",", "output_channel", ",", "1", ",", "1", ")", ")", "\n", "", "input_channel", "=", "output_channel", "\n", "\n", "\n", "# make it nn.Sequential", "\n", "", "", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "features", ")", "\n", "\n", "# building last several layers", "\n", "self", ".", "conv_last", "=", "conv_1x1_bn", "(", "input_channel", ",", "self", ".", "stage_out_channels", "[", "-", "1", "]", ")", "\n", "self", ".", "globalpool", "=", "nn", ".", "Sequential", "(", "nn", ".", "AvgPool2d", "(", "int", "(", "input_size", "/", "32", ")", ")", ")", "\n", "\n", "# building classifier", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "stage_out_channels", "[", "-", "1", "]", ",", "n_class", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.ShuffleNetV2.forward": [[157, 166], ["shufflenetv2.ShuffleNetV2.conv1", "shufflenetv2.ShuffleNetV2.maxpool", "shufflenetv2.ShuffleNetV2.features", "shufflenetv2.ShuffleNetV2.conv_last", "shufflenetv2.ShuffleNetV2.globalpool", "shufflenetv2.ShuffleNetV2.view", "shufflenetv2.ShuffleNetV2.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_last", "(", "x", ")", "\n", "x", "=", "self", ".", "globalpool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "stage_out_channels", "[", "-", "1", "]", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.conv_bn": [[11, 16], ["torch.Sequential", "torch.Conv2d", "torch.BatchNorm2d", "torch.ReLU"], "function", ["None"], ["def", "conv_bn", "(", "inp", ",", "oup", ",", "stride", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "oup", ",", "3", ",", "stride", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.conv_1x1_bn": [[19, 24], ["torch.Sequential", "torch.Conv2d", "torch.BatchNorm2d", "torch.ReLU"], "function", ["None"], ["", "def", "conv_1x1_bn", "(", "inp", ",", "oup", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inp", ",", "oup", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "oup", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.shufflenetv2.channel_shuffle": [[26, 41], ["x.view.data.size", "x.view.view", "torch.transpose().contiguous", "torch.transpose().contiguous", "torch.transpose().contiguous", "x.view.view", "torch.transpose", "torch.transpose", "torch.transpose"], "function", ["None"], ["", "def", "channel_shuffle", "(", "x", ",", "groups", ")", ":", "\n", "    ", "batchsize", ",", "num_channels", ",", "height", ",", "width", "=", "x", ".", "data", ".", "size", "(", ")", "\n", "\n", "channels_per_group", "=", "num_channels", "//", "groups", "\n", "\n", "# reshape", "\n", "x", "=", "x", ".", "view", "(", "batchsize", ",", "groups", ",", "\n", "channels_per_group", ",", "height", ",", "width", ")", "\n", "\n", "x", "=", "torch", ".", "transpose", "(", "x", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# flatten", "\n", "x", "=", "x", ".", "view", "(", "batchsize", ",", "-", "1", ",", "height", ",", "width", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.BasicBlock.__init__": [[47, 88], ["torch.Module.__init__", "resnet.conv3x3", "torch.BatchNorm2d", "resnet.conv3x3", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU", "espnet.nets.pytorch_backend.transformer.convolution.Swish", "espnet.nets.pytorch_backend.transformer.convolution.Swish"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.conv3x3", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.conv3x3"], ["def", "__init__", "(", "\n", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "relu_type", "=", "\"swish\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"", "\n", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "relu_type", "in", "[", "\"relu\"", ",", "\"prelu\"", ",", "\"swish\"", "]", "\n", "\n", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "\n", "if", "relu_type", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "", "elif", "relu_type", "==", "\"prelu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "", "elif", "relu_type", "==", "\"swish\"", ":", "\n", "            ", "self", ".", "relu1", "=", "Swish", "(", ")", "\n", "self", ".", "relu2", "=", "Swish", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# --------", "\n", "\n", "", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.BasicBlock.forward": [[89, 107], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.bn1", "resnet.BasicBlock.relu1", "resnet.BasicBlock.conv2", "resnet.BasicBlock.bn2", "resnet.BasicBlock.relu2", "resnet.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"", "\n", "residual", "=", "x", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu1", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "residual", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "residual", "\n", "out", "=", "self", ".", "relu2", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.ResNet.__init__": [[111, 127], ["torch.Module.__init__", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "torch.AdaptiveAvgPool2d"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "block", ",", "\n", "layers", ",", "\n", "relu_type", "=", "\"swish\"", ",", "\n", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "relu_type", "=", "relu_type", "\n", "self", ".", "downsample_block", "=", "downsample_basic_block", "\n", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.ResNet._make_layer": [[129, 166], ["layers.append", "range", "torch.Sequential", "resnet.ResNet.downsample_block", "block", "layers.append", "block"], "methods", ["None"], ["", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ")", ":", "\n", "        ", "\"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"", "\n", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "self", ".", "downsample_block", "(", "\n", "inplanes", "=", "self", ".", "inplanes", ",", "\n", "outplanes", "=", "planes", "*", "block", ".", "expansion", ",", "\n", "stride", "=", "stride", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "stride", ",", "\n", "downsample", ",", "\n", "relu_type", "=", "self", ".", "relu_type", ",", "\n", ")", "\n", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "relu_type", "=", "self", ".", "relu_type", ",", "\n", ")", "\n", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.ResNet.forward": [[167, 179], ["resnet.ResNet.layer1", "resnet.ResNet.layer2", "resnet.ResNet.layer3", "resnet.ResNet.layer4", "resnet.ResNet.avgpool", "x.view.view.view", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T, H, W).\n        \"\"\"", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.conv3x3": [[8, 22], ["torch.Conv2d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"", "\n", "return", "nn", ".", "Conv2d", "(", "\n", "in_planes", ",", "\n", "out_planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet.downsample_basic_block": [[25, 41], ["torch.Sequential", "torch.Conv2d", "torch.BatchNorm2d"], "function", ["None"], ["", "def", "downsample_basic_block", "(", "inplanes", ",", "outplanes", ",", "stride", ")", ":", "\n", "    ", "\"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"", "\n", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "inplanes", ",", "\n", "outplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "outplanes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.BasicBlock1D.__init__": [[47, 89], ["torch.Module.__init__", "resnet1d.conv3x3", "torch.BatchNorm1d", "resnet1d.conv3x3", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU", "espnet.nets.pytorch_backend.transformer.convolution.Swish", "espnet.nets.pytorch_backend.transformer.convolution.Swish"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.conv3x3", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.conv3x3"], ["def", "__init__", "(", "\n", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "relu_type", "=", "\"relu\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param inplanes: int, number of channels in the input sequence.\n        :param planes: int,  number of channels produced by the convolution.\n        :param stride: int, size of the convolving kernel.\n        :param downsample: boolean, if True, the temporal resolution is downsampled.\n        :param relu_type: str, type of activation function.\n        \"\"\"", "\n", "super", "(", "BasicBlock1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "relu_type", "in", "[", "\"relu\"", ",", "\"prelu\"", ",", "\"swish\"", "]", "\n", "\n", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm1d", "(", "planes", ")", "\n", "\n", "# type of ReLU is an input option", "\n", "if", "relu_type", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "", "elif", "relu_type", "==", "\"prelu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "", "elif", "relu_type", "==", "\"swish\"", ":", "\n", "            ", "self", ".", "relu1", "=", "Swish", "(", ")", "\n", "self", ".", "relu2", "=", "Swish", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# --------", "\n", "\n", "", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm1d", "(", "planes", ")", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.BasicBlock1D.forward": [[90, 108], ["resnet1d.BasicBlock1D.conv1", "resnet1d.BasicBlock1D.bn1", "resnet1d.BasicBlock1D.relu1", "resnet1d.BasicBlock1D.conv2", "resnet1d.BasicBlock1D.bn2", "resnet1d.BasicBlock1D.relu2", "resnet1d.BasicBlock1D.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"", "\n", "residual", "=", "x", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu1", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "residual", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "residual", "\n", "out", "=", "self", ".", "relu2", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D.__init__": [[112, 157], ["torch.Module.__init__", "torch.Conv1d", "torch.BatchNorm1d", "resnet1d.ResNet1D._make_layer", "resnet1d.ResNet1D._make_layer", "resnet1d.ResNet1D._make_layer", "resnet1d.ResNet1D._make_layer", "torch.AvgPool1d", "torch.ReLU", "torch.PReLU", "espnet.nets.pytorch_backend.transformer.convolution.Swish"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "\n", "block", ",", "\n", "layers", ",", "\n", "relu_type", "=", "\"swish\"", ",", "\n", "a_upsample_ratio", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"__init__.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param layers: List, customised layers in each block.\n        :param relu_type: str, type of activation function.\n        :param a_upsample_ratio: int, The ratio related to the \\\n            temporal resolution of output features of the frontend. \\\n            a_upsample_ratio=1 produce features with a fps of 25.\n        \"\"\"", "\n", "super", "(", "ResNet1D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "relu_type", "=", "relu_type", "\n", "self", ".", "downsample_block", "=", "downsample_basic_block", "\n", "self", ".", "a_upsample_ratio", "=", "a_upsample_ratio", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "1", ",", "\n", "out_channels", "=", "self", ".", "inplanes", ",", "\n", "kernel_size", "=", "80", ",", "\n", "stride", "=", "4", ",", "\n", "padding", "=", "38", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm1d", "(", "self", ".", "inplanes", ")", "\n", "\n", "if", "relu_type", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "", "elif", "relu_type", "==", "\"prelu\"", ":", "\n", "            ", "self", ".", "relu", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "self", ".", "inplanes", ")", "\n", "", "elif", "relu_type", "==", "\"swish\"", ":", "\n", "            ", "self", ".", "relu", "=", "Swish", "(", ")", "\n", "\n", "", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AvgPool1d", "(", "\n", "kernel_size", "=", "20", "//", "self", ".", "a_upsample_ratio", ",", "\n", "stride", "=", "20", "//", "self", ".", "a_upsample_ratio", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D._make_layer": [[160, 198], ["layers.append", "range", "torch.Sequential", "resnet1d.ResNet1D.downsample_block", "block", "layers.append", "block"], "methods", ["None"], ["", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ")", ":", "\n", "        ", "\"\"\"_make_layer.\n\n        :param block: torch.nn.Module, class of blocks.\n        :param planes: int,  number of channels produced by the convolution.\n        :param blocks: int, number of layers in a block.\n        :param stride: int, size of the convolving kernel.\n        \"\"\"", "\n", "\n", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "self", ".", "downsample_block", "(", "\n", "inplanes", "=", "self", ".", "inplanes", ",", "\n", "outplanes", "=", "planes", "*", "block", ".", "expansion", ",", "\n", "stride", "=", "stride", ",", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "stride", ",", "\n", "downsample", ",", "\n", "relu_type", "=", "self", ".", "relu_type", ",", "\n", ")", "\n", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "relu_type", "=", "self", ".", "relu_type", ",", "\n", ")", "\n", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.ResNet1D.forward": [[199, 214], ["resnet1d.ResNet1D.conv1", "resnet1d.ResNet1D.bn1", "resnet1d.ResNet1D.relu", "resnet1d.ResNet1D.layer1", "resnet1d.ResNet1D.layer2", "resnet1d.ResNet1D.layer3", "resnet1d.ResNet1D.layer4", "resnet1d.ResNet1D.avgpool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"forward.\n\n        :param x: torch.Tensor, input tensor with input size (B, C, T)\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.conv3x3": [[8, 22], ["torch.Conv1d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"conv3x3.\n\n    :param in_planes: int, number of channels in the input sequence.\n    :param out_planes: int,  number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"", "\n", "return", "nn", ".", "Conv1d", "(", "\n", "in_planes", ",", "\n", "out_planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.modules.resnet1d.downsample_basic_block": [[25, 41], ["torch.Sequential", "torch.Conv1d", "torch.BatchNorm1d"], "function", ["None"], ["", "def", "downsample_basic_block", "(", "inplanes", ",", "outplanes", ",", "stride", ")", ":", "\n", "    ", "\"\"\"downsample_basic_block.\n\n    :param inplanes: int, number of channels in the input sequence.\n    :param outplanes: int, number of channels produced by the convolution.\n    :param stride: int, size of the convolving kernel.\n    \"\"\"", "\n", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "inplanes", ",", "\n", "outplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "outplanes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.add_arguments": [[28, 68], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_arguments", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to command line argument parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\n", "\"--type\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"lstm\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "choices", "=", "[", "\"lstm\"", ",", "\"gru\"", "]", ",", "\n", "help", "=", "\"Which type of RNN to use\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--layer\"", ",", "\"-l\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Number of hidden layers\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--unit\"", ",", "\"-u\"", ",", "type", "=", "int", ",", "default", "=", "650", ",", "help", "=", "\"Number of hidden units\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--embed-unit\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Number of hidden units in embedding layer, \"", "\n", "\"if it is not specified, it keeps the same number with hidden units.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dropout-rate\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "\"dropout probability\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--emb-dropout-rate\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.0", ",", "\n", "help", "=", "\"emb dropout probability\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tie-weights\"", ",", "\n", "type", "=", "strtobool", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"Tie input and output embeddings\"", ",", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.__init__": [[69, 97], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "getattr", "getattr", "getattr", "getattr", "default.ClassifierWithState", "default.RNNLM"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["", "def", "__init__", "(", "self", ",", "n_vocab", ",", "args", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        \"\"\"", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "# NOTE: for a compatibility with less than 0.5.0 version models", "\n", "dropout_rate", "=", "getattr", "(", "args", ",", "\"dropout_rate\"", ",", "0.0", ")", "\n", "# NOTE: for a compatibility with less than 0.6.1 version models", "\n", "embed_unit", "=", "getattr", "(", "args", ",", "\"embed_unit\"", ",", "None", ")", "\n", "# NOTE: for a compatibility with less than 0.9.7 version models", "\n", "emb_dropout_rate", "=", "getattr", "(", "args", ",", "\"emb_dropout_rate\"", ",", "0.0", ")", "\n", "# NOTE: for a compatibility with less than 0.9.7 version models", "\n", "tie_weights", "=", "getattr", "(", "args", ",", "\"tie_weights\"", ",", "False", ")", "\n", "\n", "self", ".", "model", "=", "ClassifierWithState", "(", "\n", "RNNLM", "(", "\n", "n_vocab", ",", "\n", "args", ".", "layer", ",", "\n", "args", ".", "unit", ",", "\n", "embed_unit", ",", "\n", "args", ".", "type", ",", "\n", "dropout_rate", ",", "\n", "emb_dropout_rate", ",", "\n", "tie_weights", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict": [[100, 103], ["default.DefaultRNNLM.model.state_dict"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Dump state dict.\"\"\"", "\n", "return", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict": [[104, 107], ["default.DefaultRNNLM.model.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "d", ")", ":", "\n", "        ", "\"\"\"Load state dict.\"\"\"", "\n", "self", ".", "model", ".", "load_state_dict", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.forward": [[108, 139], ["torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "range", "default.DefaultRNNLM.model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "int", "torch.tensor().long.to", "torch.tensor().long.to", "torch.tensor().long.to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "loss_batch.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "t", ")", ":", "\n", "        ", "\"\"\"Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        \"\"\"", "\n", "loss", "=", "0", "\n", "logp", "=", "0", "\n", "count", "=", "torch", ".", "tensor", "(", "0", ")", ".", "long", "(", ")", "\n", "state", "=", "None", "\n", "batch_size", ",", "sequence_length", "=", "x", ".", "shape", "\n", "for", "i", "in", "range", "(", "sequence_length", ")", ":", "\n", "# Compute the loss at this time step and accumulate it", "\n", "            ", "state", ",", "loss_batch", "=", "self", ".", "model", "(", "state", ",", "x", "[", ":", ",", "i", "]", ",", "t", "[", ":", ",", "i", "]", ")", "\n", "non_zeros", "=", "torch", ".", "sum", "(", "x", "[", ":", ",", "i", "]", "!=", "0", ",", "dtype", "=", "loss_batch", ".", "dtype", ")", "\n", "loss", "+=", "loss_batch", ".", "mean", "(", ")", "*", "non_zeros", "\n", "logp", "+=", "torch", ".", "sum", "(", "loss_batch", "*", "non_zeros", ")", "\n", "count", "+=", "int", "(", "non_zeros", ")", "\n", "", "return", "loss", "/", "batch_size", ",", "loss", ",", "count", ".", "to", "(", "loss", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.score": [[140, 156], ["default.DefaultRNNLM.model.predict", "y[].unsqueeze", "scores.squeeze"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict"], ["", "def", "score", "(", "self", ",", "y", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        \"\"\"", "\n", "new_state", ",", "scores", "=", "self", ".", "model", ".", "predict", "(", "state", ",", "y", "[", "-", "1", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "return", "scores", ".", "squeeze", "(", "0", ")", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.final_score": [[157, 168], ["default.DefaultRNNLM.model.final"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.final"], ["", "def", "final_score", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Score eos.\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        \"\"\"", "\n", "return", "self", ".", "model", ".", "final", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.DefaultRNNLM.batch_score": [[170, 214], ["len", "default.DefaultRNNLM.model.predict", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "range", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict"], ["", "def", "batch_score", "(", "\n", "self", ",", "ys", ":", "torch", ".", "Tensor", ",", "states", ":", "List", "[", "Any", "]", ",", "xs", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "List", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"", "\n", "# merge states", "\n", "n_batch", "=", "len", "(", "ys", ")", "\n", "n_layers", "=", "self", ".", "model", ".", "predictor", ".", "n_layers", "\n", "if", "self", ".", "model", ".", "predictor", ".", "typ", "==", "\"lstm\"", ":", "\n", "            ", "keys", "=", "(", "\"c\"", ",", "\"h\"", ")", "\n", "", "else", ":", "\n", "            ", "keys", "=", "(", "\"h\"", ",", ")", "\n", "\n", "", "if", "states", "[", "0", "]", "is", "None", ":", "\n", "            ", "states", "=", "None", "\n", "", "else", ":", "\n", "# transpose state of [batch, key, layer] into [key, layer, batch]", "\n", "            ", "states", "=", "{", "\n", "k", ":", "[", "\n", "torch", ".", "stack", "(", "[", "states", "[", "b", "]", "[", "k", "]", "[", "i", "]", "for", "b", "in", "range", "(", "n_batch", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "n_layers", ")", "\n", "]", "\n", "for", "k", "in", "keys", "\n", "}", "\n", "", "states", ",", "logp", "=", "self", ".", "model", ".", "predict", "(", "states", ",", "ys", "[", ":", ",", "-", "1", "]", ")", "\n", "\n", "# transpose state of [key, layer, batch] into [batch, key, layer]", "\n", "return", "(", "\n", "logp", ",", "\n", "[", "\n", "{", "k", ":", "[", "states", "[", "k", "]", "[", "i", "]", "[", "b", "]", "for", "i", "in", "range", "(", "n_layers", ")", "]", "for", "k", "in", "keys", "}", "\n", "for", "b", "in", "range", "(", "n_batch", ")", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.__init__": [[221, 239], ["torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.Module.__init__", "isinstance", "TypeError", "type"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "predictor", ",", "lossfun", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "\"none\"", ")", ",", "label_key", "=", "-", "1", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        :param torch.nn.Module predictor : The RNNLM\n        :param function lossfun : The loss function to use\n        :param int/str label_key :\n\n        \"\"\"", "\n", "if", "not", "(", "isinstance", "(", "label_key", ",", "(", "int", ",", "str", ")", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"label_key must be int or str, but is %s\"", "%", "type", "(", "label_key", ")", ")", "\n", "", "super", "(", "ClassifierWithState", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "lossfun", "=", "lossfun", "\n", "self", ".", "y", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "label_key", "=", "label_key", "\n", "self", ".", "predictor", "=", "predictor", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.forward": [[240, 281], ["isinstance", "default.ClassifierWithState.predictor", "default.ClassifierWithState.lossfun", "isinstance", "ValueError", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Compute the loss value for an input and label pair.\n\n        Notes:\n            It also computes accuracy and stores it to the attribute.\n            When ``label_key`` is ``int``, the corresponding element in ``args``\n            is treated as ground truth labels. And when it is ``str``, the\n            element in ``kwargs`` is used.\n            The all elements of ``args`` and ``kwargs`` except the groundtruth\n            labels are features.\n            It feeds features to the predictor and compare the result\n            with ground truth labels.\n\n        :param torch.Tensor state : the LM state\n        :param list[torch.Tensor] args : Input minibatch\n        :param dict[torch.Tensor] kwargs : Input minibatch\n        :return loss value\n        :rtype torch.Tensor\n\n        \"\"\"", "\n", "if", "isinstance", "(", "self", ".", "label_key", ",", "int", ")", ":", "\n", "            ", "if", "not", "(", "-", "len", "(", "args", ")", "<=", "self", ".", "label_key", "<", "len", "(", "args", ")", ")", ":", "\n", "                ", "msg", "=", "\"Label key %d is out of bounds\"", "%", "self", ".", "label_key", "\n", "raise", "ValueError", "(", "msg", ")", "\n", "", "t", "=", "args", "[", "self", ".", "label_key", "]", "\n", "if", "self", ".", "label_key", "==", "-", "1", ":", "\n", "                ", "args", "=", "args", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "args", "=", "args", "[", ":", "self", ".", "label_key", "]", "+", "args", "[", "self", ".", "label_key", "+", "1", ":", "]", "\n", "", "", "elif", "isinstance", "(", "self", ".", "label_key", ",", "str", ")", ":", "\n", "            ", "if", "self", ".", "label_key", "not", "in", "kwargs", ":", "\n", "                ", "msg", "=", "'Label key \"%s\" is not found'", "%", "self", ".", "label_key", "\n", "raise", "ValueError", "(", "msg", ")", "\n", "", "t", "=", "kwargs", "[", "self", ".", "label_key", "]", "\n", "del", "kwargs", "[", "self", ".", "label_key", "]", "\n", "\n", "", "self", ".", "y", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "state", ",", "self", ".", "y", "=", "self", ".", "predictor", "(", "state", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "loss", "=", "self", ".", "lossfun", "(", "self", ".", "y", ",", "t", ")", "\n", "return", "state", ",", "self", ".", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict": [[282, 295], ["hasattr", "default.ClassifierWithState.predictor", "default.ClassifierWithState.predictor", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "predict", "(", "self", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Predict log probabilities for given state and input x using the predictor.\n\n        :param torch.Tensor state : The current state\n        :param torch.Tensor x : The input\n        :return a tuple (new state, log prob vector)\n        :rtype (torch.Tensor, torch.Tensor)\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ".", "predictor", ",", "\"normalized\"", ")", "and", "self", ".", "predictor", ".", "normalized", ":", "\n", "            ", "return", "self", ".", "predictor", "(", "state", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "state", ",", "z", "=", "self", ".", "predictor", "(", "state", ",", "x", ")", "\n", "return", "state", ",", "F", ".", "log_softmax", "(", "z", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.buff_predict": [[296, 310], ["range", "default.ClassifierWithState.predict", "default.ClassifierWithState.predict", "new_state.append", "new_log_y.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.predict"], ["", "", "def", "buff_predict", "(", "self", ",", "state", ",", "x", ",", "n", ")", ":", "\n", "        ", "\"\"\"Predict new tokens from buffered inputs.\"\"\"", "\n", "if", "self", ".", "predictor", ".", "__class__", ".", "__name__", "==", "\"RNNLM\"", ":", "\n", "            ", "return", "self", ".", "predict", "(", "state", ",", "x", ")", "\n", "\n", "", "new_state", "=", "[", "]", "\n", "new_log_y", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "state_i", "=", "None", "if", "state", "is", "None", "else", "state", "[", "i", "]", "\n", "state_i", ",", "log_y", "=", "self", ".", "predict", "(", "state_i", ",", "x", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "new_state", ".", "append", "(", "state_i", ")", "\n", "new_log_y", ".", "append", "(", "log_y", ")", "\n", "\n", "", "return", "new_state", ",", "torch", ".", "cat", "(", "new_log_y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.final": [[311, 325], ["hasattr", "default.ClassifierWithState.predictor.final", "default.ClassifierWithState.predictor.final"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.final", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.ClassifierWithState.final"], ["", "def", "final", "(", "self", ",", "state", ",", "index", "=", "None", ")", ":", "\n", "        ", "\"\"\"Predict final log probabilities for given state using the predictor.\n\n        :param state: The state\n        :return The final log probabilities\n        :rtype torch.Tensor\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ".", "predictor", ",", "\"final\"", ")", ":", "\n", "            ", "if", "index", "is", "not", "None", ":", "\n", "                ", "return", "self", ".", "predictor", ".", "final", "(", "state", "[", "index", "]", ")", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "predictor", ".", "final", "(", "state", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.RNNLM.__init__": [[331, 392], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "logging.info", "logging.info", "logging.info", "default.RNNLM.parameters", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "param.data.uniform_", "torch.Dropout", "torch.Dropout", "torch.Dropout", "range", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "range", "range"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "n_vocab", ",", "\n", "n_layers", ",", "\n", "n_units", ",", "\n", "n_embed", "=", "None", ",", "\n", "typ", "=", "\"lstm\"", ",", "\n", "dropout_rate", "=", "0.5", ",", "\n", "emb_dropout_rate", "=", "0.0", ",", "\n", "tie_weights", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        :param int n_vocab: The size of the vocabulary\n        :param int n_layers: The number of layers to create\n        :param int n_units: The number of units per layer\n        :param str typ: The RNN type\n        \"\"\"", "\n", "super", "(", "RNNLM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "n_embed", "is", "None", ":", "\n", "            ", "n_embed", "=", "n_units", "\n", "\n", "", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "n_vocab", ",", "n_embed", ")", "\n", "\n", "if", "emb_dropout_rate", "==", "0.0", ":", "\n", "            ", "self", ".", "embed_drop", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_drop", "=", "nn", ".", "Dropout", "(", "emb_dropout_rate", ")", "\n", "\n", "", "if", "typ", "==", "\"lstm\"", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "LSTMCell", "(", "n_embed", ",", "n_units", ")", "]", "\n", "+", "[", "nn", ".", "LSTMCell", "(", "n_units", ",", "n_units", ")", "for", "_", "in", "range", "(", "n_layers", "-", "1", ")", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "GRUCell", "(", "n_embed", ",", "n_units", ")", "]", "\n", "+", "[", "nn", ".", "GRUCell", "(", "n_units", ",", "n_units", ")", "for", "_", "in", "range", "(", "n_layers", "-", "1", ")", "]", "\n", ")", "\n", "\n", "", "self", ".", "dropout", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Dropout", "(", "dropout_rate", ")", "for", "_", "in", "range", "(", "n_layers", "+", "1", ")", "]", "\n", ")", "\n", "self", ".", "lo", "=", "nn", ".", "Linear", "(", "n_units", ",", "n_vocab", ")", "\n", "self", ".", "n_layers", "=", "n_layers", "\n", "self", ".", "n_units", "=", "n_units", "\n", "self", ".", "typ", "=", "typ", "\n", "\n", "logging", ".", "info", "(", "\"Tie weights set to {}\"", ".", "format", "(", "tie_weights", ")", ")", "\n", "logging", ".", "info", "(", "\"Dropout set to {}\"", ".", "format", "(", "dropout_rate", ")", ")", "\n", "logging", ".", "info", "(", "\"Emb Dropout set to {}\"", ".", "format", "(", "emb_dropout_rate", ")", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "assert", "(", "\n", "n_embed", "==", "n_units", "\n", ")", ",", "\"Tie Weights: True need embedding and final dimensions to match\"", "\n", "self", ".", "lo", ".", "weight", "=", "self", ".", "embed", ".", "weight", "\n", "\n", "# initialize parameters from uniform distribution", "\n", "", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.RNNLM.zero_state": [[393, 397], ["next", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "default.RNNLM.parameters", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "", "def", "zero_state", "(", "self", ",", "batchsize", ")", ":", "\n", "        ", "\"\"\"Initialize state.\"\"\"", "\n", "p", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "return", "torch", ".", "zeros", "(", "batchsize", ",", "self", ".", "n_units", ")", ".", "to", "(", "device", "=", "p", ".", "device", ",", "dtype", "=", "p", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.RNNLM.forward": [[398, 432], ["default.RNNLM.lo", "default.RNNLM.embed_drop", "default.RNNLM.embed", "range", "range", "espnet.nets.pytorch_backend.e2e_asr.to_device", "default.RNNLM.embed", "default.RNNLM.zero_state", "range", "espnet.nets.pytorch_backend.e2e_asr.to_device", "x.size", "default.RNNLM.zero_state", "range", "x.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.RNNLM.zero_state", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.nets_utils.to_device", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.default.RNNLM.zero_state"], ["", "def", "forward", "(", "self", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward neural networks.\"\"\"", "\n", "if", "state", "is", "None", ":", "\n", "            ", "h", "=", "[", "to_device", "(", "x", ",", "self", ".", "zero_state", "(", "x", ".", "size", "(", "0", ")", ")", ")", "for", "n", "in", "range", "(", "self", ".", "n_layers", ")", "]", "\n", "state", "=", "{", "\"h\"", ":", "h", "}", "\n", "if", "self", ".", "typ", "==", "\"lstm\"", ":", "\n", "                ", "c", "=", "[", "\n", "to_device", "(", "x", ",", "self", ".", "zero_state", "(", "x", ".", "size", "(", "0", ")", ")", ")", "\n", "for", "n", "in", "range", "(", "self", ".", "n_layers", ")", "\n", "]", "\n", "state", "=", "{", "\"c\"", ":", "c", ",", "\"h\"", ":", "h", "}", "\n", "\n", "", "", "h", "=", "[", "None", "]", "*", "self", ".", "n_layers", "\n", "if", "self", ".", "embed_drop", "is", "not", "None", ":", "\n", "            ", "emb", "=", "self", ".", "embed_drop", "(", "self", ".", "embed", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "emb", "=", "self", ".", "embed", "(", "x", ")", "\n", "", "if", "self", ".", "typ", "==", "\"lstm\"", ":", "\n", "            ", "c", "=", "[", "None", "]", "*", "self", ".", "n_layers", "\n", "h", "[", "0", "]", ",", "c", "[", "0", "]", "=", "self", ".", "rnn", "[", "0", "]", "(", "\n", "self", ".", "dropout", "[", "0", "]", "(", "emb", ")", ",", "(", "state", "[", "\"h\"", "]", "[", "0", "]", ",", "state", "[", "\"c\"", "]", "[", "0", "]", ")", "\n", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "self", ".", "n_layers", ")", ":", "\n", "                ", "h", "[", "n", "]", ",", "c", "[", "n", "]", "=", "self", ".", "rnn", "[", "n", "]", "(", "\n", "self", ".", "dropout", "[", "n", "]", "(", "h", "[", "n", "-", "1", "]", ")", ",", "(", "state", "[", "\"h\"", "]", "[", "n", "]", ",", "state", "[", "\"c\"", "]", "[", "n", "]", ")", "\n", ")", "\n", "", "state", "=", "{", "\"c\"", ":", "c", ",", "\"h\"", ":", "h", "}", "\n", "", "else", ":", "\n", "            ", "h", "[", "0", "]", "=", "self", ".", "rnn", "[", "0", "]", "(", "self", ".", "dropout", "[", "0", "]", "(", "emb", ")", ",", "state", "[", "\"h\"", "]", "[", "0", "]", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "self", ".", "n_layers", ")", ":", "\n", "                ", "h", "[", "n", "]", "=", "self", ".", "rnn", "[", "n", "]", "(", "self", ".", "dropout", "[", "n", "]", "(", "h", "[", "n", "-", "1", "]", ")", ",", "state", "[", "\"h\"", "]", "[", "n", "]", ")", "\n", "", "state", "=", "{", "\"h\"", ":", "h", "}", "\n", "", "y", "=", "self", ".", "lo", "(", "self", ".", "dropout", "[", "-", "1", "]", "(", "h", "[", "-", "1", "]", ")", ")", "\n", "return", "state", ",", "y", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.add_arguments": [[18, 39], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_arguments", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to command line argument parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\n", "\"--type\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"lstm\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "choices", "=", "[", "\"lstm\"", ",", "\"gru\"", "]", ",", "\n", "help", "=", "\"Which type of RNN to use\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--layer\"", ",", "\"-l\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Number of hidden layers\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--unit\"", ",", "\"-u\"", ",", "type", "=", "int", ",", "default", "=", "650", ",", "help", "=", "\"Number of hidden units\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dropout-rate\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "\"dropout probability\"", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.__init__": [[40, 56], ["torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "torch.nn.Module.__init__", "seq_rnn.SequentialRNNLM._setup", "args.type.upper"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._setup"], ["", "def", "__init__", "(", "self", ",", "n_vocab", ",", "args", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        \"\"\"", "\n", "torch", ".", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "self", ".", "_setup", "(", "\n", "rnn_type", "=", "args", ".", "type", ".", "upper", "(", ")", ",", "\n", "ntoken", "=", "n_vocab", ",", "\n", "ninp", "=", "args", ".", "unit", ",", "\n", "nhid", "=", "args", ".", "unit", ",", "\n", "nlayers", "=", "args", ".", "layer", ",", "\n", "dropout", "=", "args", ".", "dropout_rate", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._setup": [[58, 97], ["torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "seq_rnn.SequentialRNNLM._init_weights", "torch.RNN", "torch.RNN", "torch.RNN", "getattr", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._init_weights"], ["", "def", "_setup", "(", "\n", "self", ",", "rnn_type", ",", "ntoken", ",", "ninp", ",", "nhid", ",", "nlayers", ",", "dropout", "=", "0.5", ",", "tie_weights", "=", "False", "\n", ")", ":", "\n", "        ", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "ntoken", ",", "ninp", ")", "\n", "if", "rnn_type", "in", "[", "\"LSTM\"", ",", "\"GRU\"", "]", ":", "\n", "            ", "self", ".", "rnn", "=", "getattr", "(", "nn", ",", "rnn_type", ")", "(", "ninp", ",", "nhid", ",", "nlayers", ",", "dropout", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "nonlinearity", "=", "{", "\"RNN_TANH\"", ":", "\"tanh\"", ",", "\"RNN_RELU\"", ":", "\"relu\"", "}", "[", "rnn_type", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"An invalid option for `--model` was supplied, \"", "\n", "\"options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"", "\n", ")", "\n", "", "self", ".", "rnn", "=", "nn", ".", "RNN", "(", "\n", "ninp", ",", "nhid", ",", "nlayers", ",", "nonlinearity", "=", "nonlinearity", ",", "dropout", "=", "dropout", "\n", ")", "\n", "", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "nhid", ",", "ntoken", ")", "\n", "\n", "# Optionally tie weights as in:", "\n", "# \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)", "\n", "# https://arxiv.org/abs/1608.05859", "\n", "# and", "\n", "# \"Tying Word Vectors and Word Classifiers:", "\n", "#  A Loss Framework for Language Modeling\" (Inan et al. 2016)", "\n", "# https://arxiv.org/abs/1611.01462", "\n", "if", "tie_weights", ":", "\n", "            ", "if", "nhid", "!=", "ninp", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"When using the tied flag, nhid must be equal to emsize\"", "\n", ")", "\n", "", "self", ".", "decoder", ".", "weight", "=", "self", ".", "encoder", ".", "weight", "\n", "\n", "", "self", ".", "_init_weights", "(", ")", "\n", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "self", ".", "nhid", "=", "nhid", "\n", "self", ".", "nlayers", "=", "nlayers", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._init_weights": [[98, 107], ["seq_rnn.SequentialRNNLM.parameters", "param.data.uniform_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ")", ":", "\n", "# NOTE: original init in pytorch/examples", "\n", "# initrange = 0.1", "\n", "# self.encoder.weight.data.uniform_(-initrange, initrange)", "\n", "# self.decoder.bias.data.zero_()", "\n", "# self.decoder.weight.data.uniform_(-initrange, initrange)", "\n", "# NOTE: our default.py:RNNLM init", "\n", "        ", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.forward": [[108, 133], ["torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "logp.sum.sum.sum", "mask.sum", "seq_rnn.SequentialRNNLM._before_loss", "y.view", "t.view", "mask.view"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._before_loss"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "t", ")", ":", "\n", "        ", "\"\"\"Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        \"\"\"", "\n", "y", "=", "self", ".", "_before_loss", "(", "x", ",", "None", ")", "[", "0", "]", "\n", "mask", "=", "(", "x", "!=", "0", ")", ".", "to", "(", "y", ".", "dtype", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "y", ".", "view", "(", "-", "1", ",", "y", ".", "shape", "[", "-", "1", "]", ")", ",", "t", ".", "view", "(", "-", "1", ")", ",", "reduction", "=", "\"none\"", ")", "\n", "logp", "=", "loss", "*", "mask", ".", "view", "(", "-", "1", ")", "\n", "logp", "=", "logp", ".", "sum", "(", ")", "\n", "count", "=", "mask", ".", "sum", "(", ")", "\n", "return", "logp", "/", "count", ",", "logp", ",", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._before_loss": [[134, 142], ["seq_rnn.SequentialRNNLM.drop", "seq_rnn.SequentialRNNLM.rnn", "seq_rnn.SequentialRNNLM.drop", "seq_rnn.SequentialRNNLM.decoder", "seq_rnn.SequentialRNNLM.encoder", "seq_rnn.SequentialRNNLM.view", "seq_rnn.SequentialRNNLM.view", "seq_rnn.SequentialRNNLM.size", "seq_rnn.SequentialRNNLM.size", "seq_rnn.SequentialRNNLM.size", "seq_rnn.SequentialRNNLM.size", "seq_rnn.SequentialRNNLM.size", "seq_rnn.SequentialRNNLM.size"], "methods", ["None"], ["", "def", "_before_loss", "(", "self", ",", "input", ",", "hidden", ")", ":", "\n", "        ", "emb", "=", "self", ".", "drop", "(", "self", ".", "encoder", "(", "input", ")", ")", "\n", "output", ",", "hidden", "=", "self", ".", "rnn", "(", "emb", ",", "hidden", ")", "\n", "output", "=", "self", ".", "drop", "(", "output", ")", "\n", "decoded", "=", "self", ".", "decoder", "(", "\n", "output", ".", "view", "(", "output", ".", "size", "(", "0", ")", "*", "output", ".", "size", "(", "1", ")", ",", "output", ".", "size", "(", "2", ")", ")", "\n", ")", "\n", "return", "decoded", ".", "view", "(", "output", ".", "size", "(", "0", ")", ",", "output", ".", "size", "(", "1", ")", ",", "decoded", ".", "size", "(", "1", ")", ")", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.init_state": [[143, 161], ["next", "seq_rnn.SequentialRNNLM.parameters", "next.new_zeros", "next.new_zeros", "next.new_zeros"], "methods", ["None"], ["", "def", "init_state", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        \"\"\"", "\n", "bsz", "=", "1", "\n", "weight", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", "if", "self", ".", "rnn_type", "==", "\"LSTM\"", ":", "\n", "            ", "return", "(", "\n", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "bsz", ",", "self", ".", "nhid", ")", ",", "\n", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "bsz", ",", "self", ".", "nhid", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "weight", ".", "new_zeros", "(", "self", ".", "nlayers", ",", "bsz", ",", "self", ".", "nhid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM.score": [[162, 179], ["seq_rnn.SequentialRNNLM._before_loss", "y.log_softmax().view", "y[].view", "y.log_softmax"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.seq_rnn.SequentialRNNLM._before_loss", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "", "def", "score", "(", "self", ",", "y", ",", "state", ",", "x", ")", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        \"\"\"", "\n", "y", ",", "new_state", "=", "self", ".", "_before_loss", "(", "y", "[", "-", "1", "]", ".", "view", "(", "1", ",", "1", ")", ",", "state", ")", "\n", "logp", "=", "y", ".", "log_softmax", "(", "dim", "=", "-", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "return", "logp", ",", "new_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.add_arguments": [[23, 78], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_arguments", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to command line argument parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\n", "\"--layer\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"Number of hidden layers\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--unit\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1024", ",", "\n", "help", "=", "\"Number of hidden units in feedforward layer\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--att-unit\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "256", ",", "\n", "help", "=", "\"Number of hidden units in attention layer\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--embed-unit\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "128", ",", "\n", "help", "=", "\"Number of hidden units in embedding layer\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--head\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Number of multi head attention\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dropout-rate\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "\"dropout probability\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--att-dropout-rate\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.0", ",", "\n", "help", "=", "\"att dropout probability\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--emb-dropout-rate\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.0", ",", "\n", "help", "=", "\"emb dropout probability\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tie-weights\"", ",", "\n", "type", "=", "strtobool", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"Tie input and output embeddings\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pos-enc\"", ",", "\n", "default", "=", "\"sinusoidal\"", ",", "\n", "choices", "=", "[", "\"sinusoidal\"", ",", "\"none\"", "]", ",", "\n", "help", "=", "\"positional encoding\"", ",", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__": [[79, 136], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "getattr", "getattr", "getattr", "torch.Embedding", "torch.Embedding", "torch.Embedding", "espnet.nets.pytorch_backend.transformer.encoder.Encoder", "torch.Linear", "torch.Linear", "torch.Linear", "logging.info", "logging.info", "logging.info", "logging.info", "torch.Dropout", "torch.Dropout", "torch.Dropout", "ValueError", "torch.Sequential", "torch.Sequential", "torch.Sequential"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.__init__"], ["", "def", "__init__", "(", "self", ",", "n_vocab", ",", "args", ")", ":", "\n", "        ", "\"\"\"Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        \"\"\"", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "\n", "# NOTE: for a compatibility with less than 0.9.7 version models", "\n", "emb_dropout_rate", "=", "getattr", "(", "args", ",", "\"emb_dropout_rate\"", ",", "0.0", ")", "\n", "# NOTE: for a compatibility with less than 0.9.7 version models", "\n", "tie_weights", "=", "getattr", "(", "args", ",", "\"tie_weights\"", ",", "False", ")", "\n", "# NOTE: for a compatibility with less than 0.9.7 version models", "\n", "att_dropout_rate", "=", "getattr", "(", "args", ",", "\"att_dropout_rate\"", ",", "0.0", ")", "\n", "\n", "if", "args", ".", "pos_enc", "==", "\"sinusoidal\"", ":", "\n", "            ", "pos_enc_class", "=", "PositionalEncoding", "\n", "", "elif", "args", ".", "pos_enc", "==", "\"none\"", ":", "\n", "\n", "            ", "def", "pos_enc_class", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                ", "return", "nn", ".", "Sequential", "(", ")", "# indentity", "\n", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"unknown pos-enc option: {args.pos_enc}\"", ")", "\n", "\n", "", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "n_vocab", ",", "args", ".", "embed_unit", ")", "\n", "\n", "if", "emb_dropout_rate", "==", "0.0", ":", "\n", "            ", "self", ".", "embed_drop", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_drop", "=", "nn", ".", "Dropout", "(", "emb_dropout_rate", ")", "\n", "\n", "", "self", ".", "encoder", "=", "Encoder", "(", "\n", "idim", "=", "args", ".", "embed_unit", ",", "\n", "attention_dim", "=", "args", ".", "att_unit", ",", "\n", "attention_heads", "=", "args", ".", "head", ",", "\n", "linear_units", "=", "args", ".", "unit", ",", "\n", "num_blocks", "=", "args", ".", "layer", ",", "\n", "dropout_rate", "=", "args", ".", "dropout_rate", ",", "\n", "attention_dropout_rate", "=", "att_dropout_rate", ",", "\n", "input_layer", "=", "\"linear\"", ",", "\n", "pos_enc_class", "=", "pos_enc_class", ",", "\n", ")", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "args", ".", "att_unit", ",", "n_vocab", ")", "\n", "\n", "logging", ".", "info", "(", "\"Tie weights set to {}\"", ".", "format", "(", "tie_weights", ")", ")", "\n", "logging", ".", "info", "(", "\"Dropout set to {}\"", ".", "format", "(", "args", ".", "dropout_rate", ")", ")", "\n", "logging", ".", "info", "(", "\"Emb Dropout set to {}\"", ".", "format", "(", "emb_dropout_rate", ")", ")", "\n", "logging", ".", "info", "(", "\"Att Dropout set to {}\"", ".", "format", "(", "att_dropout_rate", ")", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "assert", "(", "\n", "args", ".", "att_unit", "==", "args", ".", "embed_unit", "\n", ")", ",", "\"Tie Weights: True need embedding and final dimensions to match\"", "\n", "self", ".", "decoder", ".", "weight", "=", "self", ".", "embed", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM._target_mask": [[137, 141], ["espnet.nets.pytorch_backend.transformer.mask.subsequent_mask().unsqueeze", "ys_mask.unsqueeze", "espnet.nets.pytorch_backend.transformer.mask.subsequent_mask", "ys_mask.size"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.mask.subsequent_mask"], ["", "", "def", "_target_mask", "(", "self", ",", "ys_in_pad", ")", ":", "\n", "        ", "ys_mask", "=", "ys_in_pad", "!=", "0", "\n", "m", "=", "subsequent_mask", "(", "ys_mask", ".", "size", "(", "-", "1", ")", ",", "device", "=", "ys_mask", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "ys_mask", ".", "unsqueeze", "(", "-", "2", ")", "&", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.forward": [[142, 177], ["transformer.TransformerLM.encoder", "transformer.TransformerLM.decoder", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "xm.to", "logp.sum.sum.sum", "xm.to.sum", "transformer.TransformerLM.embed_drop", "transformer.TransformerLM.embed", "transformer.TransformerLM._target_mask", "transformer.TransformerLM.view", "t.view", "xm.to.view", "transformer.TransformerLM.embed"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM._target_mask"], ["", "def", "forward", "(", "\n", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "t", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        \"\"\"", "\n", "xm", "=", "x", "!=", "0", "\n", "\n", "if", "self", ".", "embed_drop", "is", "not", "None", ":", "\n", "            ", "emb", "=", "self", ".", "embed_drop", "(", "self", ".", "embed", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "emb", "=", "self", ".", "embed", "(", "x", ")", "\n", "\n", "", "h", ",", "_", "=", "self", ".", "encoder", "(", "emb", ",", "self", ".", "_target_mask", "(", "x", ")", ")", "\n", "y", "=", "self", ".", "decoder", "(", "h", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "y", ".", "view", "(", "-", "1", ",", "y", ".", "shape", "[", "-", "1", "]", ")", ",", "t", ".", "view", "(", "-", "1", ")", ",", "reduction", "=", "\"none\"", ")", "\n", "mask", "=", "xm", ".", "to", "(", "dtype", "=", "loss", ".", "dtype", ")", "\n", "logp", "=", "loss", "*", "mask", ".", "view", "(", "-", "1", ")", "\n", "logp", "=", "logp", ".", "sum", "(", ")", "\n", "count", "=", "mask", ".", "sum", "(", ")", "\n", "return", "logp", "/", "count", ",", "logp", ",", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.score": [[178, 207], ["y.unsqueeze.unsqueeze.unsqueeze", "transformer.TransformerLM.encoder.forward_one_step", "transformer.TransformerLM.decoder", "transformer.TransformerLM.log_softmax().squeeze", "transformer.TransformerLM.embed_drop", "transformer.TransformerLM.embed", "transformer.TransformerLM._target_mask", "transformer.TransformerLM.embed", "transformer.TransformerLM.log_softmax"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward_one_step", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM._target_mask", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax"], ["", "def", "score", "(", "\n", "self", ",", "y", ":", "torch", ".", "Tensor", ",", "state", ":", "Any", ",", "x", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Any", "]", ":", "\n", "        ", "\"\"\"Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        \"\"\"", "\n", "y", "=", "y", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "if", "self", ".", "embed_drop", "is", "not", "None", ":", "\n", "            ", "emb", "=", "self", ".", "embed_drop", "(", "self", ".", "embed", "(", "y", ")", ")", "\n", "", "else", ":", "\n", "            ", "emb", "=", "self", ".", "embed", "(", "y", ")", "\n", "\n", "", "h", ",", "_", ",", "cache", "=", "self", ".", "encoder", ".", "forward_one_step", "(", "\n", "emb", ",", "self", ".", "_target_mask", "(", "y", ")", ",", "cache", "=", "state", "\n", ")", "\n", "h", "=", "self", ".", "decoder", "(", "h", "[", ":", ",", "-", "1", "]", ")", "\n", "logp", "=", "h", ".", "log_softmax", "(", "dim", "=", "-", "1", ")", ".", "squeeze", "(", "0", ")", "\n", "return", "logp", ",", "cache", "\n", "\n"]], "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM.batch_score": [[209, 253], ["len", "len", "transformer.TransformerLM.encoder.forward_one_step", "transformer.TransformerLM.decoder", "transformer.TransformerLM.log_softmax", "transformer.TransformerLM.embed_drop", "transformer.TransformerLM.embed", "transformer.TransformerLM._target_mask", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "transformer.TransformerLM.embed", "range", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.transformer.decoder.Decoder.forward_one_step", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.pytorch_backend.ctc.CTC.log_softmax", "home.repos.pwc.inspect_result.mpc001_Visual_Speech_Recognition_for_Multiple_Languages.lm.transformer.TransformerLM._target_mask"], ["", "def", "batch_score", "(", "\n", "self", ",", "ys", ":", "torch", ".", "Tensor", ",", "states", ":", "List", "[", "Any", "]", ",", "xs", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "List", "[", "Any", "]", "]", ":", "\n", "        ", "\"\"\"Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        \"\"\"", "\n", "# merge states", "\n", "n_batch", "=", "len", "(", "ys", ")", "\n", "n_layers", "=", "len", "(", "self", ".", "encoder", ".", "encoders", ")", "\n", "if", "states", "[", "0", "]", "is", "None", ":", "\n", "            ", "batch_state", "=", "None", "\n", "", "else", ":", "\n", "# transpose state of [batch, layer] into [layer, batch]", "\n", "            ", "batch_state", "=", "[", "\n", "torch", ".", "stack", "(", "[", "states", "[", "b", "]", "[", "i", "]", "for", "b", "in", "range", "(", "n_batch", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "n_layers", ")", "\n", "]", "\n", "\n", "", "if", "self", ".", "embed_drop", "is", "not", "None", ":", "\n", "            ", "emb", "=", "self", ".", "embed_drop", "(", "self", ".", "embed", "(", "ys", ")", ")", "\n", "", "else", ":", "\n", "            ", "emb", "=", "self", ".", "embed", "(", "ys", ")", "\n", "\n", "# batch decoding", "\n", "", "h", ",", "_", ",", "states", "=", "self", ".", "encoder", ".", "forward_one_step", "(", "\n", "emb", ",", "self", ".", "_target_mask", "(", "ys", ")", ",", "cache", "=", "batch_state", "\n", ")", "\n", "h", "=", "self", ".", "decoder", "(", "h", "[", ":", ",", "-", "1", "]", ")", "\n", "logp", "=", "h", ".", "log_softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# transpose state of [layer, batch] into [batch, layer]", "\n", "state_list", "=", "[", "[", "states", "[", "i", "]", "[", "b", "]", "for", "i", "in", "range", "(", "n_layers", ")", "]", "for", "b", "in", "range", "(", "n_batch", ")", "]", "\n", "return", "logp", ",", "state_list", "\n", "", "", ""]]}