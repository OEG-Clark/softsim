{"home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.TrainingInstance.__init__": [[42, 50], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokens", ",", "segment_ids", ",", "masked_lm_positions", ",", "\n", "masked_lm_labels", ",", "is_random_next", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "is_random_next", "=", "is_random_next", "\n", "self", ".", "masked_lm_positions", "=", "masked_lm_positions", "\n", "self", ".", "masked_lm_labels", "=", "masked_lm_labels", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.TrainingInstance.__str__": [[51, 64], ["create_pretraining_data.TrainingInstance.vocab.to_tokens", "create_pretraining_data.TrainingInstance.vocab.to_tokens", "str", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "tks", "=", "self", ".", "vocab", ".", "to_tokens", "(", "self", ".", "tokens", ")", "\n", "mask_tks", "=", "self", ".", "vocab", ".", "to_tokens", "(", "self", ".", "masked_lm_labels", ")", "\n", "s", "=", "''", "\n", "s", "+=", "'tokens: %s\\n'", "%", "(", "' '", ".", "join", "(", "tks", ")", ")", "\n", "s", "+=", "'segment_ids: %s\\n'", "%", "(", "' '", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "segment_ids", "]", ")", ")", "\n", "s", "+=", "'is_random_next: %s\\n'", "%", "self", ".", "is_random_next", "\n", "s", "+=", "'masked_lm_positions: %s\\n'", "%", "(", "' '", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "masked_lm_positions", "]", ")", ")", "\n", "s", "+=", "'masked_lm_labels: %s\\n'", "%", "(", "' '", ".", "join", "(", "mask_tks", ")", ")", "\n", "s", "+=", "'\\n'", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.TrainingInstance.__repr__": [[65, 67], ["create_pretraining_data.TrainingInstance.__str__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.TrainingInstance.__str__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.transform": [[69, 91], ["len", "len", "len"], "function", ["None"], ["", "", "def", "transform", "(", "instance", ",", "max_seq_length", ")", ":", "\n", "    ", "\"\"\"Transform instance to inputs for MLM and NSP.\"\"\"", "\n", "input_ids", "=", "instance", ".", "tokens", "\n", "assert", "len", "(", "input_ids", ")", "<=", "max_seq_length", "\n", "segment_ids", "=", "instance", ".", "segment_ids", "\n", "masked_lm_positions", "=", "instance", ".", "masked_lm_positions", "\n", "valid_lengths", "=", "len", "(", "input_ids", ")", "\n", "\n", "masked_lm_ids", "=", "instance", ".", "masked_lm_labels", "\n", "masked_lm_weights", "=", "[", "1.0", "]", "*", "len", "(", "masked_lm_ids", ")", "\n", "\n", "next_sentence_label", "=", "1", "if", "instance", ".", "is_random_next", "else", "0", "\n", "\n", "features", "=", "{", "}", "\n", "features", "[", "'input_ids'", "]", "=", "input_ids", "\n", "features", "[", "'segment_ids'", "]", "=", "segment_ids", "\n", "features", "[", "'masked_lm_positions'", "]", "=", "masked_lm_positions", "\n", "features", "[", "'masked_lm_ids'", "]", "=", "masked_lm_ids", "\n", "features", "[", "'masked_lm_weights'", "]", "=", "masked_lm_weights", "\n", "features", "[", "'next_sentence_labels'", "]", "=", "[", "next_sentence_label", "]", "\n", "features", "[", "'valid_lengths'", "]", "=", "[", "valid_lengths", "]", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.print_example": [[93, 100], ["logging.debug", "logging.debug", "features.keys", "logging.debug"], "function", ["None"], ["", "def", "print_example", "(", "instance", ",", "features", ")", ":", "\n", "    ", "logging", ".", "debug", "(", "'*** Example Instance ***'", ")", "\n", "logging", ".", "debug", "(", "'\\n%s'", ",", "instance", ")", "\n", "\n", "for", "feature_name", "in", "features", ".", "keys", "(", ")", ":", "\n", "        ", "feature", "=", "features", "[", "feature_name", "]", "\n", "logging", ".", "debug", "(", "'Generated %s: %s'", ",", "feature_name", ",", "feature", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.write_to_files_np": [[102, 137], ["len", "collections.OrderedDict", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "logging.info", "len", "numpy.savez_compressed", "logging.error", "numpy.savez_compressed", "numpy.savez_compressed", "len", "collections.OrderedDict.items", "collections.OrderedDict.items"], "function", ["None"], ["", "", "def", "write_to_files_np", "(", "features", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "max_predictions_per_seq", ",", "output_files", ")", ":", "\n", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"Write to numpy files from `TrainingInstance`s.\"\"\"", "\n", "next_sentence_labels", "=", "[", "]", "\n", "valid_lengths", "=", "[", "]", "\n", "\n", "assert", "len", "(", "output_files", ")", "==", "1", ",", "'numpy format only support single output file'", "\n", "output_file", "=", "output_files", "[", "0", "]", "\n", "(", "input_ids", ",", "segment_ids", ",", "masked_lm_positions", ",", "masked_lm_ids", ",", "\n", "masked_lm_weights", ",", "next_sentence_labels", ",", "valid_lengths", ")", "=", "features", "\n", "total_written", "=", "len", "(", "next_sentence_labels", ")", "\n", "\n", "# store variable length numpy array object directly.", "\n", "outputs", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "outputs", "[", "'input_ids'", "]", "=", "np", ".", "array", "(", "input_ids", ",", "dtype", "=", "object", ")", "\n", "outputs", "[", "'segment_ids'", "]", "=", "np", ".", "array", "(", "segment_ids", ",", "dtype", "=", "object", ")", "\n", "outputs", "[", "'masked_lm_positions'", "]", "=", "np", ".", "array", "(", "\n", "masked_lm_positions", ",", "dtype", "=", "object", ")", "\n", "outputs", "[", "'masked_lm_ids'", "]", "=", "np", ".", "array", "(", "masked_lm_ids", ",", "dtype", "=", "object", ")", "\n", "outputs", "[", "'masked_lm_weights'", "]", "=", "np", ".", "array", "(", "masked_lm_weights", ",", "dtype", "=", "object", ")", "\n", "outputs", "[", "'next_sentence_labels'", "]", "=", "np", ".", "array", "(", "\n", "next_sentence_labels", ",", "dtype", "=", "'int32'", ")", "\n", "outputs", "[", "'valid_lengths'", "]", "=", "np", ".", "array", "(", "valid_lengths", ",", "dtype", "=", "'int32'", ")", "\n", "\n", "try", ":", "\n", "        ", "np", ".", "savez_compressed", "(", "output_file", ",", "**", "outputs", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "        ", "logging", ".", "error", "(", "f\"Runtime error: {e}, attempting to save half the data\"", ")", "\n", "halfway", "=", "len", "(", "outputs", "[", "'input_ids'", "]", ")", "//", "2", "\n", "output1", "=", "{", "k", ":", "v", "[", ":", "halfway", "]", "for", "k", ",", "v", "in", "outputs", ".", "items", "(", ")", "}", "\n", "np", ".", "savez_compressed", "(", "f\"{output_file}_1.npz\"", ",", "**", "output1", ")", "\n", "output2", "=", "{", "k", ":", "v", "[", "halfway", ":", "]", "for", "k", ",", "v", "in", "outputs", ".", "items", "(", ")", "}", "\n", "np", ".", "savez_compressed", "(", "f\"{output_file}_2.npz\"", ",", "**", "output2", ")", "\n", "", "logging", ".", "info", "(", "'Wrote %d total instances'", ",", "total_written", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.tokenize_lines_fn": [[139, 156], ["line.strip.strip", "results.append", "results.append", "tokenizer"], "function", ["None"], ["", "def", "tokenize_lines_fn", "(", "x", ")", ":", "\n", "    ", "\"\"\"Worker function to tokenize lines based on the tokenizer, and perform vocabulary lookup.\"\"\"", "\n", "lines", ",", "tokenizer", ",", "vocab", "=", "x", "\n", "results", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "if", "not", "line", ":", "\n", "            ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "# Empty lines are used as document delimiters", "\n", "if", "not", "line", ":", "\n", "            ", "results", ".", "append", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "vocab", "[", "[", "vocab", ".", "bos_token", "]", "+", "\n", "vocab", "[", "tokenizer", "(", "line", ")", "]", "+", "[", "vocab", ".", "eos_token", "]", "]", "\n", "if", "tokens", ":", "\n", "                ", "results", ".", "append", "(", "tokens", ")", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.convert_to_npz": [[158, 192], ["enumerate", "create_pretraining_data.transform", "input_ids.append", "segment_ids.append", "masked_lm_positions.append", "masked_lm_ids.append", "masked_lm_weights.append", "next_sentence_labels.append", "valid_lengths.append", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "numpy.ascontiguousarray", "create_pretraining_data.print_example"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.transform", "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.print_example"], ["", "def", "convert_to_npz", "(", "instances", ",", "max_seq_length", ")", ":", "\n", "    ", "\"\"\"Create masked language model and next sentence prediction samples as numpy arrays.\"\"\"", "\n", "input_ids", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "masked_lm_positions", "=", "[", "]", "\n", "masked_lm_ids", "=", "[", "]", "\n", "masked_lm_weights", "=", "[", "]", "\n", "next_sentence_labels", "=", "[", "]", "\n", "valid_lengths", "=", "[", "]", "\n", "\n", "for", "inst_index", ",", "instance", "in", "enumerate", "(", "instances", ")", ":", "\n", "        ", "features", "=", "transform", "(", "instance", ",", "max_seq_length", ")", "\n", "input_id", "=", "features", "[", "'input_ids'", "]", "\n", "segment_id", "=", "features", "[", "'segment_ids'", "]", "\n", "masked_lm_position", "=", "features", "[", "'masked_lm_positions'", "]", "\n", "masked_lm_id", "=", "features", "[", "'masked_lm_ids'", "]", "\n", "masked_lm_weight", "=", "features", "[", "'masked_lm_weights'", "]", "\n", "next_sentence_label", "=", "features", "[", "'next_sentence_labels'", "]", "[", "0", "]", "\n", "valid_length", "=", "features", "[", "'valid_lengths'", "]", "[", "0", "]", "\n", "\n", "input_ids", ".", "append", "(", "np", ".", "ascontiguousarray", "(", "input_id", ",", "dtype", "=", "'int32'", ")", ")", "\n", "segment_ids", ".", "append", "(", "np", ".", "ascontiguousarray", "(", "segment_id", ",", "dtype", "=", "'int32'", ")", ")", "\n", "masked_lm_positions", ".", "append", "(", "np", ".", "ascontiguousarray", "(", "\n", "masked_lm_position", ",", "dtype", "=", "'int32'", ")", ")", "\n", "masked_lm_ids", ".", "append", "(", "np", ".", "ascontiguousarray", "(", "masked_lm_id", ",", "dtype", "=", "'int32'", ")", ")", "\n", "masked_lm_weights", ".", "append", "(", "np", ".", "ascontiguousarray", "(", "\n", "masked_lm_weight", ",", "dtype", "=", "'float32'", ")", ")", "\n", "next_sentence_labels", ".", "append", "(", "next_sentence_label", ")", "\n", "valid_lengths", ".", "append", "(", "valid_length", ")", "\n", "# debugging information", "\n", "if", "inst_index", "<", "1", ":", "\n", "            ", "print_example", "(", "instance", ",", "features", ")", "\n", "", "", "return", "input_ids", ",", "masked_lm_ids", ",", "masked_lm_positions", ",", "masked_lm_weights", ",", "next_sentence_labels", ",", "segment_ids", ",", "valid_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_training_instances": [[194, 325], ["time.time", "time.time", "logging.debug", "range", "range", "worker_pool.apply", "range", "create_pretraining_data.convert_to_npz", "logging.debug", "create_pretraining_data.write_to_files_np", "len", "io.open", "reader.readlines", "len", "range", "len", "process_args.append", "worker_pool.map", "range", "min", "process_args.append", "worker_pool.map", "instances.extend", "len", "instances.extend", "create_pretraining_data.tokenize_lines_fn", "create_pretraining_data.create_instances_from_document", "all_documents[].append", "all_documents.append"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.convert_to_npz", "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.write_to_files_np", "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.tokenize_lines_fn", "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_instances_from_document"], ["", "def", "create_training_instances", "(", "x", ")", ":", "\n", "    ", "\"\"\"Create `TrainingInstance`s from raw text.\n\n    The expected input file format is the following:\n\n    (1) One sentence per line. These should ideally be actual sentences, not\n    entire paragraphs or arbitrary spans of text. (Because we use the\n    sentence boundaries for the \"next sentence prediction\" task).\n    (2) Blank lines between documents. Document boundaries are needed so\n    that the \"next sentence prediction\" task doesn't span between documents.\n\n    The function expect arguments packed in a tuple as described below.\n\n    Parameters\n    ----------\n    input_files : list of str\n        List of paths to input text files.\n    tokenizer : Tokenizer\n        The tokenizer\n    max_seq_length : int\n        The hard limit of maximum sequence length of sentence pairs\n    dupe_factor : int\n        Duplication factor.\n    short_seq_prob : float\n        The probability of sampling sequences shorter than the max_seq_length.\n    masked_lm_prob : float\n        The probability of replacing texts with masks/random words/original words.\n    max_predictions_per_seq : int\n        The hard limit of the number of predictions for masked words\n    whole_word_mask : bool\n        Whether to do masking for whole words\n    vocab : Vocab\n        The vocab for the model\n    nworker : int\n        The number of processes to help processing texts in parallel\n    worker_pool : multiprocessing.Pool\n        Must be provided if nworker > 1. The caller is responsible for the destruction of\n        the worker pool.\n    output_file : str or None\n        Path to the output file. If None, the result is not serialized. If provided,\n        results are  stored in the order of (input_ids, segment_ids, masked_lm_positions,\n        masked_lm_ids, masked_lm_weights, next_sentence_labels, valid_lengths).\n\n    Returns\n    -------\n    A tuple of np.ndarray : input_ids, masked_lm_ids, masked_lm_positions, masked_lm_weights\n                            next_sentence_labels, segment_ids, valid_lengths\n    \"\"\"", "\n", "(", "input_files", ",", "tokenizer", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "whole_word_mask", ",", "vocab", ",", "\n", "dupe_factor", ",", "nworker", ",", "worker_pool", ",", "output_file", ")", "=", "x", "\n", "\n", "time_start", "=", "time", ".", "time", "(", ")", "\n", "if", "nworker", ">", "1", ":", "\n", "        ", "assert", "worker_pool", "is", "not", "None", "\n", "\n", "", "all_documents", "=", "[", "[", "]", "]", "\n", "\n", "for", "input_file", "in", "input_files", ":", "\n", "        ", "with", "io", ".", "open", "(", "input_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "lines", "=", "reader", ".", "readlines", "(", ")", "\n", "num_lines", "=", "len", "(", "lines", ")", "\n", "num_lines_per_worker", "=", "(", "num_lines", "+", "nworker", "-", "1", ")", "//", "nworker", "\n", "process_args", "=", "[", "]", "\n", "\n", "# tokenize in parallel", "\n", "for", "worker_idx", "in", "range", "(", "nworker", ")", ":", "\n", "                ", "start", "=", "worker_idx", "*", "num_lines_per_worker", "\n", "end", "=", "min", "(", "(", "worker_idx", "+", "1", ")", "*", "num_lines_per_worker", ",", "num_lines", ")", "\n", "process_args", ".", "append", "(", "(", "lines", "[", "start", ":", "end", "]", ",", "tokenizer", ",", "vocab", ")", ")", "\n", "", "if", "worker_pool", ":", "\n", "                ", "tokenized_results", "=", "worker_pool", ".", "map", "(", "\n", "tokenize_lines_fn", ",", "process_args", ")", "\n", "", "else", ":", "\n", "                ", "tokenized_results", "=", "[", "tokenize_lines_fn", "(", "process_args", "[", "0", "]", ")", "]", "\n", "\n", "", "for", "tokenized_result", "in", "tokenized_results", ":", "\n", "                ", "for", "line", "in", "tokenized_result", ":", "\n", "                    ", "if", "not", "line", ":", "\n", "                        ", "if", "all_documents", "[", "-", "1", "]", ":", "\n", "                            ", "all_documents", ".", "append", "(", "[", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "all_documents", "[", "-", "1", "]", ".", "append", "(", "line", ")", "\n", "\n", "# remove the last empty document if any", "\n", "", "", "", "", "", "if", "not", "all_documents", "[", "-", "1", "]", ":", "\n", "        ", "all_documents", "=", "all_documents", "[", ":", "-", "1", "]", "\n", "\n", "# generate training instances", "\n", "", "instances", "=", "[", "]", "\n", "if", "worker_pool", ":", "\n", "        ", "process_args", "=", "[", "]", "\n", "for", "document_index", "in", "range", "(", "len", "(", "all_documents", ")", ")", ":", "\n", "            ", "process_args", ".", "append", "(", "(", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "whole_word_mask", ",", "\n", "vocab", ",", "tokenizer", ")", ")", "\n", "", "for", "_", "in", "range", "(", "dupe_factor", ")", ":", "\n", "            ", "instances_results", "=", "worker_pool", ".", "map", "(", "\n", "create_instances_from_document", ",", "process_args", ")", "\n", "for", "instances_result", "in", "instances_results", ":", "\n", "                ", "instances", ".", "extend", "(", "instances_result", ")", "\n", "", "", "npz_instances", "=", "worker_pool", ".", "apply", "(", "\n", "convert_to_npz", ",", "(", "instances", ",", "max_seq_length", ")", ")", "\n", "", "else", ":", "\n", "        ", "for", "_", "in", "range", "(", "dupe_factor", ")", ":", "\n", "            ", "for", "document_index", "in", "range", "(", "len", "(", "all_documents", ")", ")", ":", "\n", "                ", "instances", ".", "extend", "(", "\n", "create_instances_from_document", "(", "\n", "(", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "whole_word_mask", ",", "\n", "vocab", ",", "tokenizer", ")", ")", ")", "\n", "", "", "npz_instances", "=", "convert_to_npz", "(", "instances", ",", "max_seq_length", ")", "\n", "\n", "", "(", "input_ids", ",", "masked_lm_ids", ",", "masked_lm_positions", ",", "masked_lm_weights", ",", "\n", "next_sentence_labels", ",", "segment_ids", ",", "valid_lengths", ")", "=", "npz_instances", "\n", "\n", "# write output to files. Used when pre-generating files", "\n", "if", "output_file", ":", "\n", "        ", "features", "=", "(", "input_ids", ",", "segment_ids", ",", "masked_lm_positions", ",", "masked_lm_ids", ",", "\n", "masked_lm_weights", ",", "next_sentence_labels", ",", "valid_lengths", ")", "\n", "logging", ".", "debug", "(", "'*** Writing to output file %s ***'", ",", "output_file", ")", "\n", "write_to_files_np", "(", "features", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "max_predictions_per_seq", ",", "[", "output_file", "]", ")", "\n", "features", "=", "None", "\n", "", "else", ":", "\n", "        ", "features", "=", "(", "input_ids", ",", "masked_lm_ids", ",", "masked_lm_positions", ",", "masked_lm_weights", ",", "\n", "next_sentence_labels", ",", "segment_ids", ",", "valid_lengths", ")", "\n", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "debug", "(", "'Process %d files took %.1f s'", ",", "\n", "len", "(", "input_files", ")", ",", "time_end", "-", "time_start", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_instances_from_document": [[327, 443], ["random.random", "random.randint", "len", "current_chunk.append", "len", "range", "create_pretraining_data.truncate_seq_pair", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "create_pretraining_data.create_masked_lm_predictions", "create_pretraining_data.TrainingInstance", "instances.append", "len", "len", "random.randint", "tokens_a.extend", "random.randint", "random.randint", "range", "range", "len", "len", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "random.random", "len", "len", "tokens_b.extend", "len", "len", "tokens_b.extend", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.truncate_seq_pair", "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_masked_lm_predictions"], ["", "def", "create_instances_from_document", "(", "x", ")", ":", "\n", "    ", "\"\"\"Creates `TrainingInstance`s for a single document.\"\"\"", "\n", "(", "all_documents", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "whole_word_mask", ",", "vocab", ",", "tokenizer", ")", "=", "x", "\n", "document", "=", "all_documents", "[", "document_index", "]", "\n", "_MASK_TOKEN", "=", "vocab", "[", "vocab", ".", "mask_token", "]", "\n", "_CLS_TOKEN", "=", "vocab", "[", "vocab", ".", "cls_token", "]", "\n", "_SEP_TOKEN", "=", "vocab", "[", "vocab", ".", "sep_token", "]", "\n", "\n", "# Account for [CLS], [SEP], [SEP]", "\n", "max_num_tokens", "=", "max_seq_length", "-", "3", "\n", "\n", "# According to the original tensorflow implementation:", "\n", "# We *usually* want to fill up the entire sequence since we are padding", "\n", "# to `max_seq_length` anyways, so short sequences are generally wasted", "\n", "# computation. However, we *sometimes*", "\n", "# (i.e., short_seq_prob == 0.1, 10% of the time) want to use shorter", "\n", "# sequences to minimize the mismatch between pre-training and fine-tuning.", "\n", "# The `target_seq_length` is just a rough target however, whereas", "\n", "# `max_seq_length` is a hard limit.", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "random", ".", "random", "(", ")", "<", "short_seq_prob", ":", "\n", "        ", "target_seq_length", "=", "random", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "\n", "# We DON'T just concatenate all of the tokens from a document into a long", "\n", "# sequence and choose an arbitrary split point because this would make the", "\n", "# next sentence prediction task too easy. Instead, we split the input into", "\n", "# segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user", "\n", "# input.", "\n", "", "instances", "=", "[", "]", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "document", ")", ":", "# pylint: disable=R1702", "\n", "        ", "segment", "=", "document", "[", "i", "]", "\n", "current_chunk", ".", "append", "(", "segment", ")", "\n", "current_length", "+=", "len", "(", "segment", ")", "\n", "if", "i", "==", "len", "(", "document", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "            ", "if", "current_chunk", ":", "\n", "# `a_end` is how many segments from `current_chunk` go into the `A`", "\n", "# (first) sentence.", "\n", "                ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "                    ", "a_end", "=", "random", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "\n", "", "tokens_a", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "a_end", ")", ":", "\n", "                    ", "tokens_a", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "# Random next", "\n", "is_random_next", "=", "False", "\n", "if", "len", "(", "current_chunk", ")", "==", "1", "or", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                    ", "is_random_next", "=", "True", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "\n", "# randomly choose a document other than itself", "\n", "random_document_index", "=", "random", ".", "randint", "(", "\n", "0", ",", "len", "(", "all_documents", ")", "-", "2", ")", "\n", "if", "random_document_index", ">=", "document_index", ":", "\n", "                        ", "random_document_index", "+=", "1", "\n", "\n", "", "random_document", "=", "all_documents", "[", "random_document_index", "]", "\n", "random_start", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "random_document", ")", "-", "1", ")", "\n", "for", "j", "in", "range", "(", "random_start", ",", "len", "(", "random_document", ")", ")", ":", "\n", "                        ", "tokens_b", ".", "extend", "(", "random_document", "[", "j", "]", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "                            ", "break", "\n", "# We didn't actually use these segments so we 'put them back' so", "\n", "# they don't go to waste.", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "i", "-=", "num_unused_segments", "\n", "# Actual next", "\n", "", "else", ":", "\n", "                    ", "is_random_next", "=", "False", "\n", "for", "j", "in", "range", "(", "a_end", ",", "len", "(", "current_chunk", ")", ")", ":", "\n", "                        ", "tokens_b", ".", "extend", "(", "current_chunk", "[", "j", "]", ")", "\n", "", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", "\n", "\n", "assert", "len", "(", "tokens_a", ")", ">=", "1", "\n", "assert", "len", "(", "tokens_b", ")", ">=", "1", "\n", "\n", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "_CLS_TOKEN", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "_SEP_TOKEN", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "token", "in", "tokens_b", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "_SEP_TOKEN", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "(", "tokens", ",", "masked_lm_positions", ",", "\n", "masked_lm_labels", ")", "=", "create_masked_lm_predictions", "(", "\n", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "\n", "whole_word_mask", ",", "vocab", ",", "tokenizer", ",", "\n", "_MASK_TOKEN", ",", "_CLS_TOKEN", ",", "_SEP_TOKEN", ")", "\n", "instance", "=", "TrainingInstance", "(", "\n", "tokens", "=", "tokens", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "is_random_next", "=", "is_random_next", ",", "\n", "masked_lm_positions", "=", "masked_lm_positions", ",", "\n", "masked_lm_labels", "=", "masked_lm_labels", ",", "\n", "vocab", "=", "vocab", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_masked_lm_predictions": [[449, 524], ["enumerate", "random.shuffle", "list", "min", "set", "sorted", "max", "len", "masked_lm_positions.append", "masked_lm_labels.append", "cand_indexes[].append", "cand_indexes.append", "int", "len", "set.add", "sorted.append", "len", "tokenizer.is_first_subword", "round", "len", "len", "random.random", "MaskedLmInstance", "random.random", "random.randint", "len", "len"], "function", ["None"], ["def", "create_masked_lm_predictions", "(", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "\n", "whole_word_mask", ",", "vocab", ",", "tokenizer", ",", "\n", "_MASK_TOKEN", ",", "_CLS_TOKEN", ",", "_SEP_TOKEN", ")", ":", "\n", "    ", "\"\"\"Creates the predictions for the masked LM objective.\"\"\"", "\n", "cand_indexes", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "token", "in", "[", "_CLS_TOKEN", ",", "_SEP_TOKEN", "]", ":", "\n", "            ", "continue", "\n", "# Whole Word Masking means that if we mask all of the subwords", "\n", "# corresponding to an original word. When a word has been split into", "\n", "# subwords, the first token does not have any marker and any subsequence", "\n", "# tokens are prefixed with ##. So whenever we see the ## token, we", "\n", "# append it to the previous set of word indexes.", "\n", "#", "\n", "# Note that Whole Word Masking does *not* change the training code", "\n", "# at all -- we still predict each subword independently, softmaxed", "\n", "# over the entire vocabulary.", "\n", "", "if", "whole_word_mask", "and", "len", "(", "cand_indexes", ")", ">=", "1", "and", "not", "tokenizer", ".", "is_first_subword", "(", "vocab", ".", "idx_to_token", "[", "token", "]", ")", ":", "\n", "            ", "cand_indexes", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "cand_indexes", ".", "append", "(", "[", "i", "]", ")", "\n", "\n", "", "", "random", ".", "shuffle", "(", "cand_indexes", ")", "\n", "\n", "output_tokens", "=", "list", "(", "tokens", ")", "\n", "\n", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "\n", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "\n", "masked_lms", "=", "[", "]", "\n", "covered_indexes", "=", "set", "(", ")", "\n", "for", "index_set", "in", "cand_indexes", ":", "\n", "        ", "if", "len", "(", "masked_lms", ")", ">=", "num_to_predict", ":", "\n", "            ", "break", "\n", "# If adding a whole-word mask would exceed the maximum number of", "\n", "# predictions, then just skip this candidate.", "\n", "", "if", "len", "(", "masked_lms", ")", "+", "len", "(", "index_set", ")", ">", "num_to_predict", ":", "\n", "            ", "continue", "\n", "", "is_any_index_covered", "=", "False", "\n", "for", "index", "in", "index_set", ":", "\n", "            ", "if", "index", "in", "covered_indexes", ":", "\n", "                ", "is_any_index_covered", "=", "True", "\n", "break", "\n", "", "", "if", "is_any_index_covered", ":", "\n", "            ", "continue", "\n", "", "for", "index", "in", "index_set", ":", "\n", "            ", "covered_indexes", ".", "add", "(", "index", ")", "\n", "masked_token", "=", "None", "\n", "# 80% of the time, replace with [MASK]", "\n", "if", "random", ".", "random", "(", ")", "<", "0.8", ":", "\n", "                ", "masked_token", "=", "_MASK_TOKEN", "\n", "", "else", ":", "\n", "# 10% of the time, keep original", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                    ", "masked_token", "=", "tokens", "[", "index", "]", "\n", "# 10% of the time, replace with random word", "\n", "", "else", ":", "\n", "# generate a random word in [0, vocab_size - 1]", "\n", "                    ", "masked_token", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "vocab", ")", "-", "1", ")", "\n", "\n", "", "", "output_tokens", "[", "index", "]", "=", "masked_token", "\n", "\n", "masked_lms", ".", "append", "(", "MaskedLmInstance", "(", "\n", "index", "=", "index", ",", "label", "=", "tokens", "[", "index", "]", ")", ")", "\n", "", "", "assert", "len", "(", "masked_lms", ")", "<=", "num_to_predict", "\n", "masked_lms", "=", "sorted", "(", "masked_lms", ",", "key", "=", "lambda", "x", ":", "x", ".", "index", ")", "\n", "\n", "masked_lm_positions", "=", "[", "]", "\n", "masked_lm_labels", "=", "[", "]", "\n", "for", "p", "in", "masked_lms", ":", "\n", "        ", "masked_lm_positions", ".", "append", "(", "p", ".", "index", ")", "\n", "masked_lm_labels", ".", "append", "(", "p", ".", "label", ")", "\n", "\n", "", "return", "(", "output_tokens", ",", "masked_lm_positions", ",", "masked_lm_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.truncate_seq_pair": [[526, 542], ["len", "len", "len", "random.random", "trunc_tokens.pop", "len", "len"], "function", ["None"], ["", "def", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_num_tokens", ")", ":", "\n", "    ", "\"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "            ", "break", "\n", "\n", "", "trunc_tokens", "=", "tokens_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "tokens_b", "\n", "assert", "len", "(", "trunc_tokens", ")", ">=", "1", "\n", "\n", "# We want to sometimes truncate from the front and sometimes from the", "\n", "# back to add more randomness and avoid biases.", "\n", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.main": [[544, 604], ["time.time", "random.seed", "os.path.expanduser", "gluonnlp.utils.mkdir", "gluonnlp.data.utils._load_pretrained_vocab", "gluonnlp.data.GPT2BPETokenizer", "args.input_file.split", "len", "min", "logging.info", "range", "enumerate", "time.time", "logging.info", "input_files.extend", "logging.info", "len", "min", "file_splits.append", "os.path.join", "len", "process_args.append", "len", "multiprocessing.Pool", "multiprocessing.Pool.map", "glob.glob", "create_pretraining_data.create_training_instances", "os.path.expanduser", "str().zfill", "str"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_training_instances"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Main function.\"\"\"", "\n", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "# random seed", "\n", "random", ".", "seed", "(", "args", ".", "random_seed", ")", "\n", "\n", "# create output dir", "\n", "output_dir", "=", "os", ".", "path", ".", "expanduser", "(", "args", ".", "output_dir", ")", "\n", "nlp", ".", "utils", ".", "mkdir", "(", "output_dir", ")", "\n", "\n", "vocab", "=", "nlp", ".", "data", ".", "utils", ".", "_load_pretrained_vocab", "(", "\n", "args", ".", "dataset_name", ",", "root", "=", "output_dir", ",", "cls", "=", "nlp", ".", "vocab", ".", "BERTVocab", ")", "\n", "tokenizer", "=", "nlp", ".", "data", ".", "GPT2BPETokenizer", "(", ")", "\n", "\n", "# count the number of input files", "\n", "input_files", "=", "[", "]", "\n", "for", "input_pattern", "in", "args", ".", "input_file", ".", "split", "(", "','", ")", ":", "\n", "        ", "input_files", ".", "extend", "(", "glob", ".", "glob", "(", "os", ".", "path", ".", "expanduser", "(", "input_pattern", ")", ")", ")", "\n", "", "for", "input_file", "in", "input_files", ":", "\n", "        ", "logging", ".", "info", "(", "'\\t%s'", ",", "input_file", ")", "\n", "", "num_inputs", "=", "len", "(", "input_files", ")", "\n", "num_outputs", "=", "min", "(", "args", ".", "num_outputs", ",", "len", "(", "input_files", ")", ")", "\n", "logging", ".", "info", "(", "'*** Reading from %d input files ***'", ",", "num_inputs", ")", "\n", "\n", "# calculate the number of splits", "\n", "file_splits", "=", "[", "]", "\n", "split_size", "=", "(", "num_inputs", "+", "num_outputs", "-", "1", ")", "//", "num_outputs", "\n", "for", "i", "in", "range", "(", "num_outputs", ")", ":", "\n", "        ", "split_start", "=", "i", "*", "split_size", "\n", "split_end", "=", "min", "(", "num_inputs", ",", "(", "i", "+", "1", ")", "*", "split_size", ")", "\n", "file_splits", ".", "append", "(", "input_files", "[", "split_start", ":", "split_end", "]", ")", "\n", "\n", "# prepare workload", "\n", "", "count", "=", "0", "\n", "process_args", "=", "[", "]", "\n", "\n", "for", "i", ",", "file_split", "in", "enumerate", "(", "file_splits", ")", ":", "\n", "        ", "output_file", "=", "os", ".", "path", ".", "join", "(", "\n", "output_dir", ",", "'part-{}.npz'", ".", "format", "(", "str", "(", "i", ")", ".", "zfill", "(", "3", ")", ")", ")", "\n", "count", "+=", "len", "(", "file_split", ")", "\n", "process_args", ".", "append", "(", "(", "file_split", ",", "tokenizer", ",", "args", ".", "max_seq_length", ",", "args", ".", "short_seq_prob", ",", "\n", "args", ".", "masked_lm_prob", ",", "args", ".", "max_predictions_per_seq", ",", "\n", "args", ".", "whole_word_mask", ",", "\n", "vocab", ",", "args", ".", "dupe_factor", ",", "1", ",", "None", ",", "output_file", ")", ")", "\n", "\n", "# sanity check", "\n", "", "assert", "count", "==", "len", "(", "input_files", ")", "\n", "\n", "# dispatch to workers", "\n", "nworker", "=", "args", ".", "num_workers", "\n", "if", "nworker", ">", "1", ":", "\n", "        ", "pool", "=", "Pool", "(", "nworker", ")", "\n", "pool", ".", "map", "(", "create_training_instances", ",", "process_args", ")", "\n", "", "else", ":", "\n", "        ", "for", "process_arg", "in", "process_args", ":", "\n", "            ", "create_training_instances", "(", "process_arg", ")", "\n", "\n", "", "", "time_end", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "info", "(", "'Time cost=%.1f'", ",", "time_end", "-", "time_start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.get_parser": [[41, 116], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "utils.finetune_utils.ALL_TASKS_DIC.keys"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Bort fine-tune examples for various NLU tasks.'", ",", "\n", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "\n", "help", "=", "'number of epochs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ramp_up_epochs'", ",", "type", "=", "int", ",", "\n", "default", "=", "3", ",", "help", "=", "'number of ramp up epochs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--training_steps'", ",", "type", "=", "int", ",", "\n", "help", "=", "'The total training steps. Note that if specified, epochs will be ignored.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size. Number of examples per gpu in a minibatch.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev_batch_size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Batch size for dev set and test set'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--init'", ",", "type", "=", "str", ",", "default", "=", "'uniform'", ",", "choices", "=", "[", "'gaussian'", ",", "'uniform'", ",", "'orthogonal'", ",", "'xavier'", "]", ",", "\n", "help", "=", "'Initialization distribution.'", ")", "\n", "parser", ".", "add_argument", "(", "'--prob'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "'The probability around which to center the distribution.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "3e-5", ",", "\n", "help", "=", "'Initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "\n", "help", "=", "'Small value to avoid division by 0'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup_ratio'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'ratio of warmup steps used in NOAM\\'s stepsize schedule'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--max_len'", ",", "type", "=", "int", ",", "default", "=", "512", ",", "\n", "help", "=", "'Maximum length of the sentence pairs'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Which GPU to use for fine-tuning.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_scheduler'", ",", "default", "=", "True", ",", "type", "=", "bool", ")", "\n", "parser", ".", "add_argument", "(", "'--accumulate'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'The number of batches for gradients accumulation to simulate large batch size. '", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--log_interval'", ",", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "help", "=", "'report interval'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_distributed'", ",", "default", "=", "False", ",", "type", "=", "bool", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--task_name'", ",", "type", "=", "str", ",", "choices", "=", "ALL_TASKS_DIC", ".", "keys", "(", ")", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "'openwebtext_ccnews_stories_books_cased'", ",", "\n", "choices", "=", "[", "'book_corpus_wiki_en_uncased'", ",", "'book_corpus_wiki_en_cased'", ",", "\n", "'openwebtext_book_corpus_wiki_en_uncased'", ",", "'wiki_multilingual_uncased'", ",", "\n", "'wiki_multilingual_cased'", ",", "'wiki_cn_cased'", ",", "\n", "'openwebtext_ccnews_stories_books_cased'", "]", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pretrained_parameters'", ",", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "help", "=", "'Pre-trained Bort model parameter file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_parameters'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output_dir'", ",", "\n", "help", "=", "'The output directory to where the model params will be written.'", ")", "\n", "parser", ".", "add_argument", "(", "'--only_inference'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If set, we skip training and only perform inference on dev and test data.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dtype'", ",", "type", "=", "str", ",", "default", "=", "'float32'", ",", "\n", "choices", "=", "[", "'float32'", ",", "'float16'", "]", ",", "help", "=", "'The data type for training.'", ")", "\n", "parser", ".", "add_argument", "(", "'--early_stop'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Whether to perform early stopping based on the metric on dev set.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--multirc_test_location'", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "\"/home/ec2-user/.mxnet/datasets/superglue_multirc/test.jsonl\"", ",", "\n", "help", "=", "'Location of the MultiRC test set, in case it is not in the default location.'", ")", "\n", "parser", ".", "add_argument", "(", "'--record_test_location'", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "\"/home/ec2-user/.mxnet/datasets/superglue_record/test.jsonl\"", ",", "\n", "help", "=", "'Location of the ReCoRD test set, in case it is not in the default location.'", ")", "\n", "parser", ".", "add_argument", "(", "'--record_dev_location'", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "\"/home/ec2-user/.mxnet/datasets/superglue_record/val.jsonl\"", ",", "\n", "help", "=", "'Location of the ReCoRD dev set, in case it is not in the default location.'", ")", "\n", "parser", ".", "add_argument", "(", "'--race_dataset_location'", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "None", ",", "\n", "help", "=", "'Location of the RACE dataset, in case it is not in the default location.'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.load_and_setup_model": [[118, 176], ["bort.bort.get_bort_model", "bort.bort.BortClassifier", "gluonnlp.utils.mkdir", "gluonnlp.data.GPT2BPETokenizer", "warnings.warn", "mxnet.gluon.loss.L2Loss", "len", "mxnet.gluon.loss.SoftmaxCELoss", "mxnet.init.Normal", "mxnet.init.Uniform", "mxnet.init.Orthogonal", "mxnet.init.Xavier", "bort.bort.BortClassifier.classifier.initialize", "logging.info", "gluonnlp.utils.load_parameters", "logging.info", "gluonnlp.utils.load_parameters"], "function", ["home.repos.pwc.inspect_result.alexa_bort.bort.bort.get_bort_model"], ["", "def", "load_and_setup_model", "(", "task", ",", "args", ")", ":", "\n", "\n", "    ", "pretrained_parameters", "=", "args", ".", "pretrained_parameters", "\n", "model_parameters", "=", "args", ".", "model_parameters", "\n", "dataset", "=", "args", ".", "dataset", "\n", "\n", "if", "only_inference", "and", "not", "model_parameters", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "'model_parameters is not set. Randomly initialized model will be used for inference.'", ")", "\n", "\n", "", "get_pretrained", "=", "not", "(", "\n", "pretrained_parameters", "is", "not", "None", "or", "model_parameters", "is", "not", "None", ")", "\n", "\n", "# STS-B is a regression task and STSBTask().class_labels returns None", "\n", "do_regression", "=", "not", "task", ".", "class_labels", "\n", "if", "do_regression", ":", "\n", "        ", "num_classes", "=", "1", "\n", "loss_function", "=", "gluon", ".", "loss", ".", "L2Loss", "(", ")", "\n", "", "else", ":", "\n", "        ", "num_classes", "=", "len", "(", "task", ".", "class_labels", ")", "\n", "loss_function", "=", "gluon", ".", "loss", ".", "SoftmaxCELoss", "(", ")", "\n", "\n", "", "bort", ",", "vocabulary", "=", "get_bort_model", "(", "\"bort_4_8_768_1024\"", ",", "dataset_name", "=", "dataset", ",", "\n", "pretrained", "=", "get_pretrained", ",", "ctx", "=", "ctx", ",", "\n", "use_pooler", "=", "True", ",", "use_decoder", "=", "False", ",", "use_classifier", "=", "False", ")", "\n", "# TODO: CoLA uses a different classifier!", "\n", "model", "=", "BortClassifier", "(", "bort", ",", "dropout", "=", "args", ".", "dropout", ",", "num_classes", "=", "num_classes", ")", "\n", "\n", "if", "args", ".", "init", "==", "\"gaussian\"", ":", "\n", "        ", "initializer", "=", "mx", ".", "init", ".", "Normal", "(", "args", ".", "prob", ")", "\n", "", "if", "args", ".", "init", "==", "\"uniform\"", ":", "\n", "        ", "initializer", "=", "mx", ".", "init", ".", "Uniform", "(", "args", ".", "prob", ")", "\n", "", "if", "args", ".", "init", "==", "\"orthogonal\"", ":", "\n", "        ", "initializer", "=", "mx", ".", "init", ".", "Orthogonal", "(", "scale", "=", "args", ".", "prob", ")", "\n", "", "if", "args", ".", "init", "==", "\"xavier\"", ":", "\n", "        ", "initializer", "=", "mx", ".", "init", ".", "Xavier", "(", ")", "\n", "\n", "", "if", "not", "model_parameters", ":", "\n", "        ", "model", ".", "classifier", ".", "initialize", "(", "init", "=", "initializer", ",", "ctx", "=", "ctx", ")", "\n", "\n", "# load checkpointing", "\n", "", "if", "pretrained_parameters", ":", "\n", "        ", "logging", ".", "info", "(", "'loading Bort params from %s'", ",", "pretrained_parameters", ")", "\n", "nlp", ".", "utils", ".", "load_parameters", "(", "\n", "model", ".", "bort", ",", "pretrained_parameters", ",", "ctx", "=", "ctx", ",", "ignore_extra", "=", "True", ",", "cast_dtype", "=", "True", ")", "\n", "\n", "", "if", "model_parameters", ":", "\n", "        ", "logging", ".", "info", "(", "'loading model params from %s'", ",", "model_parameters", ")", "\n", "nlp", ".", "utils", ".", "load_parameters", "(", "\n", "model", ",", "model_parameters", ",", "ctx", "=", "ctx", ",", "cast_dtype", "=", "True", ")", "\n", "\n", "# data processing", "\n", "", "nlp", ".", "utils", ".", "mkdir", "(", "output_dir", ")", "\n", "do_lower_case", "=", "'uncased'", "in", "dataset", "\n", "\n", "tokenizer", "=", "nlp", ".", "data", ".", "GPT2BPETokenizer", "(", ")", "\n", "\n", "return", "model", ",", "tokenizer", ",", "loss_function", ",", "vocabulary", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.setup_logger": [[178, 192], ["logging.getLogger", "logging.getLogger.setLevel", "logging.captureWarnings", "logging.FileHandler", "logging.Formatter", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logging.getLogger.addHandler"], "function", ["None"], ["", "def", "setup_logger", "(", "args", ")", ":", "\n", "    ", "log", "=", "logging", ".", "getLogger", "(", ")", "\n", "log", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "logging", ".", "captureWarnings", "(", "True", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "'log_{0}.txt'", ".", "format", "(", "task_name", ")", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "fmt", "=", "'%(levelname)s:%(name)s:%(asctime)s %(message)s'", ",", "\n", "datefmt", "=", "'%H:%M:%S'", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "console", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "console", ".", "setFormatter", "(", "formatter", ")", "\n", "log", ".", "addHandler", "(", "console", ")", "\n", "log", ".", "addHandler", "(", "fh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.train": [[194, 345], ["model.collect_params", "mxnet.gluon.Trainer", "int", "logging.info", "int", "model.collect_params().items", "time.time", "range", "logging.info", "mxnet.contrib.amp.amp.init_trainer", "metric_history.sort", "os.path.join", "gluonnlp.utils.load_parameters", "logging.info", "model.collect_params", "model.collect_params.values", "logging.info", "metric.reset", "time.time", "model.collect_params.zero_grad", "enumerate", "mxnet.nd.waitall", "metric_history.append", "os.path.join", "gluonnlp.utils.save_parameters", "logging.info", "time.time", "logging.info", "finetune_bort.test", "loss_function().mean.asscalar", "metric.update", "finetune_bort.evaluate", "gluon.Trainer.set_learning_rate", "mxnet.autograd.record", "model", "loss_function().mean", "gluon.Trainer.allreduce_grads", "gluonnlp.utils.clip_grad_global_norm", "gluon.Trainer.update", "label.reshape.reshape", "max", "input_ids.as_in_context", "valid_length.as_in_context().astype", "loss_function().mean.backward", "model.collect_params.zero_grad", "utils.finetune_utils.do_log", "loss_function", "mxnet.contrib.amp.amp.scale_loss", "mxnet.autograd.backward", "len", "hvd.rank", "valid_length.as_in_context", "label.reshape.as_in_context", "sum", "len"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_parameters", "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.test", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.evaluate", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.do_log", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward"], ["", "def", "train", "(", "metric", ")", ":", "\n", "    ", "\"\"\"Training function.\"\"\"", "\n", "if", "not", "only_inference", ":", "\n", "        ", "logging", ".", "info", "(", "\n", "'Now we are doing Bort classification training on %s!'", ",", "ctx", ")", "\n", "\n", "", "all_model_params", "=", "model", ".", "collect_params", "(", ")", "\n", "optimizer_params", "=", "{", "'learning_rate'", ":", "lr", ",", "\n", "'epsilon'", ":", "epsilon", ",", "'wd'", ":", "args", ".", "weight_decay", "}", "\n", "trainer", "=", "gluon", ".", "Trainer", "(", "all_model_params", ",", "\"bertadam\"", ",", "optimizer_params", ",", "\n", "update_on_kvstore", "=", "False", ")", "\n", "if", "args", ".", "dtype", "==", "'float16'", ":", "\n", "        ", "amp", ".", "init_trainer", "(", "trainer", ")", "\n", "\n", "", "epoch_number", "=", "args", ".", "ramp_up_epochs", "\n", "step_size", "=", "batch_size", "*", "accumulate", "if", "accumulate", "else", "batch_size", "\n", "num_train_steps", "=", "int", "(", "num_train_examples", "/", "step_size", "*", "args", ".", "ramp_up_epochs", ")", "\n", "if", "args", ".", "training_steps", ":", "\n", "        ", "num_train_steps", "=", "args", ".", "training_steps", "\n", "epoch_number", "=", "9999", "\n", "\n", "", "logging", ".", "info", "(", "'training steps=%d'", ",", "num_train_steps", ")", "\n", "warmup_ratio", "=", "args", ".", "warmup_ratio", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "warmup_ratio", ")", "\n", "step_num", "=", "0", "\n", "\n", "# Do not apply weight decay on LayerNorm and bias terms", "\n", "for", "_", ",", "v", "in", "model", ".", "collect_params", "(", "'.*beta|.*gamma|.*bias'", ")", ".", "items", "(", ")", ":", "\n", "        ", "v", ".", "wd_mult", "=", "0.0", "\n", "# Collect differentiable parameters", "\n", "", "params", "=", "[", "p", "for", "p", "in", "all_model_params", ".", "values", "(", ")", "if", "p", ".", "grad_req", "!=", "'null'", "]", "\n", "\n", "# Set grad_req if gradient accumulation is required", "\n", "if", "accumulate", "and", "accumulate", ">", "1", ":", "\n", "        ", "for", "p", "in", "params", ":", "\n", "            ", "p", ".", "grad_req", "=", "'add'", "\n", "\n", "# track best eval score", "\n", "", "", "metric_history", "=", "[", "]", "\n", "best_metric", "=", "None", "\n", "patience", "=", "args", ".", "early_stop", "\n", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "finish_flag", "=", "False", "\n", "for", "epoch_id", "in", "range", "(", "args", ".", "epochs", ")", ":", "\n", "        ", "if", "args", ".", "early_stop", "and", "patience", "==", "0", ":", "\n", "            ", "logging", ".", "info", "(", "'Early stopping at epoch %d'", ",", "epoch_id", ")", "\n", "break", "\n", "", "if", "finish_flag", ":", "\n", "            ", "break", "\n", "", "if", "not", "only_inference", ":", "\n", "            ", "metric", ".", "reset", "(", ")", "\n", "step_loss", "=", "0", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "all_model_params", ".", "zero_grad", "(", ")", "\n", "\n", "for", "batch_id", ",", "seqs", "in", "enumerate", "(", "train_data", ")", ":", "\n", "# learning rate schedule", "\n", "                ", "if", "args", ".", "use_scheduler", ":", "\n", "                    ", "if", "step_num", "<", "num_warmup_steps", ":", "\n", "                        ", "new_lr", "=", "lr", "*", "step_num", "/", "num_warmup_steps", "\n", "", "else", ":", "\n", "                        ", "non_warmup_steps", "=", "step_num", "-", "num_warmup_steps", "\n", "offset", "=", "non_warmup_steps", "/", "(", "num_train_steps", "-", "num_warmup_steps", ")", "\n", "new_lr", "=", "max", "(", "1e-7", ",", "lr", "-", "offset", "*", "lr", ")", "\n", "\n", "", "trainer", ".", "set_learning_rate", "(", "new_lr", ")", "\n", "\n", "# forward and backward", "\n", "", "with", "mx", ".", "autograd", ".", "record", "(", ")", ":", "\n", "                    ", "if", "args", ".", "no_distributed", ":", "\n", "                        ", "input_ids", ",", "segment_ids", ",", "valid_length", ",", "label", "=", "seqs", "\n", "", "else", ":", "\n", "                        ", "input_ids", ",", "segment_ids", ",", "valid_length", ",", "label", "=", "seqs", "[", "\n", "hvd", ".", "rank", "(", ")", "]", "\n", "\n", "", "out", "=", "model", "(", "input_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_length", ".", "as_in_context", "(", "ctx", ")", ".", "astype", "(", "'float32'", ")", ")", "\n", "ls", "=", "loss_function", "(", "out", ",", "label", ".", "as_in_context", "(", "ctx", ")", ")", ".", "mean", "(", ")", "\n", "if", "args", ".", "dtype", "==", "'float16'", ":", "\n", "                        ", "with", "amp", ".", "scale_loss", "(", "ls", ",", "trainer", ")", "as", "scaled_loss", ":", "\n", "                            ", "mx", ".", "autograd", ".", "backward", "(", "scaled_loss", ")", "\n", "", "", "else", ":", "\n", "                        ", "ls", ".", "backward", "(", ")", "\n", "\n", "# update", "\n", "", "", "if", "not", "accumulate", "or", "(", "batch_id", "+", "1", ")", "%", "accumulate", "==", "0", ":", "\n", "                    ", "trainer", ".", "allreduce_grads", "(", ")", "\n", "nlp", ".", "utils", ".", "clip_grad_global_norm", "(", "params", ",", "1", ")", "\n", "trainer", ".", "update", "(", "accumulate", "if", "accumulate", "else", "1", ")", "\n", "step_num", "+=", "1", "\n", "if", "accumulate", "and", "accumulate", ">", "1", ":", "\n", "# set grad to zero for gradient accumulation", "\n", "                        ", "all_model_params", ".", "zero_grad", "(", ")", "\n", "\n", "", "", "step_loss", "+=", "ls", ".", "asscalar", "(", ")", "\n", "if", "not", "do_regression", ":", "\n", "                    ", "label", "=", "label", ".", "reshape", "(", "(", "-", "1", ")", ")", "\n", "", "metric", ".", "update", "(", "[", "label", "]", ",", "[", "out", "]", ")", "\n", "if", "(", "batch_id", "+", "1", ")", "%", "(", "args", ".", "log_interval", ")", "==", "0", ":", "\n", "                    ", "if", "is_master_node", ":", "\n", "                        ", "do_log", "(", "batch_id", ",", "len", "(", "train_data", ")", ",", "metric", ",", "step_loss", ",", "args", ".", "log_interval", ",", "\n", "epoch_id", "=", "epoch_id", ",", "learning_rate", "=", "trainer", ".", "learning_rate", ")", "\n", "", "step_loss", "=", "0", "\n", "", "", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "\n", "# inference on dev data", "\n", "", "tmp_metric", "=", "[", "]", "\n", "for", "segment", ",", "dev_data", "in", "dev_data_list", ":", "\n", "            ", "if", "is_master_node", ":", "\n", "                ", "metric_nm", ",", "metric_val", "=", "evaluate", "(", "\n", "dev_data", ",", "metric", ",", "segment", ",", "epoch", "=", "epoch_id", ")", "\n", "if", "best_metric", "is", "None", "or", "metric_val", ">=", "best_metric", ":", "\n", "                    ", "best_metric", "=", "metric_val", "\n", "patience", "=", "args", ".", "early_stop", "\n", "", "else", ":", "\n", "                    ", "if", "args", ".", "early_stop", "is", "not", "None", ":", "\n", "                        ", "patience", "-=", "1", "\n", "", "", "tmp_metric", "+=", "metric_val", "\n", "\n", "", "", "if", "is_master_node", ":", "\n", "# For multi-valued tasks (e.g., MNLI), we maximize the average of", "\n", "# the metrics.", "\n", "            ", "metric_history", ".", "append", "(", "\n", "(", "epoch_id", ",", "metric_nm", "+", "[", "\"average\"", "]", ",", "metric_val", "+", "[", "sum", "(", "tmp_metric", ")", "/", "len", "(", "tmp_metric", ")", "]", ")", ")", "\n", "\n", "", "if", "not", "only_inference", "and", "is_master_node", ":", "\n", "            ", "ckpt_name", "=", "'model_bort_{0}_{1}.params'", ".", "format", "(", "task_name", ",", "epoch_id", ")", "\n", "params_saved", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "ckpt_name", ")", "\n", "nlp", ".", "utils", ".", "save_parameters", "(", "model", ",", "params_saved", ")", "\n", "logging", ".", "info", "(", "'params saved in: %s'", ",", "params_saved", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "info", "(", "'Time cost=%.2fs'", ",", "toc", "-", "tic", ")", "\n", "tic", "=", "toc", "\n", "\n", "", "", "if", "not", "only_inference", "and", "is_master_node", ":", "\n", "        ", "metric_history", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "-", "1", "]", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "epoch_id", ",", "metric_nm", ",", "metric_val", "=", "metric_history", "[", "0", "]", "\n", "ckpt_name", "=", "'model_bort_{0}_{1}.params'", ".", "format", "(", "task_name", ",", "epoch_id", ")", "\n", "params_saved", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "ckpt_name", ")", "\n", "nlp", ".", "utils", ".", "load_parameters", "(", "model", ",", "params_saved", ")", "\n", "metric_str", "=", "'Best model at epoch {}. Validation metrics:'", ".", "format", "(", "\n", "epoch_id", ")", "\n", "metric_str", "+=", "','", ".", "join", "(", "[", "i", "+", "':%.4f'", "for", "i", "in", "metric_nm", "]", ")", "\n", "logging", ".", "info", "(", "metric_str", ",", "*", "metric_val", ")", "\n", "\n", "# inference on test data", "\n", "", "for", "segment", ",", "test_data", "in", "test_data_list", ":", "\n", "        ", "if", "is_master_node", ":", "\n", "            ", "test", "(", "test_data", ",", "segment", ",", "acc", "=", "metric_val", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.evaluate": [[347, 425], ["logging.info", "metric.reset", "time.time", "collections.defaultdict", "enumerate", "logging.info", "mxnet.nd.waitall", "time.time", "logging.info", "RACEHash", "ReCoRDHash", "model", "loss_function().mean", "loss_function().mean.asscalar", "metric.update", "enumerate", "metric.get", "metric.get", "isinstance", "process_RACE_answers", "process_ReCoRD_answers", "input_ids.as_in_context", "valid_length.as_in_context().astype", "label.reshape.reshape", "zip", "all_results[].append", "mxnet.nd.topk().asnumpy", "zip", "utils.finetune_utils.do_log", "[].asnumpy().argmax", "metric_nm.append", "metric_val.append", "loss_function", "results.append", "len", "[].asnumpy", "collections.defaultdict.values", "collections.defaultdict.values", "e_metric_val", "str", "len", "valid_length.as_in_context", "label.reshape.as_in_context", "mxnet.nd.topk", "[].asnumpy", "mxnet.nd.softmax", "int"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.RACEHash", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.ReCoRDHash", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_RACE_answers", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_ReCoRD_answers", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.do_log"], ["", "", "", "def", "evaluate", "(", "loader_dev", ",", "metric", ",", "segment", ",", "epoch", "=", "0", ")", ":", "\n", "    ", "\"\"\"Evaluate the model on validation dataset.\"\"\"", "\n", "logging", ".", "info", "(", "'Now we are doing evaluation on %s with %s.'", ",", "segment", ",", "ctx", ")", "\n", "metric", ".", "reset", "(", ")", "\n", "step_loss", "=", "0", "\n", "counter", "=", "0", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "\n", "all_results", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "if", "\"RACE\"", "in", "args", ".", "task_name", ":", "\n", "        ", "from", "utils", ".", "finetune_utils", "import", "process_RACE_answers", ",", "RACEHash", "\n", "race_dev_data", "=", "RACEHash", "(", "\n", "args", ".", "race_dataset_location", ",", "args", ".", "task_name", ",", "segment", "=", "\"dev\"", ")", "\n", "results", "=", "[", "]", "\n", "", "if", "\"ReCoRD\"", "in", "args", ".", "task_name", ":", "\n", "        ", "from", "utils", ".", "finetune_utils", "import", "process_ReCoRD_answers", ",", "ReCoRDHash", "\n", "record_dev_data", "=", "ReCoRDHash", "(", "args", ".", "record_dev_location", ")", "\n", "results", "=", "[", "]", "\n", "\n", "", "for", "batch_id", ",", "seqs", "in", "enumerate", "(", "loader_dev", ")", ":", "\n", "        ", "input_ids", ",", "segment_ids", ",", "valid_length", ",", "label", "=", "seqs", "\n", "out", "=", "model", "(", "input_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_length", ".", "as_in_context", "(", "ctx", ")", ".", "astype", "(", "'float32'", ")", ")", "\n", "ls", "=", "loss_function", "(", "out", ",", "label", ".", "as_in_context", "(", "ctx", ")", ")", ".", "mean", "(", ")", "\n", "step_loss", "+=", "ls", ".", "asscalar", "(", ")", "\n", "if", "not", "do_regression", ":", "\n", "            ", "label", "=", "label", ".", "reshape", "(", "(", "-", "1", ")", ")", "\n", "\n", "", "metric", ".", "update", "(", "[", "label", "]", ",", "[", "out", "]", ")", "\n", "for", "example_id", ",", "(", "l", ",", "p", ")", "in", "enumerate", "(", "zip", "(", "label", ",", "out", ")", ")", ":", "\n", "            ", "all_results", "[", "counter", "]", ".", "append", "(", "[", "[", "l", "]", ",", "[", "p", "]", "]", ")", "\n", "counter", "+=", "1", "\n", "\n", "", "if", "\"RACE\"", "in", "args", ".", "task_name", ":", "\n", "            ", "indices", "=", "mx", ".", "nd", ".", "topk", "(", "\n", "out", ",", "k", "=", "1", ",", "ret_typ", "=", "'indices'", ",", "dtype", "=", "'int32'", ")", ".", "asnumpy", "(", ")", "\n", "for", "index", ",", "logits", "in", "zip", "(", "indices", ",", "out", ")", ":", "\n", "                ", "results", ".", "append", "(", "\n", "(", "task", ".", "class_labels", "[", "int", "(", "index", ")", "]", ",", "mx", ".", "nd", ".", "softmax", "(", "logits", ")", ")", ")", "\n", "\n", "", "", "if", "(", "batch_id", "+", "1", ")", "%", "(", "args", ".", "log_interval", ")", "==", "0", ":", "\n", "            ", "do_log", "(", "batch_id", ",", "len", "(", "loader_dev", ")", ",", "metric", ",", "\n", "step_loss", ",", "args", ".", "log_interval", ")", "\n", "step_loss", "=", "0", "\n", "\n", "", "", "if", "args", ".", "task_name", "in", "EXTRA_METRICS", ":", "\n", "        ", "metric_nm_", ",", "metric_val_", "=", "metric", ".", "get", "(", ")", "\n", "metric_nm", ",", "metric_val", "=", "[", "metric_nm_", "]", ",", "[", "metric_val_", "]", "\n", "labels", "=", "[", "v", "[", "0", "]", "[", "0", "]", "[", "0", "]", ".", "asnumpy", "(", ")", "[", "0", "]", "for", "v", "in", "all_results", ".", "values", "(", ")", "]", "\n", "preds", "=", "[", "(", "v", "[", "0", "]", "[", "-", "1", "]", "[", "0", "]", ".", "asnumpy", "(", ")", ")", ".", "argmax", "(", ")", "\n", "for", "v", "in", "all_results", ".", "values", "(", ")", "]", "\n", "for", "e_metric_nm", ",", "e_metric_val", "in", "EXTRA_METRICS", "[", "args", ".", "task_name", "]", ":", "\n", "            ", "metric_nm", ".", "append", "(", "e_metric_nm", ")", "\n", "metric_val", ".", "append", "(", "e_metric_val", "(", "labels", ",", "preds", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "metric_nm", ",", "metric_val", "=", "metric", ".", "get", "(", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "metric_nm", ",", "list", ")", ":", "\n", "        ", "metric_nm", ",", "metric_val", "=", "[", "metric_nm", "]", ",", "[", "metric_val", "]", "\n", "\n", "", "if", "\"RACE\"", "in", "args", ".", "task_name", ":", "\n", "# Class accuracy is bugged; we only need regular accuracy anyway", "\n", "        ", "result_data", ",", "class_accuracy", "=", "process_RACE_answers", "(", "\n", "race_dev_data", ",", "results", ")", "\n", "", "if", "\"ReCoRD\"", "in", "args", ".", "task_name", ":", "\n", "        ", "result_data", ",", "class_accuracy", "=", "process_ReCoRD_answers", "(", "\n", "results", ",", "record_dev_data", ")", "\n", "\n", "", "metric_str", "=", "'epoch: '", "+", "str", "(", "epoch", ")", "+", "'; validation metrics:'", "+", "','", ".", "join", "(", "[", "i", "+", "':%.4f'", "for", "i", "in", "metric_nm", "]", ")", "\n", "logging", ".", "info", "(", "metric_str", ",", "*", "metric_val", ")", "\n", "\n", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "info", "(", "'Time cost=%.2fs, throughput=%.2f samples/s'", ",", "toc", "-", "tic", ",", "\n", "dev_batch_size", "*", "len", "(", "loader_dev", ")", "/", "(", "toc", "-", "tic", ")", ")", "\n", "return", "metric_nm", ",", "metric_val", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.finetune_bort.test": [[427, 499], ["logging.info", "time.time", "enumerate", "mxnet.nd.waitall", "time.time", "logging.info", "segment.replace.replace", "segment.replace.replace", "segment.replace.replace", "os.path.join", "model", "ReCoRDHash", "process_ReCoRD_answers", "process_MultiRC_answers", "RACEHash", "process_RACE_answers", "input_ids.as_in_context", "valid_length.as_in_context().astype", "model.asnumpy().reshape().tolist", "mxnet.nd.topk().asnumpy", "zip", "io.open", "f.write", "enumerate", "results.append", "len", "str", "f.write", "io.open", "io.open", "valid_length.as_in_context", "model.asnumpy().reshape", "mxnet.nd.topk", "results.append", "results.append", "segment.replace.replace", "f.write", "f.write", "enumerate", "json.dumps", "f.write", "f.write", "f.write", "model.asnumpy", "mxnet.nd.softmax", "str", "json.dumps", "int", "int", "str"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.ReCoRDHash", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_ReCoRD_answers", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_MultiRC_answers", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.RACEHash", "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_RACE_answers"], ["", "def", "test", "(", "loader_test", ",", "segment", ",", "acc", "=", "0", ")", ":", "\n", "    ", "\"\"\"Inference function on the test dataset.\"\"\"", "\n", "logging", ".", "info", "(", "'Now we are doing testing on %s with %s.'", ",", "segment", ",", "ctx", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "\n", "# Eval loop", "\n", "for", "_", ",", "seqs", "in", "enumerate", "(", "loader_test", ")", ":", "\n", "        ", "input_ids", ",", "segment_ids", ",", "valid_length", "=", "seqs", "\n", "out", "=", "model", "(", "input_ids", ".", "as_in_context", "(", "ctx", ")", ",", "\n", "valid_length", ".", "as_in_context", "(", "ctx", ")", ".", "astype", "(", "'float32'", ")", ")", "\n", "if", "not", "task", ".", "class_labels", ":", "\n", "# regression task", "\n", "            ", "for", "result", "in", "out", ".", "asnumpy", "(", ")", ".", "reshape", "(", "-", "1", ")", ".", "tolist", "(", ")", ":", "\n", "                ", "results", ".", "append", "(", "'{:.3f}'", ".", "format", "(", "result", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "indices", "=", "mx", ".", "nd", ".", "topk", "(", "\n", "out", ",", "k", "=", "1", ",", "ret_typ", "=", "'indices'", ",", "dtype", "=", "'int32'", ")", ".", "asnumpy", "(", ")", "\n", "for", "index", ",", "logits", "in", "zip", "(", "indices", ",", "out", ")", ":", "\n", "                ", "if", "args", ".", "task_name", "==", "\"ReCoRD\"", "or", "\"RACE\"", "in", "args", ".", "task_name", ":", "\n", "                    ", "results", ".", "append", "(", "\n", "(", "task", ".", "class_labels", "[", "int", "(", "index", ")", "]", ",", "mx", ".", "nd", ".", "softmax", "(", "logits", ")", ")", ")", "\n", "", "else", ":", "\n", "                    ", "results", ".", "append", "(", "task", ".", "class_labels", "[", "int", "(", "index", ")", "]", ")", "\n", "\n", "", "", "", "", "if", "args", ".", "task_name", "==", "\"ReCoRD\"", ":", "\n", "        ", "from", "utils", ".", "finetune_utils", "import", "process_ReCoRD_answers", ",", "ReCoRDHash", "\n", "record_test_data", "=", "ReCoRDHash", "(", "args", ".", "record_test_location", ")", "\n", "result_data", ",", "_", "=", "process_ReCoRD_answers", "(", "results", ",", "record_test_data", ")", "\n", "", "if", "args", ".", "task_name", "==", "\"MultiRC\"", ":", "\n", "        ", "from", "utils", ".", "finetune_utils", "import", "process_MultiRC_answers", "\n", "result_data", "=", "process_MultiRC_answers", "(", "\n", "results", ",", "args", ".", "multirc_test_location", ")", "\n", "", "if", "\"RACE\"", "in", "args", ".", "task_name", ":", "\n", "        ", "from", "utils", ".", "finetune_utils", "import", "process_RACE_answers", ",", "RACEHash", "\n", "race_test_data", "=", "RACEHash", "(", "\n", "args", ".", "race_dataset_location", ",", "args", ".", "task_name", ",", "segment", "=", "\"test\"", ")", "\n", "result_data", ",", "_", "=", "process_RACE_answers", "(", "race_test_data", ",", "results", ")", "\n", "\n", "", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logging", ".", "info", "(", "'Time cost=%.2fs, throughput=%.2f samples/s'", ",", "toc", "-", "tic", ",", "\n", "dev_batch_size", "*", "len", "(", "loader_test", ")", "/", "(", "toc", "-", "tic", ")", ")", "\n", "\n", "# Write the results to a file.", "\n", "segment", "=", "segment", ".", "replace", "(", "'_mismatched'", ",", "'-mm'", ")", "\n", "segment", "=", "segment", ".", "replace", "(", "'_matched'", ",", "'-m'", ")", "\n", "segment", "=", "segment", ".", "replace", "(", "'SST'", ",", "'SST-2'", ")", "\n", "\n", "filename", "=", "args", ".", "task_name", "+", "segment", ".", "replace", "(", "'test'", ",", "''", ")", "+", "str", "(", "acc", ")", "+", "'.'", "+", "task", ".", "output_format", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "filename", ")", "\n", "\n", "if", "task", ".", "output_format", "==", "\"tsv\"", ":", "\n", "        ", "with", "io", ".", "open", "(", "test_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "u'index\\tprediction\\n'", ")", "\n", "for", "i", ",", "pred", "in", "enumerate", "(", "results", ")", ":", "\n", "                ", "f", ".", "write", "(", "u'%d\\t%s\\n'", "%", "(", "i", ",", "str", "(", "pred", ")", ")", ")", "\n", "", "", "", "elif", "task", ".", "output_format", "==", "\"txt\"", ":", "\n", "        ", "with", "io", ".", "open", "(", "test_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "pred", "in", "result_data", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "pred", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "", "", "else", ":", "\n", "        ", "with", "io", ".", "open", "(", "test_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "if", "args", ".", "task_name", "==", "\"MultiRC\"", "or", "args", ".", "task_name", "==", "\"ReCoRD\"", ":", "\n", "                ", "for", "pred", "in", "result_data", ":", "\n", "                    ", "f", ".", "write", "(", "json", ".", "dumps", "(", "pred", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "i", ",", "pred", "in", "enumerate", "(", "results", ")", ":", "\n", "                    ", "f", ".", "write", "(", "u'{\"idx\":%d,\"label\":\"%s\"}\\n'", "%", "(", "i", ",", "str", "(", "pred", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.None.run_pretraining_distillation_hvd.train": [[74, 259], ["model.collect_params", "gluonnlp.metric.MaskedAccuracy", "gluonnlp.metric.MaskedAccuracy.reset", "logging.debug", "hvd.DistributedTrainer", "int", "model.collect_params", "model.collect_params().items", "time.time", "time.time", "logging.debug", "tqdm.tqdm", "mxnet.nd.waitall", "time.time", "tqdm.tqdm.close", "logging.info", "hvd.broadcast_parameters", "utils.fp16_utils.FP16Trainer", "os.path.join", "os.path.join", "logging.info", "gluonnlp.utils.load_states", "enumerate", "tqdm.tqdm.update", "os.path.join", "utils.pretraining_distillation_utils.save_states", "utils.fp16_utils.FP16Trainer.step", "hvd.DistributedTrainer.step", "model.collect_params().values", "model.collect_params", "sys.stdout.flush", "mlm_loss_val.as_in_context", "teacher_ce_loss_val.as_in_context", "teacher_mse_loss_val.as_in_context", "valid_len.sum().as_in_context", "gluonnlp.metric.MaskedAccuracy.update", "os.path.isdir", "os.mkdir", "utils.pretraining_distillation_utils.save_parameters", "hvd.DistributedTrainer.set_learning_rate", "list", "mxnet.autograd.record", "utils.pretraining_distillation_utils.forward", "mxnet.cpu", "mxnet.cpu", "mxnet.cpu", "mxnet.cpu", "trainer_step", "utils.pretraining_distillation_utils.log", "time.time", "gluonnlp.metric.MaskedAccuracy.reset_local", "model.collect_params", "model.collect_params.zero_grad", "utils.pretraining_distillation_utils.profile", "utils.pretraining_distillation_utils.split_and_load", "utils.fp16_utils.FP16Trainer.backward", "loss_val.backward", "valid_len.sum", "os.path.join", "utils.pretraining_distillation_utils.save_states", "utils.pretraining_distillation_utils.get_pretrain_data_npz", "utils.pretraining_distillation_utils.evaluate", "zip", "os.path.isdir", "gluonnlp.utils.mkdir", "utils.pretraining_distillation_utils.save_parameters", "len", "s.as_in_context", "str"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_states", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.step", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.step", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_parameters", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.forward", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.log", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.profile", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.split_and_load", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_states", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.get_pretrain_data_npz", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.evaluate", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_parameters"], ["def", "train", "(", "data_train", ",", "dataset_eval", ",", "model", ",", "teacher_model", ",", "mlm_loss", ",", "teacher_ce_loss", ",", "teacher_mse_loss", ",", "\n", "vocab_size", ",", "ctx", ",", "teacher_ce_weight", ",", "distillation_temperature", ",", "mlm_weight", ",", "log_tb", ")", ":", "\n", "    ", "\"\"\"Training function.\"\"\"", "\n", "params", "=", "model", ".", "collect_params", "(", ")", "\n", "if", "params", "is", "not", "None", ":", "\n", "        ", "hvd", ".", "broadcast_parameters", "(", "params", ",", "root_rank", "=", "0", ")", "\n", "\n", "", "mlm_metric", "=", "MaskedAccuracy", "(", ")", "\n", "mlm_metric", ".", "reset", "(", ")", "\n", "\n", "logging", ".", "debug", "(", "'Creating distributed trainer...'", ")", "\n", "lr", "=", "args", ".", "lr", "\n", "optim_params", "=", "{", "'learning_rate'", ":", "lr", ",", "'epsilon'", ":", "1e-6", ",", "'wd'", ":", "0.01", "}", "\n", "if", "args", ".", "dtype", "==", "'float16'", ":", "\n", "        ", "optim_params", "[", "'multi_precision'", "]", "=", "True", "\n", "\n", "", "dynamic_loss_scale", "=", "args", ".", "dtype", "==", "'float16'", "\n", "if", "dynamic_loss_scale", ":", "\n", "        ", "loss_scale_param", "=", "{", "'scale_window'", ":", "2000", "/", "num_workers", "}", "\n", "", "else", ":", "\n", "        ", "loss_scale_param", "=", "None", "\n", "", "trainer", "=", "hvd", ".", "DistributedTrainer", "(", "params", ",", "'bertadam'", ",", "optim_params", ")", "\n", "\n", "if", "args", ".", "dtype", "==", "'float16'", ":", "\n", "        ", "fp16_trainer", "=", "FP16Trainer", "(", "trainer", ",", "dynamic_loss_scale", "=", "dynamic_loss_scale", ",", "\n", "loss_scaler_params", "=", "loss_scale_param", ")", "\n", "trainer_step", "=", "lambda", ":", "fp16_trainer", ".", "step", "(", "1", ",", "max_norm", "=", "1", "*", "num_workers", ")", "\n", "", "else", ":", "\n", "        ", "trainer_step", "=", "lambda", ":", "trainer", ".", "step", "(", "1", ")", "\n", "\n", "\n", "", "if", "args", ".", "start_step", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "ckpt_dir", ",", "f\"checkpoint_{args.start_step}\"", ")", "\n", "state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "out_dir", ",", "'%07d.states.%02d'", "%", "(", "args", ".", "start_step", ",", "local_rank", ")", ")", "\n", "logging", ".", "info", "(", "'Loading trainer state from %s'", ",", "state_path", ")", "\n", "nlp", ".", "utils", ".", "load_states", "(", "trainer", ",", "state_path", ")", "\n", "\n", "", "accumulate", "=", "args", ".", "accumulate", "\n", "num_train_steps", "=", "args", ".", "num_steps", "\n", "warmup_ratio", "=", "args", ".", "warmup_ratio", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "warmup_ratio", ")", "\n", "params", "=", "[", "p", "for", "p", "in", "model", ".", "collect_params", "(", ")", ".", "values", "(", ")", "\n", "if", "p", ".", "grad_req", "!=", "'null'", "]", "\n", "param_dict", "=", "model", ".", "collect_params", "(", ")", "\n", "\n", "# Do not apply weight decay on LayerNorm and bias terms", "\n", "for", "_", ",", "v", "in", "model", ".", "collect_params", "(", "'.*beta|.*gamma|.*bias'", ")", ".", "items", "(", ")", ":", "\n", "        ", "v", ".", "wd_mult", "=", "0.0", "\n", "", "if", "accumulate", ">", "1", ":", "\n", "        ", "for", "p", "in", "params", ":", "\n", "            ", "p", ".", "grad_req", "=", "'add'", "\n", "\n", "", "", "train_begin_time", "=", "time", ".", "time", "(", ")", "\n", "begin_time", "=", "time", ".", "time", "(", ")", "\n", "running_mlm_loss", ",", "running_teacher_ce_loss", ",", "running_teacher_mse_loss", "=", "0", ",", "0", ",", "0", "\n", "running_num_tks", "=", "0", "\n", "batch_num", "=", "0", "\n", "step_num", "=", "args", ".", "start_step", "\n", "\n", "logging", ".", "debug", "(", "'Training started'", ")", "\n", "\n", "pbar", "=", "tqdm", "(", "total", "=", "num_train_steps", ",", "desc", "=", "\"Training:\"", ")", "\n", "\n", "while", "step_num", "<", "num_train_steps", ":", "\n", "        ", "for", "raw_batch_num", ",", "data_batch", "in", "enumerate", "(", "data_train", ")", ":", "\n", "            ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "step_num", ">=", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "if", "batch_num", "%", "accumulate", "==", "0", ":", "\n", "                ", "step_num", "+=", "1", "\n", "# if accumulate > 1, grad_req is set to 'add', and zero_grad is", "\n", "# required", "\n", "if", "accumulate", ">", "1", ":", "\n", "                    ", "param_dict", ".", "zero_grad", "(", ")", "\n", "# update learning rate", "\n", "", "if", "step_num", "<=", "num_warmup_steps", ":", "\n", "                    ", "new_lr", "=", "lr", "*", "step_num", "/", "num_warmup_steps", "\n", "", "else", ":", "\n", "                    ", "offset", "=", "lr", "*", "step_num", "/", "num_train_steps", "\n", "new_lr", "=", "lr", "-", "offset", "\n", "", "trainer", ".", "set_learning_rate", "(", "new_lr", ")", "\n", "if", "args", ".", "profile", ":", "\n", "                    ", "profile", "(", "step_num", ",", "10", ",", "14", ",", "\n", "profile_name", "=", "args", ".", "profile", "+", "str", "(", "rank", ")", ")", "\n", "\n", "# load data", "\n", "", "", "if", "args", ".", "use_avg_len", ":", "\n", "                ", "data_list", "=", "[", "[", "[", "s", ".", "as_in_context", "(", "context", ")", "for", "s", "in", "seq", "]", "for", "seq", "in", "shard", "]", "\n", "for", "context", ",", "shard", "in", "zip", "(", "[", "ctx", "]", ",", "data_batch", ")", "]", "\n", "", "else", ":", "\n", "                ", "data_list", "=", "list", "(", "split_and_load", "(", "data_batch", ",", "[", "ctx", "]", ")", ")", "\n", "#data = data_list[0]", "\n", "", "data", "=", "data_list", "\n", "\n", "# forward", "\n", "with", "mx", ".", "autograd", ".", "record", "(", ")", ":", "\n", "                ", "(", "loss_val", ",", "ns_label", ",", "classified", ",", "masked_id", ",", "decoded", ",", "masked_weight", ",", "mlm_loss_val", ",", "teacher_ce_loss_val", ",", "\n", "teacher_mse_loss_val", ",", "valid_len", ")", "=", "forward", "(", "data", ",", "model", ",", "mlm_loss", ",", "vocab_size", ",", "\n", "args", ".", "dtype", ",", "\n", "mlm_weight", "=", "mlm_weight", ",", "\n", "teacher_ce_loss", "=", "teacher_ce_loss", ",", "\n", "teacher_mse_loss", "=", "teacher_mse_loss", ",", "\n", "teacher_model", "=", "teacher_model", ",", "\n", "teacher_ce_weight", "=", "teacher_ce_weight", ",", "\n", "distillation_temperature", "=", "distillation_temperature", ")", "\n", "loss_val", "=", "loss_val", "/", "accumulate", "\n", "# backward", "\n", "if", "args", ".", "dtype", "==", "'float16'", ":", "\n", "                    ", "fp16_trainer", ".", "backward", "(", "loss_val", ")", "\n", "", "else", ":", "\n", "                    ", "loss_val", ".", "backward", "(", ")", "\n", "\n", "", "", "running_mlm_loss", "+=", "mlm_loss_val", ".", "as_in_context", "(", "mx", ".", "cpu", "(", ")", ")", "\n", "running_teacher_ce_loss", "+=", "teacher_ce_loss_val", ".", "as_in_context", "(", "\n", "mx", ".", "cpu", "(", ")", ")", "\n", "running_teacher_mse_loss", "+=", "teacher_mse_loss_val", ".", "as_in_context", "(", "\n", "mx", ".", "cpu", "(", ")", ")", "\n", "running_num_tks", "+=", "valid_len", ".", "sum", "(", ")", ".", "as_in_context", "(", "mx", ".", "cpu", "(", ")", ")", "\n", "\n", "# update", "\n", "if", "(", "batch_num", "+", "1", ")", "%", "accumulate", "==", "0", ":", "\n", "# step() performs 3 things:", "\n", "# 1. allreduce gradients from all workers", "\n", "# 2. checking the global_norm of gradients and clip them if necessary", "\n", "# 3. averaging the gradients and apply updates", "\n", "                ", "trainer_step", "(", ")", "\n", "\n", "", "mlm_metric", ".", "update", "(", "[", "masked_id", "]", ",", "[", "decoded", "]", ",", "[", "masked_weight", "]", ")", "\n", "\n", "# logging", "\n", "if", "step_num", "%", "args", ".", "log_interval", "==", "0", "and", "batch_num", "%", "accumulate", "==", "0", ":", "\n", "                ", "log", "(", "\"train \"", ",", "\n", "begin_time", ",", "\n", "running_num_tks", ",", "\n", "running_mlm_loss", "/", "accumulate", ",", "\n", "running_teacher_ce_loss", "/", "accumulate", ",", "\n", "running_teacher_mse_loss", "/", "accumulate", ",", "\n", "step_num", ",", "\n", "mlm_metric", ",", "\n", "trainer", ",", "\n", "args", ".", "log_interval", ",", "\n", "model", "=", "model", ",", "\n", "log_tb", "=", "log_tb", ",", "\n", "is_master_node", "=", "is_master_node", ")", "\n", "begin_time", "=", "time", ".", "time", "(", ")", "\n", "running_mlm_loss", "=", "running_teacher_ce_loss", "=", "running_teacher_mse_loss", "=", "running_num_tks", "=", "0", "\n", "mlm_metric", ".", "reset_local", "(", ")", "\n", "\n", "# saving checkpoints", "\n", "", "if", "step_num", "%", "args", ".", "ckpt_interval", "==", "0", "and", "batch_num", "%", "accumulate", "==", "0", ":", "\n", "                ", "if", "is_master_node", ":", "\n", "                    ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "ckpt_dir", ",", "f\"checkpoint_{step_num}\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "                        ", "nlp", ".", "utils", ".", "mkdir", "(", "out_dir", ")", "\n", "", "save_states", "(", "step_num", ",", "trainer", ",", "out_dir", ",", "local_rank", ")", "\n", "if", "local_rank", "==", "0", ":", "\n", "                        ", "save_parameters", "(", "step_num", ",", "model", ",", "out_dir", ")", "\n", "", "", "if", "data_eval", ":", "\n", "                    ", "dataset_eval", "=", "get_pretrain_data_npz", "(", "\n", "data_eval", ",", "args", ".", "batch_size_eval", ",", "1", ",", "False", ",", "False", ",", "1", ")", "\n", "evaluate", "(", "dataset_eval", ",", "model", ",", "mlm_loss", ",", "len", "(", "vocab", ")", ",", "[", "ctx", "]", ",", "args", ".", "log_interval", ",", "args", ".", "dtype", ",", "\n", "mlm_weight", "=", "mlm_weight", ",", "\n", "teacher_ce_loss", "=", "teacher_ce_loss", ",", "\n", "teacher_mse_loss", "=", "teacher_mse_loss", ",", "\n", "teacher_model", "=", "teacher_model", ",", "\n", "teacher_ce_weight", "=", "teacher_ce_weight", ",", "\n", "distillation_temperature", "=", "distillation_temperature", ",", "\n", "log_tb", "=", "log_tb", ")", "\n", "\n", "", "", "batch_num", "+=", "1", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "del", "data_batch", "\n", "", "if", "is_master_node", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "ckpt_dir", ",", "\"checkpoint_last\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "out_dir", ")", "\n", "", "save_states", "(", "step_num", ",", "trainer", ",", "out_dir", ",", "local_rank", ")", "\n", "if", "local_rank", "==", "0", ":", "\n", "            ", "save_parameters", "(", "step_num", ",", "model", ",", "args", ".", "ckpt_dir", ")", "\n", "", "", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "train_end_time", "=", "time", ".", "time", "(", ")", "\n", "pbar", ".", "close", "(", ")", "\n", "logging", ".", "info", "(", "'Train cost={:.1f}s'", ".", "format", "(", "\n", "train_end_time", "-", "train_begin_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.metrics.AvgF1.__init__": [[25, 27], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "AvgF1", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.metrics.AvgF1.__call__": [[28, 30], ["sklearn.metrics.f1_score"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "label", ",", "preds", ")", ":", "\n", "        ", "return", "sk_f1_score", "(", "label", ",", "preds", ",", "average", "=", "\"macro\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.metrics.GP.__init__": [[37, 39], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "GP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.metrics.GP.__call__": [[40, 61], ["range", "int", "float", "float", "len", "str", "str"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "data", ",", "preds", ")", ":", "\n", "        ", "\"\"\"\n        Calculate gender parity. \n        data: [{hypothesis, context, pair_id, preds}]\n        preds:unused\n        \"\"\"", "\n", "same_preds", ",", "dif_preds", "=", "0", ",", "0", "\n", "for", "idx", "in", "range", "(", "int", "(", "len", "(", "data", ")", "/", "2", ")", ")", ":", "\n", "            ", "pred1", "=", "data", "[", "idx", "*", "2", "]", "\n", "pred2", "=", "data", "[", "(", "idx", "*", "2", ")", "+", "1", "]", "\n", "assert", "(", "\n", "pred1", "[", "\"hypothesis\"", "]", "==", "pred2", "[", "\"hypothesis\"", "]", "\n", ")", ",", "\"Mismatched hypotheses for ids %s and %s\"", "%", "(", "str", "(", "pred1", "[", "\"idx\"", "]", ")", ",", "str", "(", "pred2", "[", "\"idx\"", "]", ")", ")", "\n", "if", "pred1", "[", "\"preds\"", "]", "==", "pred2", "[", "\"preds\"", "]", ":", "\n", "                ", "same_preds", "+=", "1", "\n", "", "else", ":", "\n", "                ", "dif_preds", "+=", "1", "\n", "\n", "", "", "if", "same_preds", "+", "dif_preds", "==", "0", ":", "\n", "            ", "return", "-", "1", "\n", "", "return", "float", "(", "same_preds", ")", "/", "float", "(", "same_preds", "+", "dif_preds", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTPretrainDataset.__init__": [[169, 181], ["logging.debug", "create_pretraining_data.create_training_instances", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_training_instances", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "filename", ",", "tokenizer", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "whole_word_mask", ",", "\n", "vocab", ",", "num_workers", "=", "1", ",", "worker_pool", "=", "None", ")", ":", "\n", "        ", "logging", ".", "debug", "(", "'start to load file %s ...'", ",", "filename", ")", "\n", "dupe_factor", "=", "1", "\n", "instances", "=", "create_training_instances", "(", "(", "[", "filename", "]", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "short_seq_prob", ",", "masked_lm_prob", ",", "\n", "max_predictions_per_seq", ",", "\n", "whole_word_mask", ",", "vocab", ",", "\n", "dupe_factor", ",", "num_workers", ",", "\n", "worker_pool", ",", "None", ")", ")", "\n", "super", "(", "BERTPretrainDataset", ",", "self", ")", ".", "__init__", "(", "*", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTSamplerFn.__init__": [[186, 192], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "use_avg_len", ",", "batch_size", ",", "shuffle", ",", "num_ctxes", ",", "num_buckets", ")", ":", "\n", "        ", "self", ".", "_use_avg_len", "=", "use_avg_len", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_shuffle", "=", "shuffle", "\n", "self", ".", "_num_ctxes", "=", "num_ctxes", "\n", "self", ".", "_num_buckets", "=", "num_buckets", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTSamplerFn.__call__": [[193, 223], ["isinstance", "logging.debug", "dataset.get_field", "isinstance", "gluonnlp.data.FixedBucketSampler", "gluonnlp.data.FixedBucketSampler", "gluonnlp.data.FixedBucketSampler.stats", "dataset.transform", "ValueError", "str"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.transform"], ["", "def", "__call__", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "\"\"\"Create data sampler based on the dataset\"\"\"", "\n", "if", "isinstance", "(", "dataset", ",", "nlp", ".", "data", ".", "NumpyDataset", ")", ":", "\n", "            ", "lengths", "=", "dataset", ".", "get_field", "(", "'valid_lengths'", ")", "\n", "", "elif", "isinstance", "(", "dataset", ",", "BERTPretrainDataset", ")", ":", "\n", "            ", "lengths", "=", "dataset", ".", "transform", "(", "lambda", "input_ids", ",", "segment_ids", ",", "masked_lm_positions", ",", "\n", "masked_lm_ids", ",", "masked_lm_weights", ",", "\n", "next_sentence_labels", ",", "valid_lengths", ":", "\n", "valid_lengths", ",", "lazy", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'unexpected dataset type: %s'", "%", "str", "(", "dataset", ")", ")", "\n", "\n", "", "if", "self", ".", "_use_avg_len", ":", "\n", "# sharded data loader", "\n", "            ", "sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", "=", "lengths", ",", "\n", "# batch_size per shard", "\n", "batch_size", "=", "self", ".", "_batch_size", ",", "\n", "num_buckets", "=", "self", ".", "_num_buckets", ",", "\n", "shuffle", "=", "self", ".", "_shuffle", ",", "\n", "use_average_length", "=", "True", ",", "\n", "num_shards", "=", "self", ".", "_num_ctxes", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", ",", "\n", "batch_size", "=", "self", ".", "_batch_size", "*", "self", ".", "_num_ctxes", ",", "\n", "num_buckets", "=", "self", ".", "_num_buckets", ",", "\n", "ratio", "=", "0", ",", "\n", "shuffle", "=", "self", ".", "_shuffle", ")", "\n", "", "logging", ".", "debug", "(", "'Sampler created for a new dataset:\\n%s'", ",", "\n", "sampler", ".", "stats", "(", ")", ")", "\n", "return", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTDataLoaderFn.__init__": [[228, 231], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "use_avg_len", ",", "num_ctxes", ")", ":", "\n", "        ", "self", ".", "_use_avg_len", "=", "use_avg_len", "\n", "self", ".", "_num_ctxes", "=", "num_ctxes", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTDataLoaderFn.__call__": [[232, 255], ["gluonnlp.data.batchify.Tuple", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "gluonnlp.data.ShardedDataLoader", "mxnet.gluon.data.DataLoader"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "dataset", ",", "sampler", ")", ":", "\n", "# A batch includes: input_id, masked_id, masked_position, masked_weight,", "\n", "#                   next_sentence_label, segment_id, valid_length", "\n", "        ", "batchify_fn", "=", "Tuple", "(", "Pad", "(", ")", ",", "# input_id", "\n", "Pad", "(", ")", ",", "# masked_id", "\n", "Pad", "(", ")", ",", "# masked_position", "\n", "Pad", "(", ")", ",", "# masked_weight", "\n", "Stack", "(", ")", ",", "# next_sentence_label", "\n", "Pad", "(", ")", ",", "# segment_id", "\n", "Stack", "(", ")", ")", "# valid_length", "\n", "\n", "if", "self", ".", "_use_avg_len", ":", "\n", "# sharded data loader", "\n", "            ", "dataloader", "=", "nlp", ".", "data", ".", "ShardedDataLoader", "(", "dataset", ",", "\n", "batch_sampler", "=", "sampler", ",", "\n", "batchify_fn", "=", "batchify_fn", ",", "\n", "num_workers", "=", "self", ".", "_num_ctxes", ")", "\n", "", "else", ":", "\n", "            ", "dataloader", "=", "DataLoader", "(", "dataset", "=", "dataset", ",", "\n", "batch_sampler", "=", "sampler", ",", "\n", "batchify_fn", "=", "batchify_fn", ",", "\n", "num_workers", "=", "self", ".", "_num_ctxes", ")", "\n", "", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTLoaderTransform.__init__": [[260, 264], ["pretraining_distillation_utils.BERTSamplerFn", "pretraining_distillation_utils.BERTDataLoaderFn"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "use_avg_len", ",", "batch_size", ",", "shuffle", ",", "num_ctxes", ",", "num_buckets", ")", ":", "\n", "        ", "self", ".", "_sampler_fn", "=", "BERTSamplerFn", "(", "\n", "use_avg_len", ",", "batch_size", ",", "shuffle", ",", "num_ctxes", ",", "num_buckets", ")", "\n", "self", ".", "_data_fn", "=", "BERTDataLoaderFn", "(", "use_avg_len", ",", "num_ctxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.BERTLoaderTransform.__call__": [[265, 270], ["pretraining_distillation_utils.BERTLoaderTransform._sampler_fn", "pretraining_distillation_utils.BERTLoaderTransform._data_fn"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "\"\"\"create data loader based on the dataset chunk\"\"\"", "\n", "sampler", "=", "self", ".", "_sampler_fn", "(", "dataset", ")", "\n", "dataloader", "=", "self", ".", "_data_fn", "(", "dataset", ",", "sampler", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.LogTB.__init__": [[315, 320], ["print", "mxboard.SummaryWriter", "pretraining_distillation_utils.LogTB.tb.add_text", "os.path.join", "str"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "print", "(", "'--- Initializing Tensorboard'", ")", "\n", "self", ".", "tb", "=", "SummaryWriter", "(", "logdir", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "ckpt_dir", ",", "'log'", ",", "'train'", ")", ")", "\n", "self", ".", "tb", ".", "add_text", "(", "tag", "=", "'config'", ",", "text", "=", "str", "(", "args", ")", ",", "global_step", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.LogTB.log": [[321, 346], ["logging.info", "context_str.strip.strip.strip", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "pretraining_distillation_utils.LogTB.tb.add_scalar", "psutil.virtual_memory()._asdict", "psutil.virtual_memory"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "\n", "student", ",", "\n", "context_str", ",", "\n", "mlm_loss", ",", "\n", "mlm_acc", ",", "\n", "teacher_ce", ",", "\n", "teacher_mse", ",", "\n", "throughput", ",", "\n", "lr", ",", "\n", "duration", ",", "\n", "latency", ",", "\n", "n_total_iter", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"{context_str}loggging to Tensorboard at {n_total_iter}\"", ")", "\n", "context_str", "=", "context_str", ".", "strip", "(", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/losses/mlm_loss\"", ",", "value", "=", "mlm_loss", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/losses/mlm_acc\"", ",", "value", "=", "mlm_acc", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/losses/teacher_ce\"", ",", "value", "=", "teacher_ce", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/losses/teacher_mse\"", ",", "value", "=", "teacher_mse", ",", "global_step", "=", "n_total_iter", ")", "\n", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/latency/throughput\"", ",", "value", "=", "throughput", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/latency/duration\"", ",", "value", "=", "duration", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/latency/latency\"", ",", "value", "=", "latency", ",", "global_step", "=", "n_total_iter", ")", "\n", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/learning_rate/lr\"", ",", "value", "=", "lr", ",", "global_step", "=", "n_total_iter", ")", "\n", "self", ".", "tb", ".", "add_scalar", "(", "tag", "=", "f\"{context_str}/global/memory_usage\"", ",", "value", "=", "psutil", ".", "virtual_memory", "(", ")", ".", "_asdict", "(", ")", "[", "'used'", "]", "/", "1_000_000", ",", "global_step", "=", "n_total_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.get_teacher_model_loss": [[42, 91], ["gluonnlp.model.get_model", "model.cast", "model.hybridize", "mxnet.gluon.loss.KLDivLoss", "mx.gluon.loss.KLDivLoss.hybridize", "mxnet.gluon.loss.L2Loss", "mx.gluon.loss.L2Loss.hybridize", "os.path.join", "gluonnlp.utils.load_parameters", "logging.info", "os.listdir", "len"], "function", ["None"], ["def", "get_teacher_model_loss", "(", "ctx", ",", "model", ",", "dataset_name", ",", "vocab", ",", "dtype", ",", "\n", "ckpt_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get model for pre-training.\n\n    Parameters\n    ----------\n    ctx : Context or list of Context\n        Contexts to initialize model\n    model : str\n        The name of the model.\n    dataset_name : str\n        The name of the dataset, which is used to retrieve the corresponding vocabulary file\n        when the vocab argument is not provided. Options include 'book_corpus_wiki_en_uncased',\n        'book_corpus_wiki_en_cased', 'wiki_multilingual_uncased', 'wiki_multilingual_cased',\n        'wiki_cn_cased'.\n    vocab : The vocabulary for the model. If not provided, The vocabulary will be constructed\n        based on dataset_name.\n    dtype : float\n        Data type of the model for training.\n    ckpt_dir : str\n        The path to the checkpoint directory.\n\n    Returns\n    -------\n    RoBERTaModel : the model for pre-training.\n    Loss : the next sentence prediction loss.\n    Loss : the masked langauge model loss.\n    RoBERTaVocab : the vocabulary.\n    \"\"\"", "\n", "download_pretrained", "=", "ckpt_dir", "is", "None", "\n", "model", ",", "vocabulary", "=", "nlp", ".", "model", ".", "get_model", "(", "model", ",", "dataset_name", "=", "dataset_name", ",", "vocab", "=", "vocab", ",", "\n", "pretrained", "=", "download_pretrained", ",", "ctx", "=", "ctx", ")", "\n", "model", ".", "cast", "(", "dtype", ")", "\n", "if", "ckpt_dir", ":", "\n", "        ", "params_fn", "=", "[", "fn", "for", "fn", "in", "os", ".", "listdir", "(", "\n", "ckpt_dir", ")", "if", "fn", "[", "-", "len", "(", "\".params\"", ")", ":", "]", "==", "\".params\"", "]", "[", "0", "]", "\n", "param_path", "=", "os", ".", "path", ".", "join", "(", "ckpt_dir", ",", "params_fn", ")", "\n", "nlp", ".", "utils", ".", "load_parameters", "(", "\n", "model", ",", "param_path", ",", "ctx", "=", "ctx", ",", "allow_missing", "=", "True", ")", "\n", "logging", ".", "info", "(", "'Loading checkpoints from %s.'", ",", "param_path", ")", "\n", "", "model", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "\n", "# losses", "\n", "teacher_ce_loss", "=", "mx", ".", "gluon", ".", "loss", ".", "KLDivLoss", "(", "from_logits", "=", "False", ")", "\n", "teacher_ce_loss", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "teacher_mse_loss", "=", "mx", ".", "gluon", ".", "loss", ".", "L2Loss", "(", ")", "\n", "teacher_mse_loss", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "\n", "return", "model", ",", "teacher_ce_loss", ",", "teacher_mse_loss", ",", "vocabulary", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.get_model_loss": [[93, 137], ["model.cast", "model.hybridize", "mxnet.gluon.loss.SoftmaxCELoss", "mx.gluon.loss.SoftmaxCELoss.hybridize", "model.initialize", "os.path.join", "os.path.join", "gluonnlp.utils.load_parameters", "logging.info", "mxnet.init.Normal"], "function", ["None"], ["", "def", "get_model_loss", "(", "ctx", ",", "model", ",", "pretrained", ",", "dtype", ",", "ckpt_dir", "=", "None", ",", "\n", "start_step", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get model for pre-training.\n\n    Parameters\n    ----------\n    ctx : Context or list of Context\n        Contexts to initialize model\n    model : BortModel\n        The model\n    pretrained : bool\n        Whether to use pre-trained model weights as initialization.\n    dtype : float\n        Data type of the model for training.\n    ckpt_dir : str\n        The path to the checkpoint directory.\n    start_step : int or None\n        If provided, it loads the model from the corresponding checkpoint from the ckpt_dir.\n\n    Returns\n    -------\n    BortModel : the model for pre-training.\n    Loss : the next sentence prediction loss.\n    Loss : the masked langauge model loss.\n    \"\"\"", "\n", "if", "not", "pretrained", ":", "\n", "        ", "model", ".", "initialize", "(", "init", "=", "mx", ".", "init", ".", "Normal", "(", "0.02", ")", ",", "ctx", "=", "ctx", ")", "\n", "", "model", ".", "cast", "(", "dtype", ")", "\n", "\n", "if", "ckpt_dir", "and", "start_step", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "ckpt_dir", ",", "f\"checkpoint_{start_step}\"", ")", "\n", "param_path", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'%07d.params'", "%", "start_step", ")", "\n", "nlp", ".", "utils", ".", "load_parameters", "(", "\n", "model", ",", "param_path", ",", "ctx", "=", "ctx", ",", "allow_missing", "=", "True", ")", "\n", "logging", ".", "info", "(", "'Loading step %d checkpoints from %s.'", ",", "\n", "start_step", ",", "param_path", ")", "\n", "\n", "", "model", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "\n", "# losses", "\n", "mlm_loss", "=", "mx", ".", "gluon", ".", "loss", ".", "SoftmaxCELoss", "(", ")", "\n", "mlm_loss", ".", "hybridize", "(", "static_alloc", "=", "True", ")", "\n", "\n", "return", "model", ",", "mlm_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.get_pretrain_data_npz": [[272, 295], ["len", "logging.info", "gluonnlp.data.SplitSampler", "gluonnlp.data.SimpleDatasetStream", "gluonnlp.data.PrefetchingStream", "pretraining_distillation_utils.BERTLoaderTransform", "stream.transform.transform", "gluonnlp.utils.glob"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.transform"], ["", "", "def", "get_pretrain_data_npz", "(", "data", ",", "batch_size", ",", "num_ctxes", ",", "shuffle", ",", "use_avg_len", ",", "\n", "num_buckets", ",", "num_parts", "=", "1", ",", "part_idx", "=", "0", ")", ":", "\n", "    ", "\"\"\"create dataset for pretraining based on pre-processed npz files.\"\"\"", "\n", "# handle commas in the provided path", "\n", "num_files", "=", "len", "(", "nlp", ".", "utils", ".", "glob", "(", "data", ")", ")", "\n", "logging", ".", "info", "(", "'%d files found.'", ",", "num_files", ")", "\n", "assert", "num_files", ">=", "num_parts", ",", "'Number of training files must be greater than the number of partitions. '", "'Only found %d files at %s'", "%", "(", "num_files", ",", "data", ")", "\n", "split_sampler", "=", "nlp", ".", "data", ".", "SplitSampler", "(", "\n", "num_files", ",", "num_parts", "=", "num_parts", ",", "part_index", "=", "part_idx", ")", "\n", "# read each file in as a separate NumpyDataset, split sample into buckets", "\n", "# for each gpu", "\n", "stream", "=", "nlp", ".", "data", ".", "SimpleDatasetStream", "(", "\n", "nlp", ".", "data", ".", "NumpyDataset", ",", "data", ",", "split_sampler", ",", "allow_pickle", "=", "True", ")", "\n", "stream", "=", "nlp", ".", "data", ".", "PrefetchingStream", "(", "stream", ",", "worker_type", "=", "'process'", ")", "\n", "\n", "# create data loader based on the dataset", "\n", "dataloader_xform", "=", "BERTLoaderTransform", "(", "use_avg_len", ",", "batch_size", ",", "\n", "shuffle", ",", "num_ctxes", ",", "num_buckets", ")", "\n", "# transform each NumpyDataset", "\n", "stream", "=", "stream", ".", "transform", "(", "dataloader_xform", ")", "\n", "return", "stream", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_parameters": [[297, 302], ["os.path.join", "logging.info", "gluonnlp.utils.save_parameters"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_parameters"], ["", "def", "save_parameters", "(", "step_num", ",", "model", ",", "ckpt_dir", ")", ":", "\n", "    ", "\"\"\"Save the model parameter, marked by step_num.\"\"\"", "\n", "param_path", "=", "os", ".", "path", ".", "join", "(", "ckpt_dir", ",", "'%07d.params'", "%", "step_num", ")", "\n", "logging", ".", "info", "(", "'[step %d] Saving model params to %s.'", ",", "step_num", ",", "param_path", ")", "\n", "nlp", ".", "utils", ".", "save_parameters", "(", "model", ",", "param_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_states": [[304, 311], ["os.path.join", "logging.info", "gluonnlp.utils.save_states"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.save_states"], ["", "def", "save_states", "(", "step_num", ",", "trainer", ",", "ckpt_dir", ",", "local_rank", "=", "0", ")", ":", "\n", "    ", "\"\"\"Save the trainer states, marked by step_num.\"\"\"", "\n", "trainer_path", "=", "os", ".", "path", ".", "join", "(", "\n", "ckpt_dir", ",", "'%07d.states.%02d'", "%", "(", "step_num", ",", "local_rank", ")", ")", "\n", "logging", ".", "info", "(", "'[step %d] Saving trainer states to %s.'", ",", "\n", "step_num", ",", "trainer_path", ")", "\n", "nlp", ".", "utils", ".", "save_states", "(", "trainer", ",", "trainer_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.log": [[348, 394], ["time.time", "logging.info", "log_tb.log", "running_mlm_loss.asscalar", "running_teacher_ce_loss.asscalar", "running_teacher_mse_loss.asscalar", "throughput.asscalar", "running_mlm_loss.asscalar", "running_teacher_ce_loss.asscalar", "running_teacher_mse_loss.asscalar", "throughput.asscalar", "logging.info", "mlm_metric.get", "mlm_metric.get"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.log"], ["", "", "def", "log", "(", "context_str", ",", "\n", "begin_time", ",", "\n", "running_num_tks", ",", "\n", "running_mlm_loss", ",", "\n", "running_teacher_ce_loss", ",", "\n", "running_teacher_mse_loss", ",", "\n", "step_num", ",", "\n", "mlm_metric", ",", "\n", "trainer", ",", "\n", "log_interval", ",", "\n", "model", "=", "None", ",", "\n", "log_tb", "=", "None", ",", "\n", "is_master_node", "=", "True", ")", ":", "\n", "    ", "\"\"\"Log training progress.\"\"\"", "\n", "end_time", "=", "time", ".", "time", "(", ")", "\n", "duration", "=", "end_time", "-", "begin_time", "\n", "throughput", "=", "running_num_tks", "/", "duration", "/", "1000.0", "\n", "running_mlm_loss", "=", "running_mlm_loss", "/", "log_interval", "\n", "lr", "=", "trainer", ".", "learning_rate", "if", "trainer", "else", "-", "1", "\n", "# pylint: disable=line-too-long", "\n", "logging", ".", "info", "(", "'{}[step {}]\\tmlm_loss={:7.5f}\\tmlm_acc={:4.2f}\\tteacher_ce={:5.2e}'", "\n", "'\\tteacher_mse={:5.2e}\\tthroughput={:.1f}K tks/s\\tlr={:5.2e} time={:.2f}, latency={:.1f} ms/batch'", "\n", ".", "format", "(", "context_str", ",", "\n", "step_num", ",", "\n", "running_mlm_loss", ".", "asscalar", "(", ")", ",", "\n", "mlm_metric", ".", "get", "(", ")", "[", "1", "]", "*", "100", ",", "\n", "running_teacher_ce_loss", ".", "asscalar", "(", ")", ",", "\n", "running_teacher_mse_loss", ".", "asscalar", "(", ")", ",", "\n", "throughput", ".", "asscalar", "(", ")", ",", "\n", "lr", ",", "\n", "duration", ",", "\n", "duration", "*", "1000", "/", "log_interval", ")", ")", "\n", "if", "model", "and", "log_tb", ":", "# and is_master_node:", "\n", "        ", "log_tb", ".", "log", "(", "model", ",", "\n", "context_str", ",", "\n", "running_mlm_loss", ".", "asscalar", "(", ")", ",", "\n", "mlm_metric", ".", "get", "(", ")", "[", "1", "]", "*", "100", ",", "\n", "running_teacher_ce_loss", ".", "asscalar", "(", ")", ",", "\n", "running_teacher_mse_loss", ".", "asscalar", "(", ")", ",", "\n", "throughput", ".", "asscalar", "(", ")", ",", "\n", "lr", ",", "\n", "duration", ",", "\n", "duration", "*", "1000", "/", "log_interval", ",", "\n", "step_num", ")", "\n", "", "elif", "is_master_node", ":", "\n", "        ", "logging", ".", "info", "(", "f\"no TB log: model: {model is None}, log_tb: {log_tb}, is_master_node: {is_master_node}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.split_and_load": [[396, 403], ["isinstance", "zip", "mxnet.gluon.utils.split_and_load"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.split_and_load"], ["", "", "def", "split_and_load", "(", "arrs", ",", "ctx", ")", ":", "\n", "    ", "\"\"\"split and load arrays to a list of contexts\"\"\"", "\n", "assert", "isinstance", "(", "arrs", ",", "(", "list", ",", "tuple", ")", ")", "\n", "# split and load", "\n", "loaded_arrs", "=", "[", "mx", ".", "gluon", ".", "utils", ".", "split_and_load", "(", "\n", "arr", ",", "ctx", ",", "even_split", "=", "False", ")", "for", "arr", "in", "arrs", "]", "\n", "return", "zip", "(", "*", "loaded_arrs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.forward": [[405, 452], ["valid_length.reshape.reshape", "masked_id.reshape.reshape", "valid_length.reshape.astype", "model", "decoded.reshape.reshape", "mlm_loss", "mxnet.nd.zeros().as_in_context", "mxnet.nd.zeros().as_in_context", "masked_weight.sum", "decoded.reshape.astype", "masked_weight.reshape", "mlm_loss.sum", "teacher_mse_loss", "teacher_ce_loss", "valid_length.reshape.astype", "mxnet.nd.zeros", "mxnet.nd.zeros", "mxnet.autograd.pause", "teacher_model", "teacher_decoded.reshape.reshape", "mxnet.nd.softmax", "mxnet.nd.softmax", "masked_weight.reshape", "mxnet.nd.softmax", "mxnet.nd.softmax", "masked_weight.reshape", "teacher_ce_loss.sum", "teacher_mse_loss.sum", "decoded.reshape.astype", "teacher_decoded.reshape.astype", "decoded.reshape.astype", "teacher_decoded.reshape.astype"], "function", ["None"], ["", "def", "forward", "(", "data", ",", "model", ",", "mlm_loss", ",", "vocab_size", ",", "dtype", ",", "is_eval", "=", "False", ",", "teacher_ce_loss", "=", "None", ",", "\n", "mlm_weight", "=", "1.0", ",", "teacher_mse_loss", "=", "None", ",", "teacher_model", "=", "None", ",", "teacher_ce_weight", "=", "0.0", ",", "distillation_temperature", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"forward computation for evaluation\"\"\"", "\n", "\n", "if", "is_eval", ":", "\n", "        ", "data", "=", "data", "\n", "", "else", ":", "\n", "        ", "data", "=", "data", "[", "0", "]", "[", "0", "]", "\n", "\n", "", "(", "input_id", ",", "masked_id", ",", "masked_position", ",", "masked_weight", ",", "\n", "next_sentence_label", ",", "segment_id", ",", "valid_length", ")", "=", "data", "\n", "num_masks", "=", "masked_weight", ".", "sum", "(", ")", "+", "1e-8", "\n", "valid_length", "=", "valid_length", ".", "reshape", "(", "-", "1", ")", "\n", "masked_id", "=", "masked_id", ".", "reshape", "(", "-", "1", ")", "\n", "valid_length_typed", "=", "valid_length", ".", "astype", "(", "dtype", ",", "copy", "=", "False", ")", "\n", "# segment_id", "\n", "classified", ",", "decoded", "=", "model", "(", "input_id", ",", "valid_length_typed", ",", "masked_position", ")", "\n", "decoded", "=", "decoded", ".", "reshape", "(", "(", "-", "1", ",", "vocab_size", ")", ")", "\n", "mlm_loss_val", "=", "mlm_loss", "(", "decoded", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", ",", "\n", "masked_id", ",", "masked_weight", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "# RoBERTa-style training", "\n", "mlm_loss_val", "=", "mlm_loss_val", ".", "sum", "(", ")", "/", "num_masks", "\n", "teacher_ce_val", "=", "mx", ".", "nd", ".", "zeros", "(", "(", "1", ",", ")", ")", ".", "as_in_context", "(", "mlm_loss_val", ".", "context", ")", "\n", "teacher_mse_val", "=", "mx", ".", "nd", ".", "zeros", "(", "(", "1", ",", ")", ")", ".", "as_in_context", "(", "mlm_loss_val", ".", "context", ")", "\n", "if", "teacher_model", ":", "\n", "        ", "with", "mx", ".", "autograd", ".", "pause", "(", ")", ":", "\n", "            ", "teacher_classified", ",", "teacher_decoded", "=", "teacher_model", "(", "\n", "input_id", ",", "valid_length_typed", ",", "masked_position", ")", "\n", "teacher_decoded", "=", "teacher_decoded", ".", "reshape", "(", "(", "-", "1", ",", "vocab_size", ")", ")", "\n", "", "teacher_mse_val", "=", "teacher_mse_loss", "(", "mx", ".", "nd", ".", "softmax", "(", "decoded", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", ",", "\n", "temperature", "=", "distillation_temperature", ")", ",", "\n", "mx", ".", "nd", ".", "softmax", "(", "teacher_decoded", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", ",", "\n", "temperature", "=", "distillation_temperature", ")", ",", "\n", "masked_weight", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "teacher_ce_val", "=", "teacher_ce_loss", "(", "mx", ".", "nd", ".", "softmax", "(", "decoded", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", ",", "\n", "temperature", "=", "distillation_temperature", ")", ",", "\n", "mx", ".", "nd", ".", "softmax", "(", "teacher_decoded", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", ",", "\n", "temperature", "=", "distillation_temperature", ")", ",", "\n", "masked_weight", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", ")", "\n", "\n", "", "teacher_ce_val", "=", "distillation_temperature", "**", "2", "*", "teacher_ce_val", ".", "sum", "(", ")", "/", "num_masks", "\n", "teacher_mse_val", "=", "distillation_temperature", "**", "2", "*", "teacher_mse_val", ".", "sum", "(", ")", "/", "num_masks", "\n", "loss_val", "=", "mlm_weight", "*", "mlm_loss_val", "+", "teacher_ce_weight", "*", "teacher_ce_val", "\n", "return", "loss_val", ",", "next_sentence_label", ",", "classified", ",", "masked_id", ",", "decoded", ",", "masked_weight", ",", "mlm_loss_val", ",", "teacher_ce_val", ",", "teacher_mse_val", ",", "valid_length", ".", "astype", "(", "\n", "'float32'", ",", "copy", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.evaluate": [[454, 537], ["logging.info", "gluonnlp.metric.MaskedAccuracy", "gluonnlp.metric.MaskedAccuracy.reset", "time.time", "time.time", "tqdm", "mxnet.nd.waitall", "time.time", "logging.info", "logging.info", "enumerate", "gluonnlp.metric.MaskedAccuracy.update", "pretraining_distillation_utils.forward", "loss_list.append", "ns_label_list.append", "ns_pred_list.append", "mask_label_list.append", "mask_pred_list.append", "mask_weight_list.append", "mlm_loss_val.as_in_context", "valid_length.sum().as_in_context", "teacher_ce_loss_val.as_in_context", "teacher_mse_loss_val.as_in_context", "pretraining_distillation_utils.log", "time.time", "gluonnlp.metric.MaskedAccuracy.reset_local", "total_mlm_loss.asscalar", "total_teacher_ce_loss.asscalar", "total_teacher_mse_loss.asscalar", "seq.as_in_context", "zip", "mxnet.cpu", "mxnet.cpu", "mxnet.cpu", "mxnet.cpu", "valid_length.sum", "gluonnlp.metric.MaskedAccuracy.get_global"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.forward", "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.log"], ["", "def", "evaluate", "(", "data_eval", ",", "model", ",", "mlm_loss", ",", "vocab_size", ",", "ctx", ",", "log_interval", ",", "dtype", ",", "\n", "mlm_weight", "=", "1.0", ",", "teacher_ce_loss", "=", "None", ",", "teacher_mse_loss", "=", "None", ",", "teacher_model", "=", "None", ",", "teacher_ce_weight", "=", "0.0", ",", "\n", "distillation_temperature", "=", "1.0", ",", "log_tb", "=", "None", ")", ":", "\n", "    ", "\"\"\"Evaluation function.\"\"\"", "\n", "logging", ".", "info", "(", "'Running evaluation ... '", ")", "\n", "mlm_metric", "=", "MaskedAccuracy", "(", ")", "\n", "mlm_metric", ".", "reset", "(", ")", "\n", "\n", "eval_begin_time", "=", "time", ".", "time", "(", ")", "\n", "begin_time", "=", "time", ".", "time", "(", ")", "\n", "step_num", "=", "0", "\n", "running_mlm_loss", "=", "0", "\n", "total_mlm_loss", "=", "0", "\n", "running_teacher_ce_loss", "=", "running_teacher_mse_loss", "=", "0", "\n", "total_teacher_ce_loss", "=", "total_teacher_mse_loss", "=", "0", "\n", "running_num_tks", "=", "0", "\n", "\n", "for", "_", ",", "dataloader", "in", "tqdm", "(", "enumerate", "(", "data_eval", ")", ",", "desc", "=", "\"Evaluation\"", ")", ":", "\n", "        ", "step_num", "+=", "1", "\n", "data_list", "=", "[", "[", "seq", ".", "as_in_context", "(", "context", ")", "for", "seq", "in", "shard", "]", "\n", "for", "context", ",", "shard", "in", "zip", "(", "ctx", ",", "dataloader", ")", "]", "\n", "loss_list", "=", "[", "]", "\n", "ns_label_list", ",", "ns_pred_list", "=", "[", "]", ",", "[", "]", "\n", "mask_label_list", ",", "mask_pred_list", ",", "mask_weight_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "data", "in", "data_list", ":", "\n", "            ", "out", "=", "forward", "(", "data", ",", "model", ",", "mlm_loss", ",", "vocab_size", ",", "dtype", ",", "is_eval", "=", "True", ",", "\n", "mlm_weight", "=", "mlm_weight", ",", "\n", "teacher_ce_loss", "=", "teacher_ce_loss", ",", "teacher_mse_loss", "=", "teacher_mse_loss", ",", "\n", "teacher_model", "=", "teacher_model", ",", "teacher_ce_weight", "=", "teacher_ce_weight", ",", "\n", "distillation_temperature", "=", "distillation_temperature", ")", "\n", "(", "loss_val", ",", "next_sentence_label", ",", "classified", ",", "masked_id", ",", "\n", "decoded", ",", "masked_weight", ",", "mlm_loss_val", ",", "teacher_ce_loss_val", ",", "teacher_mse_loss_val", ",", "valid_length", ")", "=", "out", "\n", "loss_list", ".", "append", "(", "loss_val", ")", "\n", "ns_label_list", ".", "append", "(", "next_sentence_label", ")", "\n", "ns_pred_list", ".", "append", "(", "classified", ")", "\n", "mask_label_list", ".", "append", "(", "masked_id", ")", "\n", "mask_pred_list", ".", "append", "(", "decoded", ")", "\n", "mask_weight_list", ".", "append", "(", "masked_weight", ")", "\n", "\n", "running_mlm_loss", "+=", "mlm_loss_val", ".", "as_in_context", "(", "mx", ".", "cpu", "(", ")", ")", "\n", "running_num_tks", "+=", "valid_length", ".", "sum", "(", ")", ".", "as_in_context", "(", "mx", ".", "cpu", "(", ")", ")", "\n", "running_teacher_ce_loss", "+=", "teacher_ce_loss_val", ".", "as_in_context", "(", "\n", "mx", ".", "cpu", "(", ")", ")", "\n", "running_teacher_mse_loss", "+=", "teacher_mse_loss_val", ".", "as_in_context", "(", "\n", "mx", ".", "cpu", "(", ")", ")", "\n", "", "mlm_metric", ".", "update", "(", "mask_label_list", ",", "mask_pred_list", ",", "mask_weight_list", ")", "\n", "\n", "# logging", "\n", "if", "(", "step_num", "+", "1", ")", "%", "(", "log_interval", ")", "==", "0", ":", "\n", "            ", "total_mlm_loss", "+=", "running_mlm_loss", "\n", "total_teacher_ce_loss", "+=", "running_teacher_ce_loss", "\n", "total_teacher_mse_loss", "+=", "running_teacher_mse_loss", "\n", "log", "(", "\"eval \"", ",", "\n", "begin_time", ",", "\n", "running_num_tks", ",", "\n", "running_mlm_loss", ",", "\n", "running_teacher_ce_loss", ",", "\n", "running_teacher_mse_loss", ",", "\n", "step_num", ",", "\n", "mlm_metric", ",", "\n", "None", ",", "\n", "log_interval", ",", "\n", "model", "=", "model", ",", "\n", "log_tb", "=", "log_tb", ")", "\n", "begin_time", "=", "time", ".", "time", "(", ")", "\n", "running_mlm_loss", "=", "running_num_tks", "=", "0", "\n", "running_teacher_ce_loss", "=", "running_teacher_mse_loss", "=", "0", "\n", "mlm_metric", ".", "reset_local", "(", ")", "\n", "\n", "", "", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "eval_end_time", "=", "time", ".", "time", "(", ")", "\n", "# accumulate losses from last few batches, too", "\n", "if", "running_mlm_loss", "!=", "0", ":", "\n", "        ", "total_mlm_loss", "+=", "running_mlm_loss", "\n", "total_teacher_ce_loss", "+=", "running_teacher_ce_loss", "\n", "total_teacher_mse_loss", "+=", "running_teacher_mse_loss", "\n", "", "total_mlm_loss", "/=", "step_num", "\n", "total_teacher_ce_loss", "/=", "step_num", "\n", "total_teacher_mse_loss", "/=", "step_num", "\n", "logging", ".", "info", "(", "'Eval mlm_loss={:.3f}\\tmlm_acc={:.1f}\\tteacher_ce={:.2e}\\tteacher_mse={:.2e}'", "\n", ".", "format", "(", "total_mlm_loss", ".", "asscalar", "(", ")", ",", "mlm_metric", ".", "get_global", "(", ")", "[", "1", "]", "*", "100", ",", "\n", "total_teacher_ce_loss", ".", "asscalar", "(", ")", ",", "total_teacher_mse_loss", ".", "asscalar", "(", ")", ")", ")", "\n", "logging", ".", "info", "(", "'Eval cost={:.1f}s'", ".", "format", "(", "eval_end_time", "-", "eval_begin_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.get_argparser": [[539, 605], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "bort.bort.predefined_borts.keys"], "function", ["None"], ["", "def", "get_argparser", "(", ")", ":", "\n", "    ", "\"\"\"Argument parser\"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Bort pretraining example.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_steps'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "'Number of optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_eval_steps'", ",", "type", "=", "int", ",", "\n", "default", "=", "None", ",", "help", "=", "'Number of eval steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_buckets'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of buckets for variable length sequence sampling'", ")", "\n", "parser", ".", "add_argument", "(", "'--dtype'", ",", "type", "=", "str", ",", "\n", "default", "=", "'float16'", ",", "help", "=", "'data dtype'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "\n", "default", "=", "8", ",", "help", "=", "'Batch size per GPU.'", ")", "\n", "parser", ".", "add_argument", "(", "'--accumulate'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of batches for gradient accumulation. '", "\n", "'The effective batch size = batch_size * accumulate.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_avg_len'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use average length information for the bucket sampler. '", "\n", "'The batch size is approximately the number of tokens in the batch'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size_eval'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Batch size per GPU for evaluation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset_name'", ",", "type", "=", "str", ",", "default", "=", "'book_corpus_wiki_en_uncased'", ",", "\n", "choices", "=", "[", "'book_corpus_wiki_en_uncased'", ",", "'book_corpus_wiki_en_cased'", ",", "\n", "'wiki_multilingual_uncased'", ",", "'wiki_multilingual_cased'", ",", "\n", "'wiki_cn_cased'", ",", "'openwebtext_ccnews_stories_books_cased'", "]", ",", "\n", "help", "=", "'The pre-defined dataset from which the vocabulary is created. '", "\n", "'Default is book_corpus_wiki_en_uncased.'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Load the pretrained model released by Google.'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "'bort_4_8_768_1024'", ",", "\n", "choices", "=", "[", "b", "for", "b", "in", "bort", ".", "predefined_borts", ".", "keys", "(", ")", "]", ",", "\n", "help", "=", "'Model to run pre-training on. '", ")", "\n", "parser", ".", "add_argument", "(", "'--teacher_model'", ",", "type", "=", "str", ",", "default", "=", "'roberta_24_1024_16'", ",", "\n", "help", "=", "'Model to run as teacher on. '", "\n", "'Options are bert_12_768_12, bert_24_1024_16, roberta_24_1024_16, roberta_12_768_12, '", "\n", "'others on https://gluon-nlp.mxnet.io/model_zoo/bert/index.html'", ")", "\n", "parser", ".", "add_argument", "(", "'--teacher_ckpt_dir'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to teacher checkpoint directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--teacher_ce_weight'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "'weight to mix teacher_ce_loss with '", "\n", "'mlm_loss: should be in range (0,1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--distillation_temperature'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'temperature for teacher/student '", "\n", "'distillation'", ")", "\n", "parser", ".", "add_argument", "(", "'--mlm_weight'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight to mix teacher_ce_loss with mlm_loss: '", "\n", "'should be in range (0,1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--data'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Path to training data. Training is skipped if not set.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_eval'", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "'Path to evaluation data. Evaluation is skipped if not set.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt_dir'", ",", "type", "=", "str", ",", "default", "=", "'./ckpt_dir'", ",", "\n", "help", "=", "'Path to checkpoint directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_step'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Start optimization step from the checkpoint.'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ",", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup_ratio'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'ratio of warmup steps used in NOAM\\'s stepsize schedule'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_interval'", ",", "type", "=", "int", ",", "\n", "default", "=", "250", ",", "help", "=", "'Report interval'", ")", "\n", "parser", ".", "add_argument", "(", "'--ckpt_interval'", ",", "type", "=", "int", ",", "\n", "default", "=", "1000", ",", "help", "=", "'Checkpoint interval'", ")", "\n", "parser", ".", "add_argument", "(", "'--verbose'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'verbose logging'", ")", "\n", "parser", ".", "add_argument", "(", "'--profile'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'output profiling result to the target file'", ")", "\n", "parser", ".", "add_argument", "(", "'--cpu_only'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'force to only use cpu'", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.generate_dev_set": [[607, 628], ["numpy.random.seed", "random.seed", "mxnet.random.seed", "multiprocessing.Pool", "gluonnlp.utils.glob", "len", "logging.info", "create_pretraining_data.create_training_instances", "logging.info", "len"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.create_training_instances"], ["", "def", "generate_dev_set", "(", "tokenizer", ",", "vocab", ",", "cache_file", ",", "args", ")", ":", "\n", "    ", "\"\"\"Generate validation set.\"\"\"", "\n", "# set random seed to generate dev data deterministically", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "mx", ".", "random", ".", "seed", "(", "0", ")", "\n", "worker_pool", "=", "multiprocessing", ".", "Pool", "(", ")", "\n", "eval_files", "=", "nlp", ".", "utils", ".", "glob", "(", "args", ".", "data_eval", ")", "\n", "num_files", "=", "len", "(", "eval_files", ")", "\n", "assert", "num_files", ">", "0", ",", "'Number of eval files must be greater than 0.'", "'Only found %d files at %s'", "%", "(", "\n", "num_files", ",", "args", ".", "data_eval", ")", "\n", "logging", ".", "info", "(", "\n", "'Generating validation set from %d files on rank 0.'", ",", "len", "(", "eval_files", ")", ")", "\n", "create_training_instances", "(", "(", "eval_files", ",", "tokenizer", ",", "args", ".", "max_seq_length", ",", "\n", "args", ".", "short_seq_prob", ",", "args", ".", "masked_lm_prob", ",", "\n", "args", ".", "max_predictions_per_seq", ",", "\n", "args", ".", "whole_word_mask", ",", "vocab", ",", "\n", "1", ",", "args", ".", "num_data_workers", ",", "\n", "worker_pool", ",", "cache_file", ")", ")", "\n", "logging", ".", "info", "(", "'Done generating validation set on rank 0.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.pretraining_distillation_utils.profile": [[630, 646], ["mxnet.nd.waitall", "mxnet.profiler.set_config", "mxnet.profiler.set_state", "mxnet.nd.waitall", "mxnet.profiler.set_state", "logging.info", "mxnet.profiler.dump", "mxnet.profiler.dumps", "exit"], "function", ["None"], ["", "def", "profile", "(", "curr_step", ",", "start_step", ",", "end_step", ",", "profile_name", "=", "'profile.json'", ",", "\n", "early_exit", "=", "True", ")", ":", "\n", "    ", "\"\"\"profile the program between [start_step, end_step).\"\"\"", "\n", "if", "curr_step", "==", "start_step", ":", "\n", "        ", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "mx", ".", "profiler", ".", "set_config", "(", "profile_memory", "=", "False", ",", "profile_symbolic", "=", "True", ",", "\n", "profile_imperative", "=", "True", ",", "filename", "=", "profile_name", ",", "\n", "aggregate_stats", "=", "True", ")", "\n", "mx", ".", "profiler", ".", "set_state", "(", "'run'", ")", "\n", "", "elif", "curr_step", "==", "end_step", ":", "\n", "        ", "mx", ".", "nd", ".", "waitall", "(", ")", "\n", "mx", ".", "profiler", ".", "set_state", "(", "'stop'", ")", "\n", "logging", ".", "info", "(", "mx", ".", "profiler", ".", "dumps", "(", ")", ")", "\n", "mx", ".", "profiler", ".", "dump", "(", ")", "\n", "if", "early_exit", ":", "\n", "            ", "exit", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.__init__": [[128, 139], ["NotImplementedError", "fp16_utils.DynamicLossScaler", "fp16_utils.StaticLossScaler"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "trainer", ",", "dynamic_loss_scale", "=", "False", ",", "loss_scaler_params", "=", "None", ")", ":", "\n", "        ", "if", "trainer", ".", "_kvstore_params", "[", "'update_on_kvstore'", "]", "is", "not", "False", "and", "trainer", ".", "_kvstore", ":", "\n", "            ", "err", "=", "'Only gluon.Trainer created with update_on_kvstore=False is supported.'", "\n", "raise", "NotImplementedError", "(", "err", ")", "\n", "", "self", ".", "fp32_trainer", "=", "trainer", "\n", "loss_scaler_params", "=", "loss_scaler_params", "if", "loss_scaler_params", "else", "{", "}", "\n", "self", ".", "_scaler", "=", "DynamicLossScaler", "(", "**", "loss_scaler_params", ")", "if", "dynamic_loss_scale", "else", "StaticLossScaler", "(", "**", "loss_scaler_params", ")", "\n", "# if the optimizer supports NaN check, we can always defer the NaN check to the optimizer", "\n", "# TODO(haibin) this should be added via registry", "\n", "self", ".", "_support_nan_check", "=", "trainer", ".", "_optimizer", ".", "__class__", ".", "__name__", "==", "'BERTAdam'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward": [[140, 148], ["mxnet.autograd.backward", "mxnet.autograd.record", "isinstance"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.backward"], ["", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"backward propagation with loss\"\"\"", "\n", "with", "mx", ".", "autograd", ".", "record", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "loss", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                ", "ls", "=", "[", "l", "*", "self", ".", "_scaler", ".", "loss_scale", "for", "l", "in", "loss", "]", "\n", "", "else", ":", "\n", "                ", "ls", "=", "loss", "*", "self", ".", "_scaler", ".", "loss_scale", "\n", "", "", "mx", ".", "autograd", ".", "backward", "(", "ls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.FP16Trainer.step": [[149, 189], ["fp16_utils.FP16Trainer.fp32_trainer.allreduce_grads", "fp16_utils.FP16Trainer._scaler.update_scale", "fp16_utils.grad_global_norm", "fp16_utils.FP16Trainer.fp32_trainer.update", "fp16_utils.FP16Trainer.fp32_trainer.update", "fp16_utils.FP16Trainer._scaler.has_overflow", "fp16_utils.FP16Trainer._scaler.has_overflow", "is_finite.asscalar", "numpy.isfinite", "fp16_utils.FP16Trainer.fp32_trainer.update", "fp16_utils.FP16Trainer.fp32_trainer.update", "norm.asscalar"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.DynamicLossScaler.update_scale", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.grad_global_norm", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.LossScaler.has_overflow", "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.LossScaler.has_overflow"], ["", "def", "step", "(", "self", ",", "batch_size", ",", "max_norm", "=", "None", ")", ":", "\n", "        ", "\"\"\"Makes one step of parameter update. Should be called after\n        `fp16_optimizer.backward()`, and outside of `record()` scope.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n        max_norm : NDArray, optional, default is None\n            max value for global 2-norm of gradients.\n        \"\"\"", "\n", "self", ".", "fp32_trainer", ".", "allreduce_grads", "(", ")", "\n", "step_size", "=", "batch_size", "*", "self", ".", "_scaler", ".", "loss_scale", "\n", "if", "max_norm", ":", "\n", "            ", "norm", ",", "ratio", ",", "is_finite", "=", "grad_global_norm", "(", "self", ".", "fp32_trainer", ".", "_params", ",", "\n", "max_norm", "*", "self", ".", "_scaler", ".", "loss_scale", ")", "\n", "step_size", "=", "ratio", "*", "step_size", "\n", "if", "not", "self", ".", "_support_nan_check", ":", "\n", "                ", "self", ".", "fp32_trainer", ".", "update", "(", "step_size", ")", "\n", "# try:", "\n", "overflow", "=", "is_finite", ".", "asscalar", "(", ")", "<", "1", "\n", "# except:", "\n", "#    overflow = True", "\n", "", "else", ":", "\n", "                ", "overflow", "=", "not", "np", ".", "isfinite", "(", "norm", ".", "asscalar", "(", ")", ")", "\n", "if", "not", "overflow", ":", "\n", "                    ", "self", ".", "fp32_trainer", ".", "update", "(", "step_size", ")", "\n", "", "", "", "else", ":", "\n", "# TODO(haibin) optimize the performance when max_norm is not present", "\n", "# sequentially adding isnan/isinf results may be slow", "\n", "            ", "if", "self", ".", "_support_nan_check", ":", "\n", "                ", "self", ".", "fp32_trainer", ".", "update", "(", "step_size", ")", "\n", "overflow", "=", "self", ".", "_scaler", ".", "has_overflow", "(", "self", ".", "fp32_trainer", ".", "_params", ")", "\n", "", "else", ":", "\n", "                ", "overflow", "=", "self", ".", "_scaler", ".", "has_overflow", "(", "self", ".", "fp32_trainer", ".", "_params", ")", "\n", "if", "not", "overflow", ":", "\n", "                    ", "self", ".", "fp32_trainer", ".", "update", "(", "step_size", ")", "\n", "# update scale based on overflow information", "\n", "", "", "", "self", ".", "_scaler", ".", "update_scale", "(", "overflow", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.LossScaler.has_overflow": [[194, 207], ["mxnet.nd.contrib.isnan().sum", "mxnet.nd.contrib.isinf().sum", "param.list_grad", "mxnet.nd.contrib.isnan", "mxnet.nd.contrib.isinf"], "methods", ["None"], ["def", "has_overflow", "(", "self", ",", "params", ")", ":", "\n", "        ", "\"\"\" detect inf and nan \"\"\"", "\n", "is_not_finite", "=", "0", "\n", "for", "param", "in", "params", ":", "\n", "            ", "if", "param", ".", "grad_req", "!=", "'null'", ":", "\n", "                ", "grad", "=", "param", ".", "list_grad", "(", ")", "[", "0", "]", "\n", "is_not_finite", "+=", "mx", ".", "nd", ".", "contrib", ".", "isnan", "(", "grad", ")", ".", "sum", "(", ")", "\n", "is_not_finite", "+=", "mx", ".", "nd", ".", "contrib", ".", "isinf", "(", "grad", ")", ".", "sum", "(", ")", "\n", "# NDArray is implicitly converted to bool", "\n", "", "", "if", "is_not_finite", "==", "0", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.LossScaler.update_scale": [[208, 210], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.StaticLossScaler.__init__": [[215, 217], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "init_scale", "=", "1", ")", ":", "\n", "        ", "self", ".", "loss_scale", "=", "init_scale", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.StaticLossScaler.update_scale": [[218, 221], ["None"], "methods", ["None"], ["", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "\"\"\"update loss scale\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.DynamicLossScaler.__init__": [[237, 247], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "init_scale", "=", "1.5", "**", "15", ",", "scale_factor", "=", "2.", ",", "scale_window", "=", "2000", ",", "\n", "tolerance", "=", "0.01", ")", ":", "\n", "        ", "self", ".", "loss_scale", "=", "init_scale", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "scale_window", "=", "scale_window", "\n", "self", ".", "tolerance", "=", "tolerance", "\n", "self", ".", "_num_steps", "=", "0", "\n", "self", ".", "_last_overflow_iter", "=", "-", "1", "\n", "self", ".", "_last_rescale_iter", "=", "-", "1", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.DynamicLossScaler.update_scale": [[248, 269], ["float", "warnings.warn"], "methods", ["None"], ["", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "\"\"\"dynamically update loss scale\"\"\"", "\n", "iter_since_rescale", "=", "self", ".", "_num_steps", "-", "self", ".", "_last_rescale_iter", "\n", "if", "overflow", ":", "\n", "            ", "self", ".", "_last_overflow_iter", "=", "self", ".", "_num_steps", "\n", "self", ".", "_overflows_since_rescale", "+=", "1", "\n", "percentage", "=", "self", ".", "_overflows_since_rescale", "/", "float", "(", "iter_since_rescale", ")", "\n", "# we tolerate a certrain amount of NaNs before actually scaling it", "\n", "# down", "\n", "if", "percentage", ">=", "self", ".", "tolerance", ":", "\n", "                ", "self", ".", "loss_scale", "/=", "self", ".", "scale_factor", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_num_steps", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "if", "self", ".", "loss_scale", "<", "1", ":", "\n", "                    ", "warnings", ".", "warn", "(", "'DynamicLossScaler: overflow detected. set loss_scale = %s'", "%", "\n", "self", ".", "loss_scale", ")", "\n", "", "", "", "elif", "(", "self", ".", "_num_steps", "-", "self", ".", "_last_overflow_iter", ")", "%", "self", ".", "scale_window", "==", "0", ":", "\n", "            ", "self", ".", "loss_scale", "*=", "self", ".", "scale_factor", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_num_steps", "\n", "", "self", ".", "_num_steps", "+=", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.utils.fp16_utils.grad_global_norm": [[26, 109], ["fp16_utils.grad_global_norm.group_by_ctx"], "function", ["None"], ["def", "grad_global_norm", "(", "parameters", ",", "max_norm", ")", ":", "\n", "    ", "\"\"\"Calculate the 2-norm of gradients of parameters, and how much they should be scaled down\n    such that their 2-norm does not exceed `max_norm`.\n\n    If gradients exist for more than one context for a parameter, user needs to explicitly call\n    ``trainer.allreduce_grads`` so that the gradients are summed first before calculating\n    the 2-norm.\n\n    .. note::\n\n        This function is only for use when `update_on_kvstore` is set to False in trainer.\n\n    Example::\n\n        trainer = Trainer(net.collect_params(), update_on_kvstore=False, ...)\n        for x, y in mx.gluon.utils.split_and_load(X, [mx.gpu(0), mx.gpu(1)]):\n            with mx.autograd.record():\n                y = net(x)\n                loss = loss_fn(y, label)\n            loss.backward()\n        trainer.allreduce_grads()\n        norm, ratio = grad_global_norm(net.collect_params().values(), max_norm)\n        trainer.update(batch_size * ratio)\n        ...\n\n    Parameters\n    ----------\n    parameters : list of Parameters\n\n    Returns\n    -------\n    NDArray\n      Total norm. Shape is (1,)\n    NDArray\n      Ratio for rescaling gradients based on max_norm s.t. grad = grad / ratio.\n      If total norm is NaN, ratio will be NaN, too. Shape is (1,)\n    NDArray\n      Whether the total norm is finite. Shape is (1,)\n    \"\"\"", "\n", "# collect gradient arrays", "\n", "arrays", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "p", "in", "parameters", ":", "\n", "        ", "if", "p", ".", "grad_req", "!=", "'null'", ":", "\n", "            ", "p_grads", "=", "p", ".", "list_grad", "(", ")", "\n", "arrays", ".", "append", "(", "p_grads", "[", "idx", "%", "len", "(", "p_grads", ")", "]", ")", "\n", "idx", "+=", "1", "\n", "", "", "assert", "len", "(", "arrays", ")", ">", "0", ",", "'No parameter found available for gradient norm.'", "\n", "\n", "# compute gradient norms", "\n", "def", "_norm", "(", "array", ")", ":", "\n", "# TODO(haibin) norm operator does not support fp16 safe reduction.", "\n", "# Issue is tracked at:", "\n", "# https://github.com/apache/incubator-mxnet/issues/14126", "\n", "        ", "x", "=", "array", ".", "reshape", "(", "(", "-", "1", ",", ")", ")", ".", "astype", "(", "'float32'", ",", "copy", "=", "False", ")", "\n", "return", "nd", ".", "dot", "(", "x", ",", "x", ")", "\n", "\n", "", "norm_arrays", "=", "[", "_norm", "(", "arr", ")", "for", "arr", "in", "arrays", "]", "\n", "\n", "# group norm arrays by ctx", "\n", "def", "group_by_ctx", "(", "arr_list", ")", ":", "\n", "        ", "groups", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "arr", "in", "arr_list", ":", "\n", "            ", "ctx", "=", "arr", ".", "context", "\n", "groups", "[", "ctx", "]", ".", "append", "(", "arr", ")", "\n", "", "return", "groups", "\n", "", "norm_groups", "=", "group_by_ctx", "(", "norm_arrays", ")", "\n", "\n", "# reduce", "\n", "ctx", ",", "dtype", "=", "arrays", "[", "0", "]", ".", "context", ",", "'float32'", "\n", "norms", "=", "[", "nd", ".", "add_n", "(", "*", "g", ")", ".", "as_in_context", "(", "ctx", ")", "for", "g", "in", "norm_groups", ".", "values", "(", ")", "]", "\n", "total_norm", "=", "nd", ".", "add_n", "(", "*", "norms", ")", ".", "sqrt", "(", ")", "\n", "scale", "=", "total_norm", "/", "max_norm", "\n", "# is_finite = 0 if NaN or Inf, 1 otherwise.", "\n", "is_finite", "=", "nd", ".", "contrib", ".", "isfinite", "(", "scale", ")", "\n", "# if scale is finite, nd.maximum selects the max between scale and 1. That is,", "\n", "# 1 is returned if total_norm does not exceed max_norm.", "\n", "# if scale = NaN or Inf, the result of nd.minimum is undefined. Therefore, we use", "\n", "# choices.take to return NaN or Inf.", "\n", "scale_or_one", "=", "nd", ".", "maximum", "(", "nd", ".", "ones", "(", "(", "1", ",", ")", ",", "dtype", "=", "dtype", ",", "ctx", "=", "ctx", ")", ",", "scale", ")", "\n", "choices", "=", "nd", ".", "concat", "(", "scale", ",", "scale_or_one", ",", "dim", "=", "0", ")", "\n", "chosen_scale", "=", "choices", ".", "take", "(", "is_finite", ")", "\n", "return", "total_norm", ",", "chosen_scale", ",", "is_finite", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.do_log": [[75, 91], ["metric.get", "isinstance", "logging.info", "logging.info"], "function", ["None"], ["def", "do_log", "(", "batch_id", ",", "batch_num", ",", "metric", ",", "step_loss", ",", "log_interval", ",", "epoch_id", "=", "None", ",", "learning_rate", "=", "None", ")", ":", "\n", "    ", "\"\"\"Generate and print out the log messages. \"\"\"", "\n", "metric_nm", ",", "metric_val", "=", "metric", ".", "get", "(", ")", "\n", "if", "not", "isinstance", "(", "metric_nm", ",", "list", ")", ":", "\n", "        ", "metric_nm", ",", "metric_val", "=", "[", "metric_nm", "]", ",", "[", "metric_val", "]", "\n", "\n", "", "if", "epoch_id", "is", "None", ":", "\n", "        ", "eval_str", "=", "'[Batch %d/%d] loss=%.4f, metrics:'", "+", "','", ".", "join", "(", "[", "i", "+", "':%.4f'", "for", "i", "in", "metric_nm", "]", ")", "\n", "logging", ".", "info", "(", "eval_str", ",", "batch_id", "+", "1", ",", "batch_num", ",", "\n", "step_loss", "/", "log_interval", ",", "*", "metric_val", ")", "\n", "", "else", ":", "\n", "        ", "train_str", "=", "'[Epoch %d Batch %d/%d] loss=%.4f, lr=%.10f, metrics:'", "+", "','", ".", "join", "(", "[", "i", "+", "':%.4f'", "for", "i", "in", "metric_nm", "]", ")", "\n", "logging", ".", "info", "(", "train_str", ",", "epoch_id", "+", "1", ",", "batch_id", "+", "1", ",", "batch_num", ",", "step_loss", "/", "log_interval", ",", "\n", "learning_rate", ",", "*", "metric_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.convert_examples_to_features": [[93, 130], ["data.preprocessing_utils.truncate_seqs_equal", "data.preprocessing_utils.concat_sequences", "len", "numpy.array", "tokenizer", "enumerate", "len", "type", "type"], "function", ["home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.truncate_seqs_equal", "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.concat_sequences"], ["", "", "def", "convert_examples_to_features", "(", "example", ",", "tokenizer", "=", "None", ",", "truncate_length", "=", "512", ",", "cls_token", "=", "None", ",", "\n", "sep_token", "=", "None", ",", "class_labels", "=", "None", ",", "label_alias", "=", "None", ",", "vocab", "=", "None", ",", "\n", "is_test", "=", "False", ")", ":", "\n", "    ", "\"\"\"Convert GLUE/SuperGLUE classification and regression examples into \n        the necessary features\"\"\"", "\n", "if", "not", "is_test", ":", "\n", "        ", "label_dtype", "=", "'int32'", "if", "class_labels", "else", "'float32'", "\n", "example", ",", "label", "=", "example", "[", ":", "-", "1", "]", ",", "example", "[", "-", "1", "]", "\n", "# create label maps if classification task", "\n", "if", "class_labels", ":", "\n", "            ", "label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "l", ")", "in", "enumerate", "(", "class_labels", ")", ":", "\n", "                ", "label_map", "[", "l", "]", "=", "i", "\n", "", "if", "label_alias", ":", "\n", "                ", "for", "key", "in", "label_alias", ":", "\n", "                    ", "label_map", "[", "key", "]", "=", "label_map", "[", "label_alias", "[", "key", "]", "]", "\n", "# Fix for BoolQ, WSC, and MultiRC, json values get loaded as boolean and not as string", "\n", "# assignments.", "\n", "", "", "if", "type", "(", "label", ")", "==", "bool", ":", "\n", "                ", "label", "=", "\"true\"", "if", "label", "else", "\"false\"", "\n", "# Fix for COPA", "\n", "", "if", "type", "(", "label", ")", "==", "int", ":", "\n", "                ", "label", "=", "\"0\"", "if", "label", "==", "0", "else", "\"1\"", "\n", "", "label", "=", "label_map", "[", "label", "]", "\n", "", "label", "=", "np", ".", "array", "(", "[", "label", "]", ",", "dtype", "=", "label_dtype", ")", "\n", "", "tokens_raw", "=", "[", "tokenizer", "(", "l", ")", "for", "l", "in", "example", "]", "\n", "tokens_trun", "=", "truncate_seqs_equal", "(", "tokens_raw", ",", "truncate_length", ")", "\n", "tokens_trun", "[", "0", "]", "=", "[", "cls_token", "]", "+", "tokens_trun", "[", "0", "]", "\n", "tokens", ",", "segment_ids", ",", "_", "=", "concat_sequences", "(", "\n", "tokens_trun", ",", "[", "[", "sep_token", "]", "]", "*", "len", "(", "tokens_trun", ")", ")", "\n", "input_ids", "=", "vocab", "[", "tokens", "]", "\n", "valid_length", "=", "len", "(", "input_ids", ")", "\n", "\n", "if", "not", "is_test", ":", "\n", "        ", "return", "input_ids", ",", "segment_ids", ",", "valid_length", ",", "label", "\n", "", "else", ":", "\n", "        ", "return", "input_ids", ",", "segment_ids", ",", "valid_length", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.preprocess_data": [[132, 201], ["functools.partial", "mxnet.gluon.data.SimpleDataset", "mx.gluon.data.SimpleDataset.transform", "gluonnlp.data.batchify.Tuple", "task.dataset_dev", "gluonnlp.data.batchify.Tuple", "functools.partial", "task.dataset_test", "task.dataset_train", "list", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "gluonnlp.data.batchify.Stack", "gluonnlp.data.sampler.FixedBucketSampler", "gluonnlp.data.ShardedDataLoader", "gluonnlp.data.sampler.FixedBucketSampler", "mxnet.gluon.data.DataLoader", "isinstance", "mxnet.gluon.data.SimpleDataset", "mxnet.gluon.data.DataLoader", "loader_dev_list.append", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Pad", "gluonnlp.data.batchify.Stack", "isinstance", "mxnet.gluon.data.SimpleDataset", "mxnet.gluon.data.DataLoader", "loader_test_list.append", "len", "map", "list", "list", "map", "map"], "function", ["home.repos.pwc.inspect_result.alexa_bort.None.create_pretraining_data.transform", "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.dataset_dev", "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.dataset_test", "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.dataset_train"], ["", "", "def", "preprocess_data", "(", "tokenizer", ",", "task", ",", "batch_size", ",", "dev_batch_size", ",", "max_len", ",", "vocab", ",", "world_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Train/eval Data preparation function.\"\"\"", "\n", "label_dtype", "=", "'int32'", "if", "task", ".", "class_labels", "else", "'float32'", "\n", "truncate_length", "=", "max_len", "-", "3", "if", "task", ".", "is_pair", "else", "max_len", "-", "2", "\n", "trans", "=", "partial", "(", "convert_examples_to_features", ",", "tokenizer", "=", "tokenizer", ",", "\n", "truncate_length", "=", "truncate_length", ",", "\n", "cls_token", "=", "vocab", ".", "bos_token", ",", "\n", "sep_token", "=", "vocab", ".", "eos_token", ",", "\n", "class_labels", "=", "task", ".", "class_labels", ",", "\n", "label_alias", "=", "task", ".", "label_alias", ",", "vocab", "=", "vocab", ")", "\n", "\n", "# task.dataset_train returns (segment_name, dataset)", "\n", "train_tsv", "=", "task", ".", "dataset_train", "(", ")", "[", "1", "]", "\n", "data_train", "=", "mx", ".", "gluon", ".", "data", ".", "SimpleDataset", "(", "list", "(", "map", "(", "trans", ",", "train_tsv", ")", ")", ")", "\n", "data_train_len", "=", "data_train", ".", "transform", "(", "lambda", "_", ",", "segment_ids", ",", "valid_length", ",", "label", ":", "valid_length", ",", "\n", "lazy", "=", "False", ")", "\n", "# bucket sampler for training", "\n", "pad_val", "=", "vocab", "[", "vocab", ".", "padding_token", "]", "\n", "batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "pad_val", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "0", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ",", "# length", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", "label_dtype", ")", ")", "# label", "\n", "\n", "if", "world_size", "is", "not", "None", ":", "\n", "        ", "batch_sampler", "=", "nlp", ".", "data", ".", "sampler", ".", "FixedBucketSampler", "(", "data_train_len", ",", "batch_size", "=", "batch_size", ",", "\n", "num_buckets", "=", "15", ",", "ratio", "=", "0", ",", "shuffle", "=", "True", ",", "\n", "num_shards", "=", "world_size", ")", "\n", "loader_train", "=", "nlp", ".", "data", ".", "ShardedDataLoader", "(", "dataset", "=", "data_train", ",", "num_workers", "=", "4", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "batchify_fn", "=", "batchify_fn", ")", "\n", "", "else", ":", "\n", "        ", "batch_sampler", "=", "nlp", ".", "data", ".", "sampler", ".", "FixedBucketSampler", "(", "data_train_len", ",", "batch_size", "=", "batch_size", ",", "\n", "num_buckets", "=", "15", ",", "ratio", "=", "0", ",", "shuffle", "=", "True", ")", "\n", "loader_train", "=", "mx", ".", "gluon", ".", "data", ".", "DataLoader", "(", "dataset", "=", "data_train", ",", "num_workers", "=", "4", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "batchify_fn", "=", "batchify_fn", ")", "\n", "\n", "# data dev. For MNLI, more than one dev set is available", "\n", "", "dev_tsv", "=", "task", ".", "dataset_dev", "(", ")", "\n", "dev_tsv_list", "=", "dev_tsv", "if", "isinstance", "(", "dev_tsv", ",", "list", ")", "else", "[", "dev_tsv", "]", "\n", "loader_dev_list", "=", "[", "]", "\n", "for", "segment", ",", "data", "in", "dev_tsv_list", ":", "\n", "        ", "data_dev", "=", "mx", ".", "gluon", ".", "data", ".", "SimpleDataset", "(", "list", "(", "map", "(", "trans", ",", "data", ")", ")", ")", "\n", "loader_dev", "=", "mx", ".", "gluon", ".", "data", ".", "DataLoader", "(", "data_dev", ",", "batch_size", "=", "dev_batch_size", ",", "num_workers", "=", "4", ",", "\n", "shuffle", "=", "False", ",", "batchify_fn", "=", "batchify_fn", ")", "\n", "loader_dev_list", ".", "append", "(", "(", "segment", ",", "loader_dev", ")", ")", "\n", "\n", "# batchify for data test", "\n", "", "test_batchify_fn", "=", "nlp", ".", "data", ".", "batchify", ".", "Tuple", "(", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "pad_val", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Pad", "(", "axis", "=", "0", ",", "pad_val", "=", "0", ")", ",", "\n", "nlp", ".", "data", ".", "batchify", ".", "Stack", "(", ")", ")", "\n", "# transform for data test", "\n", "test_trans", "=", "partial", "(", "convert_examples_to_features", ",", "tokenizer", "=", "tokenizer", ",", "truncate_length", "=", "truncate_length", ",", "\n", "cls_token", "=", "vocab", ".", "bos_token", ",", "\n", "sep_token", "=", "vocab", ".", "eos_token", ",", "\n", "class_labels", "=", "None", ",", "is_test", "=", "True", ",", "vocab", "=", "vocab", ")", "\n", "\n", "# data test. For MNLI, more than one test set is available", "\n", "test_tsv", "=", "task", ".", "dataset_test", "(", ")", "\n", "test_tsv_list", "=", "test_tsv", "if", "isinstance", "(", "test_tsv", ",", "list", ")", "else", "[", "test_tsv", "]", "\n", "loader_test_list", "=", "[", "]", "\n", "for", "segment", ",", "data", "in", "test_tsv_list", ":", "\n", "        ", "data_test", "=", "mx", ".", "gluon", ".", "data", ".", "SimpleDataset", "(", "list", "(", "map", "(", "test_trans", ",", "data", ")", ")", ")", "\n", "loader_test", "=", "mx", ".", "gluon", ".", "data", ".", "DataLoader", "(", "data_test", ",", "batch_size", "=", "dev_batch_size", ",", "num_workers", "=", "4", ",", "\n", "shuffle", "=", "False", ",", "batchify_fn", "=", "test_batchify_fn", ")", "\n", "loader_test_list", ".", "append", "(", "(", "segment", ",", "loader_test", ")", ")", "\n", "\n", "# Return data_dev for ReCoRD and MultiRC", "\n", "", "return", "loader_train", ",", "data_dev", ",", "loader_dev_list", ",", "loader_test_list", ",", "len", "(", "data_train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.MultiRCHash": [[203, 225], ["json.loads", "lines.append", "open().readlines", "[].append", "question_dict[].append", "open"], "function", ["None"], ["", "def", "MultiRCHash", "(", "test_dataset_location", ")", ":", "\n", "    ", "\"\"\" MultiRC has multiple nested points. Return a list of dictionaries with\n        the predictions to fill out.\n    \"\"\"", "\n", "dataset", "=", "[", "json", ".", "loads", "(", "l", ",", "object_pairs_hook", "=", "OrderedDict", ")", "\n", "for", "l", "in", "open", "(", "test_dataset_location", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", ".", "readlines", "(", ")", "]", "\n", "line_dict", "=", "[", "(", "l", "[", "\"idx\"", "]", ",", "l", "[", "\"passage\"", "]", ")", "for", "l", "in", "dataset", "]", "\n", "lines", "=", "[", "]", "\n", "\n", "for", "idx", ",", "line", "in", "line_dict", ":", "\n", "        ", "questions", "=", "line", "[", "\"questions\"", "]", "\n", "line_hashes", "=", "{", "\"idx\"", ":", "idx", ",", "\"passage\"", ":", "{", "\"questions\"", ":", "[", "]", "}", "}", "\n", "for", "question", "in", "questions", ":", "\n", "            ", "question_dict", "=", "{", "\"idx\"", ":", "question", "[", "\"idx\"", "]", ",", "\"answers\"", ":", "[", "]", "}", "\n", "for", "answer", "in", "question", "[", "\"answers\"", "]", ":", "\n", "                ", "question_dict", "[", "\"answers\"", "]", ".", "append", "(", "\n", "{", "\"idx\"", ":", "answer", "[", "\"idx\"", "]", ",", "\"label\"", ":", "0", "}", ")", "\n", "", "line_hashes", "[", "\"passage\"", "]", "[", "\"questions\"", "]", ".", "append", "(", "question_dict", ")", "\n", "\n", "", "lines", ".", "append", "(", "line_hashes", ")", "\n", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.ReCoRDHash": [[227, 252], ["json.loads", "sorted", "open().readlines", "set", "tmp_lines.append", "open"], "function", ["None"], ["", "def", "ReCoRDHash", "(", "dataset_location", ")", ":", "\n", "    ", "\"\"\" Because of the way we've setup ReCoRD, we need to figure out a way to translate\n        it back into a viable answer. \n    \"\"\"", "\n", "dataset", "=", "[", "json", ".", "loads", "(", "l", ",", "object_pairs_hook", "=", "OrderedDict", ")", "\n", "for", "l", "in", "open", "(", "dataset_location", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", ".", "readlines", "(", ")", "]", "\n", "is_test", "=", "\"test\"", "in", "dataset_location", "\n", "\n", "all_lines", "=", "[", "(", "l", "[", "\"idx\"", "]", ",", "l", "[", "\"passage\"", "]", ",", "l", "[", "\"qas\"", "]", ")", "for", "l", "in", "dataset", "]", "\n", "lines", "=", "{", "}", "\n", "for", "idx", ",", "line", ",", "qas", "in", "all_lines", ":", "\n", "        ", "entities", "=", "sorted", "(", "\n", "set", "(", "[", "line", "[", "\"text\"", "]", "[", "e", "[", "\"start\"", "]", ":", "e", "[", "\"end\"", "]", "+", "1", "]", "for", "e", "in", "line", "[", "\"entities\"", "]", "]", ")", ")", "\n", "for", "question", "in", "qas", ":", "\n", "            ", "tmp_lines", "=", "[", "]", "\n", "answers", "=", "None", "if", "is_test", "else", "[", "\n", "ans", "[", "\"text\"", "]", "for", "ans", "in", "question", "[", "\"answers\"", "]", "]", "\n", "for", "entity", "in", "entities", ":", "\n", "                ", "is_answer", "=", "False", "\n", "if", "not", "is_test", ":", "\n", "                    ", "is_answer", "=", "entity", "in", "answers", "\n", "", "tmp_lines", ".", "append", "(", "\n", "{", "\"idx\"", ":", "question", "[", "\"idx\"", "]", ",", "\"label\"", ":", "entity", ",", "\"is_answer\"", ":", "is_answer", "}", ")", "\n", "", "lines", "[", "question", "[", "\"idx\"", "]", "]", "=", "tmp_lines", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.RACEHash": [[254, 271], ["os.path.join", "filenames.sort", "os.path.join", "os.path.expanduser", "gluonnlp.base.get_home_dir", "os.listdir", "fnmatch.fnmatch", "json.loads", "open().readlines", "open", "os.path.join"], "function", ["None"], ["", "def", "RACEHash", "(", "dataset_location", ",", "task_name", ",", "segment", "=", "'test'", ")", ":", "\n", "    ", "\"\"\" Because of the way we've setup RACE-H/RACE-M, we need to figure out a way to\n        translate it back into a viable answer. \n    \"\"\"", "\n", "if", "dataset_location", "is", "None", ":", "\n", "        ", "dataset_location", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'race'", ")", "\n", "\n", "", "task", "=", "\"high\"", "if", "task_name", "[", "-", "1", "]", "==", "\"H\"", "else", "\"middle\"", "\n", "test_dataset_location", "=", "os", ".", "path", ".", "join", "(", "dataset_location", ",", "segment", ",", "task", ")", "\n", "filenames", "=", "[", "os", ".", "path", ".", "expanduser", "(", "f", ")", "for", "f", "in", "os", ".", "listdir", "(", "\n", "test_dataset_location", ")", "if", "fnmatch", ".", "fnmatch", "(", "f", ",", "'*.txt'", ")", "]", "\n", "filenames", ".", "sort", "(", ")", "\n", "dataset", "=", "[", "]", "\n", "for", "f", "in", "filenames", ":", "\n", "        ", "dataset", "+=", "[", "json", ".", "loads", "(", "l", ",", "object_pairs_hook", "=", "OrderedDict", ")", "\n", "for", "l", "in", "open", "(", "os", ".", "path", ".", "join", "(", "test_dataset_location", ",", "f", ")", ",", "'r'", ")", ".", "readlines", "(", ")", "]", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_ReCoRD_answers": [[273, 310], ["range", "len", "range", "preds.append", "label.append", "chosen_candidate.pop", "tmp_results.append", "sum", "len", "len", "len", "len", "zip"], "function", ["None"], ["", "def", "process_ReCoRD_answers", "(", "results", ",", "result_data", ")", ":", "\n", "# In practice we should get the max confidence over the question space.", "\n", "# First assign label and confidence to every single point on the set, then", "\n", "# prune out low-confidence elements.", "\n", "    ", "tmp_results", "=", "[", "]", "\n", "start_index", "=", "0", "\n", "preds", ",", "label", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "result_data", ")", ")", ":", "\n", "        ", "candidate_result_array", "=", "result_data", "[", "i", "]", "\n", "results_subarray", "=", "results", "[", "\n", "start_index", ":", "start_index", "+", "len", "(", "candidate_result_array", ")", "]", "\n", "idx", ",", "max_confidence", "=", "0", ",", "-", "np", ".", "inf", "\n", "backup_idx", ",", "backup_max_confidence", "=", "0", ",", "-", "np", ".", "inf", "\n", "for", "j", "in", "range", "(", "len", "(", "results_subarray", ")", ")", ":", "\n", "            ", "score", ",", "logits", "=", "results_subarray", "[", "j", "]", "[", "0", "]", ",", "results_subarray", "[", "j", "]", "[", "-", "1", "]", "\n", "if", "score", "==", "1", "and", "logits", "[", "-", "1", "]", ">", "max_confidence", ":", "\n", "                ", "idx", "=", "j", "\n", "", "else", ":", "\n", "                ", "if", "logits", "[", "-", "1", "]", ">", "backup_max_confidence", ":", "\n", "                    ", "backup_idx", "=", "j", "\n", "backup_max_confidence", "=", "logits", "[", "-", "1", "]", "\n", "", "", "", "if", "max_confidence", "==", "-", "np", ".", "inf", ":", "\n", "            ", "idx", "=", "backup_idx", "\n", "", "chosen_candidate", "=", "candidate_result_array", "[", "idx", "]", "\n", "preds", ".", "append", "(", "chosen_candidate", "[", "\"label\"", "]", ")", "\n", "label", ".", "append", "(", "chosen_candidate", "[", "\"label\"", "]", "if", "candidate_result_array", "[", "\n", "idx", "]", "[", "\"is_answer\"", "]", "else", "\"glorp\"", ")", "\n", "\n", "chosen_candidate", ".", "pop", "(", "\"is_answer\"", ",", "None", ")", "\n", "tmp_results", ".", "append", "(", "chosen_candidate", ")", "\n", "\n", "start_index", "=", "start_index", "+", "len", "(", "results_subarray", ")", "\n", "\n", "# This number is meaningless in test (all eval to False), and", "\n", "# might have high false negatives", "\n", "", "score", "=", "sum", "(", "[", "p", "==", "l", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "preds", ",", "label", ")", "]", ")", "/", "len", "(", "preds", ")", "\n", "return", "tmp_results", ",", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_MultiRC_answers": [[312, 329], ["finetune_utils.MultiRCHash", "int", "len", "len"], "function", ["home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.MultiRCHash"], ["", "def", "process_MultiRC_answers", "(", "results", ",", "test_dataset_location", ")", ":", "\n", "# \"Re-roll\" the unrolled prediction vector into the required format.", "\n", "    ", "result_data", "=", "MultiRCHash", "(", "test_dataset_location", ")", "\n", "p_idx", ",", "q_idx", ",", "a_idx", "=", "0", ",", "0", ",", "0", "\n", "\n", "for", "label", "in", "results", ":", "\n", "        ", "if", "len", "(", "result_data", "[", "p_idx", "]", "[", "\"passage\"", "]", "[", "\"questions\"", "]", "[", "q_idx", "]", "[", "\"answers\"", "]", ")", "==", "a_idx", ":", "\n", "            ", "a_idx", "=", "0", "\n", "q_idx", "+=", "1", "\n", "", "if", "len", "(", "result_data", "[", "p_idx", "]", "[", "\"passage\"", "]", "[", "\"questions\"", "]", ")", "==", "q_idx", ":", "\n", "            ", "q_idx", "=", "0", "\n", "p_idx", "+=", "1", "\n", "", "result_data", "[", "p_idx", "]", "[", "\"passage\"", "]", "[", "\"questions\"", "]", "[", "\n", "q_idx", "]", "[", "\"answers\"", "]", "[", "a_idx", "]", "[", "\"label\"", "]", "=", "int", "(", "label", ")", "\n", "a_idx", "+=", "1", "\n", "\n", "", "return", "result_data", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.utils.finetune_utils.process_RACE_answers": [[331, 364], ["zip", "len", "tmp_results.append", "new_line[].append", "zip", "line.items", "len"], "function", ["None"], ["", "def", "process_RACE_answers", "(", "result_data", ",", "results", ")", ":", "\n", "# In practice we should get the max confidence over the question space.", "\n", "# First assign label and confidence to every single point on the set, then", "\n", "# prune out low-confidence elements.", "\n", "\n", "    ", "IDX_TO_ANSWER", "=", "{", "\"0\"", ":", "\"A\"", ",", "\"1\"", ":", "\"B\"", ",", "\"2\"", ":", "\"C\"", ",", "\"3\"", ":", "\"D\"", "}", "\n", "\n", "tmp_results", "=", "[", "]", "\n", "start_index", "=", "0", "\n", "total", ",", "correct", "=", "0", ",", "0", "\n", "\n", "for", "line", "in", "result_data", ":", "\n", "\n", "        ", "new_line", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "line", ".", "items", "(", ")", "}", "\n", "new_line", "[", "\"answers\"", "]", "=", "[", "]", "\n", "\n", "for", "question", ",", "prediction", "in", "zip", "(", "new_line", "[", "\"questions\"", "]", ",", "results", "[", "start_index", ":", "start_index", "+", "len", "(", "line", "[", "\"answers\"", "]", ")", "]", ")", ":", "\n", "            ", "label", "=", "IDX_TO_ANSWER", "[", "prediction", "[", "0", "]", "]", "\n", "new_line", "[", "\"answers\"", "]", ".", "append", "(", "label", ")", "\n", "", "start_index", "+=", "len", "(", "line", "[", "\"answers\"", "]", ")", "\n", "\n", "if", "\"answers\"", "in", "line", ":", "\n", "            ", "for", "pred", ",", "label", "in", "zip", "(", "new_line", "[", "\"answers\"", "]", ",", "line", "[", "\"answers\"", "]", ")", ":", "\n", "                ", "if", "pred", "==", "label", ":", "\n", "                    ", "correct", "+=", "1", "\n", "", "total", "+=", "1", "\n", "\n", "", "", "tmp_results", ".", "append", "(", "new_line", ")", "\n", "\n", "", "class_accuracy", "=", "correct", "/", "total", "if", "total", "!=", "0", "else", "0", "\n", "\n", "# Class accuracy is bugged, but we only need the actual accuracy anyway", "\n", "return", "tmp_results", ",", "class_accuracy", "\n", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.BortModel.__init__": [[68, 79], ["gluonnlp.model.bert.BERTModel.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "vocab_size", "=", "None", ",", "units", "=", "None", ",", "\n", "embed_size", "=", "None", ",", "embed_dropout", "=", "0.0", ",", "embed_initializer", "=", "None", ",", "\n", "word_embed", "=", "None", ",", "use_decoder", "=", "True", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", "BortModel", ",", "self", ")", ".", "__init__", "(", "encoder", ",", "vocab_size", "=", "vocab_size", ",", "\n", "token_type_vocab_size", "=", "None", ",", "units", "=", "units", ",", "\n", "embed_size", "=", "embed_size", ",", "embed_dropout", "=", "embed_dropout", ",", "\n", "embed_initializer", "=", "embed_initializer", ",", "\n", "word_embed", "=", "word_embed", ",", "token_type_embed", "=", "None", ",", "\n", "use_pooler", "=", "False", ",", "use_decoder", "=", "use_decoder", ",", "\n", "use_classifier", "=", "False", ",", "use_token_type_embed", "=", "False", ",", "\n", "prefix", "=", "prefix", ",", "params", "=", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.BortModel.__call__": [[80, 92], ["super().__call__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDatasetFn.__call__"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "valid_length", "=", "None", ",", "masked_positions", "=", "None", ")", ":", "\n", "# pylint: disable=dangerous-default-value", "\n", "        ", "\"\"\"Generate the representation given the inputs.\n\n        This is used in training or fine-tuning a Bort model.\n        \"\"\"", "\n", "# Temporary hack for hybridization as hybridblock does not support None", "\n", "# inputs", "\n", "valid_length", "=", "[", "]", "if", "valid_length", "is", "None", "else", "valid_length", "\n", "masked_positions", "=", "[", "]", "if", "masked_positions", "is", "None", "else", "masked_positions", "\n", "return", "super", "(", "BortModel", ",", "self", ")", ".", "__call__", "(", "inputs", ",", "[", "]", ",", "valid_length", "=", "valid_length", ",", "\n", "masked_positions", "=", "masked_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.BortClassifier.__init__": [[98, 111], ["mxnet.gluon.HybridBlock.__init__", "bort.BortClassifier.name_scope", "mxnet.gluon.nn.HybridSequential", "bort.BortClassifier.classifier.add", "bort.BortClassifier.classifier.add", "bort.BortClassifier.classifier.add", "mxnet.gluon.nn.Dense", "bort.BortClassifier.classifier.add", "mxnet.gluon.nn.Dense", "mxnet.gluon.nn.Dropout", "mxnet.gluon.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "bort", ",", "num_classes", "=", "2", ",", "dropout", "=", "0.1", ",", "prefix", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "super", "(", "BortClassifier", ",", "self", ")", ".", "__init__", "(", "prefix", "=", "prefix", ",", "params", "=", "params", ")", "\n", "self", ".", "bort", "=", "bort", "\n", "self", ".", "_units", "=", "bort", ".", "_units", "\n", "\n", "with", "self", ".", "name_scope", "(", ")", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "HybridSequential", "(", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "self", ".", "_units", ",", "activation", "=", "\"tanh\"", ")", ")", "\n", "if", "dropout", ":", "\n", "                ", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dropout", "(", "rate", "=", "dropout", ")", ")", "\n", "", "self", ".", "classifier", ".", "add", "(", "nn", ".", "Dense", "(", "units", "=", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.BortClassifier.__call__": [[112, 117], ["super().__call__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDatasetFn.__call__"], ["", "", "def", "__call__", "(", "self", ",", "inputs", ",", "valid_length", "=", "None", ")", ":", "\n", "# pylint: disable=dangerous-default-value, arguments-differ", "\n", "        ", "\"\"\"Generate the unnormalized score for the given the input sequences.\n        \"\"\"", "\n", "return", "super", "(", "BortClassifier", ",", "self", ")", ".", "__call__", "(", "inputs", ",", "valid_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.BortClassifier.hybrid_forward": [[118, 129], ["bort.BortClassifier.bort", "bort.BortClassifier.slice", "outputs.reshape.reshape.reshape", "bort.BortClassifier.classifier", "isinstance"], "methods", ["None"], ["", "def", "hybrid_forward", "(", "self", ",", "F", ",", "inputs", ",", "valid_length", "=", "None", ")", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"Generate the unnormalized score for the given the input sequences.\n        \"\"\"", "\n", "seq_out", "=", "self", ".", "bort", "(", "inputs", ",", "valid_length", ")", "\n", "assert", "not", "isinstance", "(", "seq_out", ",", "(", "tuple", ",", "list", ")", "\n", ")", ",", "'Expected one output from BortModel'", "\n", "outputs", "=", "seq_out", ".", "slice", "(", "begin", "=", "(", "0", ",", "0", ",", "0", ")", ",", "end", "=", "(", "None", ",", "1", ",", "None", ")", ")", "\n", "outputs", "=", "outputs", ".", "reshape", "(", "shape", "=", "(", "-", "1", ",", "self", ".", "_units", ")", ")", "\n", "\n", "return", "self", ".", "classifier", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.bort.bort.get_bort_model": [[131, 171], ["mxnet.cpu", "os.path.join", "logging.info", "frozenset", "print", "all", "predefined_args.update", "gluonnlp.model.bert.BERTEncoder", "gluonnlp.model.utils._load_vocab", "bort.BortModel", "gluonnlp.base.get_home_dir", "len", "gluonnlp.model.utils._load_pretrained_params", "predefined_args.get", "predefined_args.get"], "function", ["None"], ["", "", "def", "get_bort_model", "(", "model_name", "=", "None", ",", "dataset_name", "=", "None", ",", "vocab", "=", "None", ",", "pretrained", "=", "True", ",", "ctx", "=", "mx", ".", "cpu", "(", ")", ",", "\n", "use_decoder", "=", "True", ",", "output_attention", "=", "False", ",", "output_all_encodings", "=", "False", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'models'", ")", ",", "**", "kwargs", ")", ":", "\n", "    ", "predefined_args", "=", "predefined_borts", "[", "model_name", "]", "\n", "logging", ".", "info", "(", "f\"get_bort_model: {model_name}\"", ")", "\n", "mutable_args", "=", "[", "'use_residual'", ",", "'dropout'", ",", "'embed_dropout'", ",", "'word_embed'", "]", "\n", "mutable_args", "=", "frozenset", "(", "mutable_args", ")", "\n", "print", "(", "\"model_name: \"", ",", "model_name", ",", "\", predefined_args: \"", ",", "predefined_args", ")", "\n", "assert", "all", "(", "(", "k", "not", "in", "kwargs", "or", "k", "in", "mutable_args", ")", "for", "k", "in", "predefined_args", ")", ",", "'Cannot override predefined model settings.'", "\n", "predefined_args", ".", "update", "(", "kwargs", ")", "\n", "# encoder", "\n", "encoder", "=", "BERTEncoder", "(", "attention_cell", "=", "predefined_args", "[", "'attention_cell'", "]", ",", "\n", "num_layers", "=", "predefined_args", "[", "'num_layers'", "]", ",", "\n", "units", "=", "predefined_args", "[", "'units'", "]", ",", "\n", "hidden_size", "=", "predefined_args", "[", "'hidden_size'", "]", ",", "\n", "max_length", "=", "predefined_args", "[", "'max_length'", "]", ",", "\n", "num_heads", "=", "predefined_args", "[", "'num_heads'", "]", ",", "\n", "scaled", "=", "predefined_args", "[", "'scaled'", "]", ",", "\n", "dropout", "=", "predefined_args", "[", "'dropout'", "]", ",", "\n", "output_attention", "=", "output_attention", ",", "\n", "output_all_encodings", "=", "output_all_encodings", ",", "\n", "use_residual", "=", "predefined_args", "[", "'use_residual'", "]", ",", "\n", "activation", "=", "predefined_args", ".", "get", "(", "'activation'", ",", "'gelu'", ")", ",", "\n", "layer_norm_eps", "=", "predefined_args", ".", "get", "(", "'layer_norm_eps'", ",", "None", ")", ")", "\n", "\n", "from", "gluonnlp", ".", "vocab", "import", "Vocab", "\n", "bort_vocab", "=", "_load_vocab", "(", "dataset_name", ",", "vocab", ",", "root", ",", "cls", "=", "Vocab", ")", "\n", "\n", "net", "=", "BortModel", "(", "encoder", ",", "len", "(", "bort_vocab", ")", ",", "\n", "units", "=", "predefined_args", "[", "'units'", "]", ",", "\n", "embed_size", "=", "predefined_args", "[", "'embed_size'", "]", ",", "\n", "embed_dropout", "=", "predefined_args", "[", "'embed_dropout'", "]", ",", "\n", "word_embed", "=", "predefined_args", "[", "'word_embed'", "]", ",", "\n", "use_decoder", "=", "use_decoder", ")", "\n", "if", "pretrained", ":", "\n", "        ", "ignore_extra", "=", "not", "use_decoder", "\n", "_load_pretrained_params", "(", "net", ",", "model_name", ",", "dataset_name", ",", "root", ",", "ctx", ",", "ignore_extra", "=", "ignore_extra", ",", "\n", "allow_missing", "=", "False", ")", "\n", "", "return", "net", ",", "bort_vocab", "\n", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.truncate_seqs_equal": [[30, 59], ["isinstance", "list", "numpy.masked_array", "map", "sum", "ma.masked_array.argmin", "divmod", "zip", "len", "len", "sum", "ma.masked_array.data.tolist", "range", "ma.masked_array.count"], "function", ["None"], ["def", "truncate_seqs_equal", "(", "seqs", ",", "max_len", ")", ":", "\n", "    ", "\"\"\"truncate a list of seqs so that the total length equals max length.\n\n    Trying to truncate the seqs to equal length.\n\n    Returns\n    -------\n    list : list of truncated sequence keeping the origin order\n    \"\"\"", "\n", "assert", "isinstance", "(", "seqs", ",", "list", ")", "\n", "lens", "=", "list", "(", "map", "(", "len", ",", "seqs", ")", ")", "\n", "if", "sum", "(", "lens", ")", "<=", "max_len", ":", "\n", "        ", "return", "seqs", "\n", "\n", "", "lens", "=", "ma", ".", "masked_array", "(", "lens", ",", "mask", "=", "[", "0", "]", "*", "len", "(", "lens", ")", ")", "\n", "while", "True", ":", "\n", "        ", "argmin", "=", "lens", ".", "argmin", "(", ")", "\n", "minval", "=", "lens", "[", "argmin", "]", "\n", "quotient", ",", "remainder", "=", "divmod", "(", "max_len", ",", "len", "(", "lens", ")", "-", "sum", "(", "lens", ".", "mask", ")", ")", "\n", "if", "minval", "<=", "quotient", ":", "# Ignore values that don't need truncation", "\n", "            ", "lens", ".", "mask", "[", "argmin", "]", "=", "1", "\n", "max_len", "-=", "minval", "\n", "", "else", ":", "# Truncate all", "\n", "            ", "lens", ".", "data", "[", "~", "lens", ".", "mask", "]", "=", "[", "\n", "quotient", "+", "1", "if", "i", "<", "remainder", "else", "quotient", "for", "i", "in", "range", "(", "lens", ".", "count", "(", ")", ")", "\n", "]", "\n", "break", "\n", "", "", "seqs", "=", "[", "seq", "[", ":", "length", "]", "for", "(", "seq", ",", "length", ")", "in", "zip", "(", "seqs", ",", "lens", ".", "data", ".", "tolist", "(", ")", ")", "]", "\n", "return", "seqs", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.concat_sequences": [[61, 132], ["isinstance", "isinstance", "sum", "sum", "isinstance", "isinstance", "sum", "isinstance", "len", "itertools.zip_longest", "enumerate", "len", "len", "itertools.zip_longest", "len", "len", "itertools.zip_longest"], "function", ["None"], ["", "def", "concat_sequences", "(", "seqs", ",", "separators", ",", "seq_mask", "=", "0", ",", "separator_mask", "=", "1", ")", ":", "\n", "    ", "\"\"\"Concatenate sequences in a list into a single sequence, using specified separators.\n\n    Example 1:\n    seqs: [['is', 'this' ,'jacksonville', '?'], ['no' ,'it' ,'is' ,'not', '.']]\n    separator: [[SEP], [SEP], [CLS]]\n    seq_mask: 0\n    separator_mask: 1\n    Returns:\n       tokens:      is this jacksonville ? [SEP] no it is not . [SEP] [CLS]\n       segment_ids: 0  0    0            0  0    1  1  1  1   1 1     2\n       p_mask:      0  0    0            0  1    0  0  0  0   0 1     1\n\n    Example 2:\n    separator_mask can also be a list.\n    seqs: [['is', 'this' ,'jacksonville', '?'], ['no' ,'it' ,'is' ,'not', '.']]\n    separator: [[SEP], [SEP], [CLS]]\n    seq_mask: 0\n    separator_mask: [[1], [1], [0]]\n\n    Returns:\n       tokens:     'is this jacksonville ? [SEP] no it is not . [SEP] [CLS]'\n       segment_ids: 0  0    0            0  0    1  1  1  1   1 1     2\n       p_mask:      1  1    1            1  1    0  0  0  0   0 1     0\n\n    Example 3:\n    seq_mask can also be a list.\n    seqs: [['is', 'this' ,'jacksonville', '?'], ['no' ,'it' ,'is' ,'not', '.']]\n    separator: [[SEP], [SEP], [CLS]]\n    seq_mask: [[1, 1, 1, 1], [0, 0, 0, 0, 0]]\n    separator_mask: [[1], [1], [0]]\n\n    Returns:\n       tokens:     'is this jacksonville ? [SEP] no it is not . [SEP] [CLS]'\n       segment_ids: 0  0    0            0  0    1  1  1  1   1 1     2\n       p_mask:      1  1    1            1  1    0  0  0  0   0 1     0\n\n    Parameters\n    ----------\n    seqs : list\n        sequences to be concatenated\n    separator : list\n        The special tokens to separate sequences.\n    seq_mask : int or list\n        A single mask value for all sequence items or a list of values for each item in sequences\n    separator_mask : int or list\n        A single mask value for all separators or a list of values for each separator\n\n    Returns\n    -------\n    np.array: input token ids in 'int32', shape (batch_size, seq_length)\n    np.array: segment ids in 'int32', shape (batch_size, seq_length)\n    np.array: mask for special tokens\n    \"\"\"", "\n", "assert", "isinstance", "(", "seqs", ",", "collections", ".", "abc", ".", "Iterable", ")", "and", "len", "(", "seqs", ")", ">", "0", "\n", "assert", "isinstance", "(", "seq_mask", ",", "(", "list", ",", "int", ")", ")", "\n", "assert", "isinstance", "(", "separator_mask", ",", "(", "list", ",", "int", ")", ")", "\n", "concat", "=", "sum", "(", "(", "seq", "+", "sep", "for", "sep", ",", "seq", "in", "itertools", ".", "zip_longest", "(", "separators", ",", "seqs", ",", "fillvalue", "=", "[", "]", ")", ")", ",", "\n", "[", "]", ")", "\n", "segment_ids", "=", "sum", "(", "\n", "(", "[", "i", "]", "*", "(", "len", "(", "seq", ")", "+", "len", "(", "sep", ")", ")", "\n", "for", "i", ",", "(", "sep", ",", "seq", ")", "in", "enumerate", "(", "itertools", ".", "zip_longest", "(", "separators", ",", "seqs", ",", "fillvalue", "=", "[", "]", ")", ")", ")", ",", "\n", "[", "]", ")", "\n", "if", "isinstance", "(", "seq_mask", ",", "int", ")", ":", "\n", "        ", "seq_mask", "=", "[", "[", "seq_mask", "]", "*", "len", "(", "seq", ")", "for", "seq", "in", "seqs", "]", "\n", "", "if", "isinstance", "(", "separator_mask", ",", "int", ")", ":", "\n", "        ", "separator_mask", "=", "[", "[", "separator_mask", "]", "*", "len", "(", "sep", ")", "for", "sep", "in", "separators", "]", "\n", "\n", "", "p_mask", "=", "sum", "(", "(", "s_mask", "+", "mask", "for", "sep", ",", "seq", ",", "s_mask", ",", "mask", "in", "itertools", ".", "zip_longest", "(", "\n", "separators", ",", "seqs", ",", "seq_mask", ",", "separator_mask", ",", "fillvalue", "=", "[", "]", ")", ")", ",", "[", "]", ")", "\n", "return", "concat", ",", "segment_ids", ",", "p_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.tokenize_and_align_positions": [[134, 168], ["enumerate", "orig_to_tok_index.append", "tokenizer", "len", "len", "len", "len"], "function", ["None"], ["", "def", "tokenize_and_align_positions", "(", "origin_text", ",", "start_position", ",", "end_position", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Tokenize the text and align the origin positions to the corresponding position.\n\n    Parameters\n    ----------\n    origin_text : list\n        list of tokens to be tokenized.\n    start_position : int\n        Start position in the origin_text\n    end_position : int\n        End position in the origin_text\n    tokenizer : callable function, e.g., BERTTokenizer.\n\n    Returns\n    -------\n    int: Aligned start position\n    int: Aligned end position\n    list: tokenized text\n    list: map from the origin index to the tokenized sequence index\n    list: map from tokenized sequence index to the origin index\n    \"\"\"", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "tok_to_orig_index", "=", "[", "]", "\n", "tokenized_text", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "origin_text", ")", ":", "\n", "        ", "orig_to_tok_index", ".", "append", "(", "len", "(", "tokenized_text", ")", ")", "\n", "sub_tokens", "=", "tokenizer", "(", "token", ")", "\n", "tokenized_text", "+=", "sub_tokens", "\n", "tok_to_orig_index", "+=", "[", "i", "]", "*", "len", "(", "sub_tokens", ")", "\n", "\n", "", "start_position", "=", "orig_to_tok_index", "[", "start_position", "]", "\n", "end_position", "=", "orig_to_tok_index", "[", "end_position", "+", "1", "]", "-", "1", "if", "end_position", "<", "len", "(", "origin_text", ")", "-", "1", "else", "len", "(", "tokenized_text", ")", "-", "1", "\n", "return", "start_position", ",", "end_position", ",", "tokenized_text", ",", "orig_to_tok_index", ",", "tok_to_orig_index", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.get_doc_spans": [[170, 198], ["list", "len", "min", "doc_spans.append", "min", "zip", "len", "len"], "function", ["None"], ["", "def", "get_doc_spans", "(", "full_doc", ",", "max_length", ",", "doc_stride", ")", ":", "\n", "    ", "\"\"\"Obtain document spans by sliding a window across the document\n\n    Parameters\n    ----------\n    full_doc: list\n        The origin doc text\n    max_length: max_length\n        Maximum size of a doc span\n    doc_stride: int\n        Step of sliding window\n\n    Returns\n    -------\n    list: a list of processed doc spans\n    list: a list of start/end index of each doc span\n    \"\"\"", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "full_doc", ")", ":", "\n", "        ", "length", "=", "min", "(", "max_length", ",", "len", "(", "full_doc", ")", "-", "start_offset", ")", "\n", "end_offset", "=", "start_offset", "+", "length", "\n", "doc_spans", ".", "append", "(", "\n", "(", "full_doc", "[", "start_offset", ":", "end_offset", "]", ",", "(", "start_offset", ",", "end_offset", ")", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "full_doc", ")", ":", "\n", "            ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "", "return", "list", "(", "zip", "(", "*", "doc_spans", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.align_position2doc_spans": [[200, 235], ["isinstance", "all", "len", "range", "range"], "function", ["None"], ["", "def", "align_position2doc_spans", "(", "positions", ",", "doc_spans_indices", ",", "offset", "=", "0", ",", "default_value", "=", "-", "1", ",", "\n", "all_in_span", "=", "True", ")", ":", "\n", "    ", "\"\"\"Align original positions to the corresponding document span positions\n\n    Parameters\n    ----------\n    positions: list or int\n        A single or a list of positions to be aligned\n    dic_spans_indices: list or tuple\n        (start_position, end_position)\n    offset: int\n        Offset of aligned positions. Sometimes the doc spans would be added\n        after a question text, in this case, the new position should add\n        len(question_text)\n    default_value: int\n        The default value to return if the positions are not in the doc span.\n    all_in_span: bool\n        If set to True, then as long as one position is out of span, all positions\n        would be set to default_value.\n\n    Returns\n    -------\n    list: a list of aligned positions\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "positions", ",", "list", ")", ":", "\n", "        ", "positions", "=", "[", "positions", "]", "\n", "", "doc_start", ",", "doc_end", "=", "doc_spans_indices", "\n", "if", "all_in_span", "and", "not", "all", "(", "[", "p", "in", "range", "(", "doc_start", ",", "doc_end", ")", "for", "p", "in", "positions", "]", ")", ":", "\n", "        ", "return", "[", "default_value", "]", "*", "len", "(", "positions", ")", "\n", "", "new_positions", "=", "[", "\n", "p", "-", "doc_start", "+", "\n", "offset", "if", "p", "in", "range", "(", "doc_start", ",", "doc_end", ")", "else", "default_value", "\n", "for", "p", "in", "positions", "\n", "]", "\n", "return", "new_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._improve_answer_span": [[237, 249], ["range", "tokenizer", "range"], "function", ["None"], ["", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "tok_answer_text", "=", "' '", ".", "join", "(", "tokenizer", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "' '", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._check_is_max_context": [[251, 287], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils.convert_squad_examples": [[295, 350], ["len", "SquadExample", "str.isspace", "char_to_word_offset.append", "len", "doc_tokens.append", "len", "len"], "function", ["None"], ["def", "convert_squad_examples", "(", "record", ",", "is_training", ")", ":", "\n", "    ", "\"\"\"read a single entry of gluonnlp.data.SQuAD and convert it to an example.\n\n    Parameters\n    ----------\n    record: list\n        An entry of gluonnlp.data.SQuAD\n    is_training: bool\n        If the example is used for training,\n        then a rough start/end position will be generated\n\n    Returns\n    -------\n    SquadExample: An instance of SquadExample\n    \"\"\"", "\n", "example_id", "=", "record", "[", "0", "]", "\n", "qas_id", "=", "record", "[", "1", "]", "\n", "question_text", "=", "record", "[", "2", "]", "\n", "paragraph_text", "=", "record", "[", "3", "]", "\n", "orig_answer_text", "=", "record", "[", "4", "]", "[", "0", "]", "if", "record", "[", "4", "]", "else", "''", "\n", "answer_offset", "=", "record", "[", "5", "]", "[", "0", "]", "if", "record", "[", "5", "]", "else", "''", "\n", "is_impossible", "=", "record", "[", "6", "]", "if", "len", "(", "record", ")", "==", "7", "else", "False", "\n", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "doc_tokens", "=", "[", "]", "\n", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "str", ".", "isspace", "(", "c", ")", ":", "\n", "            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "prev_is_whitespace", ":", "\n", "                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "if", "not", "is_training", ":", "\n", "        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "start_position", "=", "char_to_word_offset", "[", "\n", "answer_offset", "]", "if", "not", "is_impossible", "else", "-", "1", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "\n", "1", "]", "if", "not", "is_impossible", "else", "-", "1", "\n", "", "answer_offset", "=", "-", "1", "if", "is_impossible", "else", "answer_offset", "\n", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "question_text", "=", "question_text", ",", "paragraph_text", "=", "paragraph_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "example_id", "=", "example_id", ",", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "end_position", "=", "end_position", ",", "start_offset", "=", "answer_offset", ",", "\n", "end_offset", "=", "answer_offset", "+", "len", "(", "orig_answer_text", ")", "-", "1", ",", "is_impossible", "=", "is_impossible", ")", "\n", "return", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._preprocess_text": [[352, 380], ["outputs.lower.replace().replace", "unicodedata.normalize", "outputs.lower.lower", "inputs.strip().split", "outputs.lower.replace", "inputs.strip", "unicodedata.combining"], "function", ["None"], ["", "def", "_preprocess_text", "(", "inputs", ",", "lower", "=", "False", ",", "remove_space", "=", "True", ",", "keep_accents", "=", "False", ")", ":", "\n", "    ", "\"\"\"Remove space, convert to lower case, keep accents.\n\n    Parameters\n    ----------\n    inputs: str\n        input string\n    lower: bool\n        If convert the input string to lower case.\n    remove_space: bool\n        If remove the spaces in the input string.\n    keep_accents: bool\n        If keep accents in the input string.\n    Returns\n    -------\n    str: processed input string\n    \"\"\"", "\n", "if", "remove_space", ":", "\n", "        ", "outputs", "=", "' '", ".", "join", "(", "inputs", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "outputs", "=", "inputs", "\n", "", "outputs", "=", "outputs", ".", "replace", "(", "'``'", ",", "'\"'", ")", ".", "replace", "(", "'\\'\\''", ",", "'\"'", ")", "\n", "if", "not", "keep_accents", ":", "\n", "        ", "outputs", "=", "unicodedata", ".", "normalize", "(", "'NFKD'", ",", "outputs", ")", "\n", "outputs", "=", "''", ".", "join", "(", "[", "c", "for", "c", "in", "outputs", "if", "not", "unicodedata", ".", "combining", "(", "c", ")", "]", ")", "\n", "", "if", "lower", ":", "\n", "        ", "outputs", "=", "outputs", ".", "lower", "(", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._convert_index": [[382, 418], ["len"], "function", ["None"], ["", "def", "_convert_index", "(", "index", ",", "pos", ",", "M", "=", "None", ",", "is_start", "=", "True", ")", ":", "\n", "    ", "\"\"\"Working best with _lcs_match(), convert the token index to origin text index\"\"\"", "\n", "if", "index", "[", "pos", "]", "is", "not", "None", ":", "\n", "        ", "return", "index", "[", "pos", "]", "\n", "", "N", "=", "len", "(", "index", ")", "\n", "rear", "=", "pos", "\n", "while", "rear", "<", "N", "-", "1", "and", "index", "[", "rear", "]", "is", "None", ":", "\n", "        ", "rear", "+=", "1", "\n", "", "front", "=", "pos", "\n", "while", "front", ">", "0", "and", "index", "[", "front", "]", "is", "None", ":", "\n", "        ", "front", "-=", "1", "\n", "", "assert", "index", "[", "front", "]", "is", "not", "None", "or", "index", "[", "rear", "]", "is", "not", "None", "\n", "if", "index", "[", "front", "]", "is", "None", ":", "\n", "        ", "if", "index", "[", "rear", "]", ">=", "1", ":", "\n", "            ", "if", "is_start", ":", "\n", "                ", "return", "0", "\n", "", "else", ":", "\n", "                ", "return", "index", "[", "rear", "]", "-", "1", "\n", "", "", "return", "index", "[", "rear", "]", "\n", "", "if", "index", "[", "rear", "]", "is", "None", ":", "\n", "        ", "if", "M", "is", "not", "None", "and", "index", "[", "front", "]", "<", "M", "-", "1", ":", "\n", "            ", "if", "is_start", ":", "\n", "                ", "return", "index", "[", "front", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "return", "M", "-", "1", "\n", "", "", "return", "index", "[", "front", "]", "\n", "", "if", "is_start", ":", "\n", "        ", "if", "index", "[", "rear", "]", ">", "index", "[", "front", "]", "+", "1", ":", "\n", "            ", "return", "index", "[", "front", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "return", "index", "[", "rear", "]", "\n", "", "", "else", ":", "\n", "        ", "if", "index", "[", "rear", "]", ">", "index", "[", "front", "]", "+", "1", ":", "\n", "            ", "return", "index", "[", "rear", "]", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "index", "[", "front", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._lcs_match": [[420, 466], ["numpy.zeros", "enumerate", "range", "max", "max", "len", "len", "len", "preprocessing_utils._preprocess_text"], "function", ["home.repos.pwc.inspect_result.alexa_bort.data.preprocessing_utils._preprocess_text"], ["", "", "", "def", "_lcs_match", "(", "max_dist", ",", "seq1", ",", "seq2", ",", "max_seq_length", "=", "1024", ",", "lower", "=", "False", ")", ":", "\n", "    ", "\"\"\"Longest common sequence match.\n\n    unlike standard LCS, this is specifically optimized for the setting\n    because the mismatch between sentence pieces and original text will be small\n\n    Parameters\n    ----------\n    max_dist: int\n        The max distance between tokens to be considered.\n    seq1: list\n        The first sequence to be matched.\n    seq2: list\n        The second sequence to be matched.\n    lower: bool\n        If match the lower-cased tokens.\n    Returns\n    -------\n    numpyArray: Token-wise lcs matrix f. Shape of ((max(len(seq1), 1024), max(len(seq2), 1024))\n    Map: The dp path in matrix f.\n        g[(i ,j)] == 2 if token_i in seq1 matches token_j in seq2.\n        g[(i, j)] == 1 if token_i in seq1 matches token_{j-1} in seq2.\n        g[(i, j)] == 0 of token_{i-1} in seq1 matches token_j in seq2.\n    \"\"\"", "\n", "f", "=", "np", ".", "zeros", "(", "(", "max", "(", "len", "(", "seq1", ")", ",", "max_seq_length", ")", ",", "max", "(", "len", "(", "seq2", ")", ",", "max_seq_length", ")", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "g", "=", "{", "}", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "seq1", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "i", "-", "max_dist", ",", "i", "+", "max_dist", ")", ":", "\n", "            ", "if", "j", ">=", "len", "(", "seq2", ")", "or", "j", "<", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "i", ">", "0", ":", "\n", "                ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "0", "\n", "f", "[", "i", ",", "j", "]", "=", "f", "[", "i", "-", "1", ",", "j", "]", "\n", "\n", "", "if", "j", ">", "0", "and", "f", "[", "i", ",", "j", "-", "1", "]", ">", "f", "[", "i", ",", "j", "]", ":", "\n", "                ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "1", "\n", "f", "[", "i", ",", "j", "]", "=", "f", "[", "i", ",", "j", "-", "1", "]", "\n", "\n", "", "f_prev", "=", "f", "[", "i", "-", "1", ",", "j", "-", "1", "]", "if", "i", ">", "0", "and", "j", ">", "0", "else", "0", "\n", "if", "(", "_preprocess_text", "(", "token", ",", "lower", "=", "lower", ",", "remove_space", "=", "False", ")", "==", "seq2", "[", "j", "]", "\n", "and", "f_prev", "+", "1", ">", "f", "[", "i", ",", "j", "]", ")", ":", "\n", "                ", "g", "[", "(", "i", ",", "j", ")", "]", "=", "2", "\n", "f", "[", "i", ",", "j", "]", "=", "f_prev", "+", "1", "\n", "", "", "", "return", "f", ",", "g", "\n", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.glue._GlueDataset.__init__": [[35, 44], ["os.path.expanduser", "os.path.join", "glue._GlueDataset._get_data", "gluonnlp.data.dataset.TSVDataset.__init__", "os.path.isdir", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue._SuperGlueDataset._get_data", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "data_file", ",", "**", "kwargs", ")", ":", "\n", "        ", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "root", ")", "\n", "", "segment", ",", "zip_hash", ",", "data_hash", "=", "data_file", "\n", "self", ".", "_root", "=", "root", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "self", ".", "_get_data", "(", "segment", ",", "zip_hash", ",", "data_hash", ",", "filename", ")", "\n", "super", "(", "_GlueDataset", ",", "self", ")", ".", "__init__", "(", "filename", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue._GlueDataset._get_data": [[45, 60], ["os.path.exists", "mxnet.gluon.utils.download", "os.path.join", "mxnet.gluon.utils._get_repo_file_url", "zipfile.ZipFile", "zf.infolist", "glue._GlueDataset._repo_dir", "os.path.basename", "zf.extract"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._repo_dir"], ["", "def", "_get_data", "(", "self", ",", "segment", ",", "zip_hash", ",", "data_hash", ",", "filename", ")", ":", "\n", "        ", "data_filename", "=", "'%s-%s.zip'", "%", "(", "segment", ",", "data_hash", "[", ":", "8", "]", ")", "\n", "# or not check_sha1(filename, data_hash):", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "download", "(", "_get_repo_file_url", "(", "self", ".", "_repo_dir", "(", ")", ",", "data_filename", ")", ",", "\n", "path", "=", "self", ".", "_root", ",", "sha1_hash", "=", "zip_hash", ")", "\n", "# unzip", "\n", "downloaded_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "data_filename", ")", "\n", "with", "zipfile", ".", "ZipFile", "(", "downloaded_path", ",", "'r'", ")", "as", "zf", ":", "\n", "# skip dir structures in the zip", "\n", "                ", "for", "zip_info", "in", "zf", ".", "infolist", "(", ")", ":", "\n", "                    ", "if", "zip_info", ".", "filename", "[", "-", "1", "]", "==", "'/'", ":", "\n", "                        ", "continue", "\n", "", "zip_info", ".", "filename", "=", "os", ".", "path", ".", "basename", "(", "zip_info", ".", "filename", ")", "\n", "zf", ".", "extract", "(", "zip_info", ",", "self", ".", "_root", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue._GlueDataset._repo_dir": [[61, 63], ["None"], "methods", ["None"], ["", "", "", "", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueCoLA.__init__": [[106, 129], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_cola'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'662227ed4d98bb96b3495234b650e37826a5ef72'", ",", "\n", "'7760a9c4b1fb05f6d003475cc7bb0d0118875190'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'6f3f5252b004eab187bf22ab5b0af31e739d3a3f'", ",", "\n", "'30ece4de38e1929545c4154d4c71ad297c7f54b4'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'b88180515ad041935793e74e3a76470b0c1b2c50'", ",", "\n", "'f38b43d31bb06accf82a3d5b2fe434a752a74c9f'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "LABEL_IDX", "=", "3", ",", "1", "\n", "field_indices", "=", "[", "\n", "A_IDX", ",", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "0", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", "=", "1", "\n", "field_indices", "=", "[", "A_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "\n", "", "super", "(", "GlueCoLA", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueCoLA._repo_dir": [[130, 132], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/CoLA'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueSST2.__init__": [[172, 195], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_sst'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'bcde781bed5caa30d5e9a9d24e5c826965ed02a2'", ",", "\n", "'ffbb67a55e27525e925b79fee110ca19585d70ca'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'85698e465ff6573fb80d0b34229c76df84cd766b'", ",", "\n", "'e166f986cec68fd4cca0ae5ce5869b917f88a2fa'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'efac1c275553ed78500e9b8d8629408f5f867b20'", ",", "\n", "'3ce8041182bf82dbbbbfe13738b39d3c69722744'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "LABEL_IDX", "=", "0", ",", "1", "\n", "field_indices", "=", "[", "\n", "A_IDX", ",", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", "=", "1", "\n", "field_indices", "=", "[", "A_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "\n", "", "super", "(", "GlueSST2", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueSST2._repo_dir": [[196, 198], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/SST-2'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueSTSB.__init__": [[241, 264], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_stsb'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'9378bd341576810730a5c666ed03122e4c5ecc9f'", ",", "\n", "'501e55248c6db2a3f416c75932a63693000a82bc'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'529c3e7c36d0807d88d0b2a5d4b954809ddd4228'", ",", "\n", "'f8bcc33b01dfa2e9ba85601d0140020735b8eff3'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'6284872d6992d8ec6d96320af89c2f46ac076d18'", ",", "\n", "'36553e5e2107b817257232350e95ff0f3271d844'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "7", ",", "8", ",", "9", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "7", ",", "8", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "\n", "", "super", "(", "GlueSTSB", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueSTSB._repo_dir": [[265, 267], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/STS-B'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueQQP.__init__": [[311, 334], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_qqp'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'494f280d651f168ad96d6cd05f8d4ddc6be73ce9'", ",", "\n", "'95c01e711ac8dbbda8f67f3a4291e583a72b6988'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'9957b60c4c62f9b98ec91b26a9d43529d2ee285d'", ",", "\n", "'755e0bf2899b8ad315d4bd7d4c85ec51beee5ad0'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'1e325cc5dbeeb358f9429c619ebe974fc2d1a8ca'", ",", "\n", "'0f50d1a62dd51fe932ba91be08238e47c3e2504a'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "3", ",", "4", ",", "5", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "1", ",", "2", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "# QQP may include broken samples", "\n", "", "super", "(", "GlueQQP", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ",", "allow_missing", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueQQP._repo_dir": [[335, 337], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/QQP'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueRTE.__init__": [[377, 399], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_rte'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'a23b0633f4f4dfa866c672af2e94f7e07344888f'", ",", "\n", "'ec2b246745bb5c9d92aee0800684c08902742730'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'a6cde090d12a10744716304008cf33dd3f0dbfcb'", ",", "\n", "'ade75e0673862dcac9c653efb9f59f51be2749aa'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'7e4e58a6fa80b1f05e603b4e220524be7976b488'", ",", "\n", "'ddda5c967fb5a4934b429bb52aaa144e70900000'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "1", ",", "2", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "super", "(", "GlueRTE", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueRTE._repo_dir": [[400, 402], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/RTE'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMNLI.__init__": [[444, 479], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_mnli'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'aa235064ab3ce47d48caa17c553561d84fdf5bf2'", ",", "\n", "'1e74055bc91e260323574bfe63186acb9420fa13'", ")", ",", "\n", "'dev_matched'", ":", "(", "'dev_matched'", ",", "\n", "'328cf527add50ee7bc20a862f97913800ba8a4b1'", ",", "\n", "'7a38c5fb5ecc875f259e1d57662d58a984753b70'", ")", ",", "\n", "'dev_mismatched'", ":", "(", "'dev_mismatched'", ",", "\n", "'9c5d6c6d2e3a676bfa19d929b32e2f9f233585c5'", ",", "\n", "'47470d91b594e767d80e5de2ef0be6a453c17be5'", ")", ",", "\n", "'test_matched'", ":", "(", "'test_matched'", ",", "\n", "'53877d9d554b6a6d402cc0e5f7e38366cd4f8e60'", ",", "\n", "'00106769e11a43eac119975ad25c2de2c8d2dbe7'", ")", ",", "\n", "'test_mismatched'", ":", "(", "'test_mismatched'", ",", "\n", "'82b03d3cc9f4a59c74beab06c141bc0c5bf74a55'", ",", "\n", "'5a31abf92f045f127dbb2e3d2e0ef8ddea04c237'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "8", ",", "9", ",", "11", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "in", "[", "'dev_matched'", ",", "'dev_mismatched'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "8", ",", "9", ",", "15", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "in", "[", "'test_matched'", ",", "'test_mismatched'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "8", ",", "9", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "super", "(", "GlueMNLI", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMNLI._repo_dir": [[480, 482], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/MNLI'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueQNLI.__init__": [[523, 545], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_qnli'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'95fae96fb1ffa6a2804192c9036d3435e63b48e8'", ",", "\n", "'d90a84eb40c6ba32bc2b34284ceaa962c46f8753'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'5652b9d4d5c8d115c080bcf64101927ea2b3a1e0'", ",", "\n", "'d14a61290301c2a9d26459c4cd036742e8591428'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'23dfb2f38adb14d3e792dbaecb7f5fd5dfa8db7e'", ",", "\n", "'f3da1a2e471ebfee81d91574b42e0f5d39153c59'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "1", ",", "2", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "super", "(", "GlueQNLI", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueQNLI._repo_dir": [[546, 548], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/QNLI'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueWNLI.__init__": [[588, 610], ["os.path.join", "glue._GlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_wnli'", ")", ",", "\n", "return_all_fields", "=", "False", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'8db0004d0e58640751a9f2875dd66c8000504ddb'", ",", "\n", "'b497281c1d848b619ea8fe427b3a6e4dc8e7fa92'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'d54834960555073fb497cf2766edb77fb62c3646'", ",", "\n", "'6bbdb866d0cccaac57c3a2505cf53103789b69a9'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'431e596a1c6627fb168e7741b3e32ef681da3c7b'", ",", "\n", "'6ba8fcf3e5b451c101a3902fb4ba3fc1dea42e50'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "1", ",", "2", ",", "3", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", ",", "\n", "LABEL_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "A_IDX", ",", "B_IDX", ",", "=", "1", ",", "2", "\n", "field_indices", "=", "[", "A_IDX", ",", "B_IDX", "]", "if", "not", "return_all_fields", "else", "None", "\n", "num_discard_samples", "=", "1", "\n", "", "super", "(", "GlueWNLI", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "\n", "num_discard_samples", "=", "num_discard_samples", ",", "\n", "field_indices", "=", "field_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueWNLI._repo_dir": [[611, 613], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/GLUE/WNLI'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMRPC.__init__": [[650, 675], ["os.path.join", "glue.GlueMRPC._generate", "os.path.join", "gluonnlp.data.dataset.TSVDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMRPC._generate", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "\n", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'glue_mrpc'", ")", ")", ":", "\n", "        ", "self", ".", "_root", "=", "root", "\n", "assert", "segment", "in", "[", "'train'", ",", "'dev'", ",", "\n", "'test'", "]", ",", "'Unsupported segment: %s'", "%", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'msr_paraphrase_train.txt'", ",", "\n", "'716e0f67af962f08220b7e97d229b293077ef41f'", ",", "\n", "'131675ffd3d2f04f286049d31cca506c8acba69e'", ")", ",", "\n", "'dev'", ":", "(", "'msr_paraphrase_train.txt'", ",", "\n", "'716e0f67af962f08220b7e97d229b293077ef41f'", ",", "\n", "'e4486577c4cb2e5c2a3fd961eb24f03c623ea02d'", ")", ",", "\n", "'test'", ":", "(", "'msr_paraphrase_test.txt'", ",", "\n", "'4265196c15cf75620b0b592b8b921f543bda7e6c'", ",", "\n", "'3602b2ca26cf574e84183c14d6c0901669ee2d0a'", ")", "}", "\n", "\n", "self", ".", "_generate", "(", "segment", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "'%s.tsv'", "%", "segment", ")", "\n", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "=", "3", ",", "4", ",", "0", "\n", "if", "segment", "==", "'test'", ":", "\n", "            ", "fields", "=", "[", "A_IDX", ",", "B_IDX", "]", "\n", "", "else", ":", "\n", "            ", "fields", "=", "[", "A_IDX", ",", "B_IDX", ",", "LABEL_IDX", "]", "\n", "", "super", "(", "GlueMRPC", ",", "self", ")", ".", "__init__", "(", "\n", "path", ",", "num_discard_samples", "=", "1", ",", "field_indices", "=", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMRPC._repo_dir": [[676, 678], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'https://dl.fbaipublicfiles.com/senteval/senteval_data/'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.glue.GlueMRPC._generate": [[679, 739], ["os.path.join", "mxnet.gluon.utils.download", "os.path.join", "os.path.isfile", "os.path.join", "mxnet.gluon.utils.download", "os.path.join", "os.path.join", "glue.GlueMRPC._repo_dir", "io.open", "io.open", "os.path.isfile", "mxnet.gluon.utils.check_sha1", "io.open", "dev_ids.append", "io.open", "io.open", "data_fh.readline", "test_fh.write", "enumerate", "row.strip().split", "io.open", "data_fh.readline", "train_fh.write", "dev_fh.write", "row.strip().split", "test_fh.write", "row.strip().split", "row.strip", "dev_fh.write", "train_fh.write", "row.strip", "row.strip"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._repo_dir"], ["", "def", "_generate", "(", "self", ",", "segment", ")", ":", "\n", "        ", "\"\"\"Partition MRPC dataset into train, dev and test.\n        Adapted from https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\n        \"\"\"", "\n", "# download raw data", "\n", "data_name", "=", "segment", "+", "'.tsv'", "\n", "raw_name", ",", "raw_hash", ",", "data_hash", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "raw_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "raw_name", ")", "\n", "download", "(", "self", ".", "_repo_dir", "(", ")", "+", "raw_name", ",", "\n", "path", "=", "raw_path", ",", "sha1_hash", "=", "raw_hash", ")", "\n", "data_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "data_name", ")", "\n", "\n", "if", "segment", "in", "(", "'train'", ",", "'dev'", ")", ":", "\n", "# and check_sha1(data_path, data_hash):", "\n", "            ", "if", "os", ".", "path", ".", "isfile", "(", "data_path", ")", ":", "\n", "                ", "return", "\n", "\n", "# retrieve dev ids for train and dev set", "\n", "", "DEV_ID_URL", "=", "'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc'", "\n", "DEV_ID_HASH", "=", "'506c7a1a5e0dd551ceec2f84070fa1a8c2bc4b41'", "\n", "dev_id_name", "=", "'dev_ids.tsv'", "\n", "dev_id_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "dev_id_name", ")", "\n", "download", "(", "DEV_ID_URL", ",", "path", "=", "dev_id_path", ",", "sha1_hash", "=", "DEV_ID_HASH", ")", "\n", "\n", "# read dev data ids", "\n", "dev_ids", "=", "[", "]", "\n", "with", "io", ".", "open", "(", "dev_id_path", ",", "encoding", "=", "'utf8'", ")", "as", "ids_fh", ":", "\n", "                ", "for", "row", "in", "ids_fh", ":", "\n", "                    ", "dev_ids", ".", "append", "(", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ")", "\n", "\n", "# generate train and dev set", "\n", "", "", "train_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "'train.tsv'", ")", "\n", "dev_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "'dev.tsv'", ")", "\n", "with", "io", ".", "open", "(", "raw_path", ",", "encoding", "=", "'utf8'", ")", "as", "data_fh", ":", "\n", "                ", "with", "io", ".", "open", "(", "train_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "train_fh", ":", "\n", "                    ", "with", "io", ".", "open", "(", "dev_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "dev_fh", ":", "\n", "                        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "train_fh", ".", "write", "(", "header", ")", "\n", "dev_fh", ".", "write", "(", "header", ")", "\n", "for", "row", "in", "data_fh", ":", "\n", "                            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "example", "=", "'%s\\t%s\\t%s\\t%s\\t%s\\n'", "%", "(", "\n", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", "\n", "if", "[", "id1", ",", "id2", "]", "in", "dev_ids", ":", "\n", "                                ", "dev_fh", ".", "write", "(", "example", ")", "\n", "", "else", ":", "\n", "                                ", "train_fh", ".", "write", "(", "example", ")", "\n", "", "", "", "", "", "", "else", ":", "\n", "# generate test set", "\n", "            ", "if", "os", ".", "path", ".", "isfile", "(", "data_path", ")", "and", "check_sha1", "(", "data_path", ",", "data_hash", ")", ":", "\n", "                ", "return", "\n", "", "with", "io", ".", "open", "(", "raw_path", ",", "encoding", "=", "'utf8'", ")", "as", "data_fh", ":", "\n", "                ", "with", "io", ".", "open", "(", "data_path", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "test_fh", ":", "\n", "                    ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "test_fh", ".", "write", "(", "\n", "'index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n'", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "data_fh", ")", ":", "\n", "                        ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "test_fh", ".", "write", "(", "'%d\\t%s\\t%s\\t%s\\t%s\\n'", "%", "\n", "(", "idx", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.__init__": [[59, 65], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "class_labels", ",", "metrics", ",", "is_pair", ",", "output_format", "=", "\"tsv\"", ",", "label_alias", "=", "None", ")", ":", "\n", "        ", "self", ".", "class_labels", "=", "class_labels", "\n", "self", ".", "metrics", "=", "metrics", "\n", "self", ".", "is_pair", "=", "is_pair", "\n", "self", ".", "label_alias", "=", "label_alias", "\n", "self", ".", "output_format", "=", "output_format", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.get_dataset": [[66, 79], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for the task.\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments.\n\n        Returns\n        -------\n        Dataset : the dataset of target segment.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.dataset_train": [[80, 88], ["classification.BaseClassificationTask.get_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset"], ["", "def", "dataset_train", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the training segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        \"\"\"", "\n", "return", "'train'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.dataset_dev": [[89, 97], ["classification.BaseClassificationTask.get_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset"], ["", "def", "dataset_dev", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the dev segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of (str, Dataset), or list of tuple : the segment name, and the dataset.\n        \"\"\"", "\n", "return", "'dev'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BaseClassificationTask.dataset_test": [[98, 106], ["classification.BaseClassificationTask.get_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset"], ["", "def", "dataset_test", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the test segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of (str, Dataset), or list of tuple : the segment name, and the dataset.\n        \"\"\"", "\n", "return", "'test'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MRPCTask.__init__": [[111, 118], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.CompositeEvalMetric.add", "mxnet.metric.CompositeEvalMetric.add", "classification.BaseClassificationTask.__init__", "mxnet.metric.F1", "mxnet.metric.Accuracy"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "metric", ".", "add", "(", "F1", "(", ")", ")", "\n", "metric", ".", "add", "(", "Accuracy", "(", ")", ")", "\n", "super", "(", "MRPCTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MRPCTask.get_dataset": [[119, 128], ["glue.GlueMRPC"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for MRPC.\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueMRPC", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.QQPTask.__init__": [[133, 140], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.CompositeEvalMetric.add", "mxnet.metric.CompositeEvalMetric.add", "classification.BaseClassificationTask.__init__", "mxnet.metric.F1", "mxnet.metric.Accuracy"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "metric", ".", "add", "(", "F1", "(", ")", ")", "\n", "metric", ".", "add", "(", "Accuracy", "(", ")", ")", "\n", "super", "(", "QQPTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.QQPTask.get_dataset": [[141, 150], ["glue.GlueQQP"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for QQP.\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueQQP", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RTETask.__init__": [[155, 160], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "RTETask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RTETask.get_dataset": [[161, 170], ["glue.GlueRTE"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for RTE.\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueRTE", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.QNLITask.__init__": [[175, 180], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "QNLITask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.QNLITask.get_dataset": [[181, 190], ["glue.GlueQNLI"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for QNLI.\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueQNLI", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.STSBTask.__init__": [[195, 200], ["mxnet.metric.PearsonCorrelation", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "None", "\n", "metric", "=", "PearsonCorrelation", "(", ")", "\n", "super", "(", "STSBTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.STSBTask.get_dataset": [[201, 210], ["glue.GlueSTSB"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for STSB\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueSTSB", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.CoLATask.__init__": [[215, 220], ["mxnet.metric.MCC", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "False", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "MCC", "(", "average", "=", "'micro'", ")", "\n", "super", "(", "CoLATask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.CoLATask.get_dataset": [[221, 230], ["glue.GlueCoLA"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for CoLA\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueCoLA", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.SSTTask.__init__": [[235, 240], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "False", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "SSTTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.SSTTask.get_dataset": [[241, 250], ["glue.GlueSST2"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for SST\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "GlueSST2", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WNLITask.__init__": [[255, 260], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "WNLITask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WNLITask.get_dataset": [[261, 270], ["glue.GlueWNLI"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for WNLI\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'dev', 'test', 'train'\n        \"\"\"", "\n", "return", "GlueWNLI", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.__init__": [[275, 280], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'neutral'", ",", "'entailment'", ",", "'contradiction'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "MNLITask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "metric", ",", "is_pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.get_dataset": [[281, 291], ["glue.GlueMNLI"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset for MNLI\n\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'dev_matched', 'dev_mismatched', 'test_matched',\n            'test_mismatched', 'train'\n        \"\"\"", "\n", "return", "GlueMNLI", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.dataset_dev": [[292, 301], ["classification.MNLITask.get_dataset", "classification.MNLITask.get_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset", "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset"], ["", "def", "dataset_dev", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the dev segment of the dataset for the task.\n\n        Returns\n        -------\n        list of TSVDataset : the dataset of the dev segment.\n        \"\"\"", "\n", "return", "[", "(", "'dev_matched'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'dev_matched'", ")", ")", ",", "\n", "(", "'dev_mismatched'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'dev_mismatched'", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MNLITask.dataset_test": [[302, 311], ["classification.MNLITask.get_dataset", "classification.MNLITask.get_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset", "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset"], ["", "def", "dataset_test", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the test segment of the dataset for the task.\n\n        Returns\n        -------\n        list of TSVDataset : the dataset of the test segment.\n        \"\"\"", "\n", "return", "[", "(", "'test_matched'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'test_matched'", ")", ")", ",", "\n", "(", "'test_mismatched'", ",", "self", ".", "get_dataset", "(", "segment", "=", "'test_mismatched'", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.SuperGLUERTETask.__init__": [[316, 322], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "SuperGLUERTETask", ",", "self", ")", ".", "__init__", "(", "\n", "class_labels", ",", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.SuperGLUERTETask.get_dataset": [[323, 331], ["super_glue.SuperGlueRTE"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueRTE", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.CBTask.__init__": [[336, 342], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'neutral'", ",", "'entailment'", ",", "'contradiction'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "CBTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.CBTask.get_dataset": [[343, 351], ["super_glue.SuperGlueCB"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueCB", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BoolQTask.__init__": [[356, 362], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'false'", ",", "'true'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "BoolQTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.BoolQTask.get_dataset": [[363, 371], ["super_glue.SuperGlueBoolQ"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueBoolQ", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WiCTask.__init__": [[376, 382], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'false'", ",", "'true'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "WiCTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WiCTask.get_dataset": [[383, 391], ["super_glue.SuperGlueWiC"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueWiC", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WSCTask.__init__": [[396, 402], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'false'", ",", "'true'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "WSCTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.WSCTask.get_dataset": [[403, 411], ["super_glue.SuperGlueWSC"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueWSC", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.COPATask.__init__": [[416, 423], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "# Technically there's two other sentences.", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "super", "(", "COPATask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.COPATask.get_dataset": [[424, 432], ["super_glue.SuperGlueCOPA"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueCOPA", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MultiRCTask.__init__": [[437, 444], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.CompositeEvalMetric.add", "classification.BaseClassificationTask.__init__", "mxnet.metric.F1"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "metric", ".", "add", "(", "F1", "(", "average", "=", "'micro'", ")", ")", "\n", "super", "(", "MultiRCTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.MultiRCTask.get_dataset": [[445, 453], ["super_glue.SuperGlueMultiRC"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueMultiRC", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.ReCoRDTask.__init__": [[459, 467], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.CompositeEvalMetric.add", "mxnet.metric.CompositeEvalMetric.add", "classification.BaseClassificationTask.__init__", "mxnet.metric.F1", "mxnet.metric.Accuracy"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", "]", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "metric", ".", "add", "(", "F1", "(", ")", ")", "\n", "metric", ".", "add", "(", "Accuracy", "(", ")", ")", "\n", "super", "(", "ReCoRDTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.ReCoRDTask.get_dataset": [[468, 476], ["super_glue.SuperGlueReCoRD"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "SuperGlueReCoRD", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.AXbTask.__init__": [[481, 487], ["mxnet.metric.MCC", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "metric", "=", "MCC", "(", "average", "=", "'micro'", ")", "\n", "super", "(", "AXbTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.AXbTask.get_dataset": [[488, 493], ["super_glue.SuperGlueAXb"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'test'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        \"\"\"", "\n", "# Only one segment", "\n", "return", "SuperGlueAXb", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.AXgTask.__init__": [[498, 505], ["mxnet.metric.CompositeEvalMetric", "mxnet.metric.CompositeEvalMetric.add", "classification.BaseClassificationTask.__init__", "mxnet.metric.Accuracy"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'not_entailment'", ",", "'entailment'", "]", "\n", "metric", "=", "CompositeEvalMetric", "(", ")", "\n", "metric", ".", "add", "(", "Accuracy", "(", ")", ")", "\n", "super", "(", "AXgTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.AXgTask.get_dataset": [[506, 511], ["super_glue.SuperGlueAXg"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'test'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        \"\"\"", "\n", "# Only one segment", "\n", "return", "SuperGlueAXg", "(", "segment", "=", "segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEHTask.__init__": [[516, 525], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", ",", "'2'", ",", "'3'", "]", "\n", "# We will also use class-accuracy (i.e., the true performance)", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "self", ".", "_root", "=", "None", "\n", "\n", "super", "(", "RACEHTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"txt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEHTask._set_dataset_location": [[526, 528], ["None"], "methods", ["None"], ["", "def", "_set_dataset_location", "(", "self", ",", "root", ")", ":", "\n", "        ", "self", ".", "_root", "=", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEHTask.get_dataset": [[529, 537], ["race.RACEH"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "RACEH", "(", "segment", "=", "segment", ",", "root", "=", "self", ".", "_root", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.__init__": [[542, 551], ["mxnet.metric.Accuracy", "classification.BaseClassificationTask.__init__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "is_pair", "=", "True", "\n", "class_labels", "=", "[", "'0'", ",", "'1'", ",", "'2'", ",", "'3'", "]", "\n", "# We will also use class-accuracy (i.e., the true performance)", "\n", "metric", "=", "Accuracy", "(", ")", "\n", "self", ".", "_root", "=", "None", "\n", "\n", "super", "(", "RACEMTask", ",", "self", ")", ".", "__init__", "(", "class_labels", ",", "\n", "metric", ",", "is_pair", ",", "output_format", "=", "\"txt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask._set_dataset_location": [[552, 554], ["None"], "methods", ["None"], ["", "def", "_set_dataset_location", "(", "self", ",", "root", ")", ":", "\n", "        ", "self", ".", "_root", "=", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.classification.RACEMTask.get_dataset": [[555, 563], ["race.RACEM"], "methods", ["None"], ["", "def", "get_dataset", "(", "self", ",", "segment", "=", "'train'", ")", ":", "\n", "        ", "\"\"\"Get the corresponding dataset\n        Parameters\n        ----------\n        segment : str, default 'train'\n            Dataset segments. Options are 'train', 'dev', 'test'.\n        \"\"\"", "\n", "return", "RACEM", "(", "segment", "=", "segment", ",", "root", "=", "self", ".", "_root", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.DatasetFn.__call__": [[31, 33], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "dataset_url", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SamplerFn.__call__": [[41, 43], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.DataLoaderFn.__call__": [[51, 53], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "dataset", ",", "sampler", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDataLoaderFn.__init__": [[60, 63], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataloader_cls", ",", "dataloader_params", ")", ":", "\n", "        ", "self", ".", "_dataloader_cls", "=", "dataloader_cls", "\n", "self", ".", "_dataloader_params", "=", "dataloader_params", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDataLoaderFn.__call__": [[64, 67], ["dataloader.SimpleDataLoaderFn._dataloader_cls"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "dataset", ",", "sampler", ")", ":", "\n", "        ", "return", "self", ".", "_dataloader_cls", "(", "dataset", ",", "batch_sampler", "=", "sampler", ",", "\n", "**", "self", ".", "_dataloader_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDatasetFn.__init__": [[74, 77], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_cls", ",", "dataset_params", ")", ":", "\n", "        ", "self", ".", "_dataset_cls", "=", "dataset_cls", "\n", "self", ".", "_dataset_params", "=", "dataset_params", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.SimpleDatasetFn.__call__": [[78, 80], ["dataloader.SimpleDatasetFn._dataset_cls"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "dataset_url", ")", ":", "\n", "        ", "return", "self", ".", "_dataset_cls", "(", "dataset_url", ",", "**", "self", ".", "_dataset_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.__init__": [[92, 117], ["iter", "len", "range", "dataloader._MultiWorkerIter._push_next_dataset"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter._push_next_dataset"], ["def", "__init__", "(", "self", ",", "worker_pool", ",", "worker_fn", ",", "dataset", ",", "file_sampler", ",", "\n", "dataset_fn", ",", "sampler_fn", ",", "dataloader_fn", ",", "prefetch", ")", ":", "\n", "        ", "self", ".", "_worker_pool", "=", "worker_pool", "\n", "self", ".", "_worker_fn", "=", "worker_fn", "\n", "self", ".", "_dataset", "=", "dataset", "\n", "self", ".", "_dataset_fn", "=", "dataset_fn", "\n", "self", ".", "_sampler_fn", "=", "sampler_fn", "\n", "self", ".", "_dataloader_fn", "=", "dataloader_fn", "\n", "self", ".", "_prefetch", "=", "prefetch", "\n", "\n", "# send and receive index for datasets", "\n", "self", ".", "_rcvd_idx", "=", "0", "\n", "self", ".", "_sent_idx", "=", "0", "\n", "self", ".", "_data_buffer", "=", "{", "}", "\n", "\n", "self", ".", "_dataset_iter", "=", "iter", "(", "file_sampler", ")", "\n", "self", ".", "_num_datasets", "=", "len", "(", "self", ".", "_dataset", ")", "\n", "\n", "# need to keep a reference of the dataloader", "\n", "self", ".", "_dataloader_ref", "=", "None", "\n", "self", ".", "_dataloader", "=", "None", "\n", "\n", "# pre-fetch", "\n", "for", "_", "in", "range", "(", "self", ".", "_prefetch", ")", ":", "\n", "            ", "self", ".", "_push_next_dataset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter._push_next_dataset": [[118, 130], ["dataloader._MultiWorkerIter._worker_pool.apply_async", "len"], "methods", ["None"], ["", "", "def", "_push_next_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Assign next dataset workload to workers.\"\"\"", "\n", "if", "self", ".", "_sent_idx", "<", "len", "(", "self", ".", "_dataset", ")", ":", "\n", "            ", "url", "=", "self", ".", "_dataset", "[", "self", ".", "_sent_idx", "]", "\n", "", "else", ":", "\n", "            ", "return", "\n", "# push to worker asynchronously", "\n", "", "async_ret", "=", "self", ".", "_worker_pool", ".", "apply_async", "(", "\n", "self", ".", "_worker_fn", ",", "(", "url", ",", "self", ".", "_dataset_fn", ",", "self", ".", "_sampler_fn", ")", ")", "\n", "# data buffer stores the async result", "\n", "self", ".", "_data_buffer", "[", "self", ".", "_sent_idx", "]", "=", "async_ret", "\n", "self", ".", "_sent_idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter._next_dataset": [[131, 146], ["dataloader._MultiWorkerIter._data_buffer.pop", "dataloader._MultiWorkerIter.get"], "methods", ["None"], ["", "def", "_next_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Retrieve the next dataset. Returns None if no dataset is available.\"\"\"", "\n", "if", "self", ".", "_rcvd_idx", "==", "self", ".", "_sent_idx", ":", "\n", "            ", "assert", "not", "self", ".", "_data_buffer", ",", "'Data buffer should be empty at this moment'", "\n", "return", "None", "\n", "\n", "", "assert", "self", ".", "_rcvd_idx", "<", "self", ".", "_sent_idx", ",", "'rcvd_idx must be smaller than sent_idx'", "\n", "assert", "self", ".", "_rcvd_idx", "in", "self", ".", "_data_buffer", ",", "'fatal error with _next_dataset, rcvd_idx missing'", "\n", "\n", "ret", "=", "self", ".", "_data_buffer", ".", "pop", "(", "self", ".", "_rcvd_idx", ")", "\n", "dataset", ",", "sampler", "=", "ret", ".", "get", "(", ")", "\n", "self", ".", "_rcvd_idx", "+=", "1", "\n", "return", "dataset", ",", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.__next__": [[147, 168], ["dataloader._MultiWorkerIter._push_next_dataset", "dataloader._MultiWorkerIter._next_dataset", "dataloader._MultiWorkerIter._dataloader_fn", "iter", "dataloader._MultiWorkerIter.next"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter._push_next_dataset", "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter._next_dataset", "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.next"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Next mini-batch\"\"\"", "\n", "while", "True", ":", "\n", "            ", "if", "self", ".", "_dataloader_ref", "is", "None", ":", "\n", "# load next dataset and create a data loader", "\n", "                ", "self", ".", "_push_next_dataset", "(", ")", "\n", "result", "=", "self", ".", "_next_dataset", "(", ")", "\n", "\n", "if", "result", "is", "None", ":", "\n", "                    ", "raise", "StopIteration", "\n", "\n", "", "dataset", ",", "sampler", "=", "result", "\n", "self", ".", "_dataloader_ref", "=", "self", ".", "_dataloader_fn", "(", "dataset", ",", "sampler", ")", "\n", "self", ".", "_dataloader", "=", "iter", "(", "self", ".", "_dataloader_ref", ")", "\n", "", "try", ":", "\n", "# load next mini-batch from the dataloader", "\n", "                ", "result", "=", "next", "(", "self", ".", "_dataloader", ")", "\n", "return", "result", "\n", "", "except", "StopIteration", ":", "\n", "                ", "self", ".", "_dataloader", "=", "None", "\n", "self", ".", "_dataloader_ref", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.next": [[169, 172], ["dataloader._MultiWorkerIter.__next__"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.__next__"], ["", "", "", "def", "next", "(", "self", ")", ":", "\n", "        ", "\"\"\"Next mini-batch\"\"\"", "\n", "return", "self", ".", "__next__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._MultiWorkerIter.__iter__": [[173, 176], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the iterator object\"\"\"", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.DatasetLoader.__init__": [[210, 229], ["gluonnlp.data.stream._PathDataset", "max", "isinstance", "isinstance", "multiprocessing.Pool", "int"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "file_patterns", ",", "file_sampler", ",", "dataset_fn", ",", "\n", "sampler_fn", ",", "dataloader_fn", ",", "num_dataset_workers", "=", "1", ",", "prefetch", "=", "None", ")", ":", "\n", "        ", "self", ".", "_dataset", "=", "_PathDataset", "(", "file_patterns", ")", "\n", "self", ".", "_file_sampler", "=", "file_sampler", "\n", "self", ".", "_dataset_fn", "=", "dataset_fn", "\n", "self", ".", "_sampler_fn", "=", "sampler_fn", "\n", "self", ".", "_dataloader_fn", "=", "dataloader_fn", "\n", "self", ".", "_num_dataset_workers", "=", "num_dataset_workers", "\n", "self", ".", "_prefetch", "=", "max", "(", "\n", "0", ",", "int", "(", "prefetch", ")", "if", "prefetch", "is", "not", "None", "else", "num_dataset_workers", ")", "\n", "self", ".", "_worker_pool", "=", "None", "\n", "if", "self", ".", "_num_dataset_workers", ">", "0", ":", "\n", "            ", "self", ".", "_worker_pool", "=", "multiprocessing", ".", "Pool", "(", "self", ".", "_num_dataset_workers", ")", "\n", "", "assert", "self", ".", "_num_dataset_workers", ">=", "0", ",", "'num_dataset_workers must be non-negative'", "\n", "assert", "isinstance", "(", "sampler_fn", ",", "SamplerFn", ")", ",", "'sampler_fn must be an instance of SamplerFn'", "\n", "assert", "isinstance", "(", "dataloader_fn", ",", "DataLoaderFn", ")", ",", "'dataloader_fn must be an instance of DataLoaderFn'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.DatasetLoader.__iter__": [[230, 251], ["dataloader.DatasetLoader._MultiWorkerIter", "dataloader.DatasetLoader.DatasetLoader.__iter__._same_process_iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_num_dataset_workers", "==", "0", ":", "\n", "            ", "def", "_same_process_iter", "(", ")", ":", "\n", "                ", "for", "idx", "in", "self", ".", "_file_sampler", ":", "\n", "                    ", "url", "=", "self", ".", "_dataset", "[", "idx", "]", "\n", "dataset", ",", "sampler", "=", "_worker_fn", "(", "\n", "url", ",", "self", ".", "_dataset_fn", ",", "self", ".", "_sampler_fn", ")", "\n", "dataloader", "=", "self", ".", "_dataloader_fn", "(", "dataset", ",", "sampler", ")", "\n", "for", "batch", "in", "dataloader", ":", "\n", "                        ", "yield", "batch", "\n", "", "", "", "return", "_same_process_iter", "(", ")", "\n", "\n", "# multi-worker", "\n", "", "return", "_MultiWorkerIter", "(", "self", ".", "_worker_pool", ",", "\n", "worker_fn", "=", "_worker_fn", ",", "\n", "dataset", "=", "self", ".", "_dataset", ",", "\n", "file_sampler", "=", "self", ".", "_file_sampler", ",", "\n", "dataset_fn", "=", "self", ".", "_dataset_fn", ",", "\n", "sampler_fn", "=", "self", ".", "_sampler_fn", ",", "\n", "dataloader_fn", "=", "self", ".", "_dataloader_fn", ",", "\n", "prefetch", "=", "self", ".", "_prefetch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader.DatasetLoader.__del__": [[252, 258], ["isinstance", "dataloader.DatasetLoader._worker_pool.terminate"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_worker_pool", ":", "\n", "# manually terminate due to a bug that pool is not automatically terminated", "\n", "# https://bugs.python.org/issue34172", "\n", "            ", "assert", "isinstance", "(", "self", ".", "_worker_pool", ",", "multiprocessing", ".", "pool", ".", "Pool", ")", "\n", "self", ".", "_worker_pool", ".", "terminate", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.dataloader._worker_fn": [[82, 87], ["dataset_fn", "sampler_fn"], "function", ["None"], ["", "", "def", "_worker_fn", "(", "url", ",", "dataset_fn", ",", "sampler_fn", ")", ":", "\n", "    ", "\"\"\"Function to generate the dataset and sampler for each worker.\"\"\"", "\n", "dataset", "=", "dataset_fn", "(", "url", ")", "\n", "sampler", "=", "sampler_fn", "(", "dataset", ")", "\n", "return", "(", "dataset", ",", "sampler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race._TextDataset.__init__": [[72, 81], ["race._TextDataset._filenames.sort", "mxnet.gluon.data.SimpleDataset.__init__", "isinstance", "os.path.expanduser", "race._TextDataset._read"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._JsonlDataset._read"], ["def", "__init__", "(", "self", ",", "filenames", ",", "segment", ")", ":", "\n", "\n", "        ", "if", "not", "isinstance", "(", "filenames", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "filenames", "=", "(", "filenames", ",", ")", "\n", "", "self", ".", "_filenames", "=", "[", "os", ".", "path", ".", "expanduser", "(", "f", ")", "for", "f", "in", "filenames", "]", "\n", "self", ".", "_filenames", ".", "sort", "(", ")", "\n", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_is_test", "=", "self", ".", "_segment", "==", "\"test\"", "\n", "super", "(", "_TextDataset", ",", "self", ")", ".", "__init__", "(", "self", ".", "_read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race._TextDataset._read": [[82, 95], ["race._TextDataset._read_samples", "open", "fin.readlines", "json.loads", "race.RACEExpansion", "race._TextDataset.append"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._read_samples", "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEExpansion"], ["", "def", "_read", "(", "self", ")", ":", "\n", "        ", "all_samples", "=", "[", "]", "\n", "for", "filename", "in", "self", ".", "_filenames", ":", "\n", "            ", "samples", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "fin", ":", "\n", "                ", "for", "line", "in", "fin", ".", "readlines", "(", ")", ":", "\n", "                    ", "line_dic", "=", "json", ".", "loads", "(", "\n", "line", ",", "object_pairs_hook", "=", "collections", ".", "OrderedDict", ")", "\n", "for", "l", "in", "RACEExpansion", "(", "line_dic", ",", "is_test", "=", "self", ".", "_is_test", ")", ":", "\n", "                        ", "samples", ".", "append", "(", "l", ")", "\n", "", "", "", "samples", "=", "self", ".", "_read_samples", "(", "samples", ")", "\n", "all_samples", "+=", "samples", "\n", "", "return", "all_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race._TextDataset._read_samples": [[96, 98], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race._RACEDataset.__init__": [[102, 114], ["os.path.expanduser", "os.path.join", "race._TextDataset.__init__", "os.path.isdir", "os.makedirs", "os.path.join", "os.listdir", "fnmatch.fnmatch"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "segment", ",", "task", ")", ":", "\n", "        ", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "root", ")", "\n", "", "self", ".", "_root", "=", "root", "\n", "self", ".", "_segment", "=", "segment", "\n", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "segment", ",", "task", ")", "\n", "filenames", "=", "[", "os", ".", "path", ".", "join", "(", "file_path", ",", "f", ")", "for", "f", "in", "os", ".", "listdir", "(", "\n", "file_path", ")", "if", "fnmatch", ".", "fnmatch", "(", "f", ",", "'*.txt'", ")", "]", "\n", "\n", "super", "(", "_RACEDataset", ",", "self", ")", ".", "__init__", "(", "filenames", ",", "segment", "=", "self", ".", "_segment", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race._RACEDataset._repo_dir": [[115, 117], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEH.__init__": [[134, 139], ["race._RACEDataset.__init__", "os.path.join", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "root", "=", "None", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "if", "root", "is", "None", ":", "\n", "            ", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'race'", ")", "\n", "", "super", "(", "RACEH", ",", "self", ")", ".", "__init__", "(", "root", ",", "segment", ",", "\"high\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEH._read_samples": [[140, 142], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEM.__init__": [[159, 164], ["race._RACEDataset.__init__", "os.path.join", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "root", "=", "None", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "if", "root", "is", "None", ":", "\n", "            ", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'race'", ")", "\n", "", "super", "(", "RACEM", ",", "self", ")", ".", "__init__", "(", "root", ",", "segment", ",", "\"middle\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEM._read_samples": [[165, 167], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.race.jaccard_similarity": [[38, 41], ["set", "set", "len", "len", "s1.intersection", "s1.union"], "function", ["None"], ["def", "jaccard_similarity", "(", "x", ",", "y", ")", ":", "\n", "    ", "s1", ",", "s2", "=", "set", "(", "x", ")", ",", "set", "(", "y", ")", "\n", "return", "len", "(", "s1", ".", "intersection", "(", "s2", ")", ")", "/", "len", "(", "s1", ".", "union", "(", "s2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.race.RACEExpansion": [[43, 61], ["zip", "expanded_lines.append", "expanded_lines.append", "str"], "function", ["None"], ["", "def", "RACEExpansion", "(", "line", ",", "is_test", "=", "False", ")", ":", "\n", "    ", "\"\"\" Each line is comprised of a dictionary with multiple-choice answers, like MultiRC\n    \"\"\"", "\n", "expanded_lines", "=", "[", "]", "\n", "passage", "=", "line", "[", "\"article\"", "]", "\n", "options", "=", "line", "[", "\"options\"", "]", "\n", "questions", "=", "line", "[", "\"questions\"", "]", "\n", "answers", "=", "[", "\"\"", "for", "_", "in", "options", "]", "if", "is_test", "else", "line", "[", "\"answers\"", "]", "\n", "\n", "for", "(", "question", ",", "ans", ",", "opts", ")", "in", "zip", "(", "questions", ",", "answers", ",", "options", ")", ":", "\n", "        ", "passage_", "=", "passage", "+", "\" \"", "+", "question", "\n", "opt_", "=", "\" </sep> \"", ".", "join", "(", "[", "o", "for", "o", "in", "opts", "]", ")", "\n", "if", "not", "is_test", ":", "\n", "            ", "expanded_lines", ".", "append", "(", "[", "passage_", ",", "opt_", ",", "str", "(", "LETTER_TO_IDX", "[", "ans", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "expanded_lines", ".", "append", "(", "[", "passage_", ",", "opt_", "]", ")", "\n", "\n", "", "", "return", "expanded_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._JsonlDataset.__init__": [[139, 148], ["mxnet.gluon.data.SimpleDataset.__init__", "isinstance", "os.path.expanduser", "super_glue._JsonlDataset._read"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._JsonlDataset._read"], ["def", "__init__", "(", "self", ",", "filename", ",", "field_keys", ",", "task", ")", ":", "\n", "\n", "        ", "if", "not", "isinstance", "(", "filename", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "filename", "=", "(", "filename", ",", ")", "\n", "\n", "", "self", ".", "_filenames", "=", "[", "os", ".", "path", ".", "expanduser", "(", "f", ")", "for", "f", "in", "filename", "]", "\n", "self", ".", "_field_keys", "=", "field_keys", "\n", "self", ".", "_task", "=", "task", "\n", "super", "(", "_JsonlDataset", ",", "self", ")", ".", "__init__", "(", "self", ".", "_read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._JsonlDataset._read": [[149, 200], ["super_glue._JsonlDataset._read_samples", "open", "fin.readlines", "json.loads", "super_glue._JsonlDataset.append", "tmp_line.append", "tmp_line.append", "tmp_line.append", "super_glue.MultiRCExpansion", "super_glue._JsonlDataset.append", "super_glue.ReCoRDExpansion", "super_glue._JsonlDataset.append", "tmp_line.append", "tmp_line.append", "tmp_line.append", "super_glue.CBExpansion", "super_glue._JsonlDataset.append", "tmp_line.append"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._read_samples", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.MultiRCExpansion", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.ReCoRDExpansion", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.CBExpansion"], ["", "def", "_read", "(", "self", ")", ":", "\n", "        ", "all_samples", "=", "[", "]", "\n", "for", "filename", "in", "self", ".", "_filenames", ":", "\n", "            ", "samples", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "'r'", ")", "as", "fin", ":", "\n", "                ", "for", "line", "in", "fin", ".", "readlines", "(", ")", ":", "\n", "                    ", "line_dic", "=", "json", ".", "loads", "(", "\n", "line", ",", "object_pairs_hook", "=", "collections", ".", "OrderedDict", ")", "\n", "# Load the spans and multiple-choice questions as dictionaries,", "\n", "# but ensure we use the same terminology.", "\n", "# We've basically casted all tasks into classification", "\n", "# tasks.", "\n", "tmp_line", "=", "[", "]", "\n", "if", "self", ".", "_task", "==", "\"COPA\"", ":", "\n", "                        ", "sep", "=", "\" because \"", "if", "line_dic", "[", "\n", "\"question\"", "]", "==", "\"cause\"", "else", "\" so \"", "\n", "text_a", "=", "line_dic", "[", "self", ".", "_field_keys", "[", "0", "]", "]", "+", "sep", "+", "line_dic", "[", "self", ".", "_field_keys", "[", "1", "]", "]", "\n", "text_b", "=", "line_dic", "[", "self", ".", "_field_keys", "[", "0", "]", "]", "+", "sep", "+", "line_dic", "[", "self", ".", "_field_keys", "[", "2", "]", "]", "\n", "tmp_line", ".", "append", "(", "text_a", ")", "\n", "tmp_line", ".", "append", "(", "text_b", ")", "\n", "if", "\"label\"", "in", "self", ".", "_field_keys", ":", "\n", "                            ", "tmp_line", ".", "append", "(", "line_dic", "[", "\"label\"", "]", ")", "\n", "", "", "elif", "self", ".", "_task", "==", "\"MultiRC\"", ":", "\n", "                        ", "for", "l", "in", "MultiRCExpansion", "(", "line_dic", "[", "\"paragraph\"", "]", ")", ":", "\n", "                            ", "samples", ".", "append", "(", "l", ")", "\n", "", "continue", "\n", "", "elif", "self", ".", "_task", "==", "\"ReCoRD\"", ":", "\n", "                        ", "for", "l", "in", "ReCoRDExpansion", "(", "line_dic", ")", ":", "\n", "                            ", "samples", ".", "append", "(", "l", ")", "\n", "", "continue", "\n", "", "elif", "self", ".", "_task", "==", "\"WSC\"", ":", "\n", "                        ", "text_a", "=", "line_dic", "[", "\"text\"", "]", "\n", "text_b", "=", "line_dic", "[", "\"target\"", "]", "[", "\n", "\"span2_text\"", "]", "+", "\" means \"", "+", "line_dic", "[", "\"target\"", "]", "[", "\"span1_text\"", "]", "\n", "tmp_line", ".", "append", "(", "text_a", ")", "\n", "tmp_line", ".", "append", "(", "text_b", ")", "\n", "if", "\"label\"", "in", "self", ".", "_field_keys", ":", "\n", "                            ", "tmp_line", ".", "append", "(", "line_dic", "[", "\"label\"", "]", ")", "\n", "", "", "elif", "self", ".", "_task", "==", "\"CB\"", ":", "\n", "                        ", "for", "l", "in", "CBExpansion", "(", "line_dic", ",", "self", ".", "_field_keys", ")", ":", "\n", "                            ", "samples", ".", "append", "(", "l", ")", "\n", "", "continue", "\n", "", "else", ":", "\n", "                        ", "for", "key", "in", "self", ".", "_field_keys", ":", "\n", "                            ", "tmp_line", ".", "append", "(", "line_dic", "[", "key", "]", ")", "\n", "", "", "samples", ".", "append", "(", "tmp_line", ")", "\n", "", "", "samples", "=", "self", ".", "_read_samples", "(", "samples", ")", "\n", "all_samples", "+=", "samples", "\n", "", "return", "all_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._JsonlDataset._read_samples": [[201, 203], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._SuperGlueDataset.__init__": [[207, 218], ["os.path.expanduser", "os.path.join", "super_glue._SuperGlueDataset._get_data", "super_glue._JsonlDataset.__init__", "os.path.isdir", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue._SuperGlueDataset._get_data", "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"\"", ")", ":", "\n", "        ", "root", "=", "os", ".", "path", ".", "expanduser", "(", "root", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "root", ")", "\n", "", "arg_segment", ",", "zip_hash", ",", "data_hash", "=", "data_file", "\n", "segment", "=", "\"val\"", "if", "arg_segment", "==", "\"dev\"", "else", "arg_segment", "\n", "self", ".", "_root", "=", "root", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "'%s.jsonl'", "%", "segment", ")", "\n", "self", ".", "_get_data", "(", "segment", ",", "zip_hash", ",", "data_hash", ",", "filename", ")", "\n", "super", "(", "_SuperGlueDataset", ",", "self", ")", ".", "__init__", "(", "\n", "filename", ",", "field_keys", ",", "task", "=", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._SuperGlueDataset._get_data": [[219, 242], ["print", "print", "mxnet.gluon.utils.download", "os.path.join", "os.path.exists", "mxnet.gluon.utils._get_repo_file_url", "zipfile.ZipFile", "zf.infolist", "super_glue._SuperGlueDataset._repo_dir", "os.path.basename", "zf.extract"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._repo_dir"], ["", "def", "_get_data", "(", "self", ",", "arg_segment", ",", "zip_hash", ",", "data_hash", ",", "filename", ")", ":", "\n", "# The GLUE API requires \"dev\", but these files are hashed as \"val\"", "\n", "        ", "if", "self", ".", "task", "==", "\"MultiRC\"", ":", "\n", "# The MultiRC version from Gluon is quite outdated.", "\n", "# Make sure you've downloaded it and extracted it as described", "\n", "# in the README.MD file.", "\n", "            ", "print", "(", "\"Make sure you have downloaded the data!\"", ")", "\n", "print", "(", "\n", "\"https://github.com/nyu-mll/jiant/blob/master/scripts/download_superglue_data.py\"", ")", "\n", "", "segment", "=", "\"val\"", "if", "arg_segment", "==", "\"dev\"", "else", "arg_segment", "\n", "data_filename", "=", "'%s-%s.zip'", "%", "(", "segment", ",", "data_hash", "[", ":", "8", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", "and", "self", ".", "task", "!=", "\"MultiRC\"", ":", "\n", "            ", "download", "(", "_get_repo_file_url", "(", "self", ".", "_repo_dir", "(", ")", ",", "data_filename", ")", ",", "\n", "path", "=", "self", ".", "_root", ",", "sha1_hash", "=", "zip_hash", ")", "\n", "# unzip", "\n", "downloaded_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_root", ",", "data_filename", ")", "\n", "with", "zipfile", ".", "ZipFile", "(", "downloaded_path", ",", "'r'", ")", "as", "zf", ":", "\n", "# skip dir structures in the zip", "\n", "                ", "for", "zip_info", "in", "zf", ".", "infolist", "(", ")", ":", "\n", "                    ", "if", "zip_info", ".", "filename", "[", "-", "1", "]", "==", "'/'", ":", "\n", "                        ", "continue", "\n", "", "zip_info", ".", "filename", "=", "os", ".", "path", ".", "basename", "(", "zip_info", ".", "filename", ")", "\n", "zf", ".", "extract", "(", "zip_info", ",", "self", ".", "_root", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue._SuperGlueDataset._repo_dir": [[243, 245], ["None"], "methods", ["None"], ["", "", "", "", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueRTE.__init__": [[264, 281], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_rte'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'a4471b47b23f6d8bc2e89b2ccdcf9a3a987c69a1'", ",", "\n", "'01ebec38ff3d2fdd849d3b33c2a83154d1476690'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'17f23360f77f04d03aee6c42a27a61a6378f1fd9'", ",", "\n", "'410f8607d9fc46572c03f5488387327b33589069'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'ef2de5f8351ef80036c4aeff9f3b46106b4f2835'", ",", "\n", "'69f9d9b4089d0db5f0605eeaebc1c7abc044336b'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"hypothesis\"", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"hypothesis\"", "]", "\n", "\n", "", "super", "(", "SuperGlueRTE", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "field_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueRTE._read_samples": [[282, 284], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueRTE._repo_dir": [[285, 287], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/RTE'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCB.__init__": [[306, 324], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_cb'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'0b27cbdbbcdf2ba82da2f760e3ab40ed694bd2b9'", ",", "\n", "'193bdb772d2fe77244e5a56b4d7ac298879ec529'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'e1f9dc77327eba953eb41d5f9b402127d6954ae0'", ",", "\n", "'d286ac7c9f722c2b660e764ec3be11bc1e1895f8'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'008f9afdc868b38fdd9f989babe034a3ac35dd06'", ",", "\n", "'cca70739162d54f3cd671829d009a1ab4fd8ec6a'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"hypothesis\"", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"hypothesis\"", "]", "\n", "\n", "", "super", "(", "SuperGlueCB", ",", "self", ")", ".", "__init__", "(", "\n", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"CB\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCB._read_samples": [[325, 327], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCB._repo_dir": [[328, 330], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/CB'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWiC.__init__": [[350, 367], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_wic'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'ec1e265bbdcde1d8da0b56948ed30d86874b1f12'", ",", "\n", "'831a58c553def448e1b1d0a8a36e2b987c81bc9c'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'2046c43e614d98d538a03924335daae7881f77cf'", ",", "\n", "'73b71136a2dc2eeb3be7ab455a08f20b8dbe7526'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'77af78a49aac602b7bbf080a03b644167b781ba9'", ",", "\n", "'1be93932d46c8f8dc665eb7af6703c56ca1b1e08'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "# We'll hope the hypernymy is clear from the sentence", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"sentence1\"", ",", "\"sentence2\"", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"sentence1\"", ",", "\"sentence2\"", "]", "\n", "\n", "", "super", "(", "SuperGlueWiC", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "field_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWiC._read_samples": [[368, 370], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWiC._repo_dir": [[371, 373], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/WiC'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueBoolQ.__init__": [[393, 410], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_boolq'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'89507ff3015c3212b72318fb932cfb6d4e8417ef'", ",", "\n", "'d5be523290f49fc0f21f4375900451fb803817c0'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'fd39562fc2c9d0b2b8289d02a8cf82aa151d0ad4'", ",", "\n", "'9b09ece2b1974e4da20f0173454ba82ff8ee1710'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'a805d4bd03112366d548473a6848601c042667d3'", ",", "\n", "'98c308620c6d6c0768ba093858c92e5a5550ce9b'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"passage\"", ",", "\"question\"", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"passage\"", ",", "\"question\"", "]", "\n", "\n", "", "super", "(", "SuperGlueBoolQ", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "field_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueBoolQ._read_samples": [[411, 413], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueBoolQ._repo_dir": [[414, 416], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/BoolQ'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCOPA.__init__": [[436, 454], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_copa'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'96d20163fa8371e2676a50469d186643a07c4e7b'", ",", "\n", "'5bb9c8df7b165e831613c8606a20cbe5c9622cc3'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'acc13ad855a1d2750a3b746fb0cfe3ca6e8b6615'", ",", "\n", "'c8b908d880ffaf69bd897d6f2a1f23b8c3a732d4'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'89347d7884e71b49dd73c6bcc317aef64bb1bac8'", ",", "\n", "'735f39f3d31409d83b16e56ad8aed7725ef5ddd5'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"choice1\"", ",", "\"choice2\"", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"choice1\"", ",", "\"choice2\"", "]", "\n", "\n", "", "super", "(", "SuperGlueCOPA", ",", "self", ")", ".", "__init__", "(", "\n", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"COPA\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCOPA._read_samples": [[455, 457], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueCOPA._repo_dir": [[458, 460], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/COPA'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueMultiRC.__init__": [[480, 494], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_multirc'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "\n", "# This implementation needs the actual SuperGLUE", "\n", "# data, available at:", "\n", "# https://github.com/nyu-mll/jiant/blob/master/scripts/download_superglue_data.py", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "''", ",", "''", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "''", ",", "''", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "''", ",", "''", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "field_keys", "=", "[", "]", "\n", "super", "(", "SuperGlueMultiRC", ",", "self", ")", ".", "__init__", "(", "\n", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"MultiRC\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueMultiRC._read_samples": [[495, 497], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueMultiRC._repo_dir": [[498, 500], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/MultiRC'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueReCoRD.__init__": [[520, 533], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_record'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'047282c912535c9a3bcea519935fde882feb619d'", ",", "\n", "'65592074cefde2ecd1b27ce7b35eb8beb86c691a'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'442d8470bff2c9295231cd10262a7abf401edc64'", ",", "\n", "'9d1850e4dfe2eca3b71bfea191d5f4b412c65309'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'fc639a18fa87befdc52f14c1092fb40475bf50d0'", ",", "\n", "'b79b22f54b5a49f98fecd05751b122ccc6947c81'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "field_keys", "=", "[", "]", "\n", "super", "(", "SuperGlueReCoRD", ",", "self", ")", ".", "__init__", "(", "\n", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"ReCoRD\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueReCoRD._read_samples": [[534, 536], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueReCoRD._repo_dir": [[537, 539], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/ReCoRD'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWSC.__init__": [[559, 579], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'train'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_wsc'", ")", ")", ":", "\n", "        ", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'train'", ":", "(", "'train'", ",", "'ed0fe96914cfe1ae8eb9978877273f6baed621cf'", ",", "\n", "'fa978f6ad4b014b5f5282dee4b6fdfdaeeb0d0df'", ")", ",", "\n", "'dev'", ":", "(", "'dev'", ",", "'cebec2f5f00baa686573ae901bb4d919ca5d3483'", ",", "\n", "'ea2413e4e6f628f2bb011c44e1d8bae301375211'", ")", ",", "\n", "'test'", ":", "(", "'test'", ",", "'3313896f315e0cb2bb1f24f3baecec7fc93124de'", ",", "\n", "'a47024aa81a5e7c9bc6e957b36c97f1d1b5da2fd'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "field_keys", "=", "[", "\"target\"", ",", "\"text\"", ",", "[", "\n", "[", "\"span1_index\"", ",", "\"span1_text\"", "]", ",", "[", "\"span2_index\"", ",", "\"span2_text\"", "]", "]", ",", "\"label\"", "]", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"target\"", ",", "\"text\"", ",", "[", "\n", "[", "\"span1_index\"", ",", "\"span1_text\"", "]", ",", "[", "\"span2_index\"", ",", "\"span2_text\"", "]", "]", "]", "\n", "\n", "", "super", "(", "SuperGlueWSC", ",", "self", ")", ".", "__init__", "(", "\n", "root", ",", "data_file", ",", "field_keys", ",", "task", "=", "\"WSC\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWSC._read_samples": [[580, 582], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueWSC._repo_dir": [[583, 585], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/WSC'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXb.__init__": [[602, 616], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir", "ValueError"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'test'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_ax_b'", ")", ")", ":", "\n", "\n", "        ", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only \\\"test\\\" is supported for AX-b\"", ")", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"sentence1\"", ",", "\"sentence2\"", "]", "\n", "\n", "", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "'test'", ":", "(", "'AX-b'", ",", "'398c5a376eb436f790723cd217ac040334140000'", ",", "\n", "'50fd8ac409897b652daa4b246917097c3c394bc8'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "super", "(", "SuperGlueAXb", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "field_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXb._read_samples": [[617, 619], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXb._repo_dir": [[620, 622], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/AX-b'", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__": [[639, 653], ["os.path.join", "super_glue._SuperGlueDataset.__init__", "gluonnlp.base.get_home_dir", "ValueError"], "methods", ["home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg.__init__"], ["def", "__init__", "(", "self", ",", "segment", "=", "'test'", ",", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "get_home_dir", "(", ")", ",", "'datasets'", ",", "'superglue_ax_g'", ")", ")", ":", "\n", "\n", "        ", "if", "segment", "in", "[", "'train'", ",", "'dev'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only \\\"test\\\" is supported for AX-g\"", ")", "\n", "", "elif", "segment", "==", "'test'", ":", "\n", "            ", "field_keys", "=", "[", "\"premise\"", ",", "\"hypothesis\"", "]", "\n", "\n", "", "self", ".", "_segment", "=", "segment", "\n", "self", ".", "_data_file", "=", "{", "\"test\"", ":", "(", "'AX-g'", ",", "'d8c92498496854807dfeacd344eddf466d7f468a'", ",", "\n", "'8a8cbfe00fd88776a2a2f20b477e5b0c6cc8ebae'", ")", "}", "\n", "data_file", "=", "self", ".", "_data_file", "[", "segment", "]", "\n", "\n", "super", "(", "SuperGlueAXg", ",", "self", ")", ".", "__init__", "(", "root", ",", "data_file", ",", "field_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._read_samples": [[654, 656], ["None"], "methods", ["None"], ["", "def", "_read_samples", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.SuperGlueAXg._repo_dir": [[657, 659], ["None"], "methods", ["None"], ["", "def", "_repo_dir", "(", "self", ")", ":", "\n", "        ", "return", "'gluon/dataset/SUPERGLUE/AX-g'", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.MultiRCExpansion": [[35, 65], ["line_dict[].replace().replace().replace().replace().replace().split", "line_dict[].replace().replace().replace().replace().replace", "lines.append", "lines.append", "line_dict[].replace().replace().replace().replace", "label_to_int", "line_dict[].replace().replace().replace", "line_dict[].replace().replace", "line_dict[].replace"], "function", ["None"], ["def", "MultiRCExpansion", "(", "line_dict", ")", ":", "\n", "    ", "\"\"\" MultiRC has multiple nested points\n    \"\"\"", "\n", "DIGITS", "=", "[", "\"1\"", ",", "\"2\"", ",", "\"3\"", ",", "\"4\"", ",", "\"5\"", ",", "\"6\"", ",", "\"7\"", ",", "\"8\"", ",", "\"9\"", ",", "\"0\"", "]", "\n", "lines", "=", "[", "]", "\n", "\n", "# Split and fix common typos to simplify training", "\n", "sentence", "=", "line_dict", "[", "\"text\"", "]", ".", "replace", "(", "\": </b>\"", ",", "\"\"", ")", ".", "replace", "(", "\"<br>\"", ",", "\"\"", ")", ".", "replace", "(", "\n", "\"</br>\"", ",", "\"\"", ")", ".", "replace", "(", "\"_\"", ",", "\"\"", ")", ".", "replace", "(", "\"\\xad\"", ",", "\"\"", ")", ".", "split", "(", "\"<b>Sent\"", ")", "[", "1", ":", "]", "\n", "all_sentences", "=", "[", "l", "[", "2", ":", "]", "if", "l", "[", "2", ":", "]", "[", "0", "]", "not", "in", "DIGITS", "else", "l", "[", "3", ":", "]", "\n", "for", "l", "in", "sentence", "]", "\n", "questions", "=", "line_dict", "[", "\"questions\"", "]", "\n", "\n", "label_to_int", "=", "lambda", "x", ":", "\"0\"", "if", "not", "x", "else", "\"1\"", "\n", "\n", "for", "question", "in", "questions", ":", "\n", "# Prototype, next use Jaccard sim to avoid passing in a long premise", "\n", "        ", "hypothesis", "=", "[", "question", "[", "\"question\"", "]", "]", "\n", "for", "answer", "in", "question", "[", "\"answers\"", "]", ":", "\n", "            ", "hypothesis_", "=", "\" \"", ".", "join", "(", "hypothesis", "+", "[", "answer", "[", "\"text\"", "]", "]", ")", "\n", "if", "\"isAnswer\"", "in", "answer", ":", "\n", "                ", "lines", ".", "append", "(", "[", "sentence", ",", "\n", "hypothesis_", ",", "\n", "label_to_int", "(", "answer", "[", "\"isAnswer\"", "]", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "                ", "lines", ".", "append", "(", "[", "sentence", ",", "\n", "hypothesis_", "\n", "]", ")", "\n", "", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.ReCoRDExpansion": [[67, 108], ["line_dict[].replace", "sorted", "set", "question_text.replace", "lines.append", "len", "len", "min", "range", "pos_examples.append", "neg_examples.append", "lines.append", "lines.append", "len", "len", "lines.append", "lines.append"], "function", ["None"], ["", "def", "ReCoRDExpansion", "(", "line_dict", ")", ":", "\n", "    ", "\"\"\" ReCoRD has separate answers. We'll turn it into a classification problem\n        and then select the best from the set, breaking ties arbitrarily.\n    \"\"\"", "\n", "line_dict", ",", "question_dict", "=", "line_dict", "[", "\"passage\"", "]", ",", "line_dict", "[", "\"qas\"", "]", "\n", "lines", "=", "[", "]", "\n", "# Maintain the highlight annotation", "\n", "sentence", "=", "line_dict", "[", "\"text\"", "]", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "entities", "=", "sorted", "(", "\n", "set", "(", "[", "line_dict", "[", "\"text\"", "]", "[", "e", "[", "\"start\"", "]", ":", "e", "[", "\"end\"", "]", "+", "1", "]", "for", "e", "in", "line_dict", "[", "\"entities\"", "]", "]", ")", ")", "\n", "\n", "for", "question", "in", "question_dict", ":", "\n", "        ", "question_text", "=", "question", "[", "\"query\"", "]", "\n", "answers", "=", "[", "a", "[", "\"text\"", "]", "\n", "for", "a", "in", "question", "[", "\"answers\"", "]", "]", "if", "\"answers\"", "in", "question", "else", "[", "]", "\n", "pos_examples", ",", "neg_examples", "=", "[", "]", ",", "[", "]", "\n", "for", "entity", "in", "entities", ":", "\n", "            ", "query", "=", "question_text", ".", "replace", "(", "\"@placeholder\"", ",", "entity", ")", "\n", "label", "=", "1", "if", "entity", "in", "answers", "else", "0", "\n", "if", "not", "answers", ":", "\n", "                ", "lines", ".", "append", "(", "[", "sentence", ",", "query", "]", ")", "\n", "", "else", ":", "\n", "# We want a semi-balanced dataset", "\n", "                ", "if", "label", "==", "1", ":", "\n", "                    ", "pos_examples", ".", "append", "(", "[", "sentence", ",", "query", ",", "label", "]", ")", "\n", "", "else", ":", "\n", "                    ", "neg_examples", ".", "append", "(", "[", "sentence", ",", "query", ",", "label", "]", ")", "\n", "", "", "", "if", "answers", ":", "\n", "# Prioritize positive (i.e., the answers) samples.", "\n", "            ", "if", "len", "(", "pos_examples", ")", ">", "len", "(", "neg_examples", ")", ":", "\n", "                ", "for", "l", "in", "pos_examples", ":", "\n", "                    ", "lines", ".", "append", "(", "l", ")", "\n", "", "for", "l", "in", "neg_examples", ":", "\n", "                    ", "lines", ".", "append", "(", "l", ")", "\n", "", "", "else", ":", "\n", "                ", "min_len", "=", "min", "(", "len", "(", "pos_examples", ")", ",", "len", "(", "neg_examples", ")", ")", "\n", "for", "i", "in", "range", "(", "min_len", ")", ":", "\n", "                    ", "lines", ".", "append", "(", "pos_examples", "[", "i", "]", ")", "\n", "lines", ".", "append", "(", "neg_examples", "[", "i", "]", ")", "\n", "\n", "", "", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.alexa_bort.data.super_glue.CBExpansion": [[110, 128], ["tmp_line.append", "len", "line.replace", "line.split", "line.split"], "function", ["None"], ["", "def", "CBExpansion", "(", "line_dict", ",", "field_keys", ")", ":", "\n", "    ", "\"\"\" CB has a lot of pet words. We keep all the \"I think\"'s since they might influence\n        the label.\n        Some dialogues are way too long, and most of them have the relevant passage at the end.\n        Note that we could have regex'ed the proper label, since it is evident that the hypotheses\n        were extracted programatically.\n    \"\"\"", "\n", "tmp_line", "=", "[", "]", "\n", "FILLER_WORDS", "=", "[", "\"Uh,\"", ",", "\"uh,\"", ",", "\"Um,\"", ",", "\"um,\"", ",", "\"you know\"", ",", "\"I mean,\"", "]", "\n", "for", "key", "in", "field_keys", ":", "\n", "        ", "line", "=", "line_dict", "[", "key", "]", "\n", "if", "key", "!=", "\"label\"", ":", "\n", "            ", "if", "len", "(", "line", ".", "split", "(", "\".\"", ")", ")", ">", "3", ":", "\n", "                ", "line", "=", "\" \"", ".", "join", "(", "line", ".", "split", "(", "\".\"", ")", "[", "-", "4", ":", "]", ")", "[", "1", ":", "]", "\n", "", "for", "p", "in", "FILLER_WORDS", ":", "\n", "                ", "line", ".", "replace", "(", "p", ",", "\"\"", ")", "\n", "", "", "tmp_line", ".", "append", "(", "line", ")", "\n", "", "return", "[", "tmp_line", "]", "\n", "\n"]]}