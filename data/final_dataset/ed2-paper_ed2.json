{"home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.parse_pyobject_configs": [[24, 36], ["dict", "config.items", "value[].rsplit", "importlib.import_module", "getattr"], "function", ["None"], ["def", "parse_pyobject_configs", "(", "config", ")", ":", "\n", "    ", "\"\"\"Substitutes PyObject config entries with imported objects.\"\"\"", "\n", "parsed_config", "=", "dict", "(", "config", ")", "\n", "for", "key", ",", "value", "in", "config", ".", "items", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "if", "value", "[", "0", "]", "==", "'@'", ":", "\n", "                ", "module_name", ",", "obj_name", "=", "value", "[", "1", ":", "]", ".", "rsplit", "(", "'.'", ",", "1", ")", "\n", "module", "=", "importlib", ".", "import_module", "(", "module_name", ")", "\n", "parsed_config", "[", "key", "]", "=", "getattr", "(", "module", ",", "obj_name", ")", "\n", "", "", "except", "(", "TypeError", ",", "KeyError", ")", ":", "\n", "            ", "pass", "# Value is not a string.", "\n", "", "", "return", "parsed_config", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.make_env_fn": [[38, 66], ["any", "gym.make", "gym.wrappers.FlattenObservation", "gym.wrappers.FilterObservation"], "function", ["None"], ["", "def", "make_env_fn", "(", "env_name", ",", "env_kwargs", ")", ":", "\n", "    ", "\"\"\"Make the environment factory function.\"\"\"", "\n", "robotics_envs", "=", "[", "\n", "'FetchPickAndPlace'", ",", "\n", "'FetchPush'", ",", "\n", "'FetchReach'", ",", "\n", "'FetchSlide'", ",", "\n", "'HandManipulateBlock'", ",", "\n", "'HandManipulateEgg'", ",", "\n", "'HandManipulatePen'", ",", "\n", "'HandReach'", ",", "\n", "]", "\n", "\n", "is_robotics", "=", "False", "\n", "if", "any", "(", "x", "in", "env_name", "for", "x", "in", "robotics_envs", ")", ":", "\n", "        ", "is_robotics", "=", "True", "\n", "\n", "", "def", "env_fn", "(", ")", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "env_name", ",", "**", "env_kwargs", ")", "\n", "if", "is_robotics", ":", "\n", "            ", "env", "=", "gym", ".", "wrappers", ".", "FlattenObservation", "(", "\n", "gym", ".", "wrappers", ".", "FilterObservation", "(", "\n", "env", ",", "[", "'observation'", ",", "'desired_goal'", "]", "\n", ")", ")", "\n", "", "return", "env", "\n", "\n", "", "env_fn", ".", "__name__", "=", "env_name", "\n", "return", "env_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.run": [[68, 80], ["run.parse_pyobject_configs", "parse_pyobject_configs.pop", "parse_pyobject_configs.pop", "parse_pyobject_configs.pop", "parse_pyobject_configs.pop", "config.pop.", "run.make_env_fn"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.parse_pyobject_configs", "home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.make_env_fn"], ["", "def", "run", "(", "config", ")", ":", "\n", "    ", "\"\"\"Run an agent based on the specification.\"\"\"", "\n", "config", "=", "parse_pyobject_configs", "(", "config", ")", "\n", "\n", "# Pop parameters from the configuration.", "\n", "agent", "=", "config", ".", "pop", "(", "'agent'", ")", "\n", "task", "=", "config", ".", "pop", "(", "'task'", ")", "\n", "task_kwargs", "=", "config", ".", "pop", "(", "'task_kwargs'", ",", "{", "}", ")", "\n", "config", ".", "pop", "(", "'experiment_id'", ")", "# Dismiss experiment id.", "\n", "\n", "# Run the agent.", "\n", "agent", "(", "make_env_fn", "(", "task", ",", "task_kwargs", ")", ",", "**", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.main": [[82, 94], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dict", "sklearn.ParameterGrid", "enumerate", "open", "exec", "run.run", "f.read"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.None.run.run"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"config\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "vars_", "=", "dict", "(", ")", "\n", "with", "open", "(", "args", ".", "config", ")", "as", "f", ":", "\n", "        ", "exec", "(", "f", ".", "read", "(", ")", ",", "vars_", ")", "\n", "\n", "", "params_grid", "=", "skl_ms", ".", "ParameterGrid", "(", "vars_", "[", "'params_grid'", "]", ")", "\n", "for", "idx", ",", "params", "in", "enumerate", "(", "params_grid", ")", ":", "\n", "        ", "run", "(", "{", "**", "vars_", "[", "'base_config'", "]", ",", "**", "params", ",", "'experiment_id'", ":", "idx", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.MLPActorCriticFactory.__init__": [[80, 94], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "observation_space", ",", "\n", "action_space", ",", "\n", "hidden_sizes", ",", "\n", "activation", ",", "\n", "ac_number", ",", "\n", ")", ":", "\n", "        ", "self", ".", "_obs_dim", "=", "observation_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_scale", "=", "action_space", ".", "high", "[", "0", "]", "\n", "self", ".", "_hidden_sizes", "=", "hidden_sizes", "\n", "self", ".", "_activation", "=", "activation", "\n", "self", ".", "_ac_number", "=", "ac_number", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.MLPActorCriticFactory._make_actor": [[95, 114], ["tensorflow.keras.Input", "tensorflow.clip_by_value", "tensorflow.exp", "core.gaussian_likelihood", "core.apply_squashing_func", "tensorflow.keras.Model", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense", "tensorflow.random.normal", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.gaussian_likelihood", "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.apply_squashing_func", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["", "def", "_make_actor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the actor model (tf.keras.Model).\"\"\"", "\n", "obs_input", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "self", ".", "_obs_dim", ",", ")", ")", "\n", "body", "=", "mlp", "(", "self", ".", "_hidden_sizes", ",", "self", ".", "_activation", ")", "(", "obs_input", ")", "\n", "mu", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "log_std", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "\n", "log_std", "=", "tf", ".", "clip_by_value", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "tf", ".", "exp", "(", "log_std", ")", "\n", "pi", "=", "mu", "+", "tf", ".", "random", ".", "normal", "(", "tf", ".", "shape", "(", "input", "=", "mu", ")", ")", "*", "std", "\n", "logp_pi", "=", "gaussian_likelihood", "(", "pi", ",", "mu", ",", "log_std", ")", "\n", "\n", "mu", ",", "pi", ",", "logp_pi", "=", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", "\n", "\n", "# Put the actions in the limit.", "\n", "mu", "=", "mu", "*", "self", ".", "_act_scale", "\n", "pi", "=", "pi", "*", "self", ".", "_act_scale", "\n", "\n", "return", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "obs_input", ",", "outputs", "=", "[", "mu", ",", "pi", ",", "logp_pi", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.MLPActorCriticFactory.make_actor": [[115, 130], ["tensorflow.keras.Input", "tensorflow.unstack", "tensorflow.keras.Model", "core.MLPActorCriticFactory._make_actor", "core.MLPActorCriticFactory.", "mus.append", "pis.append", "logp_pis.append", "tensorflow.stack", "tensorflow.stack", "tensorflow.stack"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_actor"], ["", "def", "make_actor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the ensemble of actor models.\"\"\"", "\n", "obs_inputs", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "None", ",", "self", ".", "_obs_dim", ")", ",", "\n", "batch_size", "=", "self", ".", "_ac_number", ")", "\n", "mus", ",", "pis", ",", "logp_pis", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "obs_input", "in", "tf", ".", "unstack", "(", "obs_inputs", ",", "axis", "=", "0", ")", ":", "\n", "            ", "model", "=", "self", ".", "_make_actor", "(", ")", "\n", "mu", ",", "pi", ",", "logp_pi", "=", "model", "(", "obs_input", ")", "\n", "mus", ".", "append", "(", "mu", ")", "\n", "pis", ".", "append", "(", "pi", ")", "\n", "logp_pis", ".", "append", "(", "logp_pi", ")", "\n", "", "return", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "obs_inputs", ",", "outputs", "=", "[", "\n", "tf", ".", "stack", "(", "mus", ",", "axis", "=", "0", ")", ",", "\n", "tf", ".", "stack", "(", "pis", ",", "axis", "=", "0", ")", ",", "\n", "tf", ".", "stack", "(", "logp_pis", ",", "axis", "=", "0", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.MLPActorCriticFactory._make_critic": [[132, 147], ["tensorflow.keras.Input", "tensorflow.keras.Input", "tensorflow.keras.Model", "tensorflow.keras.layers.Concatenate", "tensorflow.keras.Sequential", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Reshape"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["", "def", "_make_critic", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the critic model (tf.keras.Model).\"\"\"", "\n", "obs_input", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "self", ".", "_obs_dim", ",", ")", ")", "\n", "act_input", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "self", ".", "_act_dim", ",", ")", ")", "\n", "\n", "concat_input", "=", "tf", ".", "keras", ".", "layers", ".", "Concatenate", "(", "\n", "axis", "=", "-", "1", ")", "(", "[", "obs_input", ",", "act_input", "]", ")", "\n", "\n", "q", "=", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "mlp", "(", "self", ".", "_hidden_sizes", ",", "self", ".", "_activation", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "1", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Reshape", "(", "[", "]", ")", "# Very important to squeeze values!", "\n", "]", ")", "(", "concat_input", ")", "\n", "\n", "return", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "[", "obs_input", ",", "act_input", "]", ",", "outputs", "=", "q", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.MLPActorCriticFactory.make_critic": [[148, 162], ["tensorflow.keras.Input", "tensorflow.keras.Input", "zip", "tensorflow.keras.Model", "tensorflow.unstack", "tensorflow.unstack", "core.MLPActorCriticFactory._make_critic", "core.MLPActorCriticFactory.", "qs.append", "tensorflow.stack"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_critic"], ["", "def", "make_critic", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the ensemble of critic models.\"\"\"", "\n", "obs_inputs", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "None", ",", "self", ".", "_obs_dim", ")", ",", "\n", "batch_size", "=", "self", ".", "_ac_number", ")", "\n", "act_inputs", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "None", ",", "self", ".", "_act_dim", ")", ",", "\n", "batch_size", "=", "self", ".", "_ac_number", ")", "\n", "qs", "=", "[", "]", "\n", "for", "obs_input", ",", "act_input", "in", "zip", "(", "tf", ".", "unstack", "(", "obs_inputs", ",", "axis", "=", "0", ")", ",", "\n", "tf", ".", "unstack", "(", "act_inputs", ",", "axis", "=", "0", ")", ")", ":", "\n", "            ", "model", "=", "self", ".", "_make_critic", "(", ")", "\n", "q", "=", "model", "(", "[", "obs_input", ",", "act_input", "]", ")", "\n", "qs", ".", "append", "(", "q", ")", "\n", "", "return", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "[", "obs_inputs", ",", "act_inputs", "]", ",", "\n", "outputs", "=", "tf", ".", "stack", "(", "qs", ",", "axis", "=", "0", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.heuristic_target_entropy": [[14, 23], ["isinstance", "NotImplementedError", "numpy.prod", "type"], "function", ["None"], ["def", "heuristic_target_entropy", "(", "action_space", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "    ", "\"\"\"Copied from https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py\"\"\"", "\n", "if", "isinstance", "(", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "# continuous space", "\n", "        ", "heuristic_target_entropy", "=", "-", "np", ".", "prod", "(", "action_space", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "(", "type", "(", "action_space", ")", ",", "action_space", ")", ")", "\n", "\n", "", "return", "heuristic_target_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.gaussian_likelihood": [[25, 32], ["tensorflow.reduce_sum", "numpy.log", "tensorflow.exp"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log"], ["", "def", "gaussian_likelihood", "(", "value", ",", "mu", ",", "log_std", ")", ":", "\n", "    ", "\"\"\"Calculates value's likelihood under Gaussian pdf.\"\"\"", "\n", "pre_sum", "=", "-", "0.5", "*", "(", "\n", "(", "(", "value", "-", "mu", ")", "/", "(", "tf", ".", "exp", "(", "log_std", ")", "+", "EPS", ")", ")", "**", "2", "+", "\n", "2", "*", "log_std", "+", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", "\n", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "pre_sum", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.apply_squashing_func": [[34, 49], ["tensorflow.reduce_sum", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.softplus", "numpy.log"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log"], ["", "def", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", ":", "\n", "    ", "\"\"\"Applies adjustment to mean, pi and log prob.\n\n    This formula is a little bit magic. To get an understanding of where it\n    comes from, check out the original SAC paper (arXiv 1801.01290) and look\n    in appendix C. This is a more numerically-stable equivalent to Eq 21.\n    Try deriving it yourself as a (very difficult) exercise. :)\n    \"\"\"", "\n", "logp_pi", "-=", "tf", ".", "reduce_sum", "(", "\n", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi", "-", "tf", ".", "nn", ".", "softplus", "(", "-", "2", "*", "pi", ")", ")", ",", "axis", "=", "1", ")", "\n", "\n", "# Squash those unbounded actions!", "\n", "mu", "=", "tf", ".", "tanh", "(", "mu", ")", "\n", "pi", "=", "tf", ".", "tanh", "(", "pi", ")", "\n", "return", "mu", ",", "pi", ",", "logp_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.mlp": [[51, 56], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense"], "function", ["None"], ["", "def", "mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "size", ",", "activation", "=", "activation", ")", "\n", "for", "size", "in", "hidden_sizes", "\n", "]", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.core.layer_norm_mlp": [[58, 65], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.LayerNormalization", "tensorflow.keras.layers.Activation", "core.mlp"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["", "def", "layer_norm_mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "hidden_sizes", "[", "0", "]", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "tf", ".", "nn", ".", "tanh", ")", ",", "\n", "mlp", "(", "hidden_sizes", "[", "1", ":", "]", ",", "activation", ")", "\n", "]", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.sunrise.ReplayBuffer.__init__": [[18, 32], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ",", "ac_number", ")", ":", "\n", "        ", "self", ".", "obs1_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acts_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "act_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rews_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "mask_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "ac_number", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "size", "=", "0", "\n", "self", ".", "obs_dim", "=", "obs_dim", "\n", "self", ".", "act_dim", "=", "act_dim", "\n", "self", ".", "max_size", "=", "size", "\n", "self", ".", "ac_number", "=", "ac_number", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.sunrise.ReplayBuffer.store": [[34, 44], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ",", "mask", ")", ":", "\n", "        ", "\"\"\"Store the transitions and masks in the replay buffer.\"\"\"", "\n", "self", ".", "obs1_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "acts_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rews_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "mask_buf", "[", "self", ".", "ptr", "]", "=", "mask", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.sunrise.ReplayBuffer.sample_batch": [[46, 68], ["numpy.random.randint", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "tensorflow.transpose", "dict", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "\"\"\"Sample batch of buffered experience.\"\"\"", "\n", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "\n", "obs_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", ",", "self", ".", "obs_dim", "]", "\n", "act_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", ",", "self", ".", "act_dim", "]", "\n", "rew_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", "]", "\n", "\n", "obs1", "=", "np", ".", "broadcast_to", "(", "self", ".", "obs1_buf", "[", "idxs", "]", ",", "obs_shape", ")", "\n", "obs2", "=", "np", ".", "broadcast_to", "(", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "obs_shape", ")", "\n", "acts", "=", "np", ".", "broadcast_to", "(", "self", ".", "acts_buf", "[", "idxs", "]", ",", "act_shape", ")", "\n", "rews", "=", "np", ".", "broadcast_to", "(", "self", ".", "rews_buf", "[", "idxs", "]", ",", "rew_shape", ")", "\n", "done", "=", "np", ".", "broadcast_to", "(", "self", ".", "done_buf", "[", "idxs", "]", ",", "rew_shape", ")", "\n", "masks", "=", "tf", ".", "transpose", "(", "self", ".", "mask_buf", "[", "idxs", "]", ")", "\n", "\n", "return", "dict", "(", "\n", "obs1", "=", "tf", ".", "convert_to_tensor", "(", "obs1", ")", ",", "\n", "obs2", "=", "tf", ".", "convert_to_tensor", "(", "obs2", ")", ",", "\n", "acts", "=", "tf", ".", "convert_to_tensor", "(", "acts", ")", ",", "\n", "rews", "=", "tf", ".", "convert_to_tensor", "(", "rews", ")", ",", "\n", "done", "=", "tf", ".", "convert_to_tensor", "(", "done", ")", ",", "\n", "masks", "=", "tf", ".", "convert_to_tensor", "(", "masks", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sunrise.sunrise.sunrise": [[71, 517], ["os.getcwd", "spinup_bis.utils.logx.EpochLogger", "logx.EpochLogger.save_config", "random.seed", "tensorflow.random.set_seed", "numpy.random.seed", "numpy.all", "actor_critic", "actor_critic.make_actor", "ac_factory.make_actor.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "ac_factory.make_critic.set_weights", "ac_factory.make_critic.set_weights", "sunrise.ReplayBuffer", "tensorflow_probability.distributions.Bernoulli", "tensorflow.keras.optimizers.Adam", "time.time", "range", "locals", "env_fn", "env_fn", "ac_factory.make_critic.get_weights", "ac_factory.make_critic.get_weights", "tensorflow.keras.optimizers.Adam", "tensorflow.Variable", "tensorflow_probability.util.DeferredTensor", "tensorflow.broadcast_to", "ac_factory.make_actor.", "tensorflow.reduce_mean", "tensorflow.broadcast_to", "tensorflow.broadcast_to", "ac_factory.make_actor.", "tensorflow.reshape", "tensorflow.broadcast_to", "tensorflow.reduce_mean", "tensorflow.math.reduce_std", "tfp.distributions.Bernoulli.sample", "ac_factory.make_actor.", "tape.gradient", "tf.keras.optimizers.Adam.apply_gradients", "dict", "g.gradient", "tf.keras.optimizers.Adam.apply_gradients", "g.gradient", "tf.keras.optimizers.Adam.apply_gradients", "zip", "zip", "dict", "range", "env.reset", "env.step", "info.get", "sunrise.sunrise.sample_mask"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.save_config", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_actor", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.step"], ["", "", "def", "sunrise", "(", "\n", "env_fn", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCriticFactory", ",", "\n", "ac_kwargs", "=", "None", ",", "\n", "ac_number", "=", "1", ",", "\n", "seed", "=", "0", ",", "\n", "total_steps", "=", "1_000_000", ",", "\n", "log_every", "=", "10_000", ",", "\n", "replay_size", "=", "1_000_000", ",", "\n", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "\n", "lr", "=", "0.001", ",", "\n", "alpha", "=", "0.2", ",", "\n", "batch_size", "=", "256", ",", "\n", "start_steps", "=", "10_000", ",", "\n", "update_after", "=", "1000", ",", "\n", "update_every", "=", "50", ",", "\n", "num_test_episodes", "=", "10", ",", "\n", "max_ep_len", "=", "1000", ",", "\n", "logger_kwargs", "=", "None", ",", "\n", "save_freq", "=", "10_000", ",", "\n", "save_path", "=", "None", ",", "\n", "autotune_alpha", "=", "False", ",", "\n", "target_entropy", "=", "None", ",", "\n", "alpha_lr", "=", "3e-4", ",", "\n", "use_weighted_bellman_backup", "=", "True", ",", "\n", "bellman_temp", "=", "10", ",", "\n", "ucb_lambda", "=", "1", ",", "\n", "beta_bernoulli", "=", "0.5", ",", "\n", ")", ":", "\n", "    ", "\"\"\"SUNRISE\n\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n\n        actor_critic: A function which takes in `action_space` kwargs\n            and returns actor and critic tf.keras.Model-s.\n\n            Actor should take an observation in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``pi``       (batch, act_dim)  | Deterministically computes actions\n                                           | from policy given states.\n            ===========  ================  =====================================\n\n            Critic should take an observation and action in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``q``        (batch,)          | Gives the current estimate of Q*\n                                           | state and action in the input.\n            ===========  ================  =====================================\n\n        ac_kwargs (dict): Any kwargs appropriate for the actor_critic\n            function you provided to SUNRISE.\n\n        ac_number (int): Number of the actor-critic models in the ensemble.\n\n        seed (int): Seed for random number generators.\n\n        total_steps (int): Number of environment interactions to run and train\n            the agent.\n\n        log_every (int): Number of environment interactions that should elapse\n            between dumping logs.\n\n        replay_size (int): Maximum length of replay buffer.\n\n        gamma (float): Discount factor. (Always between 0 and 1.)\n\n        polyak (float): Interpolation factor in polyak averaging for target\n            networks. Target networks are updated towards main networks\n            according to:\n\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow\n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually\n            close to 1.)\n\n        lr (float): Learning rate (used for both policy and value learning).\n\n        alpha (float): Entropy regularization coefficient. (Equivalent to\n            inverse of reward scale in the original SAC paper.)\n\n        batch_size (int): Minibatch size for SGD.\n\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long\n            you wait between updates, the ratio of env steps to gradient steps\n            is locked to 1.\n\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n\n        logger_kwargs (dict): Keyword args for EpochLogger.\n\n        save_freq (int): How often (in terms of environment iterations) to save\n            the current policy and value function.\n\n        save_path (str): The path specifying where to save the trained actor\n            model. Setting the value to None turns off the saving.\n\n        autotune_alpha (bool): Tune alpha automatically to achieve certain\n            entropy of policy.\n\n        target_entropy (Optional[float]): If not none optimize alpha parameter\n            to encourage actor to achieve selected entropy.\n\n        alpha_lr (float): Learning rate of alpha optimizer. Has effect only when\n            autotune_alpha is not True.\n\n        use_weighted_bellman_backup (bool): Whether the Bellman backup should\n            be reweighted based on the critic ensemble disagreement.\n\n        bellman_temp (float): Temperature parameter used in calculating the\n            weight for the weighted Bellman backup.\n\n        ucb_lambda (float): Weight of the standard deviation part\n            of the UCB exploration scoring formula.\n\n        beta_bernoulli (float): Probability of drawing 1 in the Bernoulli\n            distribution, used for generating masks for the bootstrap.\n    \"\"\"", "\n", "pwd", "=", "os", ".", "getcwd", "(", ")", "# pylint: disable=possibly-unused-variable", "\n", "logger", "=", "logx", ".", "EpochLogger", "(", "**", "(", "logger_kwargs", "or", "{", "}", ")", ")", "\n", "logger", ".", "save_config", "(", "locals", "(", ")", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "# This implementation assumes all dimensions share the same bound!", "\n", "assert", "np", ".", "all", "(", "env", ".", "action_space", ".", "high", "==", "env", ".", "action_space", ".", "high", "[", "0", "]", ")", "\n", "\n", "# Share information about observation and action spaces with policy.", "\n", "ac_kwargs", "=", "ac_kwargs", "or", "{", "}", "\n", "ac_kwargs", "[", "'observation_space'", "]", "=", "env", ".", "observation_space", "\n", "ac_kwargs", "[", "'action_space'", "]", "=", "env", ".", "action_space", "\n", "ac_kwargs", "[", "'ac_number'", "]", "=", "ac_number", "\n", "\n", "# Network", "\n", "ac_factory", "=", "actor_critic", "(", "**", "ac_kwargs", ")", "\n", "\n", "actor", "=", "ac_factory", ".", "make_actor", "(", ")", "\n", "actor", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", ")", ")", "\n", "\n", "critic1", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "critic1", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "critic2", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "critic2", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "\n", "critic_variables", "=", "critic1", ".", "trainable_variables", "+", "critic2", ".", "trainable_variables", "\n", "\n", "# Target networks", "\n", "target_critic1", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "target_critic1", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "target_critic2", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "target_critic2", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "\n", "# Copy weights", "\n", "target_critic1", ".", "set_weights", "(", "critic1", ".", "get_weights", "(", ")", ")", "\n", "target_critic2", ".", "set_weights", "(", "critic2", ".", "get_weights", "(", ")", ")", "\n", "\n", "# Experience buffer.", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "\n", "obs_dim", "=", "obs_dim", ",", "\n", "act_dim", "=", "act_dim", ",", "\n", "size", "=", "replay_size", ",", "\n", "ac_number", "=", "ac_number", ",", "\n", ")", "\n", "\n", "# Bernoulli distribution used for generating masks", "\n", "bernoulli_distr", "=", "tfp", ".", "distributions", ".", "Bernoulli", "(", "probs", "=", "beta_bernoulli", ")", "\n", "\n", "optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "lr", ")", "\n", "\n", "if", "autotune_alpha", ":", "\n", "        ", "if", "target_entropy", "is", "None", ":", "\n", "            ", "target_entropy", "=", "core", ".", "heuristic_target_entropy", "(", "env", ".", "action_space", ")", "\n", "", "alpha_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "alpha_lr", ")", "\n", "log_alpha", "=", "tf", ".", "Variable", "(", "tf", ".", "math", ".", "log", "(", "alpha", ")", ")", "\n", "alpha", "=", "tfp", ".", "util", ".", "DeferredTensor", "(", "log_alpha", ",", "tf", ".", "math", ".", "exp", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "evaluation_policy", "(", "obs", ")", ":", "\n", "        ", "obs", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "1", ",", "*", "obs", ".", "shape", "]", ")", "\n", "mu", ",", "_", ",", "_", "=", "actor", "(", "obs", ")", "\n", "act", "=", "tf", ".", "reduce_mean", "(", "mu", ",", "axis", "=", "0", ")", "\n", "return", "act", "[", "0", "]", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "behavioural_policy", "(", "obs", ")", ":", "\n", "        ", "obs_actor", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "1", ",", "*", "obs", ".", "shape", "]", ")", "\n", "obs_critic", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "ac_number", ",", "*", "obs", ".", "shape", "]", ")", "\n", "\n", "# Take the action", "\n", "_", ",", "pi", ",", "_", "=", "actor", "(", "obs_actor", ")", "\n", "act", "=", "tf", ".", "reshape", "(", "pi", ",", "[", "1", ",", "ac_number", ",", "*", "pi", ".", "shape", "[", "2", ":", "]", "]", ")", "\n", "act", "=", "tf", ".", "broadcast_to", "(", "act", ",", "[", "ac_number", ",", "ac_number", ",", "*", "pi", ".", "shape", "[", "2", ":", "]", "]", ")", "\n", "\n", "# Evaluate the actions through the critic ensemble", "\n", "qs", "=", "0.5", "*", "critic1", "(", "[", "obs_critic", ",", "act", "]", ")", "+", "0.5", "*", "critic2", "(", "[", "obs_critic", ",", "act", "]", ")", "\n", "q_mean", "=", "tf", ".", "reduce_mean", "(", "qs", ",", "axis", "=", "0", ")", "\n", "q_std", "=", "tf", ".", "math", ".", "reduce_std", "(", "qs", ",", "axis", "=", "0", ")", "\n", "scores", "=", "q_mean", "+", "ucb_lambda", "*", "q_std", "\n", "\n", "# Return the action with the highest score", "\n", "return", "pi", "[", "tf", ".", "math", ".", "argmax", "(", "scores", ")", "]", "[", "0", "]", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "sample_mask", "(", ")", ":", "\n", "        ", "return", "bernoulli_distr", ".", "sample", "(", "sample_shape", "=", "(", "ac_number", ",", ")", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "update_alpha", "(", "obs", ")", ":", "\n", "        ", "_", ",", "_", ",", "logp_pi", "=", "actor", "(", "obs", ")", "# batch['obs1']", "\n", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "            ", "alpha_losses", "=", "-", "1.0", "*", "alpha", "*", "tf", ".", "stop_gradient", "(", "logp_pi", "+", "target_entropy", ")", "\n", "alpha_loss", "=", "tf", ".", "reduce_mean", "(", "alpha_losses", ")", "\n", "\n", "", "alpha_gradients", "=", "tape", ".", "gradient", "(", "alpha_loss", ",", "[", "log_alpha", "]", ")", "\n", "alpha_optimizer", ".", "apply_gradients", "(", "zip", "(", "\n", "alpha_gradients", ",", "[", "log_alpha", "]", ")", ")", "\n", "\n", "return", "dict", "(", "\n", "AlphaLoss", "=", "alpha_loss", ",", "\n", "ActorEntropy", "=", "tf", ".", "reduce_mean", "(", "-", "logp_pi", ")", ",", "\n", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "learn_on_batch", "(", "obs1", ",", "obs2", ",", "acts", ",", "rews", ",", "done", ",", "masks", ")", ":", "\n", "        ", "with", "tf", ".", "GradientTape", "(", "persistent", "=", "True", ")", "as", "g", ":", "\n", "# Main outputs from computation graph.", "\n", "            ", "_", ",", "pi", ",", "logp_pi", "=", "actor", "(", "obs1", ")", "\n", "q1", "=", "critic1", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "q2", "=", "critic2", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "\n", "# Compose q with pi, for pi-learning.", "\n", "q1_pi", "=", "critic1", "(", "[", "obs1", ",", "pi", "]", ")", "\n", "q2_pi", "=", "critic2", "(", "[", "obs1", ",", "pi", "]", ")", "\n", "\n", "# Get actions and log probs of actions for next states.", "\n", "_", ",", "pi_next", ",", "logp_pi_next", "=", "actor", "(", "obs2", ")", "\n", "\n", "# Target Q-values, using actions from *current* policy.", "\n", "target_q1", "=", "target_critic1", "(", "[", "obs2", ",", "pi_next", "]", ")", "\n", "target_q2", "=", "target_critic2", "(", "[", "obs2", ",", "pi_next", "]", ")", "\n", "\n", "# Min Double-Q:", "\n", "min_q_pi", "=", "tf", ".", "minimum", "(", "q1_pi", ",", "q2_pi", ")", "\n", "min_target_q", "=", "tf", ".", "minimum", "(", "target_q1", ",", "target_q2", ")", "\n", "\n", "# Entropy-regularized Bellman backup for Q functions.", "\n", "# Using Clipped Double-Q targets.", "\n", "q_backup", "=", "tf", ".", "stop_gradient", "(", "rews", "+", "gamma", "*", "(", "1", "-", "done", ")", "*", "(", "\n", "min_target_q", "-", "alpha", "*", "logp_pi_next", ")", ")", "\n", "\n", "# Actor loss", "\n", "pi_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "masks", "*", "(", "alpha", "*", "logp_pi", "-", "min_q_pi", ")", "\n", ")", "\n", "\n", "# Critics loss", "\n", "q1_loss", "=", "0.5", "*", "(", "(", "q_backup", "-", "q1", ")", "**", "2", ")", "\n", "q2_loss", "=", "0.5", "*", "(", "(", "q_backup", "-", "q2", ")", "**", "2", ")", "\n", "\n", "if", "use_weighted_bellman_backup", ":", "\n", "                ", "q_std", "=", "tf", ".", "math", ".", "reduce_std", "(", "min_target_q", ",", "axis", "=", "0", ")", "\n", "q_weight", "=", "tf", ".", "math", ".", "sigmoid", "(", "-", "q_std", "*", "bellman_temp", ")", "+", "0.5", "\n", "\n", "value_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "q_weight", "*", "tf", ".", "reduce_mean", "(", "\n", "masks", "*", "(", "q1_loss", "+", "q2_loss", ")", ",", "\n", "axis", "=", "0", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "value_loss", "=", "tf", ".", "reduce_mean", "(", "masks", "*", "(", "q1_loss", "+", "q2_loss", ")", ")", "\n", "\n", "# Compute gradients and do updates.", "\n", "", "", "actor_gradients", "=", "g", ".", "gradient", "(", "pi_loss", ",", "actor", ".", "trainable_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "actor_gradients", ",", "actor", ".", "trainable_variables", ")", ")", "\n", "critic_gradients", "=", "g", ".", "gradient", "(", "value_loss", ",", "critic_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "critic_gradients", ",", "critic_variables", ")", ")", "\n", "\n", "# Polyak averaging for target variables.", "\n", "for", "v", ",", "target_v", "in", "zip", "(", "critic1", ".", "trainable_variables", ",", "\n", "target_critic1", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "", "for", "v", ",", "target_v", "in", "zip", "(", "critic2", ".", "trainable_variables", ",", "\n", "target_critic2", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "\n", "", "del", "g", "\n", "return", "dict", "(", "\n", "pi_loss", "=", "pi_loss", ",", "\n", "q1_loss", "=", "q1_loss", ",", "\n", "q2_loss", "=", "q2_loss", ",", "\n", "q1", "=", "q1", ",", "\n", "q2", "=", "q2", ",", "\n", "logp_pi", "=", "logp_pi", ",", "\n", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time.", "\n", "                ", "o", ",", "r", ",", "d", ",", "info", "=", "test_env", ".", "step", "(", "\n", "evaluation_policy", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "\n", "TestEpLen", "=", "ep_len", ",", "\n", "TestTaskRet", "=", "task_ret", ",", "\n", "TestTaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "\n", "", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "iter_begin_time", "=", "start_time", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", ",", "0", "\n", "# Main loop: collect experience in env and update/log each epoch.", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards,", "\n", "# use the learned policy.", "\n", "        ", "if", "t", ">", "start_steps", ":", "\n", "            ", "a", "=", "behavioural_policy", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ")", "\n", "", "else", ":", "\n", "            ", "a", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# Step the environment.", "\n", "", "o2", ",", "r", ",", "d", ",", "info", "=", "env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state).", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer.", "\n", "mask", "=", "sample_mask", "(", ")", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ",", "mask", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling.", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "            ", "logger", ".", "store", "(", "EpRet", "=", "ep_ret", ",", "\n", "EpLen", "=", "ep_len", ",", "\n", "TaskRet", "=", "task_ret", ",", "\n", "TaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", ",", "0", "\n", "\n", "# Update handling.", "\n", "", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "            ", "for", "_", "in", "range", "(", "update_every", ")", ":", "\n", "                ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "results", "=", "learn_on_batch", "(", "**", "batch", ")", "\n", "metrics", "=", "dict", "(", "\n", "LossPi", "=", "results", "[", "'pi_loss'", "]", ",", "\n", "LossQ1", "=", "results", "[", "'q1_loss'", "]", ",", "\n", "LossQ2", "=", "results", "[", "'q2_loss'", "]", ",", "\n", "LogPi", "=", "results", "[", "'logp_pi'", "]", ",", "\n", ")", "\n", "\n", "for", "idx", ",", "(", "q1", ",", "q2", ")", "in", "enumerate", "(", "\n", "zip", "(", "results", "[", "'q1'", "]", ",", "results", "[", "'q2'", "]", ")", ")", ":", "\n", "                    ", "metrics", ".", "update", "(", "{", "\n", "f'Q1Vals_{idx + 1}'", ":", "q1", ",", "\n", "f'Q2Vals_{idx + 1}'", ":", "q2", ",", "\n", "f'QDiff_{idx + 1}'", ":", "np", ".", "abs", "(", "q1", "-", "q2", ")", ",", "\n", "}", ")", "\n", "", "logger", ".", "store", "(", "**", "metrics", ")", "\n", "\n", "if", "autotune_alpha", ":", "\n", "                    ", "results", "=", "update_alpha", "(", "batch", "[", "'obs1'", "]", ")", "\n", "logger", ".", "store", "(", "**", "results", ")", "\n", "logger", ".", "store", "(", "Alpha", "=", "alpha", ".", "numpy", "(", ")", ")", "\n", "\n", "# End of epoch wrap-up.", "\n", "", "", "", "if", "(", "(", "t", "+", "1", ")", "%", "log_every", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "# Test the performance of the deterministic version of the agent.", "\n", "            ", "test_agent", "(", ")", "\n", "\n", "# Log info about epoch.", "\n", "logger", ".", "log_tabular", "(", "'EpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'EpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TotalEnvInteracts'", ",", "t", "+", "1", ")", "\n", "logger", ".", "log_tabular", "(", "'LogPi'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossPi'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ1'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ2'", ",", "average_only", "=", "True", ")", "\n", "for", "idx", "in", "range", "(", "ac_number", ")", ":", "\n", "                ", "logger", ".", "log_tabular", "(", "f'Q1Vals_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "f'Q2Vals_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "f'QDiff_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "\n", "", "if", "autotune_alpha", ":", "\n", "                ", "logger", ".", "log_tabular", "(", "'AlphaLoss'", ")", "\n", "logger", ".", "log_tabular", "(", "'ActorEntropy'", ")", "\n", "logger", ".", "log_tabular", "(", "'Alpha'", ")", "\n", "logger", ".", "log_tabular", "(", "'TargetEntropy'", ",", "target_entropy", ")", "\n", "\n", "", "iter_end_time", "=", "time", ".", "time", "(", ")", "\n", "steps_elapsed", "=", "(", "t", "+", "1", ")", "%", "log_every", "or", "log_every", "\n", "time_elapsed", "=", "iter_end_time", "-", "iter_begin_time", "\n", "logger", ".", "log_tabular", "(", "'StepsPerSecond'", ",", "steps_elapsed", "/", "time_elapsed", ")", "\n", "iter_begin_time", "=", "iter_end_time", "\n", "logger", ".", "log_tabular", "(", "'Time'", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "\n", "logger", ".", "dump_tabular", "(", ")", "\n", "\n", "# Save model.", "\n", "", "if", "(", "(", "t", "+", "1", ")", "%", "save_freq", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "            ", "if", "save_path", "is", "not", "None", ":", "\n", "                ", "tf", ".", "keras", ".", "models", ".", "save_model", "(", "actor", ",", "save_path", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.heuristic_target_entropy": [[14, 23], ["isinstance", "NotImplementedError", "numpy.prod", "type"], "function", ["None"], ["def", "heuristic_target_entropy", "(", "action_space", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "    ", "\"\"\"Copied from https://github.com/rail-berkeley/softlearning/blob/master/softlearning/algorithms/sac.py\"\"\"", "\n", "if", "isinstance", "(", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "# continuous space", "\n", "        ", "heuristic_target_entropy", "=", "-", "np", ".", "prod", "(", "action_space", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "(", "type", "(", "action_space", ")", ",", "action_space", ")", ")", "\n", "\n", "", "return", "heuristic_target_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.gaussian_likelihood": [[25, 32], ["tensorflow.reduce_sum", "numpy.log", "tensorflow.exp"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log"], ["", "def", "gaussian_likelihood", "(", "value", ",", "mu", ",", "log_std", ")", ":", "\n", "    ", "\"\"\"Calculates value's likelihood under Gaussian pdf.\"\"\"", "\n", "pre_sum", "=", "-", "0.5", "*", "(", "\n", "(", "(", "value", "-", "mu", ")", "/", "(", "tf", ".", "exp", "(", "log_std", ")", "+", "EPS", ")", ")", "**", "2", "+", "\n", "2", "*", "log_std", "+", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", "\n", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "pre_sum", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.apply_squashing_func": [[34, 49], ["tensorflow.reduce_sum", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.nn.softplus", "numpy.log"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log"], ["", "def", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", ":", "\n", "    ", "\"\"\"Applies adjustment to mean, pi and log prob.\n\n    This formula is a little bit magic. To get an understanding of where it\n    comes from, check out the original SAC paper (arXiv 1801.01290) and look\n    in appendix C. This is a more numerically-stable equivalent to Eq 21.\n    Try deriving it yourself as a (very difficult) exercise. :)\n    \"\"\"", "\n", "logp_pi", "-=", "tf", ".", "reduce_sum", "(", "\n", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi", "-", "tf", ".", "nn", ".", "softplus", "(", "-", "2", "*", "pi", ")", ")", ",", "axis", "=", "1", ")", "\n", "\n", "# Squash those unbounded actions!", "\n", "mu", "=", "tf", ".", "tanh", "(", "mu", ")", "\n", "pi", "=", "tf", ".", "tanh", "(", "pi", ")", "\n", "return", "mu", ",", "pi", ",", "logp_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.mlp": [[51, 56], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense"], "function", ["None"], ["", "def", "mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "size", ",", "activation", "=", "activation", ")", "\n", "for", "size", "in", "hidden_sizes", "\n", "]", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.layer_norm_mlp": [[58, 65], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.LayerNormalization", "tensorflow.keras.layers.Activation", "core.mlp"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["", "def", "layer_norm_mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "hidden_sizes", "[", "0", "]", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "tf", ".", "nn", ".", "tanh", ")", ",", "\n", "mlp", "(", "hidden_sizes", "[", "1", ":", "]", ",", "activation", ")", "\n", "]", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.mlp_actor_critic": [[67, 119], ["_MlpActor", "tensorflow.keras.Input", "tensorflow.keras.Input", "tensorflow.keras.Sequential", "tensorflow.keras.Model", "tensorflow.keras.layers.Concatenate", "super().__init__", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense", "core.._body", "core.._mu", "core.._log_std", "tensorflow.clip_by_value", "tensorflow.exp", "core.gaussian_likelihood", "core.apply_squashing_func", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Reshape", "tf.keras.Sequential.", "tensorflow.random.normal", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.__init__", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp", "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.gaussian_likelihood", "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.apply_squashing_func", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["", "class", "MLPActorCriticFactory", ":", "\n", "    ", "\"\"\"Factory of MLP stochastic actors and critics.\n\n    Args:\n        observation_space (gym.spaces.Box): A continuous observation space\n          specification.\n        action_space (gym.spaces.Box): A continuous action space\n          specification.\n        hidden_sizes (list): A hidden layers shape specification.\n        activation (tf.function): A hidden layers activations specification.\n        ac_number (int): Number of the actor-critic models in the ensemble.\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "observation_space", ",", "\n", "action_space", ",", "\n", "hidden_sizes", ",", "\n", "activation", ",", "\n", "ac_number", ",", "\n", ")", ":", "\n", "        ", "self", ".", "_obs_dim", "=", "observation_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_scale", "=", "action_space", ".", "high", "[", "0", "]", "\n", "self", ".", "_hidden_sizes", "=", "hidden_sizes", "\n", "self", ".", "_activation", "=", "activation", "\n", "self", ".", "_ac_number", "=", "ac_number", "\n", "\n", "", "def", "_make_actor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the actor model (tf.keras.Model).\"\"\"", "\n", "obs_input", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "self", ".", "_obs_dim", ",", ")", ")", "\n", "body", "=", "mlp", "(", "self", ".", "_hidden_sizes", ",", "self", ".", "_activation", ")", "(", "obs_input", ")", "\n", "mu", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "log_std", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "\n", "log_std", "=", "tf", ".", "clip_by_value", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "tf", ".", "exp", "(", "log_std", ")", "\n", "pi", "=", "mu", "+", "tf", ".", "random", ".", "normal", "(", "tf", ".", "shape", "(", "input", "=", "mu", ")", ")", "*", "std", "\n", "logp_pi", "=", "gaussian_likelihood", "(", "pi", ",", "mu", ",", "log_std", ")", "\n", "\n", "mu", ",", "pi", ",", "logp_pi", "=", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", "\n", "\n", "# Put the actions in the limit.", "\n", "mu", "=", "mu", "*", "self", ".", "_act_scale", "\n", "pi", "=", "pi", "*", "self", ".", "_act_scale", "\n", "\n", "return", "tf", ".", "keras", ".", "Model", "(", "inputs", "=", "obs_input", ",", "outputs", "=", "[", "mu", ",", "pi", ",", "logp_pi", "]", ")", "\n", "\n", "", "def", "make_actor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the ensemble of actor models.\"\"\"", "\n", "obs_inputs", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "None", ",", "self", ".", "_obs_dim", ")", ",", "\n", "batch_size", "=", "self", ".", "_ac_number", ")", "\n", "mus", ",", "pis", ",", "logp_pis", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.sac.ReplayBuffer.__init__": [[18, 25], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "self", ".", "obs1_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acts_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "act_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rews_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.sac.ReplayBuffer.store": [[26, 34], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "self", ".", "obs1_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "acts_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rews_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.sac.ReplayBuffer.sample_batch": [[35, 42], ["numpy.random.randint", "dict", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "return", "dict", "(", "obs1", "=", "tf", ".", "convert_to_tensor", "(", "self", ".", "obs1_buf", "[", "idxs", "]", ")", ",", "\n", "obs2", "=", "tf", ".", "convert_to_tensor", "(", "self", ".", "obs2_buf", "[", "idxs", "]", ")", ",", "\n", "acts", "=", "tf", ".", "convert_to_tensor", "(", "self", ".", "acts_buf", "[", "idxs", "]", ")", ",", "\n", "rews", "=", "tf", ".", "convert_to_tensor", "(", "self", ".", "rews_buf", "[", "idxs", "]", ")", ",", "\n", "done", "=", "tf", ".", "convert_to_tensor", "(", "self", ".", "done_buf", "[", "idxs", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.sac.sac": [[44, 396], ["os.getcwd", "spinup_bis.utils.logx.EpochLogger", "logx.EpochLogger.save_config", "random.seed", "tensorflow.random.set_seed", "numpy.random.seed", "numpy.all", "sac.ReplayBuffer", "actor_critic", "tensorflow.keras.models.clone_model", "critic1.build", "tensorflow.keras.models.clone_model", "tf.keras.models.clone_model.set_weights", "tf.keras.models.clone_model.build", "tensorflow.keras.models.clone_model", "tf.keras.models.clone_model.set_weights", "tensorflow.keras.optimizers.Adam", "time.time", "range", "locals", "env_fn", "env_fn", "critic1.get_weights", "tf.keras.models.clone_model.get_weights", "tensorflow.keras.optimizers.Adam", "tensorflow.Variable", "tensorflow_probability.util.DeferredTensor", "tensorflow.constant", "actor", "tensorflow.cond", "actor", "tape.gradient", "tf.keras.optimizers.Adam.apply_gradients", "dict", "g.gradient", "tf.keras.optimizers.Adam.apply_gradients", "g.gradient", "tf.keras.optimizers.Adam.apply_gradients", "zip", "zip", "dict", "range", "env.reset", "env.step", "info.get", "sac.ReplayBuffer.store", "spinup_bis.algos.tf2.sac.core.heuristic_target_entropy", "tensorflow.math.log", "tensorflow.expand_dims", "tensorflow.GradientTape", "tensorflow.reduce_mean", "zip", "tensorflow.GradientTape", "actor", "critic1", "tf.keras.models.clone_model.", "critic1", "tf.keras.models.clone_model.", "actor", "tf.keras.models.clone_model.", "tf.keras.models.clone_model.", "tensorflow.minimum", "tensorflow.minimum", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "zip", "zip", "target_v.assign", "target_v.assign", "logx.EpochLogger.store", "sac.sac.get_action"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.save_config", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.step", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.ed2-paper_ed2.sac.core.heuristic_target_entropy", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.store"], ["", "", "def", "sac", "(", "env_fn", ",", "actor_critic", "=", "core", ".", "mlp_actor_critic", ",", "ac_kwargs", "=", "None", ",", "seed", "=", "0", ",", "\n", "total_steps", "=", "1_000_000", ",", "log_every", "=", "10_000", ",", "replay_size", "=", "1_000_000", ",", "\n", "gamma", "=", "0.99", ",", "polyak", "=", "0.995", ",", "lr", "=", "0.001", ",", "alpha", "=", "0.2", ",", "batch_size", "=", "256", ",", "\n", "start_steps", "=", "10_000", ",", "update_after", "=", "1000", ",", "update_every", "=", "50", ",", "\n", "num_test_episodes", "=", "10", ",", "max_ep_len", "=", "1000", ",", "logger_kwargs", "=", "None", ",", "\n", "save_freq", "=", "10_000", ",", "save_path", "=", "None", ",", "autotune_alpha", "=", "False", ",", "\n", "target_entropy", "=", "None", ",", "alpha_lr", "=", "3e-4", ")", ":", "\n", "    ", "\"\"\"Soft Actor-Critic (SAC)\n\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n\n        actor_critic: A function which takes in `action_space` and\n            `observation_space` kwargs, and returns actor and critic\n            tf.keras.Model-s.\n\n            Actor should take an observation in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``mu``       (batch, act_dim)  | Computes mean actions from policy\n                                           | given states.\n            ``pi``       (batch, act_dim)  | Samples actions from policy given\n                                           | states.\n            ``logp_pi``  (batch,)          | Gives log probability, according to\n                                           | the policy, of the action sampled\n                                           | by ``pi``. Critical: must be\n                                           | differentiable with respect to\n                                           | policy parameters all the way\n                                           | through action sampling.\n            ===========  ================  =====================================\n\n            Critic should take an observation and an action in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``q``        (batch,)          | Gives one estimate of Q* for\n                                           | states and actions in the input.\n            ===========  ================  =====================================\n\n        ac_kwargs (dict): Any kwargs appropriate for the actor_critic\n            function you provided to SAC.\n\n        seed (int): Seed for random number generators.\n\n        total_steps (int): Number of environment interactions to run and train\n            the agent.\n\n        log_every (int): Number of environment interactions that should elapse\n            between dumping logs.\n\n        replay_size (int): Maximum length of replay buffer.\n\n        gamma (float): Discount factor. (Always between 0 and 1.)\n\n        polyak (float): Interpolation factor in polyak averaging for target\n            networks. Target networks are updated towards main networks\n            according to:\n\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow\n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually\n            close to 1.)\n\n        lr (float): Learning rate (used for both policy and value learning).\n\n        alpha (float): Entropy regularization coefficient. (Equivalent to\n            inverse of reward scale in the original SAC paper.)\n\n        batch_size (int): Minibatch size for SGD.\n\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long\n            you wait between updates, the ratio of env steps to gradient steps\n            is locked to 1.\n\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n\n        logger_kwargs (dict): Keyword args for EpochLogger.\n\n        save_freq (int): How often (in terms of environment iterations) to save\n            the current policy and value function.\n\n        save_path (str): The path specifying where to save the trained actor\n            model. Setting the value to None turns off the saving.\n\n        autotune_alpha (bool): Tune alpha automatically to achieve certain\n            entropy of policy.\n\n        target_entropy (Optional[float]): If not none optimize alpha parameter\n            to encourage actor to achieve selected entropy.\n\n        alpha_lr (float): Learning rate of alpha optimizer. Has effect only when\n            autotune_alpha is not True.\n    \"\"\"", "\n", "pwd", "=", "os", ".", "getcwd", "(", ")", "# pylint: disable=possibly-unused-variable", "\n", "logger", "=", "logx", ".", "EpochLogger", "(", "**", "(", "logger_kwargs", "or", "{", "}", ")", ")", "\n", "logger", ".", "save_config", "(", "locals", "(", ")", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "# This implementation assumes all dimensions share the same bound!", "\n", "assert", "np", ".", "all", "(", "env", ".", "action_space", ".", "high", "==", "env", ".", "action_space", ".", "high", "[", "0", "]", ")", "\n", "\n", "# Share information about observation and action spaces with policy.", "\n", "ac_kwargs", "=", "ac_kwargs", "or", "{", "}", "\n", "ac_kwargs", "[", "'action_space'", "]", "=", "env", ".", "action_space", "\n", "ac_kwargs", "[", "'observation_space'", "]", "=", "env", ".", "observation_space", "\n", "\n", "# Experience buffer.", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "\n", "size", "=", "replay_size", ")", "\n", "\n", "# Build an actor and critics.", "\n", "actor", ",", "critic", "=", "actor_critic", "(", "**", "ac_kwargs", ")", "\n", "\n", "critic1", "=", "critic", "\n", "critic2", "=", "tf", ".", "keras", ".", "models", ".", "clone_model", "(", "critic", ")", "\n", "\n", "input_shape", "=", "[", "(", "None", ",", "obs_dim", ")", ",", "(", "None", ",", "act_dim", ")", "]", "\n", "\n", "critic1", ".", "build", "(", "input_shape", ")", "# Initialize weights.", "\n", "target_critic1", "=", "tf", ".", "keras", ".", "models", ".", "clone_model", "(", "critic", ")", "\n", "target_critic1", ".", "set_weights", "(", "critic1", ".", "get_weights", "(", ")", ")", "\n", "\n", "critic2", ".", "build", "(", "input_shape", ")", "# Initialize weights.", "\n", "target_critic2", "=", "tf", ".", "keras", ".", "models", ".", "clone_model", "(", "critic", ")", "\n", "target_critic2", ".", "set_weights", "(", "critic2", ".", "get_weights", "(", ")", ")", "\n", "\n", "critic_variables", "=", "critic1", ".", "trainable_variables", "+", "critic2", ".", "trainable_variables", "\n", "\n", "optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "lr", ")", "\n", "\n", "if", "autotune_alpha", ":", "\n", "        ", "if", "target_entropy", "is", "None", ":", "\n", "            ", "target_entropy", "=", "core", ".", "heuristic_target_entropy", "(", "env", ".", "action_space", ")", "\n", "", "alpha_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "alpha_lr", ")", "\n", "log_alpha", "=", "tf", ".", "Variable", "(", "tf", ".", "math", ".", "log", "(", "alpha", ")", ")", "\n", "alpha", "=", "tfp", ".", "util", ".", "DeferredTensor", "(", "log_alpha", ",", "tf", ".", "math", ".", "exp", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "get_action", "(", "o", ",", "deterministic", "=", "tf", ".", "constant", "(", "False", ")", ")", ":", "\n", "        ", "mu", ",", "pi", ",", "_", "=", "actor", "(", "tf", ".", "expand_dims", "(", "o", ",", "0", ")", ")", "\n", "result", "=", "tf", ".", "cond", "(", "deterministic", ",", "lambda", ":", "mu", "[", "0", "]", ",", "lambda", ":", "pi", "[", "0", "]", ")", "\n", "return", "result", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "update_alpha", "(", "obs", ")", ":", "\n", "        ", "_", ",", "_", ",", "logp_pi", "=", "actor", "(", "obs", ")", "# batch['obs1']", "\n", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "            ", "alpha_losses", "=", "-", "1.0", "*", "alpha", "*", "tf", ".", "stop_gradient", "(", "logp_pi", "+", "target_entropy", ")", "\n", "alpha_loss", "=", "tf", ".", "reduce_mean", "(", "alpha_losses", ")", "\n", "\n", "", "alpha_gradients", "=", "tape", ".", "gradient", "(", "alpha_loss", ",", "[", "log_alpha", "]", ")", "\n", "alpha_optimizer", ".", "apply_gradients", "(", "zip", "(", "\n", "alpha_gradients", ",", "[", "log_alpha", "]", ")", ")", "\n", "\n", "return", "dict", "(", "\n", "AlphaLoss", "=", "alpha_loss", ",", "\n", "ActorEntropy", "=", "tf", ".", "reduce_mean", "(", "-", "logp_pi", ")", ",", "\n", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "learn_on_batch", "(", "obs1", ",", "obs2", ",", "acts", ",", "rews", ",", "done", ")", ":", "\n", "        ", "with", "tf", ".", "GradientTape", "(", "persistent", "=", "True", ")", "as", "g", ":", "\n", "# Main outputs from computation graph.", "\n", "            ", "_", ",", "pi", ",", "logp_pi", "=", "actor", "(", "obs1", ")", "\n", "q1", "=", "critic1", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "q2", "=", "critic2", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "\n", "# Compose q with pi, for pi-learning.", "\n", "q1_pi", "=", "critic1", "(", "[", "obs1", ",", "pi", "]", ")", "\n", "q2_pi", "=", "critic2", "(", "[", "obs1", ",", "pi", "]", ")", "\n", "\n", "# Get actions and log probs of actions for next states.", "\n", "_", ",", "pi_next", ",", "logp_pi_next", "=", "actor", "(", "obs2", ")", "\n", "\n", "# Target Q-values, using actions from *current* policy.", "\n", "target_q1", "=", "target_critic1", "(", "[", "obs2", ",", "pi_next", "]", ")", "\n", "target_q2", "=", "target_critic2", "(", "[", "obs2", ",", "pi_next", "]", ")", "\n", "\n", "# Min Double-Q:", "\n", "min_q_pi", "=", "tf", ".", "minimum", "(", "q1_pi", ",", "q2_pi", ")", "\n", "min_target_q", "=", "tf", ".", "minimum", "(", "target_q1", ",", "target_q2", ")", "\n", "\n", "# Entropy-regularized Bellman backup for Q functions.", "\n", "# Using Clipped Double-Q targets.", "\n", "q_backup", "=", "tf", ".", "stop_gradient", "(", "rews", "+", "gamma", "*", "(", "1", "-", "done", ")", "*", "(", "\n", "min_target_q", "-", "alpha", "*", "logp_pi_next", ")", ")", "\n", "\n", "# Soft actor-critic losses.", "\n", "pi_loss", "=", "tf", ".", "reduce_mean", "(", "alpha", "*", "logp_pi", "-", "min_q_pi", ")", "\n", "q1_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "(", "q_backup", "-", "q1", ")", "**", "2", ")", "\n", "q2_loss", "=", "0.5", "*", "tf", ".", "reduce_mean", "(", "(", "q_backup", "-", "q2", ")", "**", "2", ")", "\n", "value_loss", "=", "q1_loss", "+", "q2_loss", "\n", "\n", "# Compute gradients and do updates.", "\n", "", "actor_gradients", "=", "g", ".", "gradient", "(", "pi_loss", ",", "actor", ".", "trainable_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "actor_gradients", ",", "actor", ".", "trainable_variables", ")", ")", "\n", "critic_gradients", "=", "g", ".", "gradient", "(", "value_loss", ",", "critic_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "critic_gradients", ",", "critic_variables", ")", ")", "\n", "del", "g", "\n", "\n", "# Polyak averaging for target variables.", "\n", "for", "v", ",", "target_v", "in", "zip", "(", "critic1", ".", "trainable_variables", ",", "\n", "target_critic1", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "", "for", "v", ",", "target_v", "in", "zip", "(", "critic2", ".", "trainable_variables", ",", "\n", "target_critic2", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "\n", "", "return", "dict", "(", "pi_loss", "=", "pi_loss", ",", "\n", "q1_loss", "=", "q1_loss", ",", "\n", "q2_loss", "=", "q2_loss", ",", "\n", "q1", "=", "q1", ",", "\n", "q2", "=", "q2", ",", "\n", "logp_pi", "=", "logp_pi", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time.", "\n", "                ", "o", ",", "r", ",", "d", ",", "info", "=", "test_env", ".", "step", "(", "\n", "get_action", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ",", "tf", ".", "constant", "(", "True", ")", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "\n", "TestEpLen", "=", "ep_len", ",", "\n", "TestTaskRet", "=", "task_ret", ",", "\n", "TestTaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "\n", "", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "iter_begin_time", "=", "start_time", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", ",", "0", "\n", "# Main loop: collect experience in env and update/log each epoch.", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards,", "\n", "# use the learned policy.", "\n", "        ", "if", "t", ">", "start_steps", ":", "\n", "            ", "a", "=", "get_action", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ")", "\n", "", "else", ":", "\n", "            ", "a", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# Step the environment.", "\n", "", "o2", ",", "r", ",", "d", ",", "info", "=", "env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state).", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer.", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling.", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "            ", "logger", ".", "store", "(", "EpRet", "=", "ep_ret", ",", "\n", "EpLen", "=", "ep_len", ",", "\n", "TaskRet", "=", "task_ret", ",", "\n", "TaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", ",", "0", "\n", "\n", "# Update handling.", "\n", "", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "            ", "for", "_", "in", "range", "(", "update_every", ")", ":", "\n", "                ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "results", "=", "learn_on_batch", "(", "**", "batch", ")", "\n", "logger", ".", "store", "(", "LossPi", "=", "results", "[", "'pi_loss'", "]", ",", "\n", "LossQ1", "=", "results", "[", "'q1_loss'", "]", ",", "\n", "LossQ2", "=", "results", "[", "'q2_loss'", "]", ",", "\n", "Q1Vals", "=", "results", "[", "'q1'", "]", ",", "\n", "Q2Vals", "=", "results", "[", "'q2'", "]", ",", "\n", "QDiff", "=", "np", ".", "abs", "(", "results", "[", "'q1'", "]", "-", "results", "[", "'q2'", "]", ")", ",", "\n", "LogPi", "=", "results", "[", "'logp_pi'", "]", ")", "\n", "if", "autotune_alpha", ":", "\n", "                    ", "results", "=", "update_alpha", "(", "batch", "[", "'obs1'", "]", ")", "\n", "logger", ".", "store", "(", "**", "results", ")", "\n", "logger", ".", "store", "(", "Alpha", "=", "alpha", ".", "numpy", "(", ")", ")", "\n", "\n", "# End of epoch wrap-up.", "\n", "", "", "", "if", "(", "(", "t", "+", "1", ")", "%", "log_every", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "# Test the performance of the deterministic version of the agent.", "\n", "            ", "test_agent", "(", ")", "\n", "\n", "# Log info about epoch.", "\n", "logger", ".", "log_tabular", "(", "'EpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'EpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TotalEnvInteracts'", ",", "t", "+", "1", ")", "\n", "logger", ".", "log_tabular", "(", "'Q1Vals'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'Q2Vals'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'QDiff'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LogPi'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossPi'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ1'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ2'", ",", "average_only", "=", "True", ")", "\n", "\n", "if", "autotune_alpha", ":", "\n", "                ", "logger", ".", "log_tabular", "(", "'AlphaLoss'", ")", "\n", "logger", ".", "log_tabular", "(", "'ActorEntropy'", ")", "\n", "logger", ".", "log_tabular", "(", "'Alpha'", ")", "\n", "logger", ".", "log_tabular", "(", "'TargetEntropy'", ",", "target_entropy", ")", "\n", "\n", "", "iter_end_time", "=", "time", ".", "time", "(", ")", "\n", "steps_elapsed", "=", "(", "t", "+", "1", ")", "%", "log_every", "or", "log_every", "\n", "time_elapsed", "=", "iter_end_time", "-", "iter_begin_time", "\n", "logger", ".", "log_tabular", "(", "'StepsPerSecond'", ",", "steps_elapsed", "/", "time_elapsed", ")", "\n", "iter_begin_time", "=", "iter_end_time", "\n", "logger", ".", "log_tabular", "(", "'Time'", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "\n", "logger", ".", "dump_tabular", "(", ")", "\n", "\n", "# Save model.", "\n", "", "if", "(", "(", "t", "+", "1", ")", "%", "save_freq", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "            ", "if", "save_path", "is", "not", "None", ":", "\n", "                ", "tf", ".", "keras", ".", "models", ".", "save_model", "(", "actor", ",", "save_path", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.__init__": [[29, 39], ["None"], "methods", ["None"], ["2", "*", "log_std", "+", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", "\n", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "pre_sum", ",", "axis", "=", "1", ")", "\n", "\n", "\n", "", "def", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_actor": [[40, 59], ["tensorflow.keras.Input", "tensorflow.reduce_mean", "tensorflow.maximum", "tensorflow.keras.Model", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.math.abs", "tensorflow.ones_like", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.random.normal", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["\n", "logp_pi", "-=", "tf", ".", "reduce_sum", "(", "\n", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi", "-", "tf", ".", "nn", ".", "softplus", "(", "-", "2", "*", "pi", ")", ")", ",", "axis", "=", "1", ")", "\n", "\n", "# Squash those unbounded actions!", "\n", "mu", "=", "tf", ".", "tanh", "(", "mu", ")", "\n", "pi", "=", "tf", ".", "tanh", "(", "pi", ")", "\n", "return", "mu", ",", "pi", ",", "logp_pi", "\n", "\n", "\n", "", "def", "mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "size", ",", "activation", "=", "activation", ")", "\n", "for", "size", "in", "hidden_sizes", "\n", "]", ",", "name", ")", "\n", "\n", "\n", "", "def", "layer_norm_mlp", "(", "hidden_sizes", ",", "activation", ",", "name", "=", "None", ")", ":", "\n", "    ", "return", "tf", ".", "keras", ".", "Sequential", "(", "[", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_actor": [[60, 73], ["tensorflow.keras.Input", "tensorflow.unstack", "tensorflow.keras.Model", "core.MLPActorCriticFactory._make_actor", "core.MLPActorCriticFactory.", "mus.append", "pis.append", "tensorflow.stack", "tensorflow.stack"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_actor"], ["tf", ".", "keras", ".", "layers", ".", "Dense", "(", "hidden_sizes", "[", "0", "]", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "LayerNormalization", "(", ")", ",", "\n", "tf", ".", "keras", ".", "layers", ".", "Activation", "(", "tf", ".", "nn", ".", "tanh", ")", ",", "\n", "mlp", "(", "hidden_sizes", "[", "1", ":", "]", ",", "activation", ")", "\n", "]", ",", "name", ")", "\n", "\n", "\n", "", "class", "MLPActorCriticFactory", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_critic": [[75, 90], ["tensorflow.keras.Input", "tensorflow.keras.Input", "tensorflow.keras.Model", "tensorflow.keras.layers.Concatenate", "tensorflow.keras.Sequential", "core.mlp", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Reshape"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp"], ["\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "observation_space", ",", "\n", "action_space", ",", "\n", "hidden_sizes", ",", "\n", "activation", ",", "\n", "ac_number", ",", "\n", ")", ":", "\n", "        ", "self", ".", "_obs_dim", "=", "observation_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "self", ".", "_act_scale", "=", "action_space", ".", "high", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic": [[91, 110], ["tensorflow.keras.Input", "tensorflow.keras.Input", "zip", "tensorflow.keras.Model", "tensorflow.unstack", "tensorflow.unstack", "core.MLPActorCriticFactory._make_critic", "core.MLPActorCriticFactory.", "qs.append", "core.MLPActorCriticFactory._make_critic", "tensorflow.stack", "core.MLPActorCriticFactory."], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory._make_critic"], ["self", ".", "_hidden_sizes", "=", "hidden_sizes", "\n", "self", ".", "_activation", "=", "activation", "\n", "self", ".", "_ac_number", "=", "ac_number", "\n", "\n", "", "def", "_make_actor", "(", "self", ")", ":", "\n", "        ", "\"\"\"Constructs and returns the actor model (tf.keras.Model).\"\"\"", "\n", "obs_input", "=", "tf", ".", "keras", ".", "Input", "(", "shape", "=", "(", "self", ".", "_obs_dim", ",", ")", ")", "\n", "body", "=", "mlp", "(", "self", ".", "_hidden_sizes", ",", "self", ".", "_activation", ")", "(", "obs_input", ")", "\n", "mu", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "log_std", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "self", ".", "_act_dim", ")", "(", "body", ")", "\n", "\n", "log_std", "=", "tf", ".", "clip_by_value", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "tf", ".", "exp", "(", "log_std", ")", "\n", "pi", "=", "mu", "+", "tf", ".", "random", ".", "normal", "(", "tf", ".", "shape", "(", "input", "=", "mu", ")", ")", "*", "std", "\n", "logp_pi", "=", "gaussian_likelihood", "(", "pi", ",", "mu", ",", "log_std", ")", "\n", "\n", "mu", ",", "pi", ",", "logp_pi", "=", "apply_squashing_func", "(", "mu", ",", "pi", ",", "logp_pi", ")", "\n", "\n", "# Put the actions in the limit.", "\n", "mu", "=", "mu", "*", "self", ".", "_act_scale", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.mlp": [[6, 12], ["tensorflow.keras.Sequential", "tensorflow.keras.layers.Dense"], "function", ["None"], ["\n", "\n", "EPS", "=", "1e-8", "\n", "\n", "LOG_STD_MAX", "=", "2", "\n", "LOG_STD_MIN", "=", "-", "20", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.ed2.ReplayBuffer.__init__": [[19, 45], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ",", "ac_number", ",", "max_ep_len", ",", "\n", "init_ere_coeff", ")", ":", "\n", "        ", "self", ".", "obs1_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acts_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "act_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rews_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "size", "=", "0", "\n", "\n", "self", ".", "obs_dim", "=", "obs_dim", "\n", "self", ".", "act_dim", "=", "act_dim", "\n", "self", ".", "max_size", "=", "size", "\n", "self", ".", "ac_number", "=", "ac_number", "\n", "self", ".", "init_ere_coeff", "=", "init_ere_coeff", "\n", "self", ".", "ere_coeff", "=", "init_ere_coeff", "\n", "\n", "# Emphasize Recent Experience (ERE)", "\n", "imprv_span_step", "=", "self", ".", "max_size", "//", "2", "\n", "imprv_span_ep", "=", "imprv_span_step", "/", "max_ep_len", "\n", "self", ".", "warmup_size", "=", "imprv_span_step", "+", "max_ep_len", "\n", "self", ".", "prev_ratio", "=", "1", "/", "imprv_span_ep", "\n", "self", ".", "recent_ratio", "=", "1", "/", "(", "imprv_span_ep", "*", "0.1", ")", "\n", "self", ".", "prev_ep_ret", "=", "None", "\n", "self", ".", "recent_ep_ret", "=", "None", "\n", "self", ".", "max_imprv", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.ed2.ReplayBuffer.store": [[46, 56], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "\"\"\"Store the transitions in the replay buffer.\"\"\"", "\n", "self", ".", "obs1_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "acts_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rews_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.ed2.ReplayBuffer.sample_batch": [[57, 82], ["max", "min", "numpy.random.randint", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "numpy.broadcast_to", "dict", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", ",", "most_recent", ")", ":", "\n", "        ", "\"\"\"Sample batch of (`most_recent`) experience.\"\"\"", "\n", "# Emphasize Recent Experience (ERE)", "\n", "most_recent", "=", "max", "(", "most_recent", ",", "5000", ")", "# Value from the paper.", "\n", "# Guard for when buffer is not full yet.", "\n", "most_recent", "=", "min", "(", "most_recent", ",", "self", ".", "size", ")", "\n", "\n", "idxs", "=", "np", ".", "random", ".", "randint", "(", "\n", "self", ".", "size", "-", "most_recent", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "# Shifts the range to the actual end of the buffer.", "\n", "idxs", "=", "(", "idxs", "+", "self", ".", "ptr", ")", "%", "self", ".", "size", "\n", "\n", "obs_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", ",", "self", ".", "obs_dim", "]", "\n", "act_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", ",", "self", ".", "act_dim", "]", "\n", "rew_shape", "=", "[", "self", ".", "ac_number", ",", "batch_size", "]", "\n", "obs1", "=", "np", ".", "broadcast_to", "(", "self", ".", "obs1_buf", "[", "idxs", "]", ",", "obs_shape", ")", "\n", "obs2", "=", "np", ".", "broadcast_to", "(", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "obs_shape", ")", "\n", "acts", "=", "np", ".", "broadcast_to", "(", "self", ".", "acts_buf", "[", "idxs", "]", ",", "act_shape", ")", "\n", "rews", "=", "np", ".", "broadcast_to", "(", "self", ".", "rews_buf", "[", "idxs", "]", ",", "rew_shape", ")", "\n", "done", "=", "np", ".", "broadcast_to", "(", "self", ".", "done_buf", "[", "idxs", "]", ",", "rew_shape", ")", "\n", "return", "dict", "(", "obs1", "=", "tf", ".", "convert_to_tensor", "(", "obs1", ")", ",", "\n", "obs2", "=", "tf", ".", "convert_to_tensor", "(", "obs2", ")", ",", "\n", "acts", "=", "tf", ".", "convert_to_tensor", "(", "acts", ")", ",", "\n", "rews", "=", "tf", ".", "convert_to_tensor", "(", "rews", ")", ",", "\n", "done", "=", "tf", ".", "convert_to_tensor", "(", "done", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.ed2.ReplayBuffer.end_trajectory": [[83, 105], ["max", "max"], "methods", ["None"], ["", "def", "end_trajectory", "(", "self", ",", "ep_ret", ")", ":", "\n", "        ", "\"\"\"Bookkeeping at the end of the trajectory.\"\"\"", "\n", "if", "self", ".", "prev_ep_ret", "is", "None", "and", "self", ".", "recent_ep_ret", "is", "None", ":", "\n", "            ", "self", ".", "prev_ep_ret", "=", "ep_ret", "\n", "self", ".", "recent_ep_ret", "=", "ep_ret", "\n", "", "else", ":", "\n", "            ", "self", ".", "prev_ep_ret", "=", "(", "self", ".", "prev_ratio", "*", "ep_ret", "+", "\n", "(", "1", "-", "self", ".", "prev_ratio", ")", "*", "self", ".", "prev_ep_ret", ")", "\n", "self", ".", "recent_ep_ret", "=", "(", "self", ".", "recent_ratio", "*", "ep_ret", "+", "\n", "(", "1", "-", "self", ".", "recent_ratio", ")", "*", "self", ".", "recent_ep_ret", ")", "\n", "\n", "# Adapt ERE coeff.", "\n", "", "if", "self", ".", "size", ">", "self", ".", "warmup_size", ":", "\n", "            ", "recent_imprv", "=", "self", ".", "recent_ep_ret", "-", "self", ".", "prev_ep_ret", "\n", "self", ".", "max_imprv", "=", "max", "(", "self", ".", "max_imprv", ",", "recent_imprv", ")", "\n", "\n", "try", ":", "\n", "                ", "imprv_rate", "=", "max", "(", "recent_imprv", "/", "self", ".", "max_imprv", ",", "0.", ")", "\n", "", "except", "ZeroDivisionError", ":", "\n", "                ", "imprv_rate", "=", "0", "\n", "\n", "", "self", ".", "ere_coeff", "=", "self", ".", "init_ere_coeff", "*", "imprv_rate", "+", "(", "1", "-", "imprv_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.ed2.ed2": [[107, 507], ["os.getcwd", "os.getcwd", "spinup_bis.utils.logx.EpochLogger", "logx.EpochLogger.save_config", "spinup_bis.utils.tracing.TraceCallback", "random.seed", "tensorflow.random.set_seed", "numpy.random.seed", "numpy.all", "actor_critic", "actor_critic.make_actor", "ac_factory.make_actor.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "actor_critic.make_critic", "ac_factory.make_critic.build", "ac_factory.make_critic.set_weights", "ac_factory.make_critic.set_weights", "ed2.ReplayBuffer", "tensorflow.keras.optimizers.Adam", "time.time", "ed2.ed2.reset_episode"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.save_config", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_actor", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic", "home.repos.pwc.inspect_result.ed2-paper_ed2.ed2.core.MLPActorCriticFactory.make_critic"], ["", "", "", "def", "ed2", "(", "\n", "env_fn", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCriticFactory", ",", "\n", "ac_kwargs", "=", "None", ",", "\n", "ac_number", "=", "5", ",", "\n", "total_steps", "=", "1_000_000", ",", "\n", "replay_size", "=", "1_000_000", ",", "\n", "init_ere_coeff", "=", "0.995", ",", "\n", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "\n", "lr", "=", "3e-4", ",", "\n", "batch_size", "=", "256", ",", "\n", "start_steps", "=", "10_000", ",", "\n", "update_after", "=", "1_000", ",", "\n", "update_every", "=", "50", ",", "\n", "train_intensity", "=", "1", ",", "\n", "act_noise", "=", "0.29", ",", "\n", "use_noise_for_exploration", "=", "False", ",", "\n", "use_vote_policy", "=", "False", ",", "\n", "max_ep_len", "=", "1_000", ",", "\n", "num_test_episodes", "=", "10", ",", "\n", "logger_kwargs", "=", "None", ",", "\n", "log_every", "=", "10_000", ",", "\n", "save_freq", "=", "10_000", ",", "\n", "save_path", "=", "None", ",", "\n", "trace_rate", "=", "None", ",", "\n", "seed", "=", "0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Ensemble Deep Deterministic Policy Gradients.\n\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n\n        actor_critic: A function which takes in `action_space` kwargs\n            and returns actor and critic tf.keras.Model-s.\n\n            Actor should take an observation in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``pi``       (batch, act_dim)  | Deterministically computes actions\n                                           | from policy given states.\n            ===========  ================  =====================================\n\n            Critic should take an observation and action in and output:\n            ===========  ================  =====================================\n            Symbol       Shape             Description\n            ===========  ================  =====================================\n            ``q``        (batch,)          | Gives the current estimate of Q*\n                                           | state and action in the input.\n            ===========  ================  =====================================\n\n        ac_kwargs (dict): Any kwargs appropriate for the actor_critic\n            function you provided to the agent.\n\n        ac_number (int): Number of the actor-critic models in the ensemble.\n\n        total_steps (int): Number of environment interactions to run and train\n            the agent.\n\n        replay_size (int): Maximum length of replay buffer.\n\n        init_ere_coeff (float): How much emphasis we put on recent data.\n            Always between 0 and 1, where 1 is uniform sampling.\n\n        gamma (float): Discount factor. (Always between 0 and 1.)\n\n        polyak (float): Interpolation factor in polyak averaging for target\n            networks. Target networks are updated towards main networks\n            according to:\n\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow\n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually\n            close to 1.).\n\n        lr (float): Learning rate (used for both policy and value learning).\n\n        batch_size (int): Minibatch size for SGD.\n\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long\n            you wait between updates, the ratio of env steps to gradient steps\n            is locked to 1 / `train_intensity`.\n\n        train_intensity (float): Number of gradient steps per each env step (see\n            `update_every` docstring).\n\n        act_noise (float): Stddev for Gaussian exploration noise added to\n            policy at training time (for exploration and smoothing).\n\n        use_noise_for_exploration (bool): If the noise should be added to the\n            behaviour policy.\n\n        use_vote_policy (bool): If true use vote_policy during evaluation\n            instead of default mean_policy.\n\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n\n        logger_kwargs (dict): Keyword args for EpochLogger.\n\n        log_every (int): Number of environment interactions that should elapse\n            between dumping logs.\n\n        save_freq (int): How often (in terms of environment iterations) to save\n            the current policy and value function.\n\n        save_path (str): The path specifying where to save the trained actor\n            model. Setting the value to None turns off the saving.\n\n        trace_rate (float): Fraction of episodes to trace, or None if traces\n            shouldn't be saved.\n\n        seed (int): Seed for random number generators.\n    \"\"\"", "\n", "pwd", "=", "os", ".", "getcwd", "(", ")", "# pylint: disable=possibly-unused-variable", "\n", "logger", "=", "logx", ".", "EpochLogger", "(", "**", "(", "logger_kwargs", "or", "{", "}", ")", ")", "\n", "logger", ".", "save_config", "(", "locals", "(", ")", ")", "\n", "\n", "trace_dir", "=", "(", "osp", ".", "join", "(", "logger", ".", "output_dir", ",", "'traces'", ")", "\n", "if", "trace_rate", "is", "not", "None", "else", "None", ")", "\n", "tracer", "=", "tracing", ".", "TraceCallback", "(", "trace_dir", ",", "trace_rate", "or", "0.0", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "# This implementation assumes all dimensions share the same bound!", "\n", "assert", "np", ".", "all", "(", "env", ".", "action_space", ".", "high", "==", "env", ".", "action_space", ".", "high", "[", "0", "]", ")", "\n", "\n", "# Share information about action space with policy architecture", "\n", "ac_kwargs", "=", "ac_kwargs", "or", "{", "}", "\n", "ac_kwargs", "[", "'observation_space'", "]", "=", "env", ".", "observation_space", "\n", "ac_kwargs", "[", "'action_space'", "]", "=", "env", ".", "action_space", "\n", "ac_kwargs", "[", "'act_noise'", "]", "=", "act_noise", "\n", "ac_kwargs", "[", "'ac_number'", "]", "=", "ac_number", "\n", "\n", "# Network", "\n", "ac_factory", "=", "actor_critic", "(", "**", "ac_kwargs", ")", "\n", "\n", "actor", "=", "ac_factory", ".", "make_actor", "(", ")", "\n", "actor", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", ")", ")", "\n", "\n", "critic1", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "critic1", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "critic2", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "critic2", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "\n", "critic_variables", "=", "critic1", ".", "trainable_variables", "+", "critic2", ".", "trainable_variables", "\n", "\n", "# Target networks", "\n", "target_critic1", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "target_critic1", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "target_critic2", "=", "ac_factory", ".", "make_critic", "(", ")", "\n", "target_critic2", ".", "build", "(", "input_shape", "=", "(", "None", ",", "obs_dim", "+", "act_dim", ")", ")", "\n", "\n", "# Copy weights", "\n", "target_critic1", ".", "set_weights", "(", "critic1", ".", "get_weights", "(", ")", ")", "\n", "target_critic2", ".", "set_weights", "(", "critic2", ".", "get_weights", "(", ")", ")", "\n", "\n", "# Experience buffer", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "obs_dim", "=", "obs_dim", ",", "\n", "act_dim", "=", "act_dim", ",", "\n", "size", "=", "replay_size", ",", "\n", "ac_number", "=", "ac_number", ",", "\n", "max_ep_len", "=", "max_ep_len", ",", "\n", "init_ere_coeff", "=", "init_ere_coeff", ")", "\n", "\n", "# Separate train ops for pi, q", "\n", "optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "learning_rate", "=", "lr", ")", "\n", "\n", "@", "tf", ".", "function", "\n", "def", "vote_evaluation_policy", "(", "obs", ")", ":", "\n", "        ", "obs_actor", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "1", ",", "*", "obs", ".", "shape", "]", ")", "\n", "obs_critic", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "ac_number", ",", "*", "obs", ".", "shape", "]", ")", "\n", "\n", "mu", ",", "_", "=", "actor", "(", "obs_actor", ")", "\n", "# One action per batch.", "\n", "act", "=", "tf", ".", "reshape", "(", "mu", ",", "[", "1", ",", "ac_number", ",", "*", "mu", ".", "shape", "[", "2", ":", "]", "]", ")", "\n", "# The same action for each component.", "\n", "act", "=", "tf", ".", "broadcast_to", "(", "act", ",", "[", "ac_number", ",", "ac_number", ",", "*", "mu", ".", "shape", "[", "2", ":", "]", "]", ")", "\n", "# Evaluate each action by all components.", "\n", "qs", "=", "critic1", "(", "[", "obs_critic", ",", "act", "]", ")", "\n", "# Average over ensemble.", "\n", "qs", "=", "tf", ".", "reduce_mean", "(", "qs", ",", "axis", "=", "0", ")", "\n", "\n", "return", "mu", "[", "tf", ".", "math", ".", "argmax", "(", "qs", ")", "]", "[", "0", "]", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "mean_evaluation_policy", "(", "obs", ")", ":", "\n", "        ", "obs_actor", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "1", ",", "*", "obs", ".", "shape", "]", ")", "\n", "mu", ",", "_", "=", "actor", "(", "obs_actor", ")", "\n", "return", "tf", ".", "reduce_mean", "(", "mu", ",", "axis", "=", "0", ")", "[", "0", "]", "\n", "\n", "", "if", "use_vote_policy", ":", "\n", "        ", "evaluation_policy", "=", "vote_evaluation_policy", "\n", "", "else", ":", "\n", "        ", "evaluation_policy", "=", "mean_evaluation_policy", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "behavioural_policy", "(", "obs", ",", "ac_idx", ",", "use_noise", ")", ":", "\n", "        ", "obs", "=", "tf", ".", "broadcast_to", "(", "obs", ",", "[", "ac_number", ",", "1", ",", "*", "obs", ".", "shape", "]", ")", "\n", "mu", ",", "pi", "=", "actor", "(", "obs", ")", "\n", "if", "use_noise", ":", "\n", "            ", "return", "pi", "[", "ac_idx", ",", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "mu", "[", "ac_idx", ",", "0", "]", "\n", "\n", "", "", "@", "tf", ".", "function", "\n", "def", "learn_on_batch", "(", "obs1", ",", "obs2", ",", "acts", ",", "rews", ",", "done", ")", ":", "\n", "        ", "with", "tf", ".", "GradientTape", "(", "persistent", "=", "True", ")", "as", "g", ":", "\n", "# Actor update.", "\n", "            ", "mu", ",", "_", "=", "actor", "(", "obs1", ")", "\n", "q_pi", "=", "critic1", "(", "[", "obs1", ",", "mu", "]", ")", "\n", "pi_loss", "=", "-", "tf", ".", "reduce_mean", "(", "q_pi", ")", "\n", "\n", "# Critic update.", "\n", "q1", "=", "critic1", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "q2", "=", "critic2", "(", "[", "obs1", ",", "acts", "]", ")", "\n", "\n", "_", ",", "pi_next", "=", "actor", "(", "obs2", ")", "\n", "min_target_q", "=", "tf", ".", "minimum", "(", "\n", "target_critic1", "(", "[", "obs2", ",", "pi_next", "]", ")", ",", "\n", "target_critic2", "(", "[", "obs2", ",", "pi_next", "]", ")", ",", "\n", ")", "\n", "\n", "# Bellman backup for Q function.", "\n", "q_backup", "=", "tf", ".", "stop_gradient", "(", "\n", "rews", "+", "gamma", "*", "(", "1", "-", "done", ")", "*", "min_target_q", ")", "\n", "q1_loss", "=", "tf", ".", "reduce_mean", "(", "(", "q_backup", "-", "q1", ")", "**", "2", ")", "\n", "q2_loss", "=", "tf", ".", "reduce_mean", "(", "(", "q_backup", "-", "q2", ")", "**", "2", ")", "\n", "value_loss", "=", "(", "q1_loss", "+", "q2_loss", ")", "*", "0.5", "\n", "\n", "", "actor_gradients", "=", "g", ".", "gradient", "(", "pi_loss", ",", "actor", ".", "trainable_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "actor_gradients", ",", "actor", ".", "trainable_variables", ")", "\n", ")", "\n", "critic_gradients", "=", "g", ".", "gradient", "(", "value_loss", ",", "critic_variables", ")", "\n", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "critic_gradients", ",", "critic_variables", ")", ")", "\n", "\n", "for", "v", ",", "target_v", "in", "zip", "(", "critic1", ".", "trainable_variables", ",", "\n", "target_critic1", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "", "for", "v", ",", "target_v", "in", "zip", "(", "critic2", ".", "trainable_variables", ",", "\n", "target_critic2", ".", "trainable_variables", ")", ":", "\n", "            ", "target_v", ".", "assign", "(", "polyak", "*", "target_v", "+", "(", "1", "-", "polyak", ")", "*", "v", ")", "\n", "\n", "", "del", "g", "\n", "return", "dict", "(", "\n", "pi_loss", "=", "pi_loss", ",", "\n", "q1_loss", "=", "q1_loss", ",", "\n", "q2_loss", "=", "q2_loss", ",", "\n", "q1", "=", "q1", ",", "\n", "q2", "=", "q2", ",", "\n", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "                ", "o", ",", "r", ",", "d", ",", "info", "=", "test_env", ".", "step", "(", "\n", "evaluation_policy", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "\n", "TestEpLen", "=", "ep_len", ",", "\n", "TestTaskRet", "=", "task_ret", ",", "\n", "TestTaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "\n", "", "", "def", "reset_episode", "(", "epoch", ")", ":", "\n", "        ", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", ",", "0", "\n", "tracer", ".", "on_episode_begin", "(", "env", ",", "o", ",", "epoch", ")", "\n", "actor_idx", "=", "np", ".", "random", ".", "choice", "(", "ac_number", ")", "# Select policy", "\n", "return", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", ",", "actor_idx", "\n", "\n", "# Prepare for interaction with environment", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", ",", "actor_idx", "=", "reset_episode", "(", "epoch", "=", "0", ")", "\n", "\n", "# Main loop: collect experience in env and update/log each epoch", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards,", "\n", "# use the learned policy (with some noise, via act_noise).", "\n", "        ", "if", "t", ">", "start_steps", ":", "\n", "            ", "a", "=", "behavioural_policy", "(", "tf", ".", "convert_to_tensor", "(", "o", ")", ",", "\n", "actor_idx", ",", "\n", "use_noise_for_exploration", ")", "\n", "", "else", ":", "\n", "            ", "a", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# Step the env", "\n", "", "o2", ",", "r", ",", "d", ",", "info", "=", "env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "task_ret", "+=", "info", ".", "get", "(", "'reward_task'", ",", "0", ")", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state)", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Trace experience.", "\n", "info", "[", "'actor_idx'", "]", "=", "actor_idx", "\n", "info", "[", "'total_steps'", "]", "=", "t", "+", "1", "\n", "tracer", ".", "on_real_step", "(", "info", ",", "a", ",", "o2", ",", "r", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling.", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "            ", "logger", ".", "store", "(", "EpRet", "=", "ep_ret", ",", "\n", "EpLen", "=", "ep_len", ",", "\n", "TaskRet", "=", "task_ret", ",", "\n", "TaskSolved", "=", "info", ".", "get", "(", "'is_solved'", ",", "False", ")", ")", "\n", "replay_buffer", ".", "end_trajectory", "(", "ep_ret", ")", "\n", "tracer", ".", "on_episode_end", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", ",", "task_ret", ",", "actor_idx", "=", "reset_episode", "(", "\n", "epoch", "=", "(", "t", "+", "1", ")", "//", "log_every", ")", "\n", "\n", "# Update handling", "\n", "", "if", "(", "t", "+", "1", ")", ">=", "update_after", "and", "(", "t", "+", "1", ")", "%", "update_every", "==", "0", ":", "\n", "            ", "number_of_updates", "=", "int", "(", "update_every", "*", "train_intensity", ")", "\n", "for", "n", "in", "range", "(", "number_of_updates", ")", ":", "\n", "                ", "most_recent", "=", "(", "\n", "replay_buffer", ".", "max_size", "*", "replay_buffer", ".", "ere_coeff", "**", "(", "\n", "(", "n", "+", "1", ")", "*", "1000", "/", "number_of_updates", ")", ")", "\n", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ",", "most_recent", ")", "\n", "results", "=", "learn_on_batch", "(", "**", "batch", ")", "\n", "metrics", "=", "dict", "(", "EREcoeff", "=", "replay_buffer", ".", "ere_coeff", ",", "\n", "LossPi", "=", "results", "[", "'pi_loss'", "]", ",", "\n", "LossQ1", "=", "results", "[", "'q1_loss'", "]", ",", "\n", "LossQ2", "=", "results", "[", "'q2_loss'", "]", ")", "\n", "for", "idx", ",", "(", "q1", ",", "q2", ")", "in", "enumerate", "(", "\n", "zip", "(", "results", "[", "'q1'", "]", ",", "results", "[", "'q2'", "]", ")", ")", ":", "\n", "                    ", "metrics", ".", "update", "(", "{", "\n", "f'Q1Vals_{idx + 1}'", ":", "q1", ",", "\n", "f'Q2Vals_{idx + 1}'", ":", "q2", ",", "\n", "f'QDiff_{idx + 1}'", ":", "np", ".", "abs", "(", "q1", "-", "q2", ")", ",", "\n", "}", ")", "\n", "", "logger", ".", "store", "(", "**", "metrics", ")", "\n", "\n", "# End of epoch wrap-up", "\n", "", "", "if", "(", "(", "t", "+", "1", ")", "%", "log_every", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "# Test the performance of the deterministic version of the agent.", "\n", "            ", "test_agent", "(", ")", "\n", "\n", "# Log info about epoch.", "\n", "logger", ".", "log_tabular", "(", "'EpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpRet'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'EpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestEpLen'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskRet'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TestTaskSolved'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'TotalEnvInteracts'", ",", "t", "+", "1", ")", "\n", "logger", ".", "log_tabular", "(", "'EREcoeff'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossPi'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ1'", ",", "average_only", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "'LossQ2'", ",", "average_only", "=", "True", ")", "\n", "for", "idx", "in", "range", "(", "ac_number", ")", ":", "\n", "                ", "logger", ".", "log_tabular", "(", "f'Q1Vals_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "f'Q2Vals_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "logger", ".", "log_tabular", "(", "f'QDiff_{idx + 1}'", ",", "with_min_and_max", "=", "True", ")", "\n", "", "logger", ".", "log_tabular", "(", "'StepsPerSecond'", ",", "\n", "log_every", "/", "(", "time", ".", "time", "(", ")", "-", "iter_time", ")", ")", "\n", "logger", ".", "log_tabular", "(", "'Time'", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "logger", ".", "dump_tabular", "(", ")", "\n", "\n", "iter_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Save model", "\n", "", "if", "(", "(", "t", "+", "1", ")", "%", "save_freq", "==", "0", ")", "or", "(", "t", "+", "1", "==", "total_steps", ")", ":", "\n", "            ", "if", "save_path", "is", "not", "None", ":", "\n", "                ", "tf", ".", "keras", ".", "models", ".", "save_model", "(", "actor", ",", "save_path", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.proc_id": [[10, 13], ["None"], "function", ["None"], ["def", "proc_id", "(", ")", ":", "\n", "    ", "\"\"\"Get rank of calling process.\"\"\"", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.mpi_statistics_scalar": [[15, 34], ["numpy.mean", "numpy.std", "numpy.min", "numpy.max"], "function", ["None"], ["", "def", "mpi_statistics_scalar", "(", "data", ",", "with_min_and_max", "=", "False", ")", ":", "\n", "    ", "\"\"\"Get mean/std and optional min/max of scalar data across MPI processes.\n\n    Args:\n        data: An array containing samples of the scalar to produce statistics\n            for.\n        with_min_and_max (bool): If true, return min and max of x in\n            addition to mean and std.\n\n    Returns:\n        Aggregate statistics.\n    \"\"\"", "\n", "mean", "=", "np", ".", "mean", "(", "data", ")", "\n", "std", "=", "np", ".", "std", "(", "data", ")", "\n", "if", "with_min_and_max", ":", "\n", "        ", "minimum", "=", "np", ".", "min", "(", "data", ")", "\n", "maximum", "=", "np", ".", "max", "(", "data", ")", "\n", "return", "mean", ",", "std", ",", "minimum", ",", "maximum", "\n", "", "return", "mean", ",", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.mpi_histogram": [[36, 47], ["numpy.histogram"], "function", ["None"], ["", "def", "mpi_histogram", "(", "data", ")", ":", "\n", "    ", "\"\"\"Get mean/std and optional min/max of scalar data across MPI processes.\n\n    Args:\n        data (list): An array containing samples to produce histogram for.\n\n    Returns:\n        numpy.ndarray: The values of the histogram.\n        numpy.ndarray: The bin edges (length(hist)+1).\n    \"\"\"", "\n", "return", "np", ".", "histogram", "(", "data", ",", "bins", "=", "20", ",", "density", "=", "True", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.DummyRenderer.__init__": [[92, 95], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Initializes EnvRenderer.\"\"\"", "\n", "del", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.DummyRenderer.render_state": [[96, 99], ["None"], "methods", ["None"], ["", "def", "render_state", "(", "self", ",", "state_info", ")", ":", "\n", "        ", "\"\"\"Renders state_info to an image.\"\"\"", "\n", "del", "state_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.DummyRenderer.render_action": [[100, 103], ["str"], "methods", ["None"], ["", "def", "render_action", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"Renders action to a string.\"\"\"", "\n", "return", "str", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.__init__": [[108, 132], ["os.makedirs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "output_dir", "=", "None", ",", "sample_rate", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"Initializes TraceCallback.\n\n        Args:\n            output_dir (str): Directory to save traces in, or None if traces\n                shouldn't be saved.\n            sample_rate (float): Fraction of episodes to trace.\n        \"\"\"", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "if", "self", ".", "_output_dir", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "self", ".", "_sample_rate", "=", "sample_rate", "\n", "\n", "self", ".", "_env", "=", "None", "\n", "self", ".", "_epoch", "=", "None", "\n", "self", ".", "_trace", "=", "None", "\n", "# Trajectory trace.", "\n", "self", ".", "_trajectory", "=", "None", "\n", "self", ".", "_passes", "=", "None", "\n", "self", ".", "_pass", "=", "None", "\n", "# Tree trace.", "\n", "self", ".", "_tree_nodes", "=", "None", "\n", "self", ".", "_current_root_id", "=", "None", "\n", "self", ".", "_current_node_id", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.trace": [[133, 136], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "trace", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_trace", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_episode_begin": [[137, 159], ["getattr", "Trajectory", "random.random", "TreeNode", "State"], "methods", ["None"], ["", "def", "on_episode_begin", "(", "self", ",", "env", ",", "observation", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Called in the beginning of a new episode.\"\"\"", "\n", "# Sample a fraction of episodes to dump traces from.", "\n", "if", "random", ".", "random", "(", ")", ">", "self", ".", "_sample_rate", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "_env", "=", "env", "\n", "self", ".", "_epoch", "=", "epoch", "\n", "self", ".", "_trace", "=", "None", "\n", "state_info", "=", "getattr", "(", "self", ".", "_env", ",", "'state_info'", ",", "observation", ")", "\n", "self", ".", "_trajectory", "=", "Trajectory", "(", "\n", "init_state", "=", "State", "(", "\n", "state_info", "=", "state_info", ",", "\n", "node_id", "=", "0", ",", "\n", "terminal", "=", "False", ",", "\n", ")", ",", "\n", "transitions", "=", "[", "]", ",", "\n", ")", "\n", "self", ".", "_passes", "=", "[", "]", "\n", "\n", "self", ".", "_tree_nodes", "=", "[", "TreeNode", "(", "agent_info", "=", "None", ",", "children", "=", "{", "}", ")", "]", "\n", "self", ".", "_current_root_id", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_pass_begin": [[160, 167], ["None"], "methods", ["None"], ["", "def", "on_pass_begin", "(", "self", ")", ":", "\n", "        ", "\"\"\"Called in the beginning of every planning pass.\"\"\"", "\n", "if", "self", ".", "_trajectory", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "_pass", "=", "[", "]", "\n", "self", ".", "_current_node_id", "=", "self", ".", "_current_root_id", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_model_step": [[168, 186], ["tracing.TraceCallback._traverse_tree", "getattr", "tracing.TraceCallback._pass.append", "copy.deepcopy", "ModelTransition", "tracing.TraceCallback._filter_agent_info", "State"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._traverse_tree", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._filter_agent_info"], ["", "def", "on_model_step", "(", "self", ",", "agent_info", ",", "action", ",", "observation", ",", "reward", ",", "done", ")", ":", "\n", "        ", "\"\"\"Called after every step in the model.\"\"\"", "\n", "if", "self", ".", "_trajectory", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "_current_node_id", "=", "self", ".", "_traverse_tree", "(", "\n", "self", ".", "_current_node_id", ",", "agent_info", ",", "action", "\n", ")", "\n", "\n", "state_info", "=", "getattr", "(", "self", ".", "_env", ",", "'state_info'", ",", "observation", ")", "\n", "self", ".", "_pass", ".", "append", "(", "copy", ".", "deepcopy", "(", "ModelTransition", "(", "\n", "agent_info", "=", "self", ".", "_filter_agent_info", "(", "agent_info", ")", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "to_state", "=", "State", "(", "\n", "state_info", "=", "state_info", ",", "\n", "node_id", "=", "self", ".", "_current_node_id", ",", "\n", "terminal", "=", "done", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_pass_end": [[189, 196], ["tracing.TraceCallback._passes.append"], "methods", ["None"], ["", "def", "on_pass_end", "(", "self", ")", ":", "\n", "        ", "\"\"\"Called in the end of every planning pass.\"\"\"", "\n", "if", "self", ".", "_trajectory", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "_passes", ".", "append", "(", "self", ".", "_pass", ")", "\n", "self", ".", "_pass", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_real_step": [[197, 220], ["tracing.TraceCallback._traverse_tree", "getattr", "tracing.TraceCallback._trajectory.transitions.append", "copy.deepcopy", "RealTransition", "tracing.TraceCallback._filter_agent_info", "State"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._traverse_tree", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._filter_agent_info"], ["", "def", "on_real_step", "(", "self", ",", "agent_info", ",", "action", ",", "observation", ",", "reward", ",", "done", ")", ":", "\n", "        ", "\"\"\"Called after every step in the real environment.\"\"\"", "\n", "if", "self", ".", "_trajectory", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "_current_root_id", "=", "self", ".", "_traverse_tree", "(", "\n", "self", ".", "_current_root_id", ",", "agent_info", ",", "action", "\n", ")", "\n", "self", ".", "_current_node_id", "=", "self", ".", "_current_root_id", "\n", "\n", "state_info", "=", "getattr", "(", "self", ".", "_env", ",", "'state_info'", ",", "observation", ")", "\n", "self", ".", "_trajectory", ".", "transitions", ".", "append", "(", "copy", ".", "deepcopy", "(", "RealTransition", "(", "\n", "agent_info", "=", "self", ".", "_filter_agent_info", "(", "agent_info", ")", ",", "\n", "passes", "=", "self", ".", "_passes", ",", "\n", "action", "=", "action", ",", "\n", "reward", "=", "reward", ",", "\n", "to_state", "=", "State", "(", "\n", "state_info", "=", "state_info", ",", "\n", "node_id", "=", "self", ".", "_current_root_id", ",", "\n", "terminal", "=", "done", ",", "\n", ")", ",", "\n", ")", ")", ")", "\n", "self", ".", "_passes", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback.on_episode_end": [[221, 253], ["Trace", "os.path.join", "tracing.dump", "type", "renderer_class", "random.randrange"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.dump"], ["", "def", "on_episode_end", "(", "self", ")", ":", "\n", "        ", "\"\"\"Called in the end of an episode.\"\"\"", "\n", "if", "self", ".", "_trajectory", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "try", ":", "\n", "            ", "renderer_class", "=", "type", "(", "self", ".", "_env", ".", "unwrapped", ")", ".", "Renderer", "\n", "", "except", "AttributeError", ":", "\n", "            ", "renderer_class", "=", "DummyRenderer", "\n", "\n", "", "self", ".", "_trace", "=", "Trace", "(", "\n", "renderer", "=", "renderer_class", "(", "self", ".", "_env", ")", ",", "\n", "trajectory", "=", "self", ".", "_trajectory", ",", "\n", "tree", "=", "self", ".", "_tree_nodes", ",", "\n", ")", "\n", "self", ".", "_env", "=", "None", "\n", "self", ".", "_trajectory", "=", "None", "\n", "self", ".", "_passes", "=", "None", "\n", "self", ".", "_pass", "=", "None", "\n", "self", ".", "_tree_nodes", "=", "None", "\n", "self", ".", "_current_root_id", "=", "None", "\n", "self", ".", "_current_node_id", "=", "None", "\n", "\n", "if", "self", ".", "_output_dir", "is", "not", "None", ":", "\n", "# Create a random, hopefully unique hexadecimal id for the trace to", "\n", "# avoid race conditions.", "\n", "            ", "trace_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_output_dir", ",", "'ep{}_{:06x}'", ".", "format", "(", "\n", "self", ".", "_epoch", ",", "random", ".", "randrange", "(", "16", "**", "6", ")", "\n", ")", "\n", ")", "\n", "dump", "(", "self", ".", "trace", ",", "trace_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._next_node_id": [[254, 257], ["len"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "_next_node_id", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_tree_nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._traverse_tree": [[258, 271], ["tracing.TraceCallback._tree_nodes[]._replace", "bytes", "tracing.TraceCallback._tree_nodes.append", "tracing.TraceCallback._filter_agent_info", "TreeNode"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._filter_agent_info"], ["", "def", "_traverse_tree", "(", "self", ",", "node_id", ",", "agent_info", ",", "action", ")", ":", "\n", "        ", "node", "=", "self", ".", "_tree_nodes", "[", "node_id", "]", ".", "_replace", "(", "\n", "agent_info", "=", "self", ".", "_filter_agent_info", "(", "agent_info", ")", "\n", ")", "\n", "self", ".", "_tree_nodes", "[", "node_id", "]", "=", "node", "\n", "action_bytes", "=", "bytes", "(", "action", ")", "# Ensure an action is hashable.", "\n", "if", "action_bytes", "not", "in", "node", ".", "children", ":", "\n", "            ", "node", ".", "children", "[", "action_bytes", "]", "=", "self", ".", "_next_node_id", "\n", "self", ".", "_tree_nodes", ".", "append", "(", "TreeNode", "(", "\n", "agent_info", "=", "None", ",", "\n", "children", "=", "{", "}", ",", "\n", ")", ")", "\n", "", "return", "node", ".", "children", "[", "action_bytes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.TraceCallback._filter_agent_info": [[272, 279], ["agent_info.items", "key.startswith"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_filter_agent_info", "(", "agent_info", ")", ":", "\n", "        ", "\"\"\"Filters out \"private\" keys - starting with an underscore.\"\"\"", "\n", "return", "{", "\n", "key", ":", "value", "\n", "for", "(", "key", ",", "value", ")", "in", "agent_info", ".", "items", "(", ")", "\n", "if", "not", "key", ".", "startswith", "(", "'_'", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.dump": [[282, 286], ["lzma.open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.dump"], ["", "", "def", "dump", "(", "trace", ",", "path", ")", ":", "\n", "    ", "\"\"\"Dumps a trace to a given path.\"\"\"", "\n", "with", "lzma", ".", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "trace", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.load": [[288, 292], ["lzma.open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.tracing.load"], ["", "", "def", "load", "(", "path", ")", ":", "\n", "    ", "\"\"\"Loads a trace from a given path.\"\"\"", "\n", "with", "lzma", ".", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.__init__": [[68, 106], ["spinup_bis.utils.mpi_tools.proc_id", "os.exists", "os.exists", "open", "atexit.register", "print", "print", "os.makedirs", "os.makedirs", "os.makedirs", "os.makedirs", "os.join", "os.join", "logx.colorize", "time.time"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.proc_id", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.colorize"], ["def", "__init__", "(", "self", ",", "\n", "output_dir", "=", "None", ",", "\n", "output_fname", "=", "'progress.txt'", ",", "\n", "exp_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize a Logger.\n\n        Args:\n            output_dir (string): A directory for saving results to. If\n                ``None``, defaults to a temp directory of the form\n                ``/tmp/experiments/somerandomnumber``.\n            output_fname (string): Name for the tab-separated-value file\n                containing metrics logged throughout a training run.\n                Defaults to ``progress.txt``.\n            exp_name (string): Experiment name. If you run multiple training\n                runs and give them all the same ``exp_name``, the plotter\n                will know to group them. (Use case: if you run the same\n                hyperparameter configuration with multiple random seeds, you\n                should give them all the same ``exp_name``.)\n        \"\"\"", "\n", "if", "mpi_tools", ".", "proc_id", "(", ")", "==", "0", ":", "\n", "            ", "self", ".", "output_dir", "=", "output_dir", "or", "f'./out/{time.time():.0f}'", "\n", "if", "osp", ".", "exists", "(", "self", ".", "output_dir", ")", ":", "\n", "                ", "print", "(", "f'Warning: Log dir {self.output_dir} already exists!'", "\n", "' Storing info there anyway.'", ")", "\n", "", "else", ":", "\n", "                ", "os", ".", "makedirs", "(", "self", ".", "output_dir", ")", "\n", "", "self", ".", "output_file", "=", "open", "(", "\n", "osp", ".", "join", "(", "self", ".", "output_dir", ",", "output_fname", ")", ",", "'w'", ")", "\n", "atexit", ".", "register", "(", "self", ".", "output_file", ".", "close", ")", "\n", "print", "(", "colorize", "(", "\n", "f'Logging data to {self.output_file}.'", ",", "'green'", ",", "bold", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "output_dir", "=", "None", "\n", "self", ".", "output_file", "=", "None", "\n", "", "self", ".", "first_row", "=", "True", "\n", "self", ".", "log_headers", "=", "[", "]", "\n", "self", ".", "log_current_row", "=", "{", "}", "\n", "self", ".", "exp_name", "=", "exp_name", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log": [[107, 112], ["spinup_bis.utils.mpi_tools.proc_id", "print", "logx.colorize"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.proc_id", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.colorize"], ["", "@", "staticmethod", "\n", "def", "log", "(", "msg", ",", "color", "=", "'green'", ")", ":", "\n", "        ", "\"\"\"Print a colorized message to stdout.\"\"\"", "\n", "if", "mpi_tools", ".", "proc_id", "(", ")", "==", "0", ":", "\n", "            ", "print", "(", "colorize", "(", "msg", ",", "color", ",", "bold", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.log_tabular": [[113, 131], ["logx.Logger.log_headers.append"], "methods", ["None"], ["", "", "def", "log_tabular", "(", "self", ",", "key", ",", "val", ")", ":", "\n", "        ", "\"\"\"Log a value of some diagnostic.\n\n        Call this only once for each diagnostic quantity, each iteration.\n        After using ``log_tabular`` to store values for each diagnostic,\n        make sure to call ``dump_tabular`` to write them out to file and\n        stdout (otherwise they will not get saved anywhere).\n        \"\"\"", "\n", "if", "self", ".", "first_row", ":", "\n", "            ", "self", ".", "log_headers", ".", "append", "(", "key", ")", "\n", "", "else", ":", "\n", "            ", "assert", "key", "in", "self", ".", "log_headers", ",", "f'Trying to introduce a new key {key}'", "' that you didn\\'t include in the first iteration!'", "\n", "", "assert", "key", "not", "in", "self", ".", "log_current_row", ",", "f'You already set {key} this iteration.'", "' Maybe you forgot to call dump_tabular()'", "\n", "self", ".", "log_current_row", "[", "key", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.save_config": [[132, 157], ["spinup_bis.utils.serialization_utils.convert_json", "spinup_bis.utils.mpi_tools.proc_id", "json.dumps", "print", "print", "logx.colorize", "open", "out.write", "os.join", "os.join"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.proc_id", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.colorize"], ["", "def", "save_config", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"Log an experiment configuration.\n\n        Call this once at the top of your experiment, passing in all important\n        config vars as a dict. This will serialize the config to JSON, while\n        handling anything which can't be serialized in a graceful way (writing\n        as informative a string as possible).\n\n        Example use:\n\n        .. code-block:: python\n\n            logger = EpochLogger(**logger_kwargs)\n            logger.save_config(locals())\n        \"\"\"", "\n", "config_json", "=", "serialization_utils", ".", "convert_json", "(", "config", ")", "\n", "if", "self", ".", "exp_name", "is", "not", "None", ":", "\n", "            ", "config_json", "[", "'exp_name'", "]", "=", "self", ".", "exp_name", "\n", "", "if", "mpi_tools", ".", "proc_id", "(", ")", "==", "0", ":", "\n", "            ", "output", "=", "json", ".", "dumps", "(", "config_json", ",", "separators", "=", "(", "\n", "','", ",", "':\\t'", ")", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "print", "(", "colorize", "(", "'Saving config:\\n'", ",", "color", "=", "'cyan'", ",", "bold", "=", "True", ")", ")", "\n", "print", "(", "output", ")", "\n", "with", "open", "(", "osp", ".", "join", "(", "self", ".", "output_dir", ",", "'config.json'", ")", ",", "'w'", ")", "as", "out", ":", "\n", "                ", "out", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.Logger.dump_tabular": [[158, 203], ["logx.Logger.log_current_row.clear", "spinup_bis.utils.mpi_tools.proc_id", "max", "print", "print", "len", "max", "logx.Logger.log_current_row.get", "hasattr", "print", "vals.append", "logx.Logger.output_file.write", "logx.Logger.output_file.flush", "isinstance", "logx.Logger.output_file.write", "range", "edges_.append", "hist_.append", "edges_.append", "map", "len", "sum", "zip"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.proc_id"], ["", "", "", "def", "dump_tabular", "(", "self", ")", ":", "\n", "        ", "\"\"\"Write all of the diagnostics from the current iteration.\n\n        Writes to stdout and to the output file:\n        (path/to/output_directory/progress.txt).\n        \"\"\"", "\n", "if", "mpi_tools", ".", "proc_id", "(", ")", "==", "0", ":", "\n", "            ", "vals", "=", "[", "]", "\n", "key_lens", "=", "[", "len", "(", "key", ")", "for", "key", "in", "self", ".", "log_headers", "]", "\n", "max_key_len", "=", "max", "(", "15", ",", "max", "(", "key_lens", ")", ")", "\n", "keystr", "=", "'%'", "+", "f'{max_key_len}'", "\n", "fmt", "=", "'| '", "+", "keystr", "+", "'s | %15s |'", "\n", "n_slashes", "=", "22", "+", "max_key_len", "\n", "print", "(", "'-'", "*", "n_slashes", ")", "\n", "for", "key", "in", "self", ".", "log_headers", ":", "\n", "                ", "val", "=", "self", ".", "log_current_row", ".", "get", "(", "key", ",", "''", ")", "\n", "# Format value string.", "\n", "if", "hasattr", "(", "val", ",", "'__float__'", ")", ":", "\n", "                    ", "valstr", "=", "f'{val:8.3g}'", "\n", "", "elif", "isinstance", "(", "val", ",", "tuple", ")", ":", "# Histogram!", "\n", "                    ", "hist", ",", "edges", "=", "val", "\n", "\n", "# Accumulate hist values.", "\n", "hist_", ",", "edges_", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "hist", ")", "//", "4", ")", ":", "\n", "# Average because hist is density.", "\n", "                        ", "hist_", ".", "append", "(", "sum", "(", "hist", "[", "4", "*", "i", ":", "4", "*", "(", "i", "+", "1", ")", "]", ")", "/", "4", ")", "\n", "edges_", ".", "append", "(", "edges", "[", "4", "*", "i", "]", ")", "\n", "", "edges_", ".", "append", "(", "edges", "[", "-", "1", "]", ")", "\n", "\n", "valstr", "=", "', '", ".", "join", "(", "[", "f'(>{e:5.3g}: {v:5.3g})'", "\n", "for", "v", ",", "e", "in", "zip", "(", "hist_", ",", "edges_", "[", ":", "-", "1", "]", ")", "]", ")", "\n", "valstr", "+=", "f', (<{edges_[-1]:5.3g})'", "\n", "", "else", ":", "\n", "                    ", "valstr", "=", "val", "\n", "", "print", "(", "fmt", "%", "(", "key", ",", "valstr", ")", ")", "\n", "vals", ".", "append", "(", "val", ")", "\n", "", "print", "(", "'-'", "*", "n_slashes", ",", "flush", "=", "True", ")", "\n", "if", "self", ".", "output_file", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "first_row", ":", "\n", "                    ", "self", ".", "output_file", ".", "write", "(", "'\\t'", ".", "join", "(", "self", ".", "log_headers", ")", "+", "'\\n'", ")", "\n", "", "self", ".", "output_file", ".", "write", "(", "'\\t'", ".", "join", "(", "map", "(", "str", ",", "vals", ")", ")", "+", "'\\n'", ")", "\n", "self", ".", "output_file", ".", "flush", "(", ")", "\n", "", "", "self", ".", "log_current_row", ".", "clear", "(", ")", "\n", "self", ".", "first_row", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.__init__": [[230, 233], ["logx.Logger.__init__", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "epoch_dict", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.store": [[234, 242], ["kwargs.items", "logx.EpochLogger.epoch_dict[].append"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Save something into the epoch_logger's current state.\n\n        Provide an arbitrary number of keyword arguments with numerical\n        values.\n        \"\"\"", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "epoch_dict", "[", "k", "]", ".", "append", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular": [[243, 290], ["logx.Logger.log_tabular", "logx.Logger.log_tabular", "spinup_bis.utils.mpi_tools.mpi_statistics_scalar", "logx.Logger.log_tabular", "logx.Logger.log_tabular", "logx.Logger.log_tabular", "spinup_bis.utils.mpi_tools.mpi_histogram", "logx.Logger.log_tabular", "numpy.concatenate", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.mpi_statistics_scalar", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.mpi_histogram", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.log_tabular"], ["", "", "def", "log_tabular", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "key", ",", "\n", "val", "=", "None", ",", "\n", "with_min_and_max", "=", "False", ",", "\n", "average_only", "=", "False", ",", "\n", "histogram", "=", "False", ")", ":", "\n", "        ", "\"\"\"Log a value or possibly the mean/std/min/max values of a diagnostic.\n\n        Args:\n            key (string): The name of the diagnostic. If you are logging a\n                diagnostic whose state has previously been saved with\n                ``store``, the key here has to match the key you used there.\n\n            val: A value for the diagnostic. If you have previously saved\n                values for this key via ``store``, do *not* provide a ``val``\n                here.\n\n            with_min_and_max (bool): If true, log min and max values of the\n                diagnostic over the epoch.\n\n            average_only (bool): If true, do not log the standard deviation\n                of the diagnostic over the epoch.\n\n            histogram (bool): If true, then log histogram of values.\n        \"\"\"", "\n", "if", "val", "is", "not", "None", ":", "\n", "            ", "super", "(", ")", ".", "log_tabular", "(", "key", ",", "val", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "self", ".", "epoch_dict", "[", "key", "]", "\n", "if", "not", "v", ":", "# in case nothing has been stored under this key", "\n", "                ", "stats", "=", "(", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ")", "\n", "", "else", ":", "\n", "                ", "vals", "=", "np", ".", "concatenate", "(", "v", ")", "if", "isinstance", "(", "\n", "v", "[", "0", "]", ",", "np", ".", "ndarray", ")", "and", "len", "(", "v", "[", "0", "]", ".", "shape", ")", ">", "0", "else", "v", "\n", "stats", "=", "mpi_tools", ".", "mpi_statistics_scalar", "(", "\n", "vals", ",", "with_min_and_max", "=", "with_min_and_max", ")", "\n", "", "super", "(", ")", ".", "log_tabular", "(", "\n", "key", "if", "average_only", "else", "'Average'", "+", "key", ",", "stats", "[", "0", "]", ")", "\n", "if", "not", "average_only", ":", "\n", "                ", "super", "(", ")", ".", "log_tabular", "(", "'Std'", "+", "key", ",", "stats", "[", "1", "]", ")", "\n", "", "if", "with_min_and_max", ":", "\n", "                ", "super", "(", ")", ".", "log_tabular", "(", "'Max'", "+", "key", ",", "stats", "[", "3", "]", ")", "\n", "super", "(", ")", ".", "log_tabular", "(", "'Min'", "+", "key", ",", "stats", "[", "2", "]", ")", "\n", "", "if", "histogram", ":", "\n", "                ", "hist", ",", "edges", "=", "mpi_tools", ".", "mpi_histogram", "(", "vals", ")", "\n", "super", "(", ")", ".", "log_tabular", "(", "'Histogram'", "+", "key", ",", "(", "hist", ",", "edges", ")", ")", "\n", "", "", "self", ".", "epoch_dict", "[", "key", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.EpochLogger.get_stats": [[291, 297], ["spinup_bis.utils.mpi_tools.mpi_statistics_scalar", "numpy.concatenate", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.mpi_tools.mpi_statistics_scalar"], ["", "def", "get_stats", "(", "self", ",", "key", ")", ":", "\n", "        ", "\"\"\"Lets an algorithm ask the logger for mean/std of a diagnostic.\"\"\"", "\n", "v", "=", "self", ".", "epoch_dict", "[", "key", "]", "\n", "vals", "=", "np", ".", "concatenate", "(", "v", ")", "if", "isinstance", "(", "\n", "v", "[", "0", "]", ",", "np", ".", "ndarray", ")", "and", "len", "(", "v", "[", "0", "]", ".", "shape", ")", ">", "0", "else", "v", "\n", "return", "mpi_tools", ".", "mpi_statistics_scalar", "(", "vals", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.colorize": [[36, 49], ["attr.append", "str", "attr.append"], "function", ["None"], ["def", "colorize", "(", "string", ",", "color", ",", "bold", "=", "False", ",", "highlight", "=", "False", ")", ":", "\n", "    ", "\"\"\"Colorize a string.\n\n    This function was originally written by John Schulman.\n    \"\"\"", "\n", "attr", "=", "[", "]", "\n", "num", "=", "COLOR2NUM", "[", "color", "]", "\n", "if", "highlight", ":", "\n", "        ", "num", "+=", "10", "\n", "", "attr", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "if", "bold", ":", "\n", "        ", "attr", ".", "append", "(", "'1'", ")", "\n", "", "return", "'\\x1b[%sm%s\\x1b[0m'", "%", "(", "';'", ".", "join", "(", "attr", ")", ",", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.flatten_config": [[51, 59], ["config.items", "isinstance", "flattened.update", "logx.flatten_config"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.logx.flatten_config"], ["", "def", "flatten_config", "(", "config", ",", "prefix", "=", "''", ")", ":", "\n", "    ", "flattened", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "config", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "dict", ")", ":", "\n", "            ", "flattened", ".", "update", "(", "flatten_config", "(", "val", ",", "prefix", "=", "prefix", "+", "key", "+", "'.'", ")", ")", "\n", "", "else", ":", "\n", "            ", "flattened", "[", "prefix", "+", "key", "]", "=", "val", "\n", "", "", "return", "flattened", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json": [[6, 30], ["serialization_utils.is_json_serializable", "isinstance", "isinstance", "isinstance", "str", "hasattr", "serialization_utils.convert_json", "hasattr", "serialization_utils.convert_json", "serialization_utils.convert_json", "serialization_utils.convert_json", "serialization_utils.convert_json", "serialization_utils.convert_json", "serialization_utils.convert_json", "str", "obj.items", "obj.__dict__.items"], "function", ["home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.is_json_serializable", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.convert_json"], ["def", "convert_json", "(", "obj", ")", ":", "\n", "    ", "\"\"\"Convert obj to a version which can be serialized with JSON.\"\"\"", "\n", "if", "is_json_serializable", "(", "obj", ")", ":", "\n", "        ", "return", "obj", "\n", "\n", "", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "{", "convert_json", "(", "k", ")", ":", "convert_json", "(", "v", ")", "\n", "for", "k", ",", "v", "in", "obj", ".", "items", "(", ")", "}", "\n", "\n", "", "if", "isinstance", "(", "obj", ",", "tuple", ")", ":", "\n", "        ", "return", "(", "convert_json", "(", "x", ")", "for", "x", "in", "obj", ")", "\n", "\n", "", "if", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "        ", "return", "[", "convert_json", "(", "x", ")", "for", "x", "in", "obj", "]", "\n", "\n", "", "if", "hasattr", "(", "obj", ",", "'__name__'", ")", "and", "not", "'lambda'", "in", "obj", ".", "__name__", ":", "\n", "        ", "return", "convert_json", "(", "obj", ".", "__name__", ")", "\n", "\n", "", "if", "hasattr", "(", "obj", ",", "'__dict__'", ")", "and", "obj", ".", "__dict__", ":", "\n", "        ", "obj_dict", "=", "{", "convert_json", "(", "k", ")", ":", "convert_json", "(", "v", ")", "\n", "for", "k", ",", "v", "in", "obj", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "return", "{", "str", "(", "obj", ")", ":", "obj_dict", "}", "\n", "\n", "", "return", "str", "(", "obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.utils.serialization_utils.is_json_serializable": [[32, 39], ["json.dumps"], "function", ["None"], ["", "def", "is_json_serializable", "(", "value", ")", ":", "\n", "    ", "\"\"\"Checks if value is json serializable.\"\"\"", "\n", "try", ":", "\n", "        ", "json", ".", "dumps", "(", "value", ")", "\n", "return", "True", "\n", "", "except", ":", "# pylint: disable=bare-except", "\n", "        ", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.__init__": [[28, 63], ["gym.utils.EzPickle.__init__", "gym.envs.mujoco.mujoco_env.MujocoEnv.__init__", "locals", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.__init__", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.__init__"], ["def", "__init__", "(", "self", ",", "\n", "xml_file", "=", "'humanoid.xml'", ",", "\n", "forward_reward_weight", "=", "1.25", ",", "\n", "ctrl_cost_weight", "=", "0.1", ",", "\n", "contact_cost_weight", "=", "5e-7", ",", "\n", "contact_cost_range", "=", "(", "-", "np", ".", "inf", ",", "10.0", ")", ",", "\n", "healthy_reward", "=", "3.0", ",", "\n", "terminate_when_unhealthy", "=", "True", ",", "\n", "healthy_z_range", "=", "(", "1.0", ",", "2.0", ")", ",", "\n", "reset_noise_scale", "=", "1e-2", ",", "\n", "exclude_current_positions_from_observation", "=", "False", ",", "\n", "destinations", "=", "None", ")", ":", "\n", "        ", "utils", ".", "EzPickle", ".", "__init__", "(", "**", "locals", "(", ")", ")", "\n", "\n", "self", ".", "_forward_reward_weight", "=", "forward_reward_weight", "\n", "self", ".", "_ctrl_cost_weight", "=", "ctrl_cost_weight", "\n", "self", ".", "_contact_cost_weight", "=", "contact_cost_weight", "\n", "self", ".", "_contact_cost_range", "=", "contact_cost_range", "\n", "self", ".", "_healthy_reward", "=", "healthy_reward", "\n", "self", ".", "_terminate_when_unhealthy", "=", "terminate_when_unhealthy", "\n", "self", ".", "_healthy_z_range", "=", "healthy_z_range", "\n", "\n", "self", ".", "_reset_noise_scale", "=", "reset_noise_scale", "\n", "\n", "self", ".", "_exclude_current_positions_from_observation", "=", "(", "\n", "exclude_current_positions_from_observation", ")", "\n", "\n", "self", ".", "_destinations", "=", "destinations", "or", "{", "\n", "'locs'", ":", "[", "np", ".", "array", "(", "(", "2", ",", "2", ")", ")", "]", ",", "\n", "'radii'", ":", "[", "1.", "]", ",", "\n", "'rewards'", ":", "[", "3.", "]", ",", "\n", "'solved'", ":", "[", "True", "]", ",", "\n", "}", "\n", "\n", "mujoco_env", ".", "MujocoEnv", ".", "__init__", "(", "self", ",", "xml_file", ",", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.healthy_reward": [[64, 70], ["float"], "methods", ["None"], ["", "@", "property", "\n", "def", "healthy_reward", "(", "self", ")", ":", "\n", "        ", "return", "float", "(", "\n", "self", ".", "is_healthy", "\n", "or", "self", ".", "_terminate_when_unhealthy", "\n", ")", "*", "self", ".", "_healthy_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.seek_reward": [[71, 87], ["zip", "numpy.sqrt", "numpy.sum", "numpy.dot", "values.append"], "methods", ["None"], ["", "def", "seek_reward", "(", "self", ",", "position", ")", ":", "\n", "        ", "\"\"\"Reward for being in the zone of influence of target destinations.\n\n        It's aggregated (summed) over all zones.\n        \"\"\"", "\n", "values", "=", "[", "]", "\n", "is_solved", "=", "False", "\n", "for", "loc", ",", "radious", ",", "reward", ",", "solved", "in", "zip", "(", "self", ".", "_destinations", "[", "'locs'", "]", ",", "\n", "self", ".", "_destinations", "[", "'radii'", "]", ",", "\n", "self", ".", "_destinations", "[", "'rewards'", "]", ",", "\n", "self", ".", "_destinations", "[", "'solved'", "]", ")", ":", "\n", "            ", "distance", "=", "np", ".", "sqrt", "(", "np", ".", "dot", "(", "(", "position", "-", "loc", ")", ",", "(", "position", "-", "loc", ")", ")", ")", "\n", "if", "distance", "<=", "radious", ":", "\n", "                ", "values", ".", "append", "(", "reward", ")", "\n", "is_solved", "=", "is_solved", "or", "solved", "\n", "", "", "return", "np", ".", "sum", "(", "values", ")", ",", "is_solved", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.control_cost": [[88, 93], ["numpy.sum", "numpy.square"], "methods", ["None"], ["", "def", "control_cost", "(", "self", ",", "action", ")", ":", "\n", "        ", "del", "action", "\n", "control_cost", "=", "self", ".", "_ctrl_cost_weight", "*", "np", ".", "sum", "(", "\n", "np", ".", "square", "(", "self", ".", "sim", ".", "data", ".", "ctrl", ")", ")", "\n", "return", "control_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.contact_cost": [[94, 102], ["numpy.clip", "numpy.sum", "numpy.square"], "methods", ["None"], ["", "@", "property", "\n", "def", "contact_cost", "(", "self", ")", ":", "\n", "        ", "contact_forces", "=", "self", ".", "sim", ".", "data", ".", "cfrc_ext", "\n", "contact_cost", "=", "self", ".", "_contact_cost_weight", "*", "np", ".", "sum", "(", "\n", "np", ".", "square", "(", "contact_forces", ")", ")", "\n", "min_cost", ",", "max_cost", "=", "self", ".", "_contact_cost_range", "\n", "contact_cost", "=", "np", ".", "clip", "(", "contact_cost", ",", "min_cost", ",", "max_cost", ")", "\n", "return", "contact_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.is_healthy": [[103, 109], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_healthy", "(", "self", ")", ":", "\n", "        ", "min_z", ",", "max_z", "=", "self", ".", "_healthy_z_range", "\n", "is_healthy", "=", "min_z", "<", "self", ".", "sim", ".", "data", ".", "qpos", "[", "2", "]", "<", "max_z", "\n", "\n", "return", "is_healthy", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.done": [[110, 116], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "done", "(", "self", ")", ":", "\n", "        ", "done", "=", "(", "(", "not", "self", ".", "is_healthy", ")", "\n", "if", "self", ".", "_terminate_when_unhealthy", "\n", "else", "False", ")", "\n", "return", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv._get_obs": [[117, 137], ["humanoid_hide_and_seek.HumanoidEnv.sim.data.qpos.flat.copy", "humanoid_hide_and_seek.HumanoidEnv.sim.data.qvel.flat.copy", "humanoid_hide_and_seek.HumanoidEnv.sim.data.cinert.flat.copy", "humanoid_hide_and_seek.HumanoidEnv.sim.data.cvel.flat.copy", "humanoid_hide_and_seek.HumanoidEnv.sim.data.qfrc_actuator.flat.copy", "humanoid_hide_and_seek.HumanoidEnv.sim.data.cfrc_ext.flat.copy", "numpy.concatenate"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "position", "=", "self", ".", "sim", ".", "data", ".", "qpos", ".", "flat", ".", "copy", "(", ")", "\n", "velocity", "=", "self", ".", "sim", ".", "data", ".", "qvel", ".", "flat", ".", "copy", "(", ")", "\n", "\n", "com_inertia", "=", "self", ".", "sim", ".", "data", ".", "cinert", ".", "flat", ".", "copy", "(", ")", "\n", "com_velocity", "=", "self", ".", "sim", ".", "data", ".", "cvel", ".", "flat", ".", "copy", "(", ")", "\n", "\n", "actuator_forces", "=", "self", ".", "sim", ".", "data", ".", "qfrc_actuator", ".", "flat", ".", "copy", "(", ")", "\n", "external_contact_forces", "=", "self", ".", "sim", ".", "data", ".", "cfrc_ext", ".", "flat", ".", "copy", "(", ")", "\n", "\n", "if", "self", ".", "_exclude_current_positions_from_observation", ":", "\n", "            ", "position", "=", "position", "[", "2", ":", "]", "\n", "\n", "", "return", "np", ".", "concatenate", "(", "(", "\n", "position", ",", "\n", "velocity", ",", "\n", "com_inertia", ",", "\n", "com_velocity", ",", "\n", "actuator_forces", ",", "\n", "external_contact_forces", ",", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.step": [[139, 170], ["humanoid_hide_and_seek.HumanoidEnv.do_simulation", "humanoid_hide_and_seek.mass_center", "humanoid_hide_and_seek.HumanoidEnv.seek_reward", "humanoid_hide_and_seek.HumanoidEnv.control_cost", "humanoid_hide_and_seek.HumanoidEnv._get_obs", "numpy.linalg.norm"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.mass_center", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.seek_reward", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.control_cost", "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv._get_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "do_simulation", "(", "action", ",", "self", ".", "frame_skip", ")", "\n", "xy_pos_after", "=", "mass_center", "(", "self", ".", "model", ",", "self", ".", "sim", ")", "\n", "\n", "seek_reward", ",", "is_solved", "=", "self", ".", "seek_reward", "(", "xy_pos_after", ")", "\n", "healthy_reward", "=", "self", ".", "healthy_reward", "\n", "\n", "ctrl_cost", "=", "self", ".", "control_cost", "(", "action", ")", "\n", "contact_cost", "=", "self", ".", "contact_cost", "\n", "\n", "rewards", "=", "self", ".", "_forward_reward_weight", "*", "seek_reward", "+", "healthy_reward", "\n", "costs", "=", "ctrl_cost", "+", "contact_cost", "\n", "\n", "observation", "=", "self", ".", "_get_obs", "(", ")", "\n", "reward", "=", "rewards", "-", "costs", "\n", "done", "=", "self", ".", "done", "\n", "\n", "info", "=", "{", "\n", "'reward_task'", ":", "seek_reward", ",", "\n", "'reward_quadctrl'", ":", "-", "ctrl_cost", ",", "\n", "'reward_alive'", ":", "healthy_reward", ",", "\n", "'reward_impact'", ":", "-", "contact_cost", ",", "\n", "\n", "'is_solved'", ":", "is_solved", ",", "\n", "\n", "'x_position'", ":", "xy_pos_after", "[", "0", "]", ",", "\n", "'y_position'", ":", "xy_pos_after", "[", "1", "]", ",", "\n", "'distance_from_origin'", ":", "np", ".", "linalg", ".", "norm", "(", "xy_pos_after", ",", "ord", "=", "2", ")", ",", "\n", "}", "\n", "\n", "return", "observation", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.reset_model": [[171, 183], ["humanoid_hide_and_seek.HumanoidEnv.set_state", "humanoid_hide_and_seek.HumanoidEnv._get_obs", "humanoid_hide_and_seek.HumanoidEnv.np_random.uniform", "humanoid_hide_and_seek.HumanoidEnv.np_random.uniform"], "methods", ["home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv._get_obs"], ["", "def", "reset_model", "(", "self", ")", ":", "\n", "        ", "noise_low", "=", "-", "self", ".", "_reset_noise_scale", "\n", "noise_high", "=", "self", ".", "_reset_noise_scale", "\n", "\n", "qpos", "=", "self", ".", "init_qpos", "+", "self", ".", "np_random", ".", "uniform", "(", "\n", "low", "=", "noise_low", ",", "high", "=", "noise_high", ",", "size", "=", "self", ".", "model", ".", "nq", ")", "\n", "qvel", "=", "self", ".", "init_qvel", "+", "self", ".", "np_random", ".", "uniform", "(", "\n", "low", "=", "noise_low", ",", "high", "=", "noise_high", ",", "size", "=", "self", ".", "model", ".", "nv", ")", "\n", "self", ".", "set_state", "(", "qpos", ",", "qvel", ")", "\n", "\n", "observation", "=", "self", ".", "_get_obs", "(", ")", "\n", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.HumanoidEnv.viewer_setup": [[184, 190], ["DEFAULT_CAMERA_CONFIG.items", "isinstance", "setattr", "getattr"], "methods", ["None"], ["", "def", "viewer_setup", "(", "self", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "DEFAULT_CAMERA_CONFIG", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "getattr", "(", "self", ".", "viewer", ".", "cam", ",", "key", ")", "[", ":", "]", "=", "value", "\n", "", "else", ":", "\n", "                ", "setattr", "(", "self", ".", "viewer", ".", "cam", ",", "key", ",", "value", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ed2-paper_ed2.envs.humanoid_hide_and_seek.mass_center": [[19, 23], ["numpy.expand_dims", "[].copy", "numpy.sum", "numpy.sum"], "function", ["None"], ["def", "mass_center", "(", "model", ",", "sim", ")", ":", "\n", "    ", "mass", "=", "np", ".", "expand_dims", "(", "model", ".", "body_mass", ",", "axis", "=", "1", ")", "\n", "xpos", "=", "sim", ".", "data", ".", "xipos", "\n", "return", "(", "np", ".", "sum", "(", "mass", "*", "xpos", ",", "axis", "=", "0", ")", "/", "np", ".", "sum", "(", "mass", ")", ")", "[", "0", ":", "2", "]", ".", "copy", "(", ")", "\n", "\n"]]}