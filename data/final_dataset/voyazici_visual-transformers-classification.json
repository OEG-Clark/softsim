{"home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.convert_to_array": [[19, 38], ["scores.data.cpu().numpy.data.cpu().numpy", "targets.data.cpu().numpy.data.cpu().numpy", "numpy.zeros", "numpy.zeros", "range", "range", "scores.data.cpu().numpy.data.cpu", "targets.data.cpu().numpy.data.cpu", "numpy.argmax", "preds_image.append"], "function", ["None"], ["def", "convert_to_array", "(", "scores", ",", "targets", ",", "target_lengths", ")", ":", "\n", "    ", "scores", "=", "scores", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "targets", "=", "targets", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "number_class", "=", "80", "\n", "N", "=", "scores", ".", "shape", "[", "0", "]", "\n", "preds", "=", "np", ".", "zeros", "(", "(", "N", ",", "number_class", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "labels", "=", "np", ".", "zeros", "(", "(", "N", ",", "number_class", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "number_time_steps", "=", "scores", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "        ", "preds_image", "=", "[", "]", "\n", "for", "step_t", "in", "range", "(", "number_time_steps", ")", ":", "\n", "            ", "step_pred", "=", "np", ".", "argmax", "(", "scores", "[", "i", "]", "[", "step_t", "]", ")", "\n", "if", "category_dict_sequential_inv", "[", "step_pred", "]", "==", "'<empty>'", ":", "\n", "                ", "continue", "\n", "", "preds_image", ".", "append", "(", "step_pred", ")", "\n", "", "preds", "[", "i", ",", "preds_image", "]", "=", "1", "\n", "labels_image", "=", "targets", "[", "i", "]", "[", "0", ":", "target_lengths", "[", "i", "]", "]", "\n", "labels", "[", "i", ",", "labels_image", "]", "=", "1", "\n", "", "return", "preds", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.order_the_targets": [[39, 78], ["scores.data.cpu().numpy.clone", "scores.data.cpu().numpy.data.cpu().numpy", "targets.data.cpu().numpy.data.cpu().numpy", "numpy.argmax", "range", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "numpy.ones", "set().intersection", "set().difference", "list", "common_locs.items", "[].tolist", "scores.data.cpu().numpy.data.cpu", "targets.data.cpu().numpy.data.cpu", "set", "set", "[].tolist", "len", "len", "common_locs.items", "m.compute", "losses_list.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "set", "set", "torch.log_softmax().data.cpu().numpy().transpose", "len", "locs_copy.remove", "available_locs.extend", "torch.log_softmax().data.cpu().numpy", "scores[].argmax", "torch.log_softmax().data.cpu", "torch.log_softmax"], "function", ["None"], ["", "def", "order_the_targets", "(", "scores", ",", "targets", ",", "label_lengths_sorted", ")", ":", "\n", "    ", "device", "=", "targets", ".", "device", "\n", "scores_tensor", "=", "scores", ".", "clone", "(", ")", "\n", "scores", "=", "scores", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "time_steps", "=", "scores", ".", "shape", "[", "1", "]", "\n", "batch_size", "=", "scores", ".", "shape", "[", "0", "]", "\n", "targets", "=", "targets", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "targets_new", "=", "category_dict_sequential", "[", "'<empty>'", "]", "*", "np", ".", "ones", "(", "(", "batch_size", ",", "time_steps", ")", ")", "\n", "N", "=", "scores", ".", "shape", "[", "0", "]", "\n", "indexes", "=", "np", ".", "argmax", "(", "scores", ",", "axis", "=", "2", ")", "\n", "\n", "losses_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "        ", "common_indexes", "=", "set", "(", "targets", "[", "i", "]", "[", ":", "label_lengths_sorted", "[", "i", "]", "]", ")", ".", "intersection", "(", "set", "(", "indexes", "[", "i", "]", ")", ")", "\n", "diff_indexes", "=", "set", "(", "targets", "[", "i", "]", "[", ":", "label_lengths_sorted", "[", "i", "]", "]", ")", ".", "difference", "(", "set", "(", "indexes", "[", "i", "]", ")", ")", "\n", "diff_indexes", "=", "list", "(", "diff_indexes", ")", "\n", "common_locs", "=", "{", "idx", ":", "(", "indexes", "[", "i", "]", "==", "idx", ")", ".", "nonzero", "(", ")", "[", "0", "]", ".", "tolist", "(", ")", "\n", "for", "idx", "in", "common_indexes", "}", "\n", "for", "idx", ",", "locs", "in", "common_locs", ".", "items", "(", ")", ":", "\n", "            ", "targets_new", "[", "i", "]", "[", "locs", "]", "=", "idx", "\n", "", "empty_locs", "=", "(", "targets_new", "[", "i", "]", "==", "category_dict_sequential", "[", "'<empty>'", "]", ")", ".", "nonzero", "(", ")", "[", "0", "]", ".", "tolist", "(", ")", "\n", "needed_loc", "=", "len", "(", "diff_indexes", ")", "-", "len", "(", "empty_locs", ")", "\n", "available_locs", "=", "[", "]", "\n", "if", "diff_indexes", ":", "\n", "            ", "available_locs", "=", "empty_locs", "[", ":", "]", "\n", "for", "idx", ",", "locs", "in", "common_locs", ".", "items", "(", ")", ":", "\n", "                ", "if", "len", "(", "locs", ")", ">", "1", ":", "\n", "                    ", "max_loc", "=", "locs", "[", "scores", "[", "i", ",", "locs", ",", "idx", "]", ".", "argmax", "(", ")", "]", "\n", "locs_copy", "=", "locs", "[", ":", "]", "\n", "locs_copy", ".", "remove", "(", "max_loc", ")", "\n", "available_locs", ".", "extend", "(", "locs_copy", ")", "\n", "", "", "losses", "=", "-", "F", ".", "log_softmax", "(", "scores_tensor", "[", "i", ",", "available_locs", "]", "[", ":", ",", "diff_indexes", "]", ",", "dim", "=", "0", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "m_indexes", "=", "m", ".", "compute", "(", "losses", ")", "\n", "for", "m_index", ",", "m_loc", "in", "m_indexes", ":", "\n", "                ", "targets_new", "[", "i", ",", "available_locs", "[", "m_loc", "]", "]", "=", "diff_indexes", "[", "m_index", "]", "\n", "", "", "else", ":", "\n", "            ", "losses_list", ".", "append", "(", "0", ")", "\n", "", "", "targets_new", "=", "torch", ".", "LongTensor", "(", "targets_new", ")", ".", "to", "(", "device", ")", "\n", "return", "targets_new", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.adjust_learning_rate": [[79, 83], ["print"], "function", ["None"], ["", "def", "adjust_learning_rate", "(", "optimizer", ",", "shrink_factor", ")", ":", "\n", "    ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "=", "param_group", "[", "'lr'", "]", "*", "shrink_factor", "\n", "", "print", "(", "\"The new learning rate is %f\\n\"", "%", "(", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.my_collate": [[84, 89], ["torch.utils.data.dataloader.default_collate"], "function", ["None"], ["", "def", "my_collate", "(", "batch", ")", ":", "\n", "    ", "batch", "=", "[", "b", "for", "b", "in", "batch", "if", "b", "is", "not", "None", "]", "\n", "if", "not", "batch", ":", "\n", "        ", "return", "None", "\n", "", "return", "default_collate", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.mix_up_images": [[90, 99], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "function", ["None"], ["", "def", "mix_up_images", "(", "data", ")", ":", "\n", "# mix up the first and second halves", "\n", "# keep the second half same", "\n", "    ", "images", "=", "torch", ".", "zeros_like", "(", "data", ")", "\n", "batch_size", "=", "data", ".", "shape", "[", "0", "]", "\n", "half", "=", "batch_size", "//", "2", "\n", "images", "[", "half", ":", "]", "=", "data", "[", "half", ":", "]", "\n", "images", "[", ":", "half", "]", "=", "(", "data", "[", ":", "half", "]", "+", "data", "[", "half", ":", "]", ")", "/", "2", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.mix_up_labels": [[100, 121], ["target_lengths.clone", "range", "range", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.ones().type().to", "torch.ones().type().to", "torch.ones().type().to", "torch.ones().type().to", "[].tolist", "[].tolist", "list", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to.append", "[].clone", "torch.LongTensor().to.append", "set", "len", "target_lengths[].item", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ones().type", "torch.ones().type", "torch.ones().type", "torch.ones().type", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "target_lengths.clone.max"], "function", ["None"], ["", "def", "mix_up_labels", "(", "target", ",", "target_lengths", ")", ":", "\n", "# mix up the first and second halves", "\n", "# keep the second half same", "\n", "    ", "num_classes", "=", "80", "\n", "temp", "=", "target_lengths", ".", "clone", "(", ")", "\n", "batch_size", "=", "target", ".", "shape", "[", "0", "]", "\n", "half", "=", "batch_size", "//", "2", "\n", "temp", "[", ":", "half", "]", "+=", "temp", "[", "half", ":", "]", "\n", "new_target", "=", "num_classes", "*", "torch", ".", "ones", "(", "batch_size", ",", "temp", ".", "max", "(", ")", ")", ".", "type", "(", "torch", ".", "LongTensor", ")", ".", "to", "(", "target", ".", "device", ")", "\n", "new_target_lengths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "half", ")", ":", "\n", "        ", "old_labels", "=", "target", "[", "i", "]", "[", ":", "target_lengths", "[", "i", "]", "]", ".", "tolist", "(", ")", "\n", "new_labels", "=", "target", "[", "i", "+", "half", "]", "[", ":", "target_lengths", "[", "i", "+", "half", "]", "]", ".", "tolist", "(", ")", "\n", "all_labels", "=", "list", "(", "set", "(", "old_labels", "+", "new_labels", ")", ")", "\n", "new_target", "[", "i", "]", "[", ":", "len", "(", "all_labels", ")", "]", "=", "torch", ".", "LongTensor", "(", "all_labels", ")", ".", "to", "(", "new_target", ".", "device", ")", "\n", "new_target_lengths", ".", "append", "(", "len", "(", "all_labels", ")", ")", "\n", "", "for", "i", "in", "range", "(", "half", ",", "batch_size", ")", ":", "\n", "        ", "new_target", "[", "i", "]", "[", ":", "target_lengths", "[", "i", "]", "]", "=", "target", "[", "i", "]", "[", ":", "target_lengths", "[", "i", "]", "]", ".", "clone", "(", ")", "\n", "new_target_lengths", ".", "append", "(", "target_lengths", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "", "new_target_lengths", "=", "torch", ".", "LongTensor", "(", "new_target_lengths", ")", ".", "to", "(", "target_lengths", ".", "device", ")", "\n", "return", "(", "new_target", ",", "new_target_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.train": [[122, 161], ["model.train", "isinstance", "enumerate", "model.module.freeze", "model.freeze", "encoder_optim.zero_grad", "decoder_optim.zero_grad", "model", "train.order_the_targets", "torch.cross_entropy", "F.cross_entropy.backward", "encoder_optim.step", "decoder_optim.step", "mix_up_images.to", "target.to", "train.mix_up_images", "train.mix_up_labels", "model.permute", "train.convert_to_array", "sklearn.metrics.precision_recall_fscore_support", "print", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "F.cross_entropy.item", "len", "F.cross_entropy.item", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.train", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.freeze", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.freeze", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.order_the_targets", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.mix_up_images", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.mix_up_labels", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.convert_to_array"], ["", "def", "train", "(", "args", ",", "model", ",", "device", ",", "train_loader", ",", "encoder_optim", ",", "decoder_optim", ",", "\n", "epoch", ",", "writer", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "nn", ".", "DataParallel", ")", ":", "\n", "        ", "model", ".", "module", ".", "freeze", "(", "\"bn\"", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "freeze", "(", "\"bn\"", ")", "\n", "", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "        ", "if", "batch", "is", "None", ":", "\n", "            ", "continue", "\n", "", "data", ",", "target", ",", "target_lengths", "=", "batch", "\n", "data", ",", "target", "=", "data", ".", "to", "(", "device", ")", ",", "target", ".", "to", "(", "device", ")", "\n", "encoder_optim", ".", "zero_grad", "(", ")", "\n", "decoder_optim", ".", "zero_grad", "(", ")", "\n", "\n", "if", "args", ".", "mix_up", "and", "data", ".", "shape", "[", "0", "]", "%", "2", "==", "0", ":", "\n", "            ", "data", "=", "mix_up_images", "(", "data", ")", "\n", "target", ",", "target_lengths", "=", "mix_up_labels", "(", "target", ",", "target_lengths", ")", "\n", "\n", "", "decoder_output", "=", "model", "(", "data", ")", "\n", "\n", "target_new", "=", "order_the_targets", "(", "decoder_output", ",", "target", ",", "target_lengths", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "decoder_output", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "target_new", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "encoder_optim", ".", "step", "(", ")", "\n", "decoder_optim", ".", "step", "(", ")", "\n", "if", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "            ", "preds", ",", "labels", "=", "convert_to_array", "(", "decoder_output", ",", "target", ",", "target_lengths", ")", "\n", "_", ",", "_", ",", "f1", ",", "_", "=", "precision_recall_fscore_support", "(", "preds", ",", "labels", ",", "average", "=", "'micro'", ")", "\n", "print", "(", "'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tF1: {:.2f}'", ".", "format", "(", "\n", "epoch", ",", "batch_idx", "*", "len", "(", "data", ")", ",", "len", "(", "train_loader", ".", "dataset", ")", ",", "\n", "100.", "*", "batch_idx", "/", "len", "(", "train_loader", ")", ",", "loss", ".", "item", "(", ")", ",", "100", "*", "f1", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'loss'", ",", "loss", ".", "item", "(", ")", ",", "epoch", "*", "len", "(", "train_loader", ")", "+", "batch_idx", ")", "\n", "writer", ".", "add_scalar", "(", "'encoder_lr'", ",", "encoder_optim", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "epoch", "*", "len", "(", "train_loader", ")", "+", "batch_idx", ")", "\n", "writer", ".", "add_scalar", "(", "'decoder_lr'", ",", "decoder_optim", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "\n", "epoch", "*", "len", "(", "train_loader", ")", "+", "batch_idx", ")", "\n", "writer", ".", "add_scalar", "(", "'train_f1'", ",", "f1", "*", "100", ",", "epoch", "*", "len", "(", "train_loader", ")", "+", "batch_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.test": [[162, 213], ["model.eval", "sklearn.metrics.precision_recall_fscore_support", "print", "sklearn.metrics.precision_recall_fscore_support", "print", "range", "print", "print", "numpy.mean", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "tqdm.tqdm", "np.concatenate.max", "sklearn.metrics.average_precision_score", "map_cats.append", "print", "data.to.to", "model", "torch.softmax().data.cpu().numpy", "train.convert_to_array", "len", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.mean", "torch.softmax().data.cpu", "torch.softmax"], "function", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.convert_to_array"], ["", "", "", "def", "test", "(", "args", ",", "model", ",", "device", ",", "test_loader", ",", "threshold", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "preds_all", "=", "None", "\n", "labels_all", "=", "None", "\n", "scores_all", "=", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch", "in", "tqdm", "(", "test_loader", ",", "total", "=", "len", "(", "test_loader", ")", ")", ":", "\n", "            ", "if", "batch", "is", "None", ":", "\n", "                ", "continue", "\n", "", "data", ",", "target", ",", "target_lengths", "=", "batch", "\n", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "decoder_output", "=", "model", "(", "data", ")", "\n", "scores", "=", "F", ".", "softmax", "(", "decoder_output", ",", "dim", "=", "2", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "preds", ",", "labels", "=", "convert_to_array", "(", "decoder_output", ",", "target", ",", "target_lengths", ")", "\n", "if", "labels_all", "is", "None", ":", "\n", "                ", "preds_all", "=", "preds", "\n", "labels_all", "=", "labels", "\n", "scores_all", "=", "scores", "\n", "", "else", ":", "\n", "                ", "preds_all", "=", "np", ".", "concatenate", "(", "(", "preds_all", ",", "preds", ")", ",", "axis", "=", "0", ")", "\n", "labels_all", "=", "np", ".", "concatenate", "(", "(", "labels_all", ",", "labels", ")", ",", "axis", "=", "0", ")", "\n", "scores_all", "=", "np", ".", "concatenate", "(", "(", "scores_all", ",", "scores", ")", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "", "results", "=", "{", "'micro'", ":", "None", ",", "'macro'", ":", "None", "}", "\n", "prec", ",", "recall", ",", "_", ",", "_", "=", "precision_recall_fscore_support", "(", "labels_all", ",", "\n", "preds_all", ",", "\n", "average", "=", "'macro'", ")", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "\n", "results", "[", "'macro'", "]", "=", "{", "'precision'", ":", "prec", ",", "'recall'", ":", "recall", ",", "'f1'", ":", "f1", "}", "\n", "print", "(", "'\\nMACRO prec: {:.2f}, recall: {:.2f}, f1: {:.2f}\\n'", ".", "format", "(", "\n", "100", "*", "prec", ",", "100", "*", "recall", ",", "100", "*", "f1", ")", ")", "\n", "prec", ",", "recall", ",", "f1", ",", "_", "=", "precision_recall_fscore_support", "(", "labels_all", ",", "\n", "preds_all", ",", "\n", "average", "=", "'micro'", ")", "\n", "results", "[", "'micro'", "]", "=", "{", "'precision'", ":", "prec", ",", "'recall'", ":", "recall", ",", "'f1'", ":", "f1", "}", "\n", "print", "(", "'\\nMICRO prec: {:.2f}, recall: {:.2f}, f1: {:.2f}\\n'", ".", "format", "(", "\n", "100", "*", "prec", ",", "100", "*", "recall", ",", "100", "*", "f1", ")", ")", "\n", "\n", "scores_max", "=", "scores_all", ".", "max", "(", "1", ")", "[", ":", ",", ":", "80", "]", "\n", "map_cats", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "80", ")", ":", "\n", "        ", "map_cat", "=", "average_precision_score", "(", "labels_all", "[", ":", ",", "j", "]", ",", "scores_max", "[", ":", ",", "j", "]", ")", "\n", "map_cats", ".", "append", "(", "map_cat", ")", "\n", "print", "(", "'%s,%.1f'", "%", "(", "category_dict_sequential_inv", "[", "j", "]", ",", "map_cat", "*", "100", ")", ")", "\n", "", "print", "(", "'mAP %.1f'", "%", "(", "np", ".", "mean", "(", "map_cats", ")", "*", "100", ")", ")", "\n", "print", "(", "','", ".", "join", "(", "[", "(", "'%.1f'", "%", "(", "x", "*", "100", ")", ")", "for", "x", "in", "map_cats", "]", ")", ")", "\n", "mAP", "=", "np", ".", "mean", "(", "map_cats", ")", "\n", "results", "[", "'mAP'", "]", "=", "mAP", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.prepare_optimizer": [[214, 227], ["print", "torch.Adam", "torch.SGD", "optim_params.get", "optim_params.get", "optim_params.get"], "function", ["None"], ["", "def", "prepare_optimizer", "(", "optim_params", ",", "model_params", ")", ":", "\n", "    ", "opt", "=", "optim_params", "[", "'opt'", "]", "\n", "assert", "opt", "in", "[", "'adam'", ",", "'sgd'", "]", "\n", "lr", "=", "optim_params", "[", "'lr'", "]", "\n", "print", "(", "opt", ",", "lr", ")", "\n", "if", "opt", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model_params", ",", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "optim_params", ".", "get", "(", "'weight_decay'", ",", "0.0", ")", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "model_params", ",", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "optim_params", ".", "get", "(", "'weight_decay'", ",", "0.0", ")", ",", "\n", "momentum", "=", "optim_params", ".", "get", "(", "'momentum'", ",", "0.9", ")", ")", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.main": [[228, 363], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "dataset.COCOMultiLabel", "dataset.COCOMultiLabel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.NetEncoderDecoder().to", "range", "os.path.join", "tensorboardX.SummaryWriter", "torch.load", "torch.load", "torch.load", "torch.load", "nn.DataParallel.load_state_dict", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.DataParallel", "nn.DataParallel.named_parameters", "eval", "train.prepare_optimizer", "train.prepare_optimizer", "os.path.isdir", "os.makedirs", "os.path.isdir", "os.mkdir", "model.NetEncoderDecoder", "key.replace", "any", "train.train", "train.test", "tensorboardX.SummaryWriter.add_scalar", "torch.save", "torch.save", "torch.save", "torch.save", "train.test", "ValueError", "torch.load.items", "print", "print", "encoder_params.append", "decoder_params.append", "nn.DataParallel.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "tensorboardX.SummaryWriter.add_scalar", "nn.DataParallel.state_dict", "train.adjust_learning_rate", "train.adjust_learning_rate"], "function", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.prepare_optimizer", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.prepare_optimizer", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.train", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.test", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.test", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.adjust_learning_rate", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.train.adjust_learning_rate"], ["", "def", "main", "(", ")", ":", "\n", "# Training settings", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'input batch size for training (default: 32)'", ")", "\n", "parser", ".", "add_argument", "(", "'-epochs'", ",", "type", "=", "int", ",", "default", "=", "40", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of epochs to train (default: 40)'", ")", "\n", "parser", ".", "add_argument", "(", "'-log_interval'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'how many batches to wait before logging training status'", ")", "\n", "parser", ".", "add_argument", "(", "'-threshold'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "'threshold for the evaluation (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'-image_path'", ",", "help", "=", "'path for the training and validation folders'", ")", "\n", "parser", ".", "add_argument", "(", "'-num_workers'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'-snapshot'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'-resume'", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'-test_model'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'-save_path'", ")", "\n", "parser", ".", "add_argument", "(", "'-input_size'", ",", "default", "=", "None", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-num_encoder_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-num_decoder_layers'", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "'-num_att_heads'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-hidden_size'", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "parser", ".", "add_argument", "(", "'-weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.0", ")", "\n", "parser", ".", "add_argument", "(", "'-optim_params'", ",", "type", "=", "str", ",", "\n", "default", "=", "\"[{'opt': 'sgd', 'lr': 1e-3}, {'opt': 'adam', 'lr': 1e-4}]\"", ")", "\n", "parser", ".", "add_argument", "(", "'-num_queries'", ",", "type", "=", "int", ",", "default", "=", "25", ")", "\n", "parser", ".", "add_argument", "(", "'-dropout'", ",", "type", "=", "float", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'-mix_up'", ",", "action", "=", "'store_true'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "args", ".", "image_path", "is", "not", "None", "\n", "\n", "device", "=", "\"cuda\"", "\n", "save_path", "=", "args", ".", "save_path", "\n", "if", "not", "args", ".", "test_model", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "save_path", ")", "\n", "", "log_path", "=", "os", ".", "path", ".", "join", "(", "save_path", ",", "'logs'", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_path", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "log_path", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "snapshot", "==", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Delete the log path manually %s'", "%", "log_path", ")", "\n", "", "", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "log_path", ")", "\n", "\n", "", "default_input_size", "=", "288", "\n", "train_dataset", "=", "COCOMultiLabel", "(", "train", "=", "True", ",", "\n", "classification", "=", "False", ",", "\n", "image_path", "=", "args", ".", "image_path", ",", "\n", "image_size", "=", "args", ".", "input_size", "if", "args", ".", "input_size", "else", "default_input_size", ")", "\n", "test_dataset", "=", "COCOMultiLabel", "(", "train", "=", "False", ",", "\n", "classification", "=", "False", ",", "\n", "image_path", "=", "args", ".", "image_path", ",", "\n", "image_size", "=", "args", ".", "input_size", "if", "args", ".", "input_size", "else", "default_input_size", ")", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "False", ",", "\n", "collate_fn", "=", "my_collate", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "False", ",", "\n", "drop_last", "=", "False", ",", "\n", "collate_fn", "=", "my_collate", ")", "\n", "model", "=", "NetEncoderDecoder", "(", "num_encoder_layers", "=", "args", ".", "num_encoder_layers", ",", "\n", "num_decoder_layers", "=", "args", ".", "num_decoder_layers", ",", "\n", "num_att_heads", "=", "args", ".", "num_att_heads", ",", "\n", "hidden_size", "=", "args", ".", "hidden_size", ",", "\n", "num_queries", "=", "args", ".", "num_queries", ",", "\n", "dropout", "=", "args", ".", "dropout", ")", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "snapshot", ":", "\n", "        ", "weights", "=", "torch", ".", "load", "(", "args", ".", "snapshot", ")", "\n", "weights", "=", "{", "key", ".", "replace", "(", "'module.'", ",", "''", ")", ":", "value", "for", "key", ",", "value", "in", "weights", ".", "items", "(", ")", "}", "\n", "model", ".", "load_state_dict", "(", "weights", ")", "\n", "if", "args", ".", "test_model", "==", "False", ":", "\n", "            ", "if", "args", ".", "resume", "is", "not", "None", ":", "\n", "                ", "resume", "=", "args", ".", "resume", "\n", "print", "(", "\"Resuming at\"", ",", "resume", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"Training from scratch\"", ")", "\n", "resume", "=", "1", "\n", "", "", "else", ":", "\n", "            ", "resume", "=", "0", "\n", "", "", "else", ":", "\n", "        ", "resume", "=", "1", "\n", "", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "        ", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "highest_f1", "=", "0", "\n", "epochs_without_imp", "=", "0", "\n", "all_epochs_without_imp", "=", "0", "\n", "backbone_layers", "=", "{", "'conv1'", ",", "'bn1'", ",", "'layer1'", ",", "'layer2'", ",", "'layer3'", ",", "'layer4'", "}", "\n", "if", "not", "args", ".", "test_model", ":", "\n", "        ", "encoder_params", "=", "[", "]", "\n", "decoder_params", "=", "[", "]", "\n", "for", "param_name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "any", "(", "[", "True", "if", "x", "in", "param_name", "else", "False", "for", "x", "in", "backbone_layers", "]", ")", ":", "\n", "                ", "encoder_params", ".", "append", "(", "{", "'params'", ":", "param", "}", ")", "\n", "", "else", ":", "\n", "                ", "decoder_params", ".", "append", "(", "{", "'params'", ":", "param", "}", ")", "\n", "", "", "encoder_optim_params", ",", "decoder_optim_params", "=", "eval", "(", "args", ".", "optim_params", ")", "\n", "encoder_optim", "=", "prepare_optimizer", "(", "encoder_optim_params", ",", "encoder_params", ")", "\n", "decoder_optim", "=", "prepare_optimizer", "(", "decoder_optim_params", ",", "decoder_params", ")", "\n", "\n", "", "for", "epoch", "in", "range", "(", "resume", ",", "args", ".", "epochs", "+", "1", ")", ":", "\n", "        ", "if", "args", ".", "test_model", "==", "False", ":", "\n", "            ", "train", "(", "args", ",", "model", ",", "device", ",", "train_loader", ",", "\n", "encoder_optim", ",", "decoder_optim", ",", "epoch", ",", "writer", ")", "\n", "results", "=", "test", "(", "args", ",", "model", ",", "device", ",", "test_loader", ",", "args", ".", "threshold", ")", "\n", "for", "x", "in", "[", "'micro'", ",", "'macro'", "]", ":", "\n", "                ", "for", "y", "in", "[", "'precision'", ",", "'recall'", ",", "'f1'", "]", ":", "\n", "                    ", "writer", ".", "add_scalar", "(", "'%s_%s'", "%", "(", "x", ",", "y", ")", ",", "results", "[", "x", "]", "[", "y", "]", "*", "100", ",", "epoch", ")", "\n", "", "", "writer", ".", "add_scalar", "(", "'mAP'", ",", "results", "[", "'mAP'", "]", "*", "100", ",", "epoch", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", "+", "\"/checkpoint.pt\"", ")", "\n", "f1", "=", "(", "results", "[", "'macro'", "]", "[", "'f1'", "]", "+", "results", "[", "'micro'", "]", "[", "'f1'", "]", ")", "/", "2", "\n", "if", "f1", ">", "highest_f1", ":", "\n", "                ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", "+", "\"/BEST_checkpoint.pt\"", ")", "\n", "highest_f1", "=", "f1", "\n", "epochs_without_imp", "=", "0", "\n", "", "else", ":", "\n", "                ", "epochs_without_imp", "+=", "1", "\n", "if", "epochs_without_imp", "==", "3", ":", "\n", "                    ", "adjust_learning_rate", "(", "encoder_optim", ",", "0.1", ")", "\n", "adjust_learning_rate", "(", "decoder_optim", ",", "0.1", ")", "\n", "epochs_without_imp", "=", "0", "\n", "all_epochs_without_imp", "+=", "1", "\n", "", "if", "all_epochs_without_imp", "==", "4", ":", "\n", "                    ", "break", "\n", "", "", "", "else", ":", "\n", "            ", "results", "=", "test", "(", "args", ",", "model", ",", "device", ",", "test_loader", ",", "args", ".", "threshold", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.COCOMultiLabel.__init__": [[45, 68], ["torch.utils.data.Dataset.__init__", "max", "list", "torchvision.Compose", "json.load", "len", "dataset.COCOMultiLabel.coco_json.keys", "open", "json.load", "dataset.COCOMultiLabel.coco_json.values", "torchvision.Resize", "torchvision.ToTensor", "torchvision.Normalize", "open"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "train", ",", "classification", ",", "image_path", ",", "image_size", ")", ":", "\n", "        ", "super", "(", "COCOMultiLabel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train", "=", "train", "\n", "if", "self", ".", "train", "==", "True", ":", "\n", "            ", "self", ".", "coco_json", "=", "json", ".", "load", "(", "open", "(", "'coco_train.json'", ",", "'r'", ")", ")", "\n", "self", ".", "image_path", "=", "image_path", "+", "'/train2014/'", "\n", "", "elif", "self", ".", "train", "==", "False", ":", "\n", "            ", "self", ".", "coco_json", "=", "json", ".", "load", "(", "open", "(", "'coco_val.json'", ",", "'r'", ")", ")", "\n", "self", ".", "image_path", "=", "image_path", "+", "'/val2014/'", "\n", "\n", "", "category_counts", "=", "[", "len", "(", "value", "[", "'categories'", "]", ")", "for", "value", "in", "self", ".", "coco_json", ".", "values", "(", ")", "]", "\n", "self", ".", "max_length", "=", "max", "(", "category_counts", ")", "\n", "self", ".", "image_size", "=", "image_size", "\n", "\n", "self", ".", "classification", "=", "classification", "\n", "self", ".", "fns", "=", "list", "(", "self", ".", "coco_json", ".", "keys", "(", ")", ")", "\n", "self", ".", "image_size", "=", "image_size", "\n", "\n", "self", ".", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "image_size", ",", "image_size", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.COCOMultiLabel.__len__": [[70, 72], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "fns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.COCOMultiLabel.__getitem__": [[73, 102], ["PIL.Image.open", "range", "torch.LongTensor", "len", "augmenter.convert", "dataset.COCOMultiLabel.transform", "torch.LongTensor.append", "torch.LongTensor.append", "dataset.augmenter", "print", "len", "print"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.augmenter"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "json_key", "=", "self", ".", "fns", "[", "idx", "]", "\n", "categories_batch", "=", "self", ".", "coco_json", "[", "json_key", "]", "[", "'categories'", "]", "\n", "image_fn", "=", "self", ".", "image_path", "+", "json_key", "\n", "\n", "image", "=", "Image", ".", "open", "(", "image_fn", ")", "\n", "if", "image", ".", "mode", "!=", "'RGB'", ":", "\n", "            ", "image", "=", "image", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "", "if", "self", ".", "train", ":", "\n", "            ", "try", ":", "\n", "                ", "image", "=", "augmenter", "(", "image", ")", "\n", "", "except", "IOError", ":", "\n", "                ", "print", "(", "\"augmentation error\"", ")", "\n", "", "", "try", ":", "\n", "            ", "image", "=", "self", ".", "transform", "(", "image", ")", "\n", "", "except", "IOError", ":", "\n", "            ", "print", "(", "\"transformer error\"", ")", "\n", "return", "None", "\n", "\n", "", "labels", "=", "[", "]", "\n", "for", "category", "in", "categories_batch", ":", "\n", "            ", "labels", ".", "append", "(", "category_dict_sequential", "[", "category", "]", ")", "\n", "", "for", "_", "in", "range", "(", "self", ".", "max_length", "-", "len", "(", "categories_batch", ")", ")", ":", "\n", "            ", "labels", ".", "append", "(", "category_dict_sequential", "[", "'<empty>'", "]", ")", "\n", "\n", "", "labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "label_number", "=", "len", "(", "categories_batch", ")", "\n", "return", "image", ",", "labels", ",", "label_number", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.augmenter": [[8, 13], ["torchvision.RandomHorizontalFlip", "torchvision.ColorJitter", "torchvision.RandomAffine"], "function", ["None"], ["def", "augmenter", "(", "image", ")", ":", "\n", "    ", "return", "transforms", ".", "RandomHorizontalFlip", "(", "p", "=", "0.5", ")", "(", "\n", "transforms", ".", "ColorJitter", "(", "contrast", "=", "0.25", ")", "(", "\n", "transforms", ".", "RandomAffine", "(", "\n", "0", ",", "translate", "=", "(", "0.03", ",", "0.03", ")", ")", "(", "image", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.dataset.process_image": [[15, 24], ["PIL.Image.fromarray", "zip", "torchvision.ToTensor", "channel.sub_().mul_", "channel.sub_"], "function", ["None"], ["", "def", "process_image", "(", "image", ")", ":", "\n", "    ", "means", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", "\n", "inv_stds", "=", "[", "1", "/", "0.229", ",", "1", "/", "0.224", ",", "1", "/", "0.225", "]", "\n", "\n", "image", "=", "Image", ".", "fromarray", "(", "image", ")", "\n", "image", "=", "transforms", ".", "ToTensor", "(", ")", "(", "image", ")", "\n", "for", "channel", ",", "mean", ",", "inv_std", "in", "zip", "(", "image", ",", "means", ",", "inv_stds", ")", ":", "\n", "        ", "channel", ".", "sub_", "(", "mean", ")", ".", "mul_", "(", "inv_std", ")", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerDecoder.__init__": [[22, 26], ["torch.Module.__init__", "model._get_clones"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_clones"], ["    ", "def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerDecoder.forward": [[27, 32], ["layer", "object_queries.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "object_queries", ",", "encoder_src", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "object_queries", ",", "attn", "=", "layer", "(", "object_queries", ",", "encoder_src", ")", "\n", "\n", "", "return", "object_queries", ".", "unsqueeze", "(", "0", ")", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerDecoderLayer.__init__": [[35, 53], ["torch.Module.__init__", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "model._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_activation_fn"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", "=", "1", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "\n", "activation", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm3", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerDecoderLayer.forward": [[54, 68], ["model.TransformerDecoderLayer.norm1", "model.TransformerDecoderLayer.multihead_attn", "model.TransformerDecoderLayer.norm2", "model.TransformerDecoderLayer.linear2", "model.TransformerDecoderLayer.norm3", "model.TransformerDecoderLayer.self_attn", "model.TransformerDecoderLayer.dropout1", "model.TransformerDecoderLayer.dropout2", "model.TransformerDecoderLayer.dropout", "model.TransformerDecoderLayer.dropout3", "model.TransformerDecoderLayer.activation", "model.TransformerDecoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "object_queries", ",", "encoder_src", ")", ":", "\n", "        ", "q", "=", "k", "=", "object_queries", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "q", ",", "k", ",", "value", "=", "object_queries", ")", "[", "0", "]", "\n", "tgt", "=", "object_queries", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", ",", "attn", "=", "self", ".", "multihead_attn", "(", "query", "=", "tgt", ",", "\n", "key", "=", "encoder_src", ",", "\n", "value", "=", "encoder_src", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerEncoder.__init__": [[70, 74], ["torch.Module.__init__", "model._get_clones"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_clones"], ["    ", "def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerEncoder.forward": [[75, 80], ["layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ")", ":", "\n", "        ", "output", "=", "src", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "output", ",", "_", "=", "layer", "(", "output", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerEncoderLayer.__init__": [[83, 98], ["torch.Module.__init__", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "model._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_activation_fn"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "nhead", "=", "1", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "\n", "activation", "=", "\"relu\"", ",", "normalize_before", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "dropout1", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "self", ".", "normalize_before", "=", "normalize_before", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.TransformerEncoderLayer.forward": [[99, 108], ["model.TransformerEncoderLayer.self_attn", "model.TransformerEncoderLayer.norm1", "model.TransformerEncoderLayer.linear2", "model.TransformerEncoderLayer.norm2", "model.TransformerEncoderLayer.dropout1", "model.TransformerEncoderLayer.dropout", "model.TransformerEncoderLayer.dropout2", "model.TransformerEncoderLayer.activation", "model.TransformerEncoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ")", ":", "\n", "        ", "q", "=", "k", "=", "src", "\n", "src2", ",", "attn", "=", "self", ".", "self_attn", "(", "q", ",", "k", ",", "value", "=", "src", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "return", "src", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__": [[110, 138], ["torch.Module.__init__", "torchvision.models.resnet101", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "model.TransformerEncoderLayer", "model.TransformerEncoder", "model.TransformerDecoderLayer", "model.TransformerDecoder", "model.NetEncoderDecoder._reset_transformer_parameters", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.__init__", "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder._reset_transformer_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_encoder_layers", "=", "None", ",", "num_decoder_layers", "=", "None", ",", "hidden_size", "=", "2048", ",", "\n", "dropout", "=", "0.1", ",", "num_att_heads", "=", "1", ",", "num_queries", "=", "20", ")", ":", "\n", "        ", "super", "(", "NetEncoderDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "resnet", "=", "torchvision", ".", "models", ".", "resnet101", "(", "True", ")", "\n", "self", ".", "conv1", "=", "resnet", ".", "conv1", "\n", "self", ".", "bn1", "=", "resnet", ".", "bn1", "\n", "self", ".", "relu", "=", "resnet", ".", "relu", "\n", "self", ".", "maxpool", "=", "resnet", ".", "maxpool", "\n", "self", ".", "layer1", "=", "resnet", ".", "layer1", "\n", "self", ".", "layer2", "=", "resnet", ".", "layer2", "\n", "self", ".", "layer3", "=", "resnet", ".", "layer3", "\n", "self", ".", "layer4", "=", "resnet", ".", "layer4", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "num_classes", "=", "80", "+", "1", "\n", "self", ".", "input_proj", "=", "None", "\n", "if", "hidden_size", "!=", "2048", ":", "\n", "            ", "self", ".", "input_proj", "=", "nn", ".", "Conv2d", "(", "2048", ",", "hidden_size", ",", "kernel_size", "=", "1", ")", "\n", "", "encoder_layer", "=", "TransformerEncoderLayer", "(", "hidden_size", ",", "\n", "nhead", "=", "num_att_heads", ")", "\n", "self", ".", "transformer_encoder", "=", "TransformerEncoder", "(", "encoder_layer", ",", "num_encoder_layers", ")", "\n", "decoder_layer", "=", "TransformerDecoderLayer", "(", "hidden_size", ",", "dropout", "=", "dropout", ",", "\n", "nhead", "=", "num_att_heads", ")", "\n", "self", ".", "transformer_decoder", "=", "TransformerDecoder", "(", "decoder_layer", ",", "num_decoder_layers", ")", "\n", "self", ".", "_reset_transformer_parameters", "(", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_classes", ")", "\n", "self", ".", "query_embed", "=", "nn", ".", "Embedding", "(", "num_queries", ",", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder._reset_transformer_parameters": [[139, 147], ["print", "model.NetEncoderDecoder.transformer_encoder.parameters", "model.NetEncoderDecoder.transformer_decoder.parameters", "p.dim", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "p.dim", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_"], "methods", ["None"], ["", "def", "_reset_transformer_parameters", "(", "self", ")", ":", "\n", "        ", "print", "(", "'Resetting the transformer parameters'", ")", "\n", "for", "p", "in", "self", ".", "transformer_encoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "p", ")", "\n", "", "", "for", "p", "in", "self", ".", "transformer_decoder", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.forward": [[148, 168], ["model.NetEncoderDecoder.conv1", "model.NetEncoderDecoder.bn1", "model.NetEncoderDecoder.relu", "model.NetEncoderDecoder.maxpool", "model.NetEncoderDecoder.layer1", "model.NetEncoderDecoder.layer2", "model.NetEncoderDecoder.layer3", "model.NetEncoderDecoder.layer4", "model.NetEncoderDecoder.forward_encoder", "query_embed.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "model.NetEncoderDecoder.transformer_decoder", "hs.transpose().squeeze.transpose().squeeze.transpose().squeeze", "model.NetEncoderDecoder.fc", "model.NetEncoderDecoder.input_proj", "query_embed.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "hs.transpose().squeeze.transpose().squeeze.transpose"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.forward_encoder"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "bsize", "=", "x", ".", "shape", "[", "0", "]", "\n", "img_size", "=", "x", ".", "shape", "[", "2", ":", "]", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "bn1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "if", "self", ".", "input_proj", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "input_proj", "(", "x", ")", "\n", "", "x", "=", "self", ".", "forward_encoder", "(", "x", ")", "\n", "query_embed", "=", "self", ".", "query_embed", ".", "weight", "\n", "query_embed", "=", "query_embed", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "bsize", ",", "1", ")", "\n", "hs", ",", "decoder_attn", "=", "self", ".", "transformer_decoder", "(", "query_embed", ",", "x", ")", "\n", "hs", "=", "hs", ".", "transpose", "(", "1", ",", "2", ")", ".", "squeeze", "(", "0", ")", "\n", "out", "=", "self", ".", "fc", "(", "hs", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.forward_encoder": [[169, 178], ["model.NetEncoderDecoder.view", "model.NetEncoderDecoder.unsqueeze", "model.NetEncoderDecoder.view().permute", "model.NetEncoderDecoder.transformer_encoder", "model.NetEncoderDecoder.view"], "methods", ["None"], ["", "def", "forward_encoder", "(", "self", ",", "x", ")", ":", "\n", "        ", "bsize", "=", "x", ".", "shape", "[", "0", "]", "\n", "dim", "=", "x", ".", "shape", "[", "1", "]", "\n", "x", "=", "x", ".", "view", "(", "bsize", ",", "dim", ",", "-", "1", ")", "\n", "bs", ",", "num_channels", ",", "num_pixels", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "unsqueeze", "(", "3", ")", "\n", "x", "=", "x", ".", "view", "(", "bs", ",", "num_channels", ",", "-", "1", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "x", "=", "self", ".", "transformer_encoder", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.freeze_bn": [[179, 189], ["print", "layer.modules", "isinstance", "m.eval"], "methods", ["None"], ["", "def", "freeze_bn", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Freezing bn.\"", ")", "\n", "backbone_layers", "=", "[", "self", ".", "conv1", ",", "self", ".", "bn1", ",", "self", ".", "layer1", ",", "self", ".", "layer2", ",", "\n", "self", ".", "layer3", ",", "self", ".", "layer4", "]", "\n", "for", "layer", "in", "backbone_layers", ":", "\n", "            ", "for", "m", "in", "layer", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "m", ".", "weight", ".", "requires_grad", "=", "False", "\n", "m", ".", "bias", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.freeze": [[190, 195], ["model.NetEncoderDecoder.freeze_bn"], "methods", ["home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model.NetEncoderDecoder.freeze_bn"], ["", "", "", "", "def", "freeze", "(", "self", ",", "layer", ")", ":", "\n", "        ", "if", "layer", "==", "'bn'", ":", "\n", "            ", "self", ".", "freeze_bn", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_clones": [[8, 10], ["torch.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.voyazici_visual-transformers-classification.None.model._get_activation_fn": [[11, 19], ["None"], "function", ["None"], ["", "def", "_get_activation_fn", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "return", "F", ".", "relu", "\n", "", "if", "activation", "==", "\"gelu\"", ":", "\n", "        ", "return", "F", ".", "gelu", "\n", "", "if", "activation", "==", "\"glu\"", ":", "\n", "        ", "return", "F", ".", "glu", "\n", "", "raise", "NotImplementedError", "\n", "\n"]]}