{"home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.idx_at_times": [[6, 30], ["np.searchsorted.clip", "numpy.searchsorted", "pandas.Series", "len", "numpy.searchsorted"], "function", ["None"], ["def", "idx_at_times", "(", "index_surv", ",", "times", ",", "steps", "=", "'pre'", ",", "assert_sorted", "=", "True", ")", ":", "\n", "    ", "\"\"\"Gives index of `index_surv` corresponding to `time`, i.e. \n    `index_surv[idx_at_times(index_surv, times)]` give the values of `index_surv`\n    closet to `times`.\n    \n    Arguments:\n        index_surv {np.array} -- Durations of survival estimates\n        times {np.array} -- Values one want to match to `index_surv`\n    \n    Keyword Arguments:\n        steps {str} -- Round 'pre' (closest value higher) or 'post'\n          (closest value lower) (default: {'pre'})\n        assert_sorted {bool} -- Assert that index_surv is monotone (default: {True})\n    \n    Returns:\n        np.array -- Index of `index_surv` that is closest to `times`\n    \"\"\"", "\n", "if", "assert_sorted", ":", "\n", "        ", "assert", "pd", ".", "Series", "(", "index_surv", ")", ".", "is_monotonic_increasing", ",", "\"Need 'index_surv' to be monotonic increasing\"", "\n", "", "if", "steps", "==", "'pre'", ":", "\n", "        ", "idx", "=", "np", ".", "searchsorted", "(", "index_surv", ",", "times", ")", "\n", "", "elif", "steps", "==", "'post'", ":", "\n", "        ", "idx", "=", "np", ".", "searchsorted", "(", "index_surv", ",", "times", ",", "side", "=", "'right'", ")", "-", "1", "\n", "", "return", "idx", ".", "clip", "(", "0", ",", "len", "(", "index_surv", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.pycox.utils._group_loop": [[31, 39], ["range"], "function", ["None"], ["", "@", "numba", ".", "njit", "\n", "def", "_group_loop", "(", "n", ",", "surv_idx", ",", "durations", ",", "events", ",", "di", ",", "ni", ")", ":", "\n", "    ", "idx", "=", "0", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "        ", "idx", "+=", "durations", "[", "i", "]", "!=", "surv_idx", "[", "idx", "]", "\n", "di", "[", "idx", "]", "+=", "events", "[", "i", "]", "\n", "ni", "[", "idx", "]", "+=", "1", "\n", "", "return", "di", ",", "ni", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.kaplan_meier": [[40, 88], ["len", "numpy.argsort", "numpy.unique", "numpy.zeros", "numpy.zeros_like", "utils._group_loop", "zero_survive.any", "pandas.Series", "len", "durations.min", "warnings.warn", "len", "np.zeros.cumsum", "numpy.argmax", "numpy.zeros_like", "numpy.exp", "numpy.exp", "np.unique.min", "numpy.ones", "numpy.zeros", "numpy.log().cumsum", "numpy.log().cumsum", "len", "len", "durations.min", "numpy.log", "numpy.log"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.pycox.utils._group_loop"], ["", "def", "kaplan_meier", "(", "durations", ",", "events", ",", "start_duration", "=", "0", ")", ":", "\n", "    ", "\"\"\"A very simple Kaplan-Meier fitter. For a more complete implementation\n    see `lifelines`.\n    \n    Arguments:\n        durations {np.array} -- durations array\n        events {np.arrray} -- events array 0/1\n    \n    Keyword Arguments:\n        start_duration {int} -- Time start as `start_duration`. (default: {0})\n    \n    Returns:\n        pd.Series -- Kaplan-Meier estimates.\n    \"\"\"", "\n", "n", "=", "len", "(", "durations", ")", "\n", "assert", "n", "==", "len", "(", "events", ")", "\n", "if", "start_duration", ">", "durations", ".", "min", "(", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "f\"start_duration {start_duration} is larger than minimum duration {durations.min()}. \"", "\n", "\"If intentional, consider changing start_duration when calling kaplan_meier.\"", ")", "\n", "", "order", "=", "np", ".", "argsort", "(", "durations", ")", "\n", "durations", "=", "durations", "[", "order", "]", "\n", "events", "=", "events", "[", "order", "]", "\n", "surv_idx", "=", "np", ".", "unique", "(", "durations", ")", "\n", "ni", "=", "np", ".", "zeros", "(", "len", "(", "surv_idx", ")", ",", "dtype", "=", "'int'", ")", "\n", "di", "=", "np", ".", "zeros_like", "(", "ni", ")", "\n", "di", ",", "ni", "=", "_group_loop", "(", "n", ",", "surv_idx", ",", "durations", ",", "events", ",", "di", ",", "ni", ")", "\n", "ni", "=", "n", "-", "ni", ".", "cumsum", "(", ")", "\n", "ni", "[", "1", ":", "]", "=", "ni", "[", ":", "-", "1", "]", "\n", "ni", "[", "0", "]", "=", "n", "\n", "survive", "=", "1", "-", "di", "/", "ni", "\n", "zero_survive", "=", "survive", "==", "0", "\n", "if", "zero_survive", ".", "any", "(", ")", ":", "\n", "        ", "i", "=", "np", ".", "argmax", "(", "zero_survive", ")", "\n", "surv", "=", "np", ".", "zeros_like", "(", "survive", ")", "\n", "surv", "[", ":", "i", "]", "=", "np", ".", "exp", "(", "np", ".", "log", "(", "survive", "[", ":", "i", "]", ")", ".", "cumsum", "(", ")", ")", "\n", "# surv[i:] = surv[i-1]", "\n", "surv", "[", "i", ":", "]", "=", "0.", "\n", "", "else", ":", "\n", "        ", "surv", "=", "np", ".", "exp", "(", "np", ".", "log", "(", "1", "-", "di", "/", "ni", ")", ".", "cumsum", "(", ")", ")", "\n", "", "if", "start_duration", "<", "surv_idx", ".", "min", "(", ")", ":", "\n", "        ", "tmp", "=", "np", ".", "ones", "(", "len", "(", "surv", ")", "+", "1", ",", "dtype", "=", "surv", ".", "dtype", ")", "\n", "tmp", "[", "1", ":", "]", "=", "surv", "\n", "surv", "=", "tmp", "\n", "tmp", "=", "np", ".", "zeros", "(", "len", "(", "surv_idx", ")", "+", "1", ",", "dtype", "=", "surv_idx", ".", "dtype", ")", "\n", "tmp", "[", "1", ":", "]", "=", "surv_idx", "\n", "surv_idx", "=", "tmp", "\n", "", "surv", "=", "pd", ".", "Series", "(", "surv", ",", "surv_idx", ")", "\n", "return", "surv", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.__init__": [[26, 29], ["sklearn.preprocessing.StandardScaler"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "log_duration", "=", "False", ",", "with_mean", "=", "True", ",", "with_std", "=", "True", ")", ":", "\n", "        ", "self", ".", "log_duration", "=", "log_duration", "\n", "self", ".", "duration_scaler", "=", "StandardScaler", "(", "copy", "=", "True", ",", "with_mean", "=", "with_mean", ",", "with_std", "=", "with_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.map_scaled_to_orig": [[30, 41], ["hasattr", "ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "map_scaled_to_orig", "(", "self", ")", ":", "\n", "        ", "\"\"\"Map from transformed durations back to the original durations, i.e. inverse transform.\n\n        Use it to e.g. set index of survival predictions:\n            surv = model.predict_surv_df(x_test)\n            surv.index = labtrans.map_scaled_to_orig(surv.index)\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'_inverse_duration_map'", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Need to fit the models before you can call this method'", ")", "\n", "", "return", "self", ".", "_inverse_duration_map", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.fit": [[42, 45], ["label_transforms.LabTransCoxTime.fit_transform"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform"], ["", "def", "fit", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "self", ".", "fit_transform", "(", "durations", ",", "events", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.fit_transform": [[46, 56], ["numpy.log1p.astype", "events.astype.astype.astype", "label_transforms.LabTransCoxTime.duration_scaler.fit_transform().flatten", "numpy.vectorize", "numpy.log1p", "label_transforms.LabTransCoxTime.duration_scaler.fit_transform", "zip", "numpy.log1p.reshape"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform"], ["", "def", "fit_transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "train_durations", "=", "durations", "\n", "durations", "=", "durations", ".", "astype", "(", "'float32'", ")", "\n", "events", "=", "events", ".", "astype", "(", "'float32'", ")", "\n", "if", "self", ".", "log_duration", ":", "\n", "            ", "durations", "=", "np", ".", "log1p", "(", "durations", ")", "\n", "", "durations", "=", "self", ".", "duration_scaler", ".", "fit_transform", "(", "durations", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ".", "flatten", "(", ")", "\n", "self", ".", "_inverse_duration_map", "=", "{", "scaled", ":", "orig", "for", "orig", ",", "scaled", "in", "zip", "(", "train_durations", ",", "durations", ")", "}", "\n", "self", ".", "_inverse_duration_map", "=", "np", ".", "vectorize", "(", "self", ".", "_inverse_duration_map", ".", "get", ")", "\n", "return", "durations", ",", "events", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.transform": [[57, 64], ["numpy.log1p.astype", "events.astype.astype.astype", "label_transforms.LabTransCoxTime.duration_scaler.transform().flatten", "numpy.log1p", "label_transforms.LabTransCoxTime.duration_scaler.transform", "numpy.log1p.reshape"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "durations", "=", "durations", ".", "astype", "(", "'float32'", ")", "\n", "events", "=", "events", ".", "astype", "(", "'float32'", ")", "\n", "if", "self", ".", "log_duration", ":", "\n", "            ", "durations", "=", "np", ".", "log1p", "(", "durations", ")", "\n", "", "durations", "=", "self", ".", "duration_scaler", ".", "transform", "(", "durations", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ".", "flatten", "(", ")", "\n", "return", "durations", ",", "events", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.out_features": [[65, 74], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_features", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of output features that should be used in the torch model.\n        This always returns 1, and is just included for api design purposes.\n        \n        Returns:\n            [int] -- Number of output features.\n        \"\"\"", "\n", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransDiscreteTime.__init__": [[94, 110], ["hasattr", "pycox.preprocessing.discretization.IdxDiscUnknownC", "type", "type", "numpy.array"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cuts", ",", "scheme", "=", "'equidistant'", ",", "min_", "=", "0.", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "self", ".", "_cuts", "=", "cuts", "\n", "self", ".", "_scheme", "=", "scheme", "\n", "self", ".", "_min", "=", "min_", "\n", "self", ".", "_dtype_init", "=", "dtype", "\n", "self", ".", "_predefined_cuts", "=", "False", "\n", "self", ".", "cuts", "=", "None", "\n", "if", "hasattr", "(", "cuts", ",", "'__iter__'", ")", ":", "\n", "            ", "if", "type", "(", "cuts", ")", "is", "list", ":", "\n", "                ", "cuts", "=", "np", ".", "array", "(", "cuts", ")", "\n", "", "self", ".", "cuts", "=", "cuts", "\n", "self", ".", "idu", "=", "IdxDiscUnknownC", "(", "self", ".", "cuts", ")", "\n", "assert", "dtype", "is", "None", ",", "\"Need `dtype` to be `None` for specified cuts\"", "\n", "self", ".", "_dtype", "=", "type", "(", "self", ".", "cuts", "[", "0", "]", ")", "\n", "self", ".", "_dtype_init", "=", "self", ".", "_dtype", "\n", "self", ".", "_predefined_cuts", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransDiscreteTime.fit": [[111, 125], ["durations.astype.astype.astype", "pycox.preprocessing.discretization.make_cuts", "pycox.preprocessing.discretization.IdxDiscUnknownC", "warnings.warn", "isinstance", "numpy.dtype"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.make_cuts"], ["", "", "def", "fit", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "if", "self", ".", "_predefined_cuts", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Calling fit method, when 'cuts' are already defined. Leaving cuts unchanged.\"", ")", "\n", "return", "self", "\n", "", "self", ".", "_dtype", "=", "self", ".", "_dtype_init", "\n", "if", "self", ".", "_dtype", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "durations", "[", "0", "]", ",", "np", ".", "floating", ")", ":", "\n", "                ", "self", ".", "_dtype", "=", "durations", ".", "dtype", "\n", "", "else", ":", "\n", "                ", "self", ".", "_dtype", "=", "np", ".", "dtype", "(", "'float64'", ")", "\n", "", "", "durations", "=", "durations", ".", "astype", "(", "self", ".", "_dtype", ")", "\n", "self", ".", "cuts", "=", "make_cuts", "(", "self", ".", "_cuts", ",", "self", ".", "_scheme", ",", "durations", ",", "events", ",", "self", ".", "_min", ",", "self", ".", "_dtype", ")", "\n", "self", ".", "idu", "=", "IdxDiscUnknownC", "(", "self", ".", "cuts", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransDiscreteTime.fit_transform": [[126, 130], ["label_transforms.LabTransDiscreteTime.fit", "label_transforms.LabTransDiscreteTime.transform"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "fit_transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "self", ".", "fit", "(", "durations", ",", "events", ")", "\n", "idx_durations", ",", "events", "=", "self", ".", "transform", "(", "durations", ",", "events", ")", "\n", "return", "idx_durations", ",", "events", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransDiscreteTime.transform": [[131, 137], ["pycox.preprocessing.discretization._values_if_series", "durations.astype.astype.astype", "pycox.preprocessing.discretization._values_if_series", "label_transforms.LabTransDiscreteTime.idu.transform", "idx_durations.astype", "pycox.preprocessing.discretization._values_if_series.astype"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._values_if_series", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._values_if_series", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "durations", "=", "_values_if_series", "(", "durations", ")", "\n", "durations", "=", "durations", ".", "astype", "(", "self", ".", "_dtype", ")", "\n", "events", "=", "_values_if_series", "(", "events", ")", "\n", "idx_durations", ",", "events", "=", "self", ".", "idu", ".", "transform", "(", "durations", ",", "events", ")", "\n", "return", "idx_durations", ".", "astype", "(", "'int64'", ")", ",", "events", ".", "astype", "(", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransDiscreteTime.out_features": [[138, 148], ["len", "ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_features", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of output features that should be used in the torch model.\n        \n        Returns:\n            [int] -- Number of output features.\n        \"\"\"", "\n", "if", "self", ".", "cuts", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to call `fit` before this is accessible.\"", ")", "\n", "", "return", "len", "(", "self", ".", "cuts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransPCHazard.__init__": [[170, 188], ["hasattr", "pycox.preprocessing.discretization.IdxDiscUnknownC", "type", "type", "numpy.array"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cuts", ",", "scheme", "=", "'equidistant'", ",", "min_", "=", "0.", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "self", ".", "_cuts", "=", "cuts", "\n", "self", ".", "_scheme", "=", "scheme", "\n", "self", ".", "_min", "=", "min_", "\n", "self", ".", "_dtype_init", "=", "dtype", "\n", "self", ".", "_predefined_cuts", "=", "False", "\n", "self", ".", "cuts", "=", "None", "\n", "if", "hasattr", "(", "cuts", ",", "'__iter__'", ")", ":", "\n", "            ", "if", "type", "(", "cuts", ")", "is", "list", ":", "\n", "                ", "cuts", "=", "np", ".", "array", "(", "cuts", ")", "\n", "", "self", ".", "cuts", "=", "cuts", "\n", "self", ".", "idu", "=", "IdxDiscUnknownC", "(", "self", ".", "cuts", ")", "\n", "assert", "dtype", "is", "None", ",", "\"Need `dtype` to be `None` for specified cuts\"", "\n", "self", ".", "_dtype", "=", "type", "(", "self", ".", "cuts", "[", "0", "]", ")", "\n", "self", ".", "_dtype_init", "=", "self", ".", "_dtype", "\n", "self", ".", "_predefined_cuts", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "_cuts", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransPCHazard.fit": [[189, 204], ["durations.astype.astype.astype", "pycox.preprocessing.discretization.make_cuts", "pycox.preprocessing.discretization.DiscretizeUnknownC", "pycox.preprocessing.discretization.Duration2Idx", "warnings.warn", "isinstance", "numpy.dtype"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.make_cuts"], ["", "", "def", "fit", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "if", "self", ".", "_predefined_cuts", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Calling fit method, when 'cuts' are already defined. Leaving cuts unchanged.\"", ")", "\n", "return", "self", "\n", "", "self", ".", "_dtype", "=", "self", ".", "_dtype_init", "\n", "if", "self", ".", "_dtype", "is", "None", ":", "\n", "            ", "if", "isinstance", "(", "durations", "[", "0", "]", ",", "np", ".", "floating", ")", ":", "\n", "                ", "self", ".", "_dtype", "=", "durations", ".", "dtype", "\n", "", "else", ":", "\n", "                ", "self", ".", "_dtype", "=", "np", ".", "dtype", "(", "'float64'", ")", "\n", "", "", "durations", "=", "durations", ".", "astype", "(", "self", ".", "_dtype", ")", "\n", "self", ".", "cuts", "=", "make_cuts", "(", "self", ".", "_cuts", ",", "self", ".", "_scheme", ",", "durations", ",", "events", ",", "self", ".", "_min", ",", "self", ".", "_dtype", ")", "\n", "self", ".", "duc", "=", "DiscretizeUnknownC", "(", "self", ".", "cuts", ",", "right_censor", "=", "True", ",", "censor_side", "=", "'right'", ")", "\n", "self", ".", "di", "=", "Duration2Idx", "(", "self", ".", "cuts", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransPCHazard.fit_transform": [[205, 208], ["label_transforms.LabTransPCHazard.fit", "label_transforms.LabTransPCHazard.transform"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "fit_transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "self", ".", "fit", "(", "durations", ",", "events", ")", "\n", "return", "self", ".", "transform", "(", "durations", ",", "events", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransPCHazard.transform": [[209, 224], ["pycox.preprocessing.discretization._values_if_series", "durations.astype.astype.astype", "pycox.preprocessing.discretization._values_if_series", "label_transforms.LabTransPCHazard.duc.transform", "label_transforms.LabTransPCHazard.di.transform", "numpy.diff", "label_transforms.LabTransPCHazard.min", "warnings.warn", "label_transforms.LabTransPCHazard.astype", "pycox.preprocessing.discretization._values_if_series.astype", "t_frac.astype"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._values_if_series", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._values_if_series", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "transform", "(", "self", ",", "durations", ",", "events", ")", ":", "\n", "        ", "durations", "=", "_values_if_series", "(", "durations", ")", "\n", "durations", "=", "durations", ".", "astype", "(", "self", ".", "_dtype", ")", "\n", "events", "=", "_values_if_series", "(", "events", ")", "\n", "dur_disc", ",", "events", "=", "self", ".", "duc", ".", "transform", "(", "durations", ",", "events", ")", "\n", "idx_durations", "=", "self", ".", "di", ".", "transform", "(", "dur_disc", ")", "\n", "cut_diff", "=", "np", ".", "diff", "(", "self", ".", "cuts", ")", "\n", "assert", "(", "cut_diff", ">", "0", ")", ".", "all", "(", ")", ",", "'Cuts are not unique.'", "\n", "t_frac", "=", "1.", "-", "(", "dur_disc", "-", "durations", ")", "/", "cut_diff", "[", "idx_durations", "-", "1", "]", "\n", "if", "idx_durations", ".", "min", "(", ")", "==", "0", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"\"\"Got event/censoring at start time. Should be removed! It is set s.t. it has no contribution to loss.\"\"\"", ")", "\n", "t_frac", "[", "idx_durations", "==", "0", "]", "=", "0", "\n", "events", "[", "idx_durations", "==", "0", "]", "=", "0", "\n", "", "idx_durations", "=", "idx_durations", "-", "1", "\n", "return", "idx_durations", ".", "astype", "(", "'int64'", ")", ",", "events", ".", "astype", "(", "'float32'", ")", ",", "t_frac", ".", "astype", "(", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransPCHazard.out_features": [[225, 235], ["ValueError", "len"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_features", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of output features that should be used in the torch model.\n        \n        Returns:\n            [int] -- Number of output features.\n        \"\"\"", "\n", "if", "self", ".", "cuts", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to call `fit` before this is accessible.\"", ")", "\n", "", "return", "len", "(", "self", ".", "cuts", ")", "-", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.feature_transforms.OrderedCategoricalLong.__init__": [[16, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "min_per_category", "=", "20", ",", "return_series", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "min_per_category", "=", "min_per_category", "\n", "self", ".", "return_series", "=", "return_series", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.feature_transforms.OrderedCategoricalLong.fit": [[21, 29], ["pandas.Series().copy", "pandas.Series().copy.value_counts", "pandas.Series", "pandas.Series().copy.astype"], "methods", ["None"], ["", "def", "fit", "(", "self", ",", "series", ",", "y", "=", "None", ")", ":", "\n", "        ", "series", "=", "pd", ".", "Series", "(", "series", ")", ".", "copy", "(", ")", "\n", "smaller", "=", "series", ".", "value_counts", "(", ")", "<", "self", ".", "min_per_category", "\n", "values", "=", "smaller", "[", "smaller", "]", ".", "index", ".", "values", "\n", "for", "v", "in", "values", ":", "\n", "            ", "series", "[", "series", "==", "v", "]", "=", "np", ".", "nan", "\n", "", "self", ".", "categories", "=", "series", ".", "astype", "(", "'category'", ")", ".", "cat", ".", "categories", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.feature_transforms.OrderedCategoricalLong.transform": [[30, 36], ["pandas.Series().copy", "pandas.Categorical", "pandas.Series", "pandas.Series.cat.codes.astype", "pandas.Series"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "series", ",", "y", "=", "None", ")", ":", "\n", "        ", "series", "=", "pd", ".", "Series", "(", "series", ")", ".", "copy", "(", ")", "\n", "transformed", "=", "pd", ".", "Categorical", "(", "series", ",", "categories", "=", "self", ".", "categories", ",", "ordered", "=", "True", ")", "\n", "transformed", "=", "pd", ".", "Series", "(", "transformed", ",", "index", "=", "series", ".", "index", ")", "\n", "transformed", "=", "transformed", ".", "cat", ".", "codes", ".", "astype", "(", "'int64'", ")", "+", "1", "\n", "return", "transformed", "if", "self", ".", "return_series", "else", "transformed", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.feature_transforms.OrderedCategoricalLong.fit_transform": [[37, 39], ["feature_transforms.OrderedCategoricalLong.fit().transform", "feature_transforms.OrderedCategoricalLong.fit"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["", "def", "fit_transform", "(", "self", ",", "series", ",", "y", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "fit", "(", "series", ",", "y", ")", ".", "transform", "(", "series", ",", "y", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit": [[97, 99], ["None"], "methods", ["None"], ["def", "fit", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.transform": [[100, 102], ["None"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform": [[103, 105], ["discretization._OnlyTransform.fit().transform", "discretization._OnlyTransform.fit"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["", "def", "fit_transform", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "return", "self", ".", "fit", "(", "*", "args", ")", ".", "transform", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.DiscretizeUnknownC.__init__": [[112, 116], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cuts", ",", "right_censor", "=", "False", ",", "censor_side", "=", "'left'", ")", ":", "\n", "        ", "self", ".", "cuts", "=", "cuts", "\n", "self", ".", "right_censor", "=", "right_censor", "\n", "self", ".", "censor_side", "=", "censor_side", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.DiscretizeUnknownC.transform": [[117, 133], ["event.astype.astype.astype", "numpy.zeros_like", "discretization.discretize", "c.any", "duration.copy.copy.copy", "discretization.DiscretizeUnknownC.cuts.max", "duration.copy.copy.max", "discretization.DiscretizeUnknownC.cuts.max", "ValueError", "discretization.discretize", "event.astype.astype.astype", "discretization.DiscretizeUnknownC.cuts.max"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.discretize", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.discretize"], ["", "def", "transform", "(", "self", ",", "duration", ",", "event", ")", ":", "\n", "        ", "dtype_event", "=", "event", ".", "dtype", "\n", "event", "=", "event", ".", "astype", "(", "'bool'", ")", "\n", "if", "self", ".", "right_censor", ":", "\n", "            ", "duration", "=", "duration", ".", "copy", "(", ")", "\n", "censor", "=", "duration", ">", "self", ".", "cuts", ".", "max", "(", ")", "\n", "duration", "[", "censor", "]", "=", "self", ".", "cuts", ".", "max", "(", ")", "\n", "event", "[", "censor", "]", "=", "False", "\n", "", "if", "duration", ".", "max", "(", ")", ">", "self", ".", "cuts", ".", "max", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"`duration` contains larger values than cuts. Set `right_censor`=True to censor these\"", ")", "\n", "", "td", "=", "np", ".", "zeros_like", "(", "duration", ")", "\n", "c", "=", "event", "==", "False", "\n", "td", "[", "event", "]", "=", "discretize", "(", "duration", "[", "event", "]", ",", "self", ".", "cuts", ",", "side", "=", "'right'", ",", "error_on_larger", "=", "True", ")", "\n", "if", "c", ".", "any", "(", ")", ":", "\n", "            ", "td", "[", "c", "]", "=", "discretize", "(", "duration", "[", "c", "]", ",", "self", ".", "cuts", ",", "side", "=", "self", ".", "censor_side", ",", "error_on_larger", "=", "True", ")", "\n", "", "return", "td", ",", "event", ".", "astype", "(", "dtype_event", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.Duration2Idx.__init__": [[143, 149], ["NotImplementedError", "discretization.Duration2Idx._make_map"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.Duration2Idx._make_map"], ["    ", "def", "__init__", "(", "self", ",", "durations", "=", "None", ")", ":", "\n", "        ", "self", ".", "durations", "=", "durations", "\n", "if", "durations", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "if", "self", ".", "durations", "is", "not", "None", ":", "\n", "            ", "self", ".", "duration_to_idx", "=", "self", ".", "_make_map", "(", "self", ".", "durations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.Duration2Idx._make_map": [[150, 153], ["numpy.vectorize", "discretization.duration_idx_map"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.duration_idx_map"], ["", "", "@", "staticmethod", "\n", "def", "_make_map", "(", "durations", ")", ":", "\n", "        ", "return", "np", ".", "vectorize", "(", "duration_idx_map", "(", "durations", ")", ".", "get", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.Duration2Idx.transform": [[154, 161], ["discretization.Duration2Idx.duration_to_idx", "numpy.isnan().any", "ValueError", "ValueError", "numpy.isnan"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "duration", ",", "y", "=", "None", ")", ":", "\n", "        ", "if", "duration", ".", "dtype", "is", "not", "self", ".", "durations", ".", "dtype", ":", "\n", "            ", "raise", "ValueError", "(", "'Need `time` to have same type as `self.durations`.'", ")", "\n", "", "idx", "=", "self", ".", "duration_to_idx", "(", "duration", ")", "\n", "if", "np", ".", "isnan", "(", "idx", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Encountered `nans` in transformed indexes.'", ")", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.__init__": [[172, 177], ["discretization.DiscretizeUnknownC", "discretization.Duration2Idx"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "cuts", ",", "label_cols", "=", "None", ",", "censor_side", "=", "'left'", ")", ":", "\n", "        ", "self", ".", "cuts", "=", "cuts", "\n", "self", ".", "duc", "=", "DiscretizeUnknownC", "(", "cuts", ",", "right_censor", "=", "True", ",", "censor_side", "=", "censor_side", ")", "\n", "self", ".", "di", "=", "Duration2Idx", "(", "cuts", ")", "\n", "self", ".", "label_cols", "=", "label_cols", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform": [[178, 182], ["discretization.IdxDiscUnknownC.duc.transform", "discretization.IdxDiscUnknownC.di.transform"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "transform", "(", "self", ",", "time", ",", "d", ")", ":", "\n", "        ", "time", ",", "d", "=", "self", ".", "duc", ".", "transform", "(", "time", ",", "d", ")", "\n", "idx", "=", "self", ".", "di", ".", "transform", "(", "time", ")", "\n", "return", "idx", ",", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform_df": [[183, 190], ["discretization.IdxDiscUnknownC.transform", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "transform_df", "(", "self", ",", "df", ")", ":", "\n", "        ", "if", "self", ".", "label_cols", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Need to set 'label_cols' to use this. Use 'transform instead'\"", ")", "\n", "", "col_duration", ",", "col_event", "=", "self", ".", "label_cols", "\n", "time", "=", "df", "[", "col_duration", "]", ".", "values", "\n", "d", "=", "df", "[", "col_event", "]", ".", "values", "\n", "return", "self", ".", "transform", "(", "time", ",", "d", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.make_cuts": [[7, 17], ["discretization.cuts_equidistant", "ValueError", "durations.max", "discretization.cuts_quantiles", "ValueError", "numpy.diff"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.cuts_equidistant", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.cuts_quantiles"], ["def", "make_cuts", "(", "n_cuts", ",", "scheme", ",", "durations", ",", "events", ",", "min_", "=", "0.", ",", "dtype", "=", "'float64'", ")", ":", "\n", "    ", "if", "scheme", "==", "'equidistant'", ":", "\n", "        ", "cuts", "=", "cuts_equidistant", "(", "durations", ".", "max", "(", ")", ",", "n_cuts", ",", "min_", ",", "dtype", ")", "\n", "", "elif", "scheme", "==", "'quantiles'", ":", "\n", "        ", "cuts", "=", "cuts_quantiles", "(", "durations", ",", "events", ",", "n_cuts", ",", "min_", ",", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Got invalid `scheme` {scheme}.\"", ")", "\n", "", "if", "(", "np", ".", "diff", "(", "cuts", ")", "==", "0", ")", ".", "any", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"cuts are not unique.\"", ")", "\n", "", "return", "cuts", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._values_if_series": [[18, 22], ["type"], "function", ["None"], ["", "def", "_values_if_series", "(", "x", ")", ":", "\n", "    ", "if", "type", "(", "x", ")", "is", "pd", ".", "Series", ":", "\n", "        ", "return", "x", ".", "values", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.cuts_equidistant": [[23, 25], ["numpy.linspace"], "function", ["None"], ["", "def", "cuts_equidistant", "(", "max_", ",", "num", ",", "min_", "=", "0.", ",", "dtype", "=", "'float64'", ")", ":", "\n", "    ", "return", "np", ".", "linspace", "(", "min_", ",", "max_", ",", "num", ",", "dtype", "=", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.cuts_quantiles": [[26, 41], ["pycox.utils.kaplan_meier", "numpy.linspace", "numpy.unique", "np.unique.astype", "utils.kaplan_meier.values.min", "utils.kaplan_meier.values.max", "numpy.searchsorted", "len", "warnings.warn", "durations.min", "durations.max", "len"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.kaplan_meier"], ["", "def", "cuts_quantiles", "(", "durations", ",", "events", ",", "num", ",", "min_", "=", "0.", ",", "dtype", "=", "'float64'", ")", ":", "\n", "    ", "\"\"\"\n    If min_ = None, we will use durations.min() for the first cut.\n    \"\"\"", "\n", "km", "=", "utils", ".", "kaplan_meier", "(", "durations", ",", "events", ")", "\n", "surv_est", ",", "surv_durations", "=", "km", ".", "values", ",", "km", ".", "index", ".", "values", "\n", "s_cuts", "=", "np", ".", "linspace", "(", "km", ".", "values", ".", "min", "(", ")", ",", "km", ".", "values", ".", "max", "(", ")", ",", "num", ")", "\n", "cuts_idx", "=", "np", ".", "searchsorted", "(", "surv_est", "[", ":", ":", "-", "1", "]", ",", "s_cuts", ")", "[", ":", ":", "-", "1", "]", "\n", "cuts", "=", "surv_durations", "[", ":", ":", "-", "1", "]", "[", "cuts_idx", "]", "\n", "cuts", "=", "np", ".", "unique", "(", "cuts", ")", "\n", "if", "len", "(", "cuts", ")", "!=", "num", ":", "\n", "        ", "warnings", ".", "warn", "(", "f\"cuts are not unique, continue with {len(cuts)} cuts instead of {num}\"", ")", "\n", "", "cuts", "[", "0", "]", "=", "durations", ".", "min", "(", ")", "if", "min_", "is", "None", "else", "min_", "\n", "assert", "cuts", "[", "-", "1", "]", "==", "durations", ".", "max", "(", ")", ",", "'something wrong...'", "\n", "return", "cuts", ".", "astype", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._is_monotonic_increasing": [[42, 45], ["len"], "function", ["None"], ["", "def", "_is_monotonic_increasing", "(", "x", ")", ":", "\n", "    ", "assert", "len", "(", "x", ".", "shape", ")", "==", "1", ",", "'Only works for 1d'", "\n", "return", "(", "x", "[", "1", ":", "]", ">=", "x", "[", ":", "-", "1", "]", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.bin_numerical": [[46, 62], ["discretization._is_monotonic_increasing", "numpy.searchsorted", "np.searchsorted.max", "ValueError"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._is_monotonic_increasing"], ["", "def", "bin_numerical", "(", "x", ",", "right_cuts", ",", "error_on_larger", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Discretize x into bins defined by right_cuts (needs to be sorted).\n    If right_cuts = [1, 2], we have bins (-inf, 1], (1, 2], (2, inf).\n    error_on_larger results in a ValueError if x contains larger\n    values than right_cuts.\n    \n    Returns index of bins.\n    To optaine values do righ_cuts[bin_numerica(x, right_cuts)].\n    \"\"\"", "\n", "assert", "_is_monotonic_increasing", "(", "right_cuts", ")", ",", "'Need `right_cuts` to be sorted.'", "\n", "bins", "=", "np", ".", "searchsorted", "(", "right_cuts", ",", "x", ",", "side", "=", "'left'", ")", "\n", "if", "bins", ".", "max", "(", ")", "==", "right_cuts", ".", "size", ":", "\n", "        ", "if", "error_on_larger", ":", "\n", "            ", "raise", "ValueError", "(", "'x contrains larger values than right_cuts.'", ")", "\n", "", "", "return", "bins", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.discretize": [[63, 91], ["discretization.bin_numerical", "bin_numerical.copy", "ValueError", "numpy.concatenate", "numpy.array"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.bin_numerical"], ["", "def", "discretize", "(", "x", ",", "cuts", ",", "side", "=", "'right'", ",", "error_on_larger", "=", "False", ")", ":", "\n", "    ", "\"\"\"Discretize x to cuts.\n    \n    Arguments:\n        x {np.array} -- Array of times.\n        cuts {np.array} -- Sortet array of discrete times.\n    \n    Keyword Arguments:\n        side {str} -- If we shold round down or up (left, right) (default: {'right'})\n        error_on_larger {bool} -- If we shold return an error if we pass higher values\n            than cuts (default: {False})\n    \n    Returns:\n        np.array -- Discretized values.\n    \"\"\"", "\n", "if", "side", "not", "in", "[", "'right'", ",", "'left'", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'side argument needs to be right or left.'", ")", "\n", "", "bins", "=", "bin_numerical", "(", "x", ",", "cuts", ",", "error_on_larger", ")", "\n", "if", "side", "==", "'right'", ":", "\n", "        ", "cuts", "=", "np", ".", "concatenate", "(", "(", "cuts", ",", "np", ".", "array", "(", "[", "np", ".", "inf", "]", ")", ")", ")", "\n", "return", "cuts", "[", "bins", "]", "\n", "", "bins_cut", "=", "bins", ".", "copy", "(", ")", "\n", "bins_cut", "[", "bins_cut", "==", "cuts", ".", "size", "]", "=", "-", "1", "\n", "exact", "=", "cuts", "[", "bins_cut", "]", "==", "x", "\n", "left_bins", "=", "bins", "-", "1", "+", "exact", "\n", "vals", "=", "cuts", "[", "left_bins", "]", "\n", "vals", "[", "left_bins", "==", "-", "1", "]", "=", "-", "np", ".", "inf", "\n", "return", "vals", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.duration_idx_map": [[135, 140], ["numpy.unique", "numpy.sort", "numpy.arange", "zip"], "function", ["None"], ["", "", "def", "duration_idx_map", "(", "duration", ")", ":", "\n", "    ", "duration", "=", "np", ".", "unique", "(", "duration", ")", "\n", "duration", "=", "np", ".", "sort", "(", "duration", ")", "\n", "idx", "=", "np", ".", "arange", "(", "duration", ".", "shape", "[", "0", "]", ")", "\n", "return", "{", "d", ":", "i", "for", "i", ",", "d", "in", "zip", "(", "idx", ",", "duration", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.base._SimBase.simulate": [[33, 47], ["None"], "methods", ["None"], ["    ", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "\"\"\"Simulate dataset of size `n`.\n        \n        Arguments:\n            n {int} -- Number of simulations\n        \n        Keyword Arguments:\n            surv_df {bool} -- If a dataframe containing the survival function should be returned.\n                (default: {False})\n        \n        Returns:\n            [dict] -- A dictionary with the results.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.base._SimBase.surv_df": [[48, 52], ["None"], "methods", ["None"], ["", "def", "surv_df", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Returns a data frame containing the survival function.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.base.dict2df": [[4, 30], ["pandas.DataFrame().assign", "numpy.concatenate", "df.assign.assign", "pandas.DataFrame", "data[].astype", "data[].astype", "data[].astype", "data[].astype", "data[].astype", "range"], "function", ["None"], ["def", "dict2df", "(", "data", ",", "add_true", "=", "True", ",", "add_censor_covs", "=", "False", ")", ":", "\n", "    ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n    Arguments:\n        data {dict} -- Dict from simulation.\n\n    Keyword Arguments:\n        add_true {bool} -- If we should include the true duration and censoring times\n            (default: {True})\n        add_censor_covs {bool} -- If we should include the censor covariates as covariates.\n            (default: {False})\n\n    Returns:\n        pd.DataFrame -- A DataFrame\n    \"\"\"", "\n", "covs", "=", "data", "[", "'covs'", "]", "\n", "if", "add_censor_covs", ":", "\n", "        ", "covs", "=", "np", ".", "concatenate", "(", "[", "covs", ",", "data", "[", "'censor_covs'", "]", "]", ",", "axis", "=", "1", ")", "\n", "", "df", "=", "(", "pd", ".", "DataFrame", "(", "covs", ",", "columns", "=", "[", "f\"x{i}\"", "for", "i", "in", "range", "(", "covs", ".", "shape", "[", "1", "]", ")", "]", ")", "\n", ".", "assign", "(", "duration", "=", "data", "[", "'durations'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "event", "=", "data", "[", "'events'", "]", ".", "astype", "(", "'float32'", ")", ")", ")", "\n", "if", "add_true", ":", "\n", "        ", "df", "=", "df", ".", "assign", "(", "duration_true", "=", "data", "[", "'durations_true'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "event_true", "=", "data", "[", "'events_true'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "censoring_true", "=", "data", "[", "'censor_durations'", "]", ".", "astype", "(", "'float32'", ")", ")", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.__init__": [[16, 21], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "h0", ",", "right_c", "=", "30.", ",", "c0", "=", "30.", ",", "surv_grid", "=", "None", ")", ":", "\n", "        ", "self", ".", "h0", "=", "h0", "\n", "self", ".", "right_c", "=", "right_c", "\n", "self", ".", "c0", "=", "c0", "\n", "self", ".", "surv_grid", "=", "surv_grid", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.simulate": [[22, 36], ["relative_risk._SimStudyRelativeRisk.sample_covs().astype", "numpy.random.exponential", "relative_risk._SimStudyRelativeRisk.inv_cum_hazard", "relative_risk._SimStudyRelativeRisk.copy", "dict", "numpy.random.exponential", "relative_risk._SimStudyRelativeRisk.surv_df", "relative_risk._SimStudyRelativeRisk.sample_covs", "numpy.ones_like", "numpy.ones_like"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.inv_cum_hazard", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_covs"], ["", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "covs", "=", "self", ".", "sample_covs", "(", "n", ")", ".", "astype", "(", "'float32'", ")", "\n", "v", "=", "np", ".", "random", ".", "exponential", "(", "size", "=", "n", ")", "\n", "t", "=", "self", ".", "inv_cum_hazard", "(", "v", ",", "covs", ")", "\n", "c", "=", "self", ".", "c0", "*", "np", ".", "random", ".", "exponential", "(", "size", "=", "n", ")", "\n", "tt", "=", "t", ".", "copy", "(", ")", "\n", "tt", "[", "c", "<", "t", "]", "=", "c", "[", "c", "<", "t", "]", "\n", "tt", "[", "tt", ">", "self", ".", "right_c", "]", "=", "self", ".", "right_c", "\n", "d", "=", "tt", "==", "t", "\n", "surv_df", "=", "self", ".", "surv_df", "(", "covs", ",", "self", ".", "surv_grid", ")", "if", "surv_df", "else", "None", "\n", "# censor_surv_df = NotImplemented if censor_df else None", "\n", "return", "dict", "(", "covs", "=", "covs", ",", "durations", "=", "tt", ",", "events", "=", "d", ",", "surv_df", "=", "surv_df", ",", "durations_true", "=", "t", ",", "\n", "events_true", "=", "np", ".", "ones_like", "(", "t", ")", ",", "censor_durations", "=", "c", ",", "\n", "censor_events", "=", "np", ".", "ones_like", "(", "c", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.sample_covs": [[37, 40], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "sample_covs", "(", "n", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.inv_cum_hazard": [[41, 44], ["None"], "methods", ["None"], ["", "def", "inv_cum_hazard", "(", "self", ",", "v", ",", "covs", ")", ":", "\n", "        ", "'''The inverse of the cumulative hazard.'''", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.cum_hazard": [[45, 48], ["None"], "methods", ["None"], ["", "def", "cum_hazard", "(", "self", ",", "t", ",", "covs", ")", ":", "\n", "        ", "'''The the cumulative hazard function.'''", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.survival_func": [[49, 52], ["numpy.exp", "relative_risk._SimStudyRelativeRisk.cum_hazard"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.cum_hazard"], ["", "def", "survival_func", "(", "self", ",", "t", ",", "covs", ")", ":", "\n", "        ", "'''Returns the survival function.'''", "\n", "return", "np", ".", "exp", "(", "-", "self", ".", "cum_hazard", "(", "t", ",", "covs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.survival_grid_single": [[53, 58], ["covs.reshape.reshape.reshape", "pandas.Series", "numpy.arange", "relative_risk._SimStudyRelativeRisk.survival_func"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.survival_func"], ["", "def", "survival_grid_single", "(", "self", ",", "covs", ",", "t", "=", "None", ")", ":", "\n", "        ", "covs", "=", "covs", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "if", "t", "is", "None", ":", "\n", "            ", "t", "=", "np", ".", "arange", "(", "0", ",", "31", ",", "0.5", ")", "\n", "", "return", "pd", ".", "Series", "(", "self", ".", "survival_func", "(", "t", ",", "covs", ")", ",", "index", "=", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.surv_df": [[59, 64], ["pandas.concat", "numpy.linspace", "relative_risk._SimStudyRelativeRisk.survival_grid_single"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.survival_grid_single"], ["", "def", "surv_df", "(", "self", ",", "covs", ",", "t", "=", "None", ")", ":", "\n", "        ", "if", "t", "is", "None", ":", "\n", "            ", "t", "=", "np", ".", "linspace", "(", "0", ",", "30", ",", "100", ")", "\n", "", "s", "=", "[", "self", ".", "survival_grid_single", "(", "xx", ",", "t", ")", "for", "xx", "in", "covs", "]", "\n", "return", "pd", ".", "concat", "(", "s", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk._SimStudyRelativeRisk.dict2df": [[65, 80], ["pycox.simulations.base.dict2df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["", "@", "staticmethod", "\n", "def", "dict2df", "(", "data", ",", "add_true", "=", "True", ")", ":", "\n", "        ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n        Arguments:\n            data {dict} -- Dict from simulation.\n\n        Keyword Arguments:\n            add_true {bool} -- If we should include the true duration and censoring times\n                (default: {True})\n\n        Returns:\n            pd.DataFrame -- A DataFrame\n        \"\"\"", "\n", "return", "base", ".", "dict2df", "(", "data", ",", "add_true", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyLinearPH.__init__": [[90, 92], ["relative_risk._SimStudyRelativeRisk.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "h0", "=", "0.1", ",", "right_c", "=", "30.", ",", "c0", "=", "30.", ",", "surv_grid", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "h0", ",", "right_c", ",", "c0", ",", "surv_grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyLinearPH.sample_covs": [[93, 96], ["numpy.random.uniform"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "sample_covs", "(", "n", ")", ":", "\n", "        ", "return", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "size", "=", "(", "n", ",", "3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyLinearPH.g": [[97, 102], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "g", "(", "covs", ")", ":", "\n", "        ", "x", "=", "covs", "\n", "x0", ",", "x1", ",", "x2", "=", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", ",", "x", "[", ":", ",", "2", "]", "\n", "return", "0.44", "*", "x0", "+", "0.66", "*", "x1", "+", "0.88", "*", "x2", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyLinearPH.inv_cum_hazard": [[103, 106], ["numpy.exp", "relative_risk.SimStudyLinearPH.g"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.g"], ["", "def", "inv_cum_hazard", "(", "self", ",", "v", ",", "covs", ")", ":", "\n", "        ", "'''The inverse of the cumulative hazard.'''", "\n", "return", "v", "/", "(", "self", ".", "h0", "*", "np", ".", "exp", "(", "self", ".", "g", "(", "covs", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyLinearPH.cum_hazard": [[107, 110], ["numpy.exp", "relative_risk.SimStudyLinearPH.g"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.g"], ["", "def", "cum_hazard", "(", "self", ",", "t", ",", "covs", ")", ":", "\n", "        ", "'''The the cumulative hazard function.'''", "\n", "return", "self", ".", "h0", "*", "t", "*", "np", ".", "exp", "(", "self", ".", "g", "(", "covs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearPH.g": [[120, 128], ["relative_risk.SimStudyLinearPH.g"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.g"], ["@", "staticmethod", "\n", "def", "g", "(", "covs", ")", ":", "\n", "        ", "x", "=", "covs", "\n", "x0", ",", "x1", ",", "x2", "=", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", ",", "x", "[", ":", ",", "2", "]", "\n", "beta", "=", "2", "/", "3", "\n", "linear", "=", "SimStudyLinearPH", ".", "g", "(", "x", ")", "\n", "nonlinear", "=", "beta", "*", "(", "x0", "**", "2", "+", "x2", "**", "2", "+", "x0", "*", "x1", "+", "x1", "*", "x2", "+", "x1", "*", "x2", ")", "\n", "return", "linear", "+", "nonlinear", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.__init__": [[144, 146], ["relative_risk.SimStudyLinearPH.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "h0", "=", "0.02", ",", "right_c", "=", "30.", ",", "c0", "=", "30.", ",", "surv_grid", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "h0", ",", "right_c", ",", "c0", ",", "surv_grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.a": [[147, 151], ["numpy.sign", "relative_risk.SimStudyNonLinearPH.g"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.g"], ["", "@", "staticmethod", "\n", "def", "a", "(", "x", ")", ":", "\n", "        ", "_", ",", "_", ",", "x2", "=", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", ",", "x", "[", ":", ",", "2", "]", "\n", "return", "np", ".", "sign", "(", "x2", ")", "+", "SimStudyNonLinearPH", ".", "g", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b": [[152, 156], ["numpy.abs"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "b", "(", "x", ")", ":", "\n", "        ", "x0", ",", "x1", ",", "_", "=", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", ",", "x", "[", ":", ",", "2", "]", "\n", "return", "np", ".", "abs", "(", "0.2", "*", "(", "x0", "+", "x1", ")", "+", "0.5", "*", "x0", "*", "x1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.g": [[157, 161], ["relative_risk.SimStudyNonLinearNonPH.a", "relative_risk.SimStudyNonLinearNonPH.b"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.a", "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b"], ["", "@", "staticmethod", "\n", "def", "g", "(", "t", ",", "covs", ")", ":", "\n", "        ", "x", "=", "covs", "\n", "return", "SimStudyNonLinearNonPH", ".", "a", "(", "x", ")", "+", "SimStudyNonLinearNonPH", ".", "b", "(", "x", ")", "*", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.inv_cum_hazard": [[162, 165], ["numpy.log", "relative_risk.SimStudyNonLinearNonPH.b", "numpy.exp", "relative_risk.SimStudyNonLinearNonPH.b", "relative_risk.SimStudyNonLinearNonPH.a"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b", "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b", "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.a"], ["", "def", "inv_cum_hazard", "(", "self", ",", "v", ",", "covs", ")", ":", "\n", "        ", "x", "=", "covs", "\n", "return", "1", "/", "self", ".", "b", "(", "x", ")", "*", "np", ".", "log", "(", "1", "+", "v", "*", "self", ".", "b", "(", "x", ")", "/", "self", ".", "h0", "*", "np", ".", "exp", "(", "-", "self", ".", "a", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.cum_hazard": [[166, 169], ["numpy.exp", "numpy.exp", "relative_risk.SimStudyNonLinearNonPH.b", "relative_risk.SimStudyNonLinearNonPH.a", "relative_risk.SimStudyNonLinearNonPH.b"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b", "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.a", "home.repos.pwc.inspect_result.havakv_pycox.simulations.relative_risk.SimStudyNonLinearNonPH.b"], ["", "def", "cum_hazard", "(", "self", ",", "t", ",", "covs", ")", ":", "\n", "        ", "x", "=", "covs", "\n", "return", "self", ".", "h0", "/", "self", ".", "b", "(", "x", ")", "*", "np", ".", "exp", "(", "self", ".", "a", "(", "x", ")", ")", "*", "(", "np", ".", "exp", "(", "self", ".", "b", "(", "x", ")", "*", "t", ")", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.__init__": [[19, 22], ["discrete_logit_hazard.SimBase.make_betas"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.make_betas"], ["def", "__init__", "(", "self", ",", "covs_per_weight", "=", "5", ",", "betas", "=", "None", ")", ":", "\n", "        ", "self", ".", "covs_per_weight", "=", "covs_per_weight", "\n", "self", ".", "betas", "=", "betas", "if", "betas", "else", "self", ".", "make_betas", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.make_betas": [[23, 25], ["tuple", "numpy.random.normal", "func", "range"], "methods", ["None"], ["", "def", "make_betas", "(", "self", ",", "func", "=", "lambda", "m", ":", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "m", ")", ")", ":", "\n", "        ", "return", "tuple", "(", "func", "(", "self", ".", "covs_per_weight", ")", "for", "_", "in", "range", "(", "self", ".", "num_weights", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase._sample_uniform": [[26, 29], ["numpy.random.uniform"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_sample_uniform", "(", "n", ")", ":", "\n", "        ", "return", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "(", "n", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.sample_weights": [[30, 32], ["discrete_logit_hazard.SimBase._sample_uniform", "range"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase._sample_uniform"], ["", "def", "sample_weights", "(", "self", ",", "n", ")", ":", "\n", "        ", "return", "[", "self", ".", "_sample_uniform", "(", "n", ")", "for", "_", "in", "range", "(", "self", ".", "num_weights", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.sample_covs": [[33, 36], ["discrete_logit_hazard.SimBase._conditional_covariate_sampling", "zip"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase._conditional_covariate_sampling"], ["", "def", "sample_covs", "(", "self", ",", "weights", ")", ":", "\n", "        ", "return", "[", "self", ".", "_conditional_covariate_sampling", "(", "beta", ",", "weight", ")", "\n", "for", "beta", ",", "weight", "in", "zip", "(", "self", ".", "betas", ",", "weights", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df": [[37, 43], ["discrete_logit_hazard.sigmoid", "numpy.ones", "haz2surv().transpose", "pandas.DataFrame", "len", "len", "len", "discrete_logit_hazard.haz2surv"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.sigmoid", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.haz2surv"], ["", "def", "surv_df", "(", "self", ",", "logit_haz", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "times", ")", "==", "(", "logit_haz", ".", "shape", "[", "1", "]", "+", "1", ")", ",", "'Need dims to be correct'", "\n", "haz", "=", "sigmoid", "(", "logit_haz", ")", "\n", "surv", "=", "np", ".", "ones", "(", "(", "len", "(", "self", ".", "times", ")", ",", "len", "(", "haz", ")", ")", ")", "\n", "surv", "[", "1", ":", ",", ":", "]", "=", "haz2surv", "(", "haz", ")", ".", "transpose", "(", ")", "\n", "return", "pd", ".", "DataFrame", "(", "surv", ",", "index", "=", "self", ".", "times", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase._conditional_covariate_sampling": [[44, 54], ["numpy.random.uniform", "numpy.empty_like", "beta.reshape", "weight.reshape", "len", "len", "numpy.diff", "x[].dot"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_conditional_covariate_sampling", "(", "beta", ",", "weight", ")", ":", "\n", "        ", "beta", ",", "weight", "=", "beta", ".", "reshape", "(", "-", "1", ")", ",", "weight", ".", "reshape", "(", "-", "1", ")", "\n", "size", "=", "len", "(", "weight", ")", ",", "len", "(", "beta", ")", "\n", "u", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "size", "=", "size", ")", "\n", "u", "[", ":", ",", "0", "]", "=", "weight", "\n", "x", "=", "np", ".", "empty_like", "(", "u", ")", "\n", "x", "[", ":", ",", ":", "-", "1", "]", "=", "-", "np", ".", "diff", "(", "u", ")", "/", "beta", "[", ":", "-", "1", "]", "\n", "x", "[", ":", ",", "-", "1", "]", "=", "(", "u", "[", ":", ",", "0", "]", "-", "x", "[", ":", ",", ":", "-", "1", "]", ".", "dot", "(", "beta", "[", ":", "-", "1", "]", ")", ")", "/", "beta", "[", "-", "1", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.sample_event_times": [[55, 65], ["discrete_logit_hazard.sigmoid", "numpy.random.uniform", "numpy.zeros", "numpy.zeros.argmax", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.sigmoid"], ["", "def", "sample_event_times", "(", "self", ",", "logit_haz", ")", ":", "\n", "        ", "haz", "=", "sigmoid", "(", "logit_haz", ")", "\n", "assert", "haz", ".", "shape", "[", "1", "]", "==", "len", "(", "self", ".", "times", ")", "-", "1", ",", "'Fix dims'", "\n", "samp", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "haz", ".", "shape", ")", "\n", "hit", "=", "np", ".", "zeros", "(", "(", "len", "(", "haz", ")", ",", "len", "(", "self", ".", "times", ")", ")", ",", "'bool'", ")", "\n", "hit", "[", ":", ",", "1", ":", "]", "=", "samp", "<", "haz", "\n", "idx_first", "=", "hit", ".", "argmax", "(", "1", ")", "\n", "durations", "=", "self", ".", "times", "[", "idx_first", "]", "# -1 because hit has one additional column", "\n", "durations", "[", "idx_first", "==", "False", "]", "=", "np", ".", "nan", "\n", "return", "durations", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.simulate": [[66, 69], ["discrete_logit_hazard.SimBase.sample_weights", "discrete_logit_hazard.SimBase.simulate_from_weights"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.simulate_from_weights"], ["", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "weights", "=", "self", ".", "sample_weights", "(", "n", ")", "\n", "return", "self", ".", "simulate_from_weights", "(", "weights", ",", "surv_df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.simulate_from_weights": [[70, 83], ["discrete_logit_hazard.SimBase.logit_haz", "discrete_logit_hazard.SimBase.sample_event_times", "numpy.isnan", "numpy.ones_like", "discrete_logit_hazard.SimBase.sample_covs", "torchtuples.tuplefy().flatten", "numpy.concatenate", "dict", "discrete_logit_hazard.SimBase.surv_df", "torchtuples.tuplefy"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.sample_event_times", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_covs", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df"], ["", "def", "simulate_from_weights", "(", "self", ",", "weights", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "logit_haz", "=", "self", ".", "logit_haz", "(", "self", ".", "times", "[", "1", ":", "]", ",", "*", "weights", ")", "\n", "durations", "=", "self", ".", "sample_event_times", "(", "logit_haz", ")", "#.astype('float32')", "\n", "is_nan", "=", "np", ".", "isnan", "(", "durations", ")", "\n", "events", "=", "np", ".", "ones_like", "(", "durations", ")", "\n", "events", "[", "is_nan", "]", "=", "0.", "\n", "durations", "[", "is_nan", "]", "=", "self", ".", "times", "[", "-", "1", "]", "\n", "covs", "=", "self", ".", "sample_covs", "(", "weights", ")", "\n", "covs", "=", "tt", ".", "tuplefy", "(", "covs", ")", ".", "flatten", "(", ")", "\n", "covs", "=", "np", ".", "concatenate", "(", "covs", ",", "axis", "=", "1", ")", "#.astype('float32')", "\n", "surv", "=", "self", ".", "surv_df", "(", "logit_haz", ")", "if", "surv_df", "is", "True", "else", "None", "\n", "return", "dict", "(", "covs", "=", "covs", ",", "durations", "=", "durations", ",", "events", "=", "events", ",", "weights", "=", "weights", ",", "\n", "surv_df", "=", "surv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.covs2weights": [[85, 87], ["cov.dot().reshape", "zip", "cov.dot"], "methods", ["None"], ["", "def", "covs2weights", "(", "self", ",", "covs", ")", ":", "\n", "        ", "return", "[", "cov", ".", "dot", "(", "beta", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "for", "cov", ",", "beta", "in", "zip", "(", "covs", ",", "self", ".", "betas", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.covs2surv_df": [[88, 92], ["discrete_logit_hazard.SimBase.covs2weights", "discrete_logit_hazard.SimBase.logit_haz", "discrete_logit_hazard.SimBase.surv_df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df"], ["", "def", "covs2surv_df", "(", "self", ",", "covs", ")", ":", "\n", "        ", "weights", "=", "self", ".", "covs2weights", "(", "covs", ")", "\n", "logit_haz", "=", "self", ".", "logit_haz", "(", "self", ".", "times", "[", "1", ":", "]", ",", "*", "weights", ")", "\n", "return", "self", ".", "surv_df", "(", "logit_haz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.logit_haz": [[93, 95], ["None"], "methods", ["None"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.dict2df": [[96, 107], ["pycox.simulations.base.dict2df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["", "@", "staticmethod", "\n", "def", "dict2df", "(", "data", ")", ":", "\n", "        ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n        Arguments:\n            data {dict} -- Dict from simulation.\n\n        Returns:\n            pd.DataFrame -- A DataFrame\n        \"\"\"", "\n", "return", "base", ".", "dict2df", "(", "data", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimSin.logit_haz": [[111, 121], ["discrete_logit_hazard.SimSin._logit_haz", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimAcceleratingHaz._logit_haz"], ["def", "logit_haz", "(", "self", ",", "times", ",", "a", ",", "bb", ",", "c", ",", "dd", ")", ":", "\n", "        ", "\"\"\"We expect a, bb, c, dd to be Unif[-1, 1] and transform them to\n        the desired ranges. Use '_logit_haz' to skip this transform.\n        \"\"\"", "\n", "a", "=", "a", "*", "5", "# Unif[-5, 5]", "\n", "idx", "=", "(", "(", "bb", "+", "1", ")", "/", "2", "*", "5", ")", ".", "astype", "(", "'int'", ")", "\n", "bb", "=", "np", ".", "arange", "(", "-", "1", ",", "4", ")", "[", "idx", "]", "# Unif[{-1, 0, 1, 2, 3}]", "\n", "c", "=", "c", "*", "15", "# Unif[-15, 15]", "\n", "dd", "=", "dd", "*", "2", "# Unif[-2, 2]", "\n", "return", "self", ".", "_logit_haz", "(", "times", ",", "a", ",", "bb", ",", "c", ",", "dd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimSin._logit_haz": [[122, 127], ["numpy.power", "abs", "numpy.sin"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_logit_haz", "(", "times", ",", "a", ",", "bb", ",", "c", ",", "dd", ")", ":", "\n", "        ", "b", "=", "2", "*", "np", ".", "pi", "/", "100", "*", "np", ".", "power", "(", "2.", ",", "bb", ")", "\n", "d", "=", "dd", "-", "6", "-", "abs", "(", "a", "/", "2", ")", "\n", "return", "a", "*", "np", ".", "sin", "(", "b", "*", "(", "times", "+", "c", ")", ")", "+", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHaz.logit_haz": [[131, 135], ["discrete_logit_hazard.SimConstHaz._logit_haz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimAcceleratingHaz._logit_haz"], ["def", "logit_haz", "(", "self", ",", "times", ",", "a", ")", ":", "\n", "        ", "\"\"\"Expect a to be Unit[-1, 1].\"\"\"", "\n", "a", "=", "(", "a", "+", "1", ")", "/", "2", "*", "5", "-", "8", "# Unif[-8, -3]", "\n", "return", "self", ".", "_logit_haz", "(", "times", ",", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHaz._logit_haz": [[136, 139], ["numpy.ones", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_logit_haz", "(", "times", ",", "a", ")", ":", "\n", "        ", "return", "a", "*", "np", ".", "ones", "(", "(", "len", "(", "a", ")", ",", "len", "(", "times", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimAcceleratingHaz.logit_haz": [[143, 148], ["discrete_logit_hazard.sigmoid", "discrete_logit_hazard.SimAcceleratingHaz._logit_haz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.sigmoid", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimAcceleratingHaz._logit_haz"], ["def", "logit_haz", "(", "self", ",", "times", ",", "aa", ")", ":", "\n", "        ", "\"\"\"Expect a to be Unit[-1, 1].\"\"\"", "\n", "aa", "=", "(", "aa", "+", "1", ")", "/", "2", "*", "6", "-", "5", "# Unif[-5, 1]", "\n", "a", "=", "sigmoid", "(", "aa", ")", "\n", "return", "self", ".", "_logit_haz", "(", "times", ",", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimAcceleratingHaz._logit_haz": [[149, 153], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_logit_haz", "(", "times", ",", "a", ")", ":", "\n", "        ", "start", "=", "-", "10", "\n", "return", "a", "*", "times", "+", "start", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHazIndependentOfWeights.__init__": [[162, 166], ["numpy.array", "discrete_logit_hazard.SimBase.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "covs_per_weight", "=", "1", "\n", "betas", "=", "np", ".", "array", "(", "[", "0.", "]", ")", "\n", "super", "(", ")", ".", "__init__", "(", "covs_per_weight", ",", "betas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHazIndependentOfWeights.sample_weights": [[167, 169], ["numpy.zeros"], "methods", ["None"], ["", "def", "sample_weights", "(", "self", ",", "n", ")", ":", "\n", "        ", "return", "[", "np", ".", "zeros", "(", "(", "n", ",", "1", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHazIndependentOfWeights.sample_covs": [[170, 172], ["None"], "methods", ["None"], ["", "def", "sample_covs", "(", "self", ",", "weights", ")", ":", "\n", "        ", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHazIndependentOfWeights.covs2weights": [[173, 175], ["None"], "methods", ["None"], ["", "def", "covs2weights", "(", "self", ",", "covs", ")", ":", "\n", "        ", "return", "covs", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstHazIndependentOfWeights.logit_haz": [[176, 178], ["numpy.ones", "len", "len"], "methods", ["None"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "a", ")", ":", "\n", "        ", "return", "-", "7.", "*", "np", ".", "ones", "(", "(", "len", "(", "a", ")", ",", "len", "(", "times", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniform.__init__": [[182, 187], ["numpy.array", "discrete_logit_hazard.SimBase.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "s_end", "=", "0.2", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "s_end", "=", "s_end", "\n", "covs_per_weight", "=", "1", "\n", "betas", "=", "np", ".", "array", "(", "[", "0.", "]", ")", "\n", "super", "(", ")", ".", "__init__", "(", "covs_per_weight", ",", "betas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniform.logit_haz": [[188, 192], ["numpy.arange().reshape().repeat", "len", "len", "numpy.log", "numpy.arange().reshape", "numpy.arange"], "methods", ["None"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "w", ")", ":", "\n", "        ", "n", ",", "m", "=", "len", "(", "w", ")", ",", "len", "(", "times", ")", "\n", "j", "=", "np", ".", "arange", "(", "1", ",", "m", "+", "1", ",", "dtype", "=", "'float'", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "repeat", "(", "n", ",", "axis", "=", "0", ")", "\n", "return", "-", "np", ".", "log", "(", "m", "/", "(", "1", "-", "self", ".", "s_end", ")", "-", "j", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniform.sample_weights": [[193, 195], ["numpy.zeros"], "methods", ["None"], ["", "def", "sample_weights", "(", "self", ",", "n", ")", ":", "\n", "        ", "return", "[", "np", ".", "zeros", "(", "(", "n", ",", "1", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniform.sample_covs": [[196, 198], ["None"], "methods", ["None"], ["", "def", "sample_covs", "(", "self", ",", "weights", ")", ":", "\n", "        ", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniform.covs2weights": [[199, 201], ["None"], "methods", ["None"], ["", "def", "covs2weights", "(", "self", ",", "covs", ")", ":", "\n", "        ", "return", "covs", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniformAdmin.__init__": [[207, 210], ["discrete_logit_hazard.SimBase.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "covs_per_weight", "=", "5", ",", "s_end", "=", "0.2", ",", "seed", "=", "None", ",", "betas", "=", "None", ")", ":", "\n", "        ", "self", ".", "s_end", "=", "s_end", "\n", "super", "(", ")", ".", "__init__", "(", "covs_per_weight", ",", "betas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimUniformAdmin.logit_haz": [[211, 225], ["len", "w.flatten", "numpy.floor().clip", "idx.astype.astype.astype", "numpy.zeros", "lh.cumsum.cumsum.cumsum", "numpy.floor", "len", "numpy.arange", "len"], "methods", ["None"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "w", ")", ":", "\n", "        ", "\"\"\"w is Unif[-1, 1]\"\"\"", "\n", "assert", "(", "self", ".", "num_weights", "==", "1", ")", "and", "(", "w", ".", "shape", "[", "1", "]", "==", "1", ")", ",", "\"We don't allow more than 1 weight here\"", "\n", "m", "=", "len", "(", "times", ")", "\n", "idx", "=", "w", ".", "flatten", "(", ")", "\n", "idx", "=", "(", "idx", "+", "1", ")", "/", "2", "/", "(", "1", "-", "self", ".", "s_end", ")", "*", "(", "m", "+", "1", ")", "\n", "idx", "=", "np", ".", "floor", "(", "idx", ")", ".", "clip", "(", "0", ",", "m", ")", "\n", "idx", "=", "idx", ".", "astype", "(", "'int'", ")", "\n", "lh", "=", "np", ".", "zeros", "(", "(", "len", "(", "idx", ")", ",", "m", "+", "1", ")", ")", "\n", "lh", "[", "np", ".", "arange", "(", "len", "(", "idx", ")", ")", ",", "idx", "]", "=", "1", "\n", "lh", "=", "lh", ".", "cumsum", "(", "1", ")", "\n", "lh", "[", "lh", "==", "0", "]", "=", "-", "np", ".", "inf", "\n", "lh", "[", "lh", "==", "1", "]", "=", "np", ".", "inf", "\n", "return", "lh", "[", ":", ",", ":", "m", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_weights": [[232, 235], ["sim.sample_weights", "discrete_logit_hazard.SimBase.sample_weights"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_weights"], ["def", "sample_weights", "(", "self", ",", "n", ")", ":", "\n", "        ", "weights", "=", "[", "sim", ".", "sample_weights", "(", "n", ")", "for", "sim", "in", "self", ".", "sims", "]", "\n", "return", "[", "super", "(", ")", ".", "sample_weights", "(", "n", ")", "]", "+", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_covs": [[236, 240], ["sim.sample_covs", "zip", "discrete_logit_hazard.SimBase.sample_covs"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_covs", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.sample_covs"], ["", "def", "sample_covs", "(", "self", ",", "weights", ")", ":", "\n", "        ", "alpha", "=", "weights", "[", "0", "]", "\n", "covs", "=", "[", "sim", ".", "sample_covs", "(", "w", ")", "for", "sim", ",", "w", "in", "zip", "(", "self", ".", "sims", ",", "weights", "[", "1", ":", "]", ")", "]", "\n", "return", "[", "super", "(", ")", ".", "sample_covs", "(", "alpha", ")", "]", "+", "covs", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.logit_haz": [[241, 249], ["numpy.concatenate", "discrete_logit_hazard.softmax", "enumerate", "zip", "sim.logit_haz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "*", "weights", ")", ":", "\n", "        ", "alpha", "=", "np", ".", "concatenate", "(", "weights", "[", "0", "]", ",", "axis", "=", "1", ")", "\n", "alpha", "[", ":", ",", "0", "]", "+=", "self", ".", "_first_pref", "\n", "alpha", "=", "softmax", "(", "alpha", "*", "self", ".", "alpha_range", ")", "\n", "logit_haz", "=", "0.", "\n", "for", "i", ",", "(", "sim", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "sims", ",", "weights", "[", "1", ":", "]", ")", ")", ":", "\n", "            ", "logit_haz", "+=", "sim", ".", "logit_haz", "(", "self", ".", "times", "[", "1", ":", "]", ",", "*", "w", ")", "*", "alpha", "[", ":", ",", "[", "i", "]", "]", "\n", "", "return", "logit_haz", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2weights": [[250, 253], ["sim.covs2weights", "zip", "discrete_logit_hazard.SimBase.covs2weights"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2weights"], ["", "def", "covs2weights", "(", "self", ",", "covs", ")", ":", "\n", "        ", "weights", "=", "[", "sim", ".", "covs2weights", "(", "cov", ")", "for", "sim", ",", "cov", "in", "zip", "(", "self", ".", "sims", ",", "covs", "[", "1", ":", "]", ")", "]", "\n", "return", "[", "super", "(", ")", ".", "covs2weights", "(", "covs", "[", "0", "]", ")", "]", "+", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2surv_df": [[254, 258], ["discrete_logit_hazard._SimCombine.covs2weights", "discrete_logit_hazard._SimCombine.logit_haz", "discrete_logit_hazard._SimCombine.surv_df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimCombine.covs2weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df"], ["", "def", "covs2surv_df", "(", "self", ",", "covs", ")", ":", "\n", "        ", "weights", "=", "self", ".", "covs2weights", "(", "covs", ")", "\n", "logit_haz", "=", "self", ".", "logit_haz", "(", "self", ".", "times", "[", "1", ":", "]", ",", "*", "weights", ")", "\n", "return", "self", ".", "surv_df", "(", "logit_haz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimSinAccConst.__init__": [[261, 270], ["discrete_logit_hazard.SimBase.__init__", "discrete_logit_hazard.SimSin", "discrete_logit_hazard.SimConstHaz", "discrete_logit_hazard.SimAcceleratingHaz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "covs_per_weight", "=", "5", ",", "alpha_range", "=", "5.", ",", "sin_pref", "=", "0.6", ")", ":", "\n", "        ", "self", ".", "num_weights", "=", "3", "\n", "super", "(", ")", ".", "__init__", "(", "covs_per_weight", ")", "\n", "self", ".", "alpha_range", "=", "alpha_range", "\n", "self", ".", "_first_pref", "=", "sin_pref", "\n", "self", ".", "sim_sin", "=", "SimSin", "(", "covs_per_weight", ")", "\n", "self", ".", "sim_const", "=", "SimConstHaz", "(", "covs_per_weight", ")", "\n", "self", ".", "sim_acc", "=", "SimAcceleratingHaz", "(", "covs_per_weight", ")", "\n", "self", ".", "sims", "=", "[", "self", ".", "sim_sin", ",", "self", ".", "sim_const", ",", "self", ".", "sim_acc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimConstAcc.__init__": [[273, 281], ["discrete_logit_hazard.SimBase.__init__", "discrete_logit_hazard.SimConstHaz", "discrete_logit_hazard.SimAcceleratingHaz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "covs_per_weight", "=", "5", ",", "alpha_range", "=", "5.", ",", "const_pref", "=", "2", ")", ":", "\n", "        ", "self", ".", "num_weights", "=", "2", "\n", "super", "(", ")", ".", "__init__", "(", "covs_per_weight", ")", "\n", "self", ".", "alpha_range", "=", "alpha_range", "\n", "self", ".", "_first_pref", "=", "const_pref", "\n", "self", ".", "sim_const", "=", "SimConstHaz", "(", "covs_per_weight", ")", "\n", "self", ".", "sim_acc", "=", "SimAcceleratingHaz", "(", "covs_per_weight", ")", "\n", "self", ".", "sims", "=", "[", "self", ".", "sim_const", ",", "self", ".", "sim_acc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.__init__": [[287, 292], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sim", ",", "threshold", ")", ":", "\n", "        ", "self", ".", "sim", "=", "sim", "\n", "assert", "(", "threshold", ">", "0", ")", "and", "(", "threshold", "<", "1", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "times", "=", "self", ".", "sim", ".", "times", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.simulate": [[293, 297], ["discrete_logit_hazard.SimThresholdWrap.sim.simulate", "discrete_logit_hazard.SimThresholdWrap.threshold_res"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.threshold_res"], ["", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "res", "=", "self", ".", "sim", ".", "simulate", "(", "n", ",", "surv_df", "=", "True", ")", "\n", "res", "=", "self", ".", "threshold_res", "(", "res", ",", "surv_df", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.threshold_res": [[298, 311], ["res.copy.copy.copy", "numpy.ones_like", "discrete_logit_hazard.SimThresholdWrap.sim.times.max", "numpy.argmax", "discrete_logit_hazard.SimThresholdWrap._get_surv"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap._get_surv"], ["", "def", "threshold_res", "(", "self", ",", "res", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "res", "=", "res", ".", "copy", "(", ")", "\n", "surv", "=", "res", "[", "'surv_df'", "]", "\n", "idx", "=", "np", ".", "argmax", "(", "(", "surv", "<", "self", ".", "threshold", ")", ".", "values", ",", "axis", "=", "0", ")", "-", "1", "\n", "durations", "=", "surv", ".", "index", ".", "values", "[", "idx", "]", "\n", "events", "=", "np", ".", "ones_like", "(", "durations", ")", "\n", "events", "[", "idx", "==", "0", "]", "=", "0", "\n", "durations", "[", "idx", "==", "0", "]", "=", "self", ".", "sim", ".", "times", ".", "max", "(", ")", "\n", "res", "[", "'durations'", "]", "=", "durations", "\n", "res", "[", "'events'", "]", "=", "events", "\n", "if", "surv_df", ":", "\n", "            ", "res", "[", "'surv_df'", "]", "=", "self", ".", "_get_surv", "(", "surv", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap._get_surv": [[312, 314], ["None"], "methods", ["None"], ["", "def", "_get_surv", "(", "self", ",", "sub_surv", ")", ":", "\n", "        ", "return", "(", "sub_surv", ">=", "self", ".", "threshold", ")", ".", "astype", "(", "sub_surv", ".", "values", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz": [[315, 322], ["discrete_logit_hazard.SimThresholdWrap.sim.logit_haz", "discrete_logit_hazard.SimThresholdWrap.sim.surv_df", "discrete_logit_hazard.SimThresholdWrap._get_surv", "discrete_logit_hazard.SimThresholdWrap.values[].transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.logit_haz", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimBase.surv_df", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap._get_surv"], ["", "def", "logit_haz", "(", "self", ",", "times", ",", "*", "weights", ")", ":", "\n", "        ", "logit_haz", "=", "self", ".", "sim", ".", "logit_haz", "(", "times", ",", "*", "weights", ")", "\n", "sub_surv", "=", "self", ".", "sim", ".", "surv_df", "(", "logit_haz", ")", "\n", "surv", "=", "self", ".", "_get_surv", "(", "sub_surv", ")", "\n", "surv", "[", "surv", "==", "1", "]", "=", "-", "np", ".", "inf", "\n", "surv", "[", "surv", "==", "0", "]", "=", "np", ".", "inf", "\n", "return", "surv", ".", "values", "[", "1", ":", ",", ":", "]", ".", "transpose", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.simulate_from_weights": [[323, 327], ["discrete_logit_hazard.SimThresholdWrap.sim.simulate_from_weights", "discrete_logit_hazard.SimThresholdWrap.threshold_res"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.simulate_from_weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.threshold_res"], ["", "def", "simulate_from_weights", "(", "self", ",", "weights", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "res", "=", "self", ".", "sim", ".", "simulate_from_weights", "(", "weights", ",", "True", ")", "\n", "res", "=", "self", ".", "threshold_res", "(", "res", ",", "surv_df", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimStudyBase._combine_surv_and_censor": [[332, 339], ["dict", "surv[].copy", "surv[].copy", "str", "censor.items"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "_combine_surv_and_censor", "(", "surv", ",", "censor", ")", ":", "\n", "        ", "surv", "[", "'durations_true'", "]", ",", "surv", "[", "'events_true'", "]", "=", "surv", "[", "'durations'", "]", ".", "copy", "(", ")", ",", "surv", "[", "'events'", "]", ".", "copy", "(", ")", "\n", "is_censor", "=", "censor", "[", "'durations'", "]", "<", "surv", "[", "'durations'", "]", "\n", "surv", "[", "'durations'", "]", "[", "is_censor", "]", "=", "censor", "[", "'durations'", "]", "[", "is_censor", "]", "\n", "surv", "[", "'events'", "]", "[", "is_censor", "]", "=", "0.", "\n", "return", "dict", "(", "**", "surv", ",", "**", "{", "'censor_'", "+", "str", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "censor", ".", "items", "(", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimStudyBase.simulate": [[340, 350], ["discrete_logit_hazard._SimStudyBase.sim_surv.simulate", "discrete_logit_hazard._SimStudyBase.sim_censor.simulate", "discrete_logit_hazard._SimStudyBase._combine_surv_and_censor", "discrete_logit_hazard._SimStudyBase.binary_surv", "ValueError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimStudyBase._combine_surv_and_censor"], ["", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ",", "censor_df", "=", "False", ",", "binary_surv", "=", "False", ")", ":", "\n", "        ", "if", "binary_surv", ":", "\n", "            ", "if", "not", "(", "surv_df", "and", "censor_df", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"To produce binary_surv, you need to also set surv_df and censor_df to True\"", ")", "\n", "", "", "surv", "=", "self", ".", "sim_surv", ".", "simulate", "(", "n", ",", "surv_df", ")", "\n", "censor", "=", "self", ".", "sim_censor", ".", "simulate", "(", "n", ",", "censor_df", ")", "\n", "res", "=", "self", ".", "_combine_surv_and_censor", "(", "surv", ",", "censor", ")", "\n", "if", "binary_surv", ":", "\n", "            ", "res", "[", "'binary_surv_df'", "]", "=", "self", ".", "binary_surv", "(", "res", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimStudyBase.dict2df": [[351, 368], ["pycox.simulations.base.dict2df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["", "@", "staticmethod", "\n", "def", "dict2df", "(", "data", ",", "add_true", "=", "True", ",", "add_censor_covs", "=", "False", ")", ":", "\n", "        ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n        Arguments:\n            data {dict} -- Dict from simulation.\n\n        Keyword Arguments:\n            add_true {bool} -- If we should include the true duration and censoring times\n                (default: {True})\n            add_censor_covs {bool} -- If we should include the censor covariates as covariates.\n                (default: {False})\n\n        Returns:\n            pd.DataFrame -- A DataFrame\n        \"\"\"", "\n", "return", "base", ".", "dict2df", "(", "data", ",", "add_true", ",", "add_censor_covs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudyIndepSurvAndCens.__init__": [[371, 374], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sim_surv", ",", "sim_censor", ")", ":", "\n", "        ", "self", ".", "sim_surv", "=", "sim_surv", "\n", "self", ".", "sim_censor", "=", "sim_censor", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.__init__": [[387, 392], ["discrete_logit_hazard.SimStudyIndepSurvAndCens.__init__", "sim_surv.simulate"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate"], ["def", "__init__", "(", "self", ",", "sim_surv", ",", "sim_censor", ",", "sim0", "=", "None", ")", ":", "\n", "        ", "if", "sim0", "is", "None", ":", "\n", "            ", "sim0", "=", "sim_surv", ".", "simulate", "(", "1", ")", "\n", "", "self", ".", "sim0", "=", "sim0", "\n", "super", "(", ")", ".", "__init__", "(", "sim_surv", ",", "sim_censor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate": [[393, 405], ["discrete_logit_hazard.SimStudySingleSurv.sim_surv.simulate_from_weights", "discrete_logit_hazard.SimStudySingleSurv.sim_censor.simulate", "discrete_logit_hazard.SimStudySingleSurv._combine_surv_and_censor", "[].repeat", "discrete_logit_hazard.SimStudySingleSurv.binary_surv", "ValueError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimThresholdWrap.simulate_from_weights", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard._SimStudyBase._combine_surv_and_censor"], ["", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ",", "censor_df", "=", "False", ",", "binary_surv", "=", "False", ")", ":", "\n", "        ", "if", "binary_surv", ":", "\n", "            ", "if", "not", "(", "surv_df", "and", "censor_df", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"To produce binary_surv, you need to also set surv_df and censor_df to True\"", ")", "\n", "", "", "surv", "=", "self", ".", "sim0", "\n", "weights", "=", "[", "surv", "[", "'weights'", "]", "[", "0", "]", ".", "repeat", "(", "n", ",", "0", ")", "]", "\n", "surv", "=", "self", ".", "sim_surv", ".", "simulate_from_weights", "(", "weights", ",", "surv_df", ")", "\n", "censor", "=", "self", ".", "sim_censor", ".", "simulate", "(", "n", ",", "censor_df", ")", "\n", "res", "=", "self", ".", "_combine_surv_and_censor", "(", "surv", ",", "censor", ")", "\n", "if", "binary_surv", ":", "\n", "            ", "res", "[", "'binary_surv_df'", "]", "=", "self", ".", "binary_surv", "(", "res", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.dict2df": [[406, 423], ["pycox.simulations.base.dict2df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["", "@", "staticmethod", "\n", "def", "dict2df", "(", "data", ",", "add_true", "=", "True", ",", "add_censor_covs", "=", "True", ")", ":", "\n", "        ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n        Arguments:\n            data {dict} -- Dict from simulation.\n\n        Keyword Arguments:\n            add_true {bool} -- If we should include the true duration and censoring times\n                (default: {True})\n            add_censor_covs {bool} -- If we should include the censor covariates as covariates.\n                (default: {True})\n\n        Returns:\n            pd.DataFrame -- A DataFrame\n        \"\"\"", "\n", "return", "base", ".", "dict2df", "(", "data", ",", "add_true", ",", "add_censor_covs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACCensorConst.__init__": [[445, 448], ["discrete_logit_hazard.SimSinAccConst", "discrete_logit_hazard.SimConstHazIndependentOfWeights"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "covs_per_weight", "=", "5", ",", "alpha_range", "=", "5.", ",", "sin_pref", "=", "0.6", ")", ":", "\n", "        ", "self", ".", "sim_surv", "=", "SimSinAccConst", "(", "covs_per_weight", ",", "alpha_range", ",", "sin_pref", ")", "\n", "self", ".", "sim_censor", "=", "SimConstHazIndependentOfWeights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.__init__": [[468, 475], ["discrete_logit_hazard.SimSinAccConst", "discrete_logit_hazard.SimThresholdWrap", "discrete_logit_hazard.SimConstHaz", "discrete_logit_hazard.SimSinAccConst"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "simple_censor", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "self", ".", "sim_surv", "=", "SimSinAccConst", "(", "2", ")", "\n", "if", "simple_censor", "is", "True", ":", "\n", "            ", "sim_censor", "=", "SimConstHaz", "(", "5", ")", "\n", "", "else", ":", "\n", "            ", "sim_censor", "=", "SimSinAccConst", "(", "2", ")", "\n", "", "self", ".", "sim_censor", "=", "SimThresholdWrap", "(", "sim_censor", ",", "0.2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df": [[476, 493], ["pycox.simulations.base.dict2df"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["", "@", "staticmethod", "\n", "def", "dict2df", "(", "data", ",", "add_true", "=", "True", ",", "add_censor_covs", "=", "True", ")", ":", "\n", "        ", "\"\"\"Make a pd.DataFrame from the dict obtained when simulating.\n\n        Arguments:\n            data {dict} -- Dict from simulation.\n\n        Keyword Arguments:\n            add_true {bool} -- If we should include the true duration and censoring times\n                (default: {True})\n            add_censor_covs {bool} -- If we should include the censor covariates as covariates.\n                (default: {True})\n\n        Returns:\n            pd.DataFrame -- A DataFrame\n        \"\"\"", "\n", "return", "base", ".", "dict2df", "(", "data", ",", "add_true", ",", "add_censor_covs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurvUniformAdmin.__init__": [[509, 513], ["discrete_logit_hazard.SimConstHaz", "discrete_logit_hazard.SimUniformAdmin", "discrete_logit_hazard.SimStudySingleSurv.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "sim_surv", "=", "SimConstHaz", "(", "1", ")", "\n", "sim_censor", "=", "SimUniformAdmin", "(", "1", ",", "0.2", ")", "\n", "super", "(", ")", ".", "__init__", "(", "sim_surv", ",", "sim_censor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.sigmoid": [[515, 517], ["numpy.exp"], "function", ["None"], ["", "", "def", "sigmoid", "(", "x", ")", ":", "\n", "    ", "return", "1", "/", "(", "1", "+", "np", ".", "exp", "(", "-", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.haz2surv": [[518, 520], ["numpy.exp", "numpy.log().cumsum", "numpy.log"], "function", ["None"], ["", "def", "haz2surv", "(", "haz", ",", "eps", "=", "1e-7", ")", ":", "\n", "    ", "return", "np", ".", "exp", "(", "np", ".", "log", "(", "(", "1", "-", "haz", ")", "+", "eps", ")", ".", "cumsum", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax": [[521, 524], ["numpy.exp", "np.exp.sum"], "function", ["None"], ["", "def", "softmax", "(", "x", ")", ":", "\n", "    ", "exp", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "exp", "/", "exp", ".", "sum", "(", "1", ",", "keepdims", "=", "True", ")", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_deepsurv._DatasetDeepSurv._download": [[17, 36], ["collections.defaultdict", "path.unlink", "from_deepsurv._make_df", "from_deepsurv._make_df", "pandas.concat().reset_index", "pandas.concat().reset_index.to_feather", "requests.Session", "s.get", "h5py.File", "open", "f.write", "pandas.concat"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets.from_deepsurv._make_df", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_deepsurv._make_df"], ["def", "_download", "(", "self", ")", ":", "\n", "        ", "url", "=", "self", ".", "_dataset_url", "+", "self", ".", "_datasets", "[", "self", ".", "name", "]", "\n", "path", "=", "self", ".", "path", ".", "parent", "/", "f\"{self.name}.h5\"", "\n", "with", "requests", ".", "Session", "(", ")", "as", "s", ":", "\n", "            ", "r", "=", "s", ".", "get", "(", "url", ")", "\n", "with", "open", "(", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "r", ".", "content", ")", "\n", "\n", "", "", "data", "=", "defaultdict", "(", "dict", ")", "\n", "with", "h5py", ".", "File", "(", "path", ")", "as", "f", ":", "\n", "            ", "for", "ds", "in", "f", ":", "\n", "                ", "for", "array", "in", "f", "[", "ds", "]", ":", "\n", "                    ", "data", "[", "ds", "]", "[", "array", "]", "=", "f", "[", "ds", "]", "[", "array", "]", "[", ":", "]", "\n", "\n", "", "", "", "path", ".", "unlink", "(", ")", "\n", "train", "=", "_make_df", "(", "data", "[", "'train'", "]", ")", "\n", "test", "=", "_make_df", "(", "data", "[", "'test'", "]", ")", "\n", "df", "=", "pd", ".", "concat", "(", "[", "train", ",", "test", "]", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "df", ".", "to_feather", "(", "self", ".", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_deepsurv._make_df": [[38, 48], ["pandas.DataFrame().assign().assign", "str", "range", "pandas.DataFrame().assign", "pandas.DataFrame"], "function", ["None"], ["", "", "def", "_make_df", "(", "data", ")", ":", "\n", "    ", "x", "=", "data", "[", "'x'", "]", "\n", "t", "=", "data", "[", "'t'", "]", "\n", "d", "=", "data", "[", "'e'", "]", "\n", "\n", "colnames", "=", "[", "'x'", "+", "str", "(", "i", ")", "for", "i", "in", "range", "(", "x", ".", "shape", "[", "1", "]", ")", "]", "\n", "df", "=", "(", "pd", ".", "DataFrame", "(", "x", ",", "columns", "=", "colnames", ")", "\n", ".", "assign", "(", "duration", "=", "t", ")", "\n", ".", "assign", "(", "event", "=", "d", ")", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn.__init__": [[73, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_path_dir", "=", "_PATH_DATA", "/", "self", ".", "name", "\n", "self", ".", "path_train", "=", "self", ".", "_path_dir", "/", "'train.feather'", "\n", "self", ".", "path_test", "=", "self", ".", "_path_dir", "/", "'test.feather'", "\n", "self", ".", "path_val", "=", "self", ".", "_path_dir", "/", "'val.feather'", "\n", "self", ".", "path_survival", "=", "self", ".", "_path_dir", "/", "'survival_data.feather'", "\n", "self", ".", "log_cols", "=", "[", "'actual_amount_paid'", ",", "'days_between_subs'", ",", "'days_since_reg_init'", ",", "\n", "'payment_plan_days'", ",", "'plan_list_price'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn.read_df": [[83, 130], ["pandas.read_feather", "path.exists", "print", "numpy.log", "df.rename.rename.assign", "df.rename.rename.rename", "pandas.read_feather", "ValueError", "from_kkbox._DatasetKKBoxChurn.read_df.log_min_p"], "methods", ["None"], ["", "def", "read_df", "(", "self", ",", "subset", "=", "'train'", ",", "log_trans", "=", "True", ")", ":", "\n", "        ", "\"\"\"Get train, test, val or general survival data set.\n\n        The columns: 'duration' and 'event' gives the duration time and event indicator.\n\n        The survival data set contains no covariates, but can be useful for extending\n        the dataset with more covariates from Kaggle.\n\n        Keyword Arguments:\n            subset {str} -- Which subset to use ('train', 'val', 'test').\n                Can also set 'survival' which will give df with survival information without\n                covariates. (default: {'train'})\n            log_trans {bool} -- If covariates in 'kkbox_v1.log_cols' (from Kvamme paper) should be\n                transformed with 'z = log(x - min(x) + 1)'. (default: {True})\n        \"\"\"", "\n", "if", "subset", "==", "'train'", ":", "\n", "            ", "path", "=", "self", ".", "path_train", "\n", "", "elif", "subset", "==", "'test'", ":", "\n", "            ", "path", "=", "self", ".", "path_test", "\n", "", "elif", "subset", "==", "'val'", ":", "\n", "            ", "path", "=", "self", ".", "path_val", "\n", "", "elif", "subset", "==", "'survival'", ":", "\n", "            ", "path", "=", "self", ".", "path_survival", "\n", "return", "pd", ".", "read_feather", "(", "path", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need 'subset' to be 'train', 'val', or 'test'. Got {subset}\"", ")", "\n", "\n", "", "if", "not", "path", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "f\"\"\"\n            The KKBox dataset not locally available.\n            If you want to download, call 'kkbox_v1.download_kkbox()', but note that\n            this might take around 10 min!\n            NOTE: You need Kaggle credentials! Follow instructions at\n            https://github.com/Kaggle/kaggle-api#api-credentials\n            \"\"\"", ")", "\n", "return", "None", "\n", "\n", "", "def", "log_min_p", "(", "col", ",", "df", ")", ":", "\n", "            ", "x", "=", "df", "[", "col", "]", "\n", "min_", "=", "-", "1.", "if", "col", "==", "'days_since_reg_init'", "else", "0.", "\n", "return", "np", ".", "log", "(", "x", "-", "min_", "+", "1", ")", "\n", "\n", "", "df", "=", "pd", ".", "read_feather", "(", "path", ")", "\n", "if", "log_trans", ":", "\n", "            ", "df", "=", "df", ".", "assign", "(", "**", "{", "col", ":", "log_min_p", "(", "col", ",", "df", ")", "for", "col", "in", "self", ".", "log_cols", "}", ")", "\n", "df", "=", "df", ".", "rename", "(", "columns", "=", "{", "col", ":", "f\"log_{col}\"", "for", "col", "in", "self", ".", "log_cols", "}", ")", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn.download_kkbox": [[132, 139], ["from_kkbox._DatasetKKBoxChurn._download"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._download"], ["", "def", "download_kkbox", "(", "self", ")", ":", "\n", "        ", "\"\"\"Download KKBox data set.\n        This is likely to take around 10 min!!!\n        NOTE: You need Kaggle credentials! Follow instructions at\n        https://github.com/Kaggle/kaggle-api#api-credentials\n        \"\"\"", "\n", "self", ".", "_download", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._download": [[140, 153], ["from_kkbox._DatasetKKBoxChurn._setup_download_dir", "from_kkbox._DatasetKKBoxChurn._7z_from_kaggle", "from_kkbox._DatasetKKBoxChurn._csv_to_feather_with_types", "print", "from_kkbox._DatasetKKBoxChurn._make_survival_data", "print", "from_kkbox._DatasetKKBoxChurn._make_survival_covariates", "print", "from_kkbox._DatasetKKBoxChurn._make_train_test_split", "print", "from_kkbox._DatasetKKBoxChurn._clean_up", "print"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._setup_download_dir", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._7z_from_kaggle", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._csv_to_feather_with_types", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_survival_data", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_survival_covariates", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_train_test_split", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._clean_up"], ["", "def", "_download", "(", "self", ")", ":", "\n", "        ", "self", ".", "_setup_download_dir", "(", ")", "\n", "self", ".", "_7z_from_kaggle", "(", ")", "\n", "self", ".", "_csv_to_feather_with_types", "(", ")", "\n", "print", "(", "'Creating survival data...'", ")", "\n", "self", ".", "_make_survival_data", "(", ")", "\n", "print", "(", "'Creating covariates...'", ")", "\n", "self", ".", "_make_survival_covariates", "(", ")", "\n", "print", "(", "'Creating train/test/val subsets...'", ")", "\n", "self", ".", "_make_train_test_split", "(", ")", "\n", "print", "(", "'Cleaning up...'", ")", "\n", "self", ".", "_clean_up", "(", ")", "\n", "print", "(", "\"Done! You can now call `df = kkbox.read_df()`.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._setup_download_dir": [[154, 159], ["from_kkbox._DatasetKKBoxChurn._path_dir.exists", "from_kkbox._DatasetKKBoxChurn._clean_up", "from_kkbox._DatasetKKBoxChurn._path_dir.mkdir"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._clean_up"], ["", "def", "_setup_download_dir", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_path_dir", ".", "exists", "(", ")", ":", "\n", "            ", "self", ".", "_clean_up", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_path_dir", ".", "mkdir", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._7z_from_kaggle": [[160, 183], ["print", "kaggle.api.competition_download_file", "print", "py7zr.SevenZipFile", "py7zr.SevenZipFile.extractall", "py7zr.SevenZipFile.close", "print", "OSError"], "methods", ["None"], ["", "", "def", "_7z_from_kaggle", "(", "self", ")", ":", "\n", "        ", "import", "subprocess", "\n", "import", "py7zr", "\n", "try", ":", "\n", "            ", "import", "kaggle", "\n", "", "except", "OSError", "as", "e", ":", "\n", "            ", "raise", "OSError", "(", "\n", "f\"\"\"\"\n            Need to provide Kaggle credentials to download this data set. See guide at\n            https://github.com/Kaggle/kaggle-api#api-credentials.\n            \"\"\"", "\n", ")", "\n", "", "files", "=", "[", "'train'", ",", "'transactions'", ",", "'members_v3'", "]", "\n", "print", "(", "'Downloading from Kaggle...'", ")", "\n", "for", "file", "in", "files", ":", "\n", "            ", "kaggle", ".", "api", ".", "competition_download_file", "(", "'kkbox-churn-prediction-challenge'", ",", "file", "+", "'.csv.7z'", ",", "\n", "path", "=", "self", ".", "_path_dir", ",", "force", "=", "True", ")", "\n", "", "for", "file", "in", "files", ":", "\n", "            ", "print", "(", "f\"Extracting '{file}'...\"", ")", "\n", "archive", "=", "py7zr", ".", "SevenZipFile", "(", "self", ".", "_path_dir", "/", "f\"{file}.csv.7z\"", ",", "mode", "=", "\"r\"", ")", "\n", "archive", ".", "extractall", "(", "path", "=", "self", ".", "_path_dir", ")", "\n", "archive", ".", "close", "(", ")", "\n", "print", "(", "f\"Finished extracting '{file}'.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._csv_to_feather_with_types": [[184, 200], ["print", "pandas.read_csv().to_feather", "pandas.read_csv", "pandas.read_csv.assign().to_feather", "pandas.read_csv", "pandas.read_csv.assign().to_feather", "pandas.read_csv", "pandas.read_csv.assign", "pandas.read_csv.assign", "members[].astype", "trans[].astype"], "methods", ["None"], ["", "", "def", "_csv_to_feather_with_types", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Making feather data frames...\"", ")", "\n", "file", "=", "'train'", "\n", "pd", ".", "read_csv", "(", "self", ".", "_path_dir", "/", "f\"{file}.csv\"", ")", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "f\"{file}_raw.feather\"", ")", "\n", "\n", "file", "=", "'members'", "\n", "members", "=", "pd", ".", "read_csv", "(", "self", ".", "_path_dir", "/", "f\"{file}_v3.csv\"", ",", "\n", "parse_dates", "=", "[", "'registration_init_time'", "]", ")", "\n", "(", "members", ".", "assign", "(", "**", "{", "col", ":", "members", "[", "col", "]", ".", "astype", "(", "'category'", ")", "\n", "for", "col", "in", "[", "'city'", ",", "'registered_via'", ",", "'gender'", "]", "}", ")", "\n", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "f\"{file}.feather\"", ")", ")", "\n", "\n", "file", "=", "'transactions'", "\n", "trans", "=", "pd", ".", "read_csv", "(", "self", ".", "_path_dir", "/", "f\"{file}.csv\"", ",", "parse_dates", "=", "[", "'transaction_date'", ",", "'membership_expire_date'", "]", ")", "\n", "(", "trans", ".", "assign", "(", "**", "{", "col", ":", "trans", "[", "col", "]", ".", "astype", "(", "'category'", ")", "for", "col", "in", "[", "'payment_method_id'", ",", "'is_auto_renew'", ",", "'is_cancel'", "]", "}", ")", "\n", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "f\"{file}.feather\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._make_survival_data": [[201, 325], ["pandas.read_feather", "pandas.read_feather", "trans.assign().assign.assign().assign.sort_values().assign().assign", "trans.assign().assign.assign().assign.assign().assign", "trans.assign().assign.assign().assign.merge().assign().drop().assign().assign", "trans.assign().assign.assign().assign.join", "trans.assign().assign.assign().assign.assign", "trans.assign().assign.assign().assign.assign().assign", "trans.assign().assign.assign().assign.assign().assign().loc[].merge", "indivs.pipe().pipe.pipe().pipe.assign", "indivs.pipe().pipe.pipe().pipe.assign().assign().drop", "indivs.pipe().pipe.pipe().pipe.pipe().pipe", "indivs.pipe().pipe.pipe().pipe.reset_index().to_feather", "pandas.read_feather", "[].count().max", "[].shift().rename", "[].cumsum().fillna().astype", "df.assign.assign.assign", "df.assign.assign.assign", "df.assign.assign.assign", "trans.assign().assign.assign().assign.sort_values().assign", "trans.assign().assign.assign().assign.assign", "trans.assign().assign.assign().assign.merge().assign().drop().assign", "diff.total_seconds", "trans.assign().assign.assign().assign.assign", "indivs.pipe().pipe.pipe().pipe.drop_duplicates", "indivs.pipe().pipe.pipe().pipe.assign().assign", "indivs.pipe().pipe.pipe().pipe.pipe", "indivs.pipe().pipe.pipe().pipe.reset_index", "[].count", "[].shift", "[].cumsum().fillna", "from_kkbox._DatasetKKBoxChurn._make_survival_data.number_of_new_starts"], "methods", ["None"], ["", "def", "_make_survival_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"Combine the downloaded files and create a survival data sets\n        (more or less without covariates).\n\n        A customer is considered churned if one of the following is true:\n            - If it has been more than 30 days since the expiration data of a membership subscription until the next transaction.\n            - If the customer has expiration in before 2017-03-01, and no transaction after that.\n        \"\"\"", "\n", "train", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'train_raw.feather'", ")", "\n", "members", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'members.feather'", ")", "\n", "trans", "=", "(", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'transactions.feather'", ")", "\n", "[", "[", "'msno'", ",", "'transaction_date'", ",", "'membership_expire_date'", ",", "'is_cancel'", "]", "]", ")", "\n", "last_churn_date", "=", "'2017-01-29'", "# 30 days before last transactions are made in the dataset.", "\n", "\n", "# Churn: More than 30 days before reentering", "\n", "def", "days_without_membership", "(", "df", ")", ":", "\n", "            ", "diff", "=", "(", "df", "[", "'next_trans_date'", "]", "-", "df", "[", "'membership_expire_date'", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "\n", "return", "diff", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "assign", "(", "next_trans_date", "=", "(", "lambda", "x", ":", "x", ".", "groupby", "(", "'msno'", ")", "[", "'transaction_date'", "]", ".", "shift", "(", "-", "1", ")", ")", ")", "\n", ".", "assign", "(", "churn30", "=", "lambda", "x", ":", "days_without_membership", "(", "x", ")", ">", "30", ")", ")", "\n", "\n", "# Remove entries with membership_expire_date < transaction_date", "\n", "trans", "=", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'transaction_date'", "]", "<=", "x", "[", "'membership_expire_date'", "]", "]", "\n", "assert", "(", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn30'", "]", "==", "True", "]", ".", "groupby", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "[", "'msno'", "]", ".", "count", "(", ")", ".", "max", "(", ")", "==", "1", ")", "\n", "\n", "# Churn: Leaves forever", "\n", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "max_trans_date", "=", "lambda", "x", ":", "x", ".", "groupby", "(", "'msno'", ")", "[", "'transaction_date'", "]", ".", "transform", "(", "'max'", ")", ")", "\n", ".", "assign", "(", "final_churn", "=", "(", "lambda", "x", ":", "\n", "(", "x", "[", "'max_trans_date'", "]", "<=", "last_churn_date", ")", "&", "\n", "(", "x", "[", "'transaction_date'", "]", "==", "x", "[", "'max_trans_date'", "]", ")", "&", "\n", "(", "x", "[", "'membership_expire_date'", "]", "<=", "last_churn_date", ")", "\n", ")", ")", ")", "\n", "\n", "# Churn: From training set", "\n", "trans", "=", "(", "trans", "\n", ".", "merge", "(", "train", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", "\n", ".", "assign", "(", "train_churn", "=", "lambda", "x", ":", "x", "[", "'is_churn'", "]", ".", "fillna", "(", "0", ")", ".", "astype", "(", "'bool'", ")", ")", "\n", ".", "drop", "(", "'is_churn'", ",", "axis", "=", "1", ")", "\n", ".", "assign", "(", "train_churn", "=", "lambda", "x", ":", "(", "x", "[", "'max_trans_date'", "]", "==", "x", "[", "'transaction_date'", "]", ")", "&", "x", "[", "'train_churn'", "]", ")", "\n", ".", "assign", "(", "churn", "=", "lambda", "x", ":", "x", "[", "'train_churn'", "]", "|", "x", "[", "'churn30'", "]", "|", "x", "[", "'final_churn'", "]", ")", ")", "\n", "\n", "# Split individuals on churn", "\n", "trans", "=", "(", "trans", "\n", ".", "join", "(", "trans", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "groupby", "(", "'msno'", ")", "[", "[", "'churn30'", ",", "'membership_expire_date'", "]", "]", ".", "shift", "(", "1", ")", "\n", ".", "rename", "(", "columns", "=", "{", "'churn30'", ":", "'new_start'", ",", "'membership_expire_date'", ":", "'prev_mem_exp_date'", "}", ")", ")", ")", "\n", "\n", "def", "number_of_new_starts", "(", "df", ")", ":", "\n", "            ", "return", "(", "df", "\n", ".", "assign", "(", "new_start", "=", "lambda", "x", ":", "x", "[", "'new_start'", "]", ".", "astype", "(", "'float'", ")", ")", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "groupby", "(", "'msno'", ")", "\n", "[", "'new_start'", "]", ".", "cumsum", "(", ")", ".", "fillna", "(", "0.", ")", "\n", ".", "astype", "(", "'int'", ")", ")", "\n", "\n", "", "def", "days_between_subs", "(", "df", ")", ":", "\n", "            ", "diff", "=", "(", "df", "[", "'transaction_date'", "]", "-", "df", "[", "'prev_mem_exp_date'", "]", ")", ".", "dt", "\n", "diff", "=", "diff", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "df", "=", "df", ".", "assign", "(", "days_between_subs", "=", "diff", ")", "\n", "df", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'new_start'", "]", "!=", "True", ",", "'days_between_subs'", "]", "=", "np", ".", "nan", "\n", "return", "df", "[", "'days_between_subs'", "]", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "n_prev_churns", "=", "lambda", "x", ":", "number_of_new_starts", "(", "x", ")", ",", "\n", "days_between_subs", "=", "lambda", "x", ":", "days_between_subs", "(", "x", ")", ")", ")", "\n", "\n", "# Set start times", "\n", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "start_date", "=", "trans", ".", "groupby", "(", "[", "'msno'", ",", "'n_prev_churns'", "]", ")", "[", "'transaction_date'", "]", ".", "transform", "(", "'min'", ")", ")", "\n", ".", "assign", "(", "first_churn", "=", "lambda", "x", ":", "(", "x", "[", "'n_prev_churns'", "]", "==", "0", ")", "&", "(", "x", "[", "'churn'", "]", "==", "True", ")", ")", ")", "\n", "\n", "# Get only last transactions (per chrun)", "\n", "indivs", "=", "(", "trans", "\n", ".", "assign", "(", "censored", "=", "lambda", "x", ":", "x", ".", "groupby", "(", "'msno'", ")", "[", "'churn'", "]", ".", "transform", "(", "'sum'", ")", "==", "0", ")", "\n", ".", "assign", "(", "last_censored", "=", "(", "lambda", "x", ":", "\n", "(", "x", "[", "'censored'", "]", "==", "True", ")", "&", "\n", "(", "x", "[", "'transaction_date'", "]", "==", "x", "[", "'max_trans_date'", "]", ")", "\n", ")", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'last_censored'", "]", "|", "x", "[", "'churn'", "]", "]", "\n", ".", "merge", "(", "members", "[", "[", "'msno'", ",", "'registration_init_time'", "]", "]", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", ")", "\n", "\n", "def", "time_diff_days", "(", "df", ",", "last", ",", "first", ")", ":", "\n", "            ", "return", "(", "df", "[", "last", "]", "-", "df", "[", "first", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "\n", "", "indivs", "=", "(", "indivs", "\n", ".", "assign", "(", "time", "=", "lambda", "x", ":", "time_diff_days", "(", "x", ",", "'membership_expire_date'", ",", "'start_date'", ")", ",", "\n", "days_since_reg_init", "=", "lambda", "x", ":", "time_diff_days", "(", "x", ",", "'start_date'", ",", "'registration_init_time'", ")", ")", ")", "\n", "\n", "# When multiple transactions on last day, remove all but the last", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'transaction_date'", "]", "!=", "x", "[", "'next_trans_date'", "]", "]", "\n", "assert", "indivs", ".", "shape", "==", "indivs", ".", "drop_duplicates", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", ".", "shape", "\n", "assert", "(", "indivs", "[", "'churn'", "]", "!=", "indivs", "[", "'censored'", "]", ")", ".", "all", "(", ")", "\n", "\n", "# Clean up and remove variables that are not from the first transaction day", "\n", "dropcols", "=", "[", "'transaction_date'", ",", "'is_cancel'", ",", "'next_trans_date'", ",", "'max_trans_date'", ",", "'prev_mem_exp_date'", ",", "\n", "'censored'", ",", "'last_censored'", ",", "'churn30'", ",", "'final_churn'", ",", "'train_churn'", ",", "'membership_expire_date'", "]", "\n", "\n", "indivs", "=", "(", "indivs", "\n", ".", "assign", "(", "churn_type", "=", "lambda", "x", ":", "1", "*", "x", "[", "'churn30'", "]", "+", "2", "*", "x", "[", "'final_churn'", "]", "+", "4", "*", "x", "[", "'train_churn'", "]", ")", "\n", ".", "assign", "(", "churn_type", "=", "lambda", "x", ":", "\n", "np", ".", "array", "(", "[", "'censoring'", ",", "'30days'", ",", "'final'", ",", "'30days_and_final'", ",", "'train'", ",", "'train_and_30'", ",", "\n", "'train_and_final'", ",", "'train_30_and_final'", "]", ")", "[", "x", "[", "'churn_type'", "]", "]", ")", "\n", ".", "drop", "(", "dropcols", ",", "axis", "=", "1", ")", ")", "\n", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn_type'", "]", "!=", "'train_30_and_final'", "]", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'time'", "]", ">", "0", "]", "\n", "\n", "def", "as_category", "(", "df", ",", "columns", ")", ":", "\n", "            ", "return", "df", ".", "assign", "(", "**", "{", "col", ":", "df", "[", "col", "]", ".", "astype", "(", "'category'", ")", "for", "col", "in", "columns", "}", ")", "\n", "\n", "", "def", "as_int", "(", "df", ",", "columns", ")", ":", "\n", "            ", "return", "df", ".", "assign", "(", "**", "{", "col", ":", "df", "[", "col", "]", ".", "astype", "(", "'int'", ")", "for", "col", "in", "columns", "}", ")", "\n", "\n", "", "indivs", "=", "(", "indivs", "\n", ".", "pipe", "(", "as_int", ",", "[", "'time'", "]", ")", "\n", ".", "pipe", "(", "as_category", ",", "[", "'new_start'", ",", "'churn_type'", "]", ")", ")", "\n", "\n", "# indivs.reset_index(drop=True).to_feather(self._path_dir / 'survival_data.feather')", "\n", "indivs", ".", "reset_index", "(", "drop", "=", "True", ")", ".", "to_feather", "(", "self", ".", "path_survival", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._make_survival_covariates": [[326, 404], ["pandas.read_feather", "pandas.read_feather", "pandas.read_feather.merge().drop().drop_duplicates", "trans.drop.drop.merge().assign().drop().assign", "trans.drop.drop.assign", "pandas.testing.assert_frame_equal", "trans.drop.drop.loc[].assign().assign", "trans.drop.drop.assign", "trans.drop.drop.drop().assign", "trans.drop.drop.drop", "trans.drop.drop.drop", "trans_log.assign.assign.rename", "trans_log.assign.assign.assign", "trans_log.assign.assign.reset_index().to_feather", "pandas.datetime", "numpy.round", "pandas.read_feather.merge().drop", "trans.drop.drop.merge().assign().drop", "trans.drop.drop.loc[].assign", "trans.drop.drop.drop", "dict", "trans_log.assign.assign.reset_index", "x[].clip", "x[].fillna", "pandas.read_feather.registration_init_time.isnull", "pandas.read_feather.bd.isnull", "x[].fillna", "x[].fillna", "trans_log[].astype", "pandas.read_feather.merge", "trans.drop.drop.merge().assign", "x[].isnull", "x[].isnull", "pandas.read_feather", "x[].isnull", "trans.drop.drop.merge", "pandas.read_feather.drop", "from_kkbox._DatasetKKBoxChurn._make_survival_covariates.get_age_at_start"], "methods", ["None"], ["", "def", "_make_survival_covariates", "(", "self", ")", ":", "\n", "# individs = pd.read_feather(self._path_dir / 'survival_data.feather')", "\n", "        ", "individs", "=", "pd", ".", "read_feather", "(", "self", ".", "path_survival", ")", "\n", "members", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'members.feather'", ")", "\n", "trans", "=", "(", "individs", "\n", ".", "merge", "(", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'transactions.feather'", ")", ",", "\n", "how", "=", "'left'", ",", "left_on", "=", "[", "'msno'", ",", "'start_date'", "]", ",", "right_on", "=", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "drop", "(", "'transaction_date'", ",", "axis", "=", "1", ")", "# same as start_date", "\n", ".", "drop_duplicates", "(", "[", "'msno'", ",", "'start_date'", "]", ",", "keep", "=", "'last'", ")", "# keep last transaction on start_date (by idx)", "\n", ")", "\n", "assert", "trans", ".", "shape", "[", "0", "]", "==", "individs", ".", "shape", "[", "0", "]", "\n", "\n", "def", "get_age_at_start", "(", "df", ")", ":", "\n", "            ", "fixed_date", "=", "pd", ".", "datetime", "(", "2017", ",", "3", ",", "1", ")", "\n", "# Not important what the date is, though it is reasonalbe to use the last.", "\n", "age_diff", "=", "(", "fixed_date", "-", "df", "[", "'start_date'", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", "*", "365", ")", "\n", "return", "np", ".", "round", "(", "df", "[", "'bd'", "]", "-", "age_diff", ")", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "merge", "(", "members", ".", "drop", "(", "[", "'registration_init_time'", "]", ",", "axis", "=", "1", ")", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", "\n", ".", "assign", "(", "age_at_start", "=", "lambda", "x", ":", "get_age_at_start", "(", "x", ")", ")", "\n", ".", "drop", "(", "[", "'bd'", "]", ",", "axis", "=", "1", ")", "\n", ".", "assign", "(", "strange_age", "=", "lambda", "x", ":", "(", "x", "[", "'age_at_start'", "]", "<=", "0", ")", "|", "(", "x", "[", "'age_at_start'", "]", ">=", "100", ")", ",", "\n", "age_at_start", "=", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "100", ")", ")", ")", "\n", "\n", "# days_between_subs", "\n", "# There are None for (not new start), so we can just set them to zero, and we don't need to include another variable (as it allready exists).", "\n", "trans", "=", "trans", ".", "assign", "(", "days_between_subs", "=", "lambda", "x", ":", "x", "[", "'days_between_subs'", "]", ".", "fillna", "(", "0.", ")", ")", "\n", "\n", "# days_since_reg_init", "\n", "# We remove negative entries, set Nans to -1, and add a categorical value for missing.", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", "]", ",", "\n", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "isnull", "(", ")", "]", ")", "\n", "assert", "(", "members", ".", "registration_init_time", ".", "isnull", "(", ")", "==", "members", ".", "bd", ".", "isnull", "(", ")", ")", ".", "all", "(", ")", "\n", "\n", "trans", "=", "(", "trans", "\n", ".", "loc", "[", "lambda", "x", ":", "(", "x", "[", "'days_since_reg_init'", "]", ">=", "0", ")", "|", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", "]", "\n", ".", "assign", "(", "nan_days_since_reg_init", "=", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", ")", "\n", ".", "assign", "(", "days_since_reg_init", "=", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "fillna", "(", "-", "1", ")", ")", ")", "\n", "\n", "# age_at_start", "\n", "# This is Nan when days_since_reg_init is nan. This is because registration_init_time is nan when bd is nan.", "\n", "# We have removed negative entries, so we set Nans to -1, but don't add dummy because its equal to days_since_reg_init dummy.", "\n", "trans", "=", "trans", ".", "assign", "(", "age_at_start", "=", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "fillna", "(", "-", "1.", ")", ")", "\n", "\n", "# First churn variable", "\n", "# The first_churn variable is false for everyone that does not churn. Therefore we can't use it.", "\n", "# We use n_prev_churns == 0 instead", "\n", "trans", "=", "(", "trans", "\n", ".", "drop", "(", "'first_churn'", ",", "axis", "=", "1", ")", "\n", ".", "assign", "(", "no_prev_churns", "=", "lambda", "x", ":", "x", "[", "'n_prev_churns'", "]", "==", "0", ")", ")", "\n", "\n", "# Drop variables that are not useful", "\n", "trans", "=", "(", "trans", "\n", ".", "drop", "(", "[", "'start_date'", ",", "'registration_init_time'", ",", "'churn_type'", ",", "\n", "'membership_expire_date'", ",", "'new_start'", "]", ",", "\n", "axis", "=", "1", ")", ")", "\n", "\n", "# Remove payment_method_id", "\n", "# We could use this covariate, but we choose not to...", "\n", "trans", "=", "trans", ".", "drop", "(", "'payment_method_id'", ",", "axis", "=", "1", ")", "\n", "\n", "# ### Log transform variables", "\n", "# log_cols = ['actual_amount_paid', 'days_between_subs', 'days_since_reg_init', 'payment_plan_days',", "\n", "#             'plan_list_price']", "\n", "\n", "# log_min_p = lambda x: np.log(x - x.min() + 1)", "\n", "# trans_log = trans.assign(**{col: log_min_p(trans[col]) for col in self.log_cols})", "\n", "# assert trans_log[self.log_cols].pipe(np.isfinite).all().all()", "\n", "trans_log", "=", "trans", "\n", "\n", "trans_log", "=", "trans_log", ".", "rename", "(", "columns", "=", "dict", "(", "churn", "=", "'event'", ",", "time", "=", "'duration'", ")", ")", "\n", "float_cols", "=", "[", "'n_prev_churns'", ",", "'days_between_subs'", ",", "'days_since_reg_init'", ",", "'payment_plan_days'", ",", "\n", "'plan_list_price'", ",", "'age_at_start'", ",", "'actual_amount_paid'", ",", "'is_auto_renew'", ",", "\n", "'is_cancel'", ",", "'strange_age'", ",", "'nan_days_since_reg_init'", ",", "'no_prev_churns'", ",", "'duration'", ",", "'event'", "]", "\n", "trans_log", "=", "trans_log", ".", "assign", "(", "**", "{", "col", ":", "trans_log", "[", "col", "]", ".", "astype", "(", "'float32'", ")", "for", "col", "in", "float_cols", "}", ")", "\n", "# cov_file = join(data_dir, 'covariates.feather')", "\n", "trans_log", ".", "reset_index", "(", "drop", "=", "True", ")", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "'covariates.feather'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._make_train_test_split": [[405, 426], ["numpy.random.seed", "pandas.read_feather", "from_kkbox._DatasetKKBoxChurn._make_train_test_split.train_test_split_customer"], "methods", ["None"], ["", "def", "_make_train_test_split", "(", "self", ",", "seed", "=", "1234", ")", ":", "\n", "        ", "from", "sklearn", ".", "model_selection", "import", "train_test_split", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "covariates", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'covariates.feather'", ")", "\n", "\n", "def", "train_test_split_customer", "(", "df", ",", "col_customer", ",", "test_size", ")", ":", "\n", "            ", "tr", ",", "te", "=", "train_test_split", "(", "df", "[", "[", "col_customer", "]", "]", ".", "drop_duplicates", "(", ")", ",", "test_size", "=", "test_size", ")", "\n", "train", "=", "df", ".", "merge", "(", "tr", ",", "how", "=", "'right'", ",", "on", "=", "col_customer", ")", "\n", "test", "=", "df", ".", "merge", "(", "te", ",", "how", "=", "'right'", ",", "on", "=", "col_customer", ")", "\n", "return", "train", ",", "test", "\n", "\n", "", "train", ",", "test", "=", "train_test_split_customer", "(", "covariates", ",", "'msno'", ",", "0.25", ")", "\n", "train", ",", "val", "=", "train_test_split_customer", "(", "train", ",", "'msno'", ",", "0.1", ")", "\n", "\n", "assert", "train", ".", "merge", "(", "test", ",", "how", "=", "'inner'", ",", "on", "=", "'msno'", ")", ".", "shape", "[", "0", "]", "==", "0", "\n", "assert", "train", ".", "merge", "(", "val", ",", "how", "=", "'inner'", ",", "on", "=", "'msno'", ")", ".", "shape", "[", "0", "]", "==", "0", "\n", "assert", "test", ".", "merge", "(", "val", ",", "how", "=", "'inner'", ",", "on", "=", "'msno'", ")", ".", "shape", "[", "0", "]", "==", "0", "\n", "\n", "train", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "'train.feather'", ")", "\n", "test", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "'test.feather'", ")", "\n", "val", ".", "to_feather", "(", "self", ".", "_path_dir", "/", "'val.feather'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn._clean_up": [[427, 438], ["from_kkbox._DatasetKKBoxChurn._path_dir.iterdir", "file.unlink", "warnings.warn"], "methods", ["None"], ["", "def", "_clean_up", "(", "self", ")", ":", "\n", "        ", "remove", "=", "[", "'covariates.feather'", ",", "'train.csv.7z'", ",", "'transactions.csv.7z'", ",", "'members_v3.csv.7z'", ",", "\n", "'train.csv'", ",", "'transactions.csv'", ",", "'members_v3.csv'", ",", "\n", "'train_raw.feather'", ",", "'transactions.feather'", ",", "'members.feather'", "]", "\n", "for", "file", "in", "self", ".", "_path_dir", ".", "iterdir", "(", ")", ":", "\n", "            ", "if", "file", ".", "name", "not", "in", "remove", ":", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "file", ".", "unlink", "(", ")", "\n", "", "except", "IsADirectoryError", ":", "\n", "                ", "warnings", ".", "warn", "(", "f\"Encountered directory in {self._path_dir}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxChurn.delete_local_copy": [[439, 442], ["path.unlink"], "methods", ["None"], ["", "", "", "def", "delete_local_copy", "(", "self", ")", ":", "\n", "        ", "for", "path", "in", "[", "self", ".", "path_train", ",", "self", ".", "path_test", ",", "self", ".", "path_val", ",", "self", ".", "path_survival", "]", ":", "\n", "            ", "path", ".", "unlink", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin.__init__": [[519, 525], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_path_dir", "=", "_PATH_DATA", "/", "self", ".", "name", "\n", "self", ".", "path_survival", "=", "self", ".", "_path_dir", "/", "'survival_data.feather'", "\n", "self", ".", "path_covariates", "=", "self", ".", "_path_dir", "/", "'covariates.feather'", "\n", "self", ".", "log_cols", "=", "[", "'actual_amount_paid'", ",", "'days_between_subs'", ",", "'days_since_reg_init'", ",", "\n", "'payment_plan_days'", ",", "'plan_list_price'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin.read_df": [[526, 575], ["pandas.read_feather().set_index", "df.rename.rename.drop", "df.rename.rename.rename", "path.exists", "print", "numpy.log", "df.rename.rename.assign", "df.rename.rename.rename", "pandas.read_feather", "pandas.read_feather", "dict", "from_kkbox._DatasetKKBoxAdmin.read_df.log_min_p"], "methods", ["None"], ["", "def", "read_df", "(", "self", ",", "log_trans", "=", "True", ",", "no_covs", "=", "False", ")", ":", "\n", "        ", "\"\"\"Get the data set as a pandas data frame.\n\n        The columns: 'duration' and 'event' gives the duration time and event indicator, and\n        the column 'censor_duration' give the administrative censoring time.\n\n        The survival data set contains no covariates, but can be useful for extending\n        the dataset with more covariates from Kaggle.\n\n        Keyword Arguments:\n            log_trans {bool} -- If covariates in 'kkbox.log_cols' (from Kvamme paper) should be\n                transformed with 'z = log(x - min(x) + 1)'. (default: {True})\n            no_covs {str} -- If False get the regular data set, if True only get the survival set\n                without the covariates. (default: {False})\n        \"\"\"", "\n", "if", "no_covs", "is", "False", ":", "\n", "            ", "path", "=", "self", ".", "path_covariates", "\n", "", "elif", "no_covs", "is", "True", ":", "\n", "            ", "path", "=", "self", ".", "path_survival", "\n", "return", "pd", ".", "read_feather", "(", "self", ".", "path_survival", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "if", "not", "path", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "f\"\"\"\n            The KKBox dataset not locally available.\n            If you want to download, call 'kkbox.download_kkbox()', but note that\n            this might take around 10 min!\n            NOTE: You need Kaggle credentials! Follow instructions at\n            https://github.com/Kaggle/kaggle-api#api-credentials\n            \"\"\"", ")", "\n", "return", "None", "\n", "\n", "", "def", "log_min_p", "(", "col", ",", "df", ")", ":", "\n", "            ", "x", "=", "df", "[", "col", "]", "\n", "min_", "=", "-", "1.", "if", "col", "==", "'days_since_reg_init'", "else", "0.", "\n", "return", "np", ".", "log", "(", "x", "-", "min_", "+", "1", ")", "\n", "\n", "", "df", "=", "pd", ".", "read_feather", "(", "path", ")", ".", "set_index", "(", "'index_survival'", ")", "\n", "if", "log_trans", ":", "\n", "            ", "df", "=", "df", ".", "assign", "(", "**", "{", "col", ":", "log_min_p", "(", "col", ",", "df", ")", "for", "col", "in", "self", ".", "log_cols", "}", ")", "\n", "df", "=", "df", ".", "rename", "(", "columns", "=", "{", "col", ":", "f\"log_{col}\"", "for", "col", "in", "self", ".", "log_cols", "}", ")", "\n", "\n", "", "drop_cols", "=", "[", "'duration'", ",", "'churn'", ",", "'duration_censor'", "]", "\n", "df", "=", "df", ".", "loc", "[", "(", "df", "[", "'duration_lcd'", "]", ">", "0", ")", "&", "(", "df", "[", "'duration_censor_lcd'", "]", ">", "0", ")", "]", "\n", "df", "=", "df", ".", "drop", "(", "drop_cols", ",", "axis", "=", "1", ")", "\n", "df", "=", "df", ".", "rename", "(", "columns", "=", "dict", "(", "duration_lcd", "=", "'duration'", ",", "churn_lcd", "=", "'event'", ",", "\n", "duration_censor_lcd", "=", "'censor_duration'", ")", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_train_test_split": [[576, 578], ["None"], "methods", ["None"], ["", "def", "_make_train_test_split", "(", "self", ",", "seed", "=", "1234", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_survival_data": [[579, 729], ["pandas.read_feather", "pandas.read_feather", "trans.assign().assign.assign().assign.sort_values().assign().assign", "trans.assign().assign.assign().assign.assign().assign", "trans.assign().assign.assign().assign.merge().assign().drop().assign().assign", "trans.assign().assign.assign().assign.join", "trans.assign().assign.assign().assign.assign", "trans.assign().assign.assign().assign.assign().assign", "trans.assign().assign.assign().assign.assign().assign().loc[].merge", "indivs.pipe().pipe.pipe().pipe.assign", "indivs.pipe().pipe.pipe().pipe.assign().assign().drop", "indivs.pipe().pipe.pipe().pipe.assign", "indivs.pipe().pipe.pipe().pipe.drop", "[].all", "indivs.pipe().pipe.pipe().pipe.assign().assign().drop", "indivs.pipe().pipe.pipe().pipe.pipe().pipe", "indivs.pipe().pipe.pipe().pipe.reset_index().to_feather", "pandas.read_feather", "[].count().max", "[].shift().rename", "[].cumsum().fillna().astype", "df.assign.assign.assign", "df.assign.assign.assign", "df.assign.assign.assign", "trans.assign().assign.assign().assign.sort_values().assign", "trans.assign().assign.assign().assign.assign", "trans.assign().assign.assign().assign.merge().assign().drop().assign", "diff.total_seconds", "trans.assign().assign.assign().assign.assign", "indivs.pipe().pipe.pipe().pipe.assign().assign", "indivs.pipe().pipe.pipe().pipe.drop_duplicates", "indivs.pipe().pipe.pipe().pipe.assign().assign", "indivs.pipe().pipe.pipe().pipe.pipe", "indivs.pipe().pipe.pipe().pipe.reset_index", "[].count", "[].shift", "[].cumsum().fillna", "from_kkbox._DatasetKKBoxAdmin._make_survival_data.number_of_new_starts"], "methods", ["None"], ["", "def", "_make_survival_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"Combine the downloaded files and create a survival data sets\n        (more or less without covariates).\n\n        A customer is considered churned if one of the following is true:\n            - If it has been more than 30 days since the expiration data of a membership subscription until the next transaction.\n            - If the customer has expiration in before 2017-03-01, and no transaction after that.\n        We include two form for administrative censoring:\n            - One marked by 'lcd' (last_churn_date), where we perform administrative censoring at '2017-01-29'.\n            - One where we use the 'membership_expire_date' for the churned individuals and 'last_churn_date'\n              for the churned individuals.\n        \"\"\"", "\n", "train", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'train_raw.feather'", ")", "\n", "members", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'members.feather'", ")", "\n", "trans", "=", "(", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'transactions.feather'", ")", "\n", "[", "[", "'msno'", ",", "'transaction_date'", ",", "'membership_expire_date'", ",", "'is_cancel'", "]", "]", ")", "\n", "LAST_CHURN_DATE", "=", "'2017-01-29'", "# 30 days before last transactions are made in the dataset.", "\n", "\n", "# Churn: More than 30 days before reentering", "\n", "def", "days_without_membership", "(", "df", ")", ":", "\n", "            ", "diff", "=", "(", "df", "[", "'next_trans_date'", "]", "-", "df", "[", "'membership_expire_date'", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "\n", "return", "diff", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "assign", "(", "next_trans_date", "=", "(", "lambda", "x", ":", "x", ".", "groupby", "(", "'msno'", ")", "[", "'transaction_date'", "]", ".", "shift", "(", "-", "1", ")", ")", ")", "\n", ".", "assign", "(", "churn30", "=", "lambda", "x", ":", "days_without_membership", "(", "x", ")", ">", "30", ")", ")", "\n", "\n", "# Remove entries with membership_expire_date < transaction_date", "\n", "trans", "=", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'transaction_date'", "]", "<=", "x", "[", "'membership_expire_date'", "]", "]", "\n", "assert", "(", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn30'", "]", "==", "True", "]", ".", "groupby", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "[", "'msno'", "]", ".", "count", "(", ")", ".", "max", "(", ")", "==", "1", ")", "\n", "\n", "# Churn: Leaves forever", "\n", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "max_trans_date", "=", "lambda", "x", ":", "x", ".", "groupby", "(", "'msno'", ")", "[", "'transaction_date'", "]", ".", "transform", "(", "'max'", ")", ")", "\n", ".", "assign", "(", "final_churn", "=", "(", "lambda", "x", ":", "\n", "(", "x", "[", "'max_trans_date'", "]", "<=", "LAST_CHURN_DATE", ")", "&", "\n", "(", "x", "[", "'transaction_date'", "]", "==", "x", "[", "'max_trans_date'", "]", ")", "&", "\n", "(", "x", "[", "'membership_expire_date'", "]", "<=", "LAST_CHURN_DATE", ")", "\n", ")", ")", ")", "\n", "# Churn: From training set", "\n", "trans", "=", "(", "trans", "\n", ".", "merge", "(", "train", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", "\n", ".", "assign", "(", "train_churn", "=", "lambda", "x", ":", "x", "[", "'is_churn'", "]", ".", "fillna", "(", "0", ")", ".", "astype", "(", "'bool'", ")", ")", "\n", ".", "drop", "(", "'is_churn'", ",", "axis", "=", "1", ")", "\n", ".", "assign", "(", "train_churn", "=", "lambda", "x", ":", "(", "x", "[", "'max_trans_date'", "]", "==", "x", "[", "'transaction_date'", "]", ")", "&", "x", "[", "'train_churn'", "]", ")", "\n", ".", "assign", "(", "churn", "=", "lambda", "x", ":", "x", "[", "'train_churn'", "]", "|", "x", "[", "'churn30'", "]", "|", "x", "[", "'final_churn'", "]", ")", ")", "\n", "# split individuals on churn", "\n", "trans", "=", "(", "trans", "\n", ".", "join", "(", "trans", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "groupby", "(", "'msno'", ")", "[", "[", "'churn30'", ",", "'membership_expire_date'", "]", "]", ".", "shift", "(", "1", ")", "\n", ".", "rename", "(", "columns", "=", "{", "'churn30'", ":", "'new_start'", ",", "'membership_expire_date'", ":", "'prev_mem_exp_date'", "}", ")", ")", ")", "\n", "\n", "def", "number_of_new_starts", "(", "df", ")", ":", "\n", "            ", "return", "(", "df", "\n", ".", "assign", "(", "new_start", "=", "lambda", "x", ":", "x", "[", "'new_start'", "]", ".", "astype", "(", "'float'", ")", ")", "\n", ".", "sort_values", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "groupby", "(", "'msno'", ")", "\n", "[", "'new_start'", "]", ".", "cumsum", "(", ")", ".", "fillna", "(", "0.", ")", "\n", ".", "astype", "(", "'int'", ")", ")", "\n", "\n", "", "def", "days_between_subs", "(", "df", ")", ":", "\n", "            ", "diff", "=", "(", "df", "[", "'transaction_date'", "]", "-", "df", "[", "'prev_mem_exp_date'", "]", ")", ".", "dt", "\n", "diff", "=", "diff", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "df", "=", "df", ".", "assign", "(", "days_between_subs", "=", "diff", ")", "\n", "df", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'new_start'", "]", "!=", "True", ",", "'days_between_subs'", "]", "=", "np", ".", "nan", "\n", "return", "df", "[", "'days_between_subs'", "]", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "n_prev_churns", "=", "lambda", "x", ":", "number_of_new_starts", "(", "x", ")", ",", "\n", "days_between_subs", "=", "lambda", "x", ":", "days_between_subs", "(", "x", ")", ")", ")", "\n", "# Set start times", "\n", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "start_date", "=", "trans", ".", "groupby", "(", "[", "'msno'", ",", "'n_prev_churns'", "]", ")", "[", "'transaction_date'", "]", ".", "transform", "(", "'min'", ")", ")", "\n", ".", "assign", "(", "first_churn", "=", "lambda", "x", ":", "(", "x", "[", "'n_prev_churns'", "]", "==", "0", ")", "&", "(", "x", "[", "'churn'", "]", "==", "True", ")", ")", ")", "\n", "\n", "# Get only last transactions (per churn)", "\n", "indivs", "=", "(", "trans", "\n", ".", "assign", "(", "last_start", "=", "lambda", "x", ":", "(", "x", "[", "'n_prev_churns'", "]", "==", "x", ".", "groupby", "(", "'msno'", ")", "[", "'n_prev_churns'", "]", ".", "transform", "(", "'max'", ")", ")", ")", "\n", ".", "assign", "(", "censored", "=", "(", "lambda", "x", ":", "(", "x", "[", "'last_start'", "]", ")", "&", "(", "x", "[", "'churn'", "]", "==", "False", ")", "&", "\n", "(", "x", "[", "'transaction_date'", "]", "==", "x", "[", "'max_trans_date'", "]", ")", ")", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'censored'", "]", "|", "x", "[", "'churn'", "]", "]", "\n", ".", "merge", "(", "members", "[", "[", "'msno'", ",", "'registration_init_time'", "]", "]", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", ")", "\n", "\n", "def", "time_diff_days", "(", "df", ",", "last", ",", "first", ")", ":", "\n", "            ", "return", "(", "df", "[", "last", "]", "-", "df", "[", "first", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", ")", "\n", "\n", "", "indivs", "=", "(", "indivs", "\n", ".", "assign", "(", "duration", "=", "lambda", "x", ":", "time_diff_days", "(", "x", ",", "'membership_expire_date'", ",", "'start_date'", ")", ",", "\n", "days_since_reg_init", "=", "lambda", "x", ":", "time_diff_days", "(", "x", ",", "'start_date'", ",", "'registration_init_time'", ")", ")", ")", "\n", "# Add administrative censoring durations.", "\n", "# We add to types:", "\n", "# - Censoring based on the fixed data `LAST_CHURN_DATE` (lcd).", "\n", "# - Censoring based on member ship expiration, where churned", "\n", "#   are censored at `last_churn_date`.", "\n", "indivs", "=", "(", "indivs", "\n", ".", "assign", "(", "duration_censor", "=", "lambda", "x", ":", "x", "[", "'duration'", "]", ",", "\n", "last_churn_date", "=", "pd", ".", "to_datetime", "(", "LAST_CHURN_DATE", ")", ",", "\n", "duration_lcd", "=", "lambda", "x", ":", "x", "[", "'duration'", "]", ",", "\n", "churn_lcd", "=", "lambda", "x", ":", "x", "[", "'churn'", "]", ")", "\n", ".", "assign", "(", "duration_censor_lcd", "=", "lambda", "x", ":", "time_diff_days", "(", "x", ",", "'last_churn_date'", ",", "'start_date'", ")", ")", "\n", ".", "drop", "(", "'last_churn_date'", ",", "axis", "=", "1", ")", ")", "\n", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn'", "]", ",", "'duration_censor'", "]", "=", "indivs", "[", "'duration_censor_lcd'", "]", "\n", "indivs", "=", "indivs", ".", "assign", "(", "fix_lcd", "=", "lambda", "x", ":", "x", "[", "'duration'", "]", ">", "x", "[", "'duration_censor_lcd'", "]", ")", "\n", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'fix_lcd'", "]", ",", "'churn_lcd'", "]", "=", "False", "\n", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'fix_lcd'", "]", ",", "'duration_lcd'", "]", "=", "indivs", "[", "'duration_censor_lcd'", "]", "\n", "indivs", "=", "indivs", ".", "drop", "(", "'fix_lcd'", ",", "axis", "=", "1", ")", "\n", "assert", "(", "indivs", "\n", ".", "assign", "(", "diff", "=", "lambda", "x", ":", "x", "[", "'duration_censor'", "]", "-", "x", "[", "'duration'", "]", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'diff'", "]", "<", "0", "]", "\n", "[", "'train_churn'", "]", ".", "all", "(", ")", ")", ",", "'All strange censoring times should come from train_churn'", "\n", "# Drop the churns from 'train_churn' that does not fit with our administrative censoring", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'duration_censor'", "]", ">=", "x", "[", "'duration'", "]", "]", "\n", "assert", "(", "indivs", "[", "'duration_censor_lcd'", "]", ">=", "indivs", "[", "'duration_lcd'", "]", ")", ".", "all", "(", ")", ",", "'Cannot have censor durations smaller than durations'", "\n", "tmp", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn'", "]", "==", "False", "]", "\n", "assert", "(", "tmp", "[", "'duration'", "]", "==", "tmp", "[", "'duration_censor'", "]", ")", ".", "all", "(", ")", ",", "'Need all censor durations to be equal'", "\n", "tmp", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn_lcd'", "]", "==", "False", "]", "\n", "assert", "(", "tmp", "[", "'duration_lcd'", "]", "==", "tmp", "[", "'duration_censor_lcd'", "]", ")", ".", "all", "(", ")", ",", "'Need all censor durations to be equal'", "\n", "\n", "# When multiple transactions on last day, remove all but the last", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'transaction_date'", "]", "!=", "x", "[", "'next_trans_date'", "]", "]", "\n", "assert", "indivs", ".", "shape", "==", "indivs", ".", "drop_duplicates", "(", "[", "'msno'", ",", "'transaction_date'", "]", ")", ".", "shape", "\n", "assert", "(", "indivs", "[", "'churn'", "]", "!=", "indivs", "[", "'censored'", "]", ")", ".", "all", "(", ")", "\n", "\n", "# Clean up and remove variables that are not from the first transaction day", "\n", "dropcols", "=", "[", "'transaction_date'", ",", "'is_cancel'", ",", "'next_trans_date'", ",", "'max_trans_date'", ",", "'prev_mem_exp_date'", ",", "\n", "'censored'", ",", "'churn30'", ",", "'final_churn'", ",", "'train_churn'", ",", "'membership_expire_date'", ",", "'last_start'", ",", "\n", "'new_start'", ",", "'first_churn'", "]", "\n", "\n", "indivs", "=", "(", "indivs", "\n", ".", "assign", "(", "churn_type", "=", "lambda", "x", ":", "1", "*", "x", "[", "'churn30'", "]", "+", "2", "*", "x", "[", "'final_churn'", "]", "+", "4", "*", "x", "[", "'train_churn'", "]", ")", "\n", ".", "assign", "(", "churn_type", "=", "lambda", "x", ":", "\n", "np", ".", "array", "(", "[", "'censoring'", ",", "'30days'", ",", "'final'", ",", "'30days_and_final'", ",", "'train'", ",", "'train_and_30'", ",", "\n", "'train_and_final'", ",", "'train_30_and_final'", "]", ")", "[", "x", "[", "'churn_type'", "]", "]", ")", "\n", ".", "drop", "(", "dropcols", ",", "axis", "=", "1", ")", ")", "\n", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'churn_type'", "]", "!=", "'train_30_and_final'", "]", "\n", "indivs", "=", "indivs", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'duration'", "]", ">", "0", "]", "\n", "\n", "def", "as_category", "(", "df", ",", "columns", ")", ":", "\n", "            ", "return", "df", ".", "assign", "(", "**", "{", "col", ":", "df", "[", "col", "]", ".", "astype", "(", "'category'", ")", "for", "col", "in", "columns", "}", ")", "\n", "\n", "", "def", "as_int", "(", "df", ",", "columns", ")", ":", "\n", "            ", "return", "df", ".", "assign", "(", "**", "{", "col", ":", "df", "[", "col", "]", ".", "round", "(", ")", ".", "astype", "(", "'int'", ")", "for", "col", "in", "columns", "}", ")", "\n", "\n", "", "indivs", "=", "(", "indivs", "\n", ".", "pipe", "(", "as_int", ",", "[", "'duration'", ",", "'duration_censor'", ",", "'duration_lcd'", ",", "'duration_censor_lcd'", "]", ")", "\n", ".", "pipe", "(", "as_category", ",", "[", "'churn_type'", "]", ")", ")", "\n", "indivs", ".", "reset_index", "(", "drop", "=", "True", ")", ".", "to_feather", "(", "self", ".", "path_survival", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._make_survival_covariates": [[730, 793], ["pandas.read_feather", "pandas.read_feather", "pandas.read_feather.merge().drop().drop_duplicates", "trans.assign().reset_index.assign().reset_index.merge().assign().drop().assign", "trans.assign().reset_index.assign().reset_index.assign", "pandas.testing.assert_frame_equal", "trans.assign().reset_index.assign().reset_index.loc[].assign().assign", "trans.assign().reset_index.assign().reset_index.assign", "trans.assign().reset_index.assign().reset_index.assign", "trans.assign().reset_index.assign().reset_index.drop", "trans.assign().reset_index.assign().reset_index.assign", "trans.assign().reset_index.assign().reset_index.assign", "trans.assign().reset_index.assign().reset_index.assign", "trans.assign().reset_index.assign().reset_index.assign().reset_index", "trans.assign().reset_index.assign().reset_index.to_feather", "pandas.datetime", "numpy.round", "pandas.read_feather.merge().drop", "trans.assign().reset_index.assign().reset_index.merge().assign().drop", "trans.assign().reset_index.assign().reset_index.loc[].assign", "trans.assign().reset_index.assign().reset_index.assign", "x[].clip", "x[].fillna", "pandas.read_feather.registration_init_time.isnull", "pandas.read_feather.bd.isnull", "x[].fillna", "x[].fillna", "trans[].astype", "trans[].cat.remove_unused_categories", "trans[].round().astype", "pandas.read_feather.merge", "trans.assign().reset_index.assign().reset_index.merge().assign", "x[].isnull", "x[].isnull", "list", "pandas.read_feather", "x[].isnull", "trans[].round", "trans.assign().reset_index.assign().reset_index.columns.drop", "trans.assign().reset_index.assign().reset_index.merge", "pandas.read_feather.drop", "from_kkbox._DatasetKKBoxAdmin._make_survival_covariates.get_age_at_start"], "methods", ["None"], ["", "def", "_make_survival_covariates", "(", "self", ")", ":", "\n", "        ", "individs", "=", "pd", ".", "read_feather", "(", "self", ".", "path_survival", ")", "\n", "members", "=", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'members.feather'", ")", "\n", "\n", "trans", "=", "(", "individs", "\n", ".", "merge", "(", "pd", ".", "read_feather", "(", "self", ".", "_path_dir", "/", "'transactions.feather'", ")", ",", "\n", "how", "=", "'left'", ",", "left_on", "=", "[", "'msno'", ",", "'start_date'", "]", ",", "right_on", "=", "[", "'msno'", ",", "'transaction_date'", "]", ")", "\n", ".", "drop", "(", "[", "'transaction_date'", "]", ",", "axis", "=", "1", ")", "# same as start_date", "\n", ".", "drop_duplicates", "(", "[", "'msno'", ",", "'start_date'", "]", ",", "keep", "=", "'last'", ")", "# keep last transaction on start_date (by idx)", "\n", ")", "\n", "assert", "trans", ".", "shape", "[", "0", "]", "==", "individs", ".", "shape", "[", "0", "]", "\n", "\n", "def", "get_age_at_start", "(", "df", ")", ":", "\n", "            ", "fixed_date", "=", "pd", ".", "datetime", "(", "2017", ",", "3", ",", "1", ")", "\n", "# Not important what the date is, though it is reasonable to use the last.", "\n", "age_diff", "=", "(", "fixed_date", "-", "df", "[", "'start_date'", "]", ")", ".", "dt", ".", "total_seconds", "(", ")", "/", "(", "60", "*", "60", "*", "24", "*", "365", ")", "\n", "return", "np", ".", "round", "(", "df", "[", "'bd'", "]", "-", "age_diff", ")", "\n", "\n", "", "trans", "=", "(", "trans", "\n", ".", "merge", "(", "members", ".", "drop", "(", "[", "'registration_init_time'", "]", ",", "axis", "=", "1", ")", ",", "how", "=", "'left'", ",", "on", "=", "'msno'", ")", "\n", ".", "assign", "(", "age_at_start", "=", "lambda", "x", ":", "get_age_at_start", "(", "x", ")", ")", "\n", ".", "drop", "(", "[", "'bd'", "]", ",", "axis", "=", "1", ")", "\n", ".", "assign", "(", "strange_age", "=", "lambda", "x", ":", "(", "x", "[", "'age_at_start'", "]", "<=", "0", ")", "|", "(", "x", "[", "'age_at_start'", "]", ">=", "100", ")", ",", "\n", "age_at_start", "=", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "100", ")", ")", ")", "\n", "# days_between_subs", "\n", "# There are None for (not new start), so we can just set them to zero, and we don't need to include another variable (as it allready exists).", "\n", "trans", "=", "trans", ".", "assign", "(", "days_between_subs", "=", "lambda", "x", ":", "x", "[", "'days_between_subs'", "]", ".", "fillna", "(", "0.", ")", ")", "\n", "# days_since_reg_init", "\n", "# We remove negative entries, set Nans to -1, and add a categorical value for missing.", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", "]", ",", "\n", "trans", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "isnull", "(", ")", "]", ")", "\n", "assert", "(", "members", ".", "registration_init_time", ".", "isnull", "(", ")", "==", "members", ".", "bd", ".", "isnull", "(", ")", ")", ".", "all", "(", ")", "\n", "\n", "trans", "=", "(", "trans", "\n", ".", "loc", "[", "lambda", "x", ":", "(", "x", "[", "'days_since_reg_init'", "]", ">=", "0", ")", "|", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", "]", "\n", ".", "assign", "(", "nan_days_since_reg_init", "=", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "isnull", "(", ")", ")", "\n", ".", "assign", "(", "days_since_reg_init", "=", "lambda", "x", ":", "x", "[", "'days_since_reg_init'", "]", ".", "fillna", "(", "-", "1", ")", ")", ")", "\n", "# age_at_start", "\n", "# This is Nan when days_since_reg_init is nan. This is because registration_init_time is nan when bd is nan.", "\n", "# We have removed negative entries, so we set Nans to -1, but don't add dummy because its equal to days_since_reg_init dummy.", "\n", "trans", "=", "trans", ".", "assign", "(", "age_at_start", "=", "lambda", "x", ":", "x", "[", "'age_at_start'", "]", ".", "fillna", "(", "-", "1.", ")", ")", "\n", "# We use n_prev_churns == 0 as an indicator that there are no previous churn", "\n", "trans", "=", "(", "trans", "\n", ".", "assign", "(", "no_prev_churns", "=", "lambda", "x", ":", "x", "[", "'n_prev_churns'", "]", "==", "0", ")", ")", "\n", "# Drop variables that are not useful", "\n", "trans", "=", "(", "trans", "\n", ".", "drop", "(", "[", "'start_date'", ",", "'registration_init_time'", ",", "'churn_type'", ",", "\n", "'membership_expire_date'", "]", ",", "\n", "axis", "=", "1", ")", ")", "\n", "\n", "bool_cols", "=", "[", "'is_auto_renew'", ",", "'is_cancel'", "]", "\n", "cat_cols", "=", "[", "'payment_method_id'", ",", "'city'", ",", "'gender'", ",", "'registered_via'", "]", "\n", "int_cols", "=", "[", "'days_between_subs'", ",", "'days_since_reg_init'", ",", "'age_at_start'", "]", "\n", "trans", "=", "trans", ".", "assign", "(", "**", "{", "col", ":", "trans", "[", "col", "]", ".", "astype", "(", "'bool'", ")", "for", "col", "in", "bool_cols", "}", ")", "\n", "trans", "=", "trans", ".", "assign", "(", "**", "{", "col", ":", "trans", "[", "col", "]", ".", "cat", ".", "remove_unused_categories", "(", ")", "for", "col", "in", "cat_cols", "}", ")", "\n", "trans", "=", "trans", ".", "assign", "(", "**", "{", "col", ":", "trans", "[", "col", "]", ".", "round", "(", ")", ".", "astype", "(", "'int'", ")", "for", "col", "in", "int_cols", "}", ")", "\n", "\n", "trans", "=", "trans", ".", "assign", "(", "index_survival", "=", "trans", ".", "index", ".", "values", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "last_cols", "=", "[", "'duration'", ",", "'churn'", ",", "'duration_censor'", ",", "'duration_lcd'", ",", "'churn_lcd'", ",", "'duration_censor_lcd'", "]", "\n", "first_cols", "=", "[", "'index_survival'", ",", "'msno'", "]", "\n", "trans", "=", "trans", "[", "first_cols", "+", "list", "(", "trans", ".", "columns", ".", "drop", "(", "first_cols", "+", "last_cols", ")", ")", "+", "last_cols", "]", "\n", "\n", "trans", ".", "to_feather", "(", "self", ".", "path_covariates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin._clean_up": [[794, 805], ["from_kkbox._DatasetKKBoxAdmin._path_dir.iterdir", "file.unlink", "warnings.warn"], "methods", ["None"], ["", "def", "_clean_up", "(", "self", ")", ":", "\n", "        ", "remove", "=", "[", "'train.csv.7z'", ",", "'transactions.csv.7z'", ",", "'members_v3.csv.7z'", ",", "\n", "'train.csv'", ",", "'transactions.csv'", ",", "'members_v3.csv'", ",", "\n", "'train_raw.feather'", ",", "'transactions.feather'", ",", "'members.feather'", "]", "\n", "for", "file", "in", "self", ".", "_path_dir", ".", "iterdir", "(", ")", ":", "\n", "            ", "if", "file", ".", "name", "not", "in", "remove", ":", "\n", "                ", "continue", "\n", "", "try", ":", "\n", "                ", "file", ".", "unlink", "(", ")", "\n", "", "except", "IsADirectoryError", ":", "\n", "                ", "warnings", ".", "warn", "(", "f\"Encountered directory in {self._path_dir}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_kkbox._DatasetKKBoxAdmin.delete_local_copy": [[806, 809], ["path.unlink"], "methods", ["None"], ["", "", "", "def", "delete_local_copy", "(", "self", ")", ":", "\n", "        ", "for", "path", "in", "[", "self", ".", "path_covariates", ",", "self", ".", "path_survival", "]", ":", "\n", "            ", "path", ".", "unlink", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SimDataset.read_df": [[14, 23], ["super().read_df", "from_simulations._SimDataset.path.exists", "print", "from_simulations._SimDataset._simulate_data", "print", "from_simulations._SimDataset._drop_true"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.read_df", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SACAdmin5._simulate_data", "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SimDataset._drop_true"], ["def", "read_df", "(", "self", ",", "add_true", "=", "True", ")", ":", "\n", "        ", "if", "not", "self", ".", "path", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "f\"Dataset '{self.name}' not created yet. Making dataset...\"", ")", "\n", "self", ".", "_simulate_data", "(", ")", "\n", "print", "(", "f\"Done\"", ")", "\n", "", "df", "=", "super", "(", ")", ".", "read_df", "(", ")", "\n", "if", "add_true", "is", "False", ":", "\n", "            ", "df", "=", "self", ".", "_drop_true", "(", "df", ")", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SimDataset._simulate_data": [[24, 26], ["None"], "methods", ["None"], ["", "def", "_simulate_data", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SimDataset._download": [[27, 29], ["NotImplementedError"], "methods", ["None"], ["", "def", "_download", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"There is no `_download` for simulated data.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SimDataset._drop_true": [[30, 32], ["df.drop"], "methods", ["None"], ["", "def", "_drop_true", "(", "self", ",", "df", ")", ":", "\n", "        ", "return", "df", ".", "drop", "(", "columns", "=", "self", ".", "cols_true", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._RRNLNPH._simulate_data": [[75, 81], ["numpy.random.seed", "pycox.simulations.SimStudyNonLinearNonPH", "pycox.simulations.SimStudyNonLinearNonPH.simulate", "pycox.simulations.SimStudyNonLinearNonPH.dict2df", "simulations.SimStudyNonLinearNonPH.dict2df.to_feather"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["def", "_simulate_data", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "1234", ")", "\n", "sim", "=", "simulations", ".", "SimStudyNonLinearNonPH", "(", ")", "\n", "data", "=", "sim", ".", "simulate", "(", "25000", ")", "\n", "df", "=", "sim", ".", "dict2df", "(", "data", ",", "True", ")", "\n", "df", ".", "to_feather", "(", "self", ".", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SAC3._simulate_data": [[121, 127], ["numpy.random.seed", "pycox.simulations.SimStudySACCensorConst", "pycox.simulations.SimStudySACCensorConst.simulate", "pycox.simulations.SimStudySACCensorConst.dict2df", "simulations.SimStudySACCensorConst.dict2df.to_feather"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["def", "_simulate_data", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "1234", ")", "\n", "sim", "=", "simulations", ".", "SimStudySACCensorConst", "(", ")", "\n", "data", "=", "sim", ".", "simulate", "(", "100000", ")", "\n", "df", "=", "sim", ".", "dict2df", "(", "data", ",", "True", ",", "False", ")", "\n", "df", ".", "to_feather", "(", "self", ".", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_simulations._SACAdmin5._simulate_data": [[163, 169], ["numpy.random.seed", "pycox.simulations.SimStudySACAdmin", "pycox.simulations.SimStudySACAdmin.simulate", "pycox.simulations.SimStudySACAdmin.dict2df", "simulations.SimStudySACAdmin.dict2df.to_feather"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySingleSurv.simulate", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.SimStudySACAdmin.dict2df"], ["def", "_simulate_data", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "1234", ")", "\n", "sim", "=", "simulations", ".", "SimStudySACAdmin", "(", ")", "\n", "data", "=", "sim", ".", "simulate", "(", "50000", ")", "\n", "df", "=", "sim", ".", "dict2df", "(", "data", ",", "True", ",", "True", ")", "\n", "df", ".", "to_feather", "(", "self", ".", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_rdatasets._DatasetRdatasetsSurvival._download": [[17, 21], ["from_rdatasets.download_from_rdatasets", "df.to_feather"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets.from_rdatasets.download_from_rdatasets"], ["def", "_download", "(", "self", ")", ":", "\n", "        ", "df", ",", "info", "=", "download_from_rdatasets", "(", "'survival'", ",", "self", ".", "name", ")", "\n", "self", ".", "info", "=", "info", "\n", "df", ".", "to_feather", "(", "self", ".", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_rdatasets._Flchain.read_df": [[65, 89], ["super().read_df", "df.drop().loc[].reset_index().assign.drop().loc[].reset_index().assign.drop().loc[].reset_index().assign", "df.drop().loc[].reset_index().assign.drop().loc[].reset_index().assign.columns.drop", "df[].astype", "df[].astype", "df.drop().loc[].reset_index().assign.drop().loc[].reset_index().assign.drop().loc[].reset_index", "df.drop().loc[].reset_index().assign.drop().loc[].reset_index().assign.drop", "x[].isna"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.read_df"], ["def", "read_df", "(", "self", ",", "processed", "=", "True", ")", ":", "\n", "        ", "\"\"\"Get dataset.\n\n        If 'processed' is False, return the raw data set.\n        See the code for processing.\n\n        Keyword Arguments:\n            processed {bool} -- If 'False' get raw data, else get processed (see '??flchain.read_df').\n                (default: {True})\n        \"\"\"", "\n", "df", "=", "super", "(", ")", ".", "read_df", "(", ")", "\n", "if", "processed", ":", "\n", "            ", "df", "=", "(", "df", "\n", ".", "drop", "(", "[", "'chapter'", ",", "'Unnamed: 0'", "]", ",", "axis", "=", "1", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'creatinine'", "]", ".", "isna", "(", ")", "==", "False", "]", "\n", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", ".", "assign", "(", "sex", "=", "lambda", "x", ":", "(", "x", "[", "'sex'", "]", "==", "'M'", ")", ")", ")", "\n", "\n", "categorical", "=", "[", "'sample.yr'", ",", "'flc.grp'", "]", "\n", "for", "col", "in", "categorical", ":", "\n", "                ", "df", "[", "col", "]", "=", "df", "[", "col", "]", ".", "astype", "(", "'category'", ")", "\n", "", "for", "col", "in", "df", ".", "columns", ".", "drop", "(", "categorical", ")", ":", "\n", "                ", "df", "[", "col", "]", "=", "df", "[", "col", "]", ".", "astype", "(", "'float32'", ")", "\n", "", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_rdatasets._Nwtco.read_df": [[129, 151], ["super().read_df", "from_rdatasets._Nwtco.assign().drop", "from_rdatasets._Nwtco.columns.drop", "from_rdatasets._Nwtco._label_cols_at_end", "df[].astype", "from_rdatasets._Nwtco.assign", "df[].astype"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.read_df", "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._label_cols_at_end"], ["def", "read_df", "(", "self", ",", "processed", "=", "True", ")", ":", "\n", "        ", "\"\"\"Get dataset.\n\n        If 'processed' is False, return the raw data set.\n        See the code for processing.\n\n        Keyword Arguments:\n            processed {bool} -- If 'False' get raw data, else get processed (see '??nwtco.read_df').\n                (default: {True})\n        \"\"\"", "\n", "df", "=", "super", "(", ")", ".", "read_df", "(", ")", "\n", "if", "processed", ":", "\n", "            ", "df", "=", "(", "df", "\n", ".", "assign", "(", "instit_2", "=", "df", "[", "'instit'", "]", "-", "1", ",", "\n", "histol_2", "=", "df", "[", "'histol'", "]", "-", "1", ",", "\n", "study_4", "=", "df", "[", "'study'", "]", "-", "3", ",", "\n", "stage", "=", "df", "[", "'stage'", "]", ".", "astype", "(", "'category'", ")", ")", "\n", ".", "drop", "(", "[", "'Unnamed: 0'", ",", "'seqno'", ",", "'instit'", ",", "'histol'", ",", "'study'", "]", ",", "axis", "=", "1", ")", ")", "\n", "for", "col", "in", "df", ".", "columns", ".", "drop", "(", "'stage'", ")", ":", "\n", "                ", "df", "[", "col", "]", "=", "df", "[", "col", "]", ".", "astype", "(", "'float32'", ")", "\n", "", "df", "=", "self", ".", "_label_cols_at_end", "(", "df", ")", "\n", "", "return", "df", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.datasets.from_rdatasets.download_from_rdatasets": [[4, 12], ["pandas.read_csv().loc[].set_index", "ValueError", "pandas.read_csv", "pandas.read_csv"], "function", ["None"], ["def", "download_from_rdatasets", "(", "package", ",", "name", ")", ":", "\n", "    ", "datasets", "=", "(", "pd", ".", "read_csv", "(", "\"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/datasets.csv\"", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'Package'", "]", "==", "package", "]", ".", "set_index", "(", "'Item'", ")", ")", "\n", "if", "not", "name", "in", "datasets", ".", "index", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Dataset {name} not found.\"", ")", "\n", "", "info", "=", "datasets", ".", "loc", "[", "name", "]", "\n", "url", "=", "info", ".", "CSV", "\n", "return", "pd", ".", "read_csv", "(", "url", ")", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.__init__": [[20, 22], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "path", "=", "_PATH_DATA", "/", "f\"{self.name}.feather\"", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.read_df": [[23, 31], ["pandas.read_feather", "_dataset_loader._DatasetLoader._label_cols_at_end", "_dataset_loader._DatasetLoader.path.exists", "print", "_dataset_loader._DatasetLoader._download", "print"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._label_cols_at_end", "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._download"], ["", "def", "read_df", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "path", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "f\"Dataset '{self.name}' not locally available. Downloading...\"", ")", "\n", "self", ".", "_download", "(", ")", "\n", "print", "(", "f\"Done\"", ")", "\n", "", "df", "=", "pd", ".", "read_feather", "(", "self", ".", "path", ")", "\n", "df", "=", "self", ".", "_label_cols_at_end", "(", "df", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._download": [[32, 34], ["None"], "methods", ["None"], ["", "def", "_download", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.delete_local_copy": [[35, 39], ["_dataset_loader._DatasetLoader.path.unlink", "_dataset_loader._DatasetLoader.path.exists", "RuntimeError"], "methods", ["None"], ["", "def", "delete_local_copy", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "path", ".", "exists", "(", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"File does not exists.\"", ")", "\n", "", "self", ".", "path", ".", "unlink", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._label_cols_at_end": [[40, 45], ["hasattr", "hasattr", "list", "df.columns.drop"], "methods", ["None"], ["", "def", "_label_cols_at_end", "(", "self", ",", "df", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "'col_duration'", ")", "and", "hasattr", "(", "self", ",", "'col_event'", ")", ":", "\n", "            ", "col_label", "=", "[", "self", ".", "col_duration", ",", "self", ".", "col_event", "]", "\n", "df", "=", "df", "[", "list", "(", "df", ".", "columns", ".", "drop", "(", "col_label", ")", ")", "+", "col_label", "]", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.checksum": [[46, 56], ["_dataset_loader._DatasetLoader.read_df", "_dataset_loader._DatasetLoader._checksum_df", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader.read_df", "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._checksum_df"], ["", "def", "checksum", "(", "self", ")", ":", "\n", "        ", "\"\"\"Checks that the dataset is correct. \n        \n        Returns:\n            bool -- If the check passed.\n        \"\"\"", "\n", "if", "self", ".", "_checksum", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"No available comparison for this dataset.\"", ")", "\n", "", "df", "=", "self", ".", "read_df", "(", ")", "\n", "return", "self", ".", "_checksum_df", "(", "df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader._DatasetLoader._checksum_df": [[57, 63], ["_dataset_loader.get_checksum", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader.get_checksum"], ["", "def", "_checksum_df", "(", "self", ",", "df", ")", ":", "\n", "        ", "if", "self", ".", "_checksum", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"No available comparison for this dataset.\"", ")", "\n", "", "import", "hashlib", "\n", "val", "=", "get_checksum", "(", "df", ")", "\n", "return", "val", "==", "self", ".", "_checksum", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.datasets._dataset_loader.get_checksum": [[65, 69], ["hashlib.sha256().hexdigest", "hashlib.sha256", "df.to_csv().encode", "df.to_csv"], "function", ["None"], ["", "", "def", "get_checksum", "(", "df", ")", ":", "\n", "    ", "import", "hashlib", "\n", "val", "=", "hashlib", ".", "sha256", "(", "df", ".", "to_csv", "(", ")", ".", "encode", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "return", "val", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime.__init__": [[39, 42], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "shrink", "=", "0.", ",", "labtrans", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "self", ".", "labtrans", "=", "labtrans", "\n", "super", "(", ")", ".", "__init__", "(", "net", ",", "optimizer", ",", "device", ",", "shrink", ",", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime.make_dataloader_predict": [[43, 50], ["torchtuples.tuplefy", "torchtuples.tuplefy", "super().make_dataloader_predict"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader_predict"], ["", "def", "make_dataloader_predict", "(", "self", ",", "input", ",", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "input", ",", "durations", "=", "input", "\n", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", "\n", "durations", "=", "tt", ".", "tuplefy", "(", "durations", ")", "\n", "new_input", "=", "input", "+", "durations", "\n", "dataloader", "=", "super", "(", ")", ".", "make_dataloader_predict", "(", "new_input", ",", "batch_size", ",", "shuffle", ",", "num_workers", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime.predict_surv_df": [[51, 58], ["super().predict_surv_df", "cox_time.CoxTime.labtrans.map_scaled_to_orig"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.label_transforms.LabTransCoxTime.map_scaled_to_orig"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "max_duration", "=", "None", ",", "batch_size", "=", "8224", ",", "verbose", "=", "False", ",", "baseline_hazards_", "=", "None", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "surv", "=", "super", "(", ")", ".", "predict_surv_df", "(", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", ",", "num_workers", ")", "\n", "if", "self", ".", "labtrans", "is", "not", "None", ":", "\n", "            ", "surv", ".", "index", "=", "self", ".", "labtrans", ".", "map_scaled_to_orig", "(", "surv", ".", "index", ")", "\n", "", "return", "surv", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime.compute_baseline_hazards": [[59, 77], ["cox_time.CoxTime.target_to_df", "cox_time.CoxTime._compute_baseline_hazards", "df.sample.sample.sort_values", "cox_time.CoxTime.compute_baseline_cumulative_hazards", "hasattr", "ValueError", "df.sample.sample.sample", "df.sample.sample.sample", "torchtuples.tuplefy().to_numpy", "torchtuples.tuplefy"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.target_to_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase._compute_baseline_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_cumulative_hazards"], ["", "def", "compute_baseline_hazards", "(", "self", ",", "input", "=", "None", ",", "target", "=", "None", ",", "max_duration", "=", "None", ",", "sample", "=", "None", ",", "batch_size", "=", "8224", ",", "\n", "set_hazards", "=", "True", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "(", "input", "is", "None", ")", "and", "(", "target", "is", "None", ")", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'training_data'", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'Need to fit, or supply a input and target to this function.'", ")", "\n", "", "input", ",", "target", "=", "self", ".", "training_data", "\n", "", "df", "=", "self", ".", "target_to_df", "(", "target", ")", "\n", "if", "sample", "is", "not", "None", ":", "\n", "            ", "if", "sample", ">=", "1", ":", "\n", "                ", "df", "=", "df", ".", "sample", "(", "n", "=", "sample", ")", "\n", "", "else", ":", "\n", "                ", "df", "=", "df", ".", "sample", "(", "frac", "=", "sample", ")", "\n", "", "df", "=", "df", ".", "sort_values", "(", "self", ".", "duration_col", ")", "\n", "", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "to_numpy", "(", ")", ".", "iloc", "[", "df", ".", "index", ".", "values", "]", "\n", "base_haz", "=", "self", ".", "_compute_baseline_hazards", "(", "input", ",", "df", ",", "max_duration", ",", "batch_size", ",", "eval_", ",", "num_workers", ")", "\n", "if", "set_hazards", ":", "\n", "            ", "self", ".", "compute_baseline_cumulative_hazards", "(", "set_hazards", "=", "True", ",", "baseline_hazards_", "=", "base_haz", ")", "\n", "", "return", "base_haz", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime._compute_baseline_hazards": [[78, 111], ["torchtuples.tuplefy", "df_train_target.reset_index", "[].loc[].drop_duplicates", "pandas.Series().rename", "events.join().pipe().fillna().rename", "sub.lens().flatten().get_if_all_equal", "numpy.repeat().reshape().astype", "numpy.exp().flatten().sum", "RuntimeError", "pandas.Series", "[].agg", "events.join().pipe().fillna", "sub.lens().flatten", "numpy.repeat().reshape", "numpy.exp().flatten", "cox_time.CoxTime._compute_baseline_hazards.compute_expg_at_risk"], "methods", ["None"], ["", "def", "_compute_baseline_hazards", "(", "self", ",", "input", ",", "df_train_target", ",", "max_duration", ",", "batch_size", ",", "eval_", "=", "True", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "max_duration", "is", "None", ":", "\n", "            ", "max_duration", "=", "np", ".", "inf", "\n", "", "def", "compute_expg_at_risk", "(", "ix", ",", "t", ")", ":", "\n", "            ", "sub", "=", "input", ".", "iloc", "[", "ix", ":", "]", "\n", "n", "=", "sub", ".", "lens", "(", ")", ".", "flatten", "(", ")", ".", "get_if_all_equal", "(", ")", "\n", "t", "=", "np", ".", "repeat", "(", "t", ",", "n", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "return", "np", ".", "exp", "(", "self", ".", "predict", "(", "(", "sub", ",", "t", ")", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ")", ".", "flatten", "(", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "not", "df_train_target", "[", "self", ".", "duration_col", "]", ".", "is_monotonic_increasing", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Need 'df_train_target' to be sorted by {self.duration_col}\"", ")", "\n", "", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", "\n", "df", "=", "df_train_target", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "times", "=", "(", "df", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "!=", "0", "]", "\n", "[", "self", ".", "duration_col", "]", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "<=", "max_duration", "]", "\n", ".", "drop_duplicates", "(", "keep", "=", "'first'", ")", ")", "\n", "at_risk_sum", "=", "(", "pd", ".", "Series", "(", "[", "compute_expg_at_risk", "(", "ix", ",", "t", ")", "for", "ix", ",", "t", "in", "times", ".", "iteritems", "(", ")", "]", ",", "\n", "index", "=", "times", ".", "values", ")", "\n", ".", "rename", "(", "'at_risk_sum'", ")", ")", "\n", "events", "=", "(", "df", "\n", ".", "groupby", "(", "self", ".", "duration_col", ")", "\n", "[", "[", "self", ".", "event_col", "]", "]", "\n", ".", "agg", "(", "'sum'", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", ".", "index", "<=", "max_duration", "]", ")", "\n", "base_haz", "=", "(", "events", "\n", ".", "join", "(", "at_risk_sum", ",", "how", "=", "'left'", ",", "sort", "=", "True", ")", "\n", ".", "pipe", "(", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "/", "x", "[", "'at_risk_sum'", "]", ")", "\n", ".", "fillna", "(", "0.", ")", "\n", ".", "rename", "(", "'baseline_hazards'", ")", ")", "\n", "return", "base_haz", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime._predict_cumulative_hazards": [[112, 134], ["torchtuples.utils.is_dl", "torchtuples.tuplefy", "numpy.empty", "enumerate", "baseline_hazards_.values.reshape", "pandas.DataFrame().cumsum", "numpy.repeat().reshape().astype", "numpy.exp().flatten", "NotImplementedError", "torchtuples.tuplefy.lens().flatten().get_if_all_equal", "cox_time.CoxTime._predict_cumulative_hazards.expg_at_time"], "methods", ["None"], ["", "def", "_predict_cumulative_hazards", "(", "self", ",", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "def", "expg_at_time", "(", "t", ")", ":", "\n", "            ", "t", "=", "np", ".", "repeat", "(", "t", ",", "n_cols", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "if", "tt", ".", "tuplefy", "(", "input", ")", ".", "type", "(", ")", "is", "torch", ".", "Tensor", ":", "\n", "                ", "t", "=", "torch", ".", "from_numpy", "(", "t", ")", "\n", "", "return", "np", ".", "exp", "(", "self", ".", "predict", "(", "(", "input", ",", "t", ")", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ")", ".", "flatten", "(", ")", "\n", "\n", "", "if", "tt", ".", "utils", ".", "is_dl", "(", "input", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "f\"Prediction with a dataloader as input is not supported \"", ")", "\n", "", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", "\n", "max_duration", "=", "np", ".", "inf", "if", "max_duration", "is", "None", "else", "max_duration", "\n", "baseline_hazards_", "=", "baseline_hazards_", ".", "loc", "[", "lambda", "x", ":", "x", ".", "index", "<=", "max_duration", "]", "\n", "n_rows", ",", "n_cols", "=", "baseline_hazards_", ".", "shape", "[", "0", "]", ",", "input", ".", "lens", "(", ")", ".", "flatten", "(", ")", ".", "get_if_all_equal", "(", ")", "\n", "hazards", "=", "np", ".", "empty", "(", "(", "n_rows", ",", "n_cols", ")", ")", "\n", "for", "idx", ",", "t", "in", "enumerate", "(", "baseline_hazards_", ".", "index", ")", ":", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "idx", ",", "'of'", ",", "len", "(", "baseline_hazards_", ")", ")", "\n", "", "hazards", "[", "idx", ",", ":", "]", "=", "expg_at_time", "(", "t", ")", "\n", "", "hazards", "[", "baseline_hazards_", ".", "values", "==", "0", "]", "=", "0.", "# in case hazards are inf here", "\n", "hazards", "*=", "baseline_hazards_", ".", "values", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "return", "pd", ".", "DataFrame", "(", "hazards", ",", "index", "=", "baseline_hazards_", ".", "index", ")", ".", "cumsum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.CoxTime.partial_log_likelihood": [[135, 173], ["pandas.DataFrame", "df.sort_values.sort_values.sort_values", "torchtuples.tuplefy", "df.sort_values.sort_values.assign().loc[].drop_duplicates().assign().drop", "pll[].values.reshape", "cox_time.CoxTime.predict().flatten", "sub.lens().flatten().get_if_all_equal", "numpy.repeat().reshape().astype", "numpy.exp().flatten().sum", "pll.assign().reset_index().merge().set_index().assign", "df.sort_values.sort_values.assign().loc[].drop_duplicates().assign", "cox_time.CoxTime.predict", "sub.lens().flatten", "numpy.repeat().reshape", "numpy.exp().flatten", "pll.assign().reset_index().merge().set_index", "df.sort_values.sort_values.assign().loc[].drop_duplicates", "sub.lens", "numpy.repeat", "numpy.exp", "pll.assign().reset_index().merge", "numpy.log", "cox_time.CoxTime.predict", "cox_time.CoxTime.partial_log_likelihood.expg_sum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict"], ["", "def", "partial_log_likelihood", "(", "self", ",", "input", ",", "target", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "def", "expg_sum", "(", "t", ",", "i", ")", ":", "\n", "            ", "sub", "=", "input_sorted", ".", "iloc", "[", "i", ":", "]", "\n", "n", "=", "sub", ".", "lens", "(", ")", ".", "flatten", "(", ")", ".", "get_if_all_equal", "(", ")", "\n", "t", "=", "np", ".", "repeat", "(", "t", ",", "n", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "astype", "(", "'float32'", ")", "\n", "return", "np", ".", "exp", "(", "self", ".", "predict", "(", "(", "sub", ",", "t", ")", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ")", ".", "flatten", "(", ")", ".", "sum", "(", ")", "\n", "\n", "", "durations", ",", "events", "=", "target", "\n", "df", "=", "pd", ".", "DataFrame", "(", "{", "self", ".", "duration_col", ":", "durations", ",", "self", ".", "event_col", ":", "events", "}", ")", "\n", "df", "=", "df", ".", "sort_values", "(", "self", ".", "duration_col", ")", "\n", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", "\n", "input_sorted", "=", "input", ".", "iloc", "[", "df", ".", "index", ".", "values", "]", "\n", "\n", "times", "=", "(", "df", "\n", ".", "assign", "(", "_idx", "=", "np", ".", "arange", "(", "len", "(", "df", ")", ")", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "==", "True", "]", "\n", ".", "drop_duplicates", "(", "self", ".", "duration_col", ",", "keep", "=", "'first'", ")", "\n", ".", "assign", "(", "_expg_sum", "=", "lambda", "x", ":", "[", "expg_sum", "(", "t", ",", "i", ")", "for", "t", ",", "i", "in", "zip", "(", "x", "[", "self", ".", "duration_col", "]", ",", "x", "[", "'_idx'", "]", ")", "]", ")", "\n", ".", "drop", "(", "[", "self", ".", "event_col", ",", "'_idx'", "]", ",", "axis", "=", "1", ")", ")", "\n", "\n", "idx_name_old", "=", "df", ".", "index", ".", "name", "\n", "idx_name", "=", "'__'", "+", "idx_name_old", "if", "idx_name_old", "else", "'__index'", "\n", "df", ".", "index", ".", "name", "=", "idx_name", "\n", "\n", "pll", "=", "df", ".", "loc", "[", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "==", "True", "]", "\n", "input_event", "=", "input", ".", "iloc", "[", "pll", ".", "index", ".", "values", "]", "\n", "durations_event", "=", "pll", "[", "self", ".", "duration_col", "]", ".", "values", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "g_preds", "=", "self", ".", "predict", "(", "(", "input_event", ",", "durations_event", ")", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ".", "flatten", "(", ")", "\n", "pll", "=", "(", "pll", "\n", ".", "assign", "(", "_g_preds", "=", "g_preds", ")", "\n", ".", "reset_index", "(", ")", "\n", ".", "merge", "(", "times", ",", "on", "=", "self", ".", "duration_col", ")", "\n", ".", "set_index", "(", "idx_name", ")", "\n", ".", "assign", "(", "pll", "=", "lambda", "x", ":", "x", "[", "'_g_preds'", "]", "-", "np", ".", "log", "(", "x", "[", "'_expg_sum'", "]", ")", ")", "\n", "[", "'pll'", "]", ")", "\n", "\n", "pll", ".", "index", ".", "name", "=", "idx_name_old", "\n", "return", "pll", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.MLPVanillaCoxTime.__init__": [[180, 189], ["torch.nn.Module.__init__", "torchtuples.practical.MLPVanilla", "torch.nn.init.kaiming_normal_"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "num_nodes", ",", "batch_norm", "=", "True", ",", "dropout", "=", "None", ",", "activation", "=", "nn", ".", "ReLU", ",", "\n", "w_init_", "=", "lambda", "w", ":", "nn", ".", "init", ".", "kaiming_normal_", "(", "w", ",", "nonlinearity", "=", "'relu'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "in_features", "+=", "1", "\n", "out_features", "=", "1", "\n", "output_activation", "=", "None", "\n", "output_bias", "=", "False", "\n", "self", ".", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "in_features", ",", "num_nodes", ",", "out_features", ",", "batch_norm", ",", "dropout", ",", "\n", "activation", ",", "output_activation", ",", "output_bias", ",", "w_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.MLPVanillaCoxTime.forward": [[190, 193], ["torch.cat", "cox_time.MLPVanillaCoxTime.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "time", ")", ":", "\n", "        ", "input", "=", "torch", ".", "cat", "(", "[", "input", ",", "time", "]", ",", "dim", "=", "1", ")", "\n", "return", "self", ".", "net", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.MixedInputMLPCoxTime.__init__": [[200, 211], ["torch.nn.Module.__init__", "torchtuples.practical.MixedInputMLP", "torch.nn.init.kaiming_normal_"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "num_embeddings", ",", "embedding_dims", ",", "num_nodes", ",", "batch_norm", "=", "True", ",", "\n", "dropout", "=", "None", ",", "activation", "=", "nn", ".", "ReLU", ",", "dropout_embedding", "=", "0.", ",", "\n", "w_init_", "=", "lambda", "w", ":", "nn", ".", "init", ".", "kaiming_normal_", "(", "w", ",", "nonlinearity", "=", "'relu'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "in_features", "+=", "1", "\n", "out_features", "=", "1", "\n", "output_activation", "=", "None", "\n", "output_bias", "=", "False", "\n", "self", ".", "net", "=", "tt", ".", "practical", ".", "MixedInputMLP", "(", "in_features", ",", "num_embeddings", ",", "embedding_dims", ",", "num_nodes", ",", "\n", "out_features", ",", "batch_norm", ",", "dropout", ",", "activation", ",", "\n", "dropout_embedding", ",", "output_activation", ",", "output_bias", ",", "w_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_time.MixedInputMLPCoxTime.forward": [[212, 215], ["torch.cat", "cox_time.MixedInputMLPCoxTime.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_numeric", ",", "input_categoric", ",", "time", ")", ":", "\n", "        ", "input_numeric", "=", "torch", ".", "cat", "(", "[", "input_numeric", ",", "time", "]", ",", "dim", "=", "1", ")", "\n", "return", "self", ".", "net", "(", "input_numeric", ",", "input_categoric", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.bce_surv.BCESurv.__init__": [[41, 46], ["super().__init__", "pycox.models.loss.BCESurvLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "BCESurvLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.bce_surv.BCESurv.duration_index": [[58, 61], ["None"], "methods", ["None"], ["", "@", "duration_index", ".", "setter", "\n", "def", "duration_index", "(", "self", ",", "val", ")", ":", "\n", "        ", "self", ".", "_duration_index", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.bce_surv.BCESurv.predict_surv_df": [[62, 65], ["bce_surv.BCESurv.predict_surv", "pandas.DataFrame", "bce_surv.BCESurv.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ",", "is_dataloader", "=", "None", ")", ":", "\n", "        ", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "True", ",", "num_workers", ",", "is_dataloader", ")", "\n", "return", "pd", ".", "DataFrame", "(", "surv", ".", "transpose", "(", ")", ",", "self", ".", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.bce_surv.BCESurv.predict_surv": [[66, 70], ["bce_surv.BCESurv.predict"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "is_dataloader", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "numpy", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ",", "\n", "is_dataloader", ",", "torch", ".", "sigmoid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.bce_surv.BCESurv.interpolate": [[71, 90], ["pycox.models.interpolation.InterpolateDiscrete"], "methods", ["None"], ["", "def", "interpolate", "(", "self", ",", "sub", "=", "10", ",", "scheme", "=", "'const_pdf'", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"Use interpolation for predictions.\n        There is only one scheme:\n            `const_pdf` and `lin_surv` which assumes pice-wise constant PMF in each interval (linear survival).\n        \n        Keyword Arguments:\n            sub {int} -- Number of \"sub\" units in interpolation grid. If `sub` is 10 we have a grid with\n                10 times the number of grid points than the original `duration_index` (default: {10}).\n            scheme {str} -- Type of interpolation {'const_pdf'}.\n                See `InterpolateDiscrete` (default: {'const_pdf'})\n            duration_index {np.array} -- Cuts used for discretization. Does not affect interpolation,\n                only for setting index in `predict_surv_df` (default: {None})\n        \n        Returns:\n            [InterpolateLogisticHazard] -- Object for prediction with interpolation.\n        \"\"\"", "\n", "if", "duration_index", "is", "None", ":", "\n", "            ", "duration_index", "=", "self", ".", "duration_index", "\n", "", "return", "models", ".", "interpolation", ".", "InterpolateDiscrete", "(", "self", ",", "scheme", ",", "duration_index", ",", "sub", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.base.SurvBase.predict_surv": [[9, 30], ["None"], "methods", ["None"], ["\n", "covs", "=", "data", "[", "'covs'", "]", "\n", "if", "add_censor_covs", ":", "\n", "        ", "covs", "=", "np", ".", "concatenate", "(", "[", "covs", ",", "data", "[", "'censor_covs'", "]", "]", ",", "axis", "=", "1", ")", "\n", "", "df", "=", "(", "pd", ".", "DataFrame", "(", "covs", ",", "columns", "=", "[", "f\"x{i}\"", "for", "i", "in", "range", "(", "covs", ".", "shape", "[", "1", "]", ")", "]", ")", "\n", ".", "assign", "(", "duration", "=", "data", "[", "'durations'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "event", "=", "data", "[", "'events'", "]", ".", "astype", "(", "'float32'", ")", ")", ")", "\n", "if", "add_true", ":", "\n", "        ", "df", "=", "df", ".", "assign", "(", "duration_true", "=", "data", "[", "'durations_true'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "event_true", "=", "data", "[", "'events_true'", "]", ".", "astype", "(", "'float32'", ")", ",", "\n", "censoring_true", "=", "data", "[", "'censor_durations'", "]", ".", "astype", "(", "'float32'", ")", ")", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.base.SurvBase.predict_surv_df": [[31, 47], ["None"], "methods", ["None"], ["\n", "", "class", "_SimBase", ":", "\n", "    ", "def", "simulate", "(", "self", ",", "n", ",", "surv_df", "=", "False", ")", ":", "\n", "        ", "\"\"\"Simulate dataset of size `n`.\n        \n        Arguments:\n            n {int} -- Number of simulations\n        \n        Keyword Arguments:\n            surv_df {bool} -- If a dataframe containing the survival function should be returned.\n                (default: {False})\n        \n        Returns:\n            [dict] -- A dictionary with the results.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.base.SurvBase.predict_hazard": [[48, 69], ["None"], "methods", ["None"], ["", "def", "surv_df", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "\"\"\"Returns a data frame containing the survival function.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.base.SurvBase.predict_pmf": [[70, 91], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.havakv_pycox.models.base._SurvModelBase.__init__": [[97, 100], ["warnings.warn", "torchtuples.Model.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], []], "home.repos.pwc.inspect_result.havakv_pycox.models.base._SurvModelBase.predict_surv": [[101, 122], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.havakv_pycox.models.base._SurvModelBase.predict_surv_df": [[123, 139], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.__init__": [[25, 30], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "scheme", "=", "'const_pdf'", ",", "duration_index", "=", "None", ",", "sub", "=", "10", ",", "epsilon", "=", "1e-7", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "scheme", "=", "scheme", "\n", "self", ".", "duration_index", "=", "duration_index", "\n", "self", ".", "sub", "=", "sub", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.sub": [[35, 40], ["type", "ValueError", "type"], "methods", ["None"], ["", "@", "sub", ".", "setter", "\n", "def", "sub", "(", "self", ",", "sub", ")", ":", "\n", "        ", "if", "type", "(", "sub", ")", "is", "not", "int", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need `sub` to have type `int`, got {type(sub)}\"", ")", "\n", "", "self", ".", "_sub", "=", "sub", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.predict_hazard": [[41, 43], ["None"], "methods", ["None"], ["", "def", "predict_hazard", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.predict_pmf": [[44, 46], ["None"], "methods", ["None"], ["", "def", "predict_pmf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.predict_surv": [[47, 67], ["interpolation.InterpolateDiscrete._surv_const_pdf"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolatePMF._surv_const_pdf"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the survival function for `input`.\n        See `prediction_surv_df` to return a DataFrame instead.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            [np.ndarray or tensor] -- Predictions\n        \"\"\"", "\n", "return", "self", ".", "_surv_const_pdf", "(", "input", ",", "batch_size", ",", "numpy", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete._surv_const_pdf": [[68, 97], ["interpolation.InterpolateDiscrete.model.predict_surv", "[].contiguous().repeat", "s[].contiguous().view().repeat().view", "torch.zeros", "torchtuples.utils.array_or_tensor", "int", "[].contiguous", "s[].contiguous().view().repeat", "s[].contiguous().view", "torch.linspace", "s[].contiguous"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "_surv_const_pdf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Basic method for constant PDF interpolation that use `self.model.predict_surv`.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            [np.ndarray or tensor] -- Predictions\n        \"\"\"", "\n", "s", "=", "self", ".", "model", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "n", ",", "m", "=", "s", ".", "shape", "\n", "device", "=", "s", ".", "device", "\n", "diff", "=", "(", "s", "[", ":", ",", "1", ":", "]", "-", "s", "[", ":", ",", ":", "-", "1", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "sub", ")", ".", "view", "(", "n", ",", "-", "1", ")", "\n", "rho", "=", "torch", ".", "linspace", "(", "0", ",", "1", ",", "self", ".", "sub", "+", "1", ",", "device", "=", "device", ")", "[", ":", "-", "1", "]", ".", "contiguous", "(", ")", ".", "repeat", "(", "n", ",", "m", "-", "1", ")", "\n", "s_prev", "=", "s", "[", ":", ",", ":", "-", "1", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "sub", ")", ".", "view", "(", "n", ",", "-", "1", ")", "\n", "surv", "=", "torch", ".", "zeros", "(", "n", ",", "int", "(", "(", "m", "-", "1", ")", "*", "self", ".", "sub", "+", "1", ")", ")", "\n", "surv", "[", ":", ",", ":", "-", "1", "]", "=", "diff", "*", "rho", "+", "s_prev", "\n", "surv", "[", ":", ",", "-", "1", "]", "=", "s", "[", ":", ",", "-", "1", "]", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateDiscrete.predict_surv_df": [[98, 118], ["interpolation.InterpolateDiscrete.predict_surv", "pandas.DataFrame", "pycox.models.utils.make_subgrid", "interpolation.InterpolateDiscrete.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.make_subgrid"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the survival function for `input` and return as a pandas DataFrame.\n        See `predict_surv` to return tensor or np.array instead.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            pd.DataFrame -- Predictions\n        \"\"\"", "\n", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "index", "=", "None", "\n", "if", "self", ".", "duration_index", "is", "not", "None", ":", "\n", "            ", "index", "=", "utils", ".", "make_subgrid", "(", "self", ".", "duration_index", ",", "self", ".", "sub", ")", "\n", "", "return", "pd", ".", "DataFrame", "(", "surv", ".", "transpose", "(", ")", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolatePMF.predict_pmf": [[121, 130], ["interpolation.InterpolatePMF.model.predict_pmf", "pmf[].contiguous().view().repeat().div().view", "pycox.models.utils.pad_col", "torchtuples.utils.array_or_tensor", "pmf[].contiguous().view().repeat().div", "pmf[].contiguous().view().repeat", "pmf[].contiguous().view", "pmf[].contiguous"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.predict_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["    ", "def", "predict_pmf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "not", "self", ".", "scheme", "in", "[", "'const_pdf'", ",", "'lin_surv'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "pmf", "=", "self", ".", "model", ".", "predict_pmf", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "n", ",", "m", "=", "pmf", ".", "shape", "\n", "pmf_cdi", "=", "pmf", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "sub", ")", ".", "div", "(", "self", ".", "sub", ")", ".", "view", "(", "n", ",", "-", "1", ")", "\n", "pmf_cdi", "=", "utils", ".", "pad_col", "(", "pmf_cdi", ",", "where", "=", "'start'", ")", "\n", "pmf_cdi", "[", ":", ",", "0", "]", "=", "pmf", "[", ":", ",", "0", "]", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "pmf_cdi", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolatePMF._surv_const_pdf": [[131, 135], ["interpolation.InterpolatePMF.predict_pmf", "torchtuples.utils.array_or_tensor", "interpolation.InterpolatePMF.cumsum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.predict_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "_surv_const_pdf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "pmf", "=", "self", ".", "predict_pmf", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv", "=", "1", "-", "pmf", ".", "cumsum", "(", "1", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard.predict_hazard": [[139, 145], ["interpolation.InterpolateLogisticHazard._hazard_const_haz"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard._hazard_const_haz"], ["def", "predict_hazard", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "self", ".", "scheme", "in", "[", "'const_hazard'", ",", "'exp_surv'", "]", ":", "\n", "            ", "haz", "=", "self", ".", "_hazard_const_haz", "(", "input", ",", "batch_size", ",", "numpy", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "haz", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard.predict_surv": [[146, 154], ["interpolation.InterpolateLogisticHazard._surv_const_haz", "interpolation.InterpolateLogisticHazard._surv_const_pdf"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard._surv_const_haz", "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolatePMF._surv_const_pdf"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "self", ".", "scheme", "in", "[", "'const_hazard'", ",", "'exp_surv'", "]", ":", "\n", "            ", "surv", "=", "self", ".", "_surv_const_haz", "(", "input", ",", "batch_size", ",", "numpy", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "", "elif", "self", ".", "scheme", "in", "[", "'const_pdf'", ",", "'lin_surv'", "]", ":", "\n", "            ", "surv", "=", "self", ".", "_surv_const_pdf", "(", "input", ",", "batch_size", ",", "numpy", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "surv", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard._hazard_const_haz": [[155, 174], ["interpolation.InterpolateLogisticHazard.model.predict_hazard", "[].contiguous", "pycox.models.utils.pad_col.view().repeat().view().div", "pycox.models.utils.pad_col", "torchtuples.utils.array_or_tensor", "pycox.models.utils.pad_col.view().repeat().view", "pycox.models.utils.pad_col.view().repeat", "pycox.models.utils.pad_col.view"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_hazard", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "_hazard_const_haz", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Computes the continuous-time constant hazard interpolation.\n        Essentially we what the discrete survival estimates to match the continuous time at the knots.\n        So essentially we want\n            $$S(tau_j) = prod_{k=1}^j [1 - h_k] = prod_{k=1}{j} exp[-eta_k].$$\n        where $h_k$ is the discrete hazard estimates and $eta_k$ continuous time hazards multiplied\n        with the length of the duration interval as they are defined for the PC-Hazard method.\n        Thus we get \n            $$eta_k = - log[1 - h_k]$$\n        which can be divided by the length of the time interval to get the continuous time hazards.\n        \"\"\"", "\n", "haz_orig", "=", "self", ".", "model", ".", "predict_hazard", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "haz", "=", "(", "1", "-", "haz_orig", ")", ".", "add", "(", "self", ".", "epsilon", ")", ".", "log", "(", ")", ".", "mul", "(", "-", "1", ")", ".", "relu", "(", ")", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "n", "=", "haz", ".", "shape", "[", "0", "]", "\n", "haz", "=", "haz", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "sub", ")", ".", "view", "(", "n", ",", "-", "1", ")", ".", "div", "(", "self", ".", "sub", ")", "\n", "haz", "=", "utils", ".", "pad_col", "(", "haz", ",", "where", "=", "'start'", ")", "\n", "haz", "[", ":", ",", "0", "]", "=", "haz_orig", "[", ":", ",", "0", "]", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "haz", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard._surv_const_haz": [[175, 180], ["interpolation.InterpolateLogisticHazard._hazard_const_haz", "pycox.models.utils.pad_col().cumsum().mul().exp().mul", "torchtuples.utils.array_or_tensor", "pycox.models.utils.pad_col().cumsum().mul().exp", "pycox.models.utils.pad_col().cumsum().mul", "pycox.models.utils.pad_col().cumsum", "pycox.models.utils.pad_col"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.interpolation.InterpolateLogisticHazard._hazard_const_haz", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "_surv_const_haz", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "haz", "=", "self", ".", "_hazard_const_haz", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv_0", "=", "1", "-", "haz", "[", ":", ",", ":", "1", "]", "\n", "surv", "=", "utils", ".", "pad_col", "(", "haz", "[", ":", ",", "1", ":", "]", ",", "where", "=", "'start'", ")", ".", "cumsum", "(", "1", ")", ".", "mul", "(", "-", "1", ")", ".", "exp", "(", ")", ".", "mul", "(", "surv_0", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.__init__": [[30, 38], ["super().__init__", "pycox.models.loss.NLLPCHazardLoss", "pc_hazard.PCHazard._check_out_features"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard._check_out_features"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "sub", "=", "1", ",", "loss", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "self", ".", "sub", "=", "sub", "\n", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "NLLPCHazardLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "if", "self", ".", "duration_index", "is", "not", "None", ":", "\n", "            ", "self", ".", "_check_out_features", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub": [[43, 48], ["type", "ValueError", "type"], "methods", ["None"], ["", "@", "sub", ".", "setter", "\n", "def", "sub", "(", "self", ",", "sub", ")", ":", "\n", "        ", "if", "type", "(", "sub", ")", "is", "not", "int", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need `sub` to have type `int`, got {type(sub)}\"", ")", "\n", "", "self", ".", "_sub", "=", "sub", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.predict_surv": [[49, 53], ["pc_hazard.PCHazard.predict_hazard", "pc_hazard.PCHazard.cumsum().mul().exp", "torchtuples.utils.array_or_tensor", "pc_hazard.PCHazard.cumsum().mul", "pc_hazard.PCHazard.cumsum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_hazard", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "hazard", "=", "self", ".", "predict_hazard", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv", "=", "hazard", ".", "cumsum", "(", "1", ")", ".", "mul", "(", "-", "1", ")", ".", "exp", "(", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.predict_hazard": [[54, 77], ["pc_hazard.PCHazard.predict", "torch.softplus().view().repeat().view().div", "torch.softplus().view().repeat().view().div", "pycox.models.utils.pad_col", "torchtuples.utils.array_or_tensor", "torch.softplus().view().repeat().view", "torch.softplus().view().repeat().view", "torch.softplus().view().repeat", "torch.softplus().view().repeat", "torch.softplus().view", "torch.softplus().view", "torch.softplus", "torch.softplus"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_hazard", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the hazard function for `input`.\n\n        Arguments:\n            input {tuple, np.ndarra, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            [np.ndarray or tensor] -- Predicted hazards\n        \"\"\"", "\n", "preds", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ")", "\n", "n", "=", "preds", ".", "shape", "[", "0", "]", "\n", "hazard", "=", "F", ".", "softplus", "(", "preds", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "sub", ")", ".", "view", "(", "n", ",", "-", "1", ")", ".", "div", "(", "self", ".", "sub", ")", "\n", "hazard", "=", "pad_col", "(", "hazard", ",", "where", "=", "'start'", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "hazard", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.predict_surv_df": [[78, 85], ["pc_hazard.PCHazard._check_out_features", "pc_hazard.PCHazard.predict_surv", "pandas.DataFrame", "pycox.models.utils.make_subgrid", "pc_hazard.PCHazard.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard._check_out_features", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.make_subgrid"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "self", ".", "_check_out_features", "(", ")", "\n", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "True", ",", "num_workers", ")", "\n", "index", "=", "None", "\n", "if", "self", ".", "duration_index", "is", "not", "None", ":", "\n", "            ", "index", "=", "make_subgrid", "(", "self", ".", "duration_index", ",", "self", ".", "sub", ")", "\n", "", "return", "pd", ".", "DataFrame", "(", "surv", ".", "transpose", "(", ")", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.fit": [[86, 93], ["super().fit", "pc_hazard.PCHazard._check_out_features"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard._check_out_features"], ["", "def", "fit", "(", "self", ",", "input", ",", "target", ",", "batch_size", "=", "256", ",", "epochs", "=", "1", ",", "callbacks", "=", "None", ",", "verbose", "=", "True", ",", "\n", "num_workers", "=", "0", ",", "shuffle", "=", "True", ",", "metrics", "=", "None", ",", "val_data", "=", "None", ",", "val_batch_size", "=", "8224", ",", "\n", "check_out_features", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "check_out_features", ":", "\n", "            ", "self", ".", "_check_out_features", "(", "target", ")", "\n", "", "return", "super", "(", ")", ".", "fit", "(", "input", ",", "target", ",", "batch_size", ",", "epochs", ",", "callbacks", ",", "verbose", ",", "num_workers", ",", "\n", "shuffle", ",", "metrics", ",", "val_data", ",", "val_batch_size", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.fit_dataloader": [[94, 99], ["super().fit_dataloader", "pc_hazard.PCHazard._check_out_features"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.fit_dataloader", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard._check_out_features"], ["", "def", "fit_dataloader", "(", "self", ",", "dataloader", ",", "epochs", "=", "1", ",", "callbacks", "=", "None", ",", "verbose", "=", "True", ",", "metrics", "=", "None", ",", "\n", "val_dataloader", "=", "None", ",", "check_out_features", "=", "True", ")", ":", "\n", "        ", "if", "check_out_features", ":", "\n", "            ", "self", ".", "_check_out_features", "(", ")", "\n", "", "return", "super", "(", ")", ".", "fit_dataloader", "(", "dataloader", ",", "epochs", ",", "callbacks", ",", "verbose", ",", "metrics", ",", "val_dataloader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard._check_out_features": [[100, 117], ["hasattr", "list", "pc_hazard.PCHazard.net.modules", "len", "[].max", "ValueError", "ValueError", "ValueError", "torchtuples.tuplefy().to_numpy", "torchtuples.tuplefy", "len"], "methods", ["None"], ["", "def", "_check_out_features", "(", "self", ",", "target", "=", "None", ")", ":", "\n", "        ", "last", "=", "list", "(", "self", ".", "net", ".", "modules", "(", ")", ")", "[", "-", "1", "]", "\n", "if", "hasattr", "(", "last", ",", "'out_features'", ")", ":", "\n", "            ", "m_output", "=", "last", ".", "out_features", "\n", "if", "self", ".", "duration_index", "is", "not", "None", ":", "\n", "                ", "n_grid", "=", "len", "(", "self", ".", "duration_index", ")", "\n", "if", "n_grid", "==", "m_output", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Output of `net` is one too large. Should have length \"", "+", "\n", "f\"{len(self.duration_index)-1}\"", ")", "\n", "", "if", "n_grid", "!=", "(", "m_output", "+", "1", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"Output of `net` does not correspond with `duration_index`\"", ")", "\n", "", "", "if", "target", "is", "not", "None", ":", "\n", "                ", "max_idx", "=", "tt", ".", "tuplefy", "(", "target", ")", ".", "to_numpy", "(", ")", "[", "0", "]", ".", "max", "(", ")", "\n", "if", "m_output", "!=", "(", "max_idx", "+", "1", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"Output of `net` is {m_output}, but data only trains {max_idx + 1} indices. \"", "+", "\n", "f\"Output of `net` should be  {max_idx + 1}.\"", "+", "\n", "\"Set `check_out_feature=False` to suppress this Error.\"", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.__init__": [[48, 53], ["super().__init__", "pycox.models.loss.NLLLogistiHazardLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "NLLLogistiHazardLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.duration_index": [[65, 68], ["None"], "methods", ["None"], ["", "@", "duration_index", ".", "setter", "\n", "def", "duration_index", "(", "self", ",", "val", ")", ":", "\n", "        ", "self", ".", "_duration_index", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_surv_df": [[69, 72], ["logistic_hazard.LogisticHazard.predict_surv", "pandas.DataFrame", "logistic_hazard.LogisticHazard.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "True", ",", "num_workers", ")", "\n", "return", "pd", ".", "DataFrame", "(", "surv", ".", "transpose", "(", ")", ",", "self", ".", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_surv": [[73, 78], ["logistic_hazard.LogisticHazard.predict_hazard", "torchtuples.utils.array_or_tensor"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_hazard", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "epsilon", "=", "1e-7", ")", ":", "\n", "        ", "hazard", "=", "self", ".", "predict_hazard", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv", "=", "(", "1", "-", "hazard", ")", ".", "add", "(", "epsilon", ")", ".", "log", "(", ")", ".", "cumsum", "(", "1", ")", ".", "exp", "(", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.predict_hazard": [[80, 84], ["logistic_hazard.LogisticHazard.predict().sigmoid", "torchtuples.utils.array_or_tensor", "logistic_hazard.LogisticHazard.predict"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.sigmoid", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor", "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict"], ["", "def", "predict_hazard", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "hazard", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ")", ".", "sigmoid", "(", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "hazard", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.logistic_hazard.LogisticHazard.interpolate": [[85, 105], ["pycox.models.interpolation.InterpolateLogisticHazard"], "methods", ["None"], ["", "def", "interpolate", "(", "self", ",", "sub", "=", "10", ",", "scheme", "=", "'const_pdf'", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"Use interpolation for predictions.\n        There are two schemes:\n            `const_hazard` and `exp_surv` which assumes pice-wise constant hazard in each interval (exponential survival).\n            `const_pdf` and `lin_surv` which assumes pice-wise constant PMF in each interval (linear survival).\n        \n        Keyword Arguments:\n            sub {int} -- Number of \"sub\" units in interpolation grid. If `sub` is 10 we have a grid with\n                10 times the number of grid points than the original `duration_index` (default: {10}).\n            scheme {str} -- Type of interpolation {'const_hazard', 'const_pdf'}.\n                See `InterpolateDiscrete` (default: {'const_pdf'})\n            duration_index {np.array} -- Cuts used for discretization. Does not affect interpolation,\n                only for setting index in `predict_surv_df` (default: {None})\n        \n        Returns:\n            [InterpolateLogisticHazard] -- Object for prediction with interpolation.\n        \"\"\"", "\n", "if", "duration_index", "is", "None", ":", "\n", "            ", "duration_index", "=", "self", ".", "duration_index", "\n", "", "return", "InterpolateLogisticHazard", "(", "self", ",", "scheme", ",", "duration_index", ",", "sub", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.DurationSortedDataset.__getitem__": [[49, 56], ["super().__getitem__", "event.float.float.float", "duration.sort", "torchtuples.tuplefy"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__getitem__"], ["def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "batch", "=", "super", "(", ")", ".", "__getitem__", "(", "index", ")", "\n", "input", ",", "(", "duration", ",", "event", ")", "=", "batch", "\n", "idx_sort", "=", "duration", ".", "sort", "(", "descending", "=", "True", ")", "[", "1", "]", "\n", "event", "=", "event", ".", "float", "(", ")", "\n", "batch", "=", "tt", ".", "tuplefy", "(", "input", ",", "event", ")", ".", "iloc", "[", "idx_sort", "]", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.CoxCCDataset.__init__": [[59, 67], ["pandas.DataFrame", "data.make_at_risk_dict", "torchtuples.tuplefy", "dict", "type"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.data.make_at_risk_dict"], ["    ", "def", "__init__", "(", "self", ",", "input", ",", "durations", ",", "events", ",", "n_control", "=", "1", ")", ":", "\n", "        ", "df_train_target", "=", "pd", ".", "DataFrame", "(", "dict", "(", "duration", "=", "durations", ",", "event", "=", "events", ")", ")", "\n", "self", ".", "durations", "=", "df_train_target", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'event'", "]", "==", "1", "]", "[", "'duration'", "]", "\n", "self", ".", "at_risk_dict", "=", "make_at_risk_dict", "(", "durations", ")", "\n", "\n", "self", ".", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", "\n", "assert", "type", "(", "self", ".", "durations", ")", "is", "pd", ".", "Series", "\n", "self", ".", "n_control", "=", "n_control", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.CoxCCDataset.__getitem__": [[68, 76], ["data.sample_alive_from_dates", "torchtuples.TupleTree", "torchtuples.tuplefy().to_tensor", "hasattr", "type", "torchtuples.tuplefy", "sample_alive_from_dates.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.data.sample_alive_from_dates"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "(", "not", "hasattr", "(", "index", ",", "'__iter__'", ")", ")", "and", "(", "type", "(", "index", ")", "is", "not", "slice", ")", ":", "\n", "            ", "index", "=", "[", "index", "]", "\n", "", "fails", "=", "self", ".", "durations", ".", "iloc", "[", "index", "]", "\n", "x_case", "=", "self", ".", "input", ".", "iloc", "[", "fails", ".", "index", "]", "\n", "control_idx", "=", "sample_alive_from_dates", "(", "fails", ".", "values", ",", "self", ".", "at_risk_dict", ",", "self", ".", "n_control", ")", "\n", "x_control", "=", "tt", ".", "TupleTree", "(", "self", ".", "input", ".", "iloc", "[", "idx", "]", "for", "idx", "in", "control_idx", ".", "transpose", "(", ")", ")", "\n", "return", "tt", ".", "tuplefy", "(", "x_case", ",", "x_control", ")", ".", "to_tensor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.CoxCCDataset.__len__": [[77, 79], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "durations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.CoxTimeDataset.__init__": [[82, 85], ["data.CoxCCDataset.__init__", "torchtuples.tuplefy().to_tensor", "torchtuples.tuplefy", "data.CoxTimeDataset.durations.values.reshape"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input", ",", "durations", ",", "events", ",", "n_control", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input", ",", "durations", ",", "events", ",", "n_control", ")", "\n", "self", ".", "durations_tensor", "=", "tt", ".", "tuplefy", "(", "self", ".", "durations", ".", "values", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ".", "to_tensor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.CoxTimeDataset.__getitem__": [[86, 94], ["data.CoxCCDataset.__getitem__", "control.apply_nrec.apply_nrec.apply_nrec", "torchtuples.tuplefy", "hasattr"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "index", ",", "'__iter__'", ")", ":", "\n", "            ", "index", "=", "[", "index", "]", "\n", "", "durations", "=", "self", ".", "durations_tensor", ".", "iloc", "[", "index", "]", "\n", "case", ",", "control", "=", "super", "(", ")", ".", "__getitem__", "(", "index", ")", "\n", "case", "=", "case", "+", "durations", "\n", "control", "=", "control", ".", "apply_nrec", "(", "lambda", "x", ":", "x", "+", "durations", ")", "\n", "return", "tt", ".", "tuplefy", "(", "case", ",", "control", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.DeepHitDataset.__getitem__": [[133, 139], ["super().__getitem__", "torchtuples.tuplefy().to_tensor.to_numpy", "data.pair_rank_mat", "torchtuples.tuplefy().to_tensor", "torchtuples.tuplefy", "torchtuples.tuplefy"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__getitem__", "home.repos.pwc.inspect_result.havakv_pycox.models.data.pair_rank_mat"], ["    ", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "input", ",", "target", "=", "super", "(", ")", ".", "__getitem__", "(", "index", ")", "\n", "target", "=", "target", ".", "to_numpy", "(", ")", "\n", "rank_mat", "=", "pair_rank_mat", "(", "*", "target", ")", "\n", "target", "=", "tt", ".", "tuplefy", "(", "*", "target", ",", "rank_mat", ")", ".", "to_tensor", "(", ")", "\n", "return", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.sample_alive_from_dates": [[9, 23], ["numpy.array", "numpy.empty", "np.empty.fill", "enumerate", "numpy.random.uniform"], "function", ["None"], ["def", "sample_alive_from_dates", "(", "dates", ",", "at_risk_dict", ",", "n_control", "=", "1", ")", ":", "\n", "    ", "'''Sample index from living at time given in dates.\n    dates: np.array of times (or pd.Series).\n    at_risk_dict: dict with at_risk_dict[time] = <array with index of alive in X matrix>.\n    n_control: number of samples.\n    '''", "\n", "lengths", "=", "np", ".", "array", "(", "[", "at_risk_dict", "[", "x", "]", ".", "shape", "[", "0", "]", "for", "x", "in", "dates", "]", ")", "# Can be moved outside", "\n", "idx", "=", "(", "np", ".", "random", ".", "uniform", "(", "size", "=", "(", "n_control", ",", "dates", ".", "size", ")", ")", "*", "lengths", ")", ".", "astype", "(", "'int'", ")", "\n", "samp", "=", "np", ".", "empty", "(", "(", "dates", ".", "size", ",", "n_control", ")", ",", "dtype", "=", "int", ")", "\n", "samp", ".", "fill", "(", "np", ".", "nan", ")", "\n", "\n", "for", "it", ",", "time", "in", "enumerate", "(", "dates", ")", ":", "\n", "        ", "samp", "[", "it", ",", ":", "]", "=", "at_risk_dict", "[", "time", "]", "[", "idx", "[", ":", ",", "it", "]", "]", "\n", "", "return", "samp", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.make_at_risk_dict": [[24, 41], ["pandas.Series", "pd.Series.drop_duplicates", "dict", "durations.drop_duplicates.iteritems", "type"], "function", ["None"], ["", "def", "make_at_risk_dict", "(", "durations", ")", ":", "\n", "    ", "\"\"\"Create dict(duration: indices) from sorted df.\n    A dict mapping durations to indices.\n    For each time => index of all individual alive.\n    \n    Arguments:\n        durations {np.arrary} -- durations.\n    \"\"\"", "\n", "assert", "type", "(", "durations", ")", "is", "np", ".", "ndarray", ",", "'Need durations to be a numpy array'", "\n", "durations", "=", "pd", ".", "Series", "(", "durations", ")", "\n", "assert", "durations", ".", "is_monotonic_increasing", ",", "'Requires durations to be monotonic'", "\n", "allidx", "=", "durations", ".", "index", ".", "values", "\n", "keys", "=", "durations", ".", "drop_duplicates", "(", "keep", "=", "'first'", ")", "\n", "at_risk_dict", "=", "dict", "(", ")", "\n", "for", "ix", ",", "t", "in", "keys", ".", "iteritems", "(", ")", ":", "\n", "        ", "at_risk_dict", "[", "t", "]", "=", "allidx", "[", "ix", ":", "]", "\n", "", "return", "at_risk_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data._pair_rank_mat": [[95, 109], ["len", "range", "range"], "function", ["None"], ["", "", "@", "numba", ".", "njit", "\n", "def", "_pair_rank_mat", "(", "mat", ",", "idx_durations", ",", "events", ",", "dtype", "=", "'float32'", ")", ":", "\n", "    ", "n", "=", "len", "(", "idx_durations", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "        ", "dur_i", "=", "idx_durations", "[", "i", "]", "\n", "ev_i", "=", "events", "[", "i", "]", "\n", "if", "ev_i", "==", "0", ":", "\n", "            ", "continue", "\n", "", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "dur_j", "=", "idx_durations", "[", "j", "]", "\n", "ev_j", "=", "events", "[", "j", "]", "\n", "if", "(", "dur_i", "<", "dur_j", ")", "or", "(", "(", "dur_i", "==", "dur_j", ")", "and", "(", "ev_j", "==", "0", ")", ")", ":", "\n", "                ", "mat", "[", "i", ",", "j", "]", "=", "1", "\n", "", "", "", "return", "mat", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.data.pair_rank_mat": [[110, 130], ["idx_durations.reshape.reshape", "events.reshape.reshape", "len", "numpy.zeros", "data._pair_rank_mat"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.data._pair_rank_mat"], ["", "def", "pair_rank_mat", "(", "idx_durations", ",", "events", ",", "dtype", "=", "'float32'", ")", ":", "\n", "    ", "\"\"\"Indicator matrix R with R_ij = 1{T_i < T_j and D_i = 1}.\n    So it takes value 1 if we observe that i has an event before j and zero otherwise.\n    \n    Arguments:\n        idx_durations {np.array} -- Array with durations.\n        events {np.array} -- Array with event indicators.\n    \n    Keyword Arguments:\n        dtype {str} -- dtype of array (default: {'float32'})\n    \n    Returns:\n        np.array -- n x n matrix indicating if i has an observerd event before j.\n    \"\"\"", "\n", "idx_durations", "=", "idx_durations", ".", "reshape", "(", "-", "1", ")", "\n", "events", "=", "events", ".", "reshape", "(", "-", "1", ")", "\n", "n", "=", "len", "(", "idx_durations", ")", "\n", "mat", "=", "np", ".", "zeros", "(", "(", "n", ",", "n", ")", ",", "dtype", "=", "dtype", ")", "\n", "mat", "=", "_pair_rank_mat", "(", "mat", ",", "idx_durations", ",", "events", ",", "dtype", ")", "\n", "return", "mat", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._Loss.__init__": [[449, 452], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.NLLLogistiHazardLoss.forward": [[467, 469], ["loss.nll_logistic_hazard"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_logistic_hazard"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "nll_logistic_hazard", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.NLLPMFLoss.forward": [[484, 486], ["loss.nll_pmf"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "nll_pmf", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.NLLMTLRLoss.forward": [[504, 506], ["loss.nll_mtlr"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_mtlr"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "nll_mtlr", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.NLLPCHazardLoss.forward": [[509, 524], ["loss.nll_pc_hazard_loss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pc_hazard_loss"], ["    ", "def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "interval_frac", ":", "Tensor", ",", "\n", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"Negative log-likelihood of the PC-Hazard parametrization model.\n        See `loss.nll_pc_hazard_loss` for details.\n    \n        Arguments:\n            reduction {string} -- How to reduce the loss.\n                'none': No reduction.\n                'mean': Mean of tensor.\n                'sum: sum.\n    \n        Returns:\n            torch.tensor -- The negative log-likelihood loss.\n        \"\"\"", "\n", "return", "nll_pc_hazard_loss", "(", "phi", ",", "idx_durations", ",", "events", ",", "interval_frac", ",", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._DeepHitLoss.__init__": [[538, 542], ["loss._Loss.__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "alpha", ":", "float", ",", "sigma", ":", "float", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "reduction", ")", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "sigma", "=", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._DeepHitLoss.alpha": [[547, 552], ["ValueError"], "methods", ["None"], ["", "@", "alpha", ".", "setter", "\n", "def", "alpha", "(", "self", ",", "alpha", ":", "float", ")", "->", "None", ":", "\n", "        ", "if", "(", "alpha", "<", "0", ")", "or", "(", "alpha", ">", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need `alpha` to be in [0, 1]. Got {alpha}.\"", ")", "\n", "", "self", ".", "_alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._DeepHitLoss.sigma": [[557, 562], ["ValueError"], "methods", ["None"], ["", "@", "sigma", ".", "setter", "\n", "def", "sigma", "(", "self", ",", "sigma", ":", "float", ")", "->", "None", ":", "\n", "        ", "if", "sigma", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need `sigma` to be positive. Got {sigma}.\"", ")", "\n", "", "self", ".", "_sigma", "=", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.DeepHitSingleLoss.forward": [[586, 591], ["loss.nll_pmf", "loss.rank_loss_deephit_single"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_single"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "rank_mat", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "nll", "=", "nll_pmf", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "rank_loss", "=", "rank_loss_deephit_single", "(", "phi", ",", "idx_durations", ",", "events", ",", "rank_mat", ",", "self", ".", "sigma", ",", "\n", "self", ".", "reduction", ")", "\n", "return", "self", ".", "alpha", "*", "nll", "+", "(", "1.", "-", "self", ".", "alpha", ")", "*", "rank_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.DeepHitLoss.forward": [[611, 615], ["loss.nll_pmf_cr", "loss.rank_loss_deephit_cr"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf_cr", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_cr"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "rank_mat", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "nll", "=", "nll_pmf_cr", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "rank_loss", "=", "rank_loss_deephit_cr", "(", "phi", ",", "idx_durations", ",", "events", ",", "rank_mat", ",", "self", ".", "sigma", ",", "self", ".", "reduction", ")", "\n", "return", "self", ".", "alpha", "*", "nll", "+", "(", "1.", "-", "self", ".", "alpha", ")", "*", "rank_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.BCESurvLoss.forward": [[630, 632], ["loss.bce_surv_loss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.bce_surv_loss"], ["def", "forward", "(", "self", ",", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "bce_surv_loss", "(", "phi", ",", "idx_durations", ",", "events", ",", "self", ".", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxCCLoss.__init__": [[645, 649], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "shrink", ":", "float", "=", "0.", ",", "clamp", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "-", "3e+38", ",", "80.", ")", ")", "->", "Tensor", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "shrink", "=", "shrink", "\n", "self", ".", "clamp", "=", "clamp", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxCCLoss.shrink": [[654, 659], ["ValueError"], "methods", ["None"], ["", "@", "shrink", ".", "setter", "\n", "def", "shrink", "(", "self", ",", "shrink", ":", "float", ")", "->", "None", ":", "\n", "        ", "if", "shrink", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Need shrink to be non-negative, got {shrink}.\"", ")", "\n", "", "self", ".", "_shrink", "=", "shrink", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxCCLoss.forward": [[660, 668], ["hasattr", "loss.cox_cc_loss", "loss.cox_cc_loss_single_ctrl", "loss.cox_cc_loss_single_ctrl", "len"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss_single_ctrl", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss_single_ctrl"], ["", "def", "forward", "(", "self", ",", "g_case", ":", "Tensor", ",", "g_control", ":", "TupleTree", ")", "->", "Tensor", ":", "\n", "        ", "single", "=", "False", "\n", "if", "hasattr", "(", "g_control", ",", "'shape'", ")", ":", "\n", "             ", "if", "g_case", ".", "shape", "==", "g_control", ".", "shape", ":", "\n", "                ", "return", "cox_cc_loss_single_ctrl", "(", "g_case", ",", "g_control", ",", "self", ".", "shrink", ")", "\n", "", "", "elif", "(", "len", "(", "g_control", ")", "==", "1", ")", "and", "(", "g_control", "[", "0", "]", ".", "shape", "==", "g_case", ".", "shape", ")", ":", "\n", "                ", "return", "cox_cc_loss_single_ctrl", "(", "g_case", ",", "g_control", "[", "0", "]", ",", "self", ".", "shrink", ")", "\n", "", "return", "cox_cc_loss", "(", "g_case", ",", "g_control", ",", "self", ".", "shrink", ",", "self", ".", "clamp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxPHLossSorted.__init__": [[681, 683], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxPHLossSorted.forward": [[684, 686], ["loss.cox_ph_loss_sorted"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_ph_loss_sorted"], ["", "def", "forward", "(", "self", ",", "log_h", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "cox_ph_loss_sorted", "(", "log_h", ",", "events", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.CoxPHLoss.forward": [[697, 699], ["loss.cox_ph_loss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_ph_loss"], ["def", "forward", "(", "self", ",", "log_h", ":", "Tensor", ",", "durations", ":", "Tensor", ",", "events", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "return", "cox_ph_loss", "(", "log_h", ",", "durations", ",", "events", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction": [[9, 17], ["ValueError", "loss.mean", "loss.sum"], "function", ["None"], ["def", "_reduction", "(", "loss", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "if", "reduction", "==", "'none'", ":", "\n", "        ", "return", "loss", "\n", "", "elif", "reduction", "==", "'mean'", ":", "\n", "        ", "return", "loss", ".", "mean", "(", ")", "\n", "", "elif", "reduction", "==", "'sum'", ":", "\n", "        ", "return", "loss", ".", "sum", "(", ")", "\n", "", "raise", "ValueError", "(", "f\"`reduction` = {reduction} is not valid. Use 'none', 'mean' or 'sum'.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_logistic_hazard": [[18, 52], ["events.float.view", "idx_durations.view.view", "torch.zeros_like().scatter", "torch.zeros_like().scatter", "torch.binary_cross_entropy_with_logits", "F.binary_cross_entropy_with_logits.cumsum().gather().view", "loss._reduction", "idx_durations.view.max", "ValueError", "events.float.float", "torch.zeros_like", "torch.zeros_like", "F.binary_cross_entropy_with_logits.cumsum().gather", "F.binary_cross_entropy_with_logits.cumsum", "idx_durations.view.max().item", "idx_durations.view.max"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction"], ["", "def", "nll_logistic_hazard", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "\n", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Negative log-likelihood of the discrete time hazard parametrized model LogisticHazard [1].\n    \n    Arguments:\n        phi {torch.tensor} -- Estimates in (-inf, inf), where hazard = sigmoid(phi).\n        idx_durations {torch.tensor} -- Event times represented as indices.\n        events {torch.tensor} -- Indicator of event (1.) or censoring (0.).\n            Same length as 'idx_durations'.\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum: sum.\n    \n    Returns:\n        torch.tensor -- The negative log-likelihood.\n\n    References:\n    [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. Continuous and Discrete-Time Survival Prediction\n        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n        https://arxiv.org/pdf/1910.06724.pdf\n    \"\"\"", "\n", "if", "phi", ".", "shape", "[", "1", "]", "<=", "idx_durations", ".", "max", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Network output `phi` is too small for `idx_durations`.\"", "+", "\n", "f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"", "+", "\n", "f\" but got `phi.shape[1] = {phi.shape[1]}`\"", ")", "\n", "", "if", "events", ".", "dtype", "is", "torch", ".", "bool", ":", "\n", "        ", "events", "=", "events", ".", "float", "(", ")", "\n", "", "events", "=", "events", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "y_bce", "=", "torch", ".", "zeros_like", "(", "phi", ")", ".", "scatter", "(", "1", ",", "idx_durations", ",", "events", ")", "\n", "bce", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "phi", ",", "y_bce", ",", "reduction", "=", "'none'", ")", "\n", "loss", "=", "bce", ".", "cumsum", "(", "1", ")", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", "\n", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf": [[53, 93], ["events.float.view", "idx_durations.view.view", "pycox.models.utils.pad_col", "utils.pad_col.sub().exp().cumsum", "utils.pad_col.gather().view().sub().mul", "sum_.sub().relu().add().log().mul", "loss._reduction", "idx_durations.view.max", "ValueError", "events.float.float", "utils.pad_col.max", "sum_.relu().add().log", "phi.gather().view().sub().mul.add().add", "utils.pad_col.sub().exp", "utils.pad_col.gather().view().sub", "sum_.sub().relu().add().log", "sum_.relu().add", "phi.gather().view().sub().mul.add", "utils.pad_col.sub", "utils.pad_col.gather().view", "sum_.sub().relu().add", "gamma.view", "sum_.relu", "utils.pad_col.gather", "sum_.sub().relu", "idx_durations.view.max().item", "sum_.sub", "idx_durations.view.max", "phi.sub().exp().cumsum.gather().view", "phi.sub().exp().cumsum.gather"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub"], ["", "def", "nll_pmf", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ",", "\n", "epsilon", ":", "float", "=", "1e-7", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Negative log-likelihood for the PMF parametrized model [1].\n    \n    Arguments:\n        phi {torch.tensor} -- Estimates in (-inf, inf), where pmf = somefunc(phi).\n        idx_durations {torch.tensor} -- Event times represented as indices.\n        events {torch.tensor} -- Indicator of event (1.) or censoring (0.).\n            Same length as 'idx_durations'.\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum: sum.\n    \n    Returns:\n        torch.tensor -- The negative log-likelihood.\n\n    References:\n    [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. Continuous and Discrete-Time Survival Prediction\n        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n        https://arxiv.org/pdf/1910.06724.pdf\n    \"\"\"", "\n", "if", "phi", ".", "shape", "[", "1", "]", "<=", "idx_durations", ".", "max", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Network output `phi` is too small for `idx_durations`.\"", "+", "\n", "f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"", "+", "\n", "f\" but got `phi.shape[1] = {phi.shape[1]}`\"", ")", "\n", "", "if", "events", ".", "dtype", "is", "torch", ".", "bool", ":", "\n", "        ", "events", "=", "events", ".", "float", "(", ")", "\n", "", "events", "=", "events", ".", "view", "(", "-", "1", ")", "\n", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "phi", "=", "utils", ".", "pad_col", "(", "phi", ")", "\n", "gamma", "=", "phi", ".", "max", "(", "1", ")", "[", "0", "]", "\n", "cumsum", "=", "phi", ".", "sub", "(", "gamma", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "exp", "(", ")", ".", "cumsum", "(", "1", ")", "\n", "sum_", "=", "cumsum", "[", ":", ",", "-", "1", "]", "\n", "part1", "=", "phi", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", ".", "sub", "(", "gamma", ")", ".", "mul", "(", "events", ")", "\n", "part2", "=", "-", "sum_", ".", "relu", "(", ")", ".", "add", "(", "epsilon", ")", ".", "log", "(", ")", "\n", "part3", "=", "sum_", ".", "sub", "(", "cumsum", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", ")", ".", "relu", "(", ")", ".", "add", "(", "epsilon", ")", ".", "log", "(", ")", ".", "mul", "(", "1.", "-", "events", ")", "\n", "# need relu() in part3 (and possibly part2) because cumsum on gpu has some bugs and we risk getting negative numbers.", "\n", "loss", "=", "-", "part1", ".", "add", "(", "part2", ")", ".", "add", "(", "part3", ")", "\n", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_mtlr": [[94, 130], ["pycox.models.utils.cumsum_reverse", "loss.nll_pmf"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf"], ["", "def", "nll_mtlr", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ",", "\n", "epsilon", ":", "float", "=", "1e-7", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Negative log-likelihood for the MTLR parametrized model [1] [2].\n\n    This is essentially a PMF parametrization with an extra cumulative sum, as explained in [3].\n    \n    Arguments:\n        phi {torch.tensor} -- Estimates in (-inf, inf), where pmf = somefunc(phi).\n        idx_durations {torch.tensor} -- Event times represented as indices.\n        events {torch.tensor} -- Indicator of event (1.) or censoring (0.).\n            Same length as 'idx_durations'.\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum: sum.\n    \n    Returns:\n        torch.tensor -- The negative log-likelihood.\n\n    References:\n    [1] Chun-Nam Yu, Russell Greiner, Hsiu-Chin Lin, and Vickie Baracos.\n        Learning patient- specific cancer survival distributions as a sequence of dependent regressors.\n        In Advances in Neural Information Processing Systems 24, pages 1845\u20131853.\n        Curran Associates, Inc., 2011.\n        https://papers.nips.cc/paper/4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors.pdf\n\n    [2] Stephane Fotso. Deep neural networks for survival analysis based on a multi-task framework.\n        arXiv preprint arXiv:1801.05512, 2018.\n        https://arxiv.org/pdf/1801.05512.pdf\n\n    [3] H\u00e5vard Kvamme and \u00d8rnulf Borgan. Continuous and Discrete-Time Survival Prediction\n        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n        https://arxiv.org/pdf/1910.06724.pdf\n    \"\"\"", "\n", "phi", "=", "utils", ".", "cumsum_reverse", "(", "phi", ",", "dim", "=", "1", ")", "\n", "return", "nll_pmf", "(", "phi", ",", "idx_durations", ",", "events", ",", "reduction", ",", "epsilon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pc_hazard_loss": [[131, 174], ["idx_durations.view.view", "events.float.view", "interval_frac.view.view", "pycox.models.utils.log_softplus().mul", "torch.softplus", "utils.pad_col.gather().view().mul", "pycox.models.utils.pad_col", "utils.pad_col.cumsum().gather().view", "loss._reduction", "events.float.float", "idx_durations.view.view", "utils.log_softplus().mul.sub().sub", "pycox.models.utils.log_softplus", "utils.pad_col.gather().view", "utils.pad_col.cumsum().gather", "phi.gather().view", "utils.log_softplus().mul.sub", "utils.pad_col.gather", "utils.pad_col.cumsum", "phi.gather"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.log_softplus", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub"], ["", "def", "nll_pc_hazard_loss", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "interval_frac", ":", "Tensor", ",", "\n", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Negative log-likelihood of the PC-Hazard parametrization model [1].\n    \n    Arguments:\n        phi {torch.tensor} -- Estimates in (-inf, inf), where hazard = sigmoid(phi).\n        idx_durations {torch.tensor} -- Event times represented as indices.\n        events {torch.tensor} -- Indicator of event (1.) or censoring (0.).\n            Same length as 'idx_durations'.\n        interval_frac {torch.tensor} -- Fraction of last interval before event/censoring.\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum: sum.\n    \n    Returns:\n        torch.tensor -- The negative log-likelihood.\n\n    References:\n    [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. Continuous and Discrete-Time Survival Prediction\n        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n        https://arxiv.org/pdf/1910.06724.pdf\n    \"\"\"", "\n", "if", "events", ".", "dtype", "is", "torch", ".", "bool", ":", "\n", "        ", "events", "=", "events", ".", "float", "(", ")", "\n", "", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "events", "=", "events", ".", "view", "(", "-", "1", ")", "\n", "interval_frac", "=", "interval_frac", ".", "view", "(", "-", "1", ")", "\n", "\n", "keep", "=", "idx_durations", ".", "view", "(", "-", "1", ")", ">=", "0", "\n", "phi", "=", "phi", "[", "keep", ",", ":", "]", "\n", "idx_durations", "=", "idx_durations", "[", "keep", ",", ":", "]", "\n", "events", "=", "events", "[", "keep", "]", "\n", "interval_frac", "=", "interval_frac", "[", "keep", "]", "\n", "\n", "# log_h_e = F.softplus(phi.gather(1, idx_durations).view(-1)).log().mul(events)", "\n", "log_h_e", "=", "utils", ".", "log_softplus", "(", "phi", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", ")", ".", "mul", "(", "events", ")", "\n", "haz", "=", "F", ".", "softplus", "(", "phi", ")", "\n", "scaled_h_e", "=", "haz", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", ".", "mul", "(", "interval_frac", ")", "\n", "haz", "=", "utils", ".", "pad_col", "(", "haz", ",", "where", "=", "'start'", ")", "\n", "sum_haz", "=", "haz", ".", "cumsum", "(", "1", ")", ".", "gather", "(", "1", ",", "idx_durations", ")", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "-", "log_h_e", ".", "sub", "(", "scaled_h_e", ")", ".", "sub", "(", "sum_haz", ")", "\n", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._rank_loss_deephit": [[176, 193], ["loss._diff_cdf_at_time_i", "loss.mean.mean", "loss._reduction", "torch.exp", "torch.exp"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss._diff_cdf_at_time_i", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction"], ["", "def", "_rank_loss_deephit", "(", "pmf", ":", "Tensor", ",", "y", ":", "Tensor", ",", "rank_mat", ":", "Tensor", ",", "sigma", ":", "float", ",", "\n", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Ranking loss from DeepHit.\n    \n    Arguments:\n        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n        y {torch.tensor} -- Matrix with indicator of duration and censoring time. \n        rank_mat {torch.tensor} -- See pair_rank_mat function.\n        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n    \n    Returns:\n        torch.tensor -- loss\n    \"\"\"", "\n", "r", "=", "_diff_cdf_at_time_i", "(", "pmf", ",", "y", ")", "\n", "loss", "=", "rank_mat", "*", "torch", ".", "exp", "(", "-", "r", "/", "sigma", ")", "\n", "loss", "=", "loss", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss._diff_cdf_at_time_i": [[194, 212], ["torch.ones", "torch.ones", "pmf.cumsum().matmul", "pmf.cumsum().matmul.diag().view", "pmf.cumsum().matmul.transpose", "y.transpose", "torch.ones.matmul", "pmf.cumsum", "pmf.cumsum().matmul.diag"], "function", ["None"], ["", "def", "_diff_cdf_at_time_i", "(", "pmf", ":", "Tensor", ",", "y", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"R is the matrix from the DeepHit code giving the difference in CDF between individual\n    i and j, at the event time of j. \n    I.e: R_ij = F_i(T_i) - F_j(T_i)\n    \n    Arguments:\n        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n        y {torch.tensor} -- Matrix with indicator of duration/censor time.\n    \n    Returns:\n        torch.tensor -- R_ij = F_i(T_i) - F_j(T_i)\n    \"\"\"", "\n", "n", "=", "pmf", ".", "shape", "[", "0", "]", "\n", "ones", "=", "torch", ".", "ones", "(", "(", "n", ",", "1", ")", ",", "device", "=", "pmf", ".", "device", ")", "\n", "r", "=", "pmf", ".", "cumsum", "(", "1", ")", ".", "matmul", "(", "y", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "diag_r", "=", "r", ".", "diag", "(", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "r", "=", "ones", ".", "matmul", "(", "diag_r", ")", "-", "r", "\n", "return", "r", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_single": [[213, 251], ["idx_durations.view.view", "pycox.models.utils.pad_col().softmax", "torch.zeros_like().scatter", "torch.zeros_like().scatter", "loss._rank_loss_deephit", "pycox.models.utils.pad_col", "torch.zeros_like", "torch.zeros_like"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._rank_loss_deephit", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "rank_loss_deephit_single", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "rank_mat", ":", "Tensor", ",", "\n", "sigma", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Rank loss proposed by DeepHit authors [1] for a single risks.\n    \n    Arguments:\n        pmf {torch.tensor} -- Matrix with probability mass function pmf_ij = f_i(t_j)\n        y {torch.tensor} -- Matrix with indicator of duration and censoring time. \n        rank_mat {torch.tensor} -- See pair_rank_mat function.\n        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n    Arguments:\n        phi {torch.tensor} -- Predictions as float tensor with shape [batch, n_durations]\n            all in (-inf, inf).\n        idx_durations {torch.tensor} -- Int tensor with index of durations.\n        events {torch.tensor} -- Float indicator of event or censoring (1 is event).\n        rank_mat {torch.tensor} -- See pair_rank_mat function.\n        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n    \n    Keyword Arguments:\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum': sum.\n    \n    Returns:\n        torch.tensor -- Rank loss.\n\n    References:\n    [1] Changhee Lee, William R Zame, Jinsung Yoon, and Mihaela van der Schaar. Deephit: A deep learning\n        approach to survival analysis with competing risks. In Thirty-Second AAAI Conference on Artificial\n        Intelligence, 2018.\n        http://medianetlab.ee.ucla.edu/papers/AAAI_2018_DeepHit\n    \"\"\"", "\n", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "# events = events.float().view(-1)", "\n", "pmf", "=", "utils", ".", "pad_col", "(", "phi", ")", ".", "softmax", "(", "1", ")", "\n", "y", "=", "torch", ".", "zeros_like", "(", "pmf", ")", ".", "scatter", "(", "1", ",", "idx_durations", ",", "1.", ")", "# one-hot", "\n", "rank_loss", "=", "_rank_loss_deephit", "(", "pmf", ",", "y", ",", "rank_mat", ",", "sigma", ",", "reduction", ")", "\n", "return", "rank_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf_cr": [[252, 283], ["idx_durations.view.view", "phi.size", "[].view", "torch.arange", "torch.arange", "sm[].relu().add().log().mul", "loss._reduction", "events.view", "sm[].relu().add().log().mul.add", "sm[].relu().add().log", "pycox.models.utils.pad_col().softmax", "sm[].relu().add", "pycox.models.utils.pad_col", "phi.view", "sm[].relu", "[].sum", "[].view.cumsum"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "nll_pmf_cr", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ",", "\n", "epsilon", ":", "float", "=", "1e-7", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Negative log-likelihood for PMF parameterizations. `phi` is the ''logit''.\n    \n    Arguments:\n        phi {torch.tensor} -- Predictions as float tensor with shape [batch, n_risks, n_durations]\n            all in (-inf, inf).\n        idx_durations {torch.tensor} -- Int tensor with index of durations.\n        events {torch.tensor} -- Int tensor with event types.\n            {0: Censored, 1: first group, ..., n_risks: n'th risk group}.\n    \n    Keyword Arguments:\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            else: sum.\n    \n    Returns:\n        torch.tensor -- Negative log-likelihood.\n    \"\"\"", "\n", "# Should improve numerical stability by, e.g., log-sum-exp trick.", "\n", "events", "=", "events", ".", "view", "(", "-", "1", ")", "-", "1", "\n", "event_01", "=", "(", "events", "!=", "-", "1", ")", ".", "float", "(", ")", "\n", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ")", "\n", "batch_size", "=", "phi", ".", "size", "(", "0", ")", "\n", "sm", "=", "utils", ".", "pad_col", "(", "phi", ".", "view", "(", "batch_size", ",", "-", "1", ")", ")", ".", "softmax", "(", "1", ")", "[", ":", ",", ":", "-", "1", "]", ".", "view", "(", "phi", ".", "shape", ")", "\n", "index", "=", "torch", ".", "arange", "(", "batch_size", ")", "\n", "part1", "=", "sm", "[", "index", ",", "events", ",", "idx_durations", "]", ".", "relu", "(", ")", ".", "add", "(", "epsilon", ")", ".", "log", "(", ")", ".", "mul", "(", "event_01", ")", "\n", "part2", "=", "(", "1", "-", "sm", ".", "cumsum", "(", "2", ")", "[", "index", ",", ":", ",", "idx_durations", "]", ".", "sum", "(", "1", ")", ")", ".", "relu", "(", ")", ".", "add", "(", "epsilon", ")", ".", "log", "(", ")", ".", "mul", "(", "1", "-", "event_01", ")", "\n", "loss", "=", "-", "part1", ".", "add", "(", "part2", ")", "\n", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_cr": [[284, 334], ["idx_durations.view.view", "pycox.models.utils.pad_col().softmax", "pmf[].view", "torch.zeros_like", "torch.zeros_like", "range", "loss._reduction", "events.view", "loss._rank_loss_deephit", "loss.append", "sum", "pycox.models.utils.pad_col", "sum", "phi.view", "torch.arange", "torch.arange", "_rank_loss_deephit.view", "sum", "lo.mean", "lo.sum"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._reduction", "home.repos.pwc.inspect_result.havakv_pycox.models.loss._rank_loss_deephit", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "rank_loss_deephit_cr", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "rank_mat", ":", "Tensor", ",", "\n", "sigma", ":", "float", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Rank loss proposed by DeepHit authors for competing risks [1].\n    \n    Arguments:\n        phi {torch.tensor} -- Predictions as float tensor with shape [batch, n_risks, n_durations]\n            all in (-inf, inf).\n        idx_durations {torch.tensor} -- Int tensor with index of durations.\n        events {torch.tensor} -- Int tensor with event types.\n            {0: Censored, 1: first group, ..., n_risks: n'th risk group}.\n        rank_mat {torch.tensor} -- See pair_rank_mat function.\n        sigma {float} -- Sigma from DeepHit paper, chosen by you.\n    \n    Keyword Arguments:\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            else: sum.\n    \n    Returns:\n        torch.tensor -- Rank loss.\n\n    References:\n    [1] Changhee Lee, William R Zame, Jinsung Yoon, and Mihaela van der Schaar. Deephit: A deep learning\n        approach to survival analysis with competing risks. In Thirty-Second AAAI Conference on Artificial\n        Intelligence, 2018.\n        http://medianetlab.ee.ucla.edu/papers/AAAI_2018_DeepHit\n    \"\"\"", "\n", "idx_durations", "=", "idx_durations", ".", "view", "(", "-", "1", ")", "\n", "events", "=", "events", ".", "view", "(", "-", "1", ")", "-", "1", "\n", "event_01", "=", "(", "events", "==", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "batch_size", ",", "n_risks", "=", "phi", ".", "shape", "[", ":", "2", "]", "\n", "pmf", "=", "utils", ".", "pad_col", "(", "phi", ".", "view", "(", "batch_size", ",", "-", "1", ")", ")", ".", "softmax", "(", "1", ")", "\n", "pmf", "=", "pmf", "[", ":", ",", ":", "-", "1", "]", ".", "view", "(", "phi", ".", "shape", ")", "\n", "y", "=", "torch", ".", "zeros_like", "(", "pmf", ")", "\n", "y", "[", "torch", ".", "arange", "(", "batch_size", ")", ",", ":", ",", "idx_durations", "]", "=", "1.", "\n", "\n", "loss", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_risks", ")", ":", "\n", "        ", "rank_loss_i", "=", "_rank_loss_deephit", "(", "pmf", "[", ":", ",", "i", ",", ":", "]", ",", "y", "[", ":", ",", "i", ",", ":", "]", ",", "rank_mat", ",", "sigma", ",", "'none'", ")", "\n", "loss", ".", "append", "(", "rank_loss_i", ".", "view", "(", "-", "1", ")", "*", "(", "events", "==", "i", ")", ".", "float", "(", ")", ")", "\n", "\n", "", "if", "reduction", "==", "'none'", ":", "\n", "        ", "return", "sum", "(", "loss", ")", "\n", "", "elif", "reduction", "==", "'mean'", ":", "\n", "        ", "return", "sum", "(", "[", "lo", ".", "mean", "(", ")", "for", "lo", "in", "loss", "]", ")", "\n", "", "elif", "reduction", "==", "'sum'", ":", "\n", "        ", "return", "sum", "(", "[", "lo", ".", "sum", "(", ")", "for", "lo", "in", "loss", "]", ")", "\n", "", "return", "_reduction", "(", "loss", ",", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.bce_surv_loss": [[335, 367], ["torch.arange", "torch.arange", "torch.binary_cross_entropy_with_logits", "idx_durations.max", "ValueError", "events.float.float", "events.float.view", "torch.arange.view", "idx_durations.view", "torch.ones_like", "torch.ones_like", "idx_durations.max().item", "idx_durations.max"], "function", ["None"], ["", "def", "bce_surv_loss", "(", "phi", ":", "Tensor", ",", "idx_durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "reduction", ":", "str", "=", "'mean'", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Loss function for a set of binary classifiers. Each output node (element in `phi`)\n    is the logit of a survival prediction at the time corresponding to that index.\n    See [1] for explanation of the method.\n    \n    Arguments:\n        phi {torch.tensor} -- Estimates in (-inf, inf), where survival = sigmoid(phi).\n        idx_durations {torch.tensor} -- Event times represented as indices.\n        events {torch.tensor} -- Indicator of event (1.) or censoring (0.).\n            Same length as 'idx_durations'.\n    \n    Keyword Arguments:\n        reduction {string} -- How to reduce the loss.\n            'none': No reduction.\n            'mean': Mean of tensor.\n            'sum: sum.\n\n    References:\n        [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n            and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n            https://arxiv.org/pdf/1912.08581.pdf\n    \"\"\"", "\n", "if", "phi", ".", "shape", "[", "1", "]", "<=", "idx_durations", ".", "max", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Network output `phi` is too small for `idx_durations`.\"", "+", "\n", "f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"", "+", "\n", "f\" but got `phi.shape[1] = {phi.shape[1]}`\"", ")", "\n", "", "if", "events", ".", "dtype", "is", "torch", ".", "bool", ":", "\n", "        ", "events", "=", "events", ".", "float", "(", ")", "\n", "", "y", "=", "torch", ".", "arange", "(", "phi", ".", "shape", "[", "1", "]", ",", "dtype", "=", "idx_durations", ".", "dtype", ",", "device", "=", "idx_durations", ".", "device", ")", "\n", "y", "=", "(", "y", ".", "view", "(", "1", ",", "-", "1", ")", "<", "idx_durations", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "float", "(", ")", "# mask with ones until idx_duration", "\n", "c", "=", "y", "+", "(", "torch", ".", "ones_like", "(", "y", ")", "-", "y", ")", "*", "events", ".", "view", "(", "-", "1", ",", "1", ")", "# mask with ones until censoring.", "\n", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "phi", ",", "y", ",", "c", ",", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss": [[368, 398], ["torch.log", "torch.log", "ValueError", "torch.clamp.abs().mean", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "len", "torch.mean", "torch.mean", "shrink_zero.abs", "torch.clamp.abs", "g_case.abs().mean", "g_case.abs"], "function", ["None"], ["", "def", "cox_cc_loss", "(", "g_case", ":", "Tensor", ",", "g_control", ":", "Tensor", ",", "shrink", ":", "float", "=", "0.", ",", "\n", "clamp", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "-", "3e+38", ",", "80.", ")", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Torch loss function for the Cox case-control models.\n    For only one control, see `cox_cc_loss_single_ctrl` instead.\n    \n    Arguments:\n        g_case {torch.Tensor} -- Result of net(input_case)\n        g_control {torch.Tensor} -- Results of [net(input_ctrl1), net(input_ctrl2), ...]\n    \n    Keyword Arguments:\n        shrink {float} -- Shrinkage that encourage the net got give g_case and g_control\n            closer to zero (a regularizer in a sense). (default: {0.})\n        clamp {tuple} -- See code (default: {(-3e+38, 80.)})\n    \n    Returns:\n        [type] -- [description]\n    \"\"\"", "\n", "control_sum", "=", "0.", "\n", "shrink_control", "=", "0.", "\n", "if", "g_case", ".", "shape", "!=", "g_control", "[", "0", "]", ".", "shape", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Need `g_case` and `g_control[0]` to have same shape. Got {g_case.shape}\"", "+", "\n", "f\" and {g_control[0].shape}\"", ")", "\n", "", "for", "ctr", "in", "g_control", ":", "\n", "        ", "shrink_control", "+=", "ctr", ".", "abs", "(", ")", ".", "mean", "(", ")", "\n", "ctr", "=", "ctr", "-", "g_case", "\n", "ctr", "=", "torch", ".", "clamp", "(", "ctr", ",", "*", "clamp", ")", "# Kills grads for very bad cases (should instead cap grads!!!).", "\n", "control_sum", "+=", "torch", ".", "exp", "(", "ctr", ")", "\n", "", "loss", "=", "torch", ".", "log", "(", "1.", "+", "control_sum", ")", "\n", "shrink_zero", "=", "shrink", "*", "(", "g_case", ".", "abs", "(", ")", ".", "mean", "(", ")", "+", "shrink_control", ")", "/", "len", "(", "g_control", ")", "\n", "return", "torch", ".", "mean", "(", "loss", ")", "+", "shrink_zero", ".", "abs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss_single_ctrl": [[399, 406], ["torch.softplus().mean", "torch.softplus", "g_case.abs().mean", "g_control.abs().mean", "g_case.abs", "g_control.abs"], "function", ["None"], ["", "def", "cox_cc_loss_single_ctrl", "(", "g_case", ":", "Tensor", ",", "g_control", ":", "Tensor", ",", "shrink", ":", "float", "=", "0.", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"CoxCC and CoxTime loss, but with only a single control.\n    \"\"\"", "\n", "loss", "=", "F", ".", "softplus", "(", "g_control", "-", "g_case", ")", ".", "mean", "(", ")", "\n", "if", "shrink", "!=", "0", ":", "\n", "        ", "loss", "+=", "shrink", "*", "(", "g_case", ".", "abs", "(", ")", ".", "mean", "(", ")", "+", "g_control", ".", "abs", "(", ")", ".", "mean", "(", ")", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_ph_loss_sorted": [[407, 424], ["events.float.view", "log_h.view.view", "log_h.view.max", "log_h.view.sub().exp().cumsum().add().log().add", "events.float.float", "log_h.view.sub().mul().sum().div", "log_h.view.sub().exp().cumsum().add().log", "events.float.sum", "log_h.view.sub().mul().sum", "log_h.view.sub().exp().cumsum().add", "log_h.view.sub().mul", "log_h.view.sub().exp().cumsum", "log_h.view.sub", "log_h.view.sub().exp", "log_h.view.sub"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.sub"], ["", "def", "cox_ph_loss_sorted", "(", "log_h", ":", "Tensor", ",", "events", ":", "Tensor", ",", "eps", ":", "float", "=", "1e-7", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Requires the input to be sorted by descending duration time.\n    See DatasetDurationSorted.\n\n    We calculate the negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n\n    We just compute a cumulative sum, and not the true Risk sets. This is a\n    limitation, but simple and fast.\n    \"\"\"", "\n", "if", "events", ".", "dtype", "is", "torch", ".", "bool", ":", "\n", "        ", "events", "=", "events", ".", "float", "(", ")", "\n", "", "events", "=", "events", ".", "view", "(", "-", "1", ")", "\n", "log_h", "=", "log_h", ".", "view", "(", "-", "1", ")", "\n", "gamma", "=", "log_h", ".", "max", "(", ")", "\n", "log_cumsum_h", "=", "log_h", ".", "sub", "(", "gamma", ")", ".", "exp", "(", ")", ".", "cumsum", "(", "0", ")", ".", "add", "(", "eps", ")", ".", "log", "(", ")", ".", "add", "(", "gamma", ")", "\n", "return", "-", "log_h", ".", "sub", "(", "log_cumsum_h", ")", ".", "mul", "(", "events", ")", ".", "sum", "(", ")", ".", "div", "(", "events", ".", "sum", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_ph_loss": [[425, 438], ["loss.cox_ph_loss_sorted", "durations.sort"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_ph_loss_sorted"], ["", "def", "cox_ph_loss", "(", "log_h", ":", "Tensor", ",", "durations", ":", "Tensor", ",", "events", ":", "Tensor", ",", "eps", ":", "float", "=", "1e-7", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Loss for CoxPH model. If data is sorted by descending duration, see `cox_ph_loss_sorted`.\n\n    We calculate the negative log of $(\\frac{h_i}{\\sum_{j \\in R_i} h_j})^d$,\n    where h = exp(log_h) are the hazards and R is the risk set, and d is event.\n\n    We just compute a cumulative sum, and not the true Risk sets. This is a\n    limitation, but simple and fast.\n    \"\"\"", "\n", "idx", "=", "durations", ".", "sort", "(", "descending", "=", "True", ")", "[", "1", "]", "\n", "events", "=", "events", "[", "idx", "]", "\n", "log_h", "=", "log_h", "[", "idx", "]", "\n", "return", "cox_ph_loss_sorted", "(", "log_h", ",", "events", ",", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHitSingle.__init__": [[39, 43], ["super().__init__", "pycox.models.loss.DeepHitSingleLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "alpha", "=", "0.2", ",", "sigma", "=", "0.1", ",", "loss", "=", "None", ")", ":", "\n", "        ", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "DeepHitSingleLoss", "(", "alpha", ",", "sigma", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ",", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHitSingle.make_dataloader": [[44, 48], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader", "(", "self", ",", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", ",", "\n", "make_dataset", "=", "models", ".", "data", ".", "DeepHitDataset", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHitSingle.make_dataloader_predict": [[49, 52], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader_predict", "(", "self", ",", "input", ",", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "input", ",", "batch_size", ",", "shuffle", ",", "num_workers", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.__init__": [[85, 90], ["torchtuples.Model.__init__", "pycox.models.loss.DeepHitLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "alpha", "=", "0.2", ",", "sigma", "=", "0.1", ",", "duration_index", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "DeepHitLoss", "(", "alpha", ",", "sigma", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.duration_index": [[102, 105], ["None"], "methods", ["None"], ["", "@", "duration_index", ".", "setter", "\n", "def", "duration_index", "(", "self", ",", "val", ")", ":", "\n", "        ", "self", ".", "_duration_index", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.make_dataloader": [[106, 110], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader", "(", "self", ",", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", ",", "\n", "make_dataset", "=", "models", ".", "data", ".", "DeepHitDataset", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.make_dataloader_predict": [[111, 114], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader_predict", "(", "self", ",", "input", ",", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "input", ",", "batch_size", ",", "shuffle", ",", "num_workers", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.predict_surv_df": [[115, 133], ["deephit.DeepHit.predict_surv", "pandas.DataFrame"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the survival function for `input`, i.e., survive all of the event types,\n        and return as a pandas DataFrame.\n        See `prediction_surv_df` to return a DataFrame instead.\n\n        Arguments:\n            input {tuple, np.ndarra, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            eval_ {bool} -- If 'True', use 'eval' modede on net. (default: {True})\n            num_workers {int} -- Number of workes in created dataloader (default: {0})\n        \n        Returns:\n            pd.DataFrame -- Predictions\n        \"\"\"", "\n", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "True", ",", "num_workers", ")", "\n", "return", "pd", ".", "DataFrame", "(", "surv", ",", "self", ".", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.predict_surv": [[134, 157], ["deephit.DeepHit.predict_cif", "torchtuples.utils.array_or_tensor", "deephit.DeepHit.sum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.predict_cif", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "\n", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the survival function for `input`, i.e., survive all of the event types.\n        See `prediction_surv_df` to return a DataFrame instead.\n\n        Arguments:\n            input {tuple, np.ndarra, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' modede on net. (default: {True})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workes in created dataloader (default: {0})\n        \n        Returns:\n            [TupleTree, np.ndarray or tensor] -- Predictions\n        \"\"\"", "\n", "cif", "=", "self", ".", "predict_cif", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv", "=", "1.", "-", "cif", ".", "sum", "(", "0", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.predict_cif": [[158, 180], ["deephit.DeepHit.predict_pmf", "deephit.DeepHit.cumsum", "torchtuples.utils.array_or_tensor"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.predict_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_cif", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "\n", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the cumulative incidence function (cif) for `input`.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            [np.ndarray or tensor] -- Predictions\n        \"\"\"", "\n", "pmf", "=", "self", ".", "predict_pmf", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "cif", "=", "pmf", ".", "cumsum", "(", "1", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "cif", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.deephit.DeepHit.predict_pmf": [[181, 205], ["deephit.DeepHit.predict", "pmf.view().transpose().transpose.view().transpose().transpose.view().transpose().transpose", "torchtuples.utils.array_or_tensor", "pycox.models.utils.pad_col().softmax", "pmf.view().transpose().transpose.view().transpose().transpose.view().transpose", "pycox.models.utils.pad_col", "deephit.DeepHit.view", "pmf.view().transpose().transpose.view().transpose().transpose.view", "deephit.DeepHit.size"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "predict_pmf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "\n", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict the probability mass fuction (PMF) for `input`.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n        \n        Keyword Arguments:\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            grads {bool} -- If gradients should be computed (default: {False})\n            to_cpu {bool} -- For larger data sets we need to move the results to cpu\n                (default: {False})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n        \n        Returns:\n            [np.ndarray or tensor] -- Predictions\n        \"\"\"", "\n", "preds", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ")", "\n", "pmf", "=", "pad_col", "(", "preds", ".", "view", "(", "preds", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", ".", "softmax", "(", "1", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "pmf", "=", "pmf", ".", "view", "(", "preds", ".", "shape", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "pmf", ",", "numpy", ",", "input", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.__init__": [[14, 17], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "loss", "=", "None", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.duration_index": [[29, 32], ["None"], "methods", ["None"], ["", "@", "duration_index", ".", "setter", "\n", "def", "duration_index", "(", "self", ",", "val", ")", ":", "\n", "        ", "self", ".", "_duration_index", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.predict_surv": [[33, 38], ["pmf.PMFBase.PMFBase.predict_pmf", "torchtuples.utils.array_or_tensor", "pmf.PMFBase.PMFBase.cumsum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.predict_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "pmf", "=", "self", ".", "predict_pmf", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "to_cpu", ",", "num_workers", ")", "\n", "surv", "=", "1", "-", "pmf", ".", "cumsum", "(", "1", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.predict_pmf": [[39, 44], ["pmf.PMFBase.predict", "torchtuples.utils.array_or_tensor", "pycox.models.utils.pad_col().softmax", "pycox.models.utils.pad_col"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "predict_pmf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "preds", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ")", "\n", "pmf", "=", "pad_col", "(", "preds", ")", ".", "softmax", "(", "1", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "pmf", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.predict_surv_df": [[45, 48], ["pmf.PMFBase.predict_surv", "pandas.DataFrame", "pmf.PMFBase.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "surv", "=", "self", ".", "predict_surv", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "True", ",", "num_workers", ")", "\n", "return", "pd", ".", "DataFrame", "(", "surv", ".", "transpose", "(", ")", ",", "self", ".", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate": [[49, 68], ["pycox.models.interpolation.InterpolatePMF"], "methods", ["None"], ["", "def", "interpolate", "(", "self", ",", "sub", "=", "10", ",", "scheme", "=", "'const_pdf'", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"Use interpolation for predictions.\n        There are only one scheme:\n            `const_pdf` and `lin_surv` which assumes pice-wise constant pmf in each interval (linear survival).\n        \n        Keyword Arguments:\n            sub {int} -- Number of \"sub\" units in interpolation grid. If `sub` is 10 we have a grid with\n                10 times the number of grid points than the original `duration_index` (default: {10}).\n            scheme {str} -- Type of interpolation {'const_hazard', 'const_pdf'}.\n                See `InterpolateDiscrete` (default: {'const_pdf'})\n            duration_index {np.array} -- Cuts used for discretization. Does not affect interpolation,\n                only for setting index in `predict_surv_df` (default: {None})\n        \n        Returns:\n            [InterpolationPMF] -- Object for prediction with interpolation.\n        \"\"\"", "\n", "if", "duration_index", "is", "None", ":", "\n", "            ", "duration_index", "=", "self", ".", "duration_index", "\n", "", "return", "InterpolatePMF", "(", "self", ",", "scheme", ",", "duration_index", ",", "sub", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMF.__init__": [[96, 100], ["pmf.PMFBase.__init__", "pycox.models.loss.NLLPMFLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "NLLPMFLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ",", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase.__init__": [[9, 13], ["super().__init__", "pycox.models.loss.CoxCCLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "shrink", "=", "0.", ",", "loss", "=", "None", ")", ":", "\n", "        ", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "CoxCCLoss", "(", "shrink", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase.fit": [[14, 43], ["cox_cc._CoxCCBase._sorted_input_target", "super().fit"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase._sorted_input_target", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["", "def", "fit", "(", "self", ",", "input", ",", "target", ",", "batch_size", "=", "256", ",", "epochs", "=", "1", ",", "callbacks", "=", "None", ",", "verbose", "=", "True", ",", "\n", "num_workers", "=", "0", ",", "shuffle", "=", "True", ",", "metrics", "=", "None", ",", "val_data", "=", "None", ",", "val_batch_size", "=", "8224", ",", "\n", "n_control", "=", "1", ",", "shrink", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\n        'target' is a tuple with (durations, events).\n        \n        Arguments:\n            input {np.array, tensor or tuple} -- Input x passed to net.\n            target {np.array, tensor or tuple} -- Target [durations, events]. \n        \n        Keyword Arguments:\n            batch_size {int} -- Elements in each batch (default: {256})\n            epochs {int} -- Number of epochs (default: {1})\n            callbacks {list} -- list of callbacks (default: {None})\n            verbose {bool} -- Print progress (default: {True})\n            num_workers {int} -- Number of workers used in the dataloader (default: {0})\n            shuffle {bool} -- If we should shuffle the order of the dataset (default: {True})\n            n_control {int} -- Number of control samples.\n            **kwargs are passed to 'make_dataloader' method.\n    \n        Returns:\n            TrainingLogger -- Training log\n        \"\"\"", "\n", "input", ",", "target", "=", "self", ".", "_sorted_input_target", "(", "input", ",", "target", ")", "\n", "if", "shrink", "is", "not", "None", ":", "\n", "            ", "self", ".", "loss", ".", "shrink", "=", "shrink", "\n", "", "return", "super", "(", ")", ".", "fit", "(", "input", ",", "target", ",", "batch_size", ",", "epochs", ",", "callbacks", ",", "verbose", ",", "\n", "num_workers", ",", "shuffle", ",", "metrics", ",", "val_data", ",", "val_batch_size", ",", "\n", "n_control", "=", "n_control", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase.compute_metrics": [[44, 59], ["cox_cc._CoxCCBase._to_device", "cox_cc._CoxCCBase.lens().flatten().get_if_all_equal", "torchtuples.TupleTree().cat", "cox_cc._CoxCCBase.net", "torchtuples.tuplefy().split().flatten", "RuntimeError", "RuntimeError", "metric", "metrics.values", "cox_cc._CoxCCBase.lens().flatten", "torchtuples.TupleTree", "torchtuples.tuplefy().split", "metrics.items", "cox_cc._CoxCCBase.lens", "torchtuples.tuplefy"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "input", ",", "metrics", ")", ":", "\n", "        ", "if", "(", "self", ".", "loss", "is", "None", ")", "and", "(", "self", ".", "loss", "in", "metrics", ".", "values", "(", ")", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Need to specify a loss (self.loss). It's currently None\"", ")", "\n", "", "input", "=", "self", ".", "_to_device", "(", "input", ")", "\n", "batch_size", "=", "input", ".", "lens", "(", ")", ".", "flatten", "(", ")", ".", "get_if_all_equal", "(", ")", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"All elements in input does not have the same length.\"", ")", "\n", "", "case", ",", "control", "=", "input", "# both are TupleTree", "\n", "input_all", "=", "tt", ".", "TupleTree", "(", "(", "case", ",", ")", "+", "control", ")", ".", "cat", "(", ")", "\n", "g_all", "=", "self", ".", "net", "(", "*", "input_all", ")", "\n", "g_all", "=", "tt", ".", "tuplefy", "(", "g_all", ")", ".", "split", "(", "batch_size", ")", ".", "flatten", "(", ")", "\n", "g_case", "=", "g_all", "[", "0", "]", "\n", "g_control", "=", "g_all", "[", "1", ":", "]", "\n", "res", "=", "{", "name", ":", "metric", "(", "g_case", ",", "g_control", ")", "for", "name", ",", "metric", "in", "metrics", ".", "items", "(", ")", "}", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase.make_dataloader_predict": [[60, 77], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader_predict", "(", "self", ",", "input", ",", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Dataloader for prediction. The input is either the regular input, or a tuple\n        with input and label.\n        \n        Arguments:\n            input {np.array, tensor, tuple} -- Input to net, or tuple with input and labels.\n            batch_size {int} -- Batch size.\n        \n        Keyword Arguments:\n            shuffle {bool} -- If we should shuffle in the dataloader. (default: {False})\n            num_workers {int} -- Number of worker in dataloader. (default: {0})\n        \n        Returns:\n            dataloader -- A dataloader.\n        \"\"\"", "\n", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "input", ",", "batch_size", ",", "shuffle", ",", "num_workers", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase.make_dataloader": [[78, 100], ["cox_cc._CoxCCBase._sorted_input_target", "cox_cc._CoxCCBase.make_dataset", "torchtuples.data.DataLoaderBatch"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase._sorted_input_target", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset"], ["", "def", "make_dataloader", "(", "self", ",", "data", ",", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "0", ",", "n_control", "=", "1", ")", ":", "\n", "        ", "\"\"\"Dataloader for training. Data is on the form (input, target), where\n        target is (durations, events).\n        \n        Arguments:\n            data {tuple} -- Tuple containing (input, (durations, events)).\n            batch_size {int} -- Batch size.\n        \n        Keyword Arguments:\n            shuffle {bool} -- If shuffle in dataloader (default: {True})\n            num_workers {int} -- Number of workers in dataloader. (default: {0})\n            n_control {int} -- Number of control samples in dataloader (default: {1})\n        \n        Returns:\n            dataloader -- Dataloader for training.\n        \"\"\"", "\n", "input", ",", "target", "=", "self", ".", "_sorted_input_target", "(", "*", "data", ")", "\n", "durations", ",", "events", "=", "target", "\n", "dataset", "=", "self", ".", "make_dataset", "(", "input", ",", "durations", ",", "events", ",", "n_control", ")", "\n", "dataloader", "=", "tt", ".", "data", ".", "DataLoaderBatch", "(", "dataset", ",", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox_cc._CoxCCBase._sorted_input_target": [[101, 111], ["torchtuples.tuplefy().to_numpy", "numpy.argsort", "torchtuples.tuplefy", "torchtuples.tuplefy", "torchtuples.tuplefy", "numpy.arange", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_sorted_input_target", "(", "input", ",", "target", ")", ":", "\n", "        ", "input", ",", "target", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", ".", "to_numpy", "(", ")", "\n", "durations", ",", "_", "=", "target", "\n", "idx_sort", "=", "np", ".", "argsort", "(", "durations", ")", "\n", "if", "(", "idx_sort", "==", "np", ".", "arange", "(", "0", ",", "len", "(", "idx_sort", ")", ")", ")", ".", "all", "(", ")", ":", "\n", "            ", "return", "input", ",", "target", "\n", "", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "iloc", "[", "idx_sort", "]", "\n", "target", "=", "tt", ".", "tuplefy", "(", "target", ")", ".", "iloc", "[", "idx_sort", "]", "\n", "return", "input", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit": [[28, 54], ["torchtuples.tuplefy", "super().fit"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["def", "fit", "(", "self", ",", "input", ",", "target", ",", "batch_size", "=", "256", ",", "epochs", "=", "1", ",", "callbacks", "=", "None", ",", "verbose", "=", "True", ",", "\n", "num_workers", "=", "0", ",", "shuffle", "=", "True", ",", "metrics", "=", "None", ",", "val_data", "=", "None", ",", "val_batch_size", "=", "8224", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Fit  model with inputs and targets. Where 'input' is the covariates, and\n        'target' is a tuple with (durations, events).\n        \n        Arguments:\n            input {np.array, tensor or tuple} -- Input x passed to net.\n            target {np.array, tensor or tuple} -- Target [durations, events]. \n        \n        Keyword Arguments:\n            batch_size {int} -- Elements in each batch (default: {256})\n            epochs {int} -- Number of epochs (default: {1})\n            callbacks {list} -- list of callbacks (default: {None})\n            verbose {bool} -- Print progress (default: {True})\n            num_workers {int} -- Number of workers used in the dataloader (default: {0})\n            shuffle {bool} -- If we should shuffle the order of the dataset (default: {True})\n            **kwargs are passed to 'make_dataloader' method.\n    \n        Returns:\n            TrainingLogger -- Training log\n        \"\"\"", "\n", "self", ".", "training_data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "return", "super", "(", ")", ".", "fit", "(", "input", ",", "target", ",", "batch_size", ",", "epochs", ",", "callbacks", ",", "verbose", ",", "\n", "num_workers", ",", "shuffle", ",", "metrics", ",", "val_data", ",", "val_batch_size", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase._compute_baseline_hazards": [[55, 57], ["None"], "methods", ["None"], ["", "def", "_compute_baseline_hazards", "(", "self", ",", "input", ",", "df", ",", "max_duration", ",", "batch_size", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.target_to_df": [[58, 62], ["torchtuples.tuplefy().to_numpy", "pandas.DataFrame", "torchtuples.tuplefy"], "methods", ["None"], ["", "def", "target_to_df", "(", "self", ",", "target", ")", ":", "\n", "        ", "durations", ",", "events", "=", "tt", ".", "tuplefy", "(", "target", ")", ".", "to_numpy", "(", ")", "\n", "df", "=", "pd", ".", "DataFrame", "(", "{", "self", ".", "duration_col", ":", "durations", ",", "self", ".", "event_col", ":", "events", "}", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_hazards": [[63, 98], ["cox._CoxBase.target_to_df", "cox._CoxBase._compute_baseline_hazards", "cox._CoxBase.compute_baseline_cumulative_hazards", "hasattr", "ValueError", "df.sample.sample.sample", "df.sample.sample.sample", "torchtuples.tuplefy().to_numpy", "torchtuples.tuplefy"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.target_to_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase._compute_baseline_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_cumulative_hazards"], ["", "def", "compute_baseline_hazards", "(", "self", ",", "input", "=", "None", ",", "target", "=", "None", ",", "max_duration", "=", "None", ",", "sample", "=", "None", ",", "batch_size", "=", "8224", ",", "\n", "set_hazards", "=", "True", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Computes the Breslow estimates form the data defined by `input` and `target`\n        (if `None` use training data).\n\n        Typically call\n        model.compute_baseline_hazards() after fitting.\n        \n        Keyword Arguments:\n            input  -- Input data (train input) (default: {None})\n            target  -- Target data (train target) (default: {None})\n            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n            sample {float or int} -- Compute estimates of subsample of data (default: {None})\n            batch_size {int} -- Batch size (default: {8224})\n            set_hazards {bool} -- Set hazards in model object, or just return hazards. (default: {True})\n        \n        Returns:\n            pd.Series -- Pandas series with baseline hazards. Index is duration_col.\n        \"\"\"", "\n", "if", "(", "input", "is", "None", ")", "and", "(", "target", "is", "None", ")", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'training_data'", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Need to give a 'input' and 'target' to this function.\"", ")", "\n", "", "input", ",", "target", "=", "self", ".", "training_data", "\n", "", "df", "=", "self", ".", "target_to_df", "(", "target", ")", "#.sort_values(self.duration_col)", "\n", "if", "sample", "is", "not", "None", ":", "\n", "            ", "if", "sample", ">=", "1", ":", "\n", "                ", "df", "=", "df", ".", "sample", "(", "n", "=", "sample", ")", "\n", "", "else", ":", "\n", "                ", "df", "=", "df", ".", "sample", "(", "frac", "=", "sample", ")", "\n", "", "", "input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "to_numpy", "(", ")", ".", "iloc", "[", "df", ".", "index", ".", "values", "]", "\n", "base_haz", "=", "self", ".", "_compute_baseline_hazards", "(", "input", ",", "df", ",", "max_duration", ",", "batch_size", ",", "\n", "eval_", "=", "eval_", ",", "num_workers", "=", "num_workers", ")", "\n", "if", "set_hazards", ":", "\n", "            ", "self", ".", "compute_baseline_cumulative_hazards", "(", "set_hazards", "=", "True", ",", "baseline_hazards_", "=", "base_haz", ")", "\n", "", "return", "base_haz", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_cumulative_hazards": [[99, 117], ["cox._CoxBase.cumsum().rename", "ValueError", "cox._CoxBase.compute_baseline_hazards", "cox._CoxBase.cumsum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_hazards"], ["", "def", "compute_baseline_cumulative_hazards", "(", "self", ",", "input", "=", "None", ",", "target", "=", "None", ",", "max_duration", "=", "None", ",", "sample", "=", "None", ",", "\n", "batch_size", "=", "8224", ",", "set_hazards", "=", "True", ",", "baseline_hazards_", "=", "None", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"See `compute_baseline_hazards. This is the cumulative version.\"\"\"", "\n", "if", "(", "(", "input", "is", "not", "None", ")", "or", "(", "target", "is", "not", "None", ")", ")", "and", "(", "baseline_hazards_", "is", "not", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"'input', 'target' and 'baseline_hazards_' can not both be different from 'None'.\"", ")", "\n", "", "if", "baseline_hazards_", "is", "None", ":", "\n", "            ", "baseline_hazards_", "=", "self", ".", "compute_baseline_hazards", "(", "input", ",", "target", ",", "max_duration", ",", "sample", ",", "batch_size", ",", "\n", "set_hazards", "=", "False", ",", "eval_", "=", "eval_", ",", "num_workers", "=", "num_workers", ")", "\n", "", "assert", "baseline_hazards_", ".", "index", ".", "is_monotonic_increasing", ",", "'Need index of baseline_hazards_ to be monotonic increasing, as it represents time.'", "\n", "bch", "=", "(", "baseline_hazards_", "\n", ".", "cumsum", "(", ")", "\n", ".", "rename", "(", "'baseline_cumulative_hazards'", ")", ")", "\n", "if", "set_hazards", ":", "\n", "            ", "self", ".", "baseline_hazards_", "=", "baseline_hazards_", "\n", "self", ".", "baseline_cumulative_hazards_", "=", "bch", "\n", "", "return", "bch", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_cumulative_hazards": [[118, 131], ["cox._CoxBase._predict_cumulative_hazards", "type", "cox._CoxBase.df_to_input", "hasattr", "ValueError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase._predict_cumulative_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.df_to_input"], ["", "def", "predict_cumulative_hazards", "(", "self", ",", "input", ",", "max_duration", "=", "None", ",", "batch_size", "=", "8224", ",", "verbose", "=", "False", ",", "\n", "baseline_hazards_", "=", "None", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"See `predict_survival_function`.\"\"\"", "\n", "if", "type", "(", "input", ")", "is", "pd", ".", "DataFrame", ":", "\n", "            ", "input", "=", "self", ".", "df_to_input", "(", "input", ")", "\n", "", "if", "baseline_hazards_", "is", "None", ":", "\n", "            ", "if", "not", "hasattr", "(", "self", ",", "'baseline_hazards_'", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'Need to compute baseline_hazards_. E.g run `model.compute_baseline_hazards()`'", ")", "\n", "", "baseline_hazards_", "=", "self", ".", "baseline_hazards_", "\n", "", "assert", "baseline_hazards_", ".", "index", ".", "is_monotonic_increasing", ",", "'Need index of baseline_hazards_ to be monotonic increasing, as it represents time.'", "\n", "return", "self", ".", "_predict_cumulative_hazards", "(", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", ",", "num_workers", "=", "num_workers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase._predict_cumulative_hazards": [[132, 135], ["None"], "methods", ["None"], ["", "def", "_predict_cumulative_hazards", "(", "self", ",", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df": [[136, 156], ["numpy.exp", "cox._CoxBase.predict_cumulative_hazards"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_cumulative_hazards"], ["", "def", "predict_surv_df", "(", "self", ",", "input", ",", "max_duration", "=", "None", ",", "batch_size", "=", "8224", ",", "verbose", "=", "False", ",", "baseline_hazards_", "=", "None", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict survival function for `input`. S(x, t) = exp(-H(x, t))\n        Require computed baseline hazards.\n\n        Arguments:\n            input {np.array, tensor or tuple} -- Input x passed to net.\n\n        Keyword Arguments:\n            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n            batch_size {int} -- Batch size (default: {8224})\n            baseline_hazards_ {pd.Series} -- Baseline hazards. If `None` used `model.baseline_hazards_` (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n\n        Returns:\n            pd.DataFrame -- Survival estimates. One columns for each individual.\n        \"\"\"", "\n", "return", "np", ".", "exp", "(", "-", "self", ".", "predict_cumulative_hazards", "(", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", ",", "num_workers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv": [[157, 181], ["cox._CoxBase.predict_surv_df", "torch.from_numpy", "torchtuples.utils.array_or_tensor", "torch.from_numpy.values.transpose"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["", "def", "predict_surv", "(", "self", ",", "input", ",", "max_duration", "=", "None", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "verbose", "=", "False", ",", "\n", "baseline_hazards_", "=", "None", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "\"\"\"Predict survival function for `input`. S(x, t) = exp(-H(x, t))\n        Require compueted baseline hazards.\n\n        Arguments:\n            input {np.array, tensor or tuple} -- Input x passed to net.\n\n        Keyword Arguments:\n            max_duration {float} -- Don't compute estimates for duration higher (default: {None})\n            batch_size {int} -- Batch size (default: {8224})\n            numpy {bool} -- 'False' gives tensor, 'True' gives numpy, and None give same as input\n                (default: {None})\n            baseline_hazards_ {pd.Series} -- Baseline hazards. If `None` used `model.baseline_hazards_` (default: {None})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n\n        Returns:\n            pd.DataFrame -- Survival estimates. One columns for each individual.\n        \"\"\"", "\n", "surv", "=", "self", ".", "predict_surv_df", "(", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", ",", "num_workers", ")", "\n", "surv", "=", "torch", ".", "from_numpy", "(", "surv", ".", "values", ".", "transpose", "(", ")", ")", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "surv", ",", "numpy", ",", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.save_net": [[182, 198], ["os.path.splitext", "super().save_net", "hasattr", "cox._CoxBase.baseline_hazards_.to_pickle"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.save_net"], ["", "def", "save_net", "(", "self", ",", "path", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Save self.net and baseline hazards to file.\n\n        Arguments:\n            path {str} -- Path to file.\n            **kwargs are passed to torch.save\n\n        Returns:\n            None\n        \"\"\"", "\n", "path", ",", "extension", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "\n", "if", "extension", "==", "\"\"", ":", "\n", "            ", "extension", "=", "'.pt'", "\n", "", "super", "(", ")", ".", "save_net", "(", "path", "+", "extension", ",", "**", "kwargs", ")", "\n", "if", "hasattr", "(", "self", ",", "'baseline_hazards_'", ")", ":", "\n", "            ", "self", ".", "baseline_hazards_", ".", "to_pickle", "(", "path", "+", "'_blh.pickle'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.load_net": [[199, 217], ["os.path.splitext", "super().load_net", "os.path.isfile", "pandas.read_pickle", "cox._CoxBase.baseline_hazards_.cumsum"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.load_net"], ["", "", "def", "load_net", "(", "self", ",", "path", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load net and hazards from file.\n\n        Arguments:\n            path {str} -- Path to file.\n            **kwargs are passed to torch.load\n\n        Returns:\n            None\n        \"\"\"", "\n", "path", ",", "extension", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "\n", "if", "extension", "==", "\"\"", ":", "\n", "            ", "extension", "=", "'.pt'", "\n", "", "super", "(", ")", ".", "load_net", "(", "path", "+", "extension", ",", "**", "kwargs", ")", "\n", "blh_path", "=", "path", "+", "'_blh.pickle'", "\n", "if", "os", ".", "path", ".", "isfile", "(", "blh_path", ")", ":", "\n", "            ", "self", ".", "baseline_hazards_", "=", "pd", ".", "read_pickle", "(", "blh_path", ")", "\n", "self", ".", "baseline_cumulative_hazards_", "=", "self", ".", "baseline_hazards_", ".", "cumsum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.df_to_input": [[218, 221], ["None"], "methods", ["None"], ["", "", "def", "df_to_input", "(", "self", ",", "df", ")", ":", "\n", "        ", "input", "=", "df", "[", "self", ".", "input_cols", "]", ".", "values", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase._compute_baseline_hazards": [[224, 241], ["df_target.assign().groupby().agg().sort_index().assign().pipe().fillna().iloc[].loc[].rename", "df_target.assign().groupby().agg().sort_index().assign().pipe().fillna", "df_target.assign().groupby().agg().sort_index().assign().pipe", "df_target.assign().groupby().agg().sort_index().assign", "df_target.assign().groupby().agg().sort_index", "x[].cumsum", "df_target.assign().groupby().agg", "df_target.assign().groupby", "df_target.assign", "numpy.exp", "cox._CoxPHBase.predict"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict"], ["    ", "def", "_compute_baseline_hazards", "(", "self", ",", "input", ",", "df_target", ",", "max_duration", ",", "batch_size", ",", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "if", "max_duration", "is", "None", ":", "\n", "            ", "max_duration", "=", "np", ".", "inf", "\n", "\n", "# Here we are computing when expg when there are no events.", "\n", "#   Could be made faster, by only computing when there are events.", "\n", "", "return", "(", "df_target", "\n", ".", "assign", "(", "expg", "=", "np", ".", "exp", "(", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ")", ")", "\n", ".", "groupby", "(", "self", ".", "duration_col", ")", "\n", ".", "agg", "(", "{", "'expg'", ":", "'sum'", ",", "self", ".", "event_col", ":", "'sum'", "}", ")", "\n", ".", "sort_index", "(", "ascending", "=", "False", ")", "\n", ".", "assign", "(", "expg", "=", "lambda", "x", ":", "x", "[", "'expg'", "]", ".", "cumsum", "(", ")", ")", "\n", ".", "pipe", "(", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "/", "x", "[", "'expg'", "]", ")", "\n", ".", "fillna", "(", "0.", ")", "\n", ".", "iloc", "[", ":", ":", "-", "1", "]", "\n", ".", "loc", "[", "lambda", "x", ":", "x", ".", "index", "<=", "max_duration", "]", "\n", ".", "rename", "(", "'baseline_hazards'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase._predict_cumulative_hazards": [[242, 254], ["numpy.exp().reshape", "pandas.DataFrame", "cox._CoxPHBase.compute_baseline_cumulative_hazards", "cox._CoxPHBase.values.reshape().dot", "numpy.exp", "cox._CoxPHBase.predict", "cox._CoxPHBase.values.reshape"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_cumulative_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict"], ["", "def", "_predict_cumulative_hazards", "(", "self", ",", "input", ",", "max_duration", ",", "batch_size", ",", "verbose", ",", "baseline_hazards_", ",", "\n", "eval_", "=", "True", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "max_duration", "=", "np", ".", "inf", "if", "max_duration", "is", "None", "else", "max_duration", "\n", "if", "baseline_hazards_", "is", "self", ".", "baseline_hazards_", ":", "\n", "            ", "bch", "=", "self", ".", "baseline_cumulative_hazards_", "\n", "", "else", ":", "\n", "            ", "bch", "=", "self", ".", "compute_baseline_cumulative_hazards", "(", "set_hazards", "=", "False", ",", "\n", "baseline_hazards_", "=", "baseline_hazards_", ")", "\n", "", "bch", "=", "bch", ".", "loc", "[", "lambda", "x", ":", "x", ".", "index", "<=", "max_duration", "]", "\n", "expg", "=", "np", ".", "exp", "(", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "return", "pd", ".", "DataFrame", "(", "bch", ".", "values", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "dot", "(", "expg", ")", ",", "\n", "index", "=", "bch", ".", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxPHBase.partial_log_likelihood": [[255, 288], ["cox._CoxPHBase.target_to_df", "cox._CoxPHBase.predict", "cox._CoxPHBase.assign().sort_values().assign().loc[].assign", "cox._CoxPHBase.assign().sort_values().assign", "numpy.log", "cox._CoxPHBase.assign().sort_values", "x[].pipe().cumsum().groupby().transform", "cox._CoxPHBase.assign", "x[].pipe().cumsum().groupby", "x[].pipe().cumsum", "x[].pipe"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.target_to_df", "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["", "def", "partial_log_likelihood", "(", "self", ",", "input", ",", "target", ",", "g_preds", "=", "None", ",", "batch_size", "=", "8224", ",", "eps", "=", "1e-7", ",", "eval_", "=", "True", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "'''Calculate the partial log-likelihood for the events in datafram df.\n        This likelihood does not sample the controls.\n        Note that censored data (non events) does not have a partial log-likelihood.\n\n        Arguments:\n            input {tuple, np.ndarray, or torch.tensor} -- Input to net.\n            target {tuple, np.ndarray, or torch.tensor} -- Target labels.\n\n        Keyword Arguments:\n            g_preds {np.array} -- Predictions from `model.predict` (default: {None})\n            batch_size {int} -- Batch size (default: {8224})\n            eval_ {bool} -- If 'True', use 'eval' mode on net. (default: {True})\n            num_workers {int} -- Number of workers in created dataloader (default: {0})\n\n        Returns:\n            Partial log-likelihood.\n        '''", "\n", "df", "=", "self", ".", "target_to_df", "(", "target", ")", "\n", "if", "g_preds", "is", "None", ":", "\n", "            ", "g_preds", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "True", ",", "eval_", ",", "num_workers", "=", "num_workers", ")", "\n", "", "return", "(", "df", "\n", ".", "assign", "(", "_g_preds", "=", "g_preds", ")", "\n", ".", "sort_values", "(", "self", ".", "duration_col", ",", "ascending", "=", "False", ")", "\n", ".", "assign", "(", "_cum_exp_g", "=", "(", "lambda", "x", ":", "x", "[", "'_g_preds'", "]", "\n", ".", "pipe", "(", "np", ".", "exp", ")", "\n", ".", "cumsum", "(", ")", "\n", ".", "groupby", "(", "x", "[", "self", ".", "duration_col", "]", ")", "\n", ".", "transform", "(", "'max'", ")", ")", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "self", ".", "event_col", "]", "==", "1", "]", "\n", ".", "assign", "(", "pll", "=", "lambda", "x", ":", "x", "[", "'_g_preds'", "]", "-", "np", ".", "log", "(", "x", "[", "'_cum_exp_g'", "]", "+", "eps", ")", ")", "\n", "[", "'pll'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPH.__init__": [[314, 318], ["super().__init__", "pycox.models.loss.CoxPHLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "CoxPHLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.__init__": [[344, 349], ["warnings.warn", "super().__init__", "pycox.models.loss.CoxPHLossSorted"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "'Use `CoxPH` instead. This will be removed'", ",", "DeprecationWarning", ")", "\n", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "CoxPHLossSorted", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader": [[350, 355], ["torchtuples.make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "@", "staticmethod", "\n", "def", "make_dataloader", "(", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "tt", ".", "make_dataloader", "(", "data", ",", "batch_size", ",", "shuffle", ",", "num_workers", ",", "\n", "make_dataset", "=", "models", ".", "data", ".", "DurationSortedDataset", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader_predict": [[356, 359], ["super().make_dataloader"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader"], ["", "def", "make_dataloader_predict", "(", "self", ",", "input", ",", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "dataloader", "=", "super", "(", ")", ".", "make_dataloader", "(", "input", ",", "batch_size", ",", "shuffle", ",", "num_workers", ")", "\n", "return", "dataloader", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.cox.search_sorted_idx": [[9, 22], ["len", "numpy.searchsorted", "any", "warnings.warn"], "function", ["None"], ["def", "search_sorted_idx", "(", "array", ",", "values", ")", ":", "\n", "    ", "'''For sorted array, get index of values.\n    If value not in array, give left index of value.\n    '''", "\n", "n", "=", "len", "(", "array", ")", "\n", "idx", "=", "np", ".", "searchsorted", "(", "array", ",", "values", ")", "\n", "idx", "[", "idx", "==", "n", "]", "=", "n", "-", "1", "# We can't have indexes higher than the length-1", "\n", "not_exact", "=", "values", "!=", "array", "[", "idx", "]", "\n", "idx", "-=", "not_exact", "\n", "if", "any", "(", "idx", "<", "0", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "'Given value smaller than first value'", ")", "\n", "idx", "[", "idx", "<", "0", "]", "=", "0", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col": [[7, 19], ["torch.zeros_like", "torch.zeros_like", "ValueError", "len", "ValueError", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["    "]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor": [[20, 23], ["warnings.warn", "torchtuples.utils.array_or_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor"], ["\n", "if", "assert_sorted", ":", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils.make_subgrid": [[24, 38], ["torchtuples.TupleTree", "tt.TupleTree.apply().flatten", "numpy.linspace", "zip", "tt.TupleTree.apply", "torchtuples.TupleTree"], "function", ["None"], ["        ", "assert", "pd", ".", "Series", "(", "index_surv", ")", ".", "is_monotonic_increasing", ",", "\"Need 'index_surv' to be monotonic increasing\"", "\n", "", "if", "steps", "==", "'pre'", ":", "\n", "        ", "idx", "=", "np", ".", "searchsorted", "(", "index_surv", ",", "times", ")", "\n", "", "elif", "steps", "==", "'post'", ":", "\n", "        ", "idx", "=", "np", ".", "searchsorted", "(", "index_surv", ",", "times", ",", "side", "=", "'right'", ")", "-", "1", "\n", "", "return", "idx", ".", "clip", "(", "0", ",", "len", "(", "index_surv", ")", "-", "1", ")", "\n", "\n", "", "@", "numba", ".", "njit", "\n", "def", "_group_loop", "(", "n", ",", "surv_idx", ",", "durations", ",", "events", ",", "di", ",", "ni", ")", ":", "\n", "    ", "idx", "=", "0", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "        ", "idx", "+=", "durations", "[", "i", "]", "!=", "surv_idx", "[", "idx", "]", "\n", "di", "[", "idx", "]", "+=", "events", "[", "i", "]", "\n", "ni", "[", "idx", "]", "+=", "1", "\n", "", "return", "di", ",", "ni", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils.log_softplus": [[39, 56], ["input.clone", "torch.softplus().log", "torch.softplus"], "function", ["None"], ["\n", "", "def", "kaplan_meier", "(", "durations", ",", "events", ",", "start_duration", "=", "0", ")", ":", "\n", "    ", "\"\"\"A very simple Kaplan-Meier fitter. For a more complete implementation\n    see `lifelines`.\n    \n    Arguments:\n        durations {np.array} -- durations array\n        events {np.arrray} -- events array 0/1\n    \n    Keyword Arguments:\n        start_duration {int} -- Time start as `start_duration`. (default: {0})\n    \n    Returns:\n        pd.Series -- Kaplan-Meier estimates.\n    \"\"\"", "\n", "n", "=", "len", "(", "durations", ")", "\n", "assert", "n", "==", "len", "(", "events", ")", "\n", "if", "start_duration", ">", "durations", ".", "min", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse": [[57, 62], ["input.sum", "pad_col().cumsum", "utils.pad_col"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["        ", "warnings", ".", "warn", "(", "f\"start_duration {start_duration} is larger than minimum duration {durations.min()}. \"", "\n", "\"If intentional, consider changing start_duration when calling kaplan_meier.\"", ")", "\n", "", "order", "=", "np", ".", "argsort", "(", "durations", ")", "\n", "durations", "=", "durations", "[", "order", "]", "\n", "events", "=", "events", "[", "order", "]", "\n", "surv_idx", "=", "np", ".", "unique", "(", "durations", ")", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.__init__": [[43, 47], ["super().__init__", "pycox.models.loss.NLLMTLRLoss"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "optimizer", "=", "None", ",", "device", "=", "None", ",", "duration_index", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "        ", "if", "loss", "is", "None", ":", "\n", "            ", "loss", "=", "models", ".", "loss", ".", "NLLMTLRLoss", "(", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "net", ",", "loss", ",", "optimizer", ",", "device", ",", "duration_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.mtlr.MTLR.predict_pmf": [[48, 53], ["mtlr.MTLR.predict", "pycox.models.utils.cumsum_reverse", "torchtuples.utils.array_or_tensor", "pycox.models.utils.pad_col().softmax", "pycox.models.utils.pad_col"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.array_or_tensor", "home.repos.pwc.inspect_result.havakv_pycox.simulations.discrete_logit_hazard.softmax", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "def", "predict_pmf", "(", "self", ",", "input", ",", "batch_size", "=", "8224", ",", "numpy", "=", "None", ",", "eval_", "=", "True", ",", "to_cpu", "=", "False", ",", "num_workers", "=", "0", ")", ":", "\n", "        ", "preds", "=", "self", ".", "predict", "(", "input", ",", "batch_size", ",", "False", ",", "eval_", ",", "False", ",", "to_cpu", ",", "num_workers", ")", "\n", "preds", "=", "utils", ".", "cumsum_reverse", "(", "preds", ",", "dim", "=", "1", ")", "\n", "pmf", "=", "utils", ".", "pad_col", "(", "preds", ")", ".", "softmax", "(", "1", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "return", "tt", ".", "utils", ".", "array_or_tensor", "(", "pmf", ",", "numpy", ",", "input", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_mtlr.test_mtlr_runs": [[8, 26], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.MTLR.label_transform", "MTLR.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.MTLR", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "pycox.models.MTLR.interpolate", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_mtlr_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "target", "=", "data", "\n", "labtrans", "=", "MTLR", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "labtrans", ".", "out_features", ")", "\n", "model", "=", "MTLR", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "cdi", "=", "model", ".", "interpolate", "(", "3", ",", "'const_pdf'", ")", "\n", "assert_survs", "(", "input", ",", "cdi", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard._make_dataset": [[10, 18], ["numpy.random.seed", "numpy.random.normal().astype", "numpy.arange().astype", "numpy.random.uniform().round().astype", "numpy.random.uniform().astype", "numpy.repeat", "numpy.random.normal", "numpy.arange", "numpy.ceil", "numpy.random.uniform().round", "numpy.random.uniform", "numpy.random.uniform"], "function", ["None"], ["def", "_make_dataset", "(", "n", ",", "m", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "x", "=", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ",", "(", "n", ",", "4", ")", ")", ".", "astype", "(", "'float32'", ")", "\n", "duration_index", "=", "np", ".", "arange", "(", "m", "+", "1", ")", ".", "astype", "(", "'int64'", ")", "\n", "durations", "=", "np", ".", "repeat", "(", "duration_index", ",", "np", ".", "ceil", "(", "n", "/", "m", ")", ")", "[", ":", "n", "]", "\n", "events", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "n", ")", ".", "round", "(", ")", ".", "astype", "(", "'float32'", ")", "\n", "fracs", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "n", ")", ".", "astype", "(", "'float32'", ")", "\n", "return", "x", ",", "(", "durations", ",", "events", ",", "fracs", ")", ",", "duration_index", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard.test_wrong_net_output": [[19, 41], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "test_pc_hazard._make_dataset", "torch.nn.Linear", "pycox.models.PCHazard", "pycox.models.PCHazard.make_dataloader", "pytest.raises", "pycox.models.PCHazard", "pytest.raises", "pycox.models.PCHazard.fit", "pytest.raises", "pycox.models.PCHazard.predict_surv_df", "pytest.raises", "pycox.models.PCHazard.fit_dataloader"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard._make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.fit_dataloader"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'n_mul'", ",", "[", "2", ",", "3", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'mp'", ",", "[", "1", ",", "2", ",", "-", "1", "]", ")", "\n", "def", "test_wrong_net_output", "(", "m", ",", "n_mul", ",", "mp", ")", ":", "\n", "    ", "n", "=", "m", "*", "n_mul", "\n", "inp", ",", "tar", ",", "dur_index", "=", "_make_dataset", "(", "n", ",", "m", ")", "\n", "net", "=", "torch", ".", "nn", ".", "Linear", "(", "inp", ".", "shape", "[", "1", "]", ",", "m", "+", "1", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "model", "=", "PCHazard", "(", "net", ",", "duration_index", "=", "dur_index", ")", "\n", "\n", "", "model", "=", "PCHazard", "(", "net", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "model", ".", "fit", "(", "inp", ",", "tar", ")", "\n", "\n", "", "model", ".", "duration_index", "=", "dur_index", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "model", ".", "predict_surv_df", "(", "inp", ")", "\n", "\n", "", "model", ".", "duration_index", "=", "dur_index", "\n", "dl", "=", "model", ".", "make_dataloader", "(", "(", "inp", ",", "tar", ")", ",", "5", ",", "True", ")", "\n", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "        ", "model", ".", "fit_dataloader", "(", "dl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard.test_right_net_output": [[42, 55], ["pytest.mark.parametrize", "pytest.mark.parametrize", "test_pc_hazard._make_dataset", "torch.nn.Linear", "pycox.models.PCHazard", "pycox.models.PCHazard", "pycox.models.PCHazard.fit", "pycox.models.PCHazard.predict_surv_df", "pycox.models.PCHazard.make_dataloader", "pycox.models.PCHazard.fit_dataloader"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard._make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader", "home.repos.pwc.inspect_result.havakv_pycox.models.pc_hazard.PCHazard.fit_dataloader"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'n_mul'", ",", "[", "2", ",", "3", "]", ")", "\n", "def", "test_right_net_output", "(", "m", ",", "n_mul", ")", ":", "\n", "    ", "n", "=", "m", "*", "n_mul", "\n", "inp", ",", "tar", ",", "dur_index", "=", "_make_dataset", "(", "n", ",", "m", ")", "\n", "net", "=", "torch", ".", "nn", ".", "Linear", "(", "inp", ".", "shape", "[", "1", "]", ",", "m", ")", "\n", "model", "=", "PCHazard", "(", "net", ")", "\n", "model", "=", "PCHazard", "(", "net", ",", "duration_index", "=", "dur_index", ")", "\n", "model", ".", "fit", "(", "inp", ",", "tar", ",", "verbose", "=", "False", ")", "\n", "model", ".", "predict_surv_df", "(", "inp", ")", "\n", "dl", "=", "model", ".", "make_dataloader", "(", "(", "inp", ",", "tar", ")", ",", "5", ",", "True", ")", "\n", "model", ".", "fit_dataloader", "(", "dl", ")", "\n", "assert", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_pc_hazard.test_pc_hazard_runs": [[56, 74], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.PCHazard.label_transform", "PCHazard.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.PCHazard", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "3", ",", "8", "]", ")", "\n", "def", "test_pc_hazard_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "(", "durations", ",", "events", ")", "=", "data", "\n", "durations", "+=", "1", "\n", "target", "=", "(", "durations", ",", "events", ")", "\n", "labtrans", "=", "PCHazard", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "num_durations", ")", "\n", "model", "=", "PCHazard", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_bce_surv.test_pmf_runs": [[8, 26], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.BCESurv.label_transform", "BCESurv.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.BCESurv", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "pycox.models.BCESurv.interpolate", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_pmf_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "target", "=", "data", "\n", "labtrans", "=", "BCESurv", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "labtrans", ".", "out_features", ")", "\n", "model", "=", "BCESurv", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "cdi", "=", "model", ".", "interpolate", "(", "3", ",", "'const_pdf'", ")", "\n", "assert_survs", "(", "input", ",", "cdi", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_logistic_hazard.test_logistic_hazard_runs": [[8, 26], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.LogisticHazard.label_transform", "LogisticHazard.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.LogisticHazard", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "pycox.models.LogisticHazard.interpolate", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_logistic_hazard_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "target", "=", "data", "\n", "labtrans", "=", "LogisticHazard", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "labtrans", ".", "out_features", ")", "\n", "model", "=", "LogisticHazard", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "cdi", "=", "model", ".", "interpolate", "(", "3", ",", "'const_pdf'", ")", "\n", "assert_survs", "(", "input", ",", "cdi", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset": [[6, 20], ["torch.randn", "torch.arange().repeat", "torch.arange().repeat().float", "torchtuples.tuplefy", "int", "data.to_numpy.to_numpy", "torch.arange", "torch.arange().repeat", "int", "torch.arange"], "function", ["None"], ["def", "make_dataset", "(", "numpy", ")", ":", "\n", "    ", "n_events", "=", "2", "\n", "n_frac", "=", "4", "\n", "m", "=", "10", "\n", "n", "=", "m", "*", "n_frac", "*", "n_events", "\n", "p", "=", "5", "\n", "input", "=", "torch", ".", "randn", "(", "(", "n", ",", "p", ")", ")", "\n", "durations", "=", "torch", ".", "arange", "(", "m", ")", ".", "repeat", "(", "int", "(", "n", "/", "m", ")", ")", "\n", "events", "=", "torch", ".", "arange", "(", "n_events", ")", ".", "repeat", "(", "int", "(", "n", "/", "n_events", ")", ")", ".", "float", "(", ")", "\n", "target", "=", "(", "durations", ",", "events", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_numpy", "(", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model": [[21, 24], ["model.fit"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["", "def", "fit_model", "(", "data", ",", "model", ")", ":", "\n", "    ", "model", ".", "fit", "(", "*", "data", ",", "epochs", "=", "1", ",", "verbose", "=", "False", ",", "val_data", "=", "data", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs": [[25, 44], ["model.predict_surv", "model.predict_surv_df", "model.predict_surv", "model.predict_surv", "type", "type", "type", "type", "torchtuples.tuplefy().to_numpy", "torchtuples.tuplefy().to_tensor", "torchtuples.tuplefy().make_dataloader", "model.predict_surv", "type", "type", "torchtuples.tuplefy", "torchtuples.tuplefy", "model.predict_surv.cpu().numpy", "torchtuples.tuplefy", "type", "type", "model.predict_surv.cpu"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv", "home.repos.pwc.inspect_result.havakv_pycox.models.cox.CoxPHSorted.make_dataloader", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv"], ["", "def", "assert_survs", "(", "input", ",", "model", ",", "with_dl", "=", "True", ")", ":", "\n", "    ", "preds", "=", "model", ".", "predict_surv", "(", "input", ")", "\n", "assert", "type", "(", "preds", ")", "is", "type", "(", "input", ")", "\n", "assert", "preds", ".", "shape", "[", "0", "]", "==", "input", ".", "shape", "[", "0", "]", "\n", "surv_df", "=", "model", ".", "predict_surv_df", "(", "input", ")", "\n", "assert", "type", "(", "surv_df", ")", "is", "pd", ".", "DataFrame", "\n", "assert", "type", "(", "surv_df", ".", "values", ")", "is", "np", ".", "ndarray", "\n", "assert", "preds", ".", "shape", "[", "0", "]", "==", "surv_df", ".", "shape", "[", "1", "]", "\n", "assert", "preds", ".", "shape", "[", "1", "]", "==", "surv_df", ".", "shape", "[", "0", "]", "\n", "np_input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "to_numpy", "(", ")", "[", "0", "]", "\n", "torch_input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "to_tensor", "(", ")", "[", "0", "]", "\n", "np_preds", "=", "model", ".", "predict_surv", "(", "np_input", ")", "\n", "torch_preds", "=", "model", ".", "predict_surv", "(", "torch_input", ")", "\n", "assert", "(", "np_preds", "==", "torch_preds", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ".", "all", "(", ")", "\n", "if", "with_dl", ":", "\n", "        ", "dl_input", "=", "tt", ".", "tuplefy", "(", "input", ")", ".", "make_dataloader", "(", "512", ",", "False", ")", "\n", "dl_preds", "=", "model", ".", "predict_surv", "(", "dl_input", ")", "\n", "assert", "type", "(", "np_preds", ")", "is", "type", "(", "dl_preds", ")", ",", "f\"got {type(np_preds)}, and, {type(dl_preds)}\"", "\n", "assert", "(", "np_preds", "==", "dl_preds", ")", ".", "all", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_cox_time.test_cox_time_runs": [[9, 22], ["pytest.mark.parametrize", "utils_model_testing.make_dataset().apply().to_numpy", "pycox.models.CoxTime.label_transform", "CoxTime.label_transform.fit_transform", "torchtuples.tuplefy", "pycox.models.cox_time.MLPVanillaCoxTime", "pycox.models.CoxTime", "utils_model_testing.fit_model", "pycox.models.CoxTime.compute_baseline_hazards", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor", "utils_model_testing.make_dataset().apply", "utils_model_testing.make_dataset", "x.float"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "def", "test_cox_time_runs", "(", "numpy", ")", ":", "\n", "    ", "input", ",", "target", "=", "make_dataset", "(", "False", ")", ".", "apply", "(", "lambda", "x", ":", "x", ".", "float", "(", ")", ")", ".", "to_numpy", "(", ")", "\n", "labtrans", "=", "CoxTime", ".", "label_transform", "(", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "MLPVanillaCoxTime", "(", "data", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "False", ")", "\n", "model", "=", "CoxTime", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "model", ".", "compute_baseline_hazards", "(", ")", "\n", "assert_survs", "(", "data", "[", "0", "]", ",", "model", ",", "with_dl", "=", "False", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_nll_pmf_cr_equals_nll_pmf": [[9, 23], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "torch.randn", "torch.arange().repeat", "torch.arange().repeat", "pycox.models.loss.nll_pmf_cr", "pycox.models.loss.nll_pmf", "torch.randn.view", "torch.arange().repeat.float", "torch.arange", "torch.arange"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf_cr", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'seed'", ",", "[", "0", ",", "1", ",", "2", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "1", ",", "5", ",", "8", "]", ")", "\n", "def", "test_nll_pmf_cr_equals_nll_pmf", "(", "seed", ",", "m", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# m = 5", "\n", "n_risk", "=", "1", "\n", "rep", "=", "7", "\n", "batch", "=", "m", "*", "(", "n_risk", "+", "1", ")", "*", "rep", "\n", "phi", "=", "torch", ".", "randn", "(", "batch", ",", "n_risk", ",", "m", ")", "\n", "idx_duration", "=", "torch", ".", "arange", "(", "m", ")", ".", "repeat", "(", "rep", "*", "(", "n_risk", "+", "1", ")", ")", "\n", "events", "=", "torch", ".", "arange", "(", "n_risk", "+", "1", ")", ".", "repeat", "(", "m", "*", "rep", ")", "\n", "r1", "=", "loss", ".", "nll_pmf_cr", "(", "phi", ",", "idx_duration", ",", "events", ")", "\n", "r2", "=", "loss", ".", "nll_pmf", "(", "phi", ".", "view", "(", "batch", "*", "n_risk", ",", "-", "1", ")", ",", "idx_duration", ",", "events", ".", "float", "(", ")", ")", "\n", "assert", "(", "r1", "-", "r2", ")", ".", "abs", "(", ")", "<", "1e-5", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_rank_loss_deephit_cr_equals_single": [[24, 41], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "torch.randn", "torch.arange().repeat", "torch.arange().repeat", "pycox.models.data.pair_rank_mat", "torch.tensor", "pycox.models.loss.rank_loss_deephit_cr", "pycox.models.loss.rank_loss_deephit_single", "torch.arange().repeat.numpy", "torch.arange().repeat.numpy", "torch.randn.view", "torch.arange().repeat.float", "torch.arange", "torch.arange"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.data.pair_rank_mat", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_cr", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.rank_loss_deephit_single"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'seed'", ",", "[", "0", ",", "1", ",", "2", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "1", ",", "5", ",", "8", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sigma'", ",", "[", "0.1", ",", "0.2", ",", "1.", "]", ")", "\n", "def", "test_rank_loss_deephit_cr_equals_single", "(", "seed", ",", "m", ",", "sigma", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "n_risk", "=", "1", "\n", "rep", "=", "7", "\n", "batch", "=", "m", "*", "(", "n_risk", "+", "1", ")", "*", "rep", "\n", "phi", "=", "torch", ".", "randn", "(", "batch", ",", "n_risk", ",", "m", ")", "\n", "idx_duration", "=", "torch", ".", "arange", "(", "m", ")", ".", "repeat", "(", "rep", "*", "(", "n_risk", "+", "1", ")", ")", "\n", "events", "=", "torch", ".", "arange", "(", "n_risk", "+", "1", ")", ".", "repeat", "(", "m", "*", "rep", ")", "\n", "rank_mat", "=", "pair_rank_mat", "(", "idx_duration", ".", "numpy", "(", ")", ",", "events", ".", "numpy", "(", ")", ")", "\n", "rank_mat", "=", "torch", ".", "tensor", "(", "rank_mat", ")", "\n", "r1", "=", "loss", ".", "rank_loss_deephit_cr", "(", "phi", ",", "idx_duration", ",", "events", ",", "rank_mat", ",", "sigma", ")", "\n", "r2", "=", "loss", ".", "rank_loss_deephit_single", "(", "phi", ".", "view", "(", "batch", ",", "-", "1", ")", ",", "idx_duration", ",", "events", ".", "float", "(", ")", ",", "\n", "rank_mat", ",", "sigma", ")", "\n", "assert", "(", "r1", "-", "r2", ")", ".", "abs", "(", ")", "<", "1e-6", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_loss_deephit_cr_equals_single": [[43, 62], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "torch.randn", "torch.arange().repeat", "torch.arange().repeat", "pycox.models.data.pair_rank_mat", "torch.tensor", "pycox.models.loss.DeepHitLoss", "pycox.models.loss.DeepHitSingleLoss", "loss.DeepHitLoss.", "loss.DeepHitSingleLoss.", "torch.arange().repeat.numpy", "torch.arange().repeat.numpy", "torch.randn.view", "torch.arange().repeat.float", "torch.arange", "torch.arange"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.data.pair_rank_mat"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'seed'", ",", "[", "0", ",", "1", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "1", ",", "8", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sigma'", ",", "[", "0.1", ",", "1.", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'alpha'", ",", "[", "1", ",", "0.5", ",", "0.", "]", ")", "\n", "def", "test_loss_deephit_cr_equals_single", "(", "seed", ",", "m", ",", "sigma", ",", "alpha", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "n_risk", "=", "1", "\n", "rep", "=", "7", "\n", "batch", "=", "m", "*", "(", "n_risk", "+", "1", ")", "*", "rep", "\n", "phi", "=", "torch", ".", "randn", "(", "batch", ",", "n_risk", ",", "m", ")", "\n", "idx_duration", "=", "torch", ".", "arange", "(", "m", ")", ".", "repeat", "(", "rep", "*", "(", "n_risk", "+", "1", ")", ")", "\n", "events", "=", "torch", ".", "arange", "(", "n_risk", "+", "1", ")", ".", "repeat", "(", "m", "*", "rep", ")", "\n", "rank_mat", "=", "pair_rank_mat", "(", "idx_duration", ".", "numpy", "(", ")", ",", "events", ".", "numpy", "(", ")", ")", "\n", "rank_mat", "=", "torch", ".", "tensor", "(", "rank_mat", ")", "\n", "loss_cr", "=", "loss", ".", "DeepHitLoss", "(", "alpha", ",", "sigma", ")", "\n", "loss_single", "=", "loss", ".", "DeepHitSingleLoss", "(", "alpha", ",", "sigma", ")", "\n", "r1", "=", "loss_cr", "(", "phi", ",", "idx_duration", ",", "events", ",", "rank_mat", ")", "\n", "r2", "=", "loss_single", "(", "phi", ".", "view", "(", "batch", ",", "-", "1", ")", ",", "idx_duration", ",", "events", ".", "float", "(", ")", ",", "rank_mat", ")", "\n", "assert", "(", "r1", "-", "r2", ")", ".", "abs", "(", ")", "<", "1e-5", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_cox_cc_loss_single_ctrl": [[63, 74], ["pytest.mark.parametrize", "pytest.mark.parametrize", "numpy.random.seed", "numpy.random.uniform", "numpy.random.uniform", "torchtuples.tuplefy().to_tensor", "pycox.models.loss.cox_cc_loss", "pycox.models.loss.cox_cc_loss_single_ctrl", "torchtuples.tuplefy"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss_single_ctrl"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'seed'", ",", "[", "0", ",", "1", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'shrink'", ",", "[", "0", ",", "0.01", ",", "1.", "]", ")", "\n", "def", "test_cox_cc_loss_single_ctrl", "(", "seed", ",", "shrink", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "n", "=", "100", "\n", "case", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "n", ")", "\n", "ctrl", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "n", ")", "\n", "case", ",", "ctrl", "=", "tt", ".", "tuplefy", "(", "case", ",", "ctrl", ")", ".", "to_tensor", "(", ")", "\n", "loss_1", "=", "loss", ".", "cox_cc_loss", "(", "case", ",", "(", "ctrl", ",", ")", ",", "shrink", ")", "\n", "loss_2", "=", "loss", ".", "cox_cc_loss_single_ctrl", "(", "case", ",", "ctrl", ",", "shrink", ")", "\n", "assert", "(", "loss_1", "-", "loss_2", ")", ".", "abs", "(", ")", "<", "1e-6", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_cox_cc_loss_single_ctrl_zero": [[75, 82], ["pytest.mark.parametrize", "torch.zeros", "pycox.models.loss.cox_cc_loss_single_ctrl", "torch.tensor().log", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss_single_ctrl"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'shrink'", ",", "[", "0", ",", "0.01", ",", "1.", "]", ")", "\n", "def", "test_cox_cc_loss_single_ctrl_zero", "(", "shrink", ")", ":", "\n", "    ", "n", "=", "10", "\n", "case", "=", "ctrl", "=", "torch", ".", "zeros", "(", "n", ")", "\n", "loss_1", "=", "loss", ".", "cox_cc_loss_single_ctrl", "(", "case", ",", "ctrl", ",", "shrink", ")", "\n", "val", "=", "torch", ".", "tensor", "(", "2.", ")", ".", "log", "(", ")", "\n", "assert", "(", "loss_1", "-", "val", ")", ".", "abs", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_cox_cc_loss_zero": [[83, 90], ["pytest.mark.parametrize", "torch.zeros", "pycox.models.loss.cox_cc_loss", "torch.tensor().log", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.cox_cc_loss"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'shrink'", ",", "[", "0", ",", "0.01", ",", "1.", "]", ")", "\n", "def", "test_cox_cc_loss_zero", "(", "shrink", ")", ":", "\n", "    ", "n", "=", "10", "\n", "case", "=", "ctrl", "=", "torch", ".", "zeros", "(", "n", ")", "\n", "loss_1", "=", "loss", ".", "cox_cc_loss", "(", "case", ",", "(", "ctrl", ",", ")", ",", "shrink", ")", "\n", "val", "=", "torch", ".", "tensor", "(", "2.", ")", ".", "log", "(", ")", "\n", "assert", "(", "loss_1", "-", "val", ")", ".", "abs", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_loss.test_nll_mtlr_zero": [[91, 105], ["torch.zeros", "torch.arange().repeat", "torch.ones_like().float", "pycox.models.loss.nll_pmf", "pycox.models.loss.nll_mtlr", "torch.zeros_like", "pycox.models.loss.nll_pmf", "pycox.models.loss.nll_mtlr", "torch.arange", "torch.ones_like"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_mtlr", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_pmf", "home.repos.pwc.inspect_result.havakv_pycox.models.loss.nll_mtlr"], ["", "def", "test_nll_mtlr_zero", "(", ")", ":", "\n", "    ", "n_frac", "=", "4", "\n", "m", "=", "5", "\n", "n", "=", "m", "*", "n_frac", "\n", "phi", "=", "torch", ".", "zeros", "(", "n", ",", "m", ")", "\n", "idx_durations", "=", "torch", ".", "arange", "(", "m", ")", ".", "repeat", "(", "n_frac", ")", "\n", "events", "=", "torch", ".", "ones_like", "(", "idx_durations", ")", ".", "float", "(", ")", "\n", "loss_pmf", "=", "loss", ".", "nll_pmf", "(", "phi", ",", "idx_durations", ",", "events", ")", "\n", "loss_mtlr", "=", "loss", ".", "nll_mtlr", "(", "phi", ",", "idx_durations", ",", "events", ")", "\n", "assert", "(", "loss_pmf", "-", "loss_mtlr", ")", ".", "abs", "(", ")", "==", "0", "\n", "events", "=", "torch", ".", "zeros_like", "(", "events", ")", "\n", "loss_pmf", "=", "loss", ".", "nll_pmf", "(", "phi", ",", "idx_durations", ",", "events", ")", "\n", "loss_mtlr", "=", "loss", ".", "nll_mtlr", "(", "phi", ",", "idx_durations", ",", "events", ")", "\n", "assert", "(", "loss_pmf", "-", "loss_mtlr", ")", ".", "abs", "(", ")", "==", "0", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockPMF.__init__": [[8, 10], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockPMF.predict": [[11, 13], ["None"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "input", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockLogisticHazard.__init__": [[15, 17], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockLogisticHazard.predict": [[18, 20], ["None"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "input", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.__init__": [[22, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "duration_index", "=", "None", ")", ":", "\n", "        ", "self", ".", "duration_index", "=", "duration_index", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.MockMTLR.predict": [[25, 27], ["None"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "input", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.test_pmf_cdi_equals_base": [[29, 41], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "[].numpy", "torch.randn", "test_interpolation.MockPMF", "MockPMF.interpolate().predict_surv_df", "pycox.models.interpolation.InterpolateDiscrete().predict_surv_df", "MockPMF.interpolate", "pycox.models.interpolation.InterpolateDiscrete", "torch.randn().abs().sort", "torch.randn().abs", "torch.randn"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "2", ",", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_pmf_cdi_equals_base", "(", "m", ",", "sub", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "n", "=", "20", "\n", "idx", "=", "torch", ".", "randn", "(", "m", ")", ".", "abs", "(", ")", ".", "sort", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "input", "=", "torch", ".", "randn", "(", "n", ",", "m", ")", "\n", "model", "=", "MockPMF", "(", "idx", ")", "\n", "surv_pmf", "=", "model", ".", "interpolate", "(", "sub", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "surv_base", "=", "InterpolateDiscrete", "(", "model", ",", "duration_index", "=", "idx", ",", "sub", "=", "sub", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "assert", "(", "surv_pmf", ".", "index", "==", "surv_base", ".", "index", ")", ".", "all", "(", ")", "\n", "assert", "(", "surv_pmf", "-", "surv_base", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.test_base_values_at_knots": [[43, 56], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "[].numpy", "torch.randn", "test_interpolation.MockPMF", "pycox.models.interpolation.InterpolateDiscrete().predict_surv_df", "MockPMF.predict_surv_df", "pycox.models.interpolation.InterpolateDiscrete", "torch.randn().abs().sort", "torch.randn().abs", "torch.randn"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "2", ",", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_base_values_at_knots", "(", "m", ",", "sub", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "n", "=", "20", "\n", "idx", "=", "torch", ".", "randn", "(", "m", ")", ".", "abs", "(", ")", ".", "sort", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "input", "=", "torch", ".", "randn", "(", "n", ",", "m", ")", "\n", "model", "=", "MockPMF", "(", "idx", ")", "\n", "surv_cdi", "=", "InterpolateDiscrete", "(", "model", ",", "duration_index", "=", "idx", ",", "sub", "=", "sub", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "surv", "=", "model", ".", "predict_surv_df", "(", "input", ")", "\n", "diff", "=", "(", "surv", "-", "surv_cdi", ")", ".", "dropna", "(", ")", "\n", "assert", "diff", ".", "shape", "==", "surv", ".", "shape", "\n", "assert", "(", "diff", "==", "0", ")", ".", "all", "(", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.test_pmf_values_at_knots": [[57, 70], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "[].numpy", "torch.randn", "test_interpolation.MockPMF", "MockPMF.predict_surv_df", "MockPMF.interpolate().predict_surv_df", "diff.max().max", "MockPMF.interpolate", "torch.randn().abs().sort", "diff.max", "torch.randn().abs", "torch.randn"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "2", ",", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_pmf_values_at_knots", "(", "m", ",", "sub", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "n", "=", "20", "\n", "idx", "=", "torch", ".", "randn", "(", "m", ")", ".", "abs", "(", ")", ".", "sort", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "input", "=", "torch", ".", "randn", "(", "n", ",", "m", ")", "\n", "model", "=", "MockPMF", "(", "idx", ")", "\n", "surv", "=", "model", ".", "predict_surv_df", "(", "input", ")", "\n", "surv_cdi", "=", "model", ".", "interpolate", "(", "sub", ",", "'const_pdf'", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "diff", "=", "(", "surv", "-", "surv_cdi", ")", ".", "dropna", "(", ")", "\n", "assert", "diff", ".", "shape", "==", "surv", ".", "shape", "\n", "assert", "diff", ".", "max", "(", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.test_logistic_hazard_values_at_knots": [[71, 89], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "[].numpy", "torch.randn", "test_interpolation.MockLogisticHazard", "MockLogisticHazard.predict_surv_df", "MockLogisticHazard.interpolate().predict_surv_df", "MockLogisticHazard.interpolate().predict_surv_df", "diff.max().max", "MockLogisticHazard.interpolate", "MockLogisticHazard.interpolate", "torch.randn().abs().sort", "diff.max", "torch.randn().abs", "torch.randn"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "2", ",", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_logistic_hazard_values_at_knots", "(", "m", ",", "sub", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "n", "=", "20", "\n", "idx", "=", "torch", ".", "randn", "(", "m", ")", ".", "abs", "(", ")", ".", "sort", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "input", "=", "torch", ".", "randn", "(", "n", ",", "m", ")", "\n", "model", "=", "MockLogisticHazard", "(", "idx", ")", "\n", "surv", "=", "model", ".", "predict_surv_df", "(", "input", ")", "\n", "surv_cdi", "=", "model", ".", "interpolate", "(", "sub", ",", "'const_pdf'", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "diff", "=", "(", "surv", "-", "surv_cdi", ")", ".", "dropna", "(", ")", "\n", "assert", "diff", ".", "shape", "==", "surv", ".", "shape", "\n", "assert", "(", "diff", "==", "0", ")", ".", "all", "(", ")", ".", "all", "(", ")", "\n", "surv_chi", "=", "model", ".", "interpolate", "(", "sub", ",", "'const_hazard'", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "diff", "=", "(", "surv", "-", "surv_chi", ")", ".", "dropna", "(", ")", "\n", "assert", "diff", ".", "shape", "==", "surv", ".", "shape", "\n", "assert", "(", "diff", ".", "index", "==", "surv", ".", "index", ")", ".", "all", "(", ")", "\n", "assert", "diff", ".", "max", "(", ")", ".", "max", "(", ")", "<", "1e-6", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_interpolation.test_mtlr_values_at_knots": [[90, 103], ["pytest.mark.parametrize", "pytest.mark.parametrize", "torch.manual_seed", "[].numpy", "torch.randn", "test_interpolation.MockMTLR", "MockMTLR.predict_surv_df", "MockMTLR.interpolate().predict_surv_df", "diff.max().max", "MockMTLR.interpolate", "torch.randn().abs().sort", "diff.max", "torch.randn().abs", "torch.randn"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.predict_surv_df", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'m'", ",", "[", "2", ",", "5", ",", "10", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_mtlr_values_at_knots", "(", "m", ",", "sub", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "n", "=", "20", "\n", "idx", "=", "torch", ".", "randn", "(", "m", ")", ".", "abs", "(", ")", ".", "sort", "(", ")", "[", "0", "]", ".", "numpy", "(", ")", "\n", "input", "=", "torch", ".", "randn", "(", "n", ",", "m", ")", "\n", "model", "=", "MockMTLR", "(", "idx", ")", "\n", "surv", "=", "model", ".", "predict_surv_df", "(", "input", ")", "\n", "surv_cdi", "=", "model", ".", "interpolate", "(", "sub", ",", "'const_pdf'", ")", ".", "predict_surv_df", "(", "input", ")", "\n", "diff", "=", "(", "surv", "-", "surv_cdi", ")", ".", "dropna", "(", ")", "\n", "assert", "diff", ".", "shape", "==", "surv", ".", "shape", "\n", "assert", "diff", ".", "max", "(", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_pad_col_start": [[6, 12], ["pytest.mark.parametrize", "torch.ones", "pycox.models.utils.pad_col", "torch.ones", "torch.cat"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'val'", ",", "[", "0", ",", "1", ",", "5", "]", ")", "\n", "def", "test_pad_col_start", "(", "val", ")", ":", "\n", "    ", "x", "=", "torch", ".", "ones", "(", "(", "2", ",", "3", ")", ")", "\n", "x_pad", "=", "pad_col", "(", "x", ",", "val", ",", "where", "=", "'start'", ")", "\n", "pad", "=", "torch", ".", "ones", "(", "2", ",", "1", ")", "*", "val", "\n", "assert", "(", "x_pad", "==", "torch", ".", "cat", "(", "[", "pad", ",", "x", "]", ",", "dim", "=", "1", ")", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_pad_col_end": [[13, 19], ["pytest.mark.parametrize", "torch.ones", "pycox.models.utils.pad_col", "torch.ones", "torch.cat"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.pad_col"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'val'", ",", "[", "0", ",", "1", ",", "5", "]", ")", "\n", "def", "test_pad_col_end", "(", "val", ")", ":", "\n", "    ", "x", "=", "torch", ".", "ones", "(", "(", "2", ",", "3", ")", ")", "\n", "x_pad", "=", "pad_col", "(", "x", ",", "val", ")", "\n", "pad", "=", "torch", ".", "ones", "(", "2", ",", "1", ")", "*", "val", "\n", "assert", "(", "x_pad", "==", "torch", ".", "cat", "(", "[", "x", ",", "pad", "]", ",", "dim", "=", "1", ")", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_make_subgrid_1": [[20, 27], ["pytest.mark.parametrize", "numpy.random.uniform", "numpy.sort", "pycox.models.utils.make_subgrid", "len", "len"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.make_subgrid"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'n'", ",", "[", "2", ",", "13", ",", "40", "]", ")", "\n", "def", "test_make_subgrid_1", "(", "n", ")", ":", "\n", "    ", "grid", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "100", ",", "n", ")", "\n", "grid", "=", "np", ".", "sort", "(", "grid", ")", "\n", "new_grid", "=", "make_subgrid", "(", "grid", ",", "1", ")", "\n", "assert", "len", "(", "new_grid", ")", "==", "len", "(", "grid", ")", "\n", "assert", "(", "new_grid", "==", "grid", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_make_subgrid": [[28, 38], ["pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "pytest.mark.parametrize", "numpy.linspace", "pycox.models.utils.make_subgrid", "numpy.linspace", "len", "len", "numpy.abs().max", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.make_subgrid"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'sub'", ",", "[", "2", ",", "10", ",", "20", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'start'", ",", "[", "0", ",", "2", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'stop'", ",", "[", "4", ",", "100", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'n'", ",", "[", "5", ",", "10", "]", ")", "\n", "def", "test_make_subgrid", "(", "sub", ",", "start", ",", "stop", ",", "n", ")", ":", "\n", "    ", "grid", "=", "np", ".", "linspace", "(", "start", ",", "stop", ",", "n", ")", "\n", "new_grid", "=", "make_subgrid", "(", "grid", ",", "sub", ")", "\n", "true_new", "=", "np", ".", "linspace", "(", "start", ",", "stop", ",", "n", "*", "sub", "-", "(", "sub", "-", "1", ")", ")", "\n", "assert", "len", "(", "new_grid", ")", "==", "len", "(", "true_new", ")", "\n", "assert", "np", ".", "abs", "(", "true_new", "-", "new_grid", ")", ".", "max", "(", ")", "<", "1e-13", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_cumsum_reverse_error_dim": [[39, 45], ["torch.randn", "pytest.raises", "pycox.models.utils.cumsum_reverse", "pytest.raises", "pycox.models.utils.cumsum_reverse"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse", "home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse"], ["", "def", "test_cumsum_reverse_error_dim", "(", ")", ":", "\n", "    ", "x", "=", "torch", ".", "randn", "(", "(", "5", ",", "3", ")", ")", "\n", "with", "pytest", ".", "raises", "(", "NotImplementedError", ")", ":", "\n", "        ", "cumsum_reverse", "(", "x", ",", "dim", "=", "0", ")", "\n", "", "with", "pytest", ".", "raises", "(", "NotImplementedError", ")", ":", "\n", "        ", "cumsum_reverse", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_models_utils.test_cumsum_reverse_dim_1": [[46, 52], ["torch.manual_seed", "torch.randn", "pycox.models.utils.cumsum_reverse", "[].cumsum", "numpy.abs().max", "numpy.abs", "torch.randn.numpy", "pycox.models.utils.cumsum_reverse.numpy"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils.cumsum_reverse"], ["", "", "def", "test_cumsum_reverse_dim_1", "(", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "1234", ")", "\n", "x", "=", "torch", ".", "randn", "(", "5", ",", "16", ")", "\n", "res_np", "=", "x", ".", "numpy", "(", ")", "[", ":", ",", ":", ":", "-", "1", "]", ".", "cumsum", "(", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "res", "=", "cumsum_reverse", "(", "x", ",", "dim", "=", "1", ")", "\n", "assert", "np", ".", "abs", "(", "res", ".", "numpy", "(", ")", "-", "res_np", ")", ".", "max", "(", ")", "<", "1e-6", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_cox.test_cox_cc_runs": [[9, 19], ["pytest.mark.parametrize", "utils_model_testing.make_dataset().apply().to_numpy", "torchtuples.practical.MLPVanilla", "pycox.models.CoxPH", "utils_model_testing.fit_model", "pycox.models.CoxPH.compute_baseline_hazards", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor", "utils_model_testing.make_dataset().apply", "utils_model_testing.make_dataset", "x.float"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "def", "test_cox_cc_runs", "(", "numpy", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "False", ")", ".", "apply", "(", "lambda", "x", ":", "x", ".", "float", "(", ")", ")", ".", "to_numpy", "(", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "data", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "1", ",", "False", ",", "output_bias", "=", "False", ")", "\n", "model", "=", "CoxPH", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "model", ".", "compute_baseline_hazards", "(", ")", "\n", "assert_survs", "(", "data", "[", "0", "]", ",", "model", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_deephit.test_deep_hit_single_runs": [[8, 26], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.DeepHitSingle.label_transform", "DeepHitSingle.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.DeepHitSingle", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "pycox.models.DeepHitSingle.interpolate", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_deep_hit_single_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "target", "=", "data", "\n", "labtrans", "=", "DeepHitSingle", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "labtrans", ".", "out_features", ")", "\n", "model", "=", "DeepHitSingle", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "cdi", "=", "model", ".", "interpolate", "(", "3", ",", "'const_pdf'", ")", "\n", "assert_survs", "(", "input", ",", "cdi", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_cox_cc.test_cox_cc_runs": [[9, 19], ["pytest.mark.parametrize", "utils_model_testing.make_dataset().apply().to_numpy", "torchtuples.practical.MLPVanilla", "pycox.models.CoxCC", "utils_model_testing.fit_model", "pycox.models.CoxCC.compute_baseline_hazards", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor", "utils_model_testing.make_dataset().apply", "utils_model_testing.make_dataset", "x.float"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.compute_baseline_hazards", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "def", "test_cox_cc_runs", "(", "numpy", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "False", ")", ".", "apply", "(", "lambda", "x", ":", "x", ".", "float", "(", ")", ")", ".", "to_numpy", "(", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "data", "[", "0", "]", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "1", ",", "False", ",", "output_bias", "=", "False", ")", "\n", "model", "=", "CoxCC", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "model", ".", "compute_baseline_hazards", "(", ")", "\n", "assert_survs", "(", "data", "[", "0", "]", ",", "model", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.models.test_pmf.test_pmf_runs": [[8, 26], ["pytest.mark.parametrize", "pytest.mark.parametrize", "utils_model_testing.make_dataset", "pycox.models.PMF.label_transform", "PMF.label_transform.fit_transform", "torchtuples.tuplefy", "torchtuples.practical.MLPVanilla", "pycox.models.PMF", "utils_model_testing.fit_model", "utils_model_testing.assert_survs", "utils_model_testing.assert_survs", "pycox.models.PMF.interpolate", "utils_model_testing.assert_survs", "data.to_tensor.to_tensor"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.make_dataset", "home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization._OnlyTransform.fit_transform", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.fit_model", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs", "home.repos.pwc.inspect_result.havakv_pycox.models.pmf.PMFBase.interpolate", "home.repos.pwc.inspect_result.havakv_pycox.models.utils_model_testing.assert_survs"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "'numpy'", ",", "[", "True", ",", "False", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'num_durations'", ",", "[", "2", ",", "5", "]", ")", "\n", "def", "test_pmf_runs", "(", "numpy", ",", "num_durations", ")", ":", "\n", "    ", "data", "=", "make_dataset", "(", "True", ")", "\n", "input", ",", "target", "=", "data", "\n", "labtrans", "=", "PMF", ".", "label_transform", "(", "num_durations", ")", "\n", "target", "=", "labtrans", ".", "fit_transform", "(", "*", "target", ")", "\n", "data", "=", "tt", ".", "tuplefy", "(", "input", ",", "target", ")", "\n", "if", "not", "numpy", ":", "\n", "        ", "data", "=", "data", ".", "to_tensor", "(", ")", "\n", "", "net", "=", "tt", ".", "practical", ".", "MLPVanilla", "(", "input", ".", "shape", "[", "1", "]", ",", "[", "4", "]", ",", "labtrans", ".", "out_features", ")", "\n", "model", "=", "PMF", "(", "net", ")", "\n", "fit_model", "(", "data", ",", "model", ")", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "model", ".", "duration_index", "=", "labtrans", ".", "cuts", "\n", "assert_survs", "(", "input", ",", "model", ")", "\n", "cdi", "=", "model", ".", "interpolate", "(", "3", ",", "'const_pdf'", ")", "\n", "assert_survs", "(", "input", ",", "cdi", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.metrics.partial_log_likelihood_ph": [[7, 36], ["pandas.DataFrame", "dict", "pd.DataFrame.sort_values().assign().loc[].assign", "pll.mean", "pd.DataFrame.sort_values().assign", "numpy.log", "pd.DataFrame.sort_values", "x[].pipe().cumsum().groupby().transform", "x[].pipe().cumsum().groupby", "x[].pipe().cumsum", "x[].pipe"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.preprocessing.discretization.IdxDiscUnknownC.transform"], ["def", "partial_log_likelihood_ph", "(", "log_partial_hazards", ",", "durations", ",", "events", ",", "mean", "=", "True", ")", ":", "\n", "    ", "\"\"\"Partial log-likelihood for PH models.\n    \n    Arguments:\n        log_partial_hazards {np.array} -- Log partial hazards (e.g. x^T beta).\n        durations {np.array} -- Durations.\n        events {np.array} -- Events.\n    \n    Keyword Arguments:\n        mean {bool} -- Return the mean. (default: {True})\n    \n    Returns:\n        pd.Series or float -- partial log-likelihood or mean.\n    \"\"\"", "\n", "\n", "df", "=", "pd", ".", "DataFrame", "(", "dict", "(", "duration", "=", "durations", ",", "event", "=", "events", ",", "lph", "=", "log_partial_hazards", ")", ")", "\n", "pll", "=", "(", "df", "\n", ".", "sort_values", "(", "'duration'", ",", "ascending", "=", "False", ")", "\n", ".", "assign", "(", "cum_ph", "=", "(", "lambda", "x", ":", "x", "[", "'lph'", "]", "\n", ".", "pipe", "(", "np", ".", "exp", ")", "\n", ".", "cumsum", "(", ")", "\n", ".", "groupby", "(", "x", "[", "'duration'", "]", ")", "\n", ".", "transform", "(", "'max'", ")", ")", ")", "\n", ".", "loc", "[", "lambda", "x", ":", "x", "[", "'event'", "]", "==", "1", "]", "\n", ".", "assign", "(", "pll", "=", "lambda", "x", ":", "x", "[", "'lph'", "]", "-", "np", ".", "log", "(", "x", "[", "'cum_ph'", "]", ")", ")", "\n", "[", "'pll'", "]", ")", "\n", "if", "mean", ":", "\n", "        ", "return", "pll", ".", "mean", "(", ")", "\n", "", "return", "pll", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._inv_cens_scores": [[6, 33], ["numba.njit", "numba.prange", "range", "ipcw._inv_cens_scores._inv_cens_score_single"], "function", ["None"], ["@", "numba", ".", "njit", "(", "parallel", "=", "True", ")", "\n", "def", "_inv_cens_scores", "(", "func", ",", "time_grid", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "idx_ts_surv", ",", "idx_ts_censor", ",", "\n", "idx_tt_censor", ",", "scores", ",", "weights", ",", "n_times", ",", "n_indiv", ",", "max_weight", ")", ":", "\n", "    ", "def", "_inv_cens_score_single", "(", "func", ",", "ts", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "idx_ts_surv_i", ",", "\n", "idx_ts_censor_i", ",", "idx_tt_censor", ",", "scores", ",", "weights", ",", "n_indiv", ",", "max_weight", ")", ":", "\n", "        ", "min_g", "=", "1.", "/", "max_weight", "\n", "for", "i", "in", "range", "(", "n_indiv", ")", ":", "\n", "            ", "tt", "=", "durations", "[", "i", "]", "\n", "d", "=", "events", "[", "i", "]", "\n", "s", "=", "surv", "[", "idx_ts_surv_i", ",", "i", "]", "\n", "g_ts", "=", "censor_surv", "[", "idx_ts_censor_i", ",", "i", "]", "\n", "g_tt", "=", "censor_surv", "[", "idx_tt_censor", "[", "i", "]", ",", "i", "]", "\n", "g_ts", "=", "max", "(", "g_ts", ",", "min_g", ")", "\n", "g_tt", "=", "max", "(", "g_tt", ",", "min_g", ")", "\n", "score", ",", "w", "=", "func", "(", "ts", ",", "tt", ",", "s", ",", "g_ts", ",", "g_tt", ",", "d", ")", "\n", "#w = min(w, max_weight)", "\n", "scores", "[", "i", "]", "=", "score", "*", "w", "\n", "weights", "[", "i", "]", "=", "w", "\n", "\n", "", "", "for", "i", "in", "numba", ".", "prange", "(", "n_times", ")", ":", "\n", "        ", "ts", "=", "time_grid", "[", "i", "]", "\n", "idx_ts_surv_i", "=", "idx_ts_surv", "[", "i", "]", "\n", "idx_ts_censor_i", "=", "idx_ts_censor", "[", "i", "]", "\n", "scores_i", "=", "scores", "[", "i", "]", "\n", "weights_i", "=", "weights", "[", "i", "]", "\n", "_inv_cens_score_single", "(", "func", ",", "ts", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "idx_ts_surv_i", ",", "\n", "idx_ts_censor_i", ",", "idx_tt_censor", ",", "scores_i", ",", "weights_i", ",", "n_indiv", ",", "max_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._inverse_censoring_weighted_metric": [[34, 59], ["func.__class__.__module__.startswith", "ValueError", "len", "len", "numpy.zeros", "numpy.zeros", "pycox.utils.idx_at_times", "pycox.utils.idx_at_times", "pycox.utils.idx_at_times", "ipcw._inv_cens_scores", "hasattr", "numpy.array", "type", "type", "type", "type", "type", "type", "type", "numpy.sum", "numpy.sum", "ipcw._brier_score", "ipcw._binomial_log_likelihood"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._inv_cens_scores", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._brier_score", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._binomial_log_likelihood"], ["", "", "def", "_inverse_censoring_weighted_metric", "(", "func", ")", ":", "\n", "    ", "if", "not", "func", ".", "__class__", ".", "__module__", ".", "startswith", "(", "'numba'", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Need to provide numba compiled function\"", ")", "\n", "", "def", "metric", "(", "time_grid", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "index_surv", ",", "index_censor", ",", "max_weight", "=", "np", ".", "inf", ",", "\n", "reduce", "=", "True", ",", "steps_surv", "=", "'post'", ",", "steps_censor", "=", "'post'", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "time_grid", ",", "'__iter__'", ")", ":", "\n", "            ", "time_grid", "=", "np", ".", "array", "(", "[", "time_grid", "]", ")", "\n", "", "assert", "(", "type", "(", "time_grid", ")", "is", "type", "(", "durations", ")", "is", "type", "(", "events", ")", "is", "type", "(", "surv", ")", "is", "type", "(", "censor_surv", ")", "is", "\n", "type", "(", "index_surv", ")", "is", "type", "(", "index_censor", ")", "is", "np", ".", "ndarray", ")", ",", "'Need all input to be np.ndarrays'", "\n", "n_times", "=", "len", "(", "time_grid", ")", "\n", "n_indiv", "=", "len", "(", "durations", ")", "\n", "scores", "=", "np", ".", "zeros", "(", "(", "n_times", ",", "n_indiv", ")", ")", "\n", "weights", "=", "np", ".", "zeros", "(", "(", "n_times", ",", "n_indiv", ")", ")", "\n", "idx_ts_surv", "=", "utils", ".", "idx_at_times", "(", "index_surv", ",", "time_grid", ",", "steps_surv", ",", "assert_sorted", "=", "True", ")", "\n", "idx_ts_censor", "=", "utils", ".", "idx_at_times", "(", "index_censor", ",", "time_grid", ",", "steps_censor", ",", "assert_sorted", "=", "True", ")", "\n", "idx_tt_censor", "=", "utils", ".", "idx_at_times", "(", "index_censor", ",", "durations", ",", "'pre'", ",", "assert_sorted", "=", "True", ")", "\n", "if", "steps_censor", "==", "'post'", ":", "\n", "            ", "idx_tt_censor", "=", "(", "idx_tt_censor", "-", "1", ")", ".", "clip", "(", "0", ")", "\n", "#  This ensures that we get G(tt-)", "\n", "", "_inv_cens_scores", "(", "func", ",", "time_grid", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "idx_ts_surv", ",", "idx_ts_censor", ",", "\n", "idx_tt_censor", ",", "scores", ",", "weights", ",", "n_times", ",", "n_indiv", ",", "max_weight", ")", "\n", "if", "reduce", "is", "True", ":", "\n", "            ", "return", "np", ".", "sum", "(", "scores", ",", "axis", "=", "1", ")", "/", "np", ".", "sum", "(", "weights", ",", "axis", "=", "1", ")", "\n", "", "return", "scores", ",", "weights", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._brier_score": [[60, 67], ["numba.njit", "numpy.power", "numpy.power"], "function", ["None"], ["", "@", "numba", ".", "njit", "(", ")", "\n", "def", "_brier_score", "(", "ts", ",", "tt", ",", "s", ",", "g_ts", ",", "g_tt", ",", "d", ")", ":", "\n", "    ", "if", "(", "tt", "<=", "ts", ")", "and", "d", "==", "1", ":", "\n", "        ", "return", "np", ".", "power", "(", "s", ",", "2", ")", ",", "1.", "/", "g_tt", "\n", "", "if", "tt", ">", "ts", ":", "\n", "        ", "return", "np", ".", "power", "(", "1", "-", "s", ",", "2", ")", ",", "1.", "/", "g_ts", "\n", "", "return", "0.", ",", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._binomial_log_likelihood": [[68, 77], ["numba.njit", "numpy.log", "numpy.log"], "function", ["None"], ["", "@", "numba", ".", "njit", "(", ")", "\n", "def", "_binomial_log_likelihood", "(", "ts", ",", "tt", ",", "s", ",", "g_ts", ",", "g_tt", ",", "d", ",", "eps", "=", "1e-7", ")", ":", "\n", "    ", "s", "=", "eps", "if", "s", "<", "eps", "else", "s", "\n", "s", "=", "(", "1", "-", "eps", ")", "if", "s", ">", "(", "1", "-", "eps", ")", "else", "s", "\n", "if", "(", "tt", "<=", "ts", ")", "and", "d", "==", "1", ":", "\n", "        ", "return", "np", ".", "log", "(", "1", "-", "s", ")", ",", "1.", "/", "g_tt", "\n", "", "if", "tt", ">", "ts", ":", "\n", "        ", "return", "np", ".", "log", "(", "s", ")", ",", "1.", "/", "g_ts", "\n", "", "return", "0.", ",", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.ipcw._integrated_inverce_censoring_weighed_metric": [[81, 89], ["func", "scipy.integrate.simps"], "function", ["None"], ["def", "_integrated_inverce_censoring_weighed_metric", "(", "func", ")", ":", "\n", "    ", "def", "metric", "(", "time_grid", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "index_surv", ",", "index_censor", ",", "\n", "max_weight", "=", "np", ".", "inf", ",", "steps_surv", "=", "'post'", ",", "steps_censor", "=", "'post'", ")", ":", "\n", "        ", "scores", "=", "func", "(", "time_grid", ",", "durations", ",", "events", ",", "surv", ",", "censor_surv", ",", "index_surv", ",", "index_censor", ",", "\n", "max_weight", ",", "True", ",", "steps_surv", ",", "steps_censor", ")", "\n", "integral", "=", "scipy", ".", "integrate", ".", "simps", "(", "scores", ",", "time_grid", ")", "\n", "return", "integral", "/", "(", "time_grid", "[", "-", "1", "]", "-", "time_grid", "[", "0", "]", ")", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_comparable": [[7, 10], ["numba.jit"], "function", ["None"], ["@", "numba", ".", "jit", "(", "nopython", "=", "True", ")", "\n", "def", "_is_comparable", "(", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", ":", "\n", "    ", "return", "(", "(", "t_i", "<", "t_j", ")", "&", "d_i", ")", "|", "(", "(", "t_i", "==", "t_j", ")", "&", "(", "d_i", "|", "d_j", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_comparable_antolini": [[11, 14], ["numba.jit"], "function", ["None"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ")", "\n", "def", "_is_comparable_antolini", "(", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", ":", "\n", "    ", "return", "(", "(", "t_i", "<", "t_j", ")", "&", "d_i", ")", "|", "(", "(", "t_i", "==", "t_j", ")", "&", "d_i", "&", "(", "d_j", "==", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_concordant": [[15, 28], ["numba.jit", "concordance._is_comparable"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_comparable"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ")", "\n", "def", "_is_concordant", "(", "s_i", ",", "s_j", ",", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", ":", "\n", "    ", "conc", "=", "0.", "\n", "if", "t_i", "<", "t_j", ":", "\n", "        ", "conc", "=", "(", "s_i", "<", "s_j", ")", "+", "(", "s_i", "==", "s_j", ")", "*", "0.5", "\n", "", "elif", "t_i", "==", "t_j", ":", "\n", "        ", "if", "d_i", "&", "d_j", ":", "\n", "            ", "conc", "=", "1.", "-", "(", "s_i", "!=", "s_j", ")", "*", "0.5", "\n", "", "elif", "d_i", ":", "\n", "            ", "conc", "=", "(", "s_i", "<", "s_j", ")", "+", "(", "s_i", "==", "s_j", ")", "*", "0.5", "# different from RSF paper.", "\n", "", "elif", "d_j", ":", "\n", "            ", "conc", "=", "(", "s_i", ">", "s_j", ")", "+", "(", "s_i", "==", "s_j", ")", "*", "0.5", "# different from RSF paper.", "\n", "", "", "return", "conc", "*", "_is_comparable", "(", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_concordant_antolini": [[29, 32], ["numba.jit", "concordance._is_comparable_antolini"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_comparable_antolini"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ")", "\n", "def", "_is_concordant_antolini", "(", "s_i", ",", "s_j", ",", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", ":", "\n", "    ", "return", "(", "s_i", "<", "s_j", ")", "&", "_is_comparable_antolini", "(", "t_i", ",", "t_j", ",", "d_i", ",", "d_j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_comparable": [[33, 42], ["numba.jit", "numba.prange", "range", "is_comparable_func"], "function", ["None"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ",", "parallel", "=", "True", ")", "\n", "def", "_sum_comparable", "(", "t", ",", "d", ",", "is_comparable_func", ")", ":", "\n", "    ", "n", "=", "t", ".", "shape", "[", "0", "]", "\n", "count", "=", "0.", "\n", "for", "i", "in", "numba", ".", "prange", "(", "n", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "if", "j", "!=", "i", ":", "\n", "                ", "count", "+=", "is_comparable_func", "(", "t", "[", "i", "]", ",", "t", "[", "j", "]", ",", "d", "[", "i", "]", ",", "d", "[", "j", "]", ")", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_concordant": [[43, 52], ["numba.jit", "len", "numba.prange", "range", "concordance._is_concordant"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._is_concordant"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ",", "parallel", "=", "True", ")", "\n", "def", "_sum_concordant", "(", "s", ",", "t", ",", "d", ")", ":", "\n", "    ", "n", "=", "len", "(", "t", ")", "\n", "count", "=", "0.", "\n", "for", "i", "in", "numba", ".", "prange", "(", "n", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "if", "j", "!=", "i", ":", "\n", "                ", "count", "+=", "_is_concordant", "(", "s", "[", "i", ",", "i", "]", ",", "s", "[", "i", ",", "j", "]", ",", "t", "[", "i", "]", ",", "t", "[", "j", "]", ",", "d", "[", "i", "]", ",", "d", "[", "j", "]", ")", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_concordant_disc": [[53, 63], ["numba.jit", "len", "numba.prange", "range", "is_concordant_func"], "function", ["None"], ["", "@", "numba", ".", "jit", "(", "nopython", "=", "True", ",", "parallel", "=", "True", ")", "\n", "def", "_sum_concordant_disc", "(", "s", ",", "t", ",", "d", ",", "s_idx", ",", "is_concordant_func", ")", ":", "\n", "    ", "n", "=", "len", "(", "t", ")", "\n", "count", "=", "0", "\n", "for", "i", "in", "numba", ".", "prange", "(", "n", ")", ":", "\n", "        ", "idx", "=", "s_idx", "[", "i", "]", "\n", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "if", "j", "!=", "i", ":", "\n", "                ", "count", "+=", "is_concordant_func", "(", "s", "[", "idx", ",", "i", "]", ",", "s", "[", "idx", ",", "j", "]", ",", "t", "[", "i", "]", ",", "t", "[", "j", "]", ",", "d", "[", "i", "]", ",", "d", "[", "j", "]", ")", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance.concordance_td": [[64, 108], ["numpy.isfortran", "ValueError", "numpy.array", "type", "type", "type", "type", "events.astype.astype", "concordance._sum_concordant_disc", "concordance._sum_comparable", "concordance._sum_concordant_disc", "concordance._sum_comparable"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_concordant_disc", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_comparable", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_concordant_disc", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.concordance._sum_comparable"], ["", "def", "concordance_td", "(", "durations", ",", "events", ",", "surv", ",", "surv_idx", ",", "method", "=", "'adj_antolini'", ")", ":", "\n", "    ", "\"\"\"Time dependent concorance index from\n    Antolini, L.; Boracchi, P.; and Biganzoli, E. 2005. A timedependent discrimination\n    index for survival data. Statistics in Medicine 24:3927\u20133944.\n\n    If 'method' is 'antolini', the concordance from Antolini et al. is computed.\n    \n    If 'method' is 'adj_antolini' (default) we have made a small modifications\n    for ties in predictions and event times.\n    We have followed step 3. in Sec 5.1. in Random Survial Forests paper, except for the last\n    point with \"T_i = T_j, but not both are deaths\", as that doesn't make much sense.\n    See '_is_concordant'.\n\n    Arguments:\n        durations {np.array[n]} -- Event times (or censoring times.)\n        events {np.array[n]} -- Event indicators (0 is censoring).\n        surv {np.array[n_times, n]} -- Survival function (each row is a duraratoin, and each col\n            is an individual).\n        surv_idx {np.array[n_test]} -- Mapping of survival_func s.t. 'surv_idx[i]' gives index in\n            'surv' corresponding to the event time of individual 'i'.\n\n    Keyword Arguments:\n        method {str} -- Type of c-index 'antolini' or 'adj_antolini' (default {'adj_antolini'}).\n\n    Returns:\n        float -- Time dependent concordance index.\n    \"\"\"", "\n", "if", "np", ".", "isfortran", "(", "surv", ")", ":", "\n", "        ", "surv", "=", "np", ".", "array", "(", "surv", ",", "order", "=", "'C'", ")", "\n", "", "assert", "durations", ".", "shape", "[", "0", "]", "==", "surv", ".", "shape", "[", "1", "]", "==", "surv_idx", ".", "shape", "[", "0", "]", "==", "events", ".", "shape", "[", "0", "]", "\n", "assert", "type", "(", "durations", ")", "is", "type", "(", "events", ")", "is", "type", "(", "surv", ")", "is", "type", "(", "surv_idx", ")", "is", "np", ".", "ndarray", "\n", "if", "events", ".", "dtype", "in", "(", "'float'", ",", "'float32'", ")", ":", "\n", "        ", "events", "=", "events", ".", "astype", "(", "'int32'", ")", "\n", "", "if", "method", "==", "'adj_antolini'", ":", "\n", "        ", "is_concordant", "=", "_is_concordant", "\n", "is_comparable", "=", "_is_comparable", "\n", "return", "(", "_sum_concordant_disc", "(", "surv", ",", "durations", ",", "events", ",", "surv_idx", ",", "is_concordant", ")", "/", "\n", "_sum_comparable", "(", "durations", ",", "events", ",", "is_comparable", ")", ")", "\n", "", "elif", "method", "==", "'antolini'", ":", "\n", "        ", "is_concordant", "=", "_is_concordant_antolini", "\n", "is_comparable", "=", "_is_comparable_antolini", "\n", "return", "(", "_sum_concordant_disc", "(", "surv", ",", "durations", ",", "events", ",", "surv_idx", ",", "is_concordant", ")", "/", "\n", "_sum_comparable", "(", "durations", ",", "events", ",", "is_comparable", ")", ")", "\n", "", "return", "ValueError", "(", "f\"Need 'method' to be e.g. 'antolini', got '{method}'.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin.administrative_scores": [[7, 23], ["func.__class__.__module__.startswith", "ValueError", "pycox.utils.idx_at_times", "admin._admin_scores", "hasattr", "numpy.array", "type", "type", "type", "type", "type", "type", "norm.reshape", "scores.sum", "admin._brier_score", "admin._binomial_log_likelihood"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._admin_scores", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._brier_score", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._binomial_log_likelihood"], ["def", "administrative_scores", "(", "func", ")", ":", "\n", "    ", "if", "not", "func", ".", "__class__", ".", "__module__", ".", "startswith", "(", "'numba'", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Need to provide numba compiled function\"", ")", "\n", "", "def", "metric", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "index_surv", ",", "reduce", "=", "True", ",", "steps_surv", "=", "'post'", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "time_grid", ",", "'__iter__'", ")", ":", "\n", "            ", "time_grid", "=", "np", ".", "array", "(", "[", "time_grid", "]", ")", "\n", "", "assert", "(", "type", "(", "time_grid", ")", "is", "type", "(", "durations", ")", "is", "type", "(", "events", ")", "is", "type", "(", "surv", ")", "is", "\n", "type", "(", "index_surv", ")", "is", "type", "(", "durations_c", ")", "is", "np", ".", "ndarray", ")", ",", "'Need all input to be np.ndarrays'", "\n", "assert", "(", "durations", "[", "events", "==", "0", "]", "==", "durations_c", "[", "events", "==", "0", "]", ")", ".", "all", "(", ")", ",", "'Censored observations need same `durations` and `durations_c`'", "\n", "assert", "(", "durations", "[", "events", "==", "1", "]", "<=", "durations_c", "[", "events", "==", "1", "]", ")", ".", "all", "(", ")", ",", "'`durations` cannot be larger than `durations_c`'", "\n", "idx_ts_surv", "=", "idx_at_times", "(", "index_surv", ",", "time_grid", ",", "steps_surv", ",", "assert_sorted", "=", "True", ")", "\n", "scores", ",", "norm", "=", "_admin_scores", "(", "func", ",", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "idx_ts_surv", ")", "\n", "if", "reduce", "is", "True", ":", "\n", "            ", "return", "scores", ".", "sum", "(", "axis", "=", "1", ")", "/", "norm", "\n", "", "return", "scores", ",", "norm", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._admin_scores": [[24, 48], ["numba.njit", "len", "len", "numpy.empty", "np.empty.fill", "numpy.empty", "np.empty.fill", "numba.prange", "range", "admin._admin_scores._single"], "function", ["None"], ["", "@", "numba", ".", "njit", "(", "parallel", "=", "True", ")", "\n", "def", "_admin_scores", "(", "func", ",", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "idx_ts_surv", ")", ":", "\n", "    ", "def", "_single", "(", "func", ",", "ts", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "idx_ts_surv_i", ",", "\n", "scores", ",", "n_indiv", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_indiv", ")", ":", "\n", "            ", "tt", "=", "durations", "[", "i", "]", "\n", "tc", "=", "durations_c", "[", "i", "]", "\n", "d", "=", "events", "[", "i", "]", "\n", "s", "=", "surv", "[", "idx_ts_surv_i", ",", "i", "]", "\n", "scores", "[", "i", "]", "=", "func", "(", "ts", ",", "tt", ",", "tc", ",", "d", ",", "s", ")", "\n", "\n", "", "", "n_times", "=", "len", "(", "time_grid", ")", "\n", "n_indiv", "=", "len", "(", "durations", ")", "\n", "scores", "=", "np", ".", "empty", "(", "(", "n_times", ",", "n_indiv", ")", ")", "\n", "scores", ".", "fill", "(", "np", ".", "nan", ")", "\n", "normalizer", "=", "np", ".", "empty", "(", "n_times", ")", "\n", "normalizer", ".", "fill", "(", "np", ".", "nan", ")", "\n", "for", "i", "in", "numba", ".", "prange", "(", "n_times", ")", ":", "\n", "        ", "ts", "=", "time_grid", "[", "i", "]", "\n", "idx_ts_surv_i", "=", "idx_ts_surv", "[", "i", "]", "\n", "scores_i", "=", "scores", "[", "i", "]", "\n", "normalizer", "[", "i", "]", "=", "(", "durations_c", ">=", "ts", ")", ".", "sum", "(", ")", "\n", "_single", "(", "func", ",", "ts", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "idx_ts_surv_i", ",", "scores_i", ",", "n_indiv", ")", "\n", "", "return", "scores", ",", "normalizer", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._brier_score": [[49, 56], ["numpy.power", "numpy.power"], "function", ["None"], ["", "@", "numba", ".", "njit", "\n", "def", "_brier_score", "(", "ts", ",", "tt", ",", "tc", ",", "d", ",", "s", ")", ":", "\n", "    ", "if", "(", "tt", "<=", "ts", ")", "and", "(", "d", "==", "1", ")", "and", "(", "tc", ">=", "ts", ")", ":", "\n", "        ", "return", "np", ".", "power", "(", "s", ",", "2", ")", "\n", "", "if", "tt", ">=", "ts", ":", "\n", "        ", "return", "np", ".", "power", "(", "1", "-", "s", ",", "2", ")", "\n", "", "return", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._binomial_log_likelihood": [[57, 68], ["numpy.log", "numpy.log"], "function", ["None"], ["", "@", "numba", ".", "njit", "\n", "def", "_binomial_log_likelihood", "(", "ts", ",", "tt", ",", "tc", ",", "d", ",", "s", ",", "eps", "=", "1e-7", ")", ":", "\n", "    ", "if", "s", "<", "eps", ":", "\n", "        ", "s", "=", "eps", "\n", "", "elif", "s", ">", "(", "1", "-", "eps", ")", ":", "\n", "        ", "s", "=", "1", "-", "eps", "\n", "", "if", "(", "tt", "<=", "ts", ")", "and", "(", "d", "==", "1", ")", "and", "(", "tc", ">=", "ts", ")", ":", "\n", "        ", "return", "np", ".", "log", "(", "1", "-", "s", ")", "\n", "", "if", "tt", ">=", "ts", ":", "\n", "        ", "return", "np", ".", "log", "(", "s", ")", "\n", "", "return", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.admin._integrated_admin_metric": [[73, 79], ["func", "scipy.integrate.simps"], "function", ["None"], ["def", "_integrated_admin_metric", "(", "func", ")", ":", "\n", "    ", "def", "metric", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "index_surv", ",", "steps_surv", "=", "'post'", ")", ":", "\n", "        ", "scores", "=", "func", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "index_surv", ",", "True", ",", "steps_surv", ")", "\n", "integral", "=", "scipy", ".", "integrate", ".", "simps", "(", "scores", ",", "time_grid", ")", "\n", "return", "integral", "/", "(", "time_grid", "[", "-", "1", "]", "-", "time_grid", "[", "0", "]", ")", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__init__": [[28, 37], ["type", "type", "pandas.Series"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "surv", ",", "durations", ",", "events", ",", "censor_surv", "=", "None", ",", "censor_durations", "=", "None", ",", "steps", "=", "'post'", ")", ":", "\n", "        ", "assert", "(", "type", "(", "durations", ")", "==", "type", "(", "events", ")", "==", "np", ".", "ndarray", ")", ",", "'Need `durations` and `events` to be arrays'", "\n", "self", ".", "surv", "=", "surv", "\n", "self", ".", "durations", "=", "durations", "\n", "self", ".", "events", "=", "events", "\n", "self", ".", "censor_surv", "=", "censor_surv", "\n", "self", ".", "censor_durations", "=", "censor_durations", "\n", "self", ".", "steps", "=", "steps", "\n", "assert", "pd", ".", "Series", "(", "self", ".", "index_surv", ")", ".", "is_monotonic", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.censor_surv": [[45, 58], ["isinstance", "type", "eval_surv.EvalSurv.add_km_censor", "ValueError", "eval_surv.EvalSurv.add_censor_est"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.add_km_censor", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.add_censor_est"], ["", "@", "censor_surv", ".", "setter", "\n", "def", "censor_surv", "(", "self", ",", "censor_surv", ")", ":", "\n", "        ", "if", "isinstance", "(", "censor_surv", ",", "EvalSurv", ")", ":", "\n", "            ", "self", ".", "_censor_surv", "=", "censor_surv", "\n", "", "elif", "type", "(", "censor_surv", ")", "is", "str", ":", "\n", "            ", "if", "censor_surv", "==", "'km'", ":", "\n", "                ", "self", ".", "add_km_censor", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"censor_surv cannot be {censor_surv}. Use e.g. 'km'\"", ")", "\n", "", "", "elif", "censor_surv", "is", "not", "None", ":", "\n", "            ", "self", ".", "add_censor_est", "(", "censor_surv", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_censor_surv", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.index_surv": [[59, 62], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "index_surv", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "surv", ".", "index", ".", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.steps": [[76, 82], ["ValueError"], "methods", ["None"], ["", "@", "steps", ".", "setter", "\n", "def", "steps", "(", "self", ",", "steps", ")", ":", "\n", "        ", "vals", "=", "[", "'post'", ",", "'pre'", "]", "\n", "if", "steps", "not", "in", "vals", ":", "\n", "            ", "raise", "ValueError", "(", "f\"`steps` needs to be {vals}, got {steps}\"", ")", "\n", "", "self", ".", "_steps", "=", "steps", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.add_censor_est": [[83, 99], ["isinstance", "eval_surv.EvalSurv._constructor"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv._constructor"], ["", "def", "add_censor_est", "(", "self", ",", "censor_surv", ",", "steps", "=", "'post'", ")", ":", "\n", "        ", "\"\"\"Add censoring estimates so one can use inverse censoring weighting.\n        `censor_surv` are the survival estimates trained on (durations, 1-events),\n        \n        Arguments:\n            censor_surv {pd.DataFrame} -- Censor survival curves.\n\n    Keyword Arguments:\n        round {str} -- For durations between values of `surv.index` choose the higher index 'pre'\n            or lower index 'post'. If `None` use `self.steps` (default: {None})\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "censor_surv", ",", "EvalSurv", ")", ":", "\n", "            ", "censor_surv", "=", "self", ".", "_constructor", "(", "censor_surv", ",", "self", ".", "durations", ",", "1", "-", "self", ".", "events", ",", "None", ",", "\n", "steps", "=", "steps", ")", "\n", "", "self", ".", "censor_surv", "=", "censor_surv", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.add_km_censor": [[100, 108], ["pycox.utils.kaplan_meier", "pandas.DataFrame", "eval_surv.EvalSurv.add_censor_est", "numpy.repeat", "pycox.utils.kaplan_meier.values.reshape", "len"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.kaplan_meier", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.add_censor_est"], ["", "def", "add_km_censor", "(", "self", ",", "steps", "=", "'post'", ")", ":", "\n", "        ", "\"\"\"Add censoring estimates obtained by Kaplan-Meier on the test set\n        (durations, 1-events).\n        \"\"\"", "\n", "km", "=", "utils", ".", "kaplan_meier", "(", "self", ".", "durations", ",", "1", "-", "self", ".", "events", ")", "\n", "surv", "=", "pd", ".", "DataFrame", "(", "np", ".", "repeat", "(", "km", ".", "values", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "len", "(", "self", ".", "durations", ")", ",", "axis", "=", "1", ")", ",", "\n", "index", "=", "km", ".", "index", ")", "\n", "return", "self", ".", "add_censor_est", "(", "surv", ",", "steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.censor_durations": [[114, 127], ["warnings.warn"], "methods", ["None"], ["", "@", "censor_durations", ".", "setter", "\n", "def", "censor_durations", "(", "self", ",", "val", ")", ":", "\n", "        ", "if", "val", "is", "not", "None", ":", "\n", "            ", "assert", "(", "self", ".", "durations", "[", "self", ".", "events", "==", "0", "]", "==", "val", "[", "self", ".", "events", "==", "0", "]", ")", ".", "all", "(", ")", ",", "'Censored observations need same `durations` and `censor_durations`'", "\n", "assert", "(", "self", ".", "durations", "[", "self", ".", "events", "==", "1", "]", "<=", "val", "[", "self", ".", "events", "==", "1", "]", ")", ".", "all", "(", ")", ",", "'`durations` cannot be larger than `censor_durations`'", "\n", "if", "(", "self", ".", "durations", "==", "val", ")", ".", "all", "(", ")", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"`censor_durations` are equal to `durations`.\"", "+", "\n", "\" `censor_durations` are likely wrong!\"", ")", "\n", "", "self", ".", "_censor_durations", "=", "val", "\n", "", "else", ":", "\n", "            ", "self", ".", "_censor_durations", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv._constructor": [[128, 131], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "_constructor", "(", "self", ")", ":", "\n", "        ", "return", "EvalSurv", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.__getitem__": [[132, 142], ["eval_surv.EvalSurv._constructor", "hasattr", "type"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv._constructor"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "not", "(", "hasattr", "(", "index", ",", "'__iter__'", ")", "or", "type", "(", "index", ")", "is", "slice", ")", ":", "\n", "            ", "index", "=", "[", "index", "]", "\n", "", "surv", "=", "self", ".", "surv", ".", "iloc", "[", ":", ",", "index", "]", "\n", "durations", "=", "self", ".", "durations", "[", "index", "]", "\n", "events", "=", "self", ".", "events", "[", "index", "]", "\n", "new", "=", "self", ".", "_constructor", "(", "surv", ",", "durations", ",", "events", ",", "None", ",", "steps", "=", "self", ".", "steps", ")", "\n", "if", "self", ".", "censor_surv", "is", "not", "None", ":", "\n", "            ", "new", ".", "censor_surv", "=", "self", ".", "censor_surv", "[", "index", "]", "\n", "", "return", "new", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.plot_surv": [[143, 152], ["eval_surv.EvalSurv.surv.plot", "len", "RuntimeError", "RuntimeError"], "methods", ["None"], ["", "def", "plot_surv", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Plot survival estimates. \n        kwargs are passed to `self.surv.plot`.\n        \"\"\"", "\n", "if", "len", "(", "self", ".", "durations", ")", ">", "50", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"We don't allow to plot more than 50 lines. Use e.g. `ev[1:5].plot()`\"", ")", "\n", "", "if", "'drawstyle'", "in", "kwargs", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"`drawstyle` is set by `self.steps`. Remove from **kwargs\"", ")", "\n", "", "return", "self", ".", "surv", ".", "plot", "(", "drawstyle", "=", "f\"steps-{self.steps}\"", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times": [[153, 160], ["pycox.utils.idx_at_times"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times"], ["", "def", "idx_at_times", "(", "self", ",", "times", ")", ":", "\n", "        ", "\"\"\"Get the index (iloc) of the `surv.index` closest to `times`.\n        I.e. surv.loc[tims] (almost)= surv.iloc[idx_at_times(times)].\n\n        Useful for finding predictions at given durations.\n        \"\"\"", "\n", "return", "utils", ".", "idx_at_times", "(", "self", ".", "index_surv", ",", "times", ",", "self", ".", "steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv._duration_idx": [[161, 163], ["eval_surv.EvalSurv.idx_at_times"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times"], ["", "def", "_duration_idx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "idx_at_times", "(", "self", ".", "durations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.surv_at_times": [[164, 167], ["eval_surv.EvalSurv.idx_at_times"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.idx_at_times"], ["", "def", "surv_at_times", "(", "self", ",", "times", ")", ":", "\n", "        ", "idx", "=", "self", ".", "idx_at_times", "(", "times", ")", "\n", "return", "self", ".", "surv", ".", "iloc", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.concordance_td": [[171, 192], ["pycox.evaluation.concordance.concordance_td", "eval_surv.EvalSurv._duration_idx"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.concordance_td", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv._duration_idx"], ["", "def", "concordance_td", "(", "self", ",", "method", "=", "'adj_antolini'", ")", ":", "\n", "        ", "\"\"\"Time dependent concorance index from\n        Antolini, L.; Boracchi, P.; and Biganzoli, E. 2005. A time-dependent discrimination\n        index for survival data. Statistics in Medicine 24:3927\u20133944.\n\n        If 'method' is 'antolini', the concordance from Antolini et al. is computed.\n    \n        If 'method' is 'adj_antolini' (default) we have made a small modifications\n        for ties in predictions and event times.\n        We have followed step 3. in Sec 5.1. in Random Survival Forests paper, except for the last\n        point with \"T_i = T_j, but not both are deaths\", as that doesn't make much sense.\n        See 'metrics._is_concordant'.\n\n        Keyword Arguments:\n            method {str} -- Type of c-index 'antolini' or 'adj_antolini' (default {'adj_antolini'}).\n\n        Returns:\n            float -- Time dependent concordance index.\n        \"\"\"", "\n", "return", "concordance_td", "(", "self", ".", "durations", ",", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "\n", "self", ".", "_duration_idx", "(", ")", ",", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score": [[193, 217], ["pycox.evaluation.ipcw.brier_score", "pandas.Series().rename", "ValueError", "pandas.Series"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score"], ["", "def", "brier_score", "(", "self", ",", "time_grid", ",", "max_weight", "=", "np", ".", "inf", ")", ":", "\n", "        ", "\"\"\"Brier score weighted by the inverse censoring distribution.\n        See Section 3.1.2 or [1] for details of the wighting scheme.\n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        Keyword Arguments:\n            max_weight {float} -- Max weight value (max number of individuals an individual\n                can represent (default {np.inf}).\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_surv", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"\"\"Need to add censor_surv to compute Brier score. Use 'add_censor_est'\n            or 'add_km_censor' for Kaplan-Meier\"\"\"", ")", "\n", "", "bs", "=", "ipcw", ".", "brier_score", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "\n", "self", ".", "censor_surv", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "\n", "self", ".", "censor_surv", ".", "index_surv", ",", "max_weight", ",", "True", ",", "self", ".", "steps", ",", "\n", "self", ".", "censor_surv", ".", "steps", ")", "\n", "return", "pd", ".", "Series", "(", "bs", ",", "index", "=", "time_grid", ")", ".", "rename", "(", "'brier_score'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.nbll": [[218, 242], ["pycox.evaluation.ipcw.binomial_log_likelihood", "pandas.Series().rename", "ValueError", "pandas.Series"], "methods", ["None"], ["", "def", "nbll", "(", "self", ",", "time_grid", ",", "max_weight", "=", "np", ".", "inf", ")", ":", "\n", "        ", "\"\"\"Negative binomial log-likelihood weighted by the inverse censoring distribution.\n        See Section 3.1.2 or [1] for details of the wighting scheme.\n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        Keyword Arguments:\n            max_weight {float} -- Max weight value (max number of individuals an individual\n                can represent (default {np.inf}).\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_surv", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"\"\"Need to add censor_surv to compute the score. Use 'add_censor_est'\n            or 'add_km_censor' for Kaplan-Meier\"\"\"", ")", "\n", "", "bll", "=", "ipcw", ".", "binomial_log_likelihood", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "\n", "self", ".", "censor_surv", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "\n", "self", ".", "censor_surv", ".", "index_surv", ",", "max_weight", ",", "True", ",", "self", ".", "steps", ",", "\n", "self", ".", "censor_surv", ".", "steps", ")", "\n", "return", "pd", ".", "Series", "(", "-", "bll", ",", "index", "=", "time_grid", ")", ".", "rename", "(", "'nbll'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_brier_score": [[243, 260], ["pycox.evaluation.ipcw.integrated_brier_score", "ValueError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_brier_score"], ["", "def", "integrated_brier_score", "(", "self", ",", "time_grid", ",", "max_weight", "=", "np", ".", "inf", ")", ":", "\n", "        ", "\"\"\"Integrated Brier score weighted by the inverse censoring distribution.\n        Essentially an integral over values obtained from `brier_score(time_grid, max_weight)`.\n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        Keyword Arguments:\n            max_weight {float} -- Max weight value (max number of individuals an individual\n                can represent (default {np.inf}).\n        \"\"\"", "\n", "if", "self", ".", "censor_surv", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to add censor_surv to compute briser score. Use 'add_censor_est'\"", ")", "\n", "", "return", "ipcw", ".", "integrated_brier_score", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "\n", "self", ".", "censor_surv", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "\n", "self", ".", "censor_surv", ".", "index_surv", ",", "max_weight", ",", "self", ".", "steps", ",", "\n", "self", ".", "censor_surv", ".", "steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_nbll": [[261, 279], ["pycox.evaluation.ipcw.integrated_binomial_log_likelihood", "ValueError"], "methods", ["None"], ["", "def", "integrated_nbll", "(", "self", ",", "time_grid", ",", "max_weight", "=", "np", ".", "inf", ")", ":", "\n", "        ", "\"\"\"Integrated negative binomial log-likelihood weighted by the inverse censoring distribution.\n        Essentially an integral over values obtained from `nbll(time_grid, max_weight)`.\n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        Keyword Arguments:\n            max_weight {float} -- Max weight value (max number of individuals an individual\n                can represent (default {np.inf}).\n        \"\"\"", "\n", "if", "self", ".", "censor_surv", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to add censor_surv to compute the score. Use 'add_censor_est'\"", ")", "\n", "", "ibll", "=", "ipcw", ".", "integrated_binomial_log_likelihood", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "\n", "self", ".", "censor_surv", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "\n", "self", ".", "censor_surv", ".", "index_surv", ",", "max_weight", ",", "self", ".", "steps", ",", "\n", "self", ".", "censor_surv", ".", "steps", ")", "\n", "return", "-", "ibll", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score_admin": [[280, 298], ["pycox.evaluation.admin.brier_score", "pandas.Series().rename", "ValueError", "pandas.Series"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score"], ["", "def", "brier_score_admin", "(", "self", ",", "time_grid", ")", ":", "\n", "        ", "\"\"\"The Administrative Brier score proposed by [1].\n        Removes individuals as they are administratively censored, event if they have experienced an\n        event. \n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_durations", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to provide `censor_durations` (censoring durations) to use this method\"", ")", "\n", "", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "censor_durations", ",", "self", ".", "events", ",", "\n", "self", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "True", ",", "self", ".", "steps", ")", "\n", "return", "pd", ".", "Series", "(", "bs", ",", "index", "=", "time_grid", ")", ".", "rename", "(", "'brier_score'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_brier_score_admin": [[299, 317], ["pycox.evaluation.admin.integrated_brier_score", "ValueError"], "methods", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_brier_score"], ["", "def", "integrated_brier_score_admin", "(", "self", ",", "time_grid", ")", ":", "\n", "        ", "\"\"\"The Integrated administrative Brier score proposed by [1].\n        Removes individuals as they are administratively censored, event if they have experienced an\n        event. \n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_durations", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to provide `censor_durations` (censoring durations) to use this method\"", ")", "\n", "", "ibs", "=", "admin", ".", "integrated_brier_score", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "censor_durations", ",", "self", ".", "events", ",", "\n", "self", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "self", ".", "steps", ")", "\n", "return", "ibs", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.nbll_admin": [[318, 336], ["pycox.evaluation.admin.binomial_log_likelihood", "pandas.Series().rename", "ValueError", "pandas.Series"], "methods", ["None"], ["", "def", "nbll_admin", "(", "self", ",", "time_grid", ")", ":", "\n", "        ", "\"\"\"The negative administrative binomial log-likelihood proposed by [1].\n        Removes individuals as they are administratively censored, event if they have experienced an\n        event. \n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_durations", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to provide `censor_durations` (censoring durations) to use this method\"", ")", "\n", "", "bll", "=", "admin", ".", "binomial_log_likelihood", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "censor_durations", ",", "self", ".", "events", ",", "\n", "self", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "True", ",", "self", ".", "steps", ")", "\n", "return", "pd", ".", "Series", "(", "-", "bll", ",", "index", "=", "time_grid", ")", ".", "rename", "(", "'nbll'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.integrated_nbll_admin": [[337, 356], ["pycox.evaluation.admin.integrated_binomial_log_likelihood", "ValueError"], "methods", ["None"], ["", "def", "integrated_nbll_admin", "(", "self", ",", "time_grid", ")", ":", "\n", "        ", "\"\"\"The Integrated negative administrative binomial log-likelihood score proposed by [1].\n        Removes individuals as they are administratively censored, event if they have experienced an\n        event. \n        \n        Arguments:\n            time_grid {np.array} -- Durations where the brier score should be calculated.\n\n        References:\n            [1] H\u00e5vard Kvamme and \u00d8rnulf Borgan. The Brier Score under Administrative Censoring: Problems\n                and Solutions. arXiv preprint arXiv:1912.08581, 2019.\n                https://arxiv.org/pdf/1912.08581.pdf\n        \"\"\"", "\n", "if", "self", ".", "censor_durations", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need to provide `censor_durations` (censoring durations) to use this method\"", ")", "\n", "", "ibll", "=", "admin", ".", "integrated_binomial_log_likelihood", "(", "time_grid", ",", "self", ".", "durations", ",", "self", ".", "censor_durations", ",", "\n", "self", ".", "events", ",", "self", ".", "surv", ".", "values", ",", "self", ".", "index_surv", ",", "\n", "self", ".", "steps", ")", "\n", "return", "-", "ibll", "\n", "", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.test_admin.test_brier_score_no_censor": [[7, 28], ["numpy.array", "numpy.ones", "numpy.array", "pycox.evaluation.admin.brier_score", "pycox.evaluation.admin.brier_score", "pycox.evaluation.admin.brier_score", "numpy.array", "pycox.evaluation.admin.brier_score", "numpy.isnan().all", "numpy.ones", "numpy.ones_like", "numpy.isnan", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score", "home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score"], ["def", "test_brier_score_no_censor", "(", ")", ":", "\n", "    ", "n", "=", "4", "\n", "durations", "=", "np", ".", "ones", "(", "n", ")", "*", "50", "\n", "durations_c", "=", "np", ".", "ones_like", "(", "durations", ")", "*", "100", "\n", "events", "=", "durations", "<=", "durations_c", "\n", "m", "=", "5", "\n", "index_surv", "=", "np", ".", "array", "(", "[", "0", ",", "25.", ",", "50.", ",", "75.", ",", "100.", "]", ")", "\n", "\n", "surv_ones", "=", "np", ".", "ones", "(", "(", "m", ",", "n", ")", ")", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "5.", ",", "40.", ",", "60.", ",", "100.", "]", ")", "\n", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_ones", ",", "index_surv", ")", "\n", "assert", "(", "bs", "==", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "1.", ",", "1.", "]", ")", ")", ".", "all", "(", ")", "\n", "surv_zeros", "=", "surv_ones", "*", "0", "\n", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_zeros", ",", "index_surv", ")", "\n", "assert", "(", "bs", "==", "np", ".", "array", "(", "[", "1.", ",", "1.", ",", "0.", ",", "0.", "]", ")", ")", ".", "all", "(", ")", "\n", "surv_05", "=", "surv_ones", "*", "0.5", "\n", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_05", ",", "index_surv", ")", "\n", "assert", "(", "bs", "==", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", "]", ")", ")", ".", "all", "(", ")", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "110.", "]", ")", "\n", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_05", ",", "index_surv", ")", "\n", "assert", "np", ".", "isnan", "(", "bs", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.test_admin.test_brier_score_censor": [[29, 43], ["numpy.array", "numpy.array", "numpy.ones", "numpy.array", "pycox.evaluation.admin.brier_score", "numpy.ones", "numpy.array"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score"], ["", "def", "test_brier_score_censor", "(", ")", ":", "\n", "    ", "n", "=", "4", "\n", "durations", "=", "np", ".", "ones", "(", "n", ")", "*", "50", "\n", "durations_c", "=", "np", ".", "array", "(", "[", "25", ",", "50", ",", "60", ",", "100", "]", ")", "\n", "events", "=", "durations", "<=", "durations_c", "\n", "durations", "[", "~", "events", "]", "=", "durations_c", "[", "~", "events", "]", "\n", "m", "=", "5", "\n", "index_surv", "=", "np", ".", "array", "(", "[", "0", ",", "25.", ",", "50.", ",", "75.", ",", "100.", "]", ")", "\n", "\n", "surv", "=", "np", ".", "ones", "(", "(", "m", ",", "n", ")", ")", "\n", "surv", "[", ":", ",", "0", "]", "=", "0", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "5.", ",", "25.", ",", "40.", ",", "60.", ",", "100.", "]", ")", "\n", "bs", "=", "admin", ".", "brier_score", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv", ",", "index_surv", ")", "\n", "assert", "(", "bs", "==", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.", ",", "1.", ",", "1.", "]", ")", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.test_admin.test_brier_score_evalsurv": [[44, 60], ["numpy.array", "numpy.array", "numpy.ones", "pandas.DataFrame", "numpy.array", "pycox.evaluation.EvalSurv", "pycox.evaluation.EvalSurv.brier_score_admin", "numpy.ones", "numpy.array"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.evaluation.eval_surv.EvalSurv.brier_score_admin"], ["", "def", "test_brier_score_evalsurv", "(", ")", ":", "\n", "    ", "n", "=", "4", "\n", "durations", "=", "np", ".", "ones", "(", "n", ")", "*", "50", "\n", "durations_c", "=", "np", ".", "array", "(", "[", "25", ",", "50", ",", "60", ",", "100", "]", ")", "\n", "events", "=", "durations", "<=", "durations_c", "\n", "durations", "[", "~", "events", "]", "=", "durations_c", "[", "~", "events", "]", "\n", "m", "=", "5", "\n", "index_surv", "=", "np", ".", "array", "(", "[", "0", ",", "25.", ",", "50.", ",", "75.", ",", "100.", "]", ")", "\n", "\n", "surv", "=", "np", ".", "ones", "(", "(", "m", ",", "n", ")", ")", "\n", "surv", "[", ":", ",", "0", "]", "=", "0", "\n", "surv", "=", "pd", ".", "DataFrame", "(", "surv", ",", "index_surv", ")", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "5.", ",", "25.", ",", "40.", ",", "60.", ",", "100.", "]", ")", "\n", "ev", "=", "EvalSurv", "(", "surv", ",", "durations", ",", "events", ",", "censor_durations", "=", "durations_c", ")", "\n", "bs", "=", "ev", ".", "brier_score_admin", "(", "time_grid", ")", "\n", "assert", "(", "bs", ".", "values", "==", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.", ",", "1.", ",", "1.", "]", ")", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.evaluation.test_admin.test_binoial_log_likelihood_no_censor": [[62, 84], ["numpy.array", "numpy.ones", "numpy.array", "pycox.evaluation.admin.binomial_log_likelihood", "pycox.evaluation.admin.binomial_log_likelihood", "pycox.evaluation.admin.binomial_log_likelihood", "numpy.array", "pycox.evaluation.admin.binomial_log_likelihood", "numpy.isnan().all", "numpy.ones", "numpy.ones_like", "abs().max", "abs().max", "abs().max", "numpy.isnan", "abs", "abs", "abs", "numpy.log", "numpy.log", "numpy.log"], "function", ["None"], ["", "def", "test_binoial_log_likelihood_no_censor", "(", ")", ":", "\n", "    ", "n", "=", "4", "\n", "durations", "=", "np", ".", "ones", "(", "n", ")", "*", "50", "\n", "durations_c", "=", "np", ".", "ones_like", "(", "durations", ")", "*", "100", "\n", "events", "=", "durations", "<=", "durations_c", "\n", "m", "=", "5", "\n", "index_surv", "=", "np", ".", "array", "(", "[", "0", ",", "25.", ",", "50.", ",", "75.", ",", "100.", "]", ")", "\n", "\n", "surv_ones", "=", "np", ".", "ones", "(", "(", "m", ",", "n", ")", ")", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "5.", ",", "40.", ",", "60.", ",", "100.", "]", ")", "\n", "bll", "=", "admin", ".", "binomial_log_likelihood", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_ones", ",", "index_surv", ")", "\n", "eps", "=", "1e-7", "\n", "assert", "abs", "(", "bll", "-", "np", ".", "log", "(", "[", "1", "-", "eps", ",", "1", "-", "eps", ",", "eps", ",", "eps", "]", ")", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "surv_zeros", "=", "surv_ones", "*", "0", "\n", "bll", "=", "admin", ".", "binomial_log_likelihood", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_zeros", ",", "index_surv", ")", "\n", "assert", "abs", "(", "bll", "-", "np", ".", "log", "(", "[", "eps", ",", "eps", ",", "1", "-", "eps", ",", "1", "-", "eps", "]", ")", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "surv_05", "=", "surv_ones", "*", "0.5", "\n", "bll", "=", "admin", ".", "binomial_log_likelihood", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_05", ",", "index_surv", ")", "\n", "assert", "abs", "(", "bll", "-", "np", ".", "log", "(", "[", "0.5", ",", "0.5", ",", "1", "-", "0.5", ",", "1", "-", "0.5", "]", ")", ")", ".", "max", "(", ")", "<", "1e-7", "\n", "time_grid", "=", "np", ".", "array", "(", "[", "110.", "]", ")", "\n", "bll", "=", "admin", ".", "binomial_log_likelihood", "(", "time_grid", ",", "durations", ",", "durations_c", ",", "events", ",", "surv_05", ",", "index_surv", ")", "\n", "assert", "np", ".", "isnan", "(", "bll", ")", ".", "all", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.havakv_pycox.tests.test_utils.test_kaplan_meier": [[5, 11], ["numpy.array", "numpy.array", "pycox.utils.kaplan_meier", "numpy.arange", "numpy.array"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.kaplan_meier"], ["def", "test_kaplan_meier", "(", ")", ":", "\n", "    ", "durations", "=", "np", ".", "array", "(", "[", "1.", ",", "1.", ",", "2.", ",", "3.", "]", ")", "\n", "events", "=", "np", ".", "array", "(", "[", "1", ",", "1", ",", "1", ",", "0", "]", ")", "\n", "surv", "=", "utils", ".", "kaplan_meier", "(", "durations", ",", "events", ")", "\n", "assert", "(", "surv", ".", "index", ".", "values", "==", "np", ".", "arange", "(", "4", ",", "dtype", "=", "float", ")", ")", ".", "all", "(", ")", "\n", "assert", "(", "surv", ".", "values", "==", "np", ".", "array", "(", "[", "1.", ",", "0.5", ",", "0.25", ",", "0.25", "]", ")", ")", ".", "all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.havakv_pycox.tests.test_utils.test_kaplan_meier_vs_lifelines": [[12, 24], ["pytest.mark.parametrize", "pytest.mark.parametrize", "numpy.random.seed", "numpy.random.uniform", "numpy.random.binomial().astype", "pycox.utils.kaplan_meier", "numpy.random.binomial", "KaplanMeierFitter().fit", "KaplanMeierFitter"], "function", ["home.repos.pwc.inspect_result.havakv_pycox.pycox.utils.kaplan_meier", "home.repos.pwc.inspect_result.havakv_pycox.models.cox._CoxBase.fit"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'n'", ",", "[", "10", ",", "85", ",", "259", "]", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'p_cens'", ",", "[", "0", ",", "0.3", ",", "0.8", "]", ")", "\n", "def", "test_kaplan_meier_vs_lifelines", "(", "n", ",", "p_cens", ")", ":", "\n", "    ", "from", "lifelines", "import", "KaplanMeierFitter", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "durations", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "100", ",", "n", ")", "\n", "events", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "1", "-", "p_cens", ",", "n", ")", ".", "astype", "(", "'float'", ")", "\n", "km", "=", "utils", ".", "kaplan_meier", "(", "durations", ",", "events", ")", "\n", "kmf", "=", "KaplanMeierFitter", "(", ")", ".", "fit", "(", "durations", ",", "events", ")", ".", "survival_function_", "[", "'KM_estimate'", "]", "\n", "assert", "km", ".", "shape", "==", "kmf", ".", "shape", "\n", "assert", "(", "km", "-", "kmf", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "<", "1e-14", "\n", "assert", "(", "km", ".", "index", "==", "kmf", ".", "index", ")", ".", "all", "(", ")", "\n", "", ""]]}